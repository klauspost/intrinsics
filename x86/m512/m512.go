package m512

import . "github.com/klauspost/intrinsics/x86"

// AbsEpi16: Compute the absolute value of packed 16-bit integers in 'a', and
// store the unsigned results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			dst[i+15:i] := ABS(a[i+15:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPABSW'. Intrinsic: '_mm512_abs_epi16'.
// Requires AVX512BW.
func AbsEpi16(a M512i) M512i {
	return M512i(absEpi16([64]byte(a)))
}

func absEpi16(a [64]byte) [64]byte


// MaskAbsEpi16: Compute the absolute value of packed 16-bit integers in 'a',
// and store the unsigned results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := ABS(a[i+15:i])
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPABSW'. Intrinsic: '_mm512_mask_abs_epi16'.
// Requires AVX512BW.
func MaskAbsEpi16(src M512i, k Mmask32, a M512i) M512i {
	return M512i(maskAbsEpi16([64]byte(src), uint32(k), [64]byte(a)))
}

func maskAbsEpi16(src [64]byte, k uint32, a [64]byte) [64]byte


// MaskzAbsEpi16: Compute the absolute value of packed 16-bit integers in 'a',
// and store the unsigned results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := ABS(a[i+15:i])
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPABSW'. Intrinsic: '_mm512_maskz_abs_epi16'.
// Requires AVX512BW.
func MaskzAbsEpi16(k Mmask32, a M512i) M512i {
	return M512i(maskzAbsEpi16(uint32(k), [64]byte(a)))
}

func maskzAbsEpi16(k uint32, a [64]byte) [64]byte


// AbsEpi32: Compute the absolute value of packed 32-bit integers in 'a', and
// store the unsigned results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ABS(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPABSD'. Intrinsic: '_mm512_abs_epi32'.
// Requires AVX512F.
func AbsEpi32(a M512i) M512i {
	return M512i(absEpi32([64]byte(a)))
}

func absEpi32(a [64]byte) [64]byte


// MaskAbsEpi32: Compute the absolute value of packed 32-bit integers in 'a',
// and store the unsigned results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ABS(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPABSD'. Intrinsic: '_mm512_mask_abs_epi32'.
// Requires AVX512F.
func MaskAbsEpi32(src M512i, k Mmask16, a M512i) M512i {
	return M512i(maskAbsEpi32([64]byte(src), uint16(k), [64]byte(a)))
}

func maskAbsEpi32(src [64]byte, k uint16, a [64]byte) [64]byte


// MaskzAbsEpi32: Compute the absolute value of packed 32-bit integers in 'a',
// and store the unsigned results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ABS(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPABSD'. Intrinsic: '_mm512_maskz_abs_epi32'.
// Requires AVX512F.
func MaskzAbsEpi32(k Mmask16, a M512i) M512i {
	return M512i(maskzAbsEpi32(uint16(k), [64]byte(a)))
}

func maskzAbsEpi32(k uint16, a [64]byte) [64]byte


// AbsEpi64: Compute the absolute value of packed 64-bit integers in 'a', and
// store the unsigned results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ABS(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPABSQ'. Intrinsic: '_mm512_abs_epi64'.
// Requires AVX512F.
func AbsEpi64(a M512i) M512i {
	return M512i(absEpi64([64]byte(a)))
}

func absEpi64(a [64]byte) [64]byte


// MaskAbsEpi64: Compute the absolute value of packed 64-bit integers in 'a',
// and store the unsigned results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ABS(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPABSQ'. Intrinsic: '_mm512_mask_abs_epi64'.
// Requires AVX512F.
func MaskAbsEpi64(src M512i, k Mmask8, a M512i) M512i {
	return M512i(maskAbsEpi64([64]byte(src), uint8(k), [64]byte(a)))
}

func maskAbsEpi64(src [64]byte, k uint8, a [64]byte) [64]byte


// MaskzAbsEpi64: Compute the absolute value of packed 64-bit integers in 'a',
// and store the unsigned results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ABS(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPABSQ'. Intrinsic: '_mm512_maskz_abs_epi64'.
// Requires AVX512F.
func MaskzAbsEpi64(k Mmask8, a M512i) M512i {
	return M512i(maskzAbsEpi64(uint8(k), [64]byte(a)))
}

func maskzAbsEpi64(k uint8, a [64]byte) [64]byte


// AbsEpi8: Compute the absolute value of packed 8-bit integers in 'a', and
// store the unsigned results in 'dst'. 
//
//		FOR j := 0 to 63
//			i := j*8
//			dst[i+7:i] := ABS(a[i+7:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPABSB'. Intrinsic: '_mm512_abs_epi8'.
// Requires AVX512BW.
func AbsEpi8(a M512i) M512i {
	return M512i(absEpi8([64]byte(a)))
}

func absEpi8(a [64]byte) [64]byte


// MaskAbsEpi8: Compute the absolute value of packed 8-bit integers in 'a', and
// store the unsigned results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := ABS(a[i+7:i])
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPABSB'. Intrinsic: '_mm512_mask_abs_epi8'.
// Requires AVX512BW.
func MaskAbsEpi8(src M512i, k Mmask64, a M512i) M512i {
	return M512i(maskAbsEpi8([64]byte(src), uint64(k), [64]byte(a)))
}

func maskAbsEpi8(src [64]byte, k uint64, a [64]byte) [64]byte


// MaskzAbsEpi8: Compute the absolute value of packed 8-bit integers in 'a',
// and store the unsigned results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := ABS(a[i+7:i])
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPABSB'. Intrinsic: '_mm512_maskz_abs_epi8'.
// Requires AVX512BW.
func MaskzAbsEpi8(k Mmask64, a M512i) M512i {
	return M512i(maskzAbsEpi8(uint64(k), [64]byte(a)))
}

func maskzAbsEpi8(k uint64, a [64]byte) [64]byte


// AbsPd: Finds the absolute value of each packed double-precision (64-bit)
// floating-point element in 'v2', storing the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ABS(v2[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPANDQ'. Intrinsic: '_mm512_abs_pd'.
// Requires KNCNI.
func AbsPd(v2 M512d) M512d {
	return M512d(absPd([8]float64(v2)))
}

func absPd(v2 [8]float64) [8]float64


// MaskAbsPd: Finds the absolute value of each packed double-precision (64-bit)
// floating-point element in 'v2', storing the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ABS(v2[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPANDQ'. Intrinsic: '_mm512_mask_abs_pd'.
// Requires KNCNI.
func MaskAbsPd(src M512d, k Mmask8, v2 M512d) M512d {
	return M512d(maskAbsPd([8]float64(src), uint8(k), [8]float64(v2)))
}

func maskAbsPd(src [8]float64, k uint8, v2 [8]float64) [8]float64


// AbsPs: Finds the absolute value of each packed single-precision (32-bit)
// floating-point element in 'v2', storing the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ABS(v2[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPANDD'. Intrinsic: '_mm512_abs_ps'.
// Requires KNCNI.
func AbsPs(v2 M512) M512 {
	return M512(absPs([16]float32(v2)))
}

func absPs(v2 [16]float32) [16]float32


// MaskAbsPs: Finds the absolute value of each packed single-precision (32-bit)
// floating-point element in 'v2', storing the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ABS(v2[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPANDD'. Intrinsic: '_mm512_mask_abs_ps'.
// Requires KNCNI.
func MaskAbsPs(src M512, k Mmask16, v2 M512) M512 {
	return M512(maskAbsPs([16]float32(src), uint16(k), [16]float32(v2)))
}

func maskAbsPs(src [16]float32, k uint16, v2 [16]float32) [16]float32


// AcosPd: Compute the inverse cosine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ACOS(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_acos_pd'.
// Requires AVX512F.
func AcosPd(a M512d) M512d {
	return M512d(acosPd([8]float64(a)))
}

func acosPd(a [8]float64) [8]float64


// MaskAcosPd: Compute the inverse cosine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ACOS(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_acos_pd'.
// Requires AVX512F.
func MaskAcosPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskAcosPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskAcosPd(src [8]float64, k uint8, a [8]float64) [8]float64


// AcosPs: Compute the inverse cosine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ACOS(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_acos_ps'.
// Requires AVX512F.
func AcosPs(a M512) M512 {
	return M512(acosPs([16]float32(a)))
}

func acosPs(a [16]float32) [16]float32


// MaskAcosPs: Compute the inverse cosine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ACOS(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_acos_ps'.
// Requires AVX512F.
func MaskAcosPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskAcosPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskAcosPs(src [16]float32, k uint16, a [16]float32) [16]float32


// AcoshPd: Compute the inverse hyperbolic cosine of packed double-precision
// (64-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ACOSH(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_acosh_pd'.
// Requires AVX512F.
func AcoshPd(a M512d) M512d {
	return M512d(acoshPd([8]float64(a)))
}

func acoshPd(a [8]float64) [8]float64


// MaskAcoshPd: Compute the inverse hyperbolic cosine of packed
// double-precision (64-bit) floating-point elements in 'a' expressed in
// radians, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ACOSH(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_acosh_pd'.
// Requires AVX512F.
func MaskAcoshPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskAcoshPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskAcoshPd(src [8]float64, k uint8, a [8]float64) [8]float64


// AcoshPs: Compute the inverse hyperbolic cosine of packed single-precision
// (32-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ACOSH(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_acosh_ps'.
// Requires AVX512F.
func AcoshPs(a M512) M512 {
	return M512(acoshPs([16]float32(a)))
}

func acoshPs(a [16]float32) [16]float32


// MaskAcoshPs: Compute the inverse hyperbolic cosine of packed
// single-precision (32-bit) floating-point elements in 'a' expressed in
// radians, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ACOSH(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_acosh_ps'.
// Requires AVX512F.
func MaskAcoshPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskAcoshPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskAcoshPs(src [16]float32, k uint16, a [16]float32) [16]float32


// AdcEpi32: Performs element-by-element addition of packed 32-bit integers in
// 'v2' and 'v3' and the corresponding bit in 'k2', storing the result of the
// addition in 'dst' and the result of the carry in 'k2_res'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			k2_res[j]   := Carry(v2[i+31:i] + v3[i+31:i] + k2[j])
//			dst[i+31:i] := v2[i+31:i] + v3[i+31:i] + k2[j]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPADCD'. Intrinsic: '_mm512_adc_epi32'.
// Requires KNCNI.
func AdcEpi32(v2 M512i, k2 Mmask16, v3 M512i, k2_res Mmask16) M512i {
	return M512i(adcEpi32([64]byte(v2), uint16(k2), [64]byte(v3), uint16(k2_res)))
}

func adcEpi32(v2 [64]byte, k2 uint16, v3 [64]byte, k2_res uint16) [64]byte


// MaskAdcEpi32: Performs element-by-element addition of packed 32-bit integers
// in 'v2' and 'v3' and the corresponding bit in 'k2', storing the result of
// the addition in 'dst' and the result of the carry in 'k2_res' using
// writemask 'k1' (elements are copied from 'v2' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k1[j]
//				k2_res[j]   := Carry(v2[i+31:i] + v3[i+31:i] + k2[j])
//				dst[i+31:i] := v2[i+31:i] + v3[i+31:i] + k2[j]
//			ELSE
//				dst[i+31:i] := v2[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPADCD'. Intrinsic: '_mm512_mask_adc_epi32'.
// Requires KNCNI.
func MaskAdcEpi32(v2 M512i, k1 Mmask16, k2 Mmask16, v3 M512i, k2_res Mmask16) M512i {
	return M512i(maskAdcEpi32([64]byte(v2), uint16(k1), uint16(k2), [64]byte(v3), uint16(k2_res)))
}

func maskAdcEpi32(v2 [64]byte, k1 uint16, k2 uint16, v3 [64]byte, k2_res uint16) [64]byte


// AddEpi16: Add packed 16-bit integers in 'a' and 'b', and store the results
// in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			dst[i+15:i] := a[i+15:i] + b[i+15:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPADDW'. Intrinsic: '_mm512_add_epi16'.
// Requires AVX512BW.
func AddEpi16(a M512i, b M512i) M512i {
	return M512i(addEpi16([64]byte(a), [64]byte(b)))
}

func addEpi16(a [64]byte, b [64]byte) [64]byte


// MaskAddEpi16: Add packed 16-bit integers in 'a' and 'b', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := a[i+15:i] + b[i+15:i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPADDW'. Intrinsic: '_mm512_mask_add_epi16'.
// Requires AVX512BW.
func MaskAddEpi16(src M512i, k Mmask32, a M512i, b M512i) M512i {
	return M512i(maskAddEpi16([64]byte(src), uint32(k), [64]byte(a), [64]byte(b)))
}

func maskAddEpi16(src [64]byte, k uint32, a [64]byte, b [64]byte) [64]byte


// MaskzAddEpi16: Add packed 16-bit integers in 'a' and 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := a[i+15:i] + b[i+15:i]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPADDW'. Intrinsic: '_mm512_maskz_add_epi16'.
// Requires AVX512BW.
func MaskzAddEpi16(k Mmask32, a M512i, b M512i) M512i {
	return M512i(maskzAddEpi16(uint32(k), [64]byte(a), [64]byte(b)))
}

func maskzAddEpi16(k uint32, a [64]byte, b [64]byte) [64]byte


// AddEpi32: Add packed 32-bit integers in 'a' and 'b', and store the results
// in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := a[i+31:i] + b[i+31:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPADDD'. Intrinsic: '_mm512_add_epi32'.
// Requires KNCNI.
func AddEpi32(a M512i, b M512i) M512i {
	return M512i(addEpi32([64]byte(a), [64]byte(b)))
}

func addEpi32(a [64]byte, b [64]byte) [64]byte


// MaskAddEpi32: Add packed 32-bit integers in 'a' and 'b', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] + b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPADDD'. Intrinsic: '_mm512_mask_add_epi32'.
// Requires KNCNI.
func MaskAddEpi32(src M512i, k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskAddEpi32([64]byte(src), uint16(k), [64]byte(a), [64]byte(b)))
}

func maskAddEpi32(src [64]byte, k uint16, a [64]byte, b [64]byte) [64]byte


// MaskzAddEpi32: Add packed 32-bit integers in 'a' and 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] + b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPADDD'. Intrinsic: '_mm512_maskz_add_epi32'.
// Requires AVX512F.
func MaskzAddEpi32(k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskzAddEpi32(uint16(k), [64]byte(a), [64]byte(b)))
}

func maskzAddEpi32(k uint16, a [64]byte, b [64]byte) [64]byte


// AddEpi64: Add packed 64-bit integers in 'a' and 'b', and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := a[i+63:i] + b[i+63:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPADDQ'. Intrinsic: '_mm512_add_epi64'.
// Requires AVX512F.
func AddEpi64(a M512i, b M512i) M512i {
	return M512i(addEpi64([64]byte(a), [64]byte(b)))
}

func addEpi64(a [64]byte, b [64]byte) [64]byte


// MaskAddEpi64: Add packed 64-bit integers in 'a' and 'b', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] + b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPADDQ'. Intrinsic: '_mm512_mask_add_epi64'.
// Requires AVX512F.
func MaskAddEpi64(src M512i, k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskAddEpi64([64]byte(src), uint8(k), [64]byte(a), [64]byte(b)))
}

func maskAddEpi64(src [64]byte, k uint8, a [64]byte, b [64]byte) [64]byte


// MaskzAddEpi64: Add packed 64-bit integers in 'a' and 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] + b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPADDQ'. Intrinsic: '_mm512_maskz_add_epi64'.
// Requires AVX512F.
func MaskzAddEpi64(k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskzAddEpi64(uint8(k), [64]byte(a), [64]byte(b)))
}

func maskzAddEpi64(k uint8, a [64]byte, b [64]byte) [64]byte


// AddEpi8: Add packed 8-bit integers in 'a' and 'b', and store the results in
// 'dst'. 
//
//		FOR j := 0 to 63
//			i := j*8
//			dst[i+7:i] := a[i+7:i] + b[i+7:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPADDB'. Intrinsic: '_mm512_add_epi8'.
// Requires AVX512BW.
func AddEpi8(a M512i, b M512i) M512i {
	return M512i(addEpi8([64]byte(a), [64]byte(b)))
}

func addEpi8(a [64]byte, b [64]byte) [64]byte


// MaskAddEpi8: Add packed 8-bit integers in 'a' and 'b', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := a[i+7:i] + b[i+7:i]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPADDB'. Intrinsic: '_mm512_mask_add_epi8'.
// Requires AVX512BW.
func MaskAddEpi8(src M512i, k Mmask64, a M512i, b M512i) M512i {
	return M512i(maskAddEpi8([64]byte(src), uint64(k), [64]byte(a), [64]byte(b)))
}

func maskAddEpi8(src [64]byte, k uint64, a [64]byte, b [64]byte) [64]byte


// MaskzAddEpi8: Add packed 8-bit integers in 'a' and 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := a[i+7:i] + b[i+7:i]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPADDB'. Intrinsic: '_mm512_maskz_add_epi8'.
// Requires AVX512BW.
func MaskzAddEpi8(k Mmask64, a M512i, b M512i) M512i {
	return M512i(maskzAddEpi8(uint64(k), [64]byte(a), [64]byte(b)))
}

func maskzAddEpi8(k uint64, a [64]byte, b [64]byte) [64]byte


// AddPd: Add packed double-precision (64-bit) floating-point elements in 'a'
// and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := a[i+63:i] + b[i+63:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VADDPD'. Intrinsic: '_mm512_add_pd'.
// Requires KNCNI.
func AddPd(a M512d, b M512d) M512d {
	return M512d(addPd([8]float64(a), [8]float64(b)))
}

func addPd(a [8]float64, b [8]float64) [8]float64


// MaskAddPd: Add packed double-precision (64-bit) floating-point elements in
// 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] + b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VADDPD'. Intrinsic: '_mm512_mask_add_pd'.
// Requires KNCNI.
func MaskAddPd(src M512d, k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskAddPd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b)))
}

func maskAddPd(src [8]float64, k uint8, a [8]float64, b [8]float64) [8]float64


// MaskzAddPd: Add packed double-precision (64-bit) floating-point elements in
// 'a' and 'b', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] + b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VADDPD'. Intrinsic: '_mm512_maskz_add_pd'.
// Requires AVX512F.
func MaskzAddPd(k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskzAddPd(uint8(k), [8]float64(a), [8]float64(b)))
}

func maskzAddPd(k uint8, a [8]float64, b [8]float64) [8]float64


// AddPs: Add packed single-precision (32-bit) floating-point elements in 'a'
// and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := a[i+31:i] + b[i+31:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VADDPS'. Intrinsic: '_mm512_add_ps'.
// Requires KNCNI.
func AddPs(a M512, b M512) M512 {
	return M512(addPs([16]float32(a), [16]float32(b)))
}

func addPs(a [16]float32, b [16]float32) [16]float32


// MaskAddPs: Add packed single-precision (32-bit) floating-point elements in
// 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] + b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VADDPS'. Intrinsic: '_mm512_mask_add_ps'.
// Requires KNCNI.
func MaskAddPs(src M512, k Mmask16, a M512, b M512) M512 {
	return M512(maskAddPs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b)))
}

func maskAddPs(src [16]float32, k uint16, a [16]float32, b [16]float32) [16]float32


// MaskzAddPs: Add packed single-precision (32-bit) floating-point elements in
// 'a' and 'b', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] + b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VADDPS'. Intrinsic: '_mm512_maskz_add_ps'.
// Requires AVX512F.
func MaskzAddPs(k Mmask16, a M512, b M512) M512 {
	return M512(maskzAddPs(uint16(k), [16]float32(a), [16]float32(b)))
}

func maskzAddPs(k uint16, a [16]float32, b [16]float32) [16]float32


// AddRoundPd: Add packed double-precision (64-bit) floating-point elements in
// 'a' and 'b', and store the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := a[i+63:i] + b[i+63:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VADDPD'. Intrinsic: '_mm512_add_round_pd'.
// Requires KNCNI.
func AddRoundPd(a M512d, b M512d, rounding int) M512d {
	return M512d(addRoundPd([8]float64(a), [8]float64(b), rounding))
}

func addRoundPd(a [8]float64, b [8]float64, rounding int) [8]float64


// MaskAddRoundPd: Add packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] + b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VADDPD'. Intrinsic: '_mm512_mask_add_round_pd'.
// Requires KNCNI.
func MaskAddRoundPd(src M512d, k Mmask8, a M512d, b M512d, rounding int) M512d {
	return M512d(maskAddRoundPd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b), rounding))
}

func maskAddRoundPd(src [8]float64, k uint8, a [8]float64, b [8]float64, rounding int) [8]float64


// MaskzAddRoundPd: Add packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] + b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VADDPD'. Intrinsic: '_mm512_maskz_add_round_pd'.
// Requires AVX512F.
func MaskzAddRoundPd(k Mmask8, a M512d, b M512d, rounding int) M512d {
	return M512d(maskzAddRoundPd(uint8(k), [8]float64(a), [8]float64(b), rounding))
}

func maskzAddRoundPd(k uint8, a [8]float64, b [8]float64, rounding int) [8]float64


// AddRoundPs: Add packed single-precision (32-bit) floating-point elements in
// 'a' and 'b', and store the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := a[i+31:i] + b[i+31:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VADDPS'. Intrinsic: '_mm512_add_round_ps'.
// Requires KNCNI.
func AddRoundPs(a M512, b M512, rounding int) M512 {
	return M512(addRoundPs([16]float32(a), [16]float32(b), rounding))
}

func addRoundPs(a [16]float32, b [16]float32, rounding int) [16]float32


// MaskAddRoundPs: Add packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] + b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VADDPS'. Intrinsic: '_mm512_mask_add_round_ps'.
// Requires KNCNI.
func MaskAddRoundPs(src M512, k Mmask16, a M512, b M512, rounding int) M512 {
	return M512(maskAddRoundPs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b), rounding))
}

func maskAddRoundPs(src [16]float32, k uint16, a [16]float32, b [16]float32, rounding int) [16]float32


// MaskzAddRoundPs: Add packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] + b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VADDPS'. Intrinsic: '_mm512_maskz_add_round_ps'.
// Requires AVX512F.
func MaskzAddRoundPs(k Mmask16, a M512, b M512, rounding int) M512 {
	return M512(maskzAddRoundPs(uint16(k), [16]float32(a), [16]float32(b), rounding))
}

func maskzAddRoundPs(k uint16, a [16]float32, b [16]float32, rounding int) [16]float32


// AddnPd: Performs element-by-element addition between packed double-precision
// (64-bit) floating-point elements in 'v2' and 'v3' and negates their sum,
// storing the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := -(v2[i+63:i] + v3[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VADDNPD'. Intrinsic: '_mm512_addn_pd'.
// Requires KNCNI.
func AddnPd(v2 M512d, v3 M512d) M512d {
	return M512d(addnPd([8]float64(v2), [8]float64(v3)))
}

func addnPd(v2 [8]float64, v3 [8]float64) [8]float64


// MaskAddnPd: Performs element-by-element addition between packed
// double-precision (64-bit) floating-point elements in 'v2' and 'v3' and
// negates their sum, storing the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(v2[i+63:i] + v3[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VADDNPD'. Intrinsic: '_mm512_mask_addn_pd'.
// Requires KNCNI.
func MaskAddnPd(src M512d, k Mmask8, v2 M512d, v3 M512d) M512d {
	return M512d(maskAddnPd([8]float64(src), uint8(k), [8]float64(v2), [8]float64(v3)))
}

func maskAddnPd(src [8]float64, k uint8, v2 [8]float64, v3 [8]float64) [8]float64


// AddnPs: Performs element-by-element addition between packed single-precision
// (32-bit) floating-point elements in 'v2' and 'v3' and negates their sum,
// storing the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := -(v2[i+31:i] + v3[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VADDNPS'. Intrinsic: '_mm512_addn_ps'.
// Requires KNCNI.
func AddnPs(v2 M512, v3 M512) M512 {
	return M512(addnPs([16]float32(v2), [16]float32(v3)))
}

func addnPs(v2 [16]float32, v3 [16]float32) [16]float32


// MaskAddnPs: Performs element-by-element addition between packed
// single-precision (32-bit) floating-point elements in 'v2' and 'v3' and
// negates their sum, storing the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(v2[i+31:i] + v3[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VADDNPS'. Intrinsic: '_mm512_mask_addn_ps'.
// Requires KNCNI.
func MaskAddnPs(src M512, k Mmask16, v2 M512, v3 M512) M512 {
	return M512(maskAddnPs([16]float32(src), uint16(k), [16]float32(v2), [16]float32(v3)))
}

func maskAddnPs(src [16]float32, k uint16, v2 [16]float32, v3 [16]float32) [16]float32


// AddnRoundPd: Performs element by element addition between packed
// double-precision (64-bit) floating-point elements in 'v2' and 'v3' and
// negates the sum, storing the result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := -(v2[i+63:i] + v3[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VADDNPD'. Intrinsic: '_mm512_addn_round_pd'.
// Requires KNCNI.
func AddnRoundPd(v2 M512d, v3 M512d, rounding int) M512d {
	return M512d(addnRoundPd([8]float64(v2), [8]float64(v3), rounding))
}

func addnRoundPd(v2 [8]float64, v3 [8]float64, rounding int) [8]float64


// MaskAddnRoundPd: Performs element by element addition between packed
// double-precision (64-bit) floating-point elements in 'v2' and 'v3' and
// negates the sum, storing the result in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(v2[i+63:i] + v3[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VADDNPD'. Intrinsic: '_mm512_mask_addn_round_pd'.
// Requires KNCNI.
func MaskAddnRoundPd(src M512d, k Mmask8, v2 M512d, v3 M512d, rounding int) M512d {
	return M512d(maskAddnRoundPd([8]float64(src), uint8(k), [8]float64(v2), [8]float64(v3), rounding))
}

func maskAddnRoundPd(src [8]float64, k uint8, v2 [8]float64, v3 [8]float64, rounding int) [8]float64


// AddnRoundPs: Performs element by element addition between packed
// single-precision (32-bit) floating-point elements in 'v2' and 'v3' and
// negates the sum, storing the result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := -(v2[i+31:i] + v3[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VADDNPS'. Intrinsic: '_mm512_addn_round_ps'.
// Requires KNCNI.
func AddnRoundPs(v2 M512, v3 M512, rounding int) M512 {
	return M512(addnRoundPs([16]float32(v2), [16]float32(v3), rounding))
}

func addnRoundPs(v2 [16]float32, v3 [16]float32, rounding int) [16]float32


// MaskAddnRoundPs: Performs element by element addition between packed
// single-precision (32-bit) floating-point elements in 'v2' and 'v3' and
// negates the sum, storing the result in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(v2[i+31:i] + v3[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VADDNPS'. Intrinsic: '_mm512_mask_addn_round_ps'.
// Requires KNCNI.
func MaskAddnRoundPs(src M512, k Mmask16, v2 M512, v3 M512, rounding int) M512 {
	return M512(maskAddnRoundPs([16]float32(src), uint16(k), [16]float32(v2), [16]float32(v3), rounding))
}

func maskAddnRoundPs(src [16]float32, k uint16, v2 [16]float32, v3 [16]float32, rounding int) [16]float32


// AddsEpi16: Add packed 16-bit integers in 'a' and 'b' using saturation, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			dst[i+15:i] := Saturate_To_Int16( a[i+15:i] + b[i+15:i] )
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPADDSW'. Intrinsic: '_mm512_adds_epi16'.
// Requires AVX512BW.
func AddsEpi16(a M512i, b M512i) M512i {
	return M512i(addsEpi16([64]byte(a), [64]byte(b)))
}

func addsEpi16(a [64]byte, b [64]byte) [64]byte


// MaskAddsEpi16: Add packed 16-bit integers in 'a' and 'b' using saturation,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := Saturate_To_Int16( a[i+15:i] + b[i+15:i] )
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPADDSW'. Intrinsic: '_mm512_mask_adds_epi16'.
// Requires AVX512BW.
func MaskAddsEpi16(src M512i, k Mmask32, a M512i, b M512i) M512i {
	return M512i(maskAddsEpi16([64]byte(src), uint32(k), [64]byte(a), [64]byte(b)))
}

func maskAddsEpi16(src [64]byte, k uint32, a [64]byte, b [64]byte) [64]byte


// MaskzAddsEpi16: Add packed 16-bit integers in 'a' and 'b' using saturation,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := Saturate_To_Int16( a[i+15:i] + b[i+15:i] )
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPADDSW'. Intrinsic: '_mm512_maskz_adds_epi16'.
// Requires AVX512BW.
func MaskzAddsEpi16(k Mmask32, a M512i, b M512i) M512i {
	return M512i(maskzAddsEpi16(uint32(k), [64]byte(a), [64]byte(b)))
}

func maskzAddsEpi16(k uint32, a [64]byte, b [64]byte) [64]byte


// AddsEpi8: Add packed 8-bit integers in 'a' and 'b' using saturation, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 63
//			i := j*8
//			dst[i+7:i] := Saturate_To_Int8( a[i+7:i] + b[i+7:i] )
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPADDSB'. Intrinsic: '_mm512_adds_epi8'.
// Requires AVX512BW.
func AddsEpi8(a M512i, b M512i) M512i {
	return M512i(addsEpi8([64]byte(a), [64]byte(b)))
}

func addsEpi8(a [64]byte, b [64]byte) [64]byte


// MaskAddsEpi8: Add packed 8-bit integers in 'a' and 'b' using saturation, and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := Saturate_To_Int8( a[i+7:i] + b[i+7:i] )
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPADDSB'. Intrinsic: '_mm512_mask_adds_epi8'.
// Requires AVX512BW.
func MaskAddsEpi8(src M512i, k Mmask64, a M512i, b M512i) M512i {
	return M512i(maskAddsEpi8([64]byte(src), uint64(k), [64]byte(a), [64]byte(b)))
}

func maskAddsEpi8(src [64]byte, k uint64, a [64]byte, b [64]byte) [64]byte


// MaskzAddsEpi8: Add packed 8-bit integers in 'a' and 'b' using saturation,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := Saturate_To_Int8( a[i+7:i] + b[i+7:i] )
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPADDSB'. Intrinsic: '_mm512_maskz_adds_epi8'.
// Requires AVX512BW.
func MaskzAddsEpi8(k Mmask64, a M512i, b M512i) M512i {
	return M512i(maskzAddsEpi8(uint64(k), [64]byte(a), [64]byte(b)))
}

func maskzAddsEpi8(k uint64, a [64]byte, b [64]byte) [64]byte


// AddsEpu16: Add packed unsigned 16-bit integers in 'a' and 'b' using
// saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			dst[i+15:i] := Saturate_To_UnsignedInt16( a[i+15:i] + b[i+15:i] )
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPADDUSW'. Intrinsic: '_mm512_adds_epu16'.
// Requires AVX512BW.
func AddsEpu16(a M512i, b M512i) M512i {
	return M512i(addsEpu16([64]byte(a), [64]byte(b)))
}

func addsEpu16(a [64]byte, b [64]byte) [64]byte


// MaskAddsEpu16: Add packed unsigned 16-bit integers in 'a' and 'b' using
// saturation, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := Saturate_To_UnsignedInt16( a[i+15:i] + b[i+15:i] )
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPADDUSW'. Intrinsic: '_mm512_mask_adds_epu16'.
// Requires AVX512BW.
func MaskAddsEpu16(src M512i, k Mmask32, a M512i, b M512i) M512i {
	return M512i(maskAddsEpu16([64]byte(src), uint32(k), [64]byte(a), [64]byte(b)))
}

func maskAddsEpu16(src [64]byte, k uint32, a [64]byte, b [64]byte) [64]byte


// MaskzAddsEpu16: Add packed unsigned 16-bit integers in 'a' and 'b' using
// saturation, and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := Saturate_To_UnsignedInt16( a[i+15:i] + b[i+15:i] )
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPADDUSW'. Intrinsic: '_mm512_maskz_adds_epu16'.
// Requires AVX512BW.
func MaskzAddsEpu16(k Mmask32, a M512i, b M512i) M512i {
	return M512i(maskzAddsEpu16(uint32(k), [64]byte(a), [64]byte(b)))
}

func maskzAddsEpu16(k uint32, a [64]byte, b [64]byte) [64]byte


// AddsEpu8: Add packed unsigned 8-bit integers in 'a' and 'b' using
// saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 63
//			i := j*8
//			dst[i+7:i] := Saturate_To_UnsignedInt8( a[i+7:i] + b[i+7:i] )
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPADDUSB'. Intrinsic: '_mm512_adds_epu8'.
// Requires AVX512BW.
func AddsEpu8(a M512i, b M512i) M512i {
	return M512i(addsEpu8([64]byte(a), [64]byte(b)))
}

func addsEpu8(a [64]byte, b [64]byte) [64]byte


// MaskAddsEpu8: Add packed unsigned 8-bit integers in 'a' and 'b' using
// saturation, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := Saturate_To_UnsignedInt8( a[i+7:i] + b[i+7:i] )
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPADDUSB'. Intrinsic: '_mm512_mask_adds_epu8'.
// Requires AVX512BW.
func MaskAddsEpu8(src M512i, k Mmask64, a M512i, b M512i) M512i {
	return M512i(maskAddsEpu8([64]byte(src), uint64(k), [64]byte(a), [64]byte(b)))
}

func maskAddsEpu8(src [64]byte, k uint64, a [64]byte, b [64]byte) [64]byte


// MaskzAddsEpu8: Add packed unsigned 8-bit integers in 'a' and 'b' using
// saturation, and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := Saturate_To_UnsignedInt8( a[i+7:i] + b[i+7:i] )
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPADDUSB'. Intrinsic: '_mm512_maskz_adds_epu8'.
// Requires AVX512BW.
func MaskzAddsEpu8(k Mmask64, a M512i, b M512i) M512i {
	return M512i(maskzAddsEpu8(uint64(k), [64]byte(a), [64]byte(b)))
}

func maskzAddsEpu8(k uint64, a [64]byte, b [64]byte) [64]byte


// AddsetcEpi32: Performs element-by-element addition of packed 32-bit integer
// elements in 'v2' and 'v3', storing the resultant carry in 'k2_res' (carry
// flag) and the addition results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := v2[i+31:i] + v3[i+31:i]
//			k2_res[j] := Carry(v2[i+31:i] + v3[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPADDSETCD'. Intrinsic: '_mm512_addsetc_epi32'.
// Requires KNCNI.
func AddsetcEpi32(v2 M512i, v3 M512i, k2_res Mmask16) M512i {
	return M512i(addsetcEpi32([64]byte(v2), [64]byte(v3), uint16(k2_res)))
}

func addsetcEpi32(v2 [64]byte, v3 [64]byte, k2_res uint16) [64]byte


// MaskAddsetcEpi32: Performs element-by-element addition of packed 32-bit
// integer elements in 'v2' and 'v3', storing the resultant carry in 'k2_res'
// (carry flag) and the addition results in 'dst' using writemask 'k' (elements
// are copied from 'v2' and 'k_old' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := v2[i+31:i] + v3[i+31:i]
//			ELSE
//				dst[i+31:i] := v2[i+31:i]
//				k2_res[j] := k_old[j]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPADDSETCD'. Intrinsic: '_mm512_mask_addsetc_epi32'.
// Requires KNCNI.
func MaskAddsetcEpi32(v2 M512i, k Mmask16, k_old Mmask16, v3 M512i, k2_res Mmask16) M512i {
	return M512i(maskAddsetcEpi32([64]byte(v2), uint16(k), uint16(k_old), [64]byte(v3), uint16(k2_res)))
}

func maskAddsetcEpi32(v2 [64]byte, k uint16, k_old uint16, v3 [64]byte, k2_res uint16) [64]byte


// AddsetsEpi32: Performs an element-by-element addition of packed 32-bit
// integer elements in 'v2' and 'v3', storing the results in 'dst' and the sign
// of the sum in 'sign' (sign flag). 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := v2[i+31:i] + v3[i+31:i]
//			sign[j] := v2[i+31:i] & v3[i+31:i] & 0x80000000
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPADDSETSD'. Intrinsic: '_mm512_addsets_epi32'.
// Requires KNCNI.
func AddsetsEpi32(v2 M512i, v3 M512i, sign Mmask16) M512i {
	return M512i(addsetsEpi32([64]byte(v2), [64]byte(v3), uint16(sign)))
}

func addsetsEpi32(v2 [64]byte, v3 [64]byte, sign uint16) [64]byte


// MaskAddsetsEpi32: Performs an element-by-element addition of packed 32-bit
// integer elements in 'v2' and 'v3', storing the results in 'dst' and the sign
// of the sum in 'sign' (sign flag). Results are stored using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := v2[i+31:i] + v3[i+31:i]
//				sign[j] := v2[i+31:i] & v3[i+31:i] & 0x80000000
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPADDSETSD'. Intrinsic: '_mm512_mask_addsets_epi32'.
// Requires KNCNI.
func MaskAddsetsEpi32(src M512i, k Mmask16, v2 M512i, v3 M512i, sign Mmask16) M512i {
	return M512i(maskAddsetsEpi32([64]byte(src), uint16(k), [64]byte(v2), [64]byte(v3), uint16(sign)))
}

func maskAddsetsEpi32(src [64]byte, k uint16, v2 [64]byte, v3 [64]byte, sign uint16) [64]byte


// AddsetsPs: Performs an element-by-element addition of packed
// single-precision (32-bit) floating-point elements in 'v2' and 'v3', storing
// the results in 'dst' and the sign of the sum in 'sign' (sign flag). 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := v2[i+31:i] + v3[i+31:i]
//			sign[j] := v2[i+31:i] & v3[i+31:i] & 0x80000000
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VADDSETSPS'. Intrinsic: '_mm512_addsets_ps'.
// Requires KNCNI.
func AddsetsPs(v2 M512, v3 M512, sign Mmask16) M512 {
	return M512(addsetsPs([16]float32(v2), [16]float32(v3), uint16(sign)))
}

func addsetsPs(v2 [16]float32, v3 [16]float32, sign uint16) [16]float32


// MaskAddsetsPs: Performs an element-by-element addition of packed
// single-precision (32-bit) floating-point elements in 'v2' and 'v3', storing
// the results in 'dst' and the sign of the sum in 'sign' (sign flag). Results
// are stored using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := v2[i+31:i] + v3[i+31:i]
//				sign[j] := v2[i+31:i] & v3[i+31:i] & 0x80000000
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VADDSETSPS'. Intrinsic: '_mm512_mask_addsets_ps'.
// Requires KNCNI.
func MaskAddsetsPs(src M512, k Mmask16, v2 M512, v3 M512, sign Mmask16) M512 {
	return M512(maskAddsetsPs([16]float32(src), uint16(k), [16]float32(v2), [16]float32(v3), uint16(sign)))
}

func maskAddsetsPs(src [16]float32, k uint16, v2 [16]float32, v3 [16]float32, sign uint16) [16]float32


// AddsetsRoundPs: Performs an element-by-element addition of packed
// single-precision (32-bit) floating-point elements in 'v2' and 'v3', storing
// the results in 'dst' and the sign of the sum in 'sign' (sign flag).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := v2[i+31:i] + v3[i+31:i]
//			sign[j] := v2[i+31:i] & v3[i+31:i] & 0x80000000
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VADDSETSPS'. Intrinsic: '_mm512_addsets_round_ps'.
// Requires KNCNI.
func AddsetsRoundPs(v2 M512, v3 M512, sign Mmask16, rounding int) M512 {
	return M512(addsetsRoundPs([16]float32(v2), [16]float32(v3), uint16(sign), rounding))
}

func addsetsRoundPs(v2 [16]float32, v3 [16]float32, sign uint16, rounding int) [16]float32


// MaskAddsetsRoundPs: Performs an element-by-element addition of packed
// single-precision (32-bit) floating-point elements in 'v2' and 'v3', storing
// the results in 'dst' and the sign of the sum in 'sign' (sign flag). Results
// are stored using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := v2[i+31:i] + v3[i+31:i]
//				sign[j] := v2[i+31:i] & v3[i+31:i] & 0x80000000
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VADDSETSPS'. Intrinsic: '_mm512_mask_addsets_round_ps'.
// Requires KNCNI.
func MaskAddsetsRoundPs(src M512, k Mmask16, v2 M512, v3 M512, sign Mmask16, rounding int) M512 {
	return M512(maskAddsetsRoundPs([16]float32(src), uint16(k), [16]float32(v2), [16]float32(v3), uint16(sign), rounding))
}

func maskAddsetsRoundPs(src [16]float32, k uint16, v2 [16]float32, v3 [16]float32, sign uint16, rounding int) [16]float32


// AlignrEpi32: Concatenate 'a' and 'b' into a 128-byte immediate result, shift
// the result right by 'count' 32-bit elements, and store the low 64 bytes (16
// elements) in 'dst'. 
//
//		temp[1023:512] := a[511:0]
//		temp[511:0] := b[511:0]
//		temp[1023:0] := temp[1023:0] >> (32*count)
//		dst[511:0] := temp[511:0]
//		dst[MAX:512] := 0
//
// Instruction: 'VALIGND'. Intrinsic: '_mm512_alignr_epi32'.
// Requires KNCNI.
func AlignrEpi32(a M512i, b M512i, count int) M512i {
	return M512i(alignrEpi32([64]byte(a), [64]byte(b), count))
}

func alignrEpi32(a [64]byte, b [64]byte, count int) [64]byte


// MaskAlignrEpi32: Concatenate 'a' and 'b' into a 128-byte immediate result,
// shift the result right by 'count' 32-bit elements, and store the low 64
// bytes (16 elements) in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		temp[1023:512] := a[511:0]
//		temp[511:0] := b[511:0]
//		temp[1023:0] := temp[1023:0] >> (32*count)
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := temp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VALIGND'. Intrinsic: '_mm512_mask_alignr_epi32'.
// Requires KNCNI.
func MaskAlignrEpi32(src M512i, k Mmask16, a M512i, b M512i, count int) M512i {
	return M512i(maskAlignrEpi32([64]byte(src), uint16(k), [64]byte(a), [64]byte(b), count))
}

func maskAlignrEpi32(src [64]byte, k uint16, a [64]byte, b [64]byte, count int) [64]byte


// MaskzAlignrEpi32: Concatenate 'a' and 'b' into a 128-byte immediate result,
// shift the result right by 'count' 32-bit elements, and stores the low 64
// bytes (16 elements) in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		temp[1023:512] := a[511:0]
//		temp[511:0] := b[511:0]
//		temp[1023:0] := temp[1023:0] >> (32*count)
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := temp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VALIGND'. Intrinsic: '_mm512_maskz_alignr_epi32'.
// Requires AVX512F.
func MaskzAlignrEpi32(k Mmask16, a M512i, b M512i, count int) M512i {
	return M512i(maskzAlignrEpi32(uint16(k), [64]byte(a), [64]byte(b), count))
}

func maskzAlignrEpi32(k uint16, a [64]byte, b [64]byte, count int) [64]byte


// AlignrEpi64: Concatenate 'a' and 'b' into a 128-byte immediate result, shift
// the result right by 'count' 64-bit elements, and store the low 64 bytes (8
// elements) in 'dst'. 
//
//		temp[1023:512] := a[511:0]
//		temp[511:0] := b[511:0]
//		temp[1023:0] := temp[1023:0] >> (64*count)
//		dst[511:0] := temp[511:0]
//		dst[MAX:512] := 0
//
// Instruction: 'VALIGNQ'. Intrinsic: '_mm512_alignr_epi64'.
// Requires AVX512F.
func AlignrEpi64(a M512i, b M512i, count int) M512i {
	return M512i(alignrEpi64([64]byte(a), [64]byte(b), count))
}

func alignrEpi64(a [64]byte, b [64]byte, count int) [64]byte


// MaskAlignrEpi64: Concatenate 'a' and 'b' into a 128-byte immediate result,
// shift the result right by 'count' 64-bit elements, and store the low 64
// bytes (8 elements) in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		temp[1023:512] := a[511:0]
//		temp[511:0] := b[511:0]
//		temp[1023:0] := temp[1023:0] >> (64*count)
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := temp[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VALIGNQ'. Intrinsic: '_mm512_mask_alignr_epi64'.
// Requires AVX512F.
func MaskAlignrEpi64(src M512i, k Mmask8, a M512i, b M512i, count int) M512i {
	return M512i(maskAlignrEpi64([64]byte(src), uint8(k), [64]byte(a), [64]byte(b), count))
}

func maskAlignrEpi64(src [64]byte, k uint8, a [64]byte, b [64]byte, count int) [64]byte


// MaskzAlignrEpi64: Concatenate 'a' and 'b' into a 128-byte immediate result,
// shift the result right by 'count' 64-bit elements, and stores the low 64
// bytes (8 elements) in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		temp[1023:512] := a[511:0]
//		temp[511:0] := b[511:0]
//		temp[1023:0] := temp[1023:0] >> (64*count)
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := temp[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VALIGNQ'. Intrinsic: '_mm512_maskz_alignr_epi64'.
// Requires AVX512F.
func MaskzAlignrEpi64(k Mmask8, a M512i, b M512i, count int) M512i {
	return M512i(maskzAlignrEpi64(uint8(k), [64]byte(a), [64]byte(b), count))
}

func maskzAlignrEpi64(k uint8, a [64]byte, b [64]byte, count int) [64]byte


// AlignrEpi8: Concatenate pairs of 16-byte blocks in 'a' and 'b' into a
// 32-byte temporary result, shift the result right by 'count' bytes, and store
// the low 16 bytes in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*128
//			tmp[255:0] := ((a[i+127:i] << 128) OR b[i+127:i]) >> (count[7:0]*8)
//			dst[i+127:i] := tmp[127:0]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPALIGNR'. Intrinsic: '_mm512_alignr_epi8'.
// Requires AVX512BW.
func AlignrEpi8(a M512i, b M512i, count int) M512i {
	return M512i(alignrEpi8([64]byte(a), [64]byte(b), count))
}

func alignrEpi8(a [64]byte, b [64]byte, count int) [64]byte


// MaskAlignrEpi8: Concatenate pairs of 16-byte blocks in 'a' and 'b' into a
// 32-byte temporary result, shift the result right by 'count' bytes, and store
// the low 16 bytes in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*128
//			tmp[255:0] := ((a[i+127:i] << 128) OR b[i+127:i]) >> (count[7:0]*8)
//			tmp_dst[i+127:i] := tmp[127:0]
//		ENDFOR
//		
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := tmp_dst[i+7:i]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPALIGNR'. Intrinsic: '_mm512_mask_alignr_epi8'.
// Requires AVX512BW.
func MaskAlignrEpi8(src M512i, k Mmask64, a M512i, b M512i, count int) M512i {
	return M512i(maskAlignrEpi8([64]byte(src), uint64(k), [64]byte(a), [64]byte(b), count))
}

func maskAlignrEpi8(src [64]byte, k uint64, a [64]byte, b [64]byte, count int) [64]byte


// MaskzAlignrEpi8: Concatenate pairs of 16-byte blocks in 'a' and 'b' into a
// 32-byte temporary result, shift the result right by 'count' bytes, and store
// the low 16 bytes in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*128
//			tmp[255:0] := ((a[i+127:i] << 128) OR b[i+127:i]) >> (count[7:0]*8)
//			tmp_dst[i+127:i] := tmp[127:0]
//		ENDFOR
//		
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := tmp_dst[i+7:i]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPALIGNR'. Intrinsic: '_mm512_maskz_alignr_epi8'.
// Requires AVX512BW.
func MaskzAlignrEpi8(k Mmask64, a M512i, b M512i, count int) M512i {
	return M512i(maskzAlignrEpi8(uint64(k), [64]byte(a), [64]byte(b), count))
}

func maskzAlignrEpi8(k uint64, a [64]byte, b [64]byte, count int) [64]byte


// AndEpi32: Compute the bitwise AND of packed 32-bit integers in 'a' and 'b',
// and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := a[i+31:i] BITWISE AND b[i+31:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPANDD'. Intrinsic: '_mm512_and_epi32'.
// Requires KNCNI.
func AndEpi32(a M512i, b M512i) M512i {
	return M512i(andEpi32([64]byte(a), [64]byte(b)))
}

func andEpi32(a [64]byte, b [64]byte) [64]byte


// MaskAndEpi32: Performs element-by-element bitwise AND between packed 32-bit
// integer elements of 'v2' and 'v3', storing the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := v2[i+31:i] & v3[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPANDD'. Intrinsic: '_mm512_mask_and_epi32'.
// Requires KNCNI.
func MaskAndEpi32(src M512i, k Mmask16, v2 M512i, v3 M512i) M512i {
	return M512i(maskAndEpi32([64]byte(src), uint16(k), [64]byte(v2), [64]byte(v3)))
}

func maskAndEpi32(src [64]byte, k uint16, v2 [64]byte, v3 [64]byte) [64]byte


// MaskzAndEpi32: Compute the bitwise AND of packed 32-bit integers in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] AND b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPANDD'. Intrinsic: '_mm512_maskz_and_epi32'.
// Requires AVX512F.
func MaskzAndEpi32(k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskzAndEpi32(uint16(k), [64]byte(a), [64]byte(b)))
}

func maskzAndEpi32(k uint16, a [64]byte, b [64]byte) [64]byte


// AndEpi64: Compute the bitwise AND of 512 bits (composed of packed 64-bit
// integers) in 'a' and 'b', and store the results in 'dst'. 
//
//		dst[511:0] := (a[511:0] AND b[511:0])
//		dst[MAX:512] := 0
//
// Instruction: 'VPANDQ'. Intrinsic: '_mm512_and_epi64'.
// Requires KNCNI.
func AndEpi64(a M512i, b M512i) M512i {
	return M512i(andEpi64([64]byte(a), [64]byte(b)))
}

func andEpi64(a [64]byte, b [64]byte) [64]byte


// MaskAndEpi64: Compute the bitwise AND of packed 64-bit integers in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] AND b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPANDQ'. Intrinsic: '_mm512_mask_and_epi64'.
// Requires KNCNI.
func MaskAndEpi64(src M512i, k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskAndEpi64([64]byte(src), uint8(k), [64]byte(a), [64]byte(b)))
}

func maskAndEpi64(src [64]byte, k uint8, a [64]byte, b [64]byte) [64]byte


// MaskzAndEpi64: Compute the bitwise AND of packed 64-bit integers in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] AND b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPANDQ'. Intrinsic: '_mm512_maskz_and_epi64'.
// Requires AVX512F.
func MaskzAndEpi64(k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskzAndEpi64(uint8(k), [64]byte(a), [64]byte(b)))
}

func maskzAndEpi64(k uint8, a [64]byte, b [64]byte) [64]byte


// AndPd: Compute the bitwise AND of packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := (a[i+63:i] AND b[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VANDPD'. Intrinsic: '_mm512_and_pd'.
// Requires AVX512DQ.
func AndPd(a M512d, b M512d) M512d {
	return M512d(andPd([8]float64(a), [8]float64(b)))
}

func andPd(a [8]float64, b [8]float64) [8]float64


// MaskAndPd: Compute the bitwise AND of packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] AND b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VANDPD'. Intrinsic: '_mm512_mask_and_pd'.
// Requires AVX512DQ.
func MaskAndPd(src M512d, k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskAndPd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b)))
}

func maskAndPd(src [8]float64, k uint8, a [8]float64, b [8]float64) [8]float64


// MaskzAndPd: Compute the bitwise AND of packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] AND b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VANDPD'. Intrinsic: '_mm512_maskz_and_pd'.
// Requires AVX512DQ.
func MaskzAndPd(k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskzAndPd(uint8(k), [8]float64(a), [8]float64(b)))
}

func maskzAndPd(k uint8, a [8]float64, b [8]float64) [8]float64


// AndPs: Compute the bitwise AND of packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := (a[i+31:i] AND b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VANDPS'. Intrinsic: '_mm512_and_ps'.
// Requires AVX512DQ.
func AndPs(a M512, b M512) M512 {
	return M512(andPs([16]float32(a), [16]float32(b)))
}

func andPs(a [16]float32, b [16]float32) [16]float32


// MaskAndPs: Compute the bitwise AND of packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] AND b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VANDPS'. Intrinsic: '_mm512_mask_and_ps'.
// Requires AVX512DQ.
func MaskAndPs(src M512, k Mmask16, a M512, b M512) M512 {
	return M512(maskAndPs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b)))
}

func maskAndPs(src [16]float32, k uint16, a [16]float32, b [16]float32) [16]float32


// MaskzAndPs: Compute the bitwise AND of packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] AND b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VANDPS'. Intrinsic: '_mm512_maskz_and_ps'.
// Requires AVX512DQ.
func MaskzAndPs(k Mmask16, a M512, b M512) M512 {
	return M512(maskzAndPs(uint16(k), [16]float32(a), [16]float32(b)))
}

func maskzAndPs(k uint16, a [16]float32, b [16]float32) [16]float32


// AndSi512: Compute the bitwise AND of 512 bits (representing integer data) in
// 'a' and 'b', and store the result in 'dst'. 
//
//		dst[511:0] := (a[511:0] AND b[511:0])
//		dst[MAX:512] := 0
//
// Instruction: 'VPANDD'. Intrinsic: '_mm512_and_si512'.
// Requires KNCNI.
func AndSi512(a M512i, b M512i) M512i {
	return M512i(andSi512([64]byte(a), [64]byte(b)))
}

func andSi512(a [64]byte, b [64]byte) [64]byte


// AndnotEpi32: Compute the bitwise AND NOT of packed 32-bit integers in 'a'
// and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := (NOT a[i+31:i]) AND b[i+31:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPANDND'. Intrinsic: '_mm512_andnot_epi32'.
// Requires KNCNI.
func AndnotEpi32(a M512i, b M512i) M512i {
	return M512i(andnotEpi32([64]byte(a), [64]byte(b)))
}

func andnotEpi32(a [64]byte, b [64]byte) [64]byte


// MaskAndnotEpi32: Compute the bitwise AND NOT of packed 32-bit integers in
// 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ((NOT a[i+31:i]) AND b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPANDND'. Intrinsic: '_mm512_mask_andnot_epi32'.
// Requires KNCNI.
func MaskAndnotEpi32(src M512i, k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskAndnotEpi32([64]byte(src), uint16(k), [64]byte(a), [64]byte(b)))
}

func maskAndnotEpi32(src [64]byte, k uint16, a [64]byte, b [64]byte) [64]byte


// MaskzAndnotEpi32: Compute the bitwise AND NOT of packed 32-bit integers in
// 'a' and 'b', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (NOT a[i+31:i]) AND b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPANDND'. Intrinsic: '_mm512_maskz_andnot_epi32'.
// Requires AVX512F.
func MaskzAndnotEpi32(k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskzAndnotEpi32(uint16(k), [64]byte(a), [64]byte(b)))
}

func maskzAndnotEpi32(k uint16, a [64]byte, b [64]byte) [64]byte


// AndnotEpi64: Compute the bitwise AND NOT of 512 bits (composed of packed
// 64-bit integers) in 'a' and 'b', and store the results in 'dst'. 
//
//		dst[511:0] := ((NOT a[511:0]) AND b[511:0])
//		dst[MAX:512] := 0
//
// Instruction: 'VPANDNQ'. Intrinsic: '_mm512_andnot_epi64'.
// Requires KNCNI.
func AndnotEpi64(a M512i, b M512i) M512i {
	return M512i(andnotEpi64([64]byte(a), [64]byte(b)))
}

func andnotEpi64(a [64]byte, b [64]byte) [64]byte


// MaskAndnotEpi64: Compute the bitwise AND NOT of packed 64-bit integers in
// 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ((NOT a[i+63:i]) AND b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPANDNQ'. Intrinsic: '_mm512_mask_andnot_epi64'.
// Requires KNCNI.
func MaskAndnotEpi64(src M512i, k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskAndnotEpi64([64]byte(src), uint8(k), [64]byte(a), [64]byte(b)))
}

func maskAndnotEpi64(src [64]byte, k uint8, a [64]byte, b [64]byte) [64]byte


// MaskzAndnotEpi64: Compute the bitwise AND NOT of packed 64-bit integers in
// 'a' and 'b', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (NOT a[i+63:i]) AND b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPANDNQ'. Intrinsic: '_mm512_maskz_andnot_epi64'.
// Requires AVX512F.
func MaskzAndnotEpi64(k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskzAndnotEpi64(uint8(k), [64]byte(a), [64]byte(b)))
}

func maskzAndnotEpi64(k uint8, a [64]byte, b [64]byte) [64]byte


// AndnotPd: Compute the bitwise AND NOT of packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ((NOT a[i+63:i]) AND b[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VANDNPD'. Intrinsic: '_mm512_andnot_pd'.
// Requires AVX512DQ.
func AndnotPd(a M512d, b M512d) M512d {
	return M512d(andnotPd([8]float64(a), [8]float64(b)))
}

func andnotPd(a [8]float64, b [8]float64) [8]float64


// MaskAndnotPd: Compute the bitwise AND NOT of packed double-precision
// (64-bit) floating-point elements in 'a' and 'b', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ((NOT a[i+63:i]) AND b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VANDNPD'. Intrinsic: '_mm512_mask_andnot_pd'.
// Requires AVX512DQ.
func MaskAndnotPd(src M512d, k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskAndnotPd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b)))
}

func maskAndnotPd(src [8]float64, k uint8, a [8]float64, b [8]float64) [8]float64


// MaskzAndnotPd: Compute the bitwise AND NOT of packed double-precision
// (64-bit) floating-point elements in 'a' and 'b', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ((NOT a[i+63:i]) AND b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VANDNPD'. Intrinsic: '_mm512_maskz_andnot_pd'.
// Requires AVX512DQ.
func MaskzAndnotPd(k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskzAndnotPd(uint8(k), [8]float64(a), [8]float64(b)))
}

func maskzAndnotPd(k uint8, a [8]float64, b [8]float64) [8]float64


// AndnotPs: Compute the bitwise AND NOT of packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ((NOT a[i+31:i]) AND b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VANDNPS'. Intrinsic: '_mm512_andnot_ps'.
// Requires AVX512DQ.
func AndnotPs(a M512, b M512) M512 {
	return M512(andnotPs([16]float32(a), [16]float32(b)))
}

func andnotPs(a [16]float32, b [16]float32) [16]float32


// MaskAndnotPs: Compute the bitwise AND NOT of packed single-precision
// (32-bit) floating-point elements in 'a' and 'b', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ((NOT a[i+31:i]) AND b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VANDNPS'. Intrinsic: '_mm512_mask_andnot_ps'.
// Requires AVX512DQ.
func MaskAndnotPs(src M512, k Mmask16, a M512, b M512) M512 {
	return M512(maskAndnotPs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b)))
}

func maskAndnotPs(src [16]float32, k uint16, a [16]float32, b [16]float32) [16]float32


// MaskzAndnotPs: Compute the bitwise AND NOT of packed single-precision
// (32-bit) floating-point elements in 'a' and 'b', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ((NOT a[i+31:i]) AND b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VANDNPS'. Intrinsic: '_mm512_maskz_andnot_ps'.
// Requires AVX512DQ.
func MaskzAndnotPs(k Mmask16, a M512, b M512) M512 {
	return M512(maskzAndnotPs(uint16(k), [16]float32(a), [16]float32(b)))
}

func maskzAndnotPs(k uint16, a [16]float32, b [16]float32) [16]float32


// AndnotSi512: Compute the bitwise AND NOT of 512 bits (representing integer
// data) in 'a' and 'b', and store the result in 'dst'. 
//
//		dst[511:0] := ((NOT a[511:0]) AND b[511:0])
//		dst[MAX:512] := 0
//
// Instruction: 'VPANDND'. Intrinsic: '_mm512_andnot_si512'.
// Requires KNCNI.
func AndnotSi512(a M512i, b M512i) M512i {
	return M512i(andnotSi512([64]byte(a), [64]byte(b)))
}

func andnotSi512(a [64]byte, b [64]byte) [64]byte


// AsinPd: Compute the inverse sine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ASIN(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_asin_pd'.
// Requires AVX512F.
func AsinPd(a M512d) M512d {
	return M512d(asinPd([8]float64(a)))
}

func asinPd(a [8]float64) [8]float64


// MaskAsinPd: Compute the inverse sine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ASIN(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_asin_pd'.
// Requires AVX512F.
func MaskAsinPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskAsinPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskAsinPd(src [8]float64, k uint8, a [8]float64) [8]float64


// AsinPs: Compute the inverse sine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ASIN(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_asin_ps'.
// Requires AVX512F.
func AsinPs(a M512) M512 {
	return M512(asinPs([16]float32(a)))
}

func asinPs(a [16]float32) [16]float32


// MaskAsinPs: Compute the inverse sine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ASIN(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_asin_ps'.
// Requires AVX512F.
func MaskAsinPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskAsinPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskAsinPs(src [16]float32, k uint16, a [16]float32) [16]float32


// AsinhPd: Compute the inverse hyperbolic sine of packed double-precision
// (64-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ASINH(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_asinh_pd'.
// Requires AVX512F.
func AsinhPd(a M512d) M512d {
	return M512d(asinhPd([8]float64(a)))
}

func asinhPd(a [8]float64) [8]float64


// MaskAsinhPd: Compute the inverse hyperbolic sine of packed double-precision
// (64-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ASINH(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_asinh_pd'.
// Requires AVX512F.
func MaskAsinhPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskAsinhPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskAsinhPd(src [8]float64, k uint8, a [8]float64) [8]float64


// AsinhPs: Compute the inverse hyperbolic sine of packed single-precision
// (32-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ASINH(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_asinh_ps'.
// Requires AVX512F.
func AsinhPs(a M512) M512 {
	return M512(asinhPs([16]float32(a)))
}

func asinhPs(a [16]float32) [16]float32


// MaskAsinhPs: Compute the inverse hyperbolic sine of packed single-precision
// (32-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ASINH(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_asinh_ps'.
// Requires AVX512F.
func MaskAsinhPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskAsinhPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskAsinhPs(src [16]float32, k uint16, a [16]float32) [16]float32


// AtanPd: Compute the inverse tangent of packed double-precision (64-bit)
// floating-point elements in 'a' and store the results in 'dst' expressed in
// radians. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ATAN(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_atan_pd'.
// Requires AVX512F.
func AtanPd(a M512d) M512d {
	return M512d(atanPd([8]float64(a)))
}

func atanPd(a [8]float64) [8]float64


// MaskAtanPd: Compute the inverse tangent of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' expressed in
// radians using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ATAN(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_atan_pd'.
// Requires AVX512F.
func MaskAtanPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskAtanPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskAtanPd(src [8]float64, k uint8, a [8]float64) [8]float64


// AtanPs: Compute the inverse tangent of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' expressed in
// radians. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ATAN(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_atan_ps'.
// Requires AVX512F.
func AtanPs(a M512) M512 {
	return M512(atanPs([16]float32(a)))
}

func atanPs(a [16]float32) [16]float32


// MaskAtanPs: Compute the inverse tangent of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ATAN(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_atan_ps'.
// Requires AVX512F.
func MaskAtanPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskAtanPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskAtanPs(src [16]float32, k uint16, a [16]float32) [16]float32


// Atan2Pd: Compute the inverse tangent of packed double-precision (64-bit)
// floating-point elements in 'a' divided by packed elements in 'b', and store
// the results in 'dst' expressed in radians. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ATAN(a[i+63:i] / b[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_atan2_pd'.
// Requires AVX512F.
func Atan2Pd(a M512d, b M512d) M512d {
	return M512d(atan2Pd([8]float64(a), [8]float64(b)))
}

func atan2Pd(a [8]float64, b [8]float64) [8]float64


// MaskAtan2Pd: Compute the inverse tangent of packed double-precision (64-bit)
// floating-point elements in 'a' divided by packed elements in 'b', and store
// the results in 'dst' expressed in radians using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ATAN(a[i+63:i] / b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_atan2_pd'.
// Requires AVX512F.
func MaskAtan2Pd(src M512d, k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskAtan2Pd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b)))
}

func maskAtan2Pd(src [8]float64, k uint8, a [8]float64, b [8]float64) [8]float64


// Atan2Ps: Compute the inverse tangent of packed single-precision (32-bit)
// floating-point elements in 'a' divided by packed elements in 'b', and store
// the results in 'dst' expressed in radians. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ATAN(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_atan2_ps'.
// Requires AVX512F.
func Atan2Ps(a M512, b M512) M512 {
	return M512(atan2Ps([16]float32(a), [16]float32(b)))
}

func atan2Ps(a [16]float32, b [16]float32) [16]float32


// MaskAtan2Ps: Compute the inverse tangent of packed single-precision (32-bit)
// floating-point elements in 'a' divided by packed elements in 'b', and store
// the results in 'dst' expressed in radians using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ATAN(a[i+31:i] / b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_atan2_ps'.
// Requires AVX512F.
func MaskAtan2Ps(src M512, k Mmask16, a M512, b M512) M512 {
	return M512(maskAtan2Ps([16]float32(src), uint16(k), [16]float32(a), [16]float32(b)))
}

func maskAtan2Ps(src [16]float32, k uint16, a [16]float32, b [16]float32) [16]float32


// AtanhPd: Compute the inverse hyperbolic tangent of packed double-precision
// (64-bit) floating-point elements in 'a' and store the results in 'dst'
// expressed in radians. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ATANH(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_atanh_pd'.
// Requires AVX512F.
func AtanhPd(a M512d) M512d {
	return M512d(atanhPd([8]float64(a)))
}

func atanhPd(a [8]float64) [8]float64


// MaskAtanhPd: Compute the inverse hyperbolic tangent of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst' expressed in radians using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ATANH(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_atanh_pd'.
// Requires AVX512F.
func MaskAtanhPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskAtanhPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskAtanhPd(src [8]float64, k uint8, a [8]float64) [8]float64


// AtanhPs: Compute the inverse hyperblic tangent of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'
// expressed in radians. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ATANH(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_atanh_ps'.
// Requires AVX512F.
func AtanhPs(a M512) M512 {
	return M512(atanhPs([16]float32(a)))
}

func atanhPs(a [16]float32) [16]float32


// MaskAtanhPs: Compute the inverse hyperbolic tangent of packed
// single-precision (32-bit) floating-point elements in 'a' expressed in
// radians, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ATANH(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_atanh_ps'.
// Requires AVX512F.
func MaskAtanhPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskAtanhPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskAtanhPs(src [16]float32, k uint16, a [16]float32) [16]float32


// AvgEpu16: Average packed unsigned 16-bit integers in 'a' and 'b', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			dst[i+15:i] := (a[i+15:i] + b[i+15:i] + 1) >> 1
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPAVGW'. Intrinsic: '_mm512_avg_epu16'.
// Requires AVX512BW.
func AvgEpu16(a M512i, b M512i) M512i {
	return M512i(avgEpu16([64]byte(a), [64]byte(b)))
}

func avgEpu16(a [64]byte, b [64]byte) [64]byte


// MaskAvgEpu16: Average packed unsigned 16-bit integers in 'a' and 'b', and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := (a[i+15:i] + b[i+15:i] + 1) >> 1
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPAVGW'. Intrinsic: '_mm512_mask_avg_epu16'.
// Requires AVX512BW.
func MaskAvgEpu16(src M512i, k Mmask32, a M512i, b M512i) M512i {
	return M512i(maskAvgEpu16([64]byte(src), uint32(k), [64]byte(a), [64]byte(b)))
}

func maskAvgEpu16(src [64]byte, k uint32, a [64]byte, b [64]byte) [64]byte


// MaskzAvgEpu16: Average packed unsigned 16-bit integers in 'a' and 'b', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := (a[i+15:i] + b[i+15:i] + 1) >> 1
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPAVGW'. Intrinsic: '_mm512_maskz_avg_epu16'.
// Requires AVX512BW.
func MaskzAvgEpu16(k Mmask32, a M512i, b M512i) M512i {
	return M512i(maskzAvgEpu16(uint32(k), [64]byte(a), [64]byte(b)))
}

func maskzAvgEpu16(k uint32, a [64]byte, b [64]byte) [64]byte


// AvgEpu8: Average packed unsigned 8-bit integers in 'a' and 'b', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 63
//			i := j*8
//			dst[i+7:i] := (a[i+7:i] + b[i+7:i] + 1) >> 1
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPAVGB'. Intrinsic: '_mm512_avg_epu8'.
// Requires AVX512BW.
func AvgEpu8(a M512i, b M512i) M512i {
	return M512i(avgEpu8([64]byte(a), [64]byte(b)))
}

func avgEpu8(a [64]byte, b [64]byte) [64]byte


// MaskAvgEpu8: Average packed unsigned 8-bit integers in 'a' and 'b', and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := (a[i+7:i] + b[i+7:i] + 1) >> 1
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPAVGB'. Intrinsic: '_mm512_mask_avg_epu8'.
// Requires AVX512BW.
func MaskAvgEpu8(src M512i, k Mmask64, a M512i, b M512i) M512i {
	return M512i(maskAvgEpu8([64]byte(src), uint64(k), [64]byte(a), [64]byte(b)))
}

func maskAvgEpu8(src [64]byte, k uint64, a [64]byte, b [64]byte) [64]byte


// MaskzAvgEpu8: Average packed unsigned 8-bit integers in 'a' and 'b', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := (a[i+7:i] + b[i+7:i] + 1) >> 1
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPAVGB'. Intrinsic: '_mm512_maskz_avg_epu8'.
// Requires AVX512BW.
func MaskzAvgEpu8(k Mmask64, a M512i, b M512i) M512i {
	return M512i(maskzAvgEpu8(uint64(k), [64]byte(a), [64]byte(b)))
}

func maskzAvgEpu8(k uint64, a [64]byte, b [64]byte) [64]byte


// MaskBlendEpi16: Blend packed 16-bit integers from 'a' and 'b' using control
// mask 'k', and store the results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := b[i+15:i]
//			ELSE
//				dst[i+15:i] := a[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBLENDMW'. Intrinsic: '_mm512_mask_blend_epi16'.
// Requires AVX512BW.
func MaskBlendEpi16(k Mmask32, a M512i, b M512i) M512i {
	return M512i(maskBlendEpi16(uint32(k), [64]byte(a), [64]byte(b)))
}

func maskBlendEpi16(k uint32, a [64]byte, b [64]byte) [64]byte


// MaskBlendEpi32: Blend packed 32-bit integers from 'a' and 'b' using control
// mask 'k', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := b[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBLENDMD'. Intrinsic: '_mm512_mask_blend_epi32'.
// Requires KNCNI.
func MaskBlendEpi32(k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskBlendEpi32(uint16(k), [64]byte(a), [64]byte(b)))
}

func maskBlendEpi32(k uint16, a [64]byte, b [64]byte) [64]byte


// MaskBlendEpi64: Blend packed 64-bit integers from 'a' and 'b' using control
// mask 'k', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := b[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBLENDMQ'. Intrinsic: '_mm512_mask_blend_epi64'.
// Requires KNCNI.
func MaskBlendEpi64(k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskBlendEpi64(uint8(k), [64]byte(a), [64]byte(b)))
}

func maskBlendEpi64(k uint8, a [64]byte, b [64]byte) [64]byte


// MaskBlendEpi8: Blend packed 8-bit integers from 'a' and 'b' using control
// mask 'k', and store the results in 'dst'. 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := b[i+7:i]
//			ELSE
//				dst[i+7:i] := a[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBLENDMB'. Intrinsic: '_mm512_mask_blend_epi8'.
// Requires AVX512BW.
func MaskBlendEpi8(k Mmask64, a M512i, b M512i) M512i {
	return M512i(maskBlendEpi8(uint64(k), [64]byte(a), [64]byte(b)))
}

func maskBlendEpi8(k uint64, a [64]byte, b [64]byte) [64]byte


// MaskBlendPd: Blend packed double-precision (64-bit) floating-point elements
// from 'a' and 'b' using control mask 'k', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := b[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBLENDMPD'. Intrinsic: '_mm512_mask_blend_pd'.
// Requires KNCNI.
func MaskBlendPd(k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskBlendPd(uint8(k), [8]float64(a), [8]float64(b)))
}

func maskBlendPd(k uint8, a [8]float64, b [8]float64) [8]float64


// MaskBlendPs: Blend packed single-precision (32-bit) floating-point elements
// from 'a' and 'b' using control mask 'k', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := b[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBLENDMPS'. Intrinsic: '_mm512_mask_blend_ps'.
// Requires KNCNI.
func MaskBlendPs(k Mmask16, a M512, b M512) M512 {
	return M512(maskBlendPs(uint16(k), [16]float32(a), [16]float32(b)))
}

func maskBlendPs(k uint16, a [16]float32, b [16]float32) [16]float32


// BroadcastF32x2: Broadcast the lower 2 packed single-precision (32-bit)
// floating-point elements from 'a' to all elements of 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			n := (j mod 2)*32
//			dst[i+31:i] := a[n+31:n]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTF32X2'. Intrinsic: '_mm512_broadcast_f32x2'.
// Requires AVX512DQ.
func BroadcastF32x2(a M128) M512 {
	return M512(broadcastF32x2([4]float32(a)))
}

func broadcastF32x2(a [4]float32) [16]float32


// MaskBroadcastF32x2: Broadcast the lower 2 packed single-precision (32-bit)
// floating-point elements from 'a' to all elements of 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			n := (j mod 2)*32
//			IF k[j]
//				dst[i+31:i] := a[n+31:n]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTF32X2'. Intrinsic: '_mm512_mask_broadcast_f32x2'.
// Requires AVX512DQ.
func MaskBroadcastF32x2(src M512, k Mmask16, a M128) M512 {
	return M512(maskBroadcastF32x2([16]float32(src), uint16(k), [4]float32(a)))
}

func maskBroadcastF32x2(src [16]float32, k uint16, a [4]float32) [16]float32


// MaskzBroadcastF32x2: Broadcast the lower 2 packed single-precision (32-bit)
// floating-point elements from 'a' to all elements of 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			n := (j mod 2)*32
//			IF k[j]
//				dst[i+31:i] := a[n+31:n]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTF32X2'. Intrinsic: '_mm512_maskz_broadcast_f32x2'.
// Requires AVX512DQ.
func MaskzBroadcastF32x2(k Mmask16, a M128) M512 {
	return M512(maskzBroadcastF32x2(uint16(k), [4]float32(a)))
}

func maskzBroadcastF32x2(k uint16, a [4]float32) [16]float32


// BroadcastF32x4: Broadcast the 4 packed single-precision (32-bit)
// floating-point elements from 'a' to all elements of 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			n := (j mod 4)*32
//			dst[i+31:i] := a[n+31:n]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTF32X4'. Intrinsic: '_mm512_broadcast_f32x4'.
// Requires AVX512F.
func BroadcastF32x4(a M128) M512 {
	return M512(broadcastF32x4([4]float32(a)))
}

func broadcastF32x4(a [4]float32) [16]float32


// MaskBroadcastF32x4: Broadcast the 4 packed single-precision (32-bit)
// floating-point elements from 'a' to all elements of 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			n := (j mod 4)*32
//			IF k[j]
//				dst[i+31:i] := a[n+31:n]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTF32X4'. Intrinsic: '_mm512_mask_broadcast_f32x4'.
// Requires AVX512F.
func MaskBroadcastF32x4(src M512, k Mmask16, a M128) M512 {
	return M512(maskBroadcastF32x4([16]float32(src), uint16(k), [4]float32(a)))
}

func maskBroadcastF32x4(src [16]float32, k uint16, a [4]float32) [16]float32


// MaskzBroadcastF32x4: Broadcast the 4 packed single-precision (32-bit)
// floating-point elements from 'a' to all elements of 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			n := (j mod 4)*32
//			IF k[j]
//				dst[i+31:i] := a[n+31:n]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTF32X4'. Intrinsic: '_mm512_maskz_broadcast_f32x4'.
// Requires AVX512F.
func MaskzBroadcastF32x4(k Mmask16, a M128) M512 {
	return M512(maskzBroadcastF32x4(uint16(k), [4]float32(a)))
}

func maskzBroadcastF32x4(k uint16, a [4]float32) [16]float32


// BroadcastF32x8: Broadcast the 8 packed single-precision (32-bit)
// floating-point elements from 'a' to all elements of 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			n := (j mod 8)*32
//			dst[i+31:i] := a[n+31:n]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTF32X8'. Intrinsic: '_mm512_broadcast_f32x8'.
// Requires AVX512DQ.
func BroadcastF32x8(a M256) M512 {
	return M512(broadcastF32x8([8]float32(a)))
}

func broadcastF32x8(a [8]float32) [16]float32


// MaskBroadcastF32x8: Broadcast the 8 packed single-precision (32-bit)
// floating-point elements from 'a' to all elements of 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			n := (j mod 8)*32
//			IF k[j]
//				dst[i+31:i] := a[n+31:n]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTF32X8'. Intrinsic: '_mm512_mask_broadcast_f32x8'.
// Requires AVX512DQ.
func MaskBroadcastF32x8(src M512, k Mmask16, a M256) M512 {
	return M512(maskBroadcastF32x8([16]float32(src), uint16(k), [8]float32(a)))
}

func maskBroadcastF32x8(src [16]float32, k uint16, a [8]float32) [16]float32


// MaskzBroadcastF32x8: Broadcast the 8 packed single-precision (32-bit)
// floating-point elements from 'a' to all elements of 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			n := (j mod 8)*32
//			IF k[j]
//				dst[i+31:i] := a[n+31:n]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTF32X8'. Intrinsic: '_mm512_maskz_broadcast_f32x8'.
// Requires AVX512DQ.
func MaskzBroadcastF32x8(k Mmask16, a M256) M512 {
	return M512(maskzBroadcastF32x8(uint16(k), [8]float32(a)))
}

func maskzBroadcastF32x8(k uint16, a [8]float32) [16]float32


// BroadcastF64x2: Broadcast the 2 packed double-precision (64-bit)
// floating-point elements from 'a' to all elements of 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			n := (j mod 2)*64
//			dst[i+63:i] := a[n+63:n]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTF64X2'. Intrinsic: '_mm512_broadcast_f64x2'.
// Requires AVX512DQ.
func BroadcastF64x2(a M128d) M512d {
	return M512d(broadcastF64x2([2]float64(a)))
}

func broadcastF64x2(a [2]float64) [8]float64


// MaskBroadcastF64x2: Broadcast the 2 packed double-precision (64-bit)
// floating-point elements from 'a' to all elements of 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			n := (j mod 2)*64
//			IF k[j]
//				dst[i+63:i] := a[n+63:n]
//			ELSE
//				dst[i+63:i] := src[n+63:n]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTF64X2'. Intrinsic: '_mm512_mask_broadcast_f64x2'.
// Requires AVX512DQ.
func MaskBroadcastF64x2(src M512d, k Mmask8, a M128d) M512d {
	return M512d(maskBroadcastF64x2([8]float64(src), uint8(k), [2]float64(a)))
}

func maskBroadcastF64x2(src [8]float64, k uint8, a [2]float64) [8]float64


// MaskzBroadcastF64x2: Broadcast the 2 packed double-precision (64-bit)
// floating-point elements from 'a' to all elements of 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			n := (j mod 2)*64
//			IF k[j]
//				dst[i+63:i] := a[n+63:n]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTF64X2'. Intrinsic: '_mm512_maskz_broadcast_f64x2'.
// Requires AVX512DQ.
func MaskzBroadcastF64x2(k Mmask8, a M128d) M512d {
	return M512d(maskzBroadcastF64x2(uint8(k), [2]float64(a)))
}

func maskzBroadcastF64x2(k uint8, a [2]float64) [8]float64


// BroadcastF64x4: Broadcast the 4 packed double-precision (64-bit)
// floating-point elements from 'a' to all elements of 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			n := (j mod 4)*64
//			dst[i+63:i] := a[n+63:n]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTF64X4'. Intrinsic: '_mm512_broadcast_f64x4'.
// Requires AVX512F.
func BroadcastF64x4(a M256d) M512d {
	return M512d(broadcastF64x4([4]float64(a)))
}

func broadcastF64x4(a [4]float64) [8]float64


// MaskBroadcastF64x4: Broadcast the 4 packed double-precision (64-bit)
// floating-point elements from 'a' to all elements of 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			n := (j mod 4)*64
//			IF k[j]
//				dst[i+63:i] := a[n+63:n]
//			ELSE
//				dst[i+63:i] := src[n+63:n]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTF64X4'. Intrinsic: '_mm512_mask_broadcast_f64x4'.
// Requires AVX512F.
func MaskBroadcastF64x4(src M512d, k Mmask8, a M256d) M512d {
	return M512d(maskBroadcastF64x4([8]float64(src), uint8(k), [4]float64(a)))
}

func maskBroadcastF64x4(src [8]float64, k uint8, a [4]float64) [8]float64


// MaskzBroadcastF64x4: Broadcast the 4 packed double-precision (64-bit)
// floating-point elements from 'a' to all elements of 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			n := (j mod 4)*64
//			IF k[j]
//				dst[i+63:i] := a[n+63:n]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTF64X4'. Intrinsic: '_mm512_maskz_broadcast_f64x4'.
// Requires AVX512F.
func MaskzBroadcastF64x4(k Mmask8, a M256d) M512d {
	return M512d(maskzBroadcastF64x4(uint8(k), [4]float64(a)))
}

func maskzBroadcastF64x4(k uint8, a [4]float64) [8]float64


// BroadcastI32x2: Broadcast the lower 2 packed 32-bit integers from 'a' to all
// elements of "dst. 
//
//		FOR j := 0 to 3
//			i := j*32
//			n := (j mod 2)*32
//			dst[i+31:i] := a[n+31:n]
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VBROADCASTI32X2'. Intrinsic: '_mm_broadcast_i32x2'.
// Requires AVX512DQ.
func BroadcastI32x2(a M128i) M128i {
	return M128i(broadcastI32x2([16]byte(a)))
}

func broadcastI32x2(a [16]byte) [16]byte


// MaskBroadcastI32x2: Broadcast the lower 2 packed 32-bit integers from 'a' to
// all elements of 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			n := (j mod 2)*32
//			IF k[j]
//				dst[i+31:i] := a[n+31:n]
//			ELSE
//				dst[i+31:i] := src[n+31:n]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VBROADCASTI32X2'. Intrinsic: '_mm_mask_broadcast_i32x2'.
// Requires AVX512DQ.
func MaskBroadcastI32x2(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskBroadcastI32x2([16]byte(src), uint8(k), [16]byte(a)))
}

func maskBroadcastI32x2(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzBroadcastI32x2: Broadcast the lower 2 packed 32-bit integers from 'a'
// to all elements of 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			n := (j mod 2)*32
//			IF k[j]
//				dst[i+31:i] := a[n+31:n]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VBROADCASTI32X2'. Intrinsic: '_mm_maskz_broadcast_i32x2'.
// Requires AVX512DQ.
func MaskzBroadcastI32x2(k Mmask8, a M128i) M128i {
	return M128i(maskzBroadcastI32x2(uint8(k), [16]byte(a)))
}

func maskzBroadcastI32x2(k uint8, a [16]byte) [16]byte


// BroadcastI32x21: Broadcast the lower 2 packed 32-bit integers from 'a' to
// all elements of "dst. 
//
//		FOR j := 0 to 15
//			i := j*32
//			n := (j mod 2)*32
//			dst[i+31:i] := a[n+31:n]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTI32X2'. Intrinsic: '_mm512_broadcast_i32x2'.
// Requires AVX512DQ.
func BroadcastI32x21(a M128i) M512i {
	return M512i(broadcastI32x21([16]byte(a)))
}

func broadcastI32x21(a [16]byte) [64]byte


// MaskBroadcastI32x21: Broadcast the lower 2 packed 32-bit integers from 'a'
// to all elements of 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			n := (j mod 2)*32
//			IF k[j]
//				dst[i+31:i] := a[n+31:n]
//			ELSE
//				dst[i+31:i] := src[n+31:n]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTI32X2'. Intrinsic: '_mm512_mask_broadcast_i32x2'.
// Requires AVX512DQ.
func MaskBroadcastI32x21(src M512i, k Mmask16, a M128i) M512i {
	return M512i(maskBroadcastI32x21([64]byte(src), uint16(k), [16]byte(a)))
}

func maskBroadcastI32x21(src [64]byte, k uint16, a [16]byte) [64]byte


// MaskzBroadcastI32x21: Broadcast the lower 2 packed 32-bit integers from 'a'
// to all elements of 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			n := (j mod 2)*32
//			IF k[j]
//				dst[i+31:i] := a[n+31:n]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTI32X2'. Intrinsic: '_mm512_maskz_broadcast_i32x2'.
// Requires AVX512DQ.
func MaskzBroadcastI32x21(k Mmask16, a M128i) M512i {
	return M512i(maskzBroadcastI32x21(uint16(k), [16]byte(a)))
}

func maskzBroadcastI32x21(k uint16, a [16]byte) [64]byte


// BroadcastI32x4: Broadcast the 4 packed 32-bit integers from 'a' to all
// elements of 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			n := (j mod 4)*32
//			dst[i+31:i] := a[n+31:n]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTI32X4'. Intrinsic: '_mm512_broadcast_i32x4'.
// Requires AVX512F.
func BroadcastI32x4(a M128i) M512i {
	return M512i(broadcastI32x4([16]byte(a)))
}

func broadcastI32x4(a [16]byte) [64]byte


// MaskBroadcastI32x4: Broadcast the 4 packed 32-bit integers from 'a' to all
// elements of 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			n := (j mod 4)*32
//			IF k[j]
//				dst[i+31:i] := a[n+31:n]
//			ELSE
//				dst[i+31:i] := src[n+31:n]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTI32X4'. Intrinsic: '_mm512_mask_broadcast_i32x4'.
// Requires AVX512F.
func MaskBroadcastI32x4(src M512i, k Mmask16, a M128i) M512i {
	return M512i(maskBroadcastI32x4([64]byte(src), uint16(k), [16]byte(a)))
}

func maskBroadcastI32x4(src [64]byte, k uint16, a [16]byte) [64]byte


// MaskzBroadcastI32x4: Broadcast the 4 packed 32-bit integers from 'a' to all
// elements of 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			n := (j mod 4)*32
//			IF k[j]
//				dst[i+31:i] := a[n+31:n]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTI32X4'. Intrinsic: '_mm512_maskz_broadcast_i32x4'.
// Requires AVX512F.
func MaskzBroadcastI32x4(k Mmask16, a M128i) M512i {
	return M512i(maskzBroadcastI32x4(uint16(k), [16]byte(a)))
}

func maskzBroadcastI32x4(k uint16, a [16]byte) [64]byte


// BroadcastI32x8: Broadcast the 8 packed 32-bit integers from 'a' to all
// elements of 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			n := (j mod 8)*32
//			dst[i+31:i] := a[n+31:n]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTI32X8'. Intrinsic: '_mm512_broadcast_i32x8'.
// Requires AVX512DQ.
func BroadcastI32x8(a M256i) M512i {
	return M512i(broadcastI32x8([32]byte(a)))
}

func broadcastI32x8(a [32]byte) [64]byte


// MaskBroadcastI32x8: Broadcast the 8 packed 32-bit integers from 'a' to all
// elements of 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			n := (j mod 8)*32
//			IF k[j]
//				dst[i+31:i] := a[n+31:n]
//			ELSE
//				dst[i+31:i] := src[n+31:n]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTI32X8'. Intrinsic: '_mm512_mask_broadcast_i32x8'.
// Requires AVX512DQ.
func MaskBroadcastI32x8(src M512i, k Mmask16, a M256i) M512i {
	return M512i(maskBroadcastI32x8([64]byte(src), uint16(k), [32]byte(a)))
}

func maskBroadcastI32x8(src [64]byte, k uint16, a [32]byte) [64]byte


// MaskzBroadcastI32x8: Broadcast the 8 packed 32-bit integers from 'a' to all
// elements of 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			n := (j mod 8)*32
//			IF k[j]
//				dst[i+31:i] := a[n+31:n]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTI32X8'. Intrinsic: '_mm512_maskz_broadcast_i32x8'.
// Requires AVX512DQ.
func MaskzBroadcastI32x8(k Mmask16, a M256i) M512i {
	return M512i(maskzBroadcastI32x8(uint16(k), [32]byte(a)))
}

func maskzBroadcastI32x8(k uint16, a [32]byte) [64]byte


// BroadcastI64x2: Broadcast the 2 packed 64-bit integers from 'a' to all
// elements of 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			n := (j mod 2)*64
//			dst[i+63:i] := a[n+63:n]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTI64X2'. Intrinsic: '_mm512_broadcast_i64x2'.
// Requires AVX512DQ.
func BroadcastI64x2(a M128i) M512i {
	return M512i(broadcastI64x2([16]byte(a)))
}

func broadcastI64x2(a [16]byte) [64]byte


// MaskBroadcastI64x2: Broadcast the 2 packed 64-bit integers from 'a' to all
// elements of 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			n := (j mod 2)*64
//			IF k[j]
//				dst[i+63:i] := a[n+63:n]
//			ELSE
//				dst[i+63:i] := src[n+63:n]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTI64X2'. Intrinsic: '_mm512_mask_broadcast_i64x2'.
// Requires AVX512DQ.
func MaskBroadcastI64x2(src M512i, k Mmask8, a M128i) M512i {
	return M512i(maskBroadcastI64x2([64]byte(src), uint8(k), [16]byte(a)))
}

func maskBroadcastI64x2(src [64]byte, k uint8, a [16]byte) [64]byte


// MaskzBroadcastI64x2: Broadcast the 2 packed 64-bit integers from 'a' to all
// elements of 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			n := (j mod 2)*64
//			IF k[j]
//				dst[i+63:i] := a[n+63:n]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTI64X2'. Intrinsic: '_mm512_maskz_broadcast_i64x2'.
// Requires AVX512DQ.
func MaskzBroadcastI64x2(k Mmask8, a M128i) M512i {
	return M512i(maskzBroadcastI64x2(uint8(k), [16]byte(a)))
}

func maskzBroadcastI64x2(k uint8, a [16]byte) [64]byte


// BroadcastI64x4: Broadcast the 4 packed 64-bit integers from 'a' to all
// elements of 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			n := (j mod 4)*64
//			dst[i+63:i] := a[n+63:n]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTI64X4'. Intrinsic: '_mm512_broadcast_i64x4'.
// Requires AVX512F.
func BroadcastI64x4(a M256i) M512i {
	return M512i(broadcastI64x4([32]byte(a)))
}

func broadcastI64x4(a [32]byte) [64]byte


// MaskBroadcastI64x4: Broadcast the 4 packed 64-bit integers from 'a' to all
// elements of 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			n := (j mod 4)*64
//			IF k[j]
//				dst[i+63:i] := a[n+63:n]
//			ELSE
//				dst[i+63:i] := src[n+63:n]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTI64X4'. Intrinsic: '_mm512_mask_broadcast_i64x4'.
// Requires AVX512F.
func MaskBroadcastI64x4(src M512i, k Mmask8, a M256i) M512i {
	return M512i(maskBroadcastI64x4([64]byte(src), uint8(k), [32]byte(a)))
}

func maskBroadcastI64x4(src [64]byte, k uint8, a [32]byte) [64]byte


// MaskzBroadcastI64x4: Broadcast the 4 packed 64-bit integers from 'a' to all
// elements of 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			n := (j mod 4)*64
//			IF k[j]
//				dst[i+63:i] := a[n+63:n]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTI64X4'. Intrinsic: '_mm512_maskz_broadcast_i64x4'.
// Requires AVX512F.
func MaskzBroadcastI64x4(k Mmask8, a M256i) M512i {
	return M512i(maskzBroadcastI64x4(uint8(k), [32]byte(a)))
}

func maskzBroadcastI64x4(k uint8, a [32]byte) [64]byte


// BroadcastbEpi8: Broadcast the low packed 8-bit integer from 'a' to all
// elements of 'dst'. 
//
//		FOR j := 0 to 63
//			i := j*8
//			dst[i+7:i] := a[7:0]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTB'. Intrinsic: '_mm512_broadcastb_epi8'.
// Requires AVX512BW.
func BroadcastbEpi8(a M128i) M512i {
	return M512i(broadcastbEpi8([16]byte(a)))
}

func broadcastbEpi8(a [16]byte) [64]byte


// MaskBroadcastbEpi8: Broadcast the low packed 8-bit integer from 'a' to all
// elements of 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := a[7:0]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTB'. Intrinsic: '_mm512_mask_broadcastb_epi8'.
// Requires AVX512BW.
func MaskBroadcastbEpi8(src M512i, k Mmask64, a M128i) M512i {
	return M512i(maskBroadcastbEpi8([64]byte(src), uint64(k), [16]byte(a)))
}

func maskBroadcastbEpi8(src [64]byte, k uint64, a [16]byte) [64]byte


// MaskzBroadcastbEpi8: Broadcast the low packed 8-bit integer from 'a' to all
// elements of 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := a[7:0]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTB'. Intrinsic: '_mm512_maskz_broadcastb_epi8'.
// Requires AVX512BW.
func MaskzBroadcastbEpi8(k Mmask64, a M128i) M512i {
	return M512i(maskzBroadcastbEpi8(uint64(k), [16]byte(a)))
}

func maskzBroadcastbEpi8(k uint64, a [16]byte) [64]byte


// BroadcastdEpi32: Broadcast the low packed 32-bit integer from 'a' to all
// elements of 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := a[31:0]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm512_broadcastd_epi32'.
// Requires AVX512F.
func BroadcastdEpi32(a M128i) M512i {
	return M512i(broadcastdEpi32([16]byte(a)))
}

func broadcastdEpi32(a [16]byte) [64]byte


// MaskBroadcastdEpi32: Broadcast the low packed 32-bit integer from 'a' to all
// elements of 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm512_mask_broadcastd_epi32'.
// Requires AVX512F.
func MaskBroadcastdEpi32(src M512i, k Mmask16, a M128i) M512i {
	return M512i(maskBroadcastdEpi32([64]byte(src), uint16(k), [16]byte(a)))
}

func maskBroadcastdEpi32(src [64]byte, k uint16, a [16]byte) [64]byte


// MaskzBroadcastdEpi32: Broadcast the low packed 32-bit integer from 'a' to
// all elements of 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm512_maskz_broadcastd_epi32'.
// Requires AVX512F.
func MaskzBroadcastdEpi32(k Mmask16, a M128i) M512i {
	return M512i(maskzBroadcastdEpi32(uint16(k), [16]byte(a)))
}

func maskzBroadcastdEpi32(k uint16, a [16]byte) [64]byte


// BroadcastmbEpi64: Broadcast the low 8-bits from input mask 'k' to all 64-bit
// elements of 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ZeroExtend(k[7:0])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTMB2Q'. Intrinsic: '_mm512_broadcastmb_epi64'.
// Requires AVX512CD.
func BroadcastmbEpi64(k Mmask8) M512i {
	return M512i(broadcastmbEpi64(uint8(k)))
}

func broadcastmbEpi64(k uint8) [64]byte


// BroadcastmwEpi32: Broadcast the low 16-bits from input mask 'k' to all
// 32-bit elements of 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ZeroExtend(k[15:0])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTMW2D'. Intrinsic: '_mm512_broadcastmw_epi32'.
// Requires AVX512CD.
func BroadcastmwEpi32(k Mmask16) M512i {
	return M512i(broadcastmwEpi32(uint16(k)))
}

func broadcastmwEpi32(k uint16) [64]byte


// BroadcastqEpi64: Broadcast the low packed 64-bit integer from 'a' to all
// elements of 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := a[63:0]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm512_broadcastq_epi64'.
// Requires AVX512F.
func BroadcastqEpi64(a M128i) M512i {
	return M512i(broadcastqEpi64([16]byte(a)))
}

func broadcastqEpi64(a [16]byte) [64]byte


// MaskBroadcastqEpi64: Broadcast the low packed 64-bit integer from 'a' to all
// elements of 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm512_mask_broadcastq_epi64'.
// Requires AVX512F.
func MaskBroadcastqEpi64(src M512i, k Mmask8, a M128i) M512i {
	return M512i(maskBroadcastqEpi64([64]byte(src), uint8(k), [16]byte(a)))
}

func maskBroadcastqEpi64(src [64]byte, k uint8, a [16]byte) [64]byte


// MaskzBroadcastqEpi64: Broadcast the low packed 64-bit integer from 'a' to
// all elements of 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm512_maskz_broadcastq_epi64'.
// Requires AVX512F.
func MaskzBroadcastqEpi64(k Mmask8, a M128i) M512i {
	return M512i(maskzBroadcastqEpi64(uint8(k), [16]byte(a)))
}

func maskzBroadcastqEpi64(k uint8, a [16]byte) [64]byte


// BroadcastsdPd: Broadcast the low double-precision (64-bit) floating-point
// element from 'a' to all elements of 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := a[63:0]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTSD'. Intrinsic: '_mm512_broadcastsd_pd'.
// Requires AVX512F.
func BroadcastsdPd(a M128d) M512d {
	return M512d(broadcastsdPd([2]float64(a)))
}

func broadcastsdPd(a [2]float64) [8]float64


// MaskBroadcastsdPd: Broadcast the low double-precision (64-bit)
// floating-point element from 'a' to all elements of 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTSD'. Intrinsic: '_mm512_mask_broadcastsd_pd'.
// Requires AVX512F.
func MaskBroadcastsdPd(src M512d, k Mmask8, a M128d) M512d {
	return M512d(maskBroadcastsdPd([8]float64(src), uint8(k), [2]float64(a)))
}

func maskBroadcastsdPd(src [8]float64, k uint8, a [2]float64) [8]float64


// MaskzBroadcastsdPd: Broadcast the low double-precision (64-bit)
// floating-point element from 'a' to all elements of 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTSD'. Intrinsic: '_mm512_maskz_broadcastsd_pd'.
// Requires AVX512F.
func MaskzBroadcastsdPd(k Mmask8, a M128d) M512d {
	return M512d(maskzBroadcastsdPd(uint8(k), [2]float64(a)))
}

func maskzBroadcastsdPd(k uint8, a [2]float64) [8]float64


// BroadcastssPs: Broadcast the low single-precision (32-bit) floating-point
// element from 'a' to all elements of 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := a[31:0]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTSS'. Intrinsic: '_mm512_broadcastss_ps'.
// Requires AVX512F.
func BroadcastssPs(a M128) M512 {
	return M512(broadcastssPs([4]float32(a)))
}

func broadcastssPs(a [4]float32) [16]float32


// MaskBroadcastssPs: Broadcast the low single-precision (32-bit)
// floating-point element from 'a' to all elements of 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTSS'. Intrinsic: '_mm512_mask_broadcastss_ps'.
// Requires AVX512F.
func MaskBroadcastssPs(src M512, k Mmask16, a M128) M512 {
	return M512(maskBroadcastssPs([16]float32(src), uint16(k), [4]float32(a)))
}

func maskBroadcastssPs(src [16]float32, k uint16, a [4]float32) [16]float32


// MaskzBroadcastssPs: Broadcast the low single-precision (32-bit)
// floating-point element from 'a' to all elements of 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTSS'. Intrinsic: '_mm512_maskz_broadcastss_ps'.
// Requires AVX512F.
func MaskzBroadcastssPs(k Mmask16, a M128) M512 {
	return M512(maskzBroadcastssPs(uint16(k), [4]float32(a)))
}

func maskzBroadcastssPs(k uint16, a [4]float32) [16]float32


// BroadcastwEpi16: Broadcast the low packed 16-bit integer from 'a' to all
// elements of 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			dst[i+15:i] := a[15:0]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTW'. Intrinsic: '_mm512_broadcastw_epi16'.
// Requires AVX512BW.
func BroadcastwEpi16(a M128i) M512i {
	return M512i(broadcastwEpi16([16]byte(a)))
}

func broadcastwEpi16(a [16]byte) [64]byte


// MaskBroadcastwEpi16: Broadcast the low packed 16-bit integer from 'a' to all
// elements of 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := a[15:0]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTW'. Intrinsic: '_mm512_mask_broadcastw_epi16'.
// Requires AVX512BW.
func MaskBroadcastwEpi16(src M512i, k Mmask32, a M128i) M512i {
	return M512i(maskBroadcastwEpi16([64]byte(src), uint32(k), [16]byte(a)))
}

func maskBroadcastwEpi16(src [64]byte, k uint32, a [16]byte) [64]byte


// MaskzBroadcastwEpi16: Broadcast the low packed 16-bit integer from 'a' to
// all elements of 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := a[15:0]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTW'. Intrinsic: '_mm512_maskz_broadcastw_epi16'.
// Requires AVX512BW.
func MaskzBroadcastwEpi16(k Mmask32, a M128i) M512i {
	return M512i(maskzBroadcastwEpi16(uint32(k), [16]byte(a)))
}

func maskzBroadcastwEpi16(k uint32, a [16]byte) [64]byte


// BslliEpi128: Shift 128-bit lanes in 'a' left by 'imm8' bytes while shifting
// in zeros, and store the results in 'dst'. 
//
//		tmp := imm8[7:0]
//		IF tmp > 15
//			tmp := 16
//		FI
//		dst[127:0] := a[127:0] << (tmp*8)
//		dst[255:128] := a[255:128] << (tmp*8)
//		dst[383:256] := a[383:256] << (tmp*8)
//		dst[511:384] := a[511:384] << (tmp*8)
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLDQ'. Intrinsic: '_mm512_bslli_epi128'.
// Requires AVX512BW.
func BslliEpi128(a M512i, imm8 int) M512i {
	return M512i(bslliEpi128([64]byte(a), imm8))
}

func bslliEpi128(a [64]byte, imm8 int) [64]byte


// BsrliEpi128: Shift 128-bit lanes in 'a' right by 'imm8' bytes while shifting
// in zeros, and store the results in 'dst'. 
//
//		tmp := imm8[7:0]
//		IF tmp > 15
//			tmp := 16
//		FI
//		dst[127:0] := a[127:0] >> (tmp*8)
//		dst[255:128] := a[255:128] >> (tmp*8)
//		dst[383:256] := a[383:256] >> (tmp*8)
//		dst[511:384] := a[511:384] >> (tmp*8)
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLDQ'. Intrinsic: '_mm512_bsrli_epi128'.
// Requires AVX512BW.
func BsrliEpi128(a M512i, imm8 int) M512i {
	return M512i(bsrliEpi128([64]byte(a), imm8))
}

func bsrliEpi128(a [64]byte, imm8 int) [64]byte


// CastpdPs: Cast vector of type __m512d to type __m512.
// 	This intrinsic is only used for compilation and does not generate any
// instructions, thus it has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_castpd_ps'.
// Requires KNCNI.
func CastpdPs(a M512d) M512 {
	return M512(castpdPs([8]float64(a)))
}

func castpdPs(a [8]float64) [16]float32


// CastpdSi512: Cast vector of type __m512d to type __m512i.
// 	This intrinsic is only used for compilation and does not generate any
// instructions, thus it has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_castpd_si512'.
// Requires KNCNI.
func CastpdSi512(a M512d) M512i {
	return M512i(castpdSi512([8]float64(a)))
}

func castpdSi512(a [8]float64) [64]byte


// Castpd128Pd512: Cast vector of type __m128d to type __m512d; the upper 384
// bits of the result are undefined. 
// 	This intrinsic is only used for compilation and does not generate any
// instructions, thus it has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_castpd128_pd512'.
// Requires AVX512F.
func Castpd128Pd512(a M128d) M512d {
	return M512d(castpd128Pd512([2]float64(a)))
}

func castpd128Pd512(a [2]float64) [8]float64


// Castpd256Pd512: Cast vector of type __m256d to type __m512d; the upper 256
// bits of the result are undefined. 
// 	This intrinsic is only used for compilation and does not generate any
// instructions, thus it has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_castpd256_pd512'.
// Requires AVX512F.
func Castpd256Pd512(a M256d) M512d {
	return M512d(castpd256Pd512([4]float64(a)))
}

func castpd256Pd512(a [4]float64) [8]float64


// Castpd512Pd128: Cast vector of type __m512d to type __m128d. 
// 	This intrinsic is only used for compilation and does not generate any
// instructions, thus it has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_castpd512_pd128'.
// Requires AVX512F.
func Castpd512Pd128(a M512d) M128d {
	return M128d(castpd512Pd128([8]float64(a)))
}

func castpd512Pd128(a [8]float64) [2]float64


// Castpd512Pd256: Cast vector of type __m512d to type __m256d. 
// 	This intrinsic is only used for compilation and does not generate any
// instructions, thus it has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_castpd512_pd256'.
// Requires AVX512F.
func Castpd512Pd256(a M512d) M256d {
	return M256d(castpd512Pd256([8]float64(a)))
}

func castpd512Pd256(a [8]float64) [4]float64


// CastpsPd: Cast vector of type __m512 to type __m512d.
// 	This intrinsic is only used for compilation and does not generate any
// instructions, thus it has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_castps_pd'.
// Requires KNCNI.
func CastpsPd(a M512) M512d {
	return M512d(castpsPd([16]float32(a)))
}

func castpsPd(a [16]float32) [8]float64


// CastpsSi512: Cast vector of type __m512 to type __m512i.
// 	This intrinsic is only used for compilation and does not generate any
// instructions, thus it has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_castps_si512'.
// Requires KNCNI.
func CastpsSi512(a M512) M512i {
	return M512i(castpsSi512([16]float32(a)))
}

func castpsSi512(a [16]float32) [64]byte


// Castps128Ps512: Cast vector of type __m128 to type __m512; the upper 384
// bits of the result are undefined. 
// 	This intrinsic is only used for compilation and does not generate any
// instructions, thus it has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_castps128_ps512'.
// Requires AVX512F.
func Castps128Ps512(a M128) M512 {
	return M512(castps128Ps512([4]float32(a)))
}

func castps128Ps512(a [4]float32) [16]float32


// Castps256Ps512: Cast vector of type __m256 to type __m512; the upper 256
// bits of the result are undefined. 
// 	This intrinsic is only used for compilation and does not generate any
// instructions, thus it has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_castps256_ps512'.
// Requires AVX512F.
func Castps256Ps512(a M256) M512 {
	return M512(castps256Ps512([8]float32(a)))
}

func castps256Ps512(a [8]float32) [16]float32


// Castps512Ps128: Cast vector of type __m512 to type __m128. 
// 	This intrinsic is only used for compilation and does not generate any
// instructions, thus it has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_castps512_ps128'.
// Requires AVX512F.
func Castps512Ps128(a M512) M128 {
	return M128(castps512Ps128([16]float32(a)))
}

func castps512Ps128(a [16]float32) [4]float32


// Castps512Ps256: Cast vector of type __m512 to type __m256. 
// 	This intrinsic is only used for compilation and does not generate any
// instructions, thus it has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_castps512_ps256'.
// Requires AVX512F.
func Castps512Ps256(a M512) M256 {
	return M256(castps512Ps256([16]float32(a)))
}

func castps512Ps256(a [16]float32) [8]float32


// Castsi128Si512: Cast vector of type __m128i to type __m512i; the upper 384
// bits of the result are undefined. 
// 	This intrinsic is only used for compilation and does not generate any
// instructions, thus it has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_castsi128_si512'.
// Requires AVX512F.
func Castsi128Si512(a M128i) M512i {
	return M512i(castsi128Si512([16]byte(a)))
}

func castsi128Si512(a [16]byte) [64]byte


// Castsi256Si512: Cast vector of type __m256i to type __m512i; the upper 256
// bits of the result are undefined.
// 	 This intrinsic is only used for compilation and does not generate any
// instructions, thus it has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_castsi256_si512'.
// Requires AVX512F.
func Castsi256Si512(a M256i) M512i {
	return M512i(castsi256Si512([32]byte(a)))
}

func castsi256Si512(a [32]byte) [64]byte


// Castsi512Pd: Cast vector of type __m512i to type __m512d.
// 	This intrinsic is only used for compilation and does not generate any
// instructions, thus it has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_castsi512_pd'.
// Requires KNCNI.
func Castsi512Pd(a M512i) M512d {
	return M512d(castsi512Pd([64]byte(a)))
}

func castsi512Pd(a [64]byte) [8]float64


// Castsi512Ps: Cast vector of type __m512i to type __m512.
// 	This intrinsic is only used for compilation and does not generate any
// instructions, thus it has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_castsi512_ps'.
// Requires KNCNI.
func Castsi512Ps(a M512i) M512 {
	return M512(castsi512Ps([64]byte(a)))
}

func castsi512Ps(a [64]byte) [16]float32


// Castsi512Si128: Cast vector of type __m512i to type __m128i.
// 	 This intrinsic is only used for compilation and does not generate any
// instructions, thus it has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_castsi512_si128'.
// Requires AVX512F.
func Castsi512Si128(a M512i) M128i {
	return M128i(castsi512Si128([64]byte(a)))
}

func castsi512Si128(a [64]byte) [16]byte


// Castsi512Si256: Cast vector of type __m512i to type __m256i.
// 	 This intrinsic is only used for compilation and does not generate any
// instructions, thus it has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_castsi512_si256'.
// Requires AVX512F.
func Castsi512Si256(a M512i) M256i {
	return M256i(castsi512Si256([64]byte(a)))
}

func castsi512Si256(a [64]byte) [32]byte


// CbrtPd: Compute the cube root of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := CubeRoot(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_cbrt_pd'.
// Requires AVX512F.
func CbrtPd(a M512d) M512d {
	return M512d(cbrtPd([8]float64(a)))
}

func cbrtPd(a [8]float64) [8]float64


// MaskCbrtPd: Compute the cube root of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := CubeRoot(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_cbrt_pd'.
// Requires AVX512F.
func MaskCbrtPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskCbrtPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskCbrtPd(src [8]float64, k uint8, a [8]float64) [8]float64


// CbrtPs: Compute the cube root of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := CubeRoot(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_cbrt_ps'.
// Requires AVX512F.
func CbrtPs(a M512) M512 {
	return M512(cbrtPs([16]float32(a)))
}

func cbrtPs(a [16]float32) [16]float32


// MaskCbrtPs: Compute the cube root of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := CubeRoot(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_cbrt_ps'.
// Requires AVX512F.
func MaskCbrtPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskCbrtPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskCbrtPs(src [16]float32, k uint16, a [16]float32) [16]float32


// CdfnormPd: Compute the cumulative distribution function of packed
// double-precision (64-bit) floating-point elements in 'a' using the normal
// distribution, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := CDFNormal(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_cdfnorm_pd'.
// Requires AVX512F.
func CdfnormPd(a M512d) M512d {
	return M512d(cdfnormPd([8]float64(a)))
}

func cdfnormPd(a [8]float64) [8]float64


// MaskCdfnormPd: Compute the cumulative distribution function of packed
// double-precision (64-bit) floating-point elements in 'a' using the normal
// distribution, and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := CDFNormal(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_cdfnorm_pd'.
// Requires AVX512F.
func MaskCdfnormPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskCdfnormPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskCdfnormPd(src [8]float64, k uint8, a [8]float64) [8]float64


// CdfnormPs: Compute the cumulative distribution function of packed
// single-precision (32-bit) floating-point elements in 'a' using the normal
// distribution, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := CDFNormal(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_cdfnorm_ps'.
// Requires AVX512F.
func CdfnormPs(a M512) M512 {
	return M512(cdfnormPs([16]float32(a)))
}

func cdfnormPs(a [16]float32) [16]float32


// MaskCdfnormPs: Compute the cumulative distribution function of packed
// single-precision (32-bit) floating-point elements in 'a' using the normal
// distribution, and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := CDFNormal(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_cdfnorm_ps'.
// Requires AVX512F.
func MaskCdfnormPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskCdfnormPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskCdfnormPs(src [16]float32, k uint16, a [16]float32) [16]float32


// CdfnorminvPd: Compute the inverse cumulative distribution function of packed
// double-precision (64-bit) floating-point elements in 'a' using the normal
// distribution, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := InverseCDFNormal(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_cdfnorminv_pd'.
// Requires AVX512F.
func CdfnorminvPd(a M512d) M512d {
	return M512d(cdfnorminvPd([8]float64(a)))
}

func cdfnorminvPd(a [8]float64) [8]float64


// MaskCdfnorminvPd: Compute the inverse cumulative distribution function of
// packed double-precision (64-bit) floating-point elements in 'a' using the
// normal distribution, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := InverseCDFNormal(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_cdfnorminv_pd'.
// Requires AVX512F.
func MaskCdfnorminvPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskCdfnorminvPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskCdfnorminvPd(src [8]float64, k uint8, a [8]float64) [8]float64


// CdfnorminvPs: Compute the inverse cumulative distribution function of packed
// single-precision (32-bit) floating-point elements in 'a' using the normal
// distribution, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := InverseCDFNormal(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_cdfnorminv_ps'.
// Requires AVX512F.
func CdfnorminvPs(a M512) M512 {
	return M512(cdfnorminvPs([16]float32(a)))
}

func cdfnorminvPs(a [16]float32) [16]float32


// MaskCdfnorminvPs: Compute the inverse cumulative distribution function of
// packed single-precision (32-bit) floating-point elements in 'a' using the
// normal distribution, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := InverseCDFNormal(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_cdfnorminv_ps'.
// Requires AVX512F.
func MaskCdfnorminvPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskCdfnorminvPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskCdfnorminvPs(src [16]float32, k uint16, a [16]float32) [16]float32


// CeilPd: Round the packed double-precision (64-bit) floating-point elements
// in 'a' up to an integer value, and store the results as packed
// double-precision floating-point elements in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := CEIL(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_ceil_pd'.
// Requires AVX512F.
func CeilPd(a M512d) M512d {
	return M512d(ceilPd([8]float64(a)))
}

func ceilPd(a [8]float64) [8]float64


// MaskCeilPd: Round the packed double-precision (64-bit) floating-point
// elements in 'a' up to an integer value, and store the results as packed
// double-precision floating-point elements in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := CEIL(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_ceil_pd'.
// Requires AVX512F.
func MaskCeilPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskCeilPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskCeilPd(src [8]float64, k uint8, a [8]float64) [8]float64


// CeilPs: Round the packed single-precision (32-bit) floating-point elements
// in 'a' up to an integer value, and store the results as packed
// single-precision floating-point elements in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := CEIL(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_ceil_ps'.
// Requires AVX512F.
func CeilPs(a M512) M512 {
	return M512(ceilPs([16]float32(a)))
}

func ceilPs(a [16]float32) [16]float32


// MaskCeilPs: Round the packed single-precision (32-bit) floating-point
// elements in 'a' up to an integer value, and store the results as packed
// single-precision floating-point elements in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := CEIL(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_ceil_ps'.
// Requires AVX512F.
func MaskCeilPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskCeilPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskCeilPs(src [16]float32, k uint16, a [16]float32) [16]float32


// CmpEpi16Mask: Compare packed 16-bit integers in 'a' and 'b' based on the
// comparison operand specified by 'imm8', and store the results in mask vector
// 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 31
//			i := j*16
//			k[j] := ( a[i+15:i] OP b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm512_cmp_epi16_mask'.
// Requires AVX512BW.
func CmpEpi16Mask(a M512i, b M512i, imm8 int) Mmask32 {
	return Mmask32(cmpEpi16Mask([64]byte(a), [64]byte(b), imm8))
}

func cmpEpi16Mask(a [64]byte, b [64]byte, imm8 int) uint32


// MaskCmpEpi16Mask: Compare packed 16-bit integers in 'a' and 'b' based on the
// comparison operand specified by 'imm8', and store the results in mask vector
// 'k1' using zeromask 'k' (elements are zeroed out when the corresponding mask
// bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 31
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] OP b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm512_mask_cmp_epi16_mask'.
// Requires AVX512BW.
func MaskCmpEpi16Mask(k1 Mmask32, a M512i, b M512i, imm8 int) Mmask32 {
	return Mmask32(maskCmpEpi16Mask(uint32(k1), [64]byte(a), [64]byte(b), imm8))
}

func maskCmpEpi16Mask(k1 uint32, a [64]byte, b [64]byte, imm8 int) uint32


// CmpEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' based on the
// comparison operand specified by 'imm8', and store the results in mask vector
// 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 15
//			i := j*32
//			k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm512_cmp_epi32_mask'.
// Requires KNCNI.
func CmpEpi32Mask(a M512i, b M512i, imm8 uint8) Mmask16 {
	return Mmask16(cmpEpi32Mask([64]byte(a), [64]byte(b), imm8))
}

func cmpEpi32Mask(a [64]byte, b [64]byte, imm8 uint8) uint16


// MaskCmpEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' based on the
// comparison operand specified by 'imm8', and store the results in mask vector
// 'k1' using zeromask 'k' (elements are zeroed out when the corresponding mask
// bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 15
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm512_mask_cmp_epi32_mask'.
// Requires KNCNI.
func MaskCmpEpi32Mask(k1 Mmask16, a M512i, b M512i, imm8 uint8) Mmask16 {
	return Mmask16(maskCmpEpi32Mask(uint16(k1), [64]byte(a), [64]byte(b), imm8))
}

func maskCmpEpi32Mask(k1 uint16, a [64]byte, b [64]byte, imm8 uint8) uint16


// CmpEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' based on the
// comparison operand specified by 'imm8', and store the results in mask vector
// 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm512_cmp_epi64_mask'.
// Requires AVX512F.
func CmpEpi64Mask(a M512i, b M512i, imm8 uint8) Mmask8 {
	return Mmask8(cmpEpi64Mask([64]byte(a), [64]byte(b), imm8))
}

func cmpEpi64Mask(a [64]byte, b [64]byte, imm8 uint8) uint8


// MaskCmpEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' based on the
// comparison operand specified by 'imm8', and store the results in mask vector
// 'k1' using zeromask 'k' (elements are zeroed out when the corresponding mask
// bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm512_mask_cmp_epi64_mask'.
// Requires AVX512F.
func MaskCmpEpi64Mask(k1 Mmask8, a M512i, b M512i, imm8 uint8) Mmask8 {
	return Mmask8(maskCmpEpi64Mask(uint8(k1), [64]byte(a), [64]byte(b), imm8))
}

func maskCmpEpi64Mask(k1 uint8, a [64]byte, b [64]byte, imm8 uint8) uint8


// CmpEpi8Mask: Compare packed 8-bit integers in 'a' and 'b' based on the
// comparison operand specified by 'imm8', and store the results in mask vector
// 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 63
//			i := j*8
//			k[j] := ( a[i+7:i] OP b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:64] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm512_cmp_epi8_mask'.
// Requires AVX512BW.
func CmpEpi8Mask(a M512i, b M512i, imm8 int) Mmask64 {
	return Mmask64(cmpEpi8Mask([64]byte(a), [64]byte(b), imm8))
}

func cmpEpi8Mask(a [64]byte, b [64]byte, imm8 int) uint64


// MaskCmpEpi8Mask: Compare packed 8-bit integers in 'a' and 'b' based on the
// comparison operand specified by 'imm8', and store the results in mask vector
// 'k1' using zeromask 'k' (elements are zeroed out when the corresponding mask
// bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 63
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] OP b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:64] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm512_mask_cmp_epi8_mask'.
// Requires AVX512BW.
func MaskCmpEpi8Mask(k1 Mmask64, a M512i, b M512i, imm8 int) Mmask64 {
	return Mmask64(maskCmpEpi8Mask(uint64(k1), [64]byte(a), [64]byte(b), imm8))
}

func maskCmpEpi8Mask(k1 uint64, a [64]byte, b [64]byte, imm8 int) uint64


// CmpEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b' based
// on the comparison operand specified by 'imm8', and store the results in mask
// vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 31
//			i := j*16
//			k[j] := ( a[i+15:i] OP b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm512_cmp_epu16_mask'.
// Requires AVX512BW.
func CmpEpu16Mask(a M512i, b M512i, imm8 int) Mmask32 {
	return Mmask32(cmpEpu16Mask([64]byte(a), [64]byte(b), imm8))
}

func cmpEpu16Mask(a [64]byte, b [64]byte, imm8 int) uint32


// MaskCmpEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b'
// based on the comparison operand specified by 'imm8', and store the results
// in mask vector 'k1' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 31
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] OP b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm512_mask_cmp_epu16_mask'.
// Requires AVX512BW.
func MaskCmpEpu16Mask(k1 Mmask32, a M512i, b M512i, imm8 int) Mmask32 {
	return Mmask32(maskCmpEpu16Mask(uint32(k1), [64]byte(a), [64]byte(b), imm8))
}

func maskCmpEpu16Mask(k1 uint32, a [64]byte, b [64]byte, imm8 int) uint32


// CmpEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b' based
// on the comparison operand specified by 'imm8', and store the results in mask
// vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 15
//			i := j*32
//			k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm512_cmp_epu32_mask'.
// Requires KNCNI.
func CmpEpu32Mask(a M512i, b M512i, imm8 uint8) Mmask16 {
	return Mmask16(cmpEpu32Mask([64]byte(a), [64]byte(b), imm8))
}

func cmpEpu32Mask(a [64]byte, b [64]byte, imm8 uint8) uint16


// MaskCmpEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// based on the comparison operand specified by 'imm8', and store the results
// in mask vector 'k1' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 15
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm512_mask_cmp_epu32_mask'.
// Requires KNCNI.
func MaskCmpEpu32Mask(k1 Mmask16, a M512i, b M512i, imm8 uint8) Mmask16 {
	return Mmask16(maskCmpEpu32Mask(uint16(k1), [64]byte(a), [64]byte(b), imm8))
}

func maskCmpEpu32Mask(k1 uint16, a [64]byte, b [64]byte, imm8 uint8) uint16


// CmpEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b' based
// on the comparison operand specified by 'imm8', and store the results in mask
// vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm512_cmp_epu64_mask'.
// Requires AVX512F.
func CmpEpu64Mask(a M512i, b M512i, imm8 uint8) Mmask8 {
	return Mmask8(cmpEpu64Mask([64]byte(a), [64]byte(b), imm8))
}

func cmpEpu64Mask(a [64]byte, b [64]byte, imm8 uint8) uint8


// MaskCmpEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// based on the comparison operand specified by 'imm8', and store the results
// in mask vector 'k1' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm512_mask_cmp_epu64_mask'.
// Requires AVX512F.
func MaskCmpEpu64Mask(k1 Mmask8, a M512i, b M512i, imm8 uint8) Mmask8 {
	return Mmask8(maskCmpEpu64Mask(uint8(k1), [64]byte(a), [64]byte(b), imm8))
}

func maskCmpEpu64Mask(k1 uint8, a [64]byte, b [64]byte, imm8 uint8) uint8


// CmpEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' based on
// the comparison operand specified by 'imm8', and store the results in mask
// vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 63
//			i := j*8
//			k[j] := ( a[i+7:i] OP b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:64] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm512_cmp_epu8_mask'.
// Requires AVX512BW.
func CmpEpu8Mask(a M512i, b M512i, imm8 int) Mmask64 {
	return Mmask64(cmpEpu8Mask([64]byte(a), [64]byte(b), imm8))
}

func cmpEpu8Mask(a [64]byte, b [64]byte, imm8 int) uint64


// MaskCmpEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' based
// on the comparison operand specified by 'imm8', and store the results in mask
// vector 'k1' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 63
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] OP b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:64] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm512_mask_cmp_epu8_mask'.
// Requires AVX512BW.
func MaskCmpEpu8Mask(k1 Mmask64, a M512i, b M512i, imm8 int) Mmask64 {
	return Mmask64(maskCmpEpu8Mask(uint64(k1), [64]byte(a), [64]byte(b), imm8))
}

func maskCmpEpu8Mask(k1 uint64, a [64]byte, b [64]byte, imm8 int) uint64


// CmpPdMask: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b' based on the comparison operand specified by 'imm8', and
// store the results in mask vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := (a[i+63:i] OP b[i+63:i]) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VCMPPD'. Intrinsic: '_mm512_cmp_pd_mask'.
// Requires KNCNI.
func CmpPdMask(a M512d, b M512d, imm8 int) Mmask8 {
	return Mmask8(cmpPdMask([8]float64(a), [8]float64(b), imm8))
}

func cmpPdMask(a [8]float64, b [8]float64, imm8 int) uint8


// MaskCmpPdMask: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' based on the comparison operand specified by 'imm8',
// and store the results in mask vector 'k' using zeromask 'k1' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VCMPPD'. Intrinsic: '_mm512_mask_cmp_pd_mask'.
// Requires KNCNI.
func MaskCmpPdMask(k1 Mmask8, a M512d, b M512d, imm8 int) Mmask8 {
	return Mmask8(maskCmpPdMask(uint8(k1), [8]float64(a), [8]float64(b), imm8))
}

func maskCmpPdMask(k1 uint8, a [8]float64, b [8]float64, imm8 int) uint8


// CmpPsMask: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b' based on the comparison operand specified by 'imm8', and
// store the results in mask vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		FOR j := 0 to 15
//			i := j*32
//			k[j] := (a[i+31:i] OP b[i+31:i]) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VCMPPS'. Intrinsic: '_mm512_cmp_ps_mask'.
// Requires KNCNI.
func CmpPsMask(a M512, b M512, imm8 int) Mmask16 {
	return Mmask16(cmpPsMask([16]float32(a), [16]float32(b), imm8))
}

func cmpPsMask(a [16]float32, b [16]float32, imm8 int) uint16


// MaskCmpPsMask: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' based on the comparison operand specified by 'imm8',
// and store the results in mask vector 'k' using zeromask 'k1' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		FOR j := 0 to 15
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VCMPPS'. Intrinsic: '_mm512_mask_cmp_ps_mask'.
// Requires KNCNI.
func MaskCmpPsMask(k1 Mmask16, a M512, b M512, imm8 int) Mmask16 {
	return Mmask16(maskCmpPsMask(uint16(k1), [16]float32(a), [16]float32(b), imm8))
}

func maskCmpPsMask(k1 uint16, a [16]float32, b [16]float32, imm8 int) uint16


// CmpRoundPdMask: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' based on the comparison operand specified by 'imm8',
// and store the results in mask vector 'k'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := (a[i+63:i] OP b[i+63:i]) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VCMPPD'. Intrinsic: '_mm512_cmp_round_pd_mask'.
// Requires KNCNI.
func CmpRoundPdMask(a M512d, b M512d, imm8 int, sae int) Mmask8 {
	return Mmask8(cmpRoundPdMask([8]float64(a), [8]float64(b), imm8, sae))
}

func cmpRoundPdMask(a [8]float64, b [8]float64, imm8 int, sae int) uint8


// MaskCmpRoundPdMask: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' based on the comparison operand specified by 'imm8',
// and store the results in mask vector 'k' using zeromask 'k1' (elements are
// zeroed out when the corresponding mask bit is not set). 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VCMPPD'. Intrinsic: '_mm512_mask_cmp_round_pd_mask'.
// Requires KNCNI.
func MaskCmpRoundPdMask(k1 Mmask8, a M512d, b M512d, imm8 int, sae int) Mmask8 {
	return Mmask8(maskCmpRoundPdMask(uint8(k1), [8]float64(a), [8]float64(b), imm8, sae))
}

func maskCmpRoundPdMask(k1 uint8, a [8]float64, b [8]float64, imm8 int, sae int) uint8


// CmpRoundPsMask: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' based on the comparison operand specified by 'imm8',
// and store the results in mask vector 'k'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		FOR j := 0 to 15
//			i := j*32
//			k[j] := (a[i+31:i] OP b[i+31:i]) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VCMPPS'. Intrinsic: '_mm512_cmp_round_ps_mask'.
// Requires KNCNI.
func CmpRoundPsMask(a M512, b M512, imm8 int, sae int) Mmask16 {
	return Mmask16(cmpRoundPsMask([16]float32(a), [16]float32(b), imm8, sae))
}

func cmpRoundPsMask(a [16]float32, b [16]float32, imm8 int, sae int) uint16


// MaskCmpRoundPsMask: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' based on the comparison operand specified by 'imm8',
// and store the results in mask vector 'k' using zeromask 'k1' (elements are
// zeroed out when the corresponding mask bit is not set). 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		FOR j := 0 to 15
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VCMPPS'. Intrinsic: '_mm512_mask_cmp_round_ps_mask'.
// Requires KNCNI.
func MaskCmpRoundPsMask(k1 Mmask16, a M512, b M512, imm8 int, sae int) Mmask16 {
	return Mmask16(maskCmpRoundPsMask(uint16(k1), [16]float32(a), [16]float32(b), imm8, sae))
}

func maskCmpRoundPsMask(k1 uint16, a [16]float32, b [16]float32, imm8 int, sae int) uint16


// CmpeqEpi16Mask: Compare packed 16-bit integers in 'a' and 'b' for equality,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			k[j] := ( a[i+15:i] == b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm512_cmpeq_epi16_mask'.
// Requires AVX512BW.
func CmpeqEpi16Mask(a M512i, b M512i) Mmask32 {
	return Mmask32(cmpeqEpi16Mask([64]byte(a), [64]byte(b)))
}

func cmpeqEpi16Mask(a [64]byte, b [64]byte) uint32


// MaskCmpeqEpi16Mask: Compare packed 16-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] == b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm512_mask_cmpeq_epi16_mask'.
// Requires AVX512BW.
func MaskCmpeqEpi16Mask(k1 Mmask32, a M512i, b M512i) Mmask32 {
	return Mmask32(maskCmpeqEpi16Mask(uint32(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpeqEpi16Mask(k1 uint32, a [64]byte, b [64]byte) uint32


// CmpeqEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for equality,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPEQD'. Intrinsic: '_mm512_cmpeq_epi32_mask'.
// Requires KNCNI.
func CmpeqEpi32Mask(a M512i, b M512i) Mmask16 {
	return Mmask16(cmpeqEpi32Mask([64]byte(a), [64]byte(b)))
}

func cmpeqEpi32Mask(a [64]byte, b [64]byte) uint16


// MaskCmpeqEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPEQD'. Intrinsic: '_mm512_mask_cmpeq_epi32_mask'.
// Requires KNCNI.
func MaskCmpeqEpi32Mask(k1 Mmask16, a M512i, b M512i) Mmask16 {
	return Mmask16(maskCmpeqEpi32Mask(uint16(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpeqEpi32Mask(k1 uint16, a [64]byte, b [64]byte) uint16


// CmpeqEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for equality,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPEQQ'. Intrinsic: '_mm512_cmpeq_epi64_mask'.
// Requires AVX512F.
func CmpeqEpi64Mask(a M512i, b M512i) Mmask8 {
	return Mmask8(cmpeqEpi64Mask([64]byte(a), [64]byte(b)))
}

func cmpeqEpi64Mask(a [64]byte, b [64]byte) uint8


// MaskCmpeqEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPEQQ'. Intrinsic: '_mm512_mask_cmpeq_epi64_mask'.
// Requires AVX512F.
func MaskCmpeqEpi64Mask(k1 Mmask8, a M512i, b M512i) Mmask8 {
	return Mmask8(maskCmpeqEpi64Mask(uint8(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpeqEpi64Mask(k1 uint8, a [64]byte, b [64]byte) uint8


// CmpeqEpi8Mask: Compare packed 8-bit integers in 'a' and 'b' for equality,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 63
//			i := j*8
//			k[j] := ( a[i+7:i] == b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:64] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm512_cmpeq_epi8_mask'.
// Requires AVX512BW.
func CmpeqEpi8Mask(a M512i, b M512i) Mmask64 {
	return Mmask64(cmpeqEpi8Mask([64]byte(a), [64]byte(b)))
}

func cmpeqEpi8Mask(a [64]byte, b [64]byte) uint64


// MaskCmpeqEpi8Mask: Compare packed 8-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k' using zeromask 'k1'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] == b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:64] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm512_mask_cmpeq_epi8_mask'.
// Requires AVX512BW.
func MaskCmpeqEpi8Mask(k1 Mmask64, a M512i, b M512i) Mmask64 {
	return Mmask64(maskCmpeqEpi8Mask(uint64(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpeqEpi8Mask(k1 uint64, a [64]byte, b [64]byte) uint64


// CmpeqEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			k[j] := ( a[i+15:i] == b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm512_cmpeq_epu16_mask'.
// Requires AVX512BW.
func CmpeqEpu16Mask(a M512i, b M512i) Mmask32 {
	return Mmask32(cmpeqEpu16Mask([64]byte(a), [64]byte(b)))
}

func cmpeqEpu16Mask(a [64]byte, b [64]byte) uint32


// MaskCmpeqEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b'
// for equality, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] == b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm512_mask_cmpeq_epu16_mask'.
// Requires AVX512BW.
func MaskCmpeqEpu16Mask(k1 Mmask32, a M512i, b M512i) Mmask32 {
	return Mmask32(maskCmpeqEpu16Mask(uint32(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpeqEpu16Mask(k1 uint32, a [64]byte, b [64]byte) uint32


// CmpeqEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm512_cmpeq_epu32_mask'.
// Requires KNCNI.
func CmpeqEpu32Mask(a M512i, b M512i) Mmask16 {
	return Mmask16(cmpeqEpu32Mask([64]byte(a), [64]byte(b)))
}

func cmpeqEpu32Mask(a [64]byte, b [64]byte) uint16


// MaskCmpeqEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for equality, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm512_mask_cmpeq_epu32_mask'.
// Requires KNCNI.
func MaskCmpeqEpu32Mask(k1 Mmask16, a M512i, b M512i) Mmask16 {
	return Mmask16(maskCmpeqEpu32Mask(uint16(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpeqEpu32Mask(k1 uint16, a [64]byte, b [64]byte) uint16


// CmpeqEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm512_cmpeq_epu64_mask'.
// Requires AVX512F.
func CmpeqEpu64Mask(a M512i, b M512i) Mmask8 {
	return Mmask8(cmpeqEpu64Mask([64]byte(a), [64]byte(b)))
}

func cmpeqEpu64Mask(a [64]byte, b [64]byte) uint8


// MaskCmpeqEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for equality, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm512_mask_cmpeq_epu64_mask'.
// Requires AVX512F.
func MaskCmpeqEpu64Mask(k1 Mmask8, a M512i, b M512i) Mmask8 {
	return Mmask8(maskCmpeqEpu64Mask(uint8(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpeqEpu64Mask(k1 uint8, a [64]byte, b [64]byte) uint8


// CmpeqEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 63
//			i := j*8
//			k[j] := ( a[i+7:i] == b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:64] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm512_cmpeq_epu8_mask'.
// Requires AVX512BW.
func CmpeqEpu8Mask(a M512i, b M512i) Mmask64 {
	return Mmask64(cmpeqEpu8Mask([64]byte(a), [64]byte(b)))
}

func cmpeqEpu8Mask(a [64]byte, b [64]byte) uint64


// MaskCmpeqEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k' using zeromask 'k1'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] == b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:64] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm512_mask_cmpeq_epu8_mask'.
// Requires AVX512BW.
func MaskCmpeqEpu8Mask(k1 Mmask64, a M512i, b M512i) Mmask64 {
	return Mmask64(maskCmpeqEpu8Mask(uint64(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpeqEpu8Mask(k1 uint64, a [64]byte, b [64]byte) uint64


// CmpeqPdMask: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' for equality, and store the results in mask vector
// 'k'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := (a[i+63:i] == b[i+63:i]) ? 1 : 0
//		ENDFOR	
//		k[MAX:8] := 0
//
// Instruction: 'VCMPPD'. Intrinsic: '_mm512_cmpeq_pd_mask'.
// Requires KNCNI.
func CmpeqPdMask(a M512d, b M512d) Mmask8 {
	return Mmask8(cmpeqPdMask([8]float64(a), [8]float64(b)))
}

func cmpeqPdMask(a [8]float64, b [8]float64) uint8


// MaskCmpeqPdMask: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' for equality, and store the results in mask vector
// 'k' using zeromask 'k1' (elements are zeroed out when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := (a[i+63:i] == b[i+63:i]) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR	
//		k[MAX:8] := 0
//
// Instruction: 'VCMPPD'. Intrinsic: '_mm512_mask_cmpeq_pd_mask'.
// Requires KNCNI.
func MaskCmpeqPdMask(k1 Mmask8, a M512d, b M512d) Mmask8 {
	return Mmask8(maskCmpeqPdMask(uint8(k1), [8]float64(a), [8]float64(b)))
}

func maskCmpeqPdMask(k1 uint8, a [8]float64, b [8]float64) uint8


// CmpeqPsMask: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' for equality, and store the results in mask vector
// 'k'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			k[j] := (a[i+31:i] == b[i+31:i]) ? 1 : 0
//		ENDFOR	
//		k[MAX:16] := 0
//
// Instruction: 'VCMPPS'. Intrinsic: '_mm512_cmpeq_ps_mask'.
// Requires KNCNI.
func CmpeqPsMask(a M512, b M512) Mmask16 {
	return Mmask16(cmpeqPsMask([16]float32(a), [16]float32(b)))
}

func cmpeqPsMask(a [16]float32, b [16]float32) uint16


// MaskCmpeqPsMask: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' for equality, and store the results in mask vector
// 'k' using zeromask 'k1' (elements are zeroed out when the corresponding mask
// bit is not set). 
//
//		y
//		FOR j := 0 to 15
//			i := j*32
//			IF k1[j]
//				k[j] := (a[i+31:i] == b[i+31:i]) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR		
//		k[MAX:16] := 0
//
// Instruction: 'VCMPPS'. Intrinsic: '_mm512_mask_cmpeq_ps_mask'.
// Requires KNCNI.
func MaskCmpeqPsMask(k1 Mmask16, a M512, b M512) Mmask16 {
	return Mmask16(maskCmpeqPsMask(uint16(k1), [16]float32(a), [16]float32(b)))
}

func maskCmpeqPsMask(k1 uint16, a [16]float32, b [16]float32) uint16


// CmpgeEpi16Mask: Compare packed 16-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			k[j] := ( a[i+15:i] >= b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm512_cmpge_epi16_mask'.
// Requires AVX512BW.
func CmpgeEpi16Mask(a M512i, b M512i) Mmask32 {
	return Mmask32(cmpgeEpi16Mask([64]byte(a), [64]byte(b)))
}

func cmpgeEpi16Mask(a [64]byte, b [64]byte) uint32


// MaskCmpgeEpi16Mask: Compare packed 16-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] >= b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm512_mask_cmpge_epi16_mask'.
// Requires AVX512BW.
func MaskCmpgeEpi16Mask(k1 Mmask32, a M512i, b M512i) Mmask32 {
	return Mmask32(maskCmpgeEpi16Mask(uint32(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpgeEpi16Mask(k1 uint32, a [64]byte, b [64]byte) uint32


// CmpgeEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			k[j] := ( a[i+31:i] >= b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm512_cmpge_epi32_mask'.
// Requires KNCNI.
func CmpgeEpi32Mask(a M512i, b M512i) Mmask16 {
	return Mmask16(cmpgeEpi32Mask([64]byte(a), [64]byte(b)))
}

func cmpgeEpi32Mask(a [64]byte, b [64]byte) uint16


// MaskCmpgeEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] >= b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm512_mask_cmpge_epi32_mask'.
// Requires KNCNI.
func MaskCmpgeEpi32Mask(k1 Mmask16, a M512i, b M512i) Mmask16 {
	return Mmask16(maskCmpgeEpi32Mask(uint16(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpgeEpi32Mask(k1 uint16, a [64]byte, b [64]byte) uint16


// CmpgeEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ( a[i+63:i] >= b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm512_cmpge_epi64_mask'.
// Requires AVX512F.
func CmpgeEpi64Mask(a M512i, b M512i) Mmask8 {
	return Mmask8(cmpgeEpi64Mask([64]byte(a), [64]byte(b)))
}

func cmpgeEpi64Mask(a [64]byte, b [64]byte) uint8


// MaskCmpgeEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] >= b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm512_mask_cmpge_epi64_mask'.
// Requires AVX512F.
func MaskCmpgeEpi64Mask(k1 Mmask8, a M512i, b M512i) Mmask8 {
	return Mmask8(maskCmpgeEpi64Mask(uint8(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpgeEpi64Mask(k1 uint8, a [64]byte, b [64]byte) uint8


// CmpgeEpi8Mask: Compare packed 8-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 63
//			i := j*8
//			k[j] := ( a[i+7:i] >= b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:64] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm512_cmpge_epi8_mask'.
// Requires AVX512BW.
func CmpgeEpi8Mask(a M512i, b M512i) Mmask64 {
	return Mmask64(cmpgeEpi8Mask([64]byte(a), [64]byte(b)))
}

func cmpgeEpi8Mask(a [64]byte, b [64]byte) uint64


// MaskCmpgeEpi8Mask: Compare packed 8-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k' using
// zeromask 'k1' (elements are zeroed out when the corresponding mask bit is
// not set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] >= b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:64] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm512_mask_cmpge_epi8_mask'.
// Requires AVX512BW.
func MaskCmpgeEpi8Mask(k1 Mmask64, a M512i, b M512i) Mmask64 {
	return Mmask64(maskCmpgeEpi8Mask(uint64(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpgeEpi8Mask(k1 uint64, a [64]byte, b [64]byte) uint64


// CmpgeEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			k[j] := ( a[i+15:i] >= b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm512_cmpge_epu16_mask'.
// Requires AVX512BW.
func CmpgeEpu16Mask(a M512i, b M512i) Mmask32 {
	return Mmask32(cmpgeEpu16Mask([64]byte(a), [64]byte(b)))
}

func cmpgeEpu16Mask(a [64]byte, b [64]byte) uint32


// MaskCmpgeEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b'
// for greater-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] >= b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm512_mask_cmpge_epu16_mask'.
// Requires AVX512BW.
func MaskCmpgeEpu16Mask(k1 Mmask32, a M512i, b M512i) Mmask32 {
	return Mmask32(maskCmpgeEpu16Mask(uint32(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpgeEpu16Mask(k1 uint32, a [64]byte, b [64]byte) uint32


// CmpgeEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			k[j] := ( a[i+31:i] >= b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm512_cmpge_epu32_mask'.
// Requires KNCNI.
func CmpgeEpu32Mask(a M512i, b M512i) Mmask16 {
	return Mmask16(cmpgeEpu32Mask([64]byte(a), [64]byte(b)))
}

func cmpgeEpu32Mask(a [64]byte, b [64]byte) uint16


// MaskCmpgeEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for greater-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] >= b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm512_mask_cmpge_epu32_mask'.
// Requires KNCNI.
func MaskCmpgeEpu32Mask(k1 Mmask16, a M512i, b M512i) Mmask16 {
	return Mmask16(maskCmpgeEpu32Mask(uint16(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpgeEpu32Mask(k1 uint16, a [64]byte, b [64]byte) uint16


// CmpgeEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ( a[i+63:i] >= b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm512_cmpge_epu64_mask'.
// Requires AVX512F.
func CmpgeEpu64Mask(a M512i, b M512i) Mmask8 {
	return Mmask8(cmpgeEpu64Mask([64]byte(a), [64]byte(b)))
}

func cmpgeEpu64Mask(a [64]byte, b [64]byte) uint8


// MaskCmpgeEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for greater-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] >= b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm512_mask_cmpge_epu64_mask'.
// Requires AVX512F.
func MaskCmpgeEpu64Mask(k1 Mmask8, a M512i, b M512i) Mmask8 {
	return Mmask8(maskCmpgeEpu64Mask(uint8(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpgeEpu64Mask(k1 uint8, a [64]byte, b [64]byte) uint8


// CmpgeEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 63
//			i := j*8
//			k[j] := ( a[i+7:i] >= b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:64] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm512_cmpge_epu8_mask'.
// Requires AVX512BW.
func CmpgeEpu8Mask(a M512i, b M512i) Mmask64 {
	return Mmask64(cmpgeEpu8Mask([64]byte(a), [64]byte(b)))
}

func cmpgeEpu8Mask(a [64]byte, b [64]byte) uint64


// MaskCmpgeEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k' using
// zeromask 'k1' (elements are zeroed out when the corresponding mask bit is
// not set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] >= b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:64] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm512_mask_cmpge_epu8_mask'.
// Requires AVX512BW.
func MaskCmpgeEpu8Mask(k1 Mmask64, a M512i, b M512i) Mmask64 {
	return Mmask64(maskCmpgeEpu8Mask(uint64(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpgeEpu8Mask(k1 uint64, a [64]byte, b [64]byte) uint64


// CmpgtEpi16Mask: Compare packed 16-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			k[j] := ( a[i+15:i] > b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm512_cmpgt_epi16_mask'.
// Requires AVX512BW.
func CmpgtEpi16Mask(a M512i, b M512i) Mmask32 {
	return Mmask32(cmpgtEpi16Mask([64]byte(a), [64]byte(b)))
}

func cmpgtEpi16Mask(a [64]byte, b [64]byte) uint32


// MaskCmpgtEpi16Mask: Compare packed 16-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] > b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm512_mask_cmpgt_epi16_mask'.
// Requires AVX512BW.
func MaskCmpgtEpi16Mask(k1 Mmask32, a M512i, b M512i) Mmask32 {
	return Mmask32(maskCmpgtEpi16Mask(uint32(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpgtEpi16Mask(k1 uint32, a [64]byte, b [64]byte) uint32


// CmpgtEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			k[j] := ( a[i+31:i] > b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPGTD'. Intrinsic: '_mm512_cmpgt_epi32_mask'.
// Requires KNCNI.
func CmpgtEpi32Mask(a M512i, b M512i) Mmask16 {
	return Mmask16(cmpgtEpi32Mask([64]byte(a), [64]byte(b)))
}

func cmpgtEpi32Mask(a [64]byte, b [64]byte) uint16


// MaskCmpgtEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] > b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPGTD'. Intrinsic: '_mm512_mask_cmpgt_epi32_mask'.
// Requires KNCNI.
func MaskCmpgtEpi32Mask(k1 Mmask16, a M512i, b M512i) Mmask16 {
	return Mmask16(maskCmpgtEpi32Mask(uint16(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpgtEpi32Mask(k1 uint16, a [64]byte, b [64]byte) uint16


// CmpgtEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ( a[i+63:i] > b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPGTQ'. Intrinsic: '_mm512_cmpgt_epi64_mask'.
// Requires AVX512F.
func CmpgtEpi64Mask(a M512i, b M512i) Mmask8 {
	return Mmask8(cmpgtEpi64Mask([64]byte(a), [64]byte(b)))
}

func cmpgtEpi64Mask(a [64]byte, b [64]byte) uint8


// MaskCmpgtEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] > b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPGTQ'. Intrinsic: '_mm512_mask_cmpgt_epi64_mask'.
// Requires AVX512F.
func MaskCmpgtEpi64Mask(k1 Mmask8, a M512i, b M512i) Mmask8 {
	return Mmask8(maskCmpgtEpi64Mask(uint8(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpgtEpi64Mask(k1 uint8, a [64]byte, b [64]byte) uint8


// CmpgtEpi8Mask: Compare packed 8-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 63
//			i := j*8
//			k[j] := ( a[i+7:i] > b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:64] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm512_cmpgt_epi8_mask'.
// Requires AVX512BW.
func CmpgtEpi8Mask(a M512i, b M512i) Mmask64 {
	return Mmask64(cmpgtEpi8Mask([64]byte(a), [64]byte(b)))
}

func cmpgtEpi8Mask(a [64]byte, b [64]byte) uint64


// MaskCmpgtEpi8Mask: Compare packed 8-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k' using zeromask 'k1'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] > b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:64] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm512_mask_cmpgt_epi8_mask'.
// Requires AVX512BW.
func MaskCmpgtEpi8Mask(k1 Mmask64, a M512i, b M512i) Mmask64 {
	return Mmask64(maskCmpgtEpi8Mask(uint64(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpgtEpi8Mask(k1 uint64, a [64]byte, b [64]byte) uint64


// CmpgtEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			k[j] := ( a[i+15:i] > b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm512_cmpgt_epu16_mask'.
// Requires AVX512BW.
func CmpgtEpu16Mask(a M512i, b M512i) Mmask32 {
	return Mmask32(cmpgtEpu16Mask([64]byte(a), [64]byte(b)))
}

func cmpgtEpu16Mask(a [64]byte, b [64]byte) uint32


// MaskCmpgtEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b'
// for greater-than, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] > b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm512_mask_cmpgt_epu16_mask'.
// Requires AVX512BW.
func MaskCmpgtEpu16Mask(k1 Mmask32, a M512i, b M512i) Mmask32 {
	return Mmask32(maskCmpgtEpu16Mask(uint32(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpgtEpu16Mask(k1 uint32, a [64]byte, b [64]byte) uint32


// CmpgtEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			k[j] := ( a[i+31:i] > b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm512_cmpgt_epu32_mask'.
// Requires KNCNI.
func CmpgtEpu32Mask(a M512i, b M512i) Mmask16 {
	return Mmask16(cmpgtEpu32Mask([64]byte(a), [64]byte(b)))
}

func cmpgtEpu32Mask(a [64]byte, b [64]byte) uint16


// MaskCmpgtEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for greater-than, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] > b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm512_mask_cmpgt_epu32_mask'.
// Requires KNCNI.
func MaskCmpgtEpu32Mask(k1 Mmask16, a M512i, b M512i) Mmask16 {
	return Mmask16(maskCmpgtEpu32Mask(uint16(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpgtEpu32Mask(k1 uint16, a [64]byte, b [64]byte) uint16


// CmpgtEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ( a[i+63:i] > b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm512_cmpgt_epu64_mask'.
// Requires AVX512F.
func CmpgtEpu64Mask(a M512i, b M512i) Mmask8 {
	return Mmask8(cmpgtEpu64Mask([64]byte(a), [64]byte(b)))
}

func cmpgtEpu64Mask(a [64]byte, b [64]byte) uint8


// MaskCmpgtEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for greater-than, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] > b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm512_mask_cmpgt_epu64_mask'.
// Requires AVX512F.
func MaskCmpgtEpu64Mask(k1 Mmask8, a M512i, b M512i) Mmask8 {
	return Mmask8(maskCmpgtEpu64Mask(uint8(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpgtEpu64Mask(k1 uint8, a [64]byte, b [64]byte) uint8


// CmpgtEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 63
//			i := j*8
//			k[j] := ( a[i+7:i] > b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:64] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm512_cmpgt_epu8_mask'.
// Requires AVX512BW.
func CmpgtEpu8Mask(a M512i, b M512i) Mmask64 {
	return Mmask64(cmpgtEpu8Mask([64]byte(a), [64]byte(b)))
}

func cmpgtEpu8Mask(a [64]byte, b [64]byte) uint64


// MaskCmpgtEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k' using zeromask 'k1'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] > b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:64] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm512_mask_cmpgt_epu8_mask'.
// Requires AVX512BW.
func MaskCmpgtEpu8Mask(k1 Mmask64, a M512i, b M512i) Mmask64 {
	return Mmask64(maskCmpgtEpu8Mask(uint64(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpgtEpu8Mask(k1 uint64, a [64]byte, b [64]byte) uint64


// CmpleEpi16Mask: Compare packed 16-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			k[j] := ( a[i+15:i] <= b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm512_cmple_epi16_mask'.
// Requires AVX512BW.
func CmpleEpi16Mask(a M512i, b M512i) Mmask32 {
	return Mmask32(cmpleEpi16Mask([64]byte(a), [64]byte(b)))
}

func cmpleEpi16Mask(a [64]byte, b [64]byte) uint32


// MaskCmpleEpi16Mask: Compare packed 16-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] <= b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm512_mask_cmple_epi16_mask'.
// Requires AVX512BW.
func MaskCmpleEpi16Mask(k1 Mmask32, a M512i, b M512i) Mmask32 {
	return Mmask32(maskCmpleEpi16Mask(uint32(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpleEpi16Mask(k1 uint32, a [64]byte, b [64]byte) uint32


// CmpleEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			k[j] := ( a[i+31:i] <= b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm512_cmple_epi32_mask'.
// Requires KNCNI.
func CmpleEpi32Mask(a M512i, b M512i) Mmask16 {
	return Mmask16(cmpleEpi32Mask([64]byte(a), [64]byte(b)))
}

func cmpleEpi32Mask(a [64]byte, b [64]byte) uint16


// MaskCmpleEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] < b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm512_mask_cmple_epi32_mask'.
// Requires KNCNI.
func MaskCmpleEpi32Mask(k1 Mmask16, a M512i, b M512i) Mmask16 {
	return Mmask16(maskCmpleEpi32Mask(uint16(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpleEpi32Mask(k1 uint16, a [64]byte, b [64]byte) uint16


// CmpleEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ( a[i+63:i] <= b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm512_cmple_epi64_mask'.
// Requires AVX512F.
func CmpleEpi64Mask(a M512i, b M512i) Mmask8 {
	return Mmask8(cmpleEpi64Mask([64]byte(a), [64]byte(b)))
}

func cmpleEpi64Mask(a [64]byte, b [64]byte) uint8


// MaskCmpleEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] <= b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm512_mask_cmple_epi64_mask'.
// Requires AVX512F.
func MaskCmpleEpi64Mask(k1 Mmask8, a M512i, b M512i) Mmask8 {
	return Mmask8(maskCmpleEpi64Mask(uint8(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpleEpi64Mask(k1 uint8, a [64]byte, b [64]byte) uint8


// CmpleEpi8Mask: Compare packed 8-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 63
//			i := j*8
//			k[j] := ( a[i+7:i] <= b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:64] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm512_cmple_epi8_mask'.
// Requires AVX512BW.
func CmpleEpi8Mask(a M512i, b M512i) Mmask64 {
	return Mmask64(cmpleEpi8Mask([64]byte(a), [64]byte(b)))
}

func cmpleEpi8Mask(a [64]byte, b [64]byte) uint64


// MaskCmpleEpi8Mask: Compare packed 8-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k' using zeromask
// 'k1' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] <= b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:64] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm512_mask_cmple_epi8_mask'.
// Requires AVX512BW.
func MaskCmpleEpi8Mask(k1 Mmask64, a M512i, b M512i) Mmask64 {
	return Mmask64(maskCmpleEpi8Mask(uint64(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpleEpi8Mask(k1 uint64, a [64]byte, b [64]byte) uint64


// CmpleEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			k[j] := ( a[i+15:i] <= b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm512_cmple_epu16_mask'.
// Requires AVX512BW.
func CmpleEpu16Mask(a M512i, b M512i) Mmask32 {
	return Mmask32(cmpleEpu16Mask([64]byte(a), [64]byte(b)))
}

func cmpleEpu16Mask(a [64]byte, b [64]byte) uint32


// MaskCmpleEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b'
// for less-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] <= b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm512_mask_cmple_epu16_mask'.
// Requires AVX512BW.
func MaskCmpleEpu16Mask(k1 Mmask32, a M512i, b M512i) Mmask32 {
	return Mmask32(maskCmpleEpu16Mask(uint32(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpleEpu16Mask(k1 uint32, a [64]byte, b [64]byte) uint32


// CmpleEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			k[j] := ( a[i+31:i] <= b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm512_cmple_epu32_mask'.
// Requires KNCNI.
func CmpleEpu32Mask(a M512i, b M512i) Mmask16 {
	return Mmask16(cmpleEpu32Mask([64]byte(a), [64]byte(b)))
}

func cmpleEpu32Mask(a [64]byte, b [64]byte) uint16


// MaskCmpleEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for less-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] < b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm512_mask_cmple_epu32_mask'.
// Requires KNCNI.
func MaskCmpleEpu32Mask(k1 Mmask16, a M512i, b M512i) Mmask16 {
	return Mmask16(maskCmpleEpu32Mask(uint16(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpleEpu32Mask(k1 uint16, a [64]byte, b [64]byte) uint16


// CmpleEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ( a[i+63:i] <= b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm512_cmple_epu64_mask'.
// Requires AVX512F.
func CmpleEpu64Mask(a M512i, b M512i) Mmask8 {
	return Mmask8(cmpleEpu64Mask([64]byte(a), [64]byte(b)))
}

func cmpleEpu64Mask(a [64]byte, b [64]byte) uint8


// MaskCmpleEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for less-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] <= b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm512_mask_cmple_epu64_mask'.
// Requires AVX512F.
func MaskCmpleEpu64Mask(k1 Mmask8, a M512i, b M512i) Mmask8 {
	return Mmask8(maskCmpleEpu64Mask(uint8(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpleEpu64Mask(k1 uint8, a [64]byte, b [64]byte) uint8


// CmpleEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 63
//			i := j*8
//			k[j] := ( a[i+7:i] <= b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:64] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm512_cmple_epu8_mask'.
// Requires AVX512BW.
func CmpleEpu8Mask(a M512i, b M512i) Mmask64 {
	return Mmask64(cmpleEpu8Mask([64]byte(a), [64]byte(b)))
}

func cmpleEpu8Mask(a [64]byte, b [64]byte) uint64


// MaskCmpleEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k' using zeromask
// 'k1' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] <= b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:64] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm512_mask_cmple_epu8_mask'.
// Requires AVX512BW.
func MaskCmpleEpu8Mask(k1 Mmask64, a M512i, b M512i) Mmask64 {
	return Mmask64(maskCmpleEpu8Mask(uint64(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpleEpu8Mask(k1 uint64, a [64]byte, b [64]byte) uint64


// CmplePdMask: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' for less-than-or-equal, and store the results in
// mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := (a[i+63:i] <= b[i+63:i]) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VCMPPD'. Intrinsic: '_mm512_cmple_pd_mask'.
// Requires KNCNI.
func CmplePdMask(a M512d, b M512d) Mmask8 {
	return Mmask8(cmplePdMask([8]float64(a), [8]float64(b)))
}

func cmplePdMask(a [8]float64, b [8]float64) uint8


// MaskCmplePdMask: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' for less-than-or-equal, and store the results in
// mask vector 'k' using zeromask 'k1' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := (a[i+63:i] <= b[i+63:i]) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VCMPPD'. Intrinsic: '_mm512_mask_cmple_pd_mask'.
// Requires KNCNI.
func MaskCmplePdMask(k1 Mmask8, a M512d, b M512d) Mmask8 {
	return Mmask8(maskCmplePdMask(uint8(k1), [8]float64(a), [8]float64(b)))
}

func maskCmplePdMask(k1 uint8, a [8]float64, b [8]float64) uint8


// CmplePsMask: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' for less-than-or-equal, and store the results in
// mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			k[j] := (a[i+31:i] <= b[i+31:i]) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VCMPPS'. Intrinsic: '_mm512_cmple_ps_mask'.
// Requires KNCNI.
func CmplePsMask(a M512, b M512) Mmask16 {
	return Mmask16(cmplePsMask([16]float32(a), [16]float32(b)))
}

func cmplePsMask(a [16]float32, b [16]float32) uint16


// MaskCmplePsMask: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' for less-than-or-equal, and store the results in
// mask vector 'k' using zeromask 'k1' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k1[j]
//				k[j] := (a[i+31:i] <= b[i+31:i]) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VCMPPS'. Intrinsic: '_mm512_mask_cmple_ps_mask'.
// Requires KNCNI.
func MaskCmplePsMask(k1 Mmask16, a M512, b M512) Mmask16 {
	return Mmask16(maskCmplePsMask(uint16(k1), [16]float32(a), [16]float32(b)))
}

func maskCmplePsMask(k1 uint16, a [16]float32, b [16]float32) uint16


// CmpltEpi16Mask: Compare packed 16-bit integers in 'a' and 'b' for less-than,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			k[j] := ( a[i+15:i] < b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm512_cmplt_epi16_mask'.
// Requires AVX512BW.
func CmpltEpi16Mask(a M512i, b M512i) Mmask32 {
	return Mmask32(cmpltEpi16Mask([64]byte(a), [64]byte(b)))
}

func cmpltEpi16Mask(a [64]byte, b [64]byte) uint32


// MaskCmpltEpi16Mask: Compare packed 16-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] < b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm512_mask_cmplt_epi16_mask'.
// Requires AVX512BW.
func MaskCmpltEpi16Mask(k1 Mmask32, a M512i, b M512i) Mmask32 {
	return Mmask32(maskCmpltEpi16Mask(uint32(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpltEpi16Mask(k1 uint32, a [64]byte, b [64]byte) uint32


// CmpltEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for less-than,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			k[j] := ( a[i+31:i] < b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPLTD'. Intrinsic: '_mm512_cmplt_epi32_mask'.
// Requires KNCNI.
func CmpltEpi32Mask(a M512i, b M512i) Mmask16 {
	return Mmask16(cmpltEpi32Mask([64]byte(a), [64]byte(b)))
}

func cmpltEpi32Mask(a [64]byte, b [64]byte) uint16


// CmpltEpi32Mask1: Compare packed 32-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			k[j] := ( a[i+31:i] < b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm512_cmplt_epi32_mask'.
// Requires AVX512F.
func CmpltEpi32Mask1(a M512i, b M512i) Mmask16 {
	return Mmask16(cmpltEpi32Mask1([64]byte(a), [64]byte(b)))
}

func cmpltEpi32Mask1(a [64]byte, b [64]byte) uint16


// MaskCmpltEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] <= b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm512_mask_cmplt_epi32_mask'.
// Requires AVX512F.
func MaskCmpltEpi32Mask(k1 Mmask16, a M512i, b M512i) Mmask16 {
	return Mmask16(maskCmpltEpi32Mask(uint16(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpltEpi32Mask(k1 uint16, a [64]byte, b [64]byte) uint16


// MaskCmpltEpi32Mask1: Compare packed 32-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] <= b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPLTD'. Intrinsic: '_mm512_mask_cmplt_epi32_mask'.
// Requires KNCNI.
func MaskCmpltEpi32Mask1(k1 Mmask16, a M512i, b M512i) Mmask16 {
	return Mmask16(maskCmpltEpi32Mask1(uint16(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpltEpi32Mask1(k1 uint16, a [64]byte, b [64]byte) uint16


// CmpltEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for less-than,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ( a[i+63:i] < b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm512_cmplt_epi64_mask'.
// Requires AVX512F.
func CmpltEpi64Mask(a M512i, b M512i) Mmask8 {
	return Mmask8(cmpltEpi64Mask([64]byte(a), [64]byte(b)))
}

func cmpltEpi64Mask(a [64]byte, b [64]byte) uint8


// MaskCmpltEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] < b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm512_mask_cmplt_epi64_mask'.
// Requires AVX512F.
func MaskCmpltEpi64Mask(k1 Mmask8, a M512i, b M512i) Mmask8 {
	return Mmask8(maskCmpltEpi64Mask(uint8(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpltEpi64Mask(k1 uint8, a [64]byte, b [64]byte) uint8


// CmpltEpi8Mask: Compare packed 8-bit integers in 'a' and 'b' for less-than,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 63
//			i := j*8
//			k[j] := ( a[i+7:i] < b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:64] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm512_cmplt_epi8_mask'.
// Requires AVX512BW.
func CmpltEpi8Mask(a M512i, b M512i) Mmask64 {
	return Mmask64(cmpltEpi8Mask([64]byte(a), [64]byte(b)))
}

func cmpltEpi8Mask(a [64]byte, b [64]byte) uint64


// MaskCmpltEpi8Mask: Compare packed 8-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k' using zeromask 'k1'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] < b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:64] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm512_mask_cmplt_epi8_mask'.
// Requires AVX512BW.
func MaskCmpltEpi8Mask(k1 Mmask64, a M512i, b M512i) Mmask64 {
	return Mmask64(maskCmpltEpi8Mask(uint64(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpltEpi8Mask(k1 uint64, a [64]byte, b [64]byte) uint64


// CmpltEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			k[j] := ( a[i+15:i] < b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm512_cmplt_epu16_mask'.
// Requires AVX512BW.
func CmpltEpu16Mask(a M512i, b M512i) Mmask32 {
	return Mmask32(cmpltEpu16Mask([64]byte(a), [64]byte(b)))
}

func cmpltEpu16Mask(a [64]byte, b [64]byte) uint32


// MaskCmpltEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b'
// for less-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] < b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm512_mask_cmplt_epu16_mask'.
// Requires AVX512BW.
func MaskCmpltEpu16Mask(k1 Mmask32, a M512i, b M512i) Mmask32 {
	return Mmask32(maskCmpltEpu16Mask(uint32(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpltEpu16Mask(k1 uint32, a [64]byte, b [64]byte) uint32


// CmpltEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			k[j] := ( a[i+31:i] < b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm512_cmplt_epu32_mask'.
// Requires KNCNI.
func CmpltEpu32Mask(a M512i, b M512i) Mmask16 {
	return Mmask16(cmpltEpu32Mask([64]byte(a), [64]byte(b)))
}

func cmpltEpu32Mask(a [64]byte, b [64]byte) uint16


// MaskCmpltEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for less-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] <= b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm512_mask_cmplt_epu32_mask'.
// Requires KNCNI.
func MaskCmpltEpu32Mask(k1 Mmask16, a M512i, b M512i) Mmask16 {
	return Mmask16(maskCmpltEpu32Mask(uint16(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpltEpu32Mask(k1 uint16, a [64]byte, b [64]byte) uint16


// CmpltEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ( a[i+63:i] < b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm512_cmplt_epu64_mask'.
// Requires AVX512F.
func CmpltEpu64Mask(a M512i, b M512i) Mmask8 {
	return Mmask8(cmpltEpu64Mask([64]byte(a), [64]byte(b)))
}

func cmpltEpu64Mask(a [64]byte, b [64]byte) uint8


// MaskCmpltEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for less-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] < b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm512_mask_cmplt_epu64_mask'.
// Requires AVX512F.
func MaskCmpltEpu64Mask(k1 Mmask8, a M512i, b M512i) Mmask8 {
	return Mmask8(maskCmpltEpu64Mask(uint8(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpltEpu64Mask(k1 uint8, a [64]byte, b [64]byte) uint8


// CmpltEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 63
//			i := j*8
//			k[j] := ( a[i+7:i] < b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:64] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm512_cmplt_epu8_mask'.
// Requires AVX512BW.
func CmpltEpu8Mask(a M512i, b M512i) Mmask64 {
	return Mmask64(cmpltEpu8Mask([64]byte(a), [64]byte(b)))
}

func cmpltEpu8Mask(a [64]byte, b [64]byte) uint64


// MaskCmpltEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k' using zeromask 'k1'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] < b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:64] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm512_mask_cmplt_epu8_mask'.
// Requires AVX512BW.
func MaskCmpltEpu8Mask(k1 Mmask64, a M512i, b M512i) Mmask64 {
	return Mmask64(maskCmpltEpu8Mask(uint64(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpltEpu8Mask(k1 uint64, a [64]byte, b [64]byte) uint64


// CmpltPdMask: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' for less-than, and store the results in mask vector
// 'k'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := (a[i+63:i] < b[i+63:i]) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VCMPPD'. Intrinsic: '_mm512_cmplt_pd_mask'.
// Requires KNCNI.
func CmpltPdMask(a M512d, b M512d) Mmask8 {
	return Mmask8(cmpltPdMask([8]float64(a), [8]float64(b)))
}

func cmpltPdMask(a [8]float64, b [8]float64) uint8


// MaskCmpltPdMask: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' for less-than, and store the results in mask vector
// 'k' using zeromask 'k1' (elements are zeroed out when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := (a[i+63:i] < b[i+63:i]) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VCMPPD'. Intrinsic: '_mm512_mask_cmplt_pd_mask'.
// Requires KNCNI.
func MaskCmpltPdMask(k1 Mmask8, a M512d, b M512d) Mmask8 {
	return Mmask8(maskCmpltPdMask(uint8(k1), [8]float64(a), [8]float64(b)))
}

func maskCmpltPdMask(k1 uint8, a [8]float64, b [8]float64) uint8


// CmpltPsMask: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' for less-than, and store the results in mask vector
// 'k'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			k[j] := (a[i+31:i] < b[i+31:i]) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VCMPPS'. Intrinsic: '_mm512_cmplt_ps_mask'.
// Requires KNCNI.
func CmpltPsMask(a M512, b M512) Mmask16 {
	return Mmask16(cmpltPsMask([16]float32(a), [16]float32(b)))
}

func cmpltPsMask(a [16]float32, b [16]float32) uint16


// MaskCmpltPsMask: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' for less-than, and store the results in mask vector
// 'k' using zeromask 'k1' (elements are zeroed out when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k1[j]
//				k[j] := (a[i+31:i] < b[i+31:i]) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VCMPPS'. Intrinsic: '_mm512_mask_cmplt_ps_mask'.
// Requires KNCNI.
func MaskCmpltPsMask(k1 Mmask16, a M512, b M512) Mmask16 {
	return Mmask16(maskCmpltPsMask(uint16(k1), [16]float32(a), [16]float32(b)))
}

func maskCmpltPsMask(k1 uint16, a [16]float32, b [16]float32) uint16


// CmpneqEpi16Mask: Compare packed 16-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			k[j] := ( a[i+15:i] != b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm512_cmpneq_epi16_mask'.
// Requires AVX512BW.
func CmpneqEpi16Mask(a M512i, b M512i) Mmask32 {
	return Mmask32(cmpneqEpi16Mask([64]byte(a), [64]byte(b)))
}

func cmpneqEpi16Mask(a [64]byte, b [64]byte) uint32


// MaskCmpneqEpi16Mask: Compare packed 16-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] != b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm512_mask_cmpneq_epi16_mask'.
// Requires AVX512BW.
func MaskCmpneqEpi16Mask(k1 Mmask32, a M512i, b M512i) Mmask32 {
	return Mmask32(maskCmpneqEpi16Mask(uint32(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpneqEpi16Mask(k1 uint32, a [64]byte, b [64]byte) uint32


// CmpneqEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm512_cmpneq_epi32_mask'.
// Requires KNCNI.
func CmpneqEpi32Mask(a M512i, b M512i) Mmask16 {
	return Mmask16(cmpneqEpi32Mask([64]byte(a), [64]byte(b)))
}

func cmpneqEpi32Mask(a [64]byte, b [64]byte) uint16


// MaskCmpneqEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm512_mask_cmpneq_epi32_mask'.
// Requires KNCNI.
func MaskCmpneqEpi32Mask(k1 Mmask16, a M512i, b M512i) Mmask16 {
	return Mmask16(maskCmpneqEpi32Mask(uint16(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpneqEpi32Mask(k1 uint16, a [64]byte, b [64]byte) uint16


// CmpneqEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm512_cmpneq_epi64_mask'.
// Requires AVX512F.
func CmpneqEpi64Mask(a M512i, b M512i) Mmask8 {
	return Mmask8(cmpneqEpi64Mask([64]byte(a), [64]byte(b)))
}

func cmpneqEpi64Mask(a [64]byte, b [64]byte) uint8


// MaskCmpneqEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm512_mask_cmpneq_epi64_mask'.
// Requires AVX512F.
func MaskCmpneqEpi64Mask(k1 Mmask8, a M512i, b M512i) Mmask8 {
	return Mmask8(maskCmpneqEpi64Mask(uint8(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpneqEpi64Mask(k1 uint8, a [64]byte, b [64]byte) uint8


// CmpneqEpi8Mask: Compare packed 8-bit integers in 'a' and 'b' for not-equal,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 63
//			i := j*8
//			k[j] := ( a[i+7:i] != b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:64] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm512_cmpneq_epi8_mask'.
// Requires AVX512BW.
func CmpneqEpi8Mask(a M512i, b M512i) Mmask64 {
	return Mmask64(cmpneqEpi8Mask([64]byte(a), [64]byte(b)))
}

func cmpneqEpi8Mask(a [64]byte, b [64]byte) uint64


// MaskCmpneqEpi8Mask: Compare packed 8-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k' using zeromask 'k1'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] != b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:64] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm512_mask_cmpneq_epi8_mask'.
// Requires AVX512BW.
func MaskCmpneqEpi8Mask(k1 Mmask64, a M512i, b M512i) Mmask64 {
	return Mmask64(maskCmpneqEpi8Mask(uint64(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpneqEpi8Mask(k1 uint64, a [64]byte, b [64]byte) uint64


// CmpneqEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			k[j] := ( a[i+15:i] != b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm512_cmpneq_epu16_mask'.
// Requires AVX512BW.
func CmpneqEpu16Mask(a M512i, b M512i) Mmask32 {
	return Mmask32(cmpneqEpu16Mask([64]byte(a), [64]byte(b)))
}

func cmpneqEpu16Mask(a [64]byte, b [64]byte) uint32


// MaskCmpneqEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b'
// for not-equal, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] != b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm512_mask_cmpneq_epu16_mask'.
// Requires AVX512BW.
func MaskCmpneqEpu16Mask(k1 Mmask32, a M512i, b M512i) Mmask32 {
	return Mmask32(maskCmpneqEpu16Mask(uint32(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpneqEpu16Mask(k1 uint32, a [64]byte, b [64]byte) uint32


// CmpneqEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm512_cmpneq_epu32_mask'.
// Requires KNCNI.
func CmpneqEpu32Mask(a M512i, b M512i) Mmask16 {
	return Mmask16(cmpneqEpu32Mask([64]byte(a), [64]byte(b)))
}

func cmpneqEpu32Mask(a [64]byte, b [64]byte) uint16


// MaskCmpneqEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for not-equal, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm512_mask_cmpneq_epu32_mask'.
// Requires KNCNI.
func MaskCmpneqEpu32Mask(k1 Mmask16, a M512i, b M512i) Mmask16 {
	return Mmask16(maskCmpneqEpu32Mask(uint16(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpneqEpu32Mask(k1 uint16, a [64]byte, b [64]byte) uint16


// CmpneqEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm512_cmpneq_epu64_mask'.
// Requires AVX512F.
func CmpneqEpu64Mask(a M512i, b M512i) Mmask8 {
	return Mmask8(cmpneqEpu64Mask([64]byte(a), [64]byte(b)))
}

func cmpneqEpu64Mask(a [64]byte, b [64]byte) uint8


// MaskCmpneqEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for not-equal, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm512_mask_cmpneq_epu64_mask'.
// Requires AVX512F.
func MaskCmpneqEpu64Mask(k1 Mmask8, a M512i, b M512i) Mmask8 {
	return Mmask8(maskCmpneqEpu64Mask(uint8(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpneqEpu64Mask(k1 uint8, a [64]byte, b [64]byte) uint8


// CmpneqEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 63
//			i := j*8
//			k[j] := ( a[i+7:i] != b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:64] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm512_cmpneq_epu8_mask'.
// Requires AVX512BW.
func CmpneqEpu8Mask(a M512i, b M512i) Mmask64 {
	return Mmask64(cmpneqEpu8Mask([64]byte(a), [64]byte(b)))
}

func cmpneqEpu8Mask(a [64]byte, b [64]byte) uint64


// MaskCmpneqEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b'
// for not-equal, and store the results in mask vector 'k' using zeromask 'k1'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] != b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:64] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm512_mask_cmpneq_epu8_mask'.
// Requires AVX512BW.
func MaskCmpneqEpu8Mask(k1 Mmask64, a M512i, b M512i) Mmask64 {
	return Mmask64(maskCmpneqEpu8Mask(uint64(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpneqEpu8Mask(k1 uint64, a [64]byte, b [64]byte) uint64


// CmpneqPdMask: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' for not-equal, and store the results in mask vector
// 'k'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := (a[i+63:i] != b[i+63:i]) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VCMPPD'. Intrinsic: '_mm512_cmpneq_pd_mask'.
// Requires KNCNI.
func CmpneqPdMask(a M512d, b M512d) Mmask8 {
	return Mmask8(cmpneqPdMask([8]float64(a), [8]float64(b)))
}

func cmpneqPdMask(a [8]float64, b [8]float64) uint8


// MaskCmpneqPdMask: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' for not-equal, and store the results in mask vector
// 'k' using zeromask 'k1' (elements are zeroed out when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := (a[i+63:i] != b[i+63:i]) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VCMPPD'. Intrinsic: '_mm512_mask_cmpneq_pd_mask'.
// Requires KNCNI.
func MaskCmpneqPdMask(k1 Mmask8, a M512d, b M512d) Mmask8 {
	return Mmask8(maskCmpneqPdMask(uint8(k1), [8]float64(a), [8]float64(b)))
}

func maskCmpneqPdMask(k1 uint8, a [8]float64, b [8]float64) uint8


// CmpneqPsMask: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' for not-equal, and store the results in mask vector
// 'k'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			k[j] := (a[i+31:i] != b[i+31:i]) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VCMPPS'. Intrinsic: '_mm512_cmpneq_ps_mask'.
// Requires KNCNI.
func CmpneqPsMask(a M512, b M512) Mmask16 {
	return Mmask16(cmpneqPsMask([16]float32(a), [16]float32(b)))
}

func cmpneqPsMask(a [16]float32, b [16]float32) uint16


// MaskCmpneqPsMask: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' for not-equal, and store the results in mask vector
// 'k' using zeromask 'k1' (elements are zeroed out when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k1[j]
//				k[j] := (a[i+31:i] != b[i+31:i]) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VCMPPS'. Intrinsic: '_mm512_mask_cmpneq_ps_mask'.
// Requires KNCNI.
func MaskCmpneqPsMask(k1 Mmask16, a M512, b M512) Mmask16 {
	return Mmask16(maskCmpneqPsMask(uint16(k1), [16]float32(a), [16]float32(b)))
}

func maskCmpneqPsMask(k1 uint16, a [16]float32, b [16]float32) uint16


// CmpnlePdMask: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' for not-less-than-or-equal, and store the results in
// mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := !(a[i+63:i] <= b[i+63:i]) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VCMPPD'. Intrinsic: '_mm512_cmpnle_pd_mask'.
// Requires KNCNI.
func CmpnlePdMask(a M512d, b M512d) Mmask8 {
	return Mmask8(cmpnlePdMask([8]float64(a), [8]float64(b)))
}

func cmpnlePdMask(a [8]float64, b [8]float64) uint8


// MaskCmpnlePdMask: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' for not-less-than-or-equal, and store the results in
// mask vector 'k' using zeromask 'k1' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := !(a[i+63:i] <= b[i+63:i]) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VCMPPD'. Intrinsic: '_mm512_mask_cmpnle_pd_mask'.
// Requires KNCNI.
func MaskCmpnlePdMask(k1 Mmask8, a M512d, b M512d) Mmask8 {
	return Mmask8(maskCmpnlePdMask(uint8(k1), [8]float64(a), [8]float64(b)))
}

func maskCmpnlePdMask(k1 uint8, a [8]float64, b [8]float64) uint8


// CmpnlePsMask: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' for not-less-than-or-equal, and store the results in
// mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			k[j] := !(a[i+31:i] <= b[i+31:i]) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VCMPPS'. Intrinsic: '_mm512_cmpnle_ps_mask'.
// Requires KNCNI.
func CmpnlePsMask(a M512, b M512) Mmask16 {
	return Mmask16(cmpnlePsMask([16]float32(a), [16]float32(b)))
}

func cmpnlePsMask(a [16]float32, b [16]float32) uint16


// MaskCmpnlePsMask: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' for not-less-than-or-equal, and store the results in
// mask vector 'k' using zeromask 'k1' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k1[j]
//				k[j] := !(a[i+31:i] <= b[i+31:i]) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VCMPPS'. Intrinsic: '_mm512_mask_cmpnle_ps_mask'.
// Requires KNCNI.
func MaskCmpnlePsMask(k1 Mmask16, a M512, b M512) Mmask16 {
	return Mmask16(maskCmpnlePsMask(uint16(k1), [16]float32(a), [16]float32(b)))
}

func maskCmpnlePsMask(k1 uint16, a [16]float32, b [16]float32) uint16


// CmpnltPdMask: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' for not-less-than, and store the results in mask
// vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := !(a[i+63:i] < b[i+63:i]) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VCMPPD'. Intrinsic: '_mm512_cmpnlt_pd_mask'.
// Requires KNCNI.
func CmpnltPdMask(a M512d, b M512d) Mmask8 {
	return Mmask8(cmpnltPdMask([8]float64(a), [8]float64(b)))
}

func cmpnltPdMask(a [8]float64, b [8]float64) uint8


// MaskCmpnltPdMask: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' for not-less-than, and store the results in mask
// vector 'k' using zeromask 'k1' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := !(a[i+63:i] < b[i+63:i]) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VCMPPD'. Intrinsic: '_mm512_mask_cmpnlt_pd_mask'.
// Requires KNCNI.
func MaskCmpnltPdMask(k1 Mmask8, a M512d, b M512d) Mmask8 {
	return Mmask8(maskCmpnltPdMask(uint8(k1), [8]float64(a), [8]float64(b)))
}

func maskCmpnltPdMask(k1 uint8, a [8]float64, b [8]float64) uint8


// CmpnltPsMask: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' for not-less-than, and store the results in mask
// vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			k[j] := !(a[i+31:i] < b[i+31:i]) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VCMPPS'. Intrinsic: '_mm512_cmpnlt_ps_mask'.
// Requires KNCNI.
func CmpnltPsMask(a M512, b M512) Mmask16 {
	return Mmask16(cmpnltPsMask([16]float32(a), [16]float32(b)))
}

func cmpnltPsMask(a [16]float32, b [16]float32) uint16


// MaskCmpnltPsMask: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' for not-less-than, and store the results in mask
// vector 'k' using zeromask 'k1' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k1[j]
//				k[j] := !(a[i+31:i] < b[i+31:i]) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VCMPPS'. Intrinsic: '_mm512_mask_cmpnlt_ps_mask'.
// Requires KNCNI.
func MaskCmpnltPsMask(k1 Mmask16, a M512, b M512) Mmask16 {
	return Mmask16(maskCmpnltPsMask(uint16(k1), [16]float32(a), [16]float32(b)))
}

func maskCmpnltPsMask(k1 uint16, a [16]float32, b [16]float32) uint16


// CmpordPdMask: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' to see if neither is NaN, and store the results in
// mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := (a[i+63:i] != NaN AND b[i+63:i] != NaN) ? 1 : 0 
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VCMPPD'. Intrinsic: '_mm512_cmpord_pd_mask'.
// Requires KNCNI.
func CmpordPdMask(a M512d, b M512d) Mmask8 {
	return Mmask8(cmpordPdMask([8]float64(a), [8]float64(b)))
}

func cmpordPdMask(a [8]float64, b [8]float64) uint8


// MaskCmpordPdMask: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' to see if neither is NaN, and store the results in
// mask vector 'k' using zeromask 'k1' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := (a[i+63:i] != NaN AND b[i+63:i] != NaN) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VCMPPD'. Intrinsic: '_mm512_mask_cmpord_pd_mask'.
// Requires KNCNI.
func MaskCmpordPdMask(k1 Mmask8, a M512d, b M512d) Mmask8 {
	return Mmask8(maskCmpordPdMask(uint8(k1), [8]float64(a), [8]float64(b)))
}

func maskCmpordPdMask(k1 uint8, a [8]float64, b [8]float64) uint8


// CmpordPsMask: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' to see if neither is NaN, and store the results in
// mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			k[j] := (a[i+31:i] != NaN AND b[i+31:i] != NaN) ? 1 : 0 
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VCMPPS'. Intrinsic: '_mm512_cmpord_ps_mask'.
// Requires KNCNI.
func CmpordPsMask(a M512, b M512) Mmask16 {
	return Mmask16(cmpordPsMask([16]float32(a), [16]float32(b)))
}

func cmpordPsMask(a [16]float32, b [16]float32) uint16


// MaskCmpordPsMask: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' to see if neither is NaN, and store the results in
// mask vector 'k' using zeromask 'k1' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k1[j]
//				k[j] := (a[i+31:i] != NaN AND b[i+31:i] != NaN) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VCMPPS'. Intrinsic: '_mm512_mask_cmpord_ps_mask'.
// Requires KNCNI.
func MaskCmpordPsMask(k1 Mmask16, a M512, b M512) Mmask16 {
	return Mmask16(maskCmpordPsMask(uint16(k1), [16]float32(a), [16]float32(b)))
}

func maskCmpordPsMask(k1 uint16, a [16]float32, b [16]float32) uint16


// CmpunordPdMask: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' to see if either is NaN, and store the results in
// mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := (a[i+63:i] == NaN OR b[i+63:i] == NaN) ? 1 : 0 
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VCMPPD'. Intrinsic: '_mm512_cmpunord_pd_mask'.
// Requires KNCNI.
func CmpunordPdMask(a M512d, b M512d) Mmask8 {
	return Mmask8(cmpunordPdMask([8]float64(a), [8]float64(b)))
}

func cmpunordPdMask(a [8]float64, b [8]float64) uint8


// MaskCmpunordPdMask: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' to see if either is NaN, and store the results in
// mask vector 'k' using zeromask 'k1' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := (a[i+63:i] == NaN OR b[i+63:i] == NaN) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VCMPPD'. Intrinsic: '_mm512_mask_cmpunord_pd_mask'.
// Requires KNCNI.
func MaskCmpunordPdMask(k1 Mmask8, a M512d, b M512d) Mmask8 {
	return Mmask8(maskCmpunordPdMask(uint8(k1), [8]float64(a), [8]float64(b)))
}

func maskCmpunordPdMask(k1 uint8, a [8]float64, b [8]float64) uint8


// CmpunordPsMask: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' to see if either is NaN, and store the results in
// mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			k[j] := (a[i+31:i] == NaN OR b[i+31:i] == NaN) ? 1 : 0 
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VCMPPS'. Intrinsic: '_mm512_cmpunord_ps_mask'.
// Requires KNCNI.
func CmpunordPsMask(a M512, b M512) Mmask16 {
	return Mmask16(cmpunordPsMask([16]float32(a), [16]float32(b)))
}

func cmpunordPsMask(a [16]float32, b [16]float32) uint16


// MaskCmpunordPsMask: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' to see if either is NaN, and store the results in
// mask vector 'k' using zeromask 'k1' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k1[j]
//				k[j] := (a[i+31:i] == NaN OR b[i+31:i] == NaN) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VCMPPS'. Intrinsic: '_mm512_mask_cmpunord_ps_mask'.
// Requires KNCNI.
func MaskCmpunordPsMask(k1 Mmask16, a M512, b M512) Mmask16 {
	return Mmask16(maskCmpunordPsMask(uint16(k1), [16]float32(a), [16]float32(b)))
}

func maskCmpunordPsMask(k1 uint16, a [16]float32, b [16]float32) uint16


// MaskCompressEpi32: Contiguously store the active 32-bit integers in 'a'
// (those with their respective bit set in writemask 'k') to 'dst', and pass
// through the remaining elements from 'src'. 
//
//		size := 32
//		m := 0
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[511:m] := src[511:m]
//		dst[MAX:512] := 0
//
// Instruction: 'VPCOMPRESSD'. Intrinsic: '_mm512_mask_compress_epi32'.
// Requires AVX512F.
func MaskCompressEpi32(src M512i, k Mmask16, a M512i) M512i {
	return M512i(maskCompressEpi32([64]byte(src), uint16(k), [64]byte(a)))
}

func maskCompressEpi32(src [64]byte, k uint16, a [64]byte) [64]byte


// MaskzCompressEpi32: Contiguously store the active 32-bit integers in 'a'
// (those with their respective bit set in zeromask 'k') to 'dst', and set the
// remaining elements to zero. 
//
//		size := 32
//		m := 0
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[511:m] := 0
//		dst[MAX:512] := 0
//
// Instruction: 'VPCOMPRESSD'. Intrinsic: '_mm512_maskz_compress_epi32'.
// Requires AVX512F.
func MaskzCompressEpi32(k Mmask16, a M512i) M512i {
	return M512i(maskzCompressEpi32(uint16(k), [64]byte(a)))
}

func maskzCompressEpi32(k uint16, a [64]byte) [64]byte


// MaskCompressEpi64: Contiguously store the active 64-bit integers in 'a'
// (those with their respective bit set in writemask 'k') to 'dst', and pass
// through the remaining elements from 'src'. 
//
//		size := 64
//		m := 0
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[511:m] := src[511:m]
//		dst[MAX:512] := 0
//
// Instruction: 'VPCOMPRESSQ'. Intrinsic: '_mm512_mask_compress_epi64'.
// Requires AVX512F.
func MaskCompressEpi64(src M512i, k Mmask8, a M512i) M512i {
	return M512i(maskCompressEpi64([64]byte(src), uint8(k), [64]byte(a)))
}

func maskCompressEpi64(src [64]byte, k uint8, a [64]byte) [64]byte


// MaskzCompressEpi64: Contiguously store the active 64-bit integers in 'a'
// (those with their respective bit set in zeromask 'k') to 'dst', and set the
// remaining elements to zero. 
//
//		size := 64
//		m := 0
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[511:m] := 0
//		dst[MAX:512] := 0
//
// Instruction: 'VPCOMPRESSQ'. Intrinsic: '_mm512_maskz_compress_epi64'.
// Requires AVX512F.
func MaskzCompressEpi64(k Mmask8, a M512i) M512i {
	return M512i(maskzCompressEpi64(uint8(k), [64]byte(a)))
}

func maskzCompressEpi64(k uint8, a [64]byte) [64]byte


// MaskCompressPd: Contiguously store the active double-precision (64-bit)
// floating-point elements in 'a' (those with their respective bit set in
// writemask 'k') to 'dst', and pass through the remaining elements from 'src'. 
//
//		size := 64
//		m := 0
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[511:m] := src[511:m]
//		dst[MAX:512] := 0
//
// Instruction: 'VCOMPRESSPD'. Intrinsic: '_mm512_mask_compress_pd'.
// Requires AVX512F.
func MaskCompressPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskCompressPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskCompressPd(src [8]float64, k uint8, a [8]float64) [8]float64


// MaskzCompressPd: Contiguously store the active double-precision (64-bit)
// floating-point elements in 'a' (those with their respective bit set in
// zeromask 'k') to 'dst', and set the remaining elements to zero. 
//
//		size := 64
//		m := 0
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[511:m] := 0
//		dst[MAX:512] := 0
//
// Instruction: 'VCOMPRESSPD'. Intrinsic: '_mm512_maskz_compress_pd'.
// Requires AVX512F.
func MaskzCompressPd(k Mmask8, a M512d) M512d {
	return M512d(maskzCompressPd(uint8(k), [8]float64(a)))
}

func maskzCompressPd(k uint8, a [8]float64) [8]float64


// MaskCompressPs: Contiguously store the active single-precision (32-bit)
// floating-point elements in 'a' (those with their respective bit set in
// writemask 'k') to 'dst', and pass through the remaining elements from 'src'. 
//
//		size := 32
//		m := 0
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[511:m] := src[511:m]
//		dst[MAX:512] := 0
//
// Instruction: 'VCOMPRESSPS'. Intrinsic: '_mm512_mask_compress_ps'.
// Requires AVX512F.
func MaskCompressPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskCompressPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskCompressPs(src [16]float32, k uint16, a [16]float32) [16]float32


// MaskzCompressPs: Contiguously store the active single-precision (32-bit)
// floating-point elements in 'a' (those with their respective bit set in
// zeromask 'k') to 'dst', and set the remaining elements to zero. 
//
//		size := 32
//		m := 0
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[511:m] := 0
//		dst[MAX:512] := 0
//
// Instruction: 'VCOMPRESSPS'. Intrinsic: '_mm512_maskz_compress_ps'.
// Requires AVX512F.
func MaskzCompressPs(k Mmask16, a M512) M512 {
	return M512(maskzCompressPs(uint16(k), [16]float32(a)))
}

func maskzCompressPs(k uint16, a [16]float32) [16]float32


// MaskCompressstoreuEpi32: Contiguously store the active 32-bit integers in
// 'a' (those with their respective bit set in writemask 'k') to unaligned
// memory at 'base_addr'. 
//
//		size := 32
//		m := base_addr
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				MEM[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//
// Instruction: 'VPCOMPRESSD'. Intrinsic: '_mm512_mask_compressstoreu_epi32'.
// Requires AVX512F.
func MaskCompressstoreuEpi32(base_addr uintptr, k Mmask16, a M512i)  {
	maskCompressstoreuEpi32(uintptr(base_addr), uint16(k), [64]byte(a))
}

func maskCompressstoreuEpi32(base_addr uintptr, k uint16, a [64]byte) 


// MaskCompressstoreuEpi64: Contiguously store the active 64-bit integers in
// 'a' (those with their respective bit set in writemask 'k') to unaligned
// memory at 'base_addr'. 
//
//		size := 64
//		m := base_addr
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				MEM[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//
// Instruction: 'VPCOMPRESSQ'. Intrinsic: '_mm512_mask_compressstoreu_epi64'.
// Requires AVX512F.
func MaskCompressstoreuEpi64(base_addr uintptr, k Mmask8, a M512i)  {
	maskCompressstoreuEpi64(uintptr(base_addr), uint8(k), [64]byte(a))
}

func maskCompressstoreuEpi64(base_addr uintptr, k uint8, a [64]byte) 


// MaskCompressstoreuPd: Contiguously store the active double-precision
// (64-bit) floating-point elements in 'a' (those with their respective bit set
// in writemask 'k') to unaligned memory at 'base_addr'. 
//
//		size := 64
//		m := base_addr
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				MEM[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//
// Instruction: 'VCOMPRESSPD'. Intrinsic: '_mm512_mask_compressstoreu_pd'.
// Requires AVX512F.
func MaskCompressstoreuPd(base_addr uintptr, k Mmask8, a M512d)  {
	maskCompressstoreuPd(uintptr(base_addr), uint8(k), [8]float64(a))
}

func maskCompressstoreuPd(base_addr uintptr, k uint8, a [8]float64) 


// MaskCompressstoreuPs: Contiguously store the active single-precision
// (32-bit) floating-point elements in 'a' (those with their respective bit set
// in writemask 'k') to unaligned memory at 'base_addr'. 
//
//		size := 32
//		m := base_addr
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				MEM[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//
// Instruction: 'VCOMPRESSPS'. Intrinsic: '_mm512_mask_compressstoreu_ps'.
// Requires AVX512F.
func MaskCompressstoreuPs(base_addr uintptr, k Mmask16, a M512)  {
	maskCompressstoreuPs(uintptr(base_addr), uint16(k), [16]float32(a))
}

func maskCompressstoreuPs(base_addr uintptr, k uint16, a [16]float32) 


// ConflictEpi32: Test each 32-bit element of 'a' for equality with all other
// elements in 'a' closer to the least significant bit. Each element's
// comparison forms a zero extended bit vector in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			FOR k := 0 to j-1
//				m := k*32
//				dst[i+k] := (a[i+31:i] == a[m+31:m]) ? 1 : 0
//			ENDFOR
//			dst[i+31:i+j] := 0
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPCONFLICTD'. Intrinsic: '_mm512_conflict_epi32'.
// Requires AVX512CD.
func ConflictEpi32(a M512i) M512i {
	return M512i(conflictEpi32([64]byte(a)))
}

func conflictEpi32(a [64]byte) [64]byte


// MaskConflictEpi32: Test each 32-bit element of 'a' for equality with all
// other elements in 'a' closer to the least significant bit using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). Each element's comparison forms a zero extended bit vector in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[i]
//				FOR l := 0 to j-1
//					m := l*32
//					dst[i+l] := (a[i+31:i] == a[m+31:m]) ? 1 : 0
//				ENDFOR
//				dst[i+31:i+j] := 0
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPCONFLICTD'. Intrinsic: '_mm512_mask_conflict_epi32'.
// Requires AVX512CD.
func MaskConflictEpi32(src M512i, k Mmask16, a M512i) M512i {
	return M512i(maskConflictEpi32([64]byte(src), uint16(k), [64]byte(a)))
}

func maskConflictEpi32(src [64]byte, k uint16, a [64]byte) [64]byte


// MaskzConflictEpi32: Test each 32-bit element of 'a' for equality with all
// other elements in 'a' closer to the least significant bit using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). Each
// element's comparison forms a zero extended bit vector in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[i]
//				FOR l := 0 to j-1
//					m := l*32
//					dst[i+l] := (a[i+31:i] == a[m+31:m]) ? 1 : 0
//				ENDFOR
//				dst[i+31:i+j] := 0
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPCONFLICTD'. Intrinsic: '_mm512_maskz_conflict_epi32'.
// Requires AVX512CD.
func MaskzConflictEpi32(k Mmask16, a M512i) M512i {
	return M512i(maskzConflictEpi32(uint16(k), [64]byte(a)))
}

func maskzConflictEpi32(k uint16, a [64]byte) [64]byte


// ConflictEpi64: Test each 64-bit element of 'a' for equality with all other
// elements in 'a' closer to the least significant bit. Each element's
// comparison forms a zero extended bit vector in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			FOR k := 0 to j-1
//				m := k*64
//				dst[i+k] := (a[i+63:i] == a[m+63:m]) ? 1 : 0
//			ENDFOR
//			dst[i+63:i+j] := 0
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPCONFLICTQ'. Intrinsic: '_mm512_conflict_epi64'.
// Requires AVX512CD.
func ConflictEpi64(a M512i) M512i {
	return M512i(conflictEpi64([64]byte(a)))
}

func conflictEpi64(a [64]byte) [64]byte


// MaskConflictEpi64: Test each 64-bit element of 'a' for equality with all
// other elements in 'a' closer to the least significant bit using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). Each element's comparison forms a zero extended bit vector in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				FOR l := 0 to j-1
//					m := l*64
//					dst[i+l] := (a[i+63:i] == a[m+63:m]) ? 1 : 0
//				ENDFOR
//				dst[i+63:i+j] := 0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPCONFLICTQ'. Intrinsic: '_mm512_mask_conflict_epi64'.
// Requires AVX512CD.
func MaskConflictEpi64(src M512i, k Mmask8, a M512i) M512i {
	return M512i(maskConflictEpi64([64]byte(src), uint8(k), [64]byte(a)))
}

func maskConflictEpi64(src [64]byte, k uint8, a [64]byte) [64]byte


// MaskzConflictEpi64: Test each 64-bit element of 'a' for equality with all
// other elements in 'a' closer to the least significant bit using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). Each
// element's comparison forms a zero extended bit vector in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				FOR l := 0 to j-1
//					m := l*64
//					dst[i+l] := (a[i+63:i] == a[m+63:m]) ? 1 : 0
//				ENDFOR
//				dst[i+63:i+j] := 0
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPCONFLICTQ'. Intrinsic: '_mm512_maskz_conflict_epi64'.
// Requires AVX512CD.
func MaskzConflictEpi64(k Mmask8, a M512i) M512i {
	return M512i(maskzConflictEpi64(uint8(k), [64]byte(a)))
}

func maskzConflictEpi64(k uint8, a [64]byte) [64]byte


// CosPd: Compute the cosine of packed double-precision (64-bit) floating-point
// elements in 'a' expressed in radians, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := COS(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_cos_pd'.
// Requires AVX512F.
func CosPd(a M512d) M512d {
	return M512d(cosPd([8]float64(a)))
}

func cosPd(a [8]float64) [8]float64


// MaskCosPd: Compute the cosine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := COS(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_cos_pd'.
// Requires AVX512F.
func MaskCosPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskCosPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskCosPd(src [8]float64, k uint8, a [8]float64) [8]float64


// CosPs: Compute the cosine of packed single-precision (32-bit) floating-point
// elements in 'a' expressed in radians, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := COS(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_cos_ps'.
// Requires AVX512F.
func CosPs(a M512) M512 {
	return M512(cosPs([16]float32(a)))
}

func cosPs(a [16]float32) [16]float32


// MaskCosPs: Compute the cosine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := COS(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_cos_ps'.
// Requires AVX512F.
func MaskCosPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskCosPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskCosPs(src [16]float32, k uint16, a [16]float32) [16]float32


// CosdPd: Compute the cosine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in degrees, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := COSD(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_cosd_pd'.
// Requires AVX512F.
func CosdPd(a M512d) M512d {
	return M512d(cosdPd([8]float64(a)))
}

func cosdPd(a [8]float64) [8]float64


// MaskCosdPd: Compute the cosine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in degrees, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := COSD(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_cosd_pd'.
// Requires AVX512F.
func MaskCosdPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskCosdPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskCosdPd(src [8]float64, k uint8, a [8]float64) [8]float64


// CosdPs: Compute the cosine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in degrees, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := COSD(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_cosd_ps'.
// Requires AVX512F.
func CosdPs(a M512) M512 {
	return M512(cosdPs([16]float32(a)))
}

func cosdPs(a [16]float32) [16]float32


// MaskCosdPs: Compute the cosine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in degrees, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := COSD(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_cosd_ps'.
// Requires AVX512F.
func MaskCosdPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskCosdPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskCosdPs(src [16]float32, k uint16, a [16]float32) [16]float32


// CoshPd: Compute the hyperbolic cosine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := COSH(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_cosh_pd'.
// Requires AVX512F.
func CoshPd(a M512d) M512d {
	return M512d(coshPd([8]float64(a)))
}

func coshPd(a [8]float64) [8]float64


// MaskCoshPd: Compute the hyperbolic cosine of packed double-precision
// (64-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := COSH(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_cosh_pd'.
// Requires AVX512F.
func MaskCoshPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskCoshPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskCoshPd(src [8]float64, k uint8, a [8]float64) [8]float64


// CoshPs: Compute the hyperbolic cosine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := COSH(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_cosh_ps'.
// Requires AVX512F.
func CoshPs(a M512) M512 {
	return M512(coshPs([16]float32(a)))
}

func coshPs(a [16]float32) [16]float32


// MaskCoshPs: Compute the hyperbolic cosine of packed single-precision
// (32-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := COSH(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_cosh_ps'.
// Requires AVX512F.
func MaskCoshPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskCoshPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskCoshPs(src [16]float32, k uint16, a [16]float32) [16]float32


// CvtRoundepi32Ps: Convert packed 32-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTDQ2PS'. Intrinsic: '_mm512_cvt_roundepi32_ps'.
// Requires AVX512F.
func CvtRoundepi32Ps(a M512i, rounding int) M512 {
	return M512(cvtRoundepi32Ps([64]byte(a), rounding))
}

func cvtRoundepi32Ps(a [64]byte, rounding int) [16]float32


// MaskCvtRoundepi32Ps: Convert packed 32-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTDQ2PS'. Intrinsic: '_mm512_mask_cvt_roundepi32_ps'.
// Requires AVX512F.
func MaskCvtRoundepi32Ps(src M512, k Mmask16, a M512i, rounding int) M512 {
	return M512(maskCvtRoundepi32Ps([16]float32(src), uint16(k), [64]byte(a), rounding))
}

func maskCvtRoundepi32Ps(src [16]float32, k uint16, a [64]byte, rounding int) [16]float32


// MaskzCvtRoundepi32Ps: Convert packed 32-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTDQ2PS'. Intrinsic: '_mm512_maskz_cvt_roundepi32_ps'.
// Requires AVX512F.
func MaskzCvtRoundepi32Ps(k Mmask16, a M512i, rounding int) M512 {
	return M512(maskzCvtRoundepi32Ps(uint16(k), [64]byte(a), rounding))
}

func maskzCvtRoundepi32Ps(k uint16, a [64]byte, rounding int) [16]float32


// CvtRoundepi64Pd: Convert packed 64-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTQQ2PD'. Intrinsic: '_mm512_cvt_roundepi64_pd'.
// Requires AVX512DQ.
func CvtRoundepi64Pd(a M512i, rounding int) M512d {
	return M512d(cvtRoundepi64Pd([64]byte(a), rounding))
}

func cvtRoundepi64Pd(a [64]byte, rounding int) [8]float64


// MaskCvtRoundepi64Pd: Convert packed 64-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTQQ2PD'. Intrinsic: '_mm512_mask_cvt_roundepi64_pd'.
// Requires AVX512DQ.
func MaskCvtRoundepi64Pd(src M512d, k Mmask8, a M512i, rounding int) M512d {
	return M512d(maskCvtRoundepi64Pd([8]float64(src), uint8(k), [64]byte(a), rounding))
}

func maskCvtRoundepi64Pd(src [8]float64, k uint8, a [64]byte, rounding int) [8]float64


// MaskzCvtRoundepi64Pd: Convert packed 64-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTQQ2PD'. Intrinsic: '_mm512_maskz_cvt_roundepi64_pd'.
// Requires AVX512DQ.
func MaskzCvtRoundepi64Pd(k Mmask8, a M512i, rounding int) M512d {
	return M512d(maskzCvtRoundepi64Pd(uint8(k), [64]byte(a), rounding))
}

func maskzCvtRoundepi64Pd(k uint8, a [64]byte, rounding int) [8]float64


// CvtRoundepi64Ps: Convert packed 64-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTQQ2PS'. Intrinsic: '_mm512_cvt_roundepi64_ps'.
// Requires AVX512DQ.
func CvtRoundepi64Ps(a M512i, rounding int) M256 {
	return M256(cvtRoundepi64Ps([64]byte(a), rounding))
}

func cvtRoundepi64Ps(a [64]byte, rounding int) [8]float32


// MaskCvtRoundepi64Ps: Convert packed 64-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := src[l+31:l]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTQQ2PS'. Intrinsic: '_mm512_mask_cvt_roundepi64_ps'.
// Requires AVX512DQ.
func MaskCvtRoundepi64Ps(src M256, k Mmask8, a M512i, rounding int) M256 {
	return M256(maskCvtRoundepi64Ps([8]float32(src), uint8(k), [64]byte(a), rounding))
}

func maskCvtRoundepi64Ps(src [8]float32, k uint8, a [64]byte, rounding int) [8]float32


// MaskzCvtRoundepi64Ps: Convert packed 64-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTQQ2PS'. Intrinsic: '_mm512_maskz_cvt_roundepi64_ps'.
// Requires AVX512DQ.
func MaskzCvtRoundepi64Ps(k Mmask8, a M512i, rounding int) M256 {
	return M256(maskzCvtRoundepi64Ps(uint8(k), [64]byte(a), rounding))
}

func maskzCvtRoundepi64Ps(k uint8, a [64]byte, rounding int) [8]float32


// CvtRoundepu32Ps: Convert packed unsigned 32-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := ConvertUnsignedInt32_To_FP32(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTUDQ2PS'. Intrinsic: '_mm512_cvt_roundepu32_ps'.
// Requires AVX512F.
func CvtRoundepu32Ps(a M512i, rounding int) M512 {
	return M512(cvtRoundepu32Ps([64]byte(a), rounding))
}

func cvtRoundepu32Ps(a [64]byte, rounding int) [16]float32


// MaskCvtRoundepu32Ps: Convert packed unsigned 32-bit integers in 'a' to
// packed single-precision (32-bit) floating-point elements, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ConvertUnsignedInt32_To_FP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTUDQ2PS'. Intrinsic: '_mm512_mask_cvt_roundepu32_ps'.
// Requires AVX512F.
func MaskCvtRoundepu32Ps(src M512, k Mmask16, a M512i, rounding int) M512 {
	return M512(maskCvtRoundepu32Ps([16]float32(src), uint16(k), [64]byte(a), rounding))
}

func maskCvtRoundepu32Ps(src [16]float32, k uint16, a [64]byte, rounding int) [16]float32


// MaskzCvtRoundepu32Ps: Convert packed unsigned 32-bit integers in 'a' to
// packed single-precision (32-bit) floating-point elements, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := ConvertUnsignedInt32_To_FP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTUDQ2PS'. Intrinsic: '_mm512_maskz_cvt_roundepu32_ps'.
// Requires AVX512F.
func MaskzCvtRoundepu32Ps(k Mmask16, a M512i, rounding int) M512 {
	return M512(maskzCvtRoundepu32Ps(uint16(k), [64]byte(a), rounding))
}

func maskzCvtRoundepu32Ps(k uint16, a [64]byte, rounding int) [16]float32


// CvtRoundepu64Pd: Convert packed unsigned 64-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ConvertUnsignedInt64_To_FP64(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTUQQ2PD'. Intrinsic: '_mm512_cvt_roundepu64_pd'.
// Requires AVX512DQ.
func CvtRoundepu64Pd(a M512i, rounding int) M512d {
	return M512d(cvtRoundepu64Pd([64]byte(a), rounding))
}

func cvtRoundepu64Pd(a [64]byte, rounding int) [8]float64


// MaskCvtRoundepu64Pd: Convert packed unsigned 64-bit integers in 'a' to
// packed double-precision (64-bit) floating-point elements, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ConvertUnsignedInt64_To_FP64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTUQQ2PD'. Intrinsic: '_mm512_mask_cvt_roundepu64_pd'.
// Requires AVX512DQ.
func MaskCvtRoundepu64Pd(src M512d, k Mmask8, a M512i, rounding int) M512d {
	return M512d(maskCvtRoundepu64Pd([8]float64(src), uint8(k), [64]byte(a), rounding))
}

func maskCvtRoundepu64Pd(src [8]float64, k uint8, a [64]byte, rounding int) [8]float64


// MaskzCvtRoundepu64Pd: Convert packed unsigned 64-bit integers in 'a' to
// packed double-precision (64-bit) floating-point elements, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ConvertUnsignedInt64_To_FP64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTUQQ2PD'. Intrinsic: '_mm512_maskz_cvt_roundepu64_pd'.
// Requires AVX512DQ.
func MaskzCvtRoundepu64Pd(k Mmask8, a M512i, rounding int) M512d {
	return M512d(maskzCvtRoundepu64Pd(uint8(k), [64]byte(a), rounding))
}

func maskzCvtRoundepu64Pd(k uint8, a [64]byte, rounding int) [8]float64


// CvtRoundepu64Ps: Convert packed unsigned 64-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			dst[l+31:l] := ConvertUnsignedInt64_To_FP32(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTUQQ2PS'. Intrinsic: '_mm512_cvt_roundepu64_ps'.
// Requires AVX512DQ.
func CvtRoundepu64Ps(a M512i, rounding int) M256 {
	return M256(cvtRoundepu64Ps([64]byte(a), rounding))
}

func cvtRoundepu64Ps(a [64]byte, rounding int) [8]float32


// MaskCvtRoundepu64Ps: Convert packed unsigned 64-bit integers in 'a' to
// packed single-precision (32-bit) floating-point elements, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[l+31:l] := ConvertUnsignedInt64_To_FP32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := src[l+31:l]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTUQQ2PS'. Intrinsic: '_mm512_mask_cvt_roundepu64_ps'.
// Requires AVX512DQ.
func MaskCvtRoundepu64Ps(src M256, k Mmask8, a M512i, rounding int) M256 {
	return M256(maskCvtRoundepu64Ps([8]float32(src), uint8(k), [64]byte(a), rounding))
}

func maskCvtRoundepu64Ps(src [8]float32, k uint8, a [64]byte, rounding int) [8]float32


// MaskzCvtRoundepu64Ps: Convert packed unsigned 64-bit integers in 'a' to
// packed single-precision (32-bit) floating-point elements, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[l+31:l] := ConvertUnsignedInt64_To_FP32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTUQQ2PS'. Intrinsic: '_mm512_maskz_cvt_roundepu64_ps'.
// Requires AVX512DQ.
func MaskzCvtRoundepu64Ps(k Mmask8, a M512i, rounding int) M256 {
	return M256(maskzCvtRoundepu64Ps(uint8(k), [64]byte(a), rounding))
}

func maskzCvtRoundepu64Ps(k uint8, a [64]byte, rounding int) [8]float32


// CvtRoundpdEpi32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_Int32(a[k+63:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2DQ'. Intrinsic: '_mm512_cvt_roundpd_epi32'.
// Requires AVX512F.
func CvtRoundpdEpi32(a M512d, rounding int) M256i {
	return M256i(cvtRoundpdEpi32([8]float64(a), rounding))
}

func cvtRoundpdEpi32(a [8]float64, rounding int) [32]byte


// MaskCvtRoundpdEpi32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*32
//			l := j*64
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_Int32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2DQ'. Intrinsic: '_mm512_mask_cvt_roundpd_epi32'.
// Requires AVX512F.
func MaskCvtRoundpdEpi32(src M256i, k Mmask8, a M512d, rounding int) M256i {
	return M256i(maskCvtRoundpdEpi32([32]byte(src), uint8(k), [8]float64(a), rounding))
}

func maskCvtRoundpdEpi32(src [32]byte, k uint8, a [8]float64, rounding int) [32]byte


// MaskzCvtRoundpdEpi32: Convert packed double-precision (64-bit)
// floating-point elements in 'a' to packed 32-bit integers, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_Int32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2DQ'. Intrinsic: '_mm512_maskz_cvt_roundpd_epi32'.
// Requires AVX512F.
func MaskzCvtRoundpdEpi32(k Mmask8, a M512d, rounding int) M256i {
	return M256i(maskzCvtRoundpdEpi32(uint8(k), [8]float64(a), rounding))
}

func maskzCvtRoundpdEpi32(k uint8, a [8]float64, rounding int) [32]byte


// CvtRoundpdEpi64: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 64-bit integers, and store the results in 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := Convert_FP64_To_Int64(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPD2QQ'. Intrinsic: '_mm512_cvt_roundpd_epi64'.
// Requires AVX512DQ.
func CvtRoundpdEpi64(a M512d, rounding int) M512i {
	return M512i(cvtRoundpdEpi64([8]float64(a), rounding))
}

func cvtRoundpdEpi64(a [8]float64, rounding int) [64]byte


// MaskCvtRoundpdEpi64: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 64-bit integers, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_FP64_To_Int64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPD2QQ'. Intrinsic: '_mm512_mask_cvt_roundpd_epi64'.
// Requires AVX512DQ.
func MaskCvtRoundpdEpi64(src M512i, k Mmask8, a M512d, rounding int) M512i {
	return M512i(maskCvtRoundpdEpi64([64]byte(src), uint8(k), [8]float64(a), rounding))
}

func maskCvtRoundpdEpi64(src [64]byte, k uint8, a [8]float64, rounding int) [64]byte


// MaskzCvtRoundpdEpi64: Convert packed double-precision (64-bit)
// floating-point elements in 'a' to packed 64-bit integers, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_FP64_To_Int64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPD2QQ'. Intrinsic: '_mm512_maskz_cvt_roundpd_epi64'.
// Requires AVX512DQ.
func MaskzCvtRoundpdEpi64(k Mmask8, a M512d, rounding int) M512i {
	return M512i(maskzCvtRoundpdEpi64(uint8(k), [8]float64(a), rounding))
}

func maskzCvtRoundpdEpi64(k uint8, a [8]float64, rounding int) [64]byte


// CvtRoundpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[k+63:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2UDQ'. Intrinsic: '_mm512_cvt_roundpd_epu32'.
// Requires AVX512F.
func CvtRoundpdEpu32(a M512d, rounding int) M256i {
	return M256i(cvtRoundpdEpu32([8]float64(a), rounding))
}

func cvtRoundpdEpu32(a [8]float64, rounding int) [32]byte


// MaskCvtRoundpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*32
//			l := j*64
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2UDQ'. Intrinsic: '_mm512_mask_cvt_roundpd_epu32'.
// Requires AVX512F.
func MaskCvtRoundpdEpu32(src M256i, k Mmask8, a M512d, rounding int) M256i {
	return M256i(maskCvtRoundpdEpu32([32]byte(src), uint8(k), [8]float64(a), rounding))
}

func maskCvtRoundpdEpu32(src [32]byte, k uint8, a [8]float64, rounding int) [32]byte


// MaskzCvtRoundpdEpu32: Convert packed double-precision (64-bit)
// floating-point elements in 'a' to packed unsigned 32-bit integers, and store
// the results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2UDQ'. Intrinsic: '_mm512_maskz_cvt_roundpd_epu32'.
// Requires AVX512F.
func MaskzCvtRoundpdEpu32(k Mmask8, a M512d, rounding int) M256i {
	return M256i(maskzCvtRoundpdEpu32(uint8(k), [8]float64(a), rounding))
}

func maskzCvtRoundpdEpu32(k uint8, a [8]float64, rounding int) [32]byte


// CvtRoundpdEpu64: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 64-bit integers, and store the results in
// 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := Convert_FP64_To_UnsignedInt64(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPD2UQQ'. Intrinsic: '_mm512_cvt_roundpd_epu64'.
// Requires AVX512DQ.
func CvtRoundpdEpu64(a M512d, rounding int) M512i {
	return M512i(cvtRoundpdEpu64([8]float64(a), rounding))
}

func cvtRoundpdEpu64(a [8]float64, rounding int) [64]byte


// MaskCvtRoundpdEpu64: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 64-bit integers, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_FP64_To_UnsignedInt64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPD2UQQ'. Intrinsic: '_mm512_mask_cvt_roundpd_epu64'.
// Requires AVX512DQ.
func MaskCvtRoundpdEpu64(src M512i, k Mmask8, a M512d, rounding int) M512i {
	return M512i(maskCvtRoundpdEpu64([64]byte(src), uint8(k), [8]float64(a), rounding))
}

func maskCvtRoundpdEpu64(src [64]byte, k uint8, a [8]float64, rounding int) [64]byte


// MaskzCvtRoundpdEpu64: Convert packed double-precision (64-bit)
// floating-point elements in 'a' to packed unsigned 64-bit integers, and store
// the results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_FP64_To_UnsignedInt64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPD2UQQ'. Intrinsic: '_mm512_maskz_cvt_roundpd_epu64'.
// Requires AVX512DQ.
func MaskzCvtRoundpdEpu64(k Mmask8, a M512d, rounding int) M512i {
	return M512i(maskzCvtRoundpdEpu64(uint8(k), [8]float64(a), rounding))
}

func maskzCvtRoundpdEpu64(k uint8, a [8]float64, rounding int) [64]byte


// CvtRoundpdPs: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed single-precision (32-bit) floating-point elements,
// and store the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_FP32(a[k+63:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2PS'. Intrinsic: '_mm512_cvt_roundpd_ps'.
// Requires AVX512F.
func CvtRoundpdPs(a M512d, rounding int) M256 {
	return M256(cvtRoundpdPs([8]float64(a), rounding))
}

func cvtRoundpdPs(a [8]float64, rounding int) [8]float32


// MaskCvtRoundpdPs: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed single-precision (32-bit) floating-point elements,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*32
//			l := j*64
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_FP32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2PS'. Intrinsic: '_mm512_mask_cvt_roundpd_ps'.
// Requires AVX512F.
func MaskCvtRoundpdPs(src M256, k Mmask8, a M512d, rounding int) M256 {
	return M256(maskCvtRoundpdPs([8]float32(src), uint8(k), [8]float64(a), rounding))
}

func maskCvtRoundpdPs(src [8]float32, k uint8, a [8]float64, rounding int) [8]float32


// MaskzCvtRoundpdPs: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed single-precision (32-bit) floating-point elements,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*32
//			l := j*64
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_FP32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2PS'. Intrinsic: '_mm512_maskz_cvt_roundpd_ps'.
// Requires AVX512F.
func MaskzCvtRoundpdPs(k Mmask8, a M512d, rounding int) M256 {
	return M256(maskzCvtRoundpdPs(uint8(k), [8]float64(a), rounding))
}

func maskzCvtRoundpdPs(k uint8, a [8]float64, rounding int) [8]float32


// CvtRoundpdPslo: Performs element-by-element conversion of packed
// double-precision (64-bit) floating-point elements in 'v2' to packed
// single-precision (32-bit) floating-point elements, storing the results in
// 'dst'. Results are written to the lower half of 'dst', and the upper half
// locations are set to '0'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			k := j*32
//			dst[k+31:k] := Float64ToFloat32(v2[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPD2PS'. Intrinsic: '_mm512_cvt_roundpd_pslo'.
// Requires KNCNI.
func CvtRoundpdPslo(v2 M512d, rounding int) M512 {
	return M512(cvtRoundpdPslo([8]float64(v2), rounding))
}

func cvtRoundpdPslo(v2 [8]float64, rounding int) [16]float32


// MaskCvtRoundpdPslo: Performs element-by-element conversion of packed
// double-precision (64-bit) floating-point elements in 'v2' to packed
// single-precision (32-bit) floating-point elements, storing the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). Results are written to the lower half of
// 'dst', and the upper half locations are set to '0'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[l+31:l] := Float64ToFloat32(v2[i+63:i])
//			ELSE
//				dst[l+31:l] := src[l+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPD2PS'. Intrinsic: '_mm512_mask_cvt_roundpd_pslo'.
// Requires KNCNI.
func MaskCvtRoundpdPslo(src M512, k Mmask8, v2 M512d, rounding int) M512 {
	return M512(maskCvtRoundpdPslo([16]float32(src), uint8(k), [8]float64(v2), rounding))
}

func maskCvtRoundpdPslo(src [16]float32, k uint8, v2 [8]float64, rounding int) [16]float32


// CvtRoundphPs: Convert packed half-precision (16-bit) floating-point elements
// in 'a' to packed single-precision (32-bit) floating-point elements, and
// store the results in 'dst'. 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 15
//			i := j*32
//			m := j*16
//			dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPH2PS'. Intrinsic: '_mm512_cvt_roundph_ps'.
// Requires AVX512F.
func CvtRoundphPs(a M256i, sae int) M512 {
	return M512(cvtRoundphPs([32]byte(a), sae))
}

func cvtRoundphPs(a [32]byte, sae int) [16]float32


// MaskCvtRoundphPs: Convert packed half-precision (16-bit) floating-point
// elements in 'a' to packed single-precision (32-bit) floating-point elements,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 15
//			i := j*32
//			m := j*16
//			IF k[j]
//				dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPH2PS'. Intrinsic: '_mm512_mask_cvt_roundph_ps'.
// Requires AVX512F.
func MaskCvtRoundphPs(src M512, k Mmask16, a M256i, sae int) M512 {
	return M512(maskCvtRoundphPs([16]float32(src), uint16(k), [32]byte(a), sae))
}

func maskCvtRoundphPs(src [16]float32, k uint16, a [32]byte, sae int) [16]float32


// MaskzCvtRoundphPs: Convert packed half-precision (16-bit) floating-point
// elements in 'a' to packed single-precision (32-bit) floating-point elements,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 15
//			i := j*32
//			m := j*16
//			IF k[j]
//				dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPH2PS'. Intrinsic: '_mm512_maskz_cvt_roundph_ps'.
// Requires AVX512F.
func MaskzCvtRoundphPs(k Mmask16, a M256i, sae int) M512 {
	return M512(maskzCvtRoundphPs(uint16(k), [32]byte(a), sae))
}

func maskzCvtRoundphPs(k uint16, a [32]byte, sae int) [16]float32


// CvtRoundpsEpi32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2DQ'. Intrinsic: '_mm512_cvt_roundps_epi32'.
// Requires AVX512F.
func CvtRoundpsEpi32(a M512, rounding int) M512i {
	return M512i(cvtRoundpsEpi32([16]float32(a), rounding))
}

func cvtRoundpsEpi32(a [16]float32, rounding int) [64]byte


// MaskCvtRoundpsEpi32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2DQ'. Intrinsic: '_mm512_mask_cvt_roundps_epi32'.
// Requires AVX512F.
func MaskCvtRoundpsEpi32(src M512i, k Mmask16, a M512, rounding int) M512i {
	return M512i(maskCvtRoundpsEpi32([64]byte(src), uint16(k), [16]float32(a), rounding))
}

func maskCvtRoundpsEpi32(src [64]byte, k uint16, a [16]float32, rounding int) [64]byte


// MaskzCvtRoundpsEpi32: Convert packed single-precision (32-bit)
// floating-point elements in 'a' to packed 32-bit integers, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2DQ'. Intrinsic: '_mm512_maskz_cvt_roundps_epi32'.
// Requires AVX512F.
func MaskzCvtRoundpsEpi32(k Mmask16, a M512, rounding int) M512i {
	return M512i(maskzCvtRoundpsEpi32(uint16(k), [16]float32(a), rounding))
}

func maskzCvtRoundpsEpi32(k uint16, a [16]float32, rounding int) [64]byte


// CvtRoundpsEpi64: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 64-bit integers, and store the results in 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			dst[i+63:i] := Convert_FP32_To_Int64(a[l+31:l])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2QQ'. Intrinsic: '_mm512_cvt_roundps_epi64'.
// Requires AVX512DQ.
func CvtRoundpsEpi64(a M256, rounding int) M512i {
	return M512i(cvtRoundpsEpi64([8]float32(a), rounding))
}

func cvtRoundpsEpi64(a [8]float32, rounding int) [64]byte


// MaskCvtRoundpsEpi64: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 64-bit integers, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
// 	 Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_Int64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2QQ'. Intrinsic: '_mm512_mask_cvt_roundps_epi64'.
// Requires AVX512DQ.
func MaskCvtRoundpsEpi64(src M512i, k Mmask8, a M256, rounding int) M512i {
	return M512i(maskCvtRoundpsEpi64([64]byte(src), uint8(k), [8]float32(a), rounding))
}

func maskCvtRoundpsEpi64(src [64]byte, k uint8, a [8]float32, rounding int) [64]byte


// MaskzCvtRoundpsEpi64: Convert packed single-precision (32-bit)
// floating-point elements in 'a' to packed 64-bit integers, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_Int64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2QQ'. Intrinsic: '_mm512_maskz_cvt_roundps_epi64'.
// Requires AVX512DQ.
func MaskzCvtRoundpsEpi64(k Mmask8, a M256, rounding int) M512i {
	return M512i(maskzCvtRoundpsEpi64(uint8(k), [8]float32(a), rounding))
}

func maskzCvtRoundpsEpi64(k uint8, a [8]float32, rounding int) [64]byte


// CvtRoundpsEpu32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2UDQ'. Intrinsic: '_mm512_cvt_roundps_epu32'.
// Requires AVX512F.
func CvtRoundpsEpu32(a M512, rounding int) M512i {
	return M512i(cvtRoundpsEpu32([16]float32(a), rounding))
}

func cvtRoundpsEpu32(a [16]float32, rounding int) [64]byte


// MaskCvtRoundpsEpu32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2UDQ'. Intrinsic: '_mm512_mask_cvt_roundps_epu32'.
// Requires AVX512F.
func MaskCvtRoundpsEpu32(src M512i, k Mmask16, a M512, rounding int) M512i {
	return M512i(maskCvtRoundpsEpu32([64]byte(src), uint16(k), [16]float32(a), rounding))
}

func maskCvtRoundpsEpu32(src [64]byte, k uint16, a [16]float32, rounding int) [64]byte


// MaskzCvtRoundpsEpu32: Convert packed single-precision (32-bit)
// floating-point elements in 'a' to packed unsigned 32-bit integers, and store
// the results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2UDQ'. Intrinsic: '_mm512_maskz_cvt_roundps_epu32'.
// Requires AVX512F.
func MaskzCvtRoundpsEpu32(k Mmask16, a M512, rounding int) M512i {
	return M512i(maskzCvtRoundpsEpu32(uint16(k), [16]float32(a), rounding))
}

func maskzCvtRoundpsEpu32(k uint16, a [16]float32, rounding int) [64]byte


// CvtRoundpsEpu64: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 64-bit integers, and store the results in
// 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			dst[i+63:i] := Convert_FP32_To_UnsignedInt64(a[l+31:l])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2UQQ'. Intrinsic: '_mm512_cvt_roundps_epu64'.
// Requires AVX512DQ.
func CvtRoundpsEpu64(a M256, rounding int) M512i {
	return M512i(cvtRoundpsEpu64([8]float32(a), rounding))
}

func cvtRoundpsEpu64(a [8]float32, rounding int) [64]byte


// MaskCvtRoundpsEpu64: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 64-bit integers, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_UnsignedInt64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2UQQ'. Intrinsic: '_mm512_mask_cvt_roundps_epu64'.
// Requires AVX512DQ.
func MaskCvtRoundpsEpu64(src M512i, k Mmask8, a M256, rounding int) M512i {
	return M512i(maskCvtRoundpsEpu64([64]byte(src), uint8(k), [8]float32(a), rounding))
}

func maskCvtRoundpsEpu64(src [64]byte, k uint8, a [8]float32, rounding int) [64]byte


// MaskzCvtRoundpsEpu64: Convert packed single-precision (32-bit)
// floating-point elements in 'a' to packed unsigned 64-bit integers, and store
// the results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_UnsignedInt64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2UQQ'. Intrinsic: '_mm512_maskz_cvt_roundps_epu64'.
// Requires AVX512DQ.
func MaskzCvtRoundpsEpu64(k Mmask8, a M256, rounding int) M512i {
	return M512i(maskzCvtRoundpsEpu64(uint8(k), [8]float32(a), rounding))
}

func maskzCvtRoundpsEpu64(k uint8, a [8]float32, rounding int) [64]byte


// CvtRoundpsPd: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed double-precision (64-bit) floating-point elements,
// and store the results in 'dst'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 32*j
//			dst[i+63:i] := Convert_FP32_To_FP64(a[k+31:k])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2PD'. Intrinsic: '_mm512_cvt_roundps_pd'.
// Requires AVX512F.
func CvtRoundpsPd(a M256, sae int) M512d {
	return M512d(cvtRoundpsPd([8]float32(a), sae))
}

func cvtRoundpsPd(a [8]float32, sae int) [8]float64


// MaskCvtRoundpsPd: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed double-precision (64-bit) floating-point elements,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_FP64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2PD'. Intrinsic: '_mm512_mask_cvt_roundps_pd'.
// Requires AVX512F.
func MaskCvtRoundpsPd(src M512d, k Mmask8, a M256, sae int) M512d {
	return M512d(maskCvtRoundpsPd([8]float64(src), uint8(k), [8]float32(a), sae))
}

func maskCvtRoundpsPd(src [8]float64, k uint8, a [8]float32, sae int) [8]float64


// MaskzCvtRoundpsPd: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed double-precision (64-bit) floating-point elements,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set).
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_FP64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2PD'. Intrinsic: '_mm512_maskz_cvt_roundps_pd'.
// Requires AVX512F.
func MaskzCvtRoundpsPd(k Mmask8, a M256, sae int) M512d {
	return M512d(maskzCvtRoundpsPd(uint8(k), [8]float32(a), sae))
}

func maskzCvtRoundpsPd(k uint8, a [8]float32, sae int) [8]float64


// CvtRoundpsPh: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed half-precision (16-bit) floating-point elements,
// and store the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 16*j
//			l := 32*j
//			dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm512_cvt_roundps_ph'.
// Requires AVX512F.
func CvtRoundpsPh(a M512, rounding int) M256i {
	return M256i(cvtRoundpsPh([16]float32(a), rounding))
}

func cvtRoundpsPh(a [16]float32, rounding int) [32]byte


// MaskCvtRoundpsPh: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed half-precision (16-bit) floating-point elements,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 16*j
//			l := 32*j
//			IF k[j]
//				dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm512_mask_cvt_roundps_ph'.
// Requires AVX512F.
func MaskCvtRoundpsPh(src M256i, k Mmask16, a M512, rounding int) M256i {
	return M256i(maskCvtRoundpsPh([32]byte(src), uint16(k), [16]float32(a), rounding))
}

func maskCvtRoundpsPh(src [32]byte, k uint16, a [16]float32, rounding int) [32]byte


// MaskzCvtRoundpsPh: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed half-precision (16-bit) floating-point elements,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 16*j
//			l := 32*j
//			IF k[j]
//				dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm512_maskz_cvt_roundps_ph'.
// Requires AVX512F.
func MaskzCvtRoundpsPh(k Mmask16, a M512, rounding int) M256i {
	return M256i(maskzCvtRoundpsPh(uint16(k), [16]float32(a), rounding))
}

func maskzCvtRoundpsPh(k uint16, a [16]float32, rounding int) [32]byte


// Cvtepi16Epi32: Sign extend packed 16-bit integers in 'a' to packed 32-bit
// integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			k := 16*j
//			dst[i+31:i] := SignExtend(a[k+15:k])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXWD'. Intrinsic: '_mm512_cvtepi16_epi32'.
// Requires AVX512F.
func Cvtepi16Epi32(a M256i) M512i {
	return M512i(cvtepi16Epi32([32]byte(a)))
}

func cvtepi16Epi32(a [32]byte) [64]byte


// MaskCvtepi16Epi32: Sign extend packed 16-bit integers in 'a' to packed
// 32-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			l := j*16
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[l+15:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXWD'. Intrinsic: '_mm512_mask_cvtepi16_epi32'.
// Requires AVX512F.
func MaskCvtepi16Epi32(src M512i, k Mmask16, a M256i) M512i {
	return M512i(maskCvtepi16Epi32([64]byte(src), uint16(k), [32]byte(a)))
}

func maskCvtepi16Epi32(src [64]byte, k uint16, a [32]byte) [64]byte


// MaskzCvtepi16Epi32: Sign extend packed 16-bit integers in 'a' to packed
// 32-bit integers, and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[l+15:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXWD'. Intrinsic: '_mm512_maskz_cvtepi16_epi32'.
// Requires AVX512F.
func MaskzCvtepi16Epi32(k Mmask16, a M256i) M512i {
	return M512i(maskzCvtepi16Epi32(uint16(k), [32]byte(a)))
}

func maskzCvtepi16Epi32(k uint16, a [32]byte) [64]byte


// Cvtepi16Epi64: Sign extend packed 16-bit integers in 'a' to packed 64-bit
// integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 16*j
//			dst[i+63:i] := SignExtend(a[k+15:k])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXWQ'. Intrinsic: '_mm512_cvtepi16_epi64'.
// Requires AVX512F.
func Cvtepi16Epi64(a M128i) M512i {
	return M512i(cvtepi16Epi64([16]byte(a)))
}

func cvtepi16Epi64(a [16]byte) [64]byte


// MaskCvtepi16Epi64: Sign extend packed 16-bit integers in 'a' to packed
// 64-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+15:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXWQ'. Intrinsic: '_mm512_mask_cvtepi16_epi64'.
// Requires AVX512F.
func MaskCvtepi16Epi64(src M512i, k Mmask8, a M128i) M512i {
	return M512i(maskCvtepi16Epi64([64]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi16Epi64(src [64]byte, k uint8, a [16]byte) [64]byte


// MaskzCvtepi16Epi64: Sign extend packed 16-bit integers in 'a' to packed
// 64-bit integers, and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+15:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXWQ'. Intrinsic: '_mm512_maskz_cvtepi16_epi64'.
// Requires AVX512F.
func MaskzCvtepi16Epi64(k Mmask8, a M128i) M512i {
	return M512i(maskzCvtepi16Epi64(uint8(k), [16]byte(a)))
}

func maskzCvtepi16Epi64(k uint8, a [16]byte) [64]byte


// Cvtepi16Epi8: Convert packed 16-bit integers in 'a' to packed 8-bit integers
// with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := 16*j
//			l := 8*j
//			dst[l+7:l] := Truncate_Int16_To_Int8(a[i+15:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVWB'. Intrinsic: '_mm512_cvtepi16_epi8'.
// Requires AVX512BW.
func Cvtepi16Epi8(a M512i) M256i {
	return M256i(cvtepi16Epi8([64]byte(a)))
}

func cvtepi16Epi8(a [64]byte) [32]byte


// MaskCvtepi16Epi8: Convert packed 16-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := 16*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int16_To_Int8(a[i+15:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVWB'. Intrinsic: '_mm512_mask_cvtepi16_epi8'.
// Requires AVX512BW.
func MaskCvtepi16Epi8(src M256i, k Mmask32, a M512i) M256i {
	return M256i(maskCvtepi16Epi8([32]byte(src), uint32(k), [64]byte(a)))
}

func maskCvtepi16Epi8(src [32]byte, k uint32, a [64]byte) [32]byte


// MaskzCvtepi16Epi8: Convert packed 16-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := 16*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int16_To_Int8(a[i+15:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVWB'. Intrinsic: '_mm512_maskz_cvtepi16_epi8'.
// Requires AVX512BW.
func MaskzCvtepi16Epi8(k Mmask32, a M512i) M256i {
	return M256i(maskzCvtepi16Epi8(uint32(k), [64]byte(a)))
}

func maskzCvtepi16Epi8(k uint32, a [64]byte) [32]byte


// MaskCvtepi16StoreuEpi8: Convert packed 16-bit integers in 'a' to packed
// 8-bit integers with truncation, and store the active results (those with
// their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 31
//			i := 16*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Truncate_Int16_To_Int8(a[i+15:i])
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVWB'. Intrinsic: '_mm512_mask_cvtepi16_storeu_epi8'.
// Requires AVX512BW.
func MaskCvtepi16StoreuEpi8(base_addr uintptr, k Mmask32, a M512i)  {
	maskCvtepi16StoreuEpi8(uintptr(base_addr), uint32(k), [64]byte(a))
}

func maskCvtepi16StoreuEpi8(base_addr uintptr, k uint32, a [64]byte) 


// Cvtepi32Epi16: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			k := 16*j
//			dst[k+15:k] := Truncate_Int32_To_Int16(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVDW'. Intrinsic: '_mm512_cvtepi32_epi16'.
// Requires AVX512F.
func Cvtepi32Epi16(a M512i) M256i {
	return M256i(cvtepi32Epi16([64]byte(a)))
}

func cvtepi32Epi16(a [64]byte) [32]byte


// MaskCvtepi32Epi16: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Truncate_Int32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVDW'. Intrinsic: '_mm512_mask_cvtepi32_epi16'.
// Requires AVX512F.
func MaskCvtepi32Epi16(src M256i, k Mmask16, a M512i) M256i {
	return M256i(maskCvtepi32Epi16([32]byte(src), uint16(k), [64]byte(a)))
}

func maskCvtepi32Epi16(src [32]byte, k uint16, a [64]byte) [32]byte


// MaskzCvtepi32Epi16: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Truncate_Int32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVDW'. Intrinsic: '_mm512_maskz_cvtepi32_epi16'.
// Requires AVX512F.
func MaskzCvtepi32Epi16(k Mmask16, a M512i) M256i {
	return M256i(maskzCvtepi32Epi16(uint16(k), [64]byte(a)))
}

func maskzCvtepi32Epi16(k uint16, a [64]byte) [32]byte


// Cvtepi32Epi64: Sign extend packed 32-bit integers in 'a' to packed 64-bit
// integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 32*j
//			dst[i+63:i] := SignExtend(a[k+31:k])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXDQ'. Intrinsic: '_mm512_cvtepi32_epi64'.
// Requires AVX512F.
func Cvtepi32Epi64(a M256i) M512i {
	return M512i(cvtepi32Epi64([32]byte(a)))
}

func cvtepi32Epi64(a [32]byte) [64]byte


// MaskCvtepi32Epi64: Sign extend packed 32-bit integers in 'a' to packed
// 64-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXDQ'. Intrinsic: '_mm512_mask_cvtepi32_epi64'.
// Requires AVX512F.
func MaskCvtepi32Epi64(src M512i, k Mmask8, a M256i) M512i {
	return M512i(maskCvtepi32Epi64([64]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtepi32Epi64(src [64]byte, k uint8, a [32]byte) [64]byte


// MaskzCvtepi32Epi64: Sign extend packed 32-bit integers in 'a' to packed
// 64-bit integers, and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+31:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXDQ'. Intrinsic: '_mm512_maskz_cvtepi32_epi64'.
// Requires AVX512F.
func MaskzCvtepi32Epi64(k Mmask8, a M256i) M512i {
	return M512i(maskzCvtepi32Epi64(uint8(k), [32]byte(a)))
}

func maskzCvtepi32Epi64(k uint8, a [32]byte) [64]byte


// Cvtepi32Epi8: Convert packed 32-bit integers in 'a' to packed 8-bit integers
// with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			k := 8*j
//			dst[k+7:k] := Truncate_Int32_To_Int8(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVDB'. Intrinsic: '_mm512_cvtepi32_epi8'.
// Requires AVX512F.
func Cvtepi32Epi8(a M512i) M128i {
	return M128i(cvtepi32Epi8([64]byte(a)))
}

func cvtepi32Epi8(a [64]byte) [16]byte


// MaskCvtepi32Epi8: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVDB'. Intrinsic: '_mm512_mask_cvtepi32_epi8'.
// Requires AVX512F.
func MaskCvtepi32Epi8(src M128i, k Mmask16, a M512i) M128i {
	return M128i(maskCvtepi32Epi8([16]byte(src), uint16(k), [64]byte(a)))
}

func maskCvtepi32Epi8(src [16]byte, k uint16, a [64]byte) [16]byte


// MaskzCvtepi32Epi8: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVDB'. Intrinsic: '_mm512_maskz_cvtepi32_epi8'.
// Requires AVX512F.
func MaskzCvtepi32Epi8(k Mmask16, a M512i) M128i {
	return M128i(maskzCvtepi32Epi8(uint16(k), [64]byte(a)))
}

func maskzCvtepi32Epi8(k uint16, a [64]byte) [16]byte


// Cvtepi32Pd: Convert packed 32-bit integers in 'a' to packed double-precision
// (64-bit) floating-point elements, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			m := j*64
//			dst[m+63:m] := Convert_Int32_To_FP64(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTDQ2PD'. Intrinsic: '_mm512_cvtepi32_pd'.
// Requires AVX512F.
func Cvtepi32Pd(a M256i) M512d {
	return M512d(cvtepi32Pd([32]byte(a)))
}

func cvtepi32Pd(a [32]byte) [8]float64


// MaskCvtepi32Pd: Convert packed 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			m := j*64
//			IF k[j]
//				dst[m+63:m] := Convert_Int32_To_FP64(a[i+31:i])
//			ELSE
//				dst[m+63:m] := src[m+63:m]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTDQ2PD'. Intrinsic: '_mm512_mask_cvtepi32_pd'.
// Requires AVX512F.
func MaskCvtepi32Pd(src M512d, k Mmask8, a M256i) M512d {
	return M512d(maskCvtepi32Pd([8]float64(src), uint8(k), [32]byte(a)))
}

func maskCvtepi32Pd(src [8]float64, k uint8, a [32]byte) [8]float64


// MaskzCvtepi32Pd: Convert packed 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			m := j*64
//			IF k[j]
//				dst[m+63:m] := Convert_Int32_To_FP64(a[i+31:i])
//			ELSE
//				dst[m+63:m] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTDQ2PD'. Intrinsic: '_mm512_maskz_cvtepi32_pd'.
// Requires AVX512F.
func MaskzCvtepi32Pd(k Mmask8, a M256i) M512d {
	return M512d(maskzCvtepi32Pd(uint8(k), [32]byte(a)))
}

func maskzCvtepi32Pd(k uint8, a [32]byte) [8]float64


// Cvtepi32Ps: Convert packed 32-bit integers in 'a' to packed single-precision
// (32-bit) floating-point elements, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTDQ2PS'. Intrinsic: '_mm512_cvtepi32_ps'.
// Requires AVX512F.
func Cvtepi32Ps(a M512i) M512 {
	return M512(cvtepi32Ps([64]byte(a)))
}

func cvtepi32Ps(a [64]byte) [16]float32


// MaskCvtepi32Ps: Convert packed 32-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTDQ2PS'. Intrinsic: '_mm512_mask_cvtepi32_ps'.
// Requires AVX512F.
func MaskCvtepi32Ps(src M512, k Mmask16, a M512i) M512 {
	return M512(maskCvtepi32Ps([16]float32(src), uint16(k), [64]byte(a)))
}

func maskCvtepi32Ps(src [16]float32, k uint16, a [64]byte) [16]float32


// MaskzCvtepi32Ps: Convert packed 32-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTDQ2PS'. Intrinsic: '_mm512_maskz_cvtepi32_ps'.
// Requires AVX512F.
func MaskzCvtepi32Ps(k Mmask16, a M512i) M512 {
	return M512(maskzCvtepi32Ps(uint16(k), [64]byte(a)))
}

func maskzCvtepi32Ps(k uint16, a [64]byte) [16]float32


// MaskCvtepi32StoreuEpi16: Convert packed 32-bit integers in 'a' to packed
// 16-bit integers with truncation, and store the active results (those with
// their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				MEM[base_addr+l+15:base_addr+l] := Truncate_Int32_To_Int16(a[i+31:i])
//			FI
//		ENDFOR
//
// Instruction: 'VPMOVDW'. Intrinsic: '_mm512_mask_cvtepi32_storeu_epi16'.
// Requires AVX512F.
func MaskCvtepi32StoreuEpi16(base_addr uintptr, k Mmask16, a M512i)  {
	maskCvtepi32StoreuEpi16(uintptr(base_addr), uint16(k), [64]byte(a))
}

func maskCvtepi32StoreuEpi16(base_addr uintptr, k uint16, a [64]byte) 


// MaskCvtepi32StoreuEpi8: Convert packed 32-bit integers in 'a' to packed
// 8-bit integers with truncation, and store the active results (those with
// their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Truncate_Int32_To_Int8(a[i+31:i])
//			FI
//		ENDFOR
//
// Instruction: 'VPMOVDB'. Intrinsic: '_mm512_mask_cvtepi32_storeu_epi8'.
// Requires AVX512F.
func MaskCvtepi32StoreuEpi8(base_addr uintptr, k Mmask16, a M512i)  {
	maskCvtepi32StoreuEpi8(uintptr(base_addr), uint16(k), [64]byte(a))
}

func maskCvtepi32StoreuEpi8(base_addr uintptr, k uint16, a [64]byte) 


// Cvtepi32loPd: Performs element-by-element conversion of the lower half of
// packed 32-bit integer elements in 'v2' to packed double-precision (64-bit)
// floating-point elements, storing the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			l := j*64
//			dst[l+63:l] := Int32ToFloat64(v2[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTDQ2PD'. Intrinsic: '_mm512_cvtepi32lo_pd'.
// Requires KNCNI.
func Cvtepi32loPd(v2 M512i) M512d {
	return M512d(cvtepi32loPd([64]byte(v2)))
}

func cvtepi32loPd(v2 [64]byte) [8]float64


// MaskCvtepi32loPd: Performs element-by-element conversion of the lower half
// of packed 32-bit integer elements in 'v2' to packed double-precision
// (64-bit) floating-point elements, storing the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			n := j*64
//			IF k[j]
//				dst[k+63:k] := Int32ToFloat64(v2[i+31:i])
//			ELSE
//				dst[n+63:n] := src[n+63:n]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTDQ2PD'. Intrinsic: '_mm512_mask_cvtepi32lo_pd'.
// Requires KNCNI.
func MaskCvtepi32loPd(src M512d, k Mmask8, v2 M512i) M512d {
	return M512d(maskCvtepi32loPd([8]float64(src), uint8(k), [64]byte(v2)))
}

func maskCvtepi32loPd(src [8]float64, k uint8, v2 [64]byte) [8]float64


// Cvtepi64Epi16: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 16*j
//			dst[k+15:k] := Truncate_Int64_To_Int16(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQW'. Intrinsic: '_mm512_cvtepi64_epi16'.
// Requires AVX512F.
func Cvtepi64Epi16(a M512i) M128i {
	return M128i(cvtepi64Epi16([64]byte(a)))
}

func cvtepi64Epi16(a [64]byte) [16]byte


// MaskCvtepi64Epi16: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Truncate_Int64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQW'. Intrinsic: '_mm512_mask_cvtepi64_epi16'.
// Requires AVX512F.
func MaskCvtepi64Epi16(src M128i, k Mmask8, a M512i) M128i {
	return M128i(maskCvtepi64Epi16([16]byte(src), uint8(k), [64]byte(a)))
}

func maskCvtepi64Epi16(src [16]byte, k uint8, a [64]byte) [16]byte


// MaskzCvtepi64Epi16: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Truncate_Int64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQW'. Intrinsic: '_mm512_maskz_cvtepi64_epi16'.
// Requires AVX512F.
func MaskzCvtepi64Epi16(k Mmask8, a M512i) M128i {
	return M128i(maskzCvtepi64Epi16(uint8(k), [64]byte(a)))
}

func maskzCvtepi64Epi16(k uint8, a [64]byte) [16]byte


// Cvtepi64Epi32: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 32*j
//			dst[k+31:k] := Truncate_Int64_To_Int32(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVQD'. Intrinsic: '_mm512_cvtepi64_epi32'.
// Requires AVX512F.
func Cvtepi64Epi32(a M512i) M256i {
	return M256i(cvtepi64Epi32([64]byte(a)))
}

func cvtepi64Epi32(a [64]byte) [32]byte


// MaskCvtepi64Epi32: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Truncate_Int64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := src[l+31:l]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVQD'. Intrinsic: '_mm512_mask_cvtepi64_epi32'.
// Requires AVX512F.
func MaskCvtepi64Epi32(src M256i, k Mmask8, a M512i) M256i {
	return M256i(maskCvtepi64Epi32([32]byte(src), uint8(k), [64]byte(a)))
}

func maskCvtepi64Epi32(src [32]byte, k uint8, a [64]byte) [32]byte


// MaskzCvtepi64Epi32: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Truncate_Int64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVQD'. Intrinsic: '_mm512_maskz_cvtepi64_epi32'.
// Requires AVX512F.
func MaskzCvtepi64Epi32(k Mmask8, a M512i) M256i {
	return M256i(maskzCvtepi64Epi32(uint8(k), [64]byte(a)))
}

func maskzCvtepi64Epi32(k uint8, a [64]byte) [32]byte


// Cvtepi64Epi8: Convert packed 64-bit integers in 'a' to packed 8-bit integers
// with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 8*j
//			dst[k+7:k] := Truncate_Int64_To_Int8(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQB'. Intrinsic: '_mm512_cvtepi64_epi8'.
// Requires AVX512F.
func Cvtepi64Epi8(a M512i) M128i {
	return M128i(cvtepi64Epi8([64]byte(a)))
}

func cvtepi64Epi8(a [64]byte) [16]byte


// MaskCvtepi64Epi8: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQB'. Intrinsic: '_mm512_mask_cvtepi64_epi8'.
// Requires AVX512F.
func MaskCvtepi64Epi8(src M128i, k Mmask8, a M512i) M128i {
	return M128i(maskCvtepi64Epi8([16]byte(src), uint8(k), [64]byte(a)))
}

func maskCvtepi64Epi8(src [16]byte, k uint8, a [64]byte) [16]byte


// MaskzCvtepi64Epi8: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQB'. Intrinsic: '_mm512_maskz_cvtepi64_epi8'.
// Requires AVX512F.
func MaskzCvtepi64Epi8(k Mmask8, a M512i) M128i {
	return M128i(maskzCvtepi64Epi8(uint8(k), [64]byte(a)))
}

func maskzCvtepi64Epi8(k uint8, a [64]byte) [16]byte


// Cvtepi64Pd: Convert packed 64-bit integers in 'a' to packed double-precision
// (64-bit) floating-point elements, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTQQ2PD'. Intrinsic: '_mm512_cvtepi64_pd'.
// Requires AVX512DQ.
func Cvtepi64Pd(a M512i) M512d {
	return M512d(cvtepi64Pd([64]byte(a)))
}

func cvtepi64Pd(a [64]byte) [8]float64


// MaskCvtepi64Pd: Convert packed 64-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTQQ2PD'. Intrinsic: '_mm512_mask_cvtepi64_pd'.
// Requires AVX512DQ.
func MaskCvtepi64Pd(src M512d, k Mmask8, a M512i) M512d {
	return M512d(maskCvtepi64Pd([8]float64(src), uint8(k), [64]byte(a)))
}

func maskCvtepi64Pd(src [8]float64, k uint8, a [64]byte) [8]float64


// MaskzCvtepi64Pd: Convert packed 64-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTQQ2PD'. Intrinsic: '_mm512_maskz_cvtepi64_pd'.
// Requires AVX512DQ.
func MaskzCvtepi64Pd(k Mmask8, a M512i) M512d {
	return M512d(maskzCvtepi64Pd(uint8(k), [64]byte(a)))
}

func maskzCvtepi64Pd(k uint8, a [64]byte) [8]float64


// Cvtepi64Ps: Convert packed 64-bit integers in 'a' to packed single-precision
// (32-bit) floating-point elements, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTQQ2PS'. Intrinsic: '_mm512_cvtepi64_ps'.
// Requires AVX512DQ.
func Cvtepi64Ps(a M512i) M256 {
	return M256(cvtepi64Ps([64]byte(a)))
}

func cvtepi64Ps(a [64]byte) [8]float32


// MaskCvtepi64Ps: Convert packed 64-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := src[l+31:l]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTQQ2PS'. Intrinsic: '_mm512_mask_cvtepi64_ps'.
// Requires AVX512DQ.
func MaskCvtepi64Ps(src M256, k Mmask8, a M512i) M256 {
	return M256(maskCvtepi64Ps([8]float32(src), uint8(k), [64]byte(a)))
}

func maskCvtepi64Ps(src [8]float32, k uint8, a [64]byte) [8]float32


// MaskzCvtepi64Ps: Convert packed 64-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTQQ2PS'. Intrinsic: '_mm512_maskz_cvtepi64_ps'.
// Requires AVX512DQ.
func MaskzCvtepi64Ps(k Mmask8, a M512i) M256 {
	return M256(maskzCvtepi64Ps(uint8(k), [64]byte(a)))
}

func maskzCvtepi64Ps(k uint8, a [64]byte) [8]float32


// MaskCvtepi64StoreuEpi16: Convert packed 64-bit integers in 'a' to packed
// 16-bit integers with truncation, and store the active results (those with
// their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				MEM[base_addr+l+15:base_addr+l] := Truncate_Int64_To_Int16(a[i+63:i])
//			FI
//		ENDFOR
//
// Instruction: 'VPMOVQW'. Intrinsic: '_mm512_mask_cvtepi64_storeu_epi16'.
// Requires AVX512F.
func MaskCvtepi64StoreuEpi16(base_addr uintptr, k Mmask8, a M512i)  {
	maskCvtepi64StoreuEpi16(uintptr(base_addr), uint8(k), [64]byte(a))
}

func maskCvtepi64StoreuEpi16(base_addr uintptr, k uint8, a [64]byte) 


// MaskCvtepi64StoreuEpi32: Convert packed 64-bit integers in 'a' to packed
// 32-bit integers with truncation, and store the active results (those with
// their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				MEM[base_addr+l+31:base_addr+l] := Truncate_Int64_To_Int32(a[i+63:i])
//			FI
//		ENDFOR
//
// Instruction: 'VPMOVQD'. Intrinsic: '_mm512_mask_cvtepi64_storeu_epi32'.
// Requires AVX512F.
func MaskCvtepi64StoreuEpi32(base_addr uintptr, k Mmask8, a M512i)  {
	maskCvtepi64StoreuEpi32(uintptr(base_addr), uint8(k), [64]byte(a))
}

func maskCvtepi64StoreuEpi32(base_addr uintptr, k uint8, a [64]byte) 


// MaskCvtepi64StoreuEpi8: Convert packed 64-bit integers in 'a' to packed
// 8-bit integers with truncation, and store the active results (those with
// their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Truncate_Int64_To_Int8(a[i+63:i])
//			FI
//		ENDFOR
//
// Instruction: 'VPMOVQB'. Intrinsic: '_mm512_mask_cvtepi64_storeu_epi8'.
// Requires AVX512F.
func MaskCvtepi64StoreuEpi8(base_addr uintptr, k Mmask8, a M512i)  {
	maskCvtepi64StoreuEpi8(uintptr(base_addr), uint8(k), [64]byte(a))
}

func maskCvtepi64StoreuEpi8(base_addr uintptr, k uint8, a [64]byte) 


// Cvtepi8Epi16: Sign extend packed 8-bit integers in 'a' to packed 16-bit
// integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			l := j*16
//			dst[l+15:l] := SignExtend(a[i+7:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXBW'. Intrinsic: '_mm512_cvtepi8_epi16'.
// Requires AVX512BW.
func Cvtepi8Epi16(a M256i) M512i {
	return M512i(cvtepi8Epi16([32]byte(a)))
}

func cvtepi8Epi16(a [32]byte) [64]byte


// MaskCvtepi8Epi16: Sign extend packed 8-bit integers in 'a' to packed 16-bit
// integers, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			l := j*16
//			IF k[j]
//				dst[l+15:l] := SignExtend(a[i+7:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXBW'. Intrinsic: '_mm512_mask_cvtepi8_epi16'.
// Requires AVX512BW.
func MaskCvtepi8Epi16(src M512i, k Mmask32, a M256i) M512i {
	return M512i(maskCvtepi8Epi16([64]byte(src), uint32(k), [32]byte(a)))
}

func maskCvtepi8Epi16(src [64]byte, k uint32, a [32]byte) [64]byte


// MaskzCvtepi8Epi16: Sign extend packed 8-bit integers in 'a' to packed 16-bit
// integers, and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			l := j*16
//			IF k[j]
//				dst[l+15:l] := SignExtend(a[i+7:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXBW'. Intrinsic: '_mm512_maskz_cvtepi8_epi16'.
// Requires AVX512BW.
func MaskzCvtepi8Epi16(k Mmask32, a M256i) M512i {
	return M512i(maskzCvtepi8Epi16(uint32(k), [32]byte(a)))
}

func maskzCvtepi8Epi16(k uint32, a [32]byte) [64]byte


// Cvtepi8Epi32: Sign extend packed 8-bit integers in 'a' to packed 32-bit
// integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			k := 8*j
//			dst[i+31:i] := SignExtend(a[k+7:k])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXBD'. Intrinsic: '_mm512_cvtepi8_epi32'.
// Requires AVX512F.
func Cvtepi8Epi32(a M128i) M512i {
	return M512i(cvtepi8Epi32([16]byte(a)))
}

func cvtepi8Epi32(a [16]byte) [64]byte


// MaskCvtepi8Epi32: Sign extend packed 8-bit integers in 'a' to packed 32-bit
// integers, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[l+7:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXBD'. Intrinsic: '_mm512_mask_cvtepi8_epi32'.
// Requires AVX512F.
func MaskCvtepi8Epi32(src M512i, k Mmask16, a M128i) M512i {
	return M512i(maskCvtepi8Epi32([64]byte(src), uint16(k), [16]byte(a)))
}

func maskCvtepi8Epi32(src [64]byte, k uint16, a [16]byte) [64]byte


// MaskzCvtepi8Epi32: Sign extend packed 8-bit integers in 'a' to packed 32-bit
// integers, and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[l+7:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXBD'. Intrinsic: '_mm512_maskz_cvtepi8_epi32'.
// Requires AVX512F.
func MaskzCvtepi8Epi32(k Mmask16, a M128i) M512i {
	return M512i(maskzCvtepi8Epi32(uint16(k), [16]byte(a)))
}

func maskzCvtepi8Epi32(k uint16, a [16]byte) [64]byte


// Cvtepi8Epi64: Sign extend packed 8-bit integers in the low 8 bytes of 'a' to
// packed 64-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 8*j
//			dst[i+63:i] := SignExtend(a[k+7:k])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXBQ'. Intrinsic: '_mm512_cvtepi8_epi64'.
// Requires AVX512F.
func Cvtepi8Epi64(a M128i) M512i {
	return M512i(cvtepi8Epi64([16]byte(a)))
}

func cvtepi8Epi64(a [16]byte) [64]byte


// MaskCvtepi8Epi64: Sign extend packed 8-bit integers in the low 8 bytes of
// 'a' to packed 64-bit integers, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+7:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXBQ'. Intrinsic: '_mm512_mask_cvtepi8_epi64'.
// Requires AVX512F.
func MaskCvtepi8Epi64(src M512i, k Mmask8, a M128i) M512i {
	return M512i(maskCvtepi8Epi64([64]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi8Epi64(src [64]byte, k uint8, a [16]byte) [64]byte


// MaskzCvtepi8Epi64: Sign extend packed 8-bit integers in the low 8 bytes of
// 'a' to packed 64-bit integers, and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+7:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXBQ'. Intrinsic: '_mm512_maskz_cvtepi8_epi64'.
// Requires AVX512F.
func MaskzCvtepi8Epi64(k Mmask8, a M128i) M512i {
	return M512i(maskzCvtepi8Epi64(uint8(k), [16]byte(a)))
}

func maskzCvtepi8Epi64(k uint8, a [16]byte) [64]byte


// Cvtepu16Epi32: Zero extend packed unsigned 16-bit integers in 'a' to packed
// 32-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			k := 16*j
//			dst[i+31:i] := ZeroExtend(a[k+15:k])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXWD'. Intrinsic: '_mm512_cvtepu16_epi32'.
// Requires AVX512F.
func Cvtepu16Epi32(a M256i) M512i {
	return M512i(cvtepu16Epi32([32]byte(a)))
}

func cvtepu16Epi32(a [32]byte) [64]byte


// MaskCvtepu16Epi32: Zero extend packed unsigned 16-bit integers in 'a' to
// packed 32-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[l+15:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXWD'. Intrinsic: '_mm512_mask_cvtepu16_epi32'.
// Requires AVX512F.
func MaskCvtepu16Epi32(src M512i, k Mmask16, a M256i) M512i {
	return M512i(maskCvtepu16Epi32([64]byte(src), uint16(k), [32]byte(a)))
}

func maskCvtepu16Epi32(src [64]byte, k uint16, a [32]byte) [64]byte


// MaskzCvtepu16Epi32: Zero extend packed unsigned 16-bit integers in 'a' to
// packed 32-bit integers, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[l+15:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXWD'. Intrinsic: '_mm512_maskz_cvtepu16_epi32'.
// Requires AVX512F.
func MaskzCvtepu16Epi32(k Mmask16, a M256i) M512i {
	return M512i(maskzCvtepu16Epi32(uint16(k), [32]byte(a)))
}

func maskzCvtepu16Epi32(k uint16, a [32]byte) [64]byte


// Cvtepu16Epi64: Zero extend packed unsigned 16-bit integers in 'a' to packed
// 64-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 16*j
//			dst[i+63:i] := ZeroExtend(a[k+15:k])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXWQ'. Intrinsic: '_mm512_cvtepu16_epi64'.
// Requires AVX512F.
func Cvtepu16Epi64(a M128i) M512i {
	return M512i(cvtepu16Epi64([16]byte(a)))
}

func cvtepu16Epi64(a [16]byte) [64]byte


// MaskCvtepu16Epi64: Zero extend packed unsigned 16-bit integers in 'a' to
// packed 64-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+15:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXWQ'. Intrinsic: '_mm512_mask_cvtepu16_epi64'.
// Requires AVX512F.
func MaskCvtepu16Epi64(src M512i, k Mmask8, a M128i) M512i {
	return M512i(maskCvtepu16Epi64([64]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepu16Epi64(src [64]byte, k uint8, a [16]byte) [64]byte


// MaskzCvtepu16Epi64: Zero extend packed unsigned 16-bit integers in 'a' to
// packed 64-bit integers, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+15:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXWQ'. Intrinsic: '_mm512_maskz_cvtepu16_epi64'.
// Requires AVX512F.
func MaskzCvtepu16Epi64(k Mmask8, a M128i) M512i {
	return M512i(maskzCvtepu16Epi64(uint8(k), [16]byte(a)))
}

func maskzCvtepu16Epi64(k uint8, a [16]byte) [64]byte


// Cvtepu32Epi64: Zero extend packed unsigned 32-bit integers in 'a' to packed
// 64-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 32*j
//			dst[i+63:i] := ZeroExtend(a[k+31:k])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXDQ'. Intrinsic: '_mm512_cvtepu32_epi64'.
// Requires AVX512F.
func Cvtepu32Epi64(a M256i) M512i {
	return M512i(cvtepu32Epi64([32]byte(a)))
}

func cvtepu32Epi64(a [32]byte) [64]byte


// MaskCvtepu32Epi64: Zero extend packed unsigned 32-bit integers in 'a' to
// packed 64-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXDQ'. Intrinsic: '_mm512_mask_cvtepu32_epi64'.
// Requires AVX512F.
func MaskCvtepu32Epi64(src M512i, k Mmask8, a M256i) M512i {
	return M512i(maskCvtepu32Epi64([64]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtepu32Epi64(src [64]byte, k uint8, a [32]byte) [64]byte


// MaskzCvtepu32Epi64: Zero extend packed unsigned 32-bit integers in 'a' to
// packed 64-bit integers, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+31:l])
//			ELSE 
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXDQ'. Intrinsic: '_mm512_maskz_cvtepu32_epi64'.
// Requires AVX512F.
func MaskzCvtepu32Epi64(k Mmask8, a M256i) M512i {
	return M512i(maskzCvtepu32Epi64(uint8(k), [32]byte(a)))
}

func maskzCvtepu32Epi64(k uint8, a [32]byte) [64]byte


// Cvtepu32Pd: Convert packed unsigned 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			dst[i+63:i] := ConvertUnsignedIntegerTo_FP64(a[l+31:l])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTUDQ2PD'. Intrinsic: '_mm512_cvtepu32_pd'.
// Requires AVX512F.
func Cvtepu32Pd(a M256i) M512d {
	return M512d(cvtepu32Pd([32]byte(a)))
}

func cvtepu32Pd(a [32]byte) [8]float64


// MaskCvtepu32Pd: Convert packed unsigned 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := ConvertUnsignedIntegerTo_FP64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTUDQ2PD'. Intrinsic: '_mm512_mask_cvtepu32_pd'.
// Requires AVX512F.
func MaskCvtepu32Pd(src M512d, k Mmask8, a M256i) M512d {
	return M512d(maskCvtepu32Pd([8]float64(src), uint8(k), [32]byte(a)))
}

func maskCvtepu32Pd(src [8]float64, k uint8, a [32]byte) [8]float64


// MaskzCvtepu32Pd: Convert packed unsigned 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := ConvertUnsignedIntegerTo_FP64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTUDQ2PD'. Intrinsic: '_mm512_maskz_cvtepu32_pd'.
// Requires AVX512F.
func MaskzCvtepu32Pd(k Mmask8, a M256i) M512d {
	return M512d(maskzCvtepu32Pd(uint8(k), [32]byte(a)))
}

func maskzCvtepu32Pd(k uint8, a [32]byte) [8]float64


// Cvtepu32Ps: Convert packed unsigned 32-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := ConvertUnsignedInt32_To_FP32(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTUDQ2PS'. Intrinsic: '_mm512_cvtepu32_ps'.
// Requires AVX512F.
func Cvtepu32Ps(a M512i) M512 {
	return M512(cvtepu32Ps([64]byte(a)))
}

func cvtepu32Ps(a [64]byte) [16]float32


// MaskCvtepu32Ps: Convert packed unsigned 32-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ConvertUnsignedInt32_To_FP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTUDQ2PS'. Intrinsic: '_mm512_mask_cvtepu32_ps'.
// Requires AVX512F.
func MaskCvtepu32Ps(src M512, k Mmask16, a M512i) M512 {
	return M512(maskCvtepu32Ps([16]float32(src), uint16(k), [64]byte(a)))
}

func maskCvtepu32Ps(src [16]float32, k uint16, a [64]byte) [16]float32


// MaskzCvtepu32Ps: Convert packed unsigned 32-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := ConvertUnsignedInt32_To_FP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTUDQ2PS'. Intrinsic: '_mm512_maskz_cvtepu32_ps'.
// Requires AVX512F.
func MaskzCvtepu32Ps(k Mmask16, a M512i) M512 {
	return M512(maskzCvtepu32Ps(uint16(k), [64]byte(a)))
}

func maskzCvtepu32Ps(k uint16, a [64]byte) [16]float32


// Cvtepu32loPd: Performs element-by-element conversion of the lower half of
// packed 32-bit unsigned integer elements in 'v2' to packed double-precision
// (64-bit) floating-point elements, storing the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k := j*64
//			dst[k+63:k] := UInt32ToFloat64(v2[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTUDQ2PD'. Intrinsic: '_mm512_cvtepu32lo_pd'.
// Requires KNCNI.
func Cvtepu32loPd(v2 M512i) M512d {
	return M512d(cvtepu32loPd([64]byte(v2)))
}

func cvtepu32loPd(v2 [64]byte) [8]float64


// MaskCvtepu32loPd: Performs element-by-element conversion of the lower half
// of 32-bit unsigned integer elements in 'v2' to packed double-precision
// (64-bit) floating-point elements, storing the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			l := j*64
//			IF k[j]
//				dst[l+63:l] := UInt32ToFloat64(v2[i+31:i])
//			ELSE
//				dst[l+63:l] := src[l+63:l]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTUDQ2PD'. Intrinsic: '_mm512_mask_cvtepu32lo_pd'.
// Requires KNCNI.
func MaskCvtepu32loPd(src M512d, k Mmask8, v2 M512i) M512d {
	return M512d(maskCvtepu32loPd([8]float64(src), uint8(k), [64]byte(v2)))
}

func maskCvtepu32loPd(src [8]float64, k uint8, v2 [64]byte) [8]float64


// Cvtepu64Pd: Convert packed unsigned 64-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ConvertUnsignedInt64_To_FP64(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTUQQ2PD'. Intrinsic: '_mm512_cvtepu64_pd'.
// Requires AVX512DQ.
func Cvtepu64Pd(a M512i) M512d {
	return M512d(cvtepu64Pd([64]byte(a)))
}

func cvtepu64Pd(a [64]byte) [8]float64


// MaskCvtepu64Pd: Convert packed unsigned 64-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ConvertUnsignedInt64_To_FP64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTUQQ2PD'. Intrinsic: '_mm512_mask_cvtepu64_pd'.
// Requires AVX512DQ.
func MaskCvtepu64Pd(src M512d, k Mmask8, a M512i) M512d {
	return M512d(maskCvtepu64Pd([8]float64(src), uint8(k), [64]byte(a)))
}

func maskCvtepu64Pd(src [8]float64, k uint8, a [64]byte) [8]float64


// MaskzCvtepu64Pd: Convert packed unsigned 64-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ConvertUnsignedInt64_To_FP64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTUQQ2PD'. Intrinsic: '_mm512_maskz_cvtepu64_pd'.
// Requires AVX512DQ.
func MaskzCvtepu64Pd(k Mmask8, a M512i) M512d {
	return M512d(maskzCvtepu64Pd(uint8(k), [64]byte(a)))
}

func maskzCvtepu64Pd(k uint8, a [64]byte) [8]float64


// Cvtepu64Ps: Convert packed unsigned 64-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			dst[l+31:l] := ConvertUnsignedInt64_To_FP32(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTUQQ2PS'. Intrinsic: '_mm512_cvtepu64_ps'.
// Requires AVX512DQ.
func Cvtepu64Ps(a M512i) M256 {
	return M256(cvtepu64Ps([64]byte(a)))
}

func cvtepu64Ps(a [64]byte) [8]float32


// MaskCvtepu64Ps: Convert packed unsigned 64-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[l+31:l] := ConvertUnsignedInt64_To_FP32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := src[l+31:l]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTUQQ2PS'. Intrinsic: '_mm512_mask_cvtepu64_ps'.
// Requires AVX512DQ.
func MaskCvtepu64Ps(src M256, k Mmask8, a M512i) M256 {
	return M256(maskCvtepu64Ps([8]float32(src), uint8(k), [64]byte(a)))
}

func maskCvtepu64Ps(src [8]float32, k uint8, a [64]byte) [8]float32


// MaskzCvtepu64Ps: Convert packed unsigned 64-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[l+31:l] := ConvertUnsignedInt64_To_FP32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTUQQ2PS'. Intrinsic: '_mm512_maskz_cvtepu64_ps'.
// Requires AVX512DQ.
func MaskzCvtepu64Ps(k Mmask8, a M512i) M256 {
	return M256(maskzCvtepu64Ps(uint8(k), [64]byte(a)))
}

func maskzCvtepu64Ps(k uint8, a [64]byte) [8]float32


// Cvtepu8Epi16: Zero extend packed unsigned 8-bit integers in 'a' to packed
// 16-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			l := j*16
//			dst[l+15:l] := ZeroExtend(a[i+7:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXBW'. Intrinsic: '_mm512_cvtepu8_epi16'.
// Requires AVX512BW.
func Cvtepu8Epi16(a M256i) M512i {
	return M512i(cvtepu8Epi16([32]byte(a)))
}

func cvtepu8Epi16(a [32]byte) [64]byte


// MaskCvtepu8Epi16: Zero extend packed unsigned 8-bit integers in 'a' to
// packed 16-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			l := j*16
//			IF k[j]
//				dst[l+15:l] := ZeroExtend(a[i+7:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXBW'. Intrinsic: '_mm512_mask_cvtepu8_epi16'.
// Requires AVX512BW.
func MaskCvtepu8Epi16(src M512i, k Mmask32, a M256i) M512i {
	return M512i(maskCvtepu8Epi16([64]byte(src), uint32(k), [32]byte(a)))
}

func maskCvtepu8Epi16(src [64]byte, k uint32, a [32]byte) [64]byte


// MaskzCvtepu8Epi16: Zero extend packed unsigned 8-bit integers in 'a' to
// packed 16-bit integers, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			l := j*16
//			IF k[j]
//				dst[l+15:l] := ZeroExtend(a[i+7:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXBW'. Intrinsic: '_mm512_maskz_cvtepu8_epi16'.
// Requires AVX512BW.
func MaskzCvtepu8Epi16(k Mmask32, a M256i) M512i {
	return M512i(maskzCvtepu8Epi16(uint32(k), [32]byte(a)))
}

func maskzCvtepu8Epi16(k uint32, a [32]byte) [64]byte


// Cvtepu8Epi32: Zero extend packed unsigned 8-bit integers in 'a' to packed
// 32-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			k := 8*j
//			dst[i+31:i] := ZeroExtend(a[k+7:k])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXBD'. Intrinsic: '_mm512_cvtepu8_epi32'.
// Requires AVX512F.
func Cvtepu8Epi32(a M128i) M512i {
	return M512i(cvtepu8Epi32([16]byte(a)))
}

func cvtepu8Epi32(a [16]byte) [64]byte


// MaskCvtepu8Epi32: Zero extend packed unsigned 8-bit integers in 'a' to
// packed 32-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[l+7:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXBD'. Intrinsic: '_mm512_mask_cvtepu8_epi32'.
// Requires AVX512F.
func MaskCvtepu8Epi32(src M512i, k Mmask16, a M128i) M512i {
	return M512i(maskCvtepu8Epi32([64]byte(src), uint16(k), [16]byte(a)))
}

func maskCvtepu8Epi32(src [64]byte, k uint16, a [16]byte) [64]byte


// MaskzCvtepu8Epi32: Zero extend packed unsigned 8-bit integers in 'a' to
// packed 32-bit integers, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[l+7:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXBD'. Intrinsic: '_mm512_maskz_cvtepu8_epi32'.
// Requires AVX512F.
func MaskzCvtepu8Epi32(k Mmask16, a M128i) M512i {
	return M512i(maskzCvtepu8Epi32(uint16(k), [16]byte(a)))
}

func maskzCvtepu8Epi32(k uint16, a [16]byte) [64]byte


// Cvtepu8Epi64: Zero extend packed unsigned 8-bit integers in the low 8 byte
// sof 'a' to packed 64-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 8*j
//			dst[i+63:i] := ZeroExtend(a[k+7:k])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXBQ'. Intrinsic: '_mm512_cvtepu8_epi64'.
// Requires AVX512F.
func Cvtepu8Epi64(a M128i) M512i {
	return M512i(cvtepu8Epi64([16]byte(a)))
}

func cvtepu8Epi64(a [16]byte) [64]byte


// MaskCvtepu8Epi64: Zero extend packed unsigned 8-bit integers in the low 8
// bytes of 'a' to packed 64-bit integers, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+7:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXBQ'. Intrinsic: '_mm512_mask_cvtepu8_epi64'.
// Requires AVX512F.
func MaskCvtepu8Epi64(src M512i, k Mmask8, a M128i) M512i {
	return M512i(maskCvtepu8Epi64([64]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepu8Epi64(src [64]byte, k uint8, a [16]byte) [64]byte


// MaskzCvtepu8Epi64: Zero extend packed unsigned 8-bit integers in the low 8
// bytes of 'a' to packed 64-bit integers, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+7:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXBQ'. Intrinsic: '_mm512_maskz_cvtepu8_epi64'.
// Requires AVX512F.
func MaskzCvtepu8Epi64(k Mmask8, a M128i) M512i {
	return M512i(maskzCvtepu8Epi64(uint8(k), [16]byte(a)))
}

func maskzCvtepu8Epi64(k uint8, a [16]byte) [64]byte


// CvtfxpntRoundAdjustepi32Ps: Performs element-by-element conversion of packed
// 32-bit integer elements in 'v2' to packed single-precision (32-bit)
// floating-point elements and performing an optional exponent adjust using
// 'expadj', storing the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := Int32ToFloat32(v2[i+31:i])
//			CASE expadj OF
//			_MM_EXPADJ_NONE: dst[i+31:i] = dst[i+31:i] * 2**0
//			_MM_EXPADJ_4:	dst[i+31:i] = dst[i+31:i] * 2**4
//			_MM_EXPADJ_5:	dst[i+31:i] = dst[i+31:i] * 2**5
//			_MM_EXPADJ_8:	dst[i+31:i] = dst[i+31:i] * 2**8
//			_MM_EXPADJ_16:   dst[i+31:i] = dst[i+31:i] * 2**16
//			_MM_EXPADJ_24:   dst[i+31:i] = dst[i+31:i] * 2**24
//			_MM_EXPADJ_31:   dst[i+31:i] = dst[i+31:i] * 2**31
//			_MM_EXPADJ_32:   dst[i+31:i] = dst[i+31:i] * 2**32
//			ESAC
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTFXPNTDQ2PS'. Intrinsic: '_mm512_cvtfxpnt_round_adjustepi32_ps'.
// Requires KNCNI.
func CvtfxpntRoundAdjustepi32Ps(v2 M512i, rounding int, expadj MMEXPADJENUM) M512 {
	return M512(cvtfxpntRoundAdjustepi32Ps([64]byte(v2), rounding, expadj))
}

func cvtfxpntRoundAdjustepi32Ps(v2 [64]byte, rounding int, expadj MMEXPADJENUM) [16]float32


// CvtfxpntRoundAdjustepu32Ps: Performs element-by-element conversion of packed
// 32-bit unsigned integer elements in 'v2' to packed single-precision (32-bit)
// floating-point elements and performing an optional exponent adjust using
// 'expadj', storing the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := UInt32ToFloat32(v2[i+31:i])
//			CASE expadj OF
//			_MM_EXPADJ_NONE: dst[i+31:i] = dst[i+31:i] * 2**0
//			_MM_EXPADJ_4:	 dst[i+31:i] = dst[i+31:i] * 2**4
//			_MM_EXPADJ_5:	 dst[i+31:i] = dst[i+31:i] * 2**5
//			_MM_EXPADJ_8:	 dst[i+31:i] = dst[i+31:i] * 2**8
//			_MM_EXPADJ_16:   dst[i+31:i] = dst[i+31:i] * 2**16
//			_MM_EXPADJ_24:   dst[i+31:i] = dst[i+31:i] * 2**24
//			_MM_EXPADJ_31:   dst[i+31:i] = dst[i+31:i] * 2**31
//			_MM_EXPADJ_32:   dst[i+31:i] = dst[i+31:i] * 2**32
//			ESAC
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTFXPNTUDQ2PS'. Intrinsic: '_mm512_cvtfxpnt_round_adjustepu32_ps'.
// Requires KNCNI.
func CvtfxpntRoundAdjustepu32Ps(v2 M512i, rounding int, expadj MMEXPADJENUM) M512 {
	return M512(cvtfxpntRoundAdjustepu32Ps([64]byte(v2), rounding, expadj))
}

func cvtfxpntRoundAdjustepu32Ps(v2 [64]byte, rounding int, expadj MMEXPADJENUM) [16]float32


// MaskCvtfxpntRoundAdjustepu32Ps: Performs element-by-element conversion of
// packed 32-bit unsigned integer elements in 'v2' to packed single-precision
// (32-bit) floating-point elements and performing an optional exponent adjust
// using 'expadj', storing the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := Int32ToFloat32(v2[i+31:i])
//				CASE expadj OF
//				_MM_EXPADJ_NONE: dst[i+31:i] = dst[i+31:i] * 2**0
//				_MM_EXPADJ_4:	 dst[i+31:i] = dst[i+31:i] * 2**4
//				_MM_EXPADJ_5:	 dst[i+31:i] = dst[i+31:i] * 2**5
//				_MM_EXPADJ_8:	 dst[i+31:i] = dst[i+31:i] * 2**8
//				_MM_EXPADJ_16:   dst[i+31:i] = dst[i+31:i] * 2**16
//				_MM_EXPADJ_24:   dst[i+31:i] = dst[i+31:i] * 2**24
//				_MM_EXPADJ_31:   dst[i+31:i] = dst[i+31:i] * 2**31
//				_MM_EXPADJ_32:   dst[i+31:i] = dst[i+31:i] * 2**32
//				ESAC
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTFXPNTUDQ2PS'. Intrinsic: '_mm512_mask_cvtfxpnt_round_adjustepu32_ps'.
// Requires KNCNI.
func MaskCvtfxpntRoundAdjustepu32Ps(src M512, k Mmask16, v2 M512i, rounding int, expadj MMEXPADJENUM) M512 {
	return M512(maskCvtfxpntRoundAdjustepu32Ps([16]float32(src), uint16(k), [64]byte(v2), rounding, expadj))
}

func maskCvtfxpntRoundAdjustepu32Ps(src [16]float32, k uint16, v2 [64]byte, rounding int, expadj MMEXPADJENUM) [16]float32


// CvtfxpntRoundAdjustpsEpi32: Performs element-by-element conversion of packed
// single-precision (32-bit) floating-point elements in 'v2' to packed 32-bit
// integer elements and performs an optional exponent adjust using 'expadj',
// storing the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := Float32ToInt32(v2[i+31:i])
//			CASE expadj OF
//			_MM_EXPADJ_NONE: dst[i+31:i] = dst[i+31:i] * 2**0
//			_MM_EXPADJ_4:	 dst[i+31:i] = dst[i+31:i] * 2**4
//			_MM_EXPADJ_5:	 dst[i+31:i] = dst[i+31:i] * 2**5
//			_MM_EXPADJ_8:	 dst[i+31:i] = dst[i+31:i] * 2**8
//			_MM_EXPADJ_16:   dst[i+31:i] = dst[i+31:i] * 2**16
//			_MM_EXPADJ_24:   dst[i+31:i] = dst[i+31:i] * 2**24
//			_MM_EXPADJ_31:   dst[i+31:i] = dst[i+31:i] * 2**31
//			_MM_EXPADJ_32:   dst[i+31:i] = dst[i+31:i] * 2**32
//			ESAC
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTFXPNTPS2DQ'. Intrinsic: '_mm512_cvtfxpnt_round_adjustps_epi32'.
// Requires KNCNI.
func CvtfxpntRoundAdjustpsEpi32(v2 M512, rounding int, expadj MMEXPADJENUM) M512i {
	return M512i(cvtfxpntRoundAdjustpsEpi32([16]float32(v2), rounding, expadj))
}

func cvtfxpntRoundAdjustpsEpi32(v2 [16]float32, rounding int, expadj MMEXPADJENUM) [64]byte


// CvtfxpntRoundAdjustpsEpu32: Performs element-by-element conversion of packed
// single-precision (32-bit) floating-point elements in 'v2' to packed 32-bit
// unsigned integer elements and performing an optional exponent adjust using
// 'expadj', storing the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := Float32ToUInt32(v2[i+31:i])
//			CASE expadj OF
//			_MM_EXPADJ_NONE: dst[i+31:i] = dst[i+31:i]  0
//			_MM_EXPADJ_4:	 dst[i+31:i] = dst[i+31:i]  4
//			_MM_EXPADJ_5:	 dst[i+31:i] = dst[i+31:i]  5
//			_MM_EXPADJ_8:	 dst[i+31:i] = dst[i+31:i]  8
//			_MM_EXPADJ_16:   dst[i+31:i] = dst[i+31:i]  16
//			_MM_EXPADJ_24:   dst[i+31:i] = dst[i+31:i]  24
//			_MM_EXPADJ_31:   dst[i+31:i] = dst[i+31:i]  31
//			_MM_EXPADJ_32:   dst[i+31:i] = dst[i+31:i]  32
//			ESAC
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTFXPNTPS2UDQ'. Intrinsic: '_mm512_cvtfxpnt_round_adjustps_epu32'.
// Requires KNCNI.
func CvtfxpntRoundAdjustpsEpu32(v2 M512, rounding int, expadj MMEXPADJENUM) M512i {
	return M512i(cvtfxpntRoundAdjustpsEpu32([16]float32(v2), rounding, expadj))
}

func cvtfxpntRoundAdjustpsEpu32(v2 [16]float32, rounding int, expadj MMEXPADJENUM) [64]byte


// CvtfxpntRoundpdEpi32lo: Performs an element-by-element conversion of
// elements in packed double-precision (64-bit) floating-point vector 'v2' to
// 32-bit integer elements, storing them in the lower half of 'dst'. The
// elements in the upper half of 'dst' are set to 0.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			k := j*32
//			dst[k+31:k] := Float64ToInt32(v2[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTFXPNTPD2DQ'. Intrinsic: '_mm512_cvtfxpnt_roundpd_epi32lo'.
// Requires KNCNI.
func CvtfxpntRoundpdEpi32lo(v2 M512d, rounding int) M512i {
	return M512i(cvtfxpntRoundpdEpi32lo([8]float64(v2), rounding))
}

func cvtfxpntRoundpdEpi32lo(v2 [8]float64, rounding int) [64]byte


// MaskCvtfxpntRoundpdEpi32lo: Performs an element-by-element conversion of
// elements in packed double-precision (64-bit) floating-point vector 'v2' to
// 32-bit integer elements, storing them in the lower half of 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). The elements in the upper half of 'dst' are set to 0.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[l+31:l] := Float64ToInt32(v2[i+63:i])
//			ELSE
//				dst[l+31:l] := src[l+31:l]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTFXPNTPD2DQ'. Intrinsic: '_mm512_mask_cvtfxpnt_roundpd_epi32lo'.
// Requires KNCNI.
func MaskCvtfxpntRoundpdEpi32lo(src M512i, k Mmask8, v2 M512d, rounding int) M512i {
	return M512i(maskCvtfxpntRoundpdEpi32lo([64]byte(src), uint8(k), [8]float64(v2), rounding))
}

func maskCvtfxpntRoundpdEpi32lo(src [64]byte, k uint8, v2 [8]float64, rounding int) [64]byte


// CvtfxpntRoundpdEpu32lo: Performs element-by-element conversion of packed
// double-precision (64-bit) floating-point elements in 'v2' to packed 32-bit
// unsigned integer elements, storing the results in 'dst'. Results are written
// to the lower half of 'dst', and the upper half locations are set to '0'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			k := j*32
//			dst[k+31:k] := Float64ToInt32(v2[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTFXPNTPD2UDQ'. Intrinsic: '_mm512_cvtfxpnt_roundpd_epu32lo'.
// Requires KNCNI.
func CvtfxpntRoundpdEpu32lo(v2 M512d, rounding int) M512i {
	return M512i(cvtfxpntRoundpdEpu32lo([8]float64(v2), rounding))
}

func cvtfxpntRoundpdEpu32lo(v2 [8]float64, rounding int) [64]byte


// MaskCvtfxpntRoundpdEpu32lo: Performs element-by-element conversion of packed
// double-precision (64-bit) floating-point elements in 'v2' to packed 32-bit
// unsigned integer elements, storing the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set).
// Results are written to the lower half of 'dst', and the upper half locations
// are set to '0'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[l+31:l] := Float64ToInt32(v2[i+63:i])
//			ELSE
//				dst[l+31:l] := src[l+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTFXPNTPD2UDQ'. Intrinsic: '_mm512_mask_cvtfxpnt_roundpd_epu32lo'.
// Requires KNCNI.
func MaskCvtfxpntRoundpdEpu32lo(src M512i, k Mmask8, v2 M512d, rounding int) M512i {
	return M512i(maskCvtfxpntRoundpdEpu32lo([64]byte(src), uint8(k), [8]float64(v2), rounding))
}

func maskCvtfxpntRoundpdEpu32lo(src [64]byte, k uint8, v2 [8]float64, rounding int) [64]byte


// CvtpdEpi32: Convert packed double-precision (64-bit) floating-point elements
// in 'a' to packed 32-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_Int32(a[k+63:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2DQ'. Intrinsic: '_mm512_cvtpd_epi32'.
// Requires AVX512F.
func CvtpdEpi32(a M512d) M256i {
	return M256i(cvtpdEpi32([8]float64(a)))
}

func cvtpdEpi32(a [8]float64) [32]byte


// MaskCvtpdEpi32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			l := j*64
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_Int32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2DQ'. Intrinsic: '_mm512_mask_cvtpd_epi32'.
// Requires AVX512F.
func MaskCvtpdEpi32(src M256i, k Mmask8, a M512d) M256i {
	return M256i(maskCvtpdEpi32([32]byte(src), uint8(k), [8]float64(a)))
}

func maskCvtpdEpi32(src [32]byte, k uint8, a [8]float64) [32]byte


// MaskzCvtpdEpi32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_Int32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2DQ'. Intrinsic: '_mm512_maskz_cvtpd_epi32'.
// Requires AVX512F.
func MaskzCvtpdEpi32(k Mmask8, a M512d) M256i {
	return M256i(maskzCvtpdEpi32(uint8(k), [8]float64(a)))
}

func maskzCvtpdEpi32(k uint8, a [8]float64) [32]byte


// CvtpdEpi64: Convert packed double-precision (64-bit) floating-point elements
// in 'a' to packed 64-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := Convert_FP64_To_Int64(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPD2QQ'. Intrinsic: '_mm512_cvtpd_epi64'.
// Requires AVX512DQ.
func CvtpdEpi64(a M512d) M512i {
	return M512i(cvtpdEpi64([8]float64(a)))
}

func cvtpdEpi64(a [8]float64) [64]byte


// MaskCvtpdEpi64: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 64-bit integers, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_FP64_To_Int64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPD2QQ'. Intrinsic: '_mm512_mask_cvtpd_epi64'.
// Requires AVX512DQ.
func MaskCvtpdEpi64(src M512i, k Mmask8, a M512d) M512i {
	return M512i(maskCvtpdEpi64([64]byte(src), uint8(k), [8]float64(a)))
}

func maskCvtpdEpi64(src [64]byte, k uint8, a [8]float64) [64]byte


// MaskzCvtpdEpi64: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 64-bit integers, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_FP64_To_Int64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPD2QQ'. Intrinsic: '_mm512_maskz_cvtpd_epi64'.
// Requires AVX512DQ.
func MaskzCvtpdEpi64(k Mmask8, a M512d) M512i {
	return M512i(maskzCvtpdEpi64(uint8(k), [8]float64(a)))
}

func maskzCvtpdEpi64(k uint8, a [8]float64) [64]byte


// CvtpdEpu32: Convert packed double-precision (64-bit) floating-point elements
// in 'a' to packed unsigned 32-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[k+63:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2UDQ'. Intrinsic: '_mm512_cvtpd_epu32'.
// Requires AVX512F.
func CvtpdEpu32(a M512d) M256i {
	return M256i(cvtpdEpu32([8]float64(a)))
}

func cvtpdEpu32(a [8]float64) [32]byte


// MaskCvtpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			l := j*64
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2UDQ'. Intrinsic: '_mm512_mask_cvtpd_epu32'.
// Requires AVX512F.
func MaskCvtpdEpu32(src M256i, k Mmask8, a M512d) M256i {
	return M256i(maskCvtpdEpu32([32]byte(src), uint8(k), [8]float64(a)))
}

func maskCvtpdEpu32(src [32]byte, k uint8, a [8]float64) [32]byte


// MaskzCvtpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2UDQ'. Intrinsic: '_mm512_maskz_cvtpd_epu32'.
// Requires AVX512F.
func MaskzCvtpdEpu32(k Mmask8, a M512d) M256i {
	return M256i(maskzCvtpdEpu32(uint8(k), [8]float64(a)))
}

func maskzCvtpdEpu32(k uint8, a [8]float64) [32]byte


// CvtpdEpu64: Convert packed double-precision (64-bit) floating-point elements
// in 'a' to packed unsigned 64-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := Convert_FP64_To_UnsignedInt64(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPD2UQQ'. Intrinsic: '_mm512_cvtpd_epu64'.
// Requires AVX512DQ.
func CvtpdEpu64(a M512d) M512i {
	return M512i(cvtpdEpu64([8]float64(a)))
}

func cvtpdEpu64(a [8]float64) [64]byte


// MaskCvtpdEpu64: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 64-bit integers, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_FP64_To_UnsignedInt64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPD2UQQ'. Intrinsic: '_mm512_mask_cvtpd_epu64'.
// Requires AVX512DQ.
func MaskCvtpdEpu64(src M512i, k Mmask8, a M512d) M512i {
	return M512i(maskCvtpdEpu64([64]byte(src), uint8(k), [8]float64(a)))
}

func maskCvtpdEpu64(src [64]byte, k uint8, a [8]float64) [64]byte


// MaskzCvtpdEpu64: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 64-bit integers, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_FP64_To_UnsignedInt64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPD2UQQ'. Intrinsic: '_mm512_maskz_cvtpd_epu64'.
// Requires AVX512DQ.
func MaskzCvtpdEpu64(k Mmask8, a M512d) M512i {
	return M512i(maskzCvtpdEpu64(uint8(k), [8]float64(a)))
}

func maskzCvtpdEpu64(k uint8, a [8]float64) [64]byte


// CvtpdPs: Convert packed double-precision (64-bit) floating-point elements in
// 'a' to packed single-precision (32-bit) floating-point elements, and store
// the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_FP32(a[k+63:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2PS'. Intrinsic: '_mm512_cvtpd_ps'.
// Requires AVX512F.
func CvtpdPs(a M512d) M256 {
	return M256(cvtpdPs([8]float64(a)))
}

func cvtpdPs(a [8]float64) [8]float32


// MaskCvtpdPs: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed single-precision (32-bit) floating-point elements,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_FP32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2PS'. Intrinsic: '_mm512_mask_cvtpd_ps'.
// Requires AVX512F.
func MaskCvtpdPs(src M256, k Mmask8, a M512d) M256 {
	return M256(maskCvtpdPs([8]float32(src), uint8(k), [8]float64(a)))
}

func maskCvtpdPs(src [8]float32, k uint8, a [8]float64) [8]float32


// MaskzCvtpdPs: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed single-precision (32-bit) floating-point elements,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			l := j*64
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_FP32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2PS'. Intrinsic: '_mm512_maskz_cvtpd_ps'.
// Requires AVX512F.
func MaskzCvtpdPs(k Mmask8, a M512d) M256 {
	return M256(maskzCvtpdPs(uint8(k), [8]float64(a)))
}

func maskzCvtpdPs(k uint8, a [8]float64) [8]float32


// CvtpdPslo: Performs an element-by-element conversion of packed
// double-precision (64-bit) floating-point elements in 'v2' to
// single-precision (32-bit) floating-point elements and stores them in 'dst'.
// The elements are stored in the lower half of the results vector, while the
// remaining upper half locations are set to 0. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k := j*32
//			dst[k+31:k] := Float64ToFloat32(v2[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPD2PS'. Intrinsic: '_mm512_cvtpd_pslo'.
// Requires KNCNI.
func CvtpdPslo(v2 M512d) M512 {
	return M512(cvtpdPslo([8]float64(v2)))
}

func cvtpdPslo(v2 [8]float64) [16]float32


// MaskCvtpdPslo: Performs an element-by-element conversion of packed
// double-precision (64-bit) floating-point elements in 'v2' to
// single-precision (32-bit) floating-point elements and stores them in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). The elements are stored in the lower half of the
// results vector, while the remaining upper half locations are set to 0. 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[l+31:l] := Float64ToFloat32(v2[i+63:i])
//			ELSE
//				dst[l+31:l] := src[l+31:l]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPD2PS'. Intrinsic: '_mm512_mask_cvtpd_pslo'.
// Requires KNCNI.
func MaskCvtpdPslo(src M512, k Mmask8, v2 M512d) M512 {
	return M512(maskCvtpdPslo([16]float32(src), uint8(k), [8]float64(v2)))
}

func maskCvtpdPslo(src [16]float32, k uint8, v2 [8]float64) [16]float32


// CvtphPs: Convert packed half-precision (16-bit) floating-point elements in
// 'a' to packed single-precision (32-bit) floating-point elements, and store
// the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			m := j*16
//			dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPH2PS'. Intrinsic: '_mm512_cvtph_ps'.
// Requires AVX512F.
func CvtphPs(a M256i) M512 {
	return M512(cvtphPs([32]byte(a)))
}

func cvtphPs(a [32]byte) [16]float32


// MaskCvtphPs: Convert packed half-precision (16-bit) floating-point elements
// in 'a' to packed single-precision (32-bit) floating-point elements, and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			m := j*16
//			IF k[j]
//				dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPH2PS'. Intrinsic: '_mm512_mask_cvtph_ps'.
// Requires AVX512F.
func MaskCvtphPs(src M512, k Mmask16, a M256i) M512 {
	return M512(maskCvtphPs([16]float32(src), uint16(k), [32]byte(a)))
}

func maskCvtphPs(src [16]float32, k uint16, a [32]byte) [16]float32


// MaskzCvtphPs: Convert packed half-precision (16-bit) floating-point elements
// in 'a' to packed single-precision (32-bit) floating-point elements, and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			m := j*16
//			IF k[j]
//				dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPH2PS'. Intrinsic: '_mm512_maskz_cvtph_ps'.
// Requires AVX512F.
func MaskzCvtphPs(k Mmask16, a M256i) M512 {
	return M512(maskzCvtphPs(uint16(k), [32]byte(a)))
}

func maskzCvtphPs(k uint16, a [32]byte) [16]float32


// CvtpsEpi32: Convert packed single-precision (32-bit) floating-point elements
// in 'a' to packed 32-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2DQ'. Intrinsic: '_mm512_cvtps_epi32'.
// Requires AVX512F.
func CvtpsEpi32(a M512) M512i {
	return M512i(cvtpsEpi32([16]float32(a)))
}

func cvtpsEpi32(a [16]float32) [64]byte


// MaskCvtpsEpi32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2DQ'. Intrinsic: '_mm512_mask_cvtps_epi32'.
// Requires AVX512F.
func MaskCvtpsEpi32(src M512i, k Mmask16, a M512) M512i {
	return M512i(maskCvtpsEpi32([64]byte(src), uint16(k), [16]float32(a)))
}

func maskCvtpsEpi32(src [64]byte, k uint16, a [16]float32) [64]byte


// MaskzCvtpsEpi32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2DQ'. Intrinsic: '_mm512_maskz_cvtps_epi32'.
// Requires AVX512F.
func MaskzCvtpsEpi32(k Mmask16, a M512) M512i {
	return M512i(maskzCvtpsEpi32(uint16(k), [16]float32(a)))
}

func maskzCvtpsEpi32(k uint16, a [16]float32) [64]byte


// CvtpsEpi64: Convert packed single-precision (32-bit) floating-point elements
// in 'a' to packed 64-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			dst[i+63:i] := Convert_FP32_To_Int64(a[l+31:l])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2QQ'. Intrinsic: '_mm512_cvtps_epi64'.
// Requires AVX512DQ.
func CvtpsEpi64(a M256) M512i {
	return M512i(cvtpsEpi64([8]float32(a)))
}

func cvtpsEpi64(a [8]float32) [64]byte


// MaskCvtpsEpi64: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 64-bit integers, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_Int64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2QQ'. Intrinsic: '_mm512_mask_cvtps_epi64'.
// Requires AVX512DQ.
func MaskCvtpsEpi64(src M512i, k Mmask8, a M256) M512i {
	return M512i(maskCvtpsEpi64([64]byte(src), uint8(k), [8]float32(a)))
}

func maskCvtpsEpi64(src [64]byte, k uint8, a [8]float32) [64]byte


// MaskzCvtpsEpi64: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 64-bit integers, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_Int64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2QQ'. Intrinsic: '_mm512_maskz_cvtps_epi64'.
// Requires AVX512DQ.
func MaskzCvtpsEpi64(k Mmask8, a M256) M512i {
	return M512i(maskzCvtpsEpi64(uint8(k), [8]float32(a)))
}

func maskzCvtpsEpi64(k uint8, a [8]float32) [64]byte


// CvtpsEpu32: Convert packed single-precision (32-bit) floating-point elements
// in 'a' to packed unsigned 32-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2UDQ'. Intrinsic: '_mm512_cvtps_epu32'.
// Requires AVX512F.
func CvtpsEpu32(a M512) M512i {
	return M512i(cvtpsEpu32([16]float32(a)))
}

func cvtpsEpu32(a [16]float32) [64]byte


// MaskCvtpsEpu32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2UDQ'. Intrinsic: '_mm512_mask_cvtps_epu32'.
// Requires AVX512F.
func MaskCvtpsEpu32(src M512i, k Mmask16, a M512) M512i {
	return M512i(maskCvtpsEpu32([64]byte(src), uint16(k), [16]float32(a)))
}

func maskCvtpsEpu32(src [64]byte, k uint16, a [16]float32) [64]byte


// MaskzCvtpsEpu32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2UDQ'. Intrinsic: '_mm512_maskz_cvtps_epu32'.
// Requires AVX512F.
func MaskzCvtpsEpu32(k Mmask16, a M512) M512i {
	return M512i(maskzCvtpsEpu32(uint16(k), [16]float32(a)))
}

func maskzCvtpsEpu32(k uint16, a [16]float32) [64]byte


// CvtpsEpu64: Convert packed single-precision (32-bit) floating-point elements
// in 'a' to packed unsigned 64-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			dst[i+63:i] := Convert_FP32_To_UnsignedInt64(a[l+31:l])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2UQQ'. Intrinsic: '_mm512_cvtps_epu64'.
// Requires AVX512DQ.
func CvtpsEpu64(a M256) M512i {
	return M512i(cvtpsEpu64([8]float32(a)))
}

func cvtpsEpu64(a [8]float32) [64]byte


// MaskCvtpsEpu64: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 64-bit integers, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_UnsignedInt64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2UQQ'. Intrinsic: '_mm512_mask_cvtps_epu64'.
// Requires AVX512DQ.
func MaskCvtpsEpu64(src M512i, k Mmask8, a M256) M512i {
	return M512i(maskCvtpsEpu64([64]byte(src), uint8(k), [8]float32(a)))
}

func maskCvtpsEpu64(src [64]byte, k uint8, a [8]float32) [64]byte


// MaskzCvtpsEpu64: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 64-bit integers, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_UnsignedInt64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2UQQ'. Intrinsic: '_mm512_maskz_cvtps_epu64'.
// Requires AVX512DQ.
func MaskzCvtpsEpu64(k Mmask8, a M256) M512i {
	return M512i(maskzCvtpsEpu64(uint8(k), [8]float32(a)))
}

func maskzCvtpsEpu64(k uint8, a [8]float32) [64]byte


// CvtpsPd: Convert packed single-precision (32-bit) floating-point elements in
// 'a' to packed double-precision (64-bit) floating-point elements, and store
// the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 32*j
//			dst[i+63:i] := Convert_FP32_To_FP64(a[k+31:k])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2PD'. Intrinsic: '_mm512_cvtps_pd'.
// Requires AVX512F.
func CvtpsPd(a M256) M512d {
	return M512d(cvtpsPd([8]float32(a)))
}

func cvtpsPd(a [8]float32) [8]float64


// MaskCvtpsPd: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed double-precision (64-bit) floating-point elements,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_FP64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2PD'. Intrinsic: '_mm512_mask_cvtps_pd'.
// Requires AVX512F.
func MaskCvtpsPd(src M512d, k Mmask8, a M256) M512d {
	return M512d(maskCvtpsPd([8]float64(src), uint8(k), [8]float32(a)))
}

func maskCvtpsPd(src [8]float64, k uint8, a [8]float32) [8]float64


// MaskzCvtpsPd: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed double-precision (64-bit) floating-point elements,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_FP64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2PD'. Intrinsic: '_mm512_maskz_cvtps_pd'.
// Requires AVX512F.
func MaskzCvtpsPd(k Mmask8, a M256) M512d {
	return M512d(maskzCvtpsPd(uint8(k), [8]float32(a)))
}

func maskzCvtpsPd(k uint8, a [8]float32) [8]float64


// CvtpsPh: Convert packed single-precision (32-bit) floating-point elements in
// 'a' to packed half-precision (16-bit) floating-point elements, and store the
// results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 16*j
//			l := 32*j
//			dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm512_cvtps_ph'.
// Requires AVX512F.
func CvtpsPh(a M512, rounding int) M256i {
	return M256i(cvtpsPh([16]float32(a), rounding))
}

func cvtpsPh(a [16]float32, rounding int) [32]byte


// MaskCvtpsPh: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed half-precision (16-bit) floating-point elements,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 16*j
//			l := 32*j
//			IF k[j]
//				dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm512_mask_cvtps_ph'.
// Requires AVX512F.
func MaskCvtpsPh(src M256i, k Mmask16, a M512, rounding int) M256i {
	return M256i(maskCvtpsPh([32]byte(src), uint16(k), [16]float32(a), rounding))
}

func maskCvtpsPh(src [32]byte, k uint16, a [16]float32, rounding int) [32]byte


// MaskzCvtpsPh: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed half-precision (16-bit) floating-point elements,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 16*j
//			l := 32*j
//			IF k[j]
//				dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm512_maskz_cvtps_ph'.
// Requires AVX512F.
func MaskzCvtpsPh(k Mmask16, a M512, rounding int) M256i {
	return M256i(maskzCvtpsPh(uint16(k), [16]float32(a), rounding))
}

func maskzCvtpsPh(k uint16, a [16]float32, rounding int) [32]byte


// CvtpsloPd: Performs element-by-element conversion of the lower half of
// packed single-precision (32-bit) floating-point elements in 'v2' to packed
// double-precision (64-bit) floating-point elements, storing the results in
// 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k := j*64
//			dst[k+63:k] := Float32ToFloat64(v2[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2PD'. Intrinsic: '_mm512_cvtpslo_pd'.
// Requires KNCNI.
func CvtpsloPd(v2 M512) M512d {
	return M512d(cvtpsloPd([16]float32(v2)))
}

func cvtpsloPd(v2 [16]float32) [8]float64


// MaskCvtpsloPd: Performs element-by-element conversion of the lower half of
// packed single-precision (32-bit) floating-point elements in 'v2' to packed
// double-precision (64-bit) floating-point elements, storing the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			l := j*64
//			IF k[j]
//				dst[l+63:l] := Float32ToFloat64(v2[i+31:i])
//			ELSE
//				dst[l+63:l] := src[l+63:l]:
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2PD'. Intrinsic: '_mm512_mask_cvtpslo_pd'.
// Requires KNCNI.
func MaskCvtpsloPd(src M512d, k Mmask8, v2 M512) M512d {
	return M512d(maskCvtpsloPd([8]float64(src), uint8(k), [16]float32(v2)))
}

func maskCvtpsloPd(src [8]float64, k uint8, v2 [16]float32) [8]float64


// Cvtsepi16Epi8: Convert packed 16-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := 16*j
//			l := 8*j
//			dst[l+7:l] := Saturate_Int16_To_Int8(a[i+15:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSWB'. Intrinsic: '_mm512_cvtsepi16_epi8'.
// Requires AVX512BW.
func Cvtsepi16Epi8(a M512i) M256i {
	return M256i(cvtsepi16Epi8([64]byte(a)))
}

func cvtsepi16Epi8(a [64]byte) [32]byte


// MaskCvtsepi16Epi8: Convert packed 16-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 31
//			i := 16*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int16_To_Int8(a[i+15:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSWB'. Intrinsic: '_mm512_mask_cvtsepi16_epi8'.
// Requires AVX512BW.
func MaskCvtsepi16Epi8(src M256i, k Mmask32, a M512i) M256i {
	return M256i(maskCvtsepi16Epi8([32]byte(src), uint32(k), [64]byte(a)))
}

func maskCvtsepi16Epi8(src [32]byte, k uint32, a [64]byte) [32]byte


// MaskzCvtsepi16Epi8: Convert packed 16-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 31
//			i := 16*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int16_To_Int8(a[i+15:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSWB'. Intrinsic: '_mm512_maskz_cvtsepi16_epi8'.
// Requires AVX512BW.
func MaskzCvtsepi16Epi8(k Mmask32, a M512i) M256i {
	return M256i(maskzCvtsepi16Epi8(uint32(k), [64]byte(a)))
}

func maskzCvtsepi16Epi8(k uint32, a [64]byte) [32]byte


// MaskCvtsepi16StoreuEpi8: Convert packed 16-bit integers in 'a' to packed
// 8-bit integers with signed saturation, and store the active results (those
// with their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 31
//			i := 16*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Saturate_Int16_To_Int8(a[i+15:i])
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSWB'. Intrinsic: '_mm512_mask_cvtsepi16_storeu_epi8'.
// Requires AVX512BW.
func MaskCvtsepi16StoreuEpi8(base_addr uintptr, k Mmask32, a M512i)  {
	maskCvtsepi16StoreuEpi8(uintptr(base_addr), uint32(k), [64]byte(a))
}

func maskCvtsepi16StoreuEpi8(base_addr uintptr, k uint32, a [64]byte) 


// Cvtsepi32Epi16: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			k := 16*j
//			dst[k+15:k] := Saturate_Int32_To_Int16(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSDW'. Intrinsic: '_mm512_cvtsepi32_epi16'.
// Requires AVX512F.
func Cvtsepi32Epi16(a M512i) M256i {
	return M256i(cvtsepi32Epi16([64]byte(a)))
}

func cvtsepi32Epi16(a [64]byte) [32]byte


// MaskCvtsepi32Epi16: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_Int32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSDW'. Intrinsic: '_mm512_mask_cvtsepi32_epi16'.
// Requires AVX512F.
func MaskCvtsepi32Epi16(src M256i, k Mmask16, a M512i) M256i {
	return M256i(maskCvtsepi32Epi16([32]byte(src), uint16(k), [64]byte(a)))
}

func maskCvtsepi32Epi16(src [32]byte, k uint16, a [64]byte) [32]byte


// MaskzCvtsepi32Epi16: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_Int32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSDW'. Intrinsic: '_mm512_maskz_cvtsepi32_epi16'.
// Requires AVX512F.
func MaskzCvtsepi32Epi16(k Mmask16, a M512i) M256i {
	return M256i(maskzCvtsepi32Epi16(uint16(k), [64]byte(a)))
}

func maskzCvtsepi32Epi16(k uint16, a [64]byte) [32]byte


// Cvtsepi32Epi8: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			k := 8*j
//			dst[k+7:k] := Saturate_Int32_To_Int8(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSDB'. Intrinsic: '_mm512_cvtsepi32_epi8'.
// Requires AVX512F.
func Cvtsepi32Epi8(a M512i) M128i {
	return M128i(cvtsepi32Epi8([64]byte(a)))
}

func cvtsepi32Epi8(a [64]byte) [16]byte


// MaskCvtsepi32Epi8: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSDB'. Intrinsic: '_mm512_mask_cvtsepi32_epi8'.
// Requires AVX512F.
func MaskCvtsepi32Epi8(src M128i, k Mmask16, a M512i) M128i {
	return M128i(maskCvtsepi32Epi8([16]byte(src), uint16(k), [64]byte(a)))
}

func maskCvtsepi32Epi8(src [16]byte, k uint16, a [64]byte) [16]byte


// MaskzCvtsepi32Epi8: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSDB'. Intrinsic: '_mm512_maskz_cvtsepi32_epi8'.
// Requires AVX512F.
func MaskzCvtsepi32Epi8(k Mmask16, a M512i) M128i {
	return M128i(maskzCvtsepi32Epi8(uint16(k), [64]byte(a)))
}

func maskzCvtsepi32Epi8(k uint16, a [64]byte) [16]byte


// MaskCvtsepi32StoreuEpi16: Convert packed 32-bit integers in 'a' to packed
// 16-bit integers with signed saturation, and store the active results (those
// with their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				MEM[base_addr+l+15:base_addr+l] := Saturate_Int32_To_Int16(a[i+31:i])
//			FI
//		ENDFOR
//
// Instruction: 'VPMOVSDW'. Intrinsic: '_mm512_mask_cvtsepi32_storeu_epi16'.
// Requires AVX512F.
func MaskCvtsepi32StoreuEpi16(base_addr uintptr, k Mmask16, a M512i)  {
	maskCvtsepi32StoreuEpi16(uintptr(base_addr), uint16(k), [64]byte(a))
}

func maskCvtsepi32StoreuEpi16(base_addr uintptr, k uint16, a [64]byte) 


// MaskCvtsepi32StoreuEpi8: Convert packed 32-bit integers in 'a' to packed
// 8-bit integers with signed saturation, and store the active results (those
// with their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Saturate_Int32_To_Int8(a[i+31:i])
//			FI
//		ENDFOR
//
// Instruction: 'VPMOVSDB'. Intrinsic: '_mm512_mask_cvtsepi32_storeu_epi8'.
// Requires AVX512F.
func MaskCvtsepi32StoreuEpi8(base_addr uintptr, k Mmask16, a M512i)  {
	maskCvtsepi32StoreuEpi8(uintptr(base_addr), uint16(k), [64]byte(a))
}

func maskCvtsepi32StoreuEpi8(base_addr uintptr, k uint16, a [64]byte) 


// Cvtsepi64Epi16: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 16*j
//			dst[k+15:k] := Saturate_Int64_To_Int16(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSQW'. Intrinsic: '_mm512_cvtsepi64_epi16'.
// Requires AVX512F.
func Cvtsepi64Epi16(a M512i) M128i {
	return M128i(cvtsepi64Epi16([64]byte(a)))
}

func cvtsepi64Epi16(a [64]byte) [16]byte


// MaskCvtsepi64Epi16: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_Int64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSQW'. Intrinsic: '_mm512_mask_cvtsepi64_epi16'.
// Requires AVX512F.
func MaskCvtsepi64Epi16(src M128i, k Mmask8, a M512i) M128i {
	return M128i(maskCvtsepi64Epi16([16]byte(src), uint8(k), [64]byte(a)))
}

func maskCvtsepi64Epi16(src [16]byte, k uint8, a [64]byte) [16]byte


// MaskzCvtsepi64Epi16: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_Int64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSQW'. Intrinsic: '_mm512_maskz_cvtsepi64_epi16'.
// Requires AVX512F.
func MaskzCvtsepi64Epi16(k Mmask8, a M512i) M128i {
	return M128i(maskzCvtsepi64Epi16(uint8(k), [64]byte(a)))
}

func maskzCvtsepi64Epi16(k uint8, a [64]byte) [16]byte


// Cvtsepi64Epi32: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 32*j
//			dst[k+31:k] := Saturate_Int64_To_Int32(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSQD'. Intrinsic: '_mm512_cvtsepi64_epi32'.
// Requires AVX512F.
func Cvtsepi64Epi32(a M512i) M256i {
	return M256i(cvtsepi64Epi32([64]byte(a)))
}

func cvtsepi64Epi32(a [64]byte) [32]byte


// MaskCvtsepi64Epi32: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Saturate_Int64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := src[l+31:l]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSQD'. Intrinsic: '_mm512_mask_cvtsepi64_epi32'.
// Requires AVX512F.
func MaskCvtsepi64Epi32(src M256i, k Mmask8, a M512i) M256i {
	return M256i(maskCvtsepi64Epi32([32]byte(src), uint8(k), [64]byte(a)))
}

func maskCvtsepi64Epi32(src [32]byte, k uint8, a [64]byte) [32]byte


// MaskzCvtsepi64Epi32: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Saturate_Int64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSQD'. Intrinsic: '_mm512_maskz_cvtsepi64_epi32'.
// Requires AVX512F.
func MaskzCvtsepi64Epi32(k Mmask8, a M512i) M256i {
	return M256i(maskzCvtsepi64Epi32(uint8(k), [64]byte(a)))
}

func maskzCvtsepi64Epi32(k uint8, a [64]byte) [32]byte


// Cvtsepi64Epi8: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 8*j
//			dst[k+7:k] := Saturate_Int64_To_Int8(a[i+63:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSQB'. Intrinsic: '_mm512_cvtsepi64_epi8'.
// Requires AVX512F.
func Cvtsepi64Epi8(a M512i) M128i {
	return M128i(cvtsepi64Epi8([64]byte(a)))
}

func cvtsepi64Epi8(a [64]byte) [16]byte


// MaskCvtsepi64Epi8: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSQB'. Intrinsic: '_mm512_mask_cvtsepi64_epi8'.
// Requires AVX512F.
func MaskCvtsepi64Epi8(src M128i, k Mmask8, a M512i) M128i {
	return M128i(maskCvtsepi64Epi8([16]byte(src), uint8(k), [64]byte(a)))
}

func maskCvtsepi64Epi8(src [16]byte, k uint8, a [64]byte) [16]byte


// MaskzCvtsepi64Epi8: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSQB'. Intrinsic: '_mm512_maskz_cvtsepi64_epi8'.
// Requires AVX512F.
func MaskzCvtsepi64Epi8(k Mmask8, a M512i) M128i {
	return M128i(maskzCvtsepi64Epi8(uint8(k), [64]byte(a)))
}

func maskzCvtsepi64Epi8(k uint8, a [64]byte) [16]byte


// MaskCvtsepi64StoreuEpi16: Convert packed 64-bit integers in 'a' to packed
// 16-bit integers with signed saturation, and store the active results (those
// with their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				MEM[base_addr+l+15:base_addr+l] := Saturate_Int64_To_Int16(a[i+63:i])
//			FI
//		ENDFOR
//
// Instruction: 'VPMOVSQW'. Intrinsic: '_mm512_mask_cvtsepi64_storeu_epi16'.
// Requires AVX512F.
func MaskCvtsepi64StoreuEpi16(base_addr uintptr, k Mmask8, a M512i)  {
	maskCvtsepi64StoreuEpi16(uintptr(base_addr), uint8(k), [64]byte(a))
}

func maskCvtsepi64StoreuEpi16(base_addr uintptr, k uint8, a [64]byte) 


// MaskCvtsepi64StoreuEpi32: Convert packed 64-bit integers in 'a' to packed
// 32-bit integers with signed saturation, and store the active results (those
// with their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				MEM[base_addr+l+31:base_addr+l] := Saturate_Int64_To_Int32(a[i+63:i])
//			FI
//		ENDFOR
//
// Instruction: 'VPMOVSQD'. Intrinsic: '_mm512_mask_cvtsepi64_storeu_epi32'.
// Requires AVX512F.
func MaskCvtsepi64StoreuEpi32(base_addr uintptr, k Mmask8, a M512i)  {
	maskCvtsepi64StoreuEpi32(uintptr(base_addr), uint8(k), [64]byte(a))
}

func maskCvtsepi64StoreuEpi32(base_addr uintptr, k uint8, a [64]byte) 


// MaskCvtsepi64StoreuEpi8: Convert packed 64-bit integers in 'a' to packed
// 8-bit integers with signed saturation, and store the active results (those
// with their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Saturate_Int64_To_Int8(a[i+63:i])
//			FI
//		ENDFOR
//
// Instruction: 'VPMOVSQB'. Intrinsic: '_mm512_mask_cvtsepi64_storeu_epi8'.
// Requires AVX512F.
func MaskCvtsepi64StoreuEpi8(base_addr uintptr, k Mmask8, a M512i)  {
	maskCvtsepi64StoreuEpi8(uintptr(base_addr), uint8(k), [64]byte(a))
}

func maskCvtsepi64StoreuEpi8(base_addr uintptr, k uint8, a [64]byte) 


// CvttRoundpdEpi32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst'. 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := 32*i
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_IntegerTruncate(a[k+63:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2DQ'. Intrinsic: '_mm512_cvtt_roundpd_epi32'.
// Requires AVX512F.
func CvttRoundpdEpi32(a M512d, sae int) M256i {
	return M256i(cvttRoundpdEpi32([8]float64(a), sae))
}

func cvttRoundpdEpi32(a [8]float64, sae int) [32]byte


// MaskCvttRoundpdEpi32: Convert packed double-precision (64-bit)
// floating-point elements in 'a' to packed 32-bit integers with truncation,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set).  Pass __MM_FROUND_NO_EXC
// to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := 32*i
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_IntegerTruncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2DQ'. Intrinsic: '_mm512_mask_cvtt_roundpd_epi32'.
// Requires AVX512F.
func MaskCvttRoundpdEpi32(src M256i, k Mmask8, a M512d, sae int) M256i {
	return M256i(maskCvttRoundpdEpi32([32]byte(src), uint8(k), [8]float64(a), sae))
}

func maskCvttRoundpdEpi32(src [32]byte, k uint8, a [8]float64, sae int) [32]byte


// MaskzCvttRoundpdEpi32: Convert packed double-precision (64-bit)
// floating-point elements in 'a' to packed 32-bit integers with truncation,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := 32*i
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_IntegerTruncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2DQ'. Intrinsic: '_mm512_maskz_cvtt_roundpd_epi32'.
// Requires AVX512F.
func MaskzCvttRoundpdEpi32(k Mmask8, a M512d, sae int) M256i {
	return M256i(maskzCvttRoundpdEpi32(uint8(k), [8]float64(a), sae))
}

func maskzCvttRoundpdEpi32(k uint8, a [8]float64, sae int) [32]byte


// CvttRoundpdEpi64: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 64-bit integers with truncation, and store the
// results in 'dst'. Pass __MM_FROUND_NO_EXC to 'sae' to suppress all
// exceptions. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := Convert_FP64_To_Int64_Truncate(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPD2QQ'. Intrinsic: '_mm512_cvtt_roundpd_epi64'.
// Requires AVX512DQ.
func CvttRoundpdEpi64(a M512d, sae int) M512i {
	return M512i(cvttRoundpdEpi64([8]float64(a), sae))
}

func cvttRoundpdEpi64(a [8]float64, sae int) [64]byte


// MaskCvttRoundpdEpi64: Convert packed double-precision (64-bit)
// floating-point elements in 'a' to packed 64-bit integers with truncation,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_FP64_To_Int64_Truncate(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPD2QQ'. Intrinsic: '_mm512_mask_cvtt_roundpd_epi64'.
// Requires AVX512DQ.
func MaskCvttRoundpdEpi64(src M512i, k Mmask8, a M512d, sae int) M512i {
	return M512i(maskCvttRoundpdEpi64([64]byte(src), uint8(k), [8]float64(a), sae))
}

func maskCvttRoundpdEpi64(src [64]byte, k uint8, a [8]float64, sae int) [64]byte


// MaskzCvttRoundpdEpi64: Convert packed double-precision (64-bit)
// floating-point elements in 'a' to packed 64-bit integers with truncation,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). Pass __MM_FROUND_NO_EXC to
// 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_FP64_To_Int64_Truncate(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPD2QQ'. Intrinsic: '_mm512_maskz_cvtt_roundpd_epi64'.
// Requires AVX512DQ.
func MaskzCvttRoundpdEpi64(k Mmask8, a M512d, sae int) M512i {
	return M512i(maskzCvttRoundpdEpi64(uint8(k), [8]float64(a), sae))
}

func maskzCvttRoundpdEpi64(k uint8, a [8]float64, sae int) [64]byte


// CvttRoundpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst'. 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := 32*i
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_UnsignedIntegerTruncate(a[k+63:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2UDQ'. Intrinsic: '_mm512_cvtt_roundpd_epu32'.
// Requires AVX512F.
func CvttRoundpdEpu32(a M512d, sae int) M256i {
	return M256i(cvttRoundpdEpu32([8]float64(a), sae))
}

func cvttRoundpdEpu32(a [8]float64, sae int) [32]byte


// MaskCvttRoundpdEpu32: Convert packed double-precision (64-bit)
// floating-point elements in 'a' to packed unsigned 32-bit integers with
// truncation, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set).  
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := 32*i
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedIntegerTruncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2UDQ'. Intrinsic: '_mm512_mask_cvtt_roundpd_epu32'.
// Requires AVX512F.
func MaskCvttRoundpdEpu32(src M256i, k Mmask8, a M512d, sae int) M256i {
	return M256i(maskCvttRoundpdEpu32([32]byte(src), uint8(k), [8]float64(a), sae))
}

func maskCvttRoundpdEpu32(src [32]byte, k uint8, a [8]float64, sae int) [32]byte


// MaskzCvttRoundpdEpu32: Convert packed double-precision (64-bit)
// floating-point elements in 'a' to packed unsigned 32-bit integers with
// truncation, and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := 32*i
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedIntegerTruncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2UDQ'. Intrinsic: '_mm512_maskz_cvtt_roundpd_epu32'.
// Requires AVX512F.
func MaskzCvttRoundpdEpu32(k Mmask8, a M512d, sae int) M256i {
	return M256i(maskzCvttRoundpdEpu32(uint8(k), [8]float64(a), sae))
}

func maskzCvttRoundpdEpu32(k uint8, a [8]float64, sae int) [32]byte


// CvttRoundpdEpu64: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 64-bit integers with truncation, and
// store the results in 'dst'. Pass __MM_FROUND_NO_EXC to 'sae' to suppress all
// exceptions. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := Convert_FP64_To_UnsignedInt64_Truncate(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPD2UQQ'. Intrinsic: '_mm512_cvtt_roundpd_epu64'.
// Requires AVX512DQ.
func CvttRoundpdEpu64(a M512d, sae int) M512i {
	return M512i(cvttRoundpdEpu64([8]float64(a), sae))
}

func cvttRoundpdEpu64(a [8]float64, sae int) [64]byte


// MaskCvttRoundpdEpu64: Convert packed double-precision (64-bit)
// floating-point elements in 'a' to packed unsigned 64-bit integers with
// truncation, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_FP64_To_UnsignedInt64_Truncate(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPD2UQQ'. Intrinsic: '_mm512_mask_cvtt_roundpd_epu64'.
// Requires AVX512DQ.
func MaskCvttRoundpdEpu64(src M512i, k Mmask8, a M512d, sae int) M512i {
	return M512i(maskCvttRoundpdEpu64([64]byte(src), uint8(k), [8]float64(a), sae))
}

func maskCvttRoundpdEpu64(src [64]byte, k uint8, a [8]float64, sae int) [64]byte


// MaskzCvttRoundpdEpu64: Convert packed double-precision (64-bit)
// floating-point elements in 'a' to packed unsigned 64-bit integers with
// truncation, and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). Pass
// __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_FP64_To_UnsignedInt64_Truncate(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPD2UQQ'. Intrinsic: '_mm512_maskz_cvtt_roundpd_epu64'.
// Requires AVX512DQ.
func MaskzCvttRoundpdEpu64(k Mmask8, a M512d, sae int) M512i {
	return M512i(maskzCvttRoundpdEpu64(uint8(k), [8]float64(a), sae))
}

func maskzCvttRoundpdEpu64(k uint8, a [8]float64, sae int) [64]byte


// CvttRoundpsEpi32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst'. 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 15
//			i := 32*i
//			dst[i+31:i] := Convert_FP32_To_IntegerTruncate(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2DQ'. Intrinsic: '_mm512_cvtt_roundps_epi32'.
// Requires AVX512F.
func CvttRoundpsEpi32(a M512, sae int) M512i {
	return M512i(cvttRoundpsEpi32([16]float32(a), sae))
}

func cvttRoundpsEpi32(a [16]float32, sae int) [64]byte


// MaskCvttRoundpsEpi32: Convert packed single-precision (32-bit)
// floating-point elements in 'a' to packed 32-bit integers with truncation,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 15
//			i := 32*i
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_IntegerTruncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2DQ'. Intrinsic: '_mm512_mask_cvtt_roundps_epi32'.
// Requires AVX512F.
func MaskCvttRoundpsEpi32(src M512i, k Mmask16, a M512, sae int) M512i {
	return M512i(maskCvttRoundpsEpi32([64]byte(src), uint16(k), [16]float32(a), sae))
}

func maskCvttRoundpsEpi32(src [64]byte, k uint16, a [16]float32, sae int) [64]byte


// MaskzCvttRoundpsEpi32: Convert packed single-precision (32-bit)
// floating-point elements in 'a' to packed 32-bit integers with truncation,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 15
//			i := 32*i
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_IntegerTruncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2DQ'. Intrinsic: '_mm512_maskz_cvtt_roundps_epi32'.
// Requires AVX512F.
func MaskzCvttRoundpsEpi32(k Mmask16, a M512, sae int) M512i {
	return M512i(maskzCvttRoundpsEpi32(uint16(k), [16]float32(a), sae))
}

func maskzCvttRoundpsEpi32(k uint16, a [16]float32, sae int) [64]byte


// CvttRoundpsEpi64: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 64-bit integers with truncation, and store the
// results in 'dst'. Pass __MM_FROUND_NO_EXC to 'sae' to suppress all
// exceptions. 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			dst[i+63:i] := Convert_FP32_To_Int64_Truncate(a[l+31:l])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2QQ'. Intrinsic: '_mm512_cvtt_roundps_epi64'.
// Requires AVX512DQ.
func CvttRoundpsEpi64(a M256, sae int) M512i {
	return M512i(cvttRoundpsEpi64([8]float32(a), sae))
}

func cvttRoundpsEpi64(a [8]float32, sae int) [64]byte


// MaskCvttRoundpsEpi64: Convert packed single-precision (32-bit)
// floating-point elements in 'a' to packed 64-bit integers with truncation,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_Int64_Truncate(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2QQ'. Intrinsic: '_mm512_mask_cvtt_roundps_epi64'.
// Requires AVX512DQ.
func MaskCvttRoundpsEpi64(src M512i, k Mmask8, a M256, sae int) M512i {
	return M512i(maskCvttRoundpsEpi64([64]byte(src), uint8(k), [8]float32(a), sae))
}

func maskCvttRoundpsEpi64(src [64]byte, k uint8, a [8]float32, sae int) [64]byte


// MaskzCvttRoundpsEpi64: Convert packed single-precision (32-bit)
// floating-point elements in 'a' to packed 64-bit integers with truncation,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). Pass __MM_FROUND_NO_EXC to
// 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_Int64_Truncate(a[l+31:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2QQ'. Intrinsic: '_mm512_maskz_cvtt_roundps_epi64'.
// Requires AVX512DQ.
func MaskzCvttRoundpsEpi64(k Mmask8, a M256, sae int) M512i {
	return M512i(maskzCvttRoundpsEpi64(uint8(k), [8]float32(a), sae))
}

func maskzCvttRoundpsEpi64(k uint8, a [8]float32, sae int) [64]byte


// CvttRoundpsEpu32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst'. 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 15
//			i := 32*i
//			dst[i+31:i] := Convert_FP32_To_UnsignedIntegerTruncate(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2UDQ'. Intrinsic: '_mm512_cvtt_roundps_epu32'.
// Requires AVX512F.
func CvttRoundpsEpu32(a M512, sae int) M512i {
	return M512i(cvttRoundpsEpu32([16]float32(a), sae))
}

func cvttRoundpsEpu32(a [16]float32, sae int) [64]byte


// MaskCvttRoundpsEpu32: Convert packed single-precision (32-bit)
// floating-point elements in 'a' to packed unsigned 32-bit integers with
// truncation, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set).  
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 15
//			i := 32*i
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_UnsignedIntegerTruncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2UDQ'. Intrinsic: '_mm512_mask_cvtt_roundps_epu32'.
// Requires AVX512F.
func MaskCvttRoundpsEpu32(src M512i, k Mmask16, a M512, sae int) M512i {
	return M512i(maskCvttRoundpsEpu32([64]byte(src), uint16(k), [16]float32(a), sae))
}

func maskCvttRoundpsEpu32(src [64]byte, k uint16, a [16]float32, sae int) [64]byte


// MaskzCvttRoundpsEpu32: Convert packed single-precision (32-bit)
// floating-point elements in 'a' to packed unsigned 32-bit integers with
// truncation, and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 15
//			i := 32*i
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_UnsignedIntegerTruncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2UDQ'. Intrinsic: '_mm512_maskz_cvtt_roundps_epu32'.
// Requires AVX512F.
func MaskzCvttRoundpsEpu32(k Mmask16, a M512, sae int) M512i {
	return M512i(maskzCvttRoundpsEpu32(uint16(k), [16]float32(a), sae))
}

func maskzCvttRoundpsEpu32(k uint16, a [16]float32, sae int) [64]byte


// CvttRoundpsEpu64: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 64-bit integers with truncation, and
// store the results in 'dst'. Pass __MM_FROUND_NO_EXC to 'sae' to suppress all
// exceptions. 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			dst[i+63:i] := Convert_FP32_To_UnsignedInt64_Truncate(a[l+31:l])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2UQQ'. Intrinsic: '_mm512_cvtt_roundps_epu64'.
// Requires AVX512DQ.
func CvttRoundpsEpu64(a M256, sae int) M512i {
	return M512i(cvttRoundpsEpu64([8]float32(a), sae))
}

func cvttRoundpsEpu64(a [8]float32, sae int) [64]byte


// MaskCvttRoundpsEpu64: Convert packed single-precision (32-bit)
// floating-point elements in 'a' to packed unsigned 64-bit integers with
// truncation, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_UnsignedInt64_Truncate(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2UQQ'. Intrinsic: '_mm512_mask_cvtt_roundps_epu64'.
// Requires AVX512DQ.
func MaskCvttRoundpsEpu64(src M512i, k Mmask8, a M256, sae int) M512i {
	return M512i(maskCvttRoundpsEpu64([64]byte(src), uint8(k), [8]float32(a), sae))
}

func maskCvttRoundpsEpu64(src [64]byte, k uint8, a [8]float32, sae int) [64]byte


// MaskzCvttRoundpsEpu64: Convert packed single-precision (32-bit)
// floating-point elements in 'a' to packed unsigned 64-bit integers with
// truncation, and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). Pass
// __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_UnsignedInt64_Truncate(a[l+31:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2UQQ'. Intrinsic: '_mm512_maskz_cvtt_roundps_epu64'.
// Requires AVX512DQ.
func MaskzCvttRoundpsEpu64(k Mmask8, a M256, sae int) M512i {
	return M512i(maskzCvttRoundpsEpu64(uint8(k), [8]float32(a), sae))
}

func maskzCvttRoundpsEpu64(k uint8, a [8]float32, sae int) [64]byte


// CvttpdEpi32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_Int32_Truncate(a[k+63:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2DQ'. Intrinsic: '_mm512_cvttpd_epi32'.
// Requires AVX512F.
func CvttpdEpi32(a M512d) M256i {
	return M256i(cvttpdEpi32([8]float64(a)))
}

func cvttpdEpi32(a [8]float64) [32]byte


// MaskCvttpdEpi32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_Int32_Truncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2DQ'. Intrinsic: '_mm512_mask_cvttpd_epi32'.
// Requires AVX512F.
func MaskCvttpdEpi32(src M256i, k Mmask8, a M512d) M256i {
	return M256i(maskCvttpdEpi32([32]byte(src), uint8(k), [8]float64(a)))
}

func maskCvttpdEpi32(src [32]byte, k uint8, a [8]float64) [32]byte


// MaskzCvttpdEpi32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_Int32_Truncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2DQ'. Intrinsic: '_mm512_maskz_cvttpd_epi32'.
// Requires AVX512F.
func MaskzCvttpdEpi32(k Mmask8, a M512d) M256i {
	return M256i(maskzCvttpdEpi32(uint8(k), [8]float64(a)))
}

func maskzCvttpdEpi32(k uint8, a [8]float64) [32]byte


// CvttpdEpi64: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 64-bit integers with truncation, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := Convert_FP64_To_Int64_Truncate(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPD2QQ'. Intrinsic: '_mm512_cvttpd_epi64'.
// Requires AVX512DQ.
func CvttpdEpi64(a M512d) M512i {
	return M512i(cvttpdEpi64([8]float64(a)))
}

func cvttpdEpi64(a [8]float64) [64]byte


// MaskCvttpdEpi64: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 64-bit integers with truncation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_FP64_To_Int64_Truncate(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPD2QQ'. Intrinsic: '_mm512_mask_cvttpd_epi64'.
// Requires AVX512DQ.
func MaskCvttpdEpi64(src M512i, k Mmask8, a M512d) M512i {
	return M512i(maskCvttpdEpi64([64]byte(src), uint8(k), [8]float64(a)))
}

func maskCvttpdEpi64(src [64]byte, k uint8, a [8]float64) [64]byte


// MaskzCvttpdEpi64: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 64-bit integers with truncation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_FP64_To_Int64_Truncate(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPD2QQ'. Intrinsic: '_mm512_maskz_cvttpd_epi64'.
// Requires AVX512DQ.
func MaskzCvttpdEpi64(k Mmask8, a M512d) M512i {
	return M512i(maskzCvttpdEpi64(uint8(k), [8]float64(a)))
}

func maskzCvttpdEpi64(k uint8, a [8]float64) [64]byte


// CvttpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[k+63:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2UDQ'. Intrinsic: '_mm512_cvttpd_epu32'.
// Requires AVX512F.
func CvttpdEpu32(a M512d) M256i {
	return M256i(cvttpdEpu32([8]float64(a)))
}

func cvttpdEpu32(a [8]float64) [32]byte


// MaskCvttpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2UDQ'. Intrinsic: '_mm512_mask_cvttpd_epu32'.
// Requires AVX512F.
func MaskCvttpdEpu32(src M256i, k Mmask8, a M512d) M256i {
	return M256i(maskCvttpdEpu32([32]byte(src), uint8(k), [8]float64(a)))
}

func maskCvttpdEpu32(src [32]byte, k uint8, a [8]float64) [32]byte


// MaskzCvttpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2UDQ'. Intrinsic: '_mm512_maskz_cvttpd_epu32'.
// Requires AVX512F.
func MaskzCvttpdEpu32(k Mmask8, a M512d) M256i {
	return M256i(maskzCvttpdEpu32(uint8(k), [8]float64(a)))
}

func maskzCvttpdEpu32(k uint8, a [8]float64) [32]byte


// CvttpdEpu64: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 64-bit integers with truncation, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := Convert_FP64_To_UnsignedInt64_Truncate(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPD2UQQ'. Intrinsic: '_mm512_cvttpd_epu64'.
// Requires AVX512DQ.
func CvttpdEpu64(a M512d) M512i {
	return M512i(cvttpdEpu64([8]float64(a)))
}

func cvttpdEpu64(a [8]float64) [64]byte


// MaskCvttpdEpu64: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 64-bit integers with truncation, and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_FP64_To_UnsignedInt64_Truncate(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPD2UQQ'. Intrinsic: '_mm512_mask_cvttpd_epu64'.
// Requires AVX512DQ.
func MaskCvttpdEpu64(src M512i, k Mmask8, a M512d) M512i {
	return M512i(maskCvttpdEpu64([64]byte(src), uint8(k), [8]float64(a)))
}

func maskCvttpdEpu64(src [64]byte, k uint8, a [8]float64) [64]byte


// MaskzCvttpdEpu64: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 64-bit integers with truncation, and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_FP64_To_UnsignedInt64_Truncate(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPD2UQQ'. Intrinsic: '_mm512_maskz_cvttpd_epu64'.
// Requires AVX512DQ.
func MaskzCvttpdEpu64(k Mmask8, a M512d) M512i {
	return M512i(maskzCvttpdEpu64(uint8(k), [8]float64(a)))
}

func maskzCvttpdEpu64(k uint8, a [8]float64) [64]byte


// CvttpsEpi32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := Convert_FP32_To_Int32_Truncate(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2DQ'. Intrinsic: '_mm512_cvttps_epi32'.
// Requires AVX512F.
func CvttpsEpi32(a M512) M512i {
	return M512i(cvttpsEpi32([16]float32(a)))
}

func cvttpsEpi32(a [16]float32) [64]byte


// MaskCvttpsEpi32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_Int32_Truncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2DQ'. Intrinsic: '_mm512_mask_cvttps_epi32'.
// Requires AVX512F.
func MaskCvttpsEpi32(src M512i, k Mmask16, a M512) M512i {
	return M512i(maskCvttpsEpi32([64]byte(src), uint16(k), [16]float32(a)))
}

func maskCvttpsEpi32(src [64]byte, k uint16, a [16]float32) [64]byte


// MaskzCvttpsEpi32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_Int32_Truncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2DQ'. Intrinsic: '_mm512_maskz_cvttps_epi32'.
// Requires AVX512F.
func MaskzCvttpsEpi32(k Mmask16, a M512) M512i {
	return M512i(maskzCvttpsEpi32(uint16(k), [16]float32(a)))
}

func maskzCvttpsEpi32(k uint16, a [16]float32) [64]byte


// CvttpsEpi64: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 64-bit integers with truncation, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			dst[i+63:i] := Convert_FP32_To_Int64_Truncate(a[l+31:l])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2QQ'. Intrinsic: '_mm512_cvttps_epi64'.
// Requires AVX512DQ.
func CvttpsEpi64(a M256) M512i {
	return M512i(cvttpsEpi64([8]float32(a)))
}

func cvttpsEpi64(a [8]float32) [64]byte


// MaskCvttpsEpi64: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 64-bit integers with truncation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_Int64_Truncate(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2QQ'. Intrinsic: '_mm512_mask_cvttps_epi64'.
// Requires AVX512DQ.
func MaskCvttpsEpi64(src M512i, k Mmask8, a M256) M512i {
	return M512i(maskCvttpsEpi64([64]byte(src), uint8(k), [8]float32(a)))
}

func maskCvttpsEpi64(src [64]byte, k uint8, a [8]float32) [64]byte


// MaskzCvttpsEpi64: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 64-bit integers with truncation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_Int64_Truncate(a[l+31:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2QQ'. Intrinsic: '_mm512_maskz_cvttps_epi64'.
// Requires AVX512DQ.
func MaskzCvttpsEpi64(k Mmask8, a M256) M512i {
	return M512i(maskzCvttpsEpi64(uint8(k), [8]float32(a)))
}

func maskzCvttpsEpi64(k uint8, a [8]float32) [64]byte


// CvttpsEpu32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := Convert_FP32_To_UnsignedInt32_Truncate(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2UDQ'. Intrinsic: '_mm512_cvttps_epu32'.
// Requires AVX512F.
func CvttpsEpu32(a M512) M512i {
	return M512i(cvttpsEpu32([16]float32(a)))
}

func cvttpsEpu32(a [16]float32) [64]byte


// MaskCvttpsEpu32: Convert packed double-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2UDQ'. Intrinsic: '_mm512_mask_cvttps_epu32'.
// Requires AVX512F.
func MaskCvttpsEpu32(src M512i, k Mmask16, a M512) M512i {
	return M512i(maskCvttpsEpu32([64]byte(src), uint16(k), [16]float32(a)))
}

func maskCvttpsEpu32(src [64]byte, k uint16, a [16]float32) [64]byte


// MaskzCvttpsEpu32: Convert packed double-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2UDQ'. Intrinsic: '_mm512_maskz_cvttps_epu32'.
// Requires AVX512F.
func MaskzCvttpsEpu32(k Mmask16, a M512) M512i {
	return M512i(maskzCvttpsEpu32(uint16(k), [16]float32(a)))
}

func maskzCvttpsEpu32(k uint16, a [16]float32) [64]byte


// CvttpsEpu64: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 64-bit integers with truncation, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			dst[i+63:i] := Convert_FP32_To_UnsignedInt64_Truncate(a[l+31:l])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2UQQ'. Intrinsic: '_mm512_cvttps_epu64'.
// Requires AVX512DQ.
func CvttpsEpu64(a M256) M512i {
	return M512i(cvttpsEpu64([8]float32(a)))
}

func cvttpsEpu64(a [8]float32) [64]byte


// MaskCvttpsEpu64: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 64-bit integers with truncation, and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_UnsignedInt64_Truncate(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2UQQ'. Intrinsic: '_mm512_mask_cvttps_epu64'.
// Requires AVX512DQ.
func MaskCvttpsEpu64(src M512i, k Mmask8, a M256) M512i {
	return M512i(maskCvttpsEpu64([64]byte(src), uint8(k), [8]float32(a)))
}

func maskCvttpsEpu64(src [64]byte, k uint8, a [8]float32) [64]byte


// MaskzCvttpsEpu64: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 64-bit integers with truncation, and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_UnsignedInt64_Truncate(a[l+31:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2UQQ'. Intrinsic: '_mm512_maskz_cvttps_epu64'.
// Requires AVX512DQ.
func MaskzCvttpsEpu64(k Mmask8, a M256) M512i {
	return M512i(maskzCvttpsEpu64(uint8(k), [8]float32(a)))
}

func maskzCvttpsEpu64(k uint8, a [8]float32) [64]byte


// Cvtusepi16Epi8: Convert packed unsigned 16-bit integers in 'a' to packed
// unsigned 8-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 31
//			i := 16*j
//			l := 8*j
//			dst[l+7:l] := Saturate_UnsignedInt16_To_Int8(a[i+15:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVUSWB'. Intrinsic: '_mm512_cvtusepi16_epi8'.
// Requires AVX512BW.
func Cvtusepi16Epi8(a M512i) M256i {
	return M256i(cvtusepi16Epi8([64]byte(a)))
}

func cvtusepi16Epi8(a [64]byte) [32]byte


// MaskCvtusepi16Epi8: Convert packed unsigned 16-bit integers in 'a' to packed
// unsigned 8-bit integers with unsigned saturation, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := 16*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt16_To_Int8(a[i+15:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVUSWB'. Intrinsic: '_mm512_mask_cvtusepi16_epi8'.
// Requires AVX512BW.
func MaskCvtusepi16Epi8(src M256i, k Mmask32, a M512i) M256i {
	return M256i(maskCvtusepi16Epi8([32]byte(src), uint32(k), [64]byte(a)))
}

func maskCvtusepi16Epi8(src [32]byte, k uint32, a [64]byte) [32]byte


// MaskzCvtusepi16Epi8: Convert packed unsigned 16-bit integers in 'a' to
// packed unsigned 8-bit integers with unsigned saturation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := 16*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt16_To_Int8(a[i+15:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVUSWB'. Intrinsic: '_mm512_maskz_cvtusepi16_epi8'.
// Requires AVX512BW.
func MaskzCvtusepi16Epi8(k Mmask32, a M512i) M256i {
	return M256i(maskzCvtusepi16Epi8(uint32(k), [64]byte(a)))
}

func maskzCvtusepi16Epi8(k uint32, a [64]byte) [32]byte


// MaskCvtusepi16StoreuEpi8: Convert packed unsigned 16-bit integers in 'a' to
// packed unsigned 8-bit integers with unsigned saturation, and store the
// active results (those with their respective bit set in writemask 'k') to
// unaligned memory at 'base_addr'. 
//
//		FOR j := 0 to 31
//			i := 16*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Saturate_UnsignedInt16_To_Int8(a[i+15:i])
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVUSWB'. Intrinsic: '_mm512_mask_cvtusepi16_storeu_epi8'.
// Requires AVX512BW.
func MaskCvtusepi16StoreuEpi8(base_addr uintptr, k Mmask32, a M512i)  {
	maskCvtusepi16StoreuEpi8(uintptr(base_addr), uint32(k), [64]byte(a))
}

func maskCvtusepi16StoreuEpi8(base_addr uintptr, k uint32, a [64]byte) 


// Cvtusepi32Epi16: Convert packed unsigned 32-bit integers in 'a' to packed
// unsigned 16-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			k := 16*j
//			dst[k+15:k] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVUSDW'. Intrinsic: '_mm512_cvtusepi32_epi16'.
// Requires AVX512F.
func Cvtusepi32Epi16(a M512i) M256i {
	return M256i(cvtusepi32Epi16([64]byte(a)))
}

func cvtusepi32Epi16(a [64]byte) [32]byte


// MaskCvtusepi32Epi16: Convert packed unsigned 32-bit integers in 'a' to
// packed unsigned 16-bit integers with unsigned saturation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVUSDW'. Intrinsic: '_mm512_mask_cvtusepi32_epi16'.
// Requires AVX512F.
func MaskCvtusepi32Epi16(src M256i, k Mmask16, a M512i) M256i {
	return M256i(maskCvtusepi32Epi16([32]byte(src), uint16(k), [64]byte(a)))
}

func maskCvtusepi32Epi16(src [32]byte, k uint16, a [64]byte) [32]byte


// MaskzCvtusepi32Epi16: Convert packed unsigned 32-bit integers in 'a' to
// packed unsigned 16-bit integers with unsigned saturation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVUSDW'. Intrinsic: '_mm512_maskz_cvtusepi32_epi16'.
// Requires AVX512F.
func MaskzCvtusepi32Epi16(k Mmask16, a M512i) M256i {
	return M256i(maskzCvtusepi32Epi16(uint16(k), [64]byte(a)))
}

func maskzCvtusepi32Epi16(k uint16, a [64]byte) [32]byte


// Cvtusepi32Epi8: Convert packed unsigned 32-bit integers in 'a' to packed
// unsigned 8-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			k := 8*j
//			dst[k+7:k] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSDB'. Intrinsic: '_mm512_cvtusepi32_epi8'.
// Requires AVX512F.
func Cvtusepi32Epi8(a M512i) M128i {
	return M128i(cvtusepi32Epi8([64]byte(a)))
}

func cvtusepi32Epi8(a [64]byte) [16]byte


// MaskCvtusepi32Epi8: Convert packed unsigned 32-bit integers in 'a' to packed
// unsigned 8-bit integers with unsigned saturation, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSDB'. Intrinsic: '_mm512_mask_cvtusepi32_epi8'.
// Requires AVX512F.
func MaskCvtusepi32Epi8(src M128i, k Mmask16, a M512i) M128i {
	return M128i(maskCvtusepi32Epi8([16]byte(src), uint16(k), [64]byte(a)))
}

func maskCvtusepi32Epi8(src [16]byte, k uint16, a [64]byte) [16]byte


// MaskzCvtusepi32Epi8: Convert packed unsigned 32-bit integers in 'a' to
// packed unsigned 8-bit integers with unsigned saturation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSDB'. Intrinsic: '_mm512_maskz_cvtusepi32_epi8'.
// Requires AVX512F.
func MaskzCvtusepi32Epi8(k Mmask16, a M512i) M128i {
	return M128i(maskzCvtusepi32Epi8(uint16(k), [64]byte(a)))
}

func maskzCvtusepi32Epi8(k uint16, a [64]byte) [16]byte


// MaskCvtusepi32StoreuEpi16: Convert packed unsigned 32-bit integers in 'a' to
// packed 16-bit integers with unsigned saturation, and store the active
// results (those with their respective bit set in writemask 'k') to unaligned
// memory at 'base_addr'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				MEM[base_addr+l+15:base_addr+l] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
//			FI
//		ENDFOR
//
// Instruction: 'VPMOVUSDW'. Intrinsic: '_mm512_mask_cvtusepi32_storeu_epi16'.
// Requires AVX512F.
func MaskCvtusepi32StoreuEpi16(base_addr uintptr, k Mmask16, a M512i)  {
	maskCvtusepi32StoreuEpi16(uintptr(base_addr), uint16(k), [64]byte(a))
}

func maskCvtusepi32StoreuEpi16(base_addr uintptr, k uint16, a [64]byte) 


// MaskCvtusepi32StoreuEpi8: Convert packed unsigned 32-bit integers in 'a' to
// packed 8-bit integers with unsigned saturation, and store the active results
// (those with their respective bit set in writemask 'k') to unaligned memory
// at 'base_addr'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
//			FI
//		ENDFOR
//
// Instruction: 'VPMOVUSDB'. Intrinsic: '_mm512_mask_cvtusepi32_storeu_epi8'.
// Requires AVX512F.
func MaskCvtusepi32StoreuEpi8(base_addr uintptr, k Mmask16, a M512i)  {
	maskCvtusepi32StoreuEpi8(uintptr(base_addr), uint16(k), [64]byte(a))
}

func maskCvtusepi32StoreuEpi8(base_addr uintptr, k uint16, a [64]byte) 


// Cvtusepi64Epi16: Convert packed unsigned 64-bit integers in 'a' to packed
// unsigned 16-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 16*j
//			dst[k+15:k] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSQW'. Intrinsic: '_mm512_cvtusepi64_epi16'.
// Requires AVX512F.
func Cvtusepi64Epi16(a M512i) M128i {
	return M128i(cvtusepi64Epi16([64]byte(a)))
}

func cvtusepi64Epi16(a [64]byte) [16]byte


// MaskCvtusepi64Epi16: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 16-bit integers with unsigned saturation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSQW'. Intrinsic: '_mm512_mask_cvtusepi64_epi16'.
// Requires AVX512F.
func MaskCvtusepi64Epi16(src M128i, k Mmask8, a M512i) M128i {
	return M128i(maskCvtusepi64Epi16([16]byte(src), uint8(k), [64]byte(a)))
}

func maskCvtusepi64Epi16(src [16]byte, k uint8, a [64]byte) [16]byte


// MaskzCvtusepi64Epi16: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 16-bit integers with unsigned saturation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSQW'. Intrinsic: '_mm512_maskz_cvtusepi64_epi16'.
// Requires AVX512F.
func MaskzCvtusepi64Epi16(k Mmask8, a M512i) M128i {
	return M128i(maskzCvtusepi64Epi16(uint8(k), [64]byte(a)))
}

func maskzCvtusepi64Epi16(k uint8, a [64]byte) [16]byte


// Cvtusepi64Epi32: Convert packed unsigned 64-bit integers in 'a' to packed
// unsigned 32-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 32*j
//			dst[k+31:k] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVUSQD'. Intrinsic: '_mm512_cvtusepi64_epi32'.
// Requires AVX512F.
func Cvtusepi64Epi32(a M512i) M256i {
	return M256i(cvtusepi64Epi32([64]byte(a)))
}

func cvtusepi64Epi32(a [64]byte) [32]byte


// MaskCvtusepi64Epi32: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 32-bit integers with unsigned saturation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := src[l+31:l]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVUSQD'. Intrinsic: '_mm512_mask_cvtusepi64_epi32'.
// Requires AVX512F.
func MaskCvtusepi64Epi32(src M256i, k Mmask8, a M512i) M256i {
	return M256i(maskCvtusepi64Epi32([32]byte(src), uint8(k), [64]byte(a)))
}

func maskCvtusepi64Epi32(src [32]byte, k uint8, a [64]byte) [32]byte


// MaskzCvtusepi64Epi32: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 32-bit integers with unsigned saturation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVUSQD'. Intrinsic: '_mm512_maskz_cvtusepi64_epi32'.
// Requires AVX512F.
func MaskzCvtusepi64Epi32(k Mmask8, a M512i) M256i {
	return M256i(maskzCvtusepi64Epi32(uint8(k), [64]byte(a)))
}

func maskzCvtusepi64Epi32(k uint8, a [64]byte) [32]byte


// Cvtusepi64Epi8: Convert packed unsigned 64-bit integers in 'a' to packed
// unsigned 8-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 8*j
//			dst[k+7:k] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSQB'. Intrinsic: '_mm512_cvtusepi64_epi8'.
// Requires AVX512F.
func Cvtusepi64Epi8(a M512i) M128i {
	return M128i(cvtusepi64Epi8([64]byte(a)))
}

func cvtusepi64Epi8(a [64]byte) [16]byte


// MaskCvtusepi64Epi8: Convert packed unsigned 64-bit integers in 'a' to packed
// unsigned 8-bit integers with unsigned saturation, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSQB'. Intrinsic: '_mm512_mask_cvtusepi64_epi8'.
// Requires AVX512F.
func MaskCvtusepi64Epi8(src M128i, k Mmask8, a M512i) M128i {
	return M128i(maskCvtusepi64Epi8([16]byte(src), uint8(k), [64]byte(a)))
}

func maskCvtusepi64Epi8(src [16]byte, k uint8, a [64]byte) [16]byte


// MaskzCvtusepi64Epi8: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 8-bit integers with unsigned saturation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSQB'. Intrinsic: '_mm512_maskz_cvtusepi64_epi8'.
// Requires AVX512F.
func MaskzCvtusepi64Epi8(k Mmask8, a M512i) M128i {
	return M128i(maskzCvtusepi64Epi8(uint8(k), [64]byte(a)))
}

func maskzCvtusepi64Epi8(k uint8, a [64]byte) [16]byte


// MaskCvtusepi64StoreuEpi16: Convert packed unsigned 64-bit integers in 'a' to
// packed 16-bit integers with unsigned saturation, and store the active
// results (those with their respective bit set in writemask 'k') to unaligned
// memory at 'base_addr'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				MEM[base_addr+l+15:base_addr+l] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
//			FI
//		ENDFOR
//
// Instruction: 'VPMOVUSQW'. Intrinsic: '_mm512_mask_cvtusepi64_storeu_epi16'.
// Requires AVX512F.
func MaskCvtusepi64StoreuEpi16(base_addr uintptr, k Mmask8, a M512i)  {
	maskCvtusepi64StoreuEpi16(uintptr(base_addr), uint8(k), [64]byte(a))
}

func maskCvtusepi64StoreuEpi16(base_addr uintptr, k uint8, a [64]byte) 


// MaskCvtusepi64StoreuEpi32: Convert packed unsigned 64-bit integers in 'a' to
// packed 32-bit integers with unsigned saturation, and store the active
// results (those with their respective bit set in writemask 'k') to unaligned
// memory at 'base_addr'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				MEM[base_addr+l+31:base_addr+l] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
//			FI
//		ENDFOR
//
// Instruction: 'VPMOVUSQD'. Intrinsic: '_mm512_mask_cvtusepi64_storeu_epi32'.
// Requires AVX512F.
func MaskCvtusepi64StoreuEpi32(base_addr uintptr, k Mmask8, a M512i)  {
	maskCvtusepi64StoreuEpi32(uintptr(base_addr), uint8(k), [64]byte(a))
}

func maskCvtusepi64StoreuEpi32(base_addr uintptr, k uint8, a [64]byte) 


// MaskCvtusepi64StoreuEpi8: Convert packed unsigned 64-bit integers in 'a' to
// packed 8-bit integers with unsigned saturation, and store the active results
// (those with their respective bit set in writemask 'k') to unaligned memory
// at 'base_addr'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
//			FI
//		ENDFOR
//
// Instruction: 'VPMOVUSQB'. Intrinsic: '_mm512_mask_cvtusepi64_storeu_epi8'.
// Requires AVX512F.
func MaskCvtusepi64StoreuEpi8(base_addr uintptr, k Mmask8, a M512i)  {
	maskCvtusepi64StoreuEpi8(uintptr(base_addr), uint8(k), [64]byte(a))
}

func maskCvtusepi64StoreuEpi8(base_addr uintptr, k uint8, a [64]byte) 


// DbsadEpu8: Compute the sum of absolute differences (SADs) of quadruplets of
// unsigned 8-bit integers in 'a' compared to those in 'b', and store the
// 16-bit results in 'dst'.
// 	Four SADs are performed on four 8-bit quadruplets for each 64-bit lane. The
// first two SADs use the lower 8-bit quadruplet of the lane from 'a', and the
// last two SADs use the uppper 8-bit quadruplet of the lane from 'a'.
// Quadruplets from 'b' are selected from within 128-bit lanes according to the
// control in 'imm8', and each SAD in each 64-bit lane uses the selected
// quadruplet at 8-bit offsets. 
//
//		FOR j := 0 to 3
//			i := j*128
//			tmp[i+31:i] := select(b[i+127:i], imm8[1:0])
//			tmp[i+63:i+32] := select(b[i+127:i], imm8[3:2])
//			tmp[i+95:i+64] := select(b[i+127:i], imm8[5:4])
//			tmp[i+127:i+96] := select(b[i+127:i], imm8[7:6])
//		ENDFOR
//		
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+15:i] := ABS(a[i+7:i] - tmp[i+7:i]) + ABS(a[i+15:i+8] - tmp[i+15:i+8])
//						 + ABS(a[i+23:i+16] - tmp[i+23:i+16]) + ABS(a[i+31:i+24] - tmp[i+31:i+24])
//			
//			dst[i+31:i+16] := ABS(a[i+7:i] - tmp[i+15:i+8]) + ABS(a[i+15:i+8] - tmp[i+23:i+16])
//						 + ABS(a[i+23:i+16] - tmp[i+31:i+24]) + ABS(a[i+31:i+24] - tmp[i+39:i+32])
//			
//			dst[i+47:i+32] := ABS(a[i+39:i+32] - tmp[i+23:i+16]) + ABS(a[i+47:i+40] - tmp[i+31:i+24])
//						 + ABS(a[i+55:i+48] - tmp[i+39:i+32]) + ABS(a[i+63:i+56] - tmp[i+47:i+40])
//			
//			dst[i+63:i+48] := ABS(a[i+39:i+32] - tmp[i+31:i+24]) + ABS(a[i+47:i+40] - tmp[i+39:i+32])
//						 + ABS(a[i+55:i+48] - tmp[i+47:i+40]) + ABS(a[i+63:i+56] - tmp[i+55:i+48])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VDBPSADBW'. Intrinsic: '_mm512_dbsad_epu8'.
// Requires AVX512BW.
func DbsadEpu8(a M512i, b M512i, imm8 int) M512i {
	return M512i(dbsadEpu8([64]byte(a), [64]byte(b), imm8))
}

func dbsadEpu8(a [64]byte, b [64]byte, imm8 int) [64]byte


// MaskDbsadEpu8: Compute the sum of absolute differences (SADs) of quadruplets
// of unsigned 8-bit integers in 'a' compared to those in 'b', and store the
// 16-bit results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set).
// 	Four SADs are performed on four 8-bit quadruplets for each 64-bit lane. The
// first two SADs use the lower 8-bit quadruplet of the lane from 'a', and the
// last two SADs use the uppper 8-bit quadruplet of the lane from 'a'.
// Quadruplets from 'b' are selected from within 128-bit lanes according to the
// control in 'imm8', and each SAD in each 64-bit lane uses the selected
// quadruplet at 8-bit offsets. 
//
//		FOR j := 0 to 3
//			i := j*128
//			tmp[i+31:i] := select(b[i+127:i], imm8[1:0])
//			tmp[i+63:i+32] := select(b[i+127:i], imm8[3:2])
//			tmp[i+95:i+64] := select(b[i+127:i], imm8[5:4])
//			tmp[i+127:i+96] := select(b[i+127:i], imm8[7:6])
//		ENDFOR
//		
//		FOR j := 0 to 7
//			i := j*64
//			tmp_dst[i+15:i] := ABS(a[i+7:i] - tmp[i+7:i]) + ABS(a[i+15:i+8] - tmp[i+15:i+8])
//						 + ABS(a[i+23:i+16] - tmp[i+23:i+16]) + ABS(a[i+31:i+24] - tmp[i+31:i+24])
//			
//			tmp_dst[i+31:i+16] := ABS(a[i+7:i] - tmp[i+15:i+8]) + ABS(a[i+15:i+8] - tmp[i+23:i+16])
//						 + ABS(a[i+23:i+16] - tmp[i+31:i+24]) + ABS(a[i+31:i+24] - tmp[i+39:i+32])
//			
//			tmp_dst[i+47:i+32] := ABS(a[i+39:i+32] - tmp[i+23:i+16]) + ABS(a[i+47:i+40] - tmp[i+31:i+24])
//						 + ABS(a[i+55:i+48] - tmp[i+39:i+32]) + ABS(a[i+63:i+56] - tmp[i+47:i+40])
//			
//			tmp_dst[i+63:i+48] := ABS(a[i+39:i+32] - tmp[i+31:i+24]) + ABS(a[i+47:i+40] - tmp[i+39:i+32])
//						 + ABS(a[i+55:i+48] - tmp[i+47:i+40]) + ABS(a[i+63:i+56] - tmp[i+55:i+48])
//		ENDFOR
//		
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VDBPSADBW'. Intrinsic: '_mm512_mask_dbsad_epu8'.
// Requires AVX512BW.
func MaskDbsadEpu8(src M512i, k Mmask32, a M512i, b M512i, imm8 int) M512i {
	return M512i(maskDbsadEpu8([64]byte(src), uint32(k), [64]byte(a), [64]byte(b), imm8))
}

func maskDbsadEpu8(src [64]byte, k uint32, a [64]byte, b [64]byte, imm8 int) [64]byte


// MaskzDbsadEpu8: Compute the sum of absolute differences (SADs) of
// quadruplets of unsigned 8-bit integers in 'a' compared to those in 'b', and
// store the 16-bit results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set).
// 	Four SADs are performed on four 8-bit quadruplets for each 64-bit lane. The
// first two SADs use the lower 8-bit quadruplet of the lane from 'a', and the
// last two SADs use the uppper 8-bit quadruplet of the lane from 'a'.
// Quadruplets from 'b' are selected from within 128-bit lanes according to the
// control in 'imm8', and each SAD in each 64-bit lane uses the selected
// quadruplet at 8-bit offsets. 
//
//		FOR j := 0 to 3
//			i := j*128
//			tmp[i+31:i] := select(b[i+127:i], imm8[1:0])
//			tmp[i+63:i+32] := select(b[i+127:i], imm8[3:2])
//			tmp[i+95:i+64] := select(b[i+127:i], imm8[5:4])
//			tmp[i+127:i+96] := select(b[i+127:i], imm8[7:6])
//		ENDFOR
//		
//		FOR j := 0 to 7
//			i := j*64
//			tmp_dst[i+15:i] := ABS(a[i+7:i] - tmp[i+7:i]) + ABS(a[i+15:i+8] - tmp[i+15:i+8])
//						 + ABS(a[i+23:i+16] - tmp[i+23:i+16]) + ABS(a[i+31:i+24] - tmp[i+31:i+24])
//			
//			tmp_dst[i+31:i+16] := ABS(a[i+7:i] - tmp[i+15:i+8]) + ABS(a[i+15:i+8] - tmp[i+23:i+16])
//						 + ABS(a[i+23:i+16] - tmp[i+31:i+24]) + ABS(a[i+31:i+24] - tmp[i+39:i+32])
//			
//			tmp_dst[i+47:i+32] := ABS(a[i+39:i+32] - tmp[i+23:i+16]) + ABS(a[i+47:i+40] - tmp[i+31:i+24])
//						 + ABS(a[i+55:i+48] - tmp[i+39:i+32]) + ABS(a[i+63:i+56] - tmp[i+47:i+40])
//			
//			tmp_dst[i+63:i+48] := ABS(a[i+39:i+32] - tmp[i+31:i+24]) + ABS(a[i+47:i+40] - tmp[i+39:i+32])
//						 + ABS(a[i+55:i+48] - tmp[i+47:i+40]) + ABS(a[i+63:i+56] - tmp[i+55:i+48])
//		ENDFOR
//		
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VDBPSADBW'. Intrinsic: '_mm512_maskz_dbsad_epu8'.
// Requires AVX512BW.
func MaskzDbsadEpu8(k Mmask32, a M512i, b M512i, imm8 int) M512i {
	return M512i(maskzDbsadEpu8(uint32(k), [64]byte(a), [64]byte(b), imm8))
}

func maskzDbsadEpu8(k uint32, a [64]byte, b [64]byte, imm8 int) [64]byte


// DivEpi16: Divide packed 16-bit integers in 'a' by packed elements in 'b',
// and store the truncated results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := 16*j
//			dst[i+15:i] := TRUNCATE(a[i+15:i] / b[i+15:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_div_epi16'.
// Requires AVX512F.
func DivEpi16(a M512i, b M512i) M512i {
	return M512i(divEpi16([64]byte(a), [64]byte(b)))
}

func divEpi16(a [64]byte, b [64]byte) [64]byte


// DivEpi32: Divide packed 32-bit integers in 'a' by packed elements in 'b',
// and store the truncated results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := TRUNCATE(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_div_epi32'.
// Requires AVX512F.
func DivEpi32(a M512i, b M512i) M512i {
	return M512i(divEpi32([64]byte(a), [64]byte(b)))
}

func divEpi32(a [64]byte, b [64]byte) [64]byte


// MaskDivEpi32: Divide packed 32-bit integers in 'a' by packed elements in
// 'b', and store the truncated results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := TRUNCATE(a[i+31:i] / b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_div_epi32'.
// Requires AVX512F.
func MaskDivEpi32(src M512i, k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskDivEpi32([64]byte(src), uint16(k), [64]byte(a), [64]byte(b)))
}

func maskDivEpi32(src [64]byte, k uint16, a [64]byte, b [64]byte) [64]byte


// DivEpi64: Divide packed 64-bit integers in 'a' by packed elements in 'b',
// and store the truncated results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			dst[i+63:i] := TRUNCATE(a[i+63:i] / b[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_div_epi64'.
// Requires AVX512F.
func DivEpi64(a M512i, b M512i) M512i {
	return M512i(divEpi64([64]byte(a), [64]byte(b)))
}

func divEpi64(a [64]byte, b [64]byte) [64]byte


// DivEpi8: Divide packed 8-bit integers in 'a' by packed elements in 'b', and
// store the truncated results in 'dst'. 
//
//		FOR j := 0 to 63
//			i := 8*j
//			dst[i+7:i] := TRUNCATE(a[i+7:i] / b[i+7:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_div_epi8'.
// Requires AVX512F.
func DivEpi8(a M512i, b M512i) M512i {
	return M512i(divEpi8([64]byte(a), [64]byte(b)))
}

func divEpi8(a [64]byte, b [64]byte) [64]byte


// DivEpu16: Divide packed unsigned 16-bit integers in 'a' by packed elements
// in 'b', and store the truncated results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := 16*j
//			dst[i+15:i] := TRUNCATE(a[i+15:i] / b[i+15:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_div_epu16'.
// Requires AVX512F.
func DivEpu16(a M512i, b M512i) M512i {
	return M512i(divEpu16([64]byte(a), [64]byte(b)))
}

func divEpu16(a [64]byte, b [64]byte) [64]byte


// DivEpu32: Divide packed unsigned 32-bit integers in 'a' by packed elements
// in 'b', and store the truncated results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := TRUNCATE(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_div_epu32'.
// Requires AVX512F.
func DivEpu32(a M512i, b M512i) M512i {
	return M512i(divEpu32([64]byte(a), [64]byte(b)))
}

func divEpu32(a [64]byte, b [64]byte) [64]byte


// MaskDivEpu32: Divide packed unsigned 32-bit integers in 'a' by packed
// elements in 'b', and store the truncated results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := TRUNCATE(a[i+31:i] / b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_div_epu32'.
// Requires AVX512F.
func MaskDivEpu32(src M512i, k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskDivEpu32([64]byte(src), uint16(k), [64]byte(a), [64]byte(b)))
}

func maskDivEpu32(src [64]byte, k uint16, a [64]byte, b [64]byte) [64]byte


// DivEpu64: Divide packed unsigned 64-bit integers in 'a' by packed elements
// in 'b', and store the truncated results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			dst[i+63:i] := TRUNCATE(a[i+63:i] / b[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_div_epu64'.
// Requires AVX512F.
func DivEpu64(a M512i, b M512i) M512i {
	return M512i(divEpu64([64]byte(a), [64]byte(b)))
}

func divEpu64(a [64]byte, b [64]byte) [64]byte


// DivEpu8: Divide packed unsigned 8-bit integers in 'a' by packed elements in
// 'b', and store the truncated results in 'dst'. 
//
//		FOR j := 0 to 63
//			i := 8*j
//			dst[i+7:i] := TRUNCATE(a[i+7:i] / b[i+7:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_div_epu8'.
// Requires AVX512F.
func DivEpu8(a M512i, b M512i) M512i {
	return M512i(divEpu8([64]byte(a), [64]byte(b)))
}

func divEpu8(a [64]byte, b [64]byte) [64]byte


// DivPd: Divide packed double-precision (64-bit) floating-point elements in
// 'a' by packed elements in 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			dst[i+63:i] := a[i+63:i] / b[i+63:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VDIVPD'. Intrinsic: '_mm512_div_pd'.
// Requires AVX512F.
func DivPd(a M512d, b M512d) M512d {
	return M512d(divPd([8]float64(a), [8]float64(b)))
}

func divPd(a [8]float64, b [8]float64) [8]float64


// MaskDivPd: Divide packed double-precision (64-bit) floating-point elements
// in 'a' by packed elements in 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] / b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VDIVPD'. Intrinsic: '_mm512_mask_div_pd'.
// Requires AVX512F.
func MaskDivPd(src M512d, k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskDivPd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b)))
}

func maskDivPd(src [8]float64, k uint8, a [8]float64, b [8]float64) [8]float64


// MaskzDivPd: Divide packed double-precision (64-bit) floating-point elements
// in 'a' by packed elements in 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] / b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VDIVPD'. Intrinsic: '_mm512_maskz_div_pd'.
// Requires AVX512F.
func MaskzDivPd(k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskzDivPd(uint8(k), [8]float64(a), [8]float64(b)))
}

func maskzDivPd(k uint8, a [8]float64, b [8]float64) [8]float64


// DivPs: Divide packed single-precision (32-bit) floating-point elements in
// 'a' by packed elements in 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := a[i+31:i] / b[i+31:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VDIVPS'. Intrinsic: '_mm512_div_ps'.
// Requires AVX512F.
func DivPs(a M512, b M512) M512 {
	return M512(divPs([16]float32(a), [16]float32(b)))
}

func divPs(a [16]float32, b [16]float32) [16]float32


// MaskDivPs: Divide packed single-precision (32-bit) floating-point elements
// in 'a' by packed elements in 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] / b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VDIVPS'. Intrinsic: '_mm512_mask_div_ps'.
// Requires AVX512F.
func MaskDivPs(src M512, k Mmask16, a M512, b M512) M512 {
	return M512(maskDivPs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b)))
}

func maskDivPs(src [16]float32, k uint16, a [16]float32, b [16]float32) [16]float32


// MaskzDivPs: Divide packed single-precision (32-bit) floating-point elements
// in 'a' by packed elements in 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] / b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VDIVPS'. Intrinsic: '_mm512_maskz_div_ps'.
// Requires AVX512F.
func MaskzDivPs(k Mmask16, a M512, b M512) M512 {
	return M512(maskzDivPs(uint16(k), [16]float32(a), [16]float32(b)))
}

func maskzDivPs(k uint16, a [16]float32, b [16]float32) [16]float32


// DivRoundPd: Divide packed double-precision (64-bit) floating-point elements
// in 'a' by packed elements in 'b', =and store the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := 64*j
//			dst[i+63:i] := a[i+63:i] / b[i+63:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VDIVPD'. Intrinsic: '_mm512_div_round_pd'.
// Requires AVX512F.
func DivRoundPd(a M512d, b M512d, rounding int) M512d {
	return M512d(divRoundPd([8]float64(a), [8]float64(b), rounding))
}

func divRoundPd(a [8]float64, b [8]float64, rounding int) [8]float64


// MaskDivRoundPd: Divide packed double-precision (64-bit) floating-point
// elements in 'a' by packed elements in 'b', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := 64*j
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] / b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VDIVPD'. Intrinsic: '_mm512_mask_div_round_pd'.
// Requires AVX512F.
func MaskDivRoundPd(src M512d, k Mmask8, a M512d, b M512d, rounding int) M512d {
	return M512d(maskDivRoundPd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b), rounding))
}

func maskDivRoundPd(src [8]float64, k uint8, a [8]float64, b [8]float64, rounding int) [8]float64


// MaskzDivRoundPd: Divide packed double-precision (64-bit) floating-point
// elements in 'a' by packed elements in 'b', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := 64*j
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] / b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VDIVPD'. Intrinsic: '_mm512_maskz_div_round_pd'.
// Requires AVX512F.
func MaskzDivRoundPd(k Mmask8, a M512d, b M512d, rounding int) M512d {
	return M512d(maskzDivRoundPd(uint8(k), [8]float64(a), [8]float64(b), rounding))
}

func maskzDivRoundPd(k uint8, a [8]float64, b [8]float64, rounding int) [8]float64


// DivRoundPs: Divide packed single-precision (32-bit) floating-point elements
// in 'a' by packed elements in 'b', and store the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := a[i+31:i] / b[i+31:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VDIVPS'. Intrinsic: '_mm512_div_round_ps'.
// Requires AVX512F.
func DivRoundPs(a M512, b M512, rounding int) M512 {
	return M512(divRoundPs([16]float32(a), [16]float32(b), rounding))
}

func divRoundPs(a [16]float32, b [16]float32, rounding int) [16]float32


// MaskDivRoundPs: Divide packed single-precision (32-bit) floating-point
// elements in 'a' by packed elements in 'b', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] / b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VDIVPS'. Intrinsic: '_mm512_mask_div_round_ps'.
// Requires AVX512F.
func MaskDivRoundPs(src M512, k Mmask16, a M512, b M512, rounding int) M512 {
	return M512(maskDivRoundPs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b), rounding))
}

func maskDivRoundPs(src [16]float32, k uint16, a [16]float32, b [16]float32, rounding int) [16]float32


// MaskzDivRoundPs: Divide packed single-precision (32-bit) floating-point
// elements in 'a' by packed elements in 'b', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] / b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VDIVPS'. Intrinsic: '_mm512_maskz_div_round_ps'.
// Requires AVX512F.
func MaskzDivRoundPs(k Mmask16, a M512, b M512, rounding int) M512 {
	return M512(maskzDivRoundPs(uint16(k), [16]float32(a), [16]float32(b), rounding))
}

func maskzDivRoundPs(k uint16, a [16]float32, b [16]float32, rounding int) [16]float32


// ErfPd: Compute the error function of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ERF(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_erf_pd'.
// Requires AVX512F.
func ErfPd(a M512d) M512d {
	return M512d(erfPd([8]float64(a)))
}

func erfPd(a [8]float64) [8]float64


// MaskErfPd: Compute the error function of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ERF(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_erf_pd'.
// Requires AVX512F.
func MaskErfPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskErfPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskErfPd(src [8]float64, k uint8, a [8]float64) [8]float64


// ErfPs: Compute the error function of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ERF(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_erf_ps'.
// Requires AVX512F.
func ErfPs(a M512) M512 {
	return M512(erfPs([16]float32(a)))
}

func erfPs(a [16]float32) [16]float32


// MaskErfPs: Compute the error function of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ERF(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_erf_ps'.
// Requires AVX512F.
func MaskErfPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskErfPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskErfPs(src [16]float32, k uint16, a [16]float32) [16]float32


// ErfcPd: Compute the complementary error function of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := 1.0 - ERF(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_erfc_pd'.
// Requires AVX512F.
func ErfcPd(a M512d) M512d {
	return M512d(erfcPd([8]float64(a)))
}

func erfcPd(a [8]float64) [8]float64


// MaskErfcPd: Compute the complementary error function of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := 1.0 - ERF(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_erfc_pd'.
// Requires AVX512F.
func MaskErfcPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskErfcPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskErfcPd(src [8]float64, k uint8, a [8]float64) [8]float64


// ErfcPs: Compute the complementary error function of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := 1.0 - ERF(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_erfc_ps'.
// Requires AVX512F.
func ErfcPs(a M512) M512 {
	return M512(erfcPs([16]float32(a)))
}

func erfcPs(a [16]float32) [16]float32


// MaskErfcPs: Compute the complementary error function of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := 1.0 - ERF(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_erfc_ps'.
// Requires AVX512F.
func MaskErfcPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskErfcPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskErfcPs(src [16]float32, k uint16, a [16]float32) [16]float32


// ErfcinvPd: Compute the inverse complementary error function of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := 1.0 / (1.0 - ERF(a[i+63:i]))
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_erfcinv_pd'.
// Requires AVX512F.
func ErfcinvPd(a M512d) M512d {
	return M512d(erfcinvPd([8]float64(a)))
}

func erfcinvPd(a [8]float64) [8]float64


// MaskErfcinvPd: Compute the inverse complementary error function of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := 1.0 / (1.0 - ERF(a[i+63:i]))
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_erfcinv_pd'.
// Requires AVX512F.
func MaskErfcinvPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskErfcinvPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskErfcinvPd(src [8]float64, k uint8, a [8]float64) [8]float64


// ErfcinvPs: Compute the inverse complementary error function of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := 1.0 / (1.0 - ERF(a[i+31:i]))
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_erfcinv_ps'.
// Requires AVX512F.
func ErfcinvPs(a M512) M512 {
	return M512(erfcinvPs([16]float32(a)))
}

func erfcinvPs(a [16]float32) [16]float32


// MaskErfcinvPs: Compute the inverse complementary error function of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := 1.0 / (1.0 - ERF(a[i+31:i]))
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_erfcinv_ps'.
// Requires AVX512F.
func MaskErfcinvPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskErfcinvPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskErfcinvPs(src [16]float32, k uint16, a [16]float32) [16]float32


// ErfinvPd: Compute the inverse error function of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := 1.0 / ERF(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_erfinv_pd'.
// Requires AVX512F.
func ErfinvPd(a M512d) M512d {
	return M512d(erfinvPd([8]float64(a)))
}

func erfinvPd(a [8]float64) [8]float64


// MaskErfinvPd: Compute the inverse error function of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := 1.0 / ERF(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_erfinv_pd'.
// Requires AVX512F.
func MaskErfinvPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskErfinvPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskErfinvPd(src [8]float64, k uint8, a [8]float64) [8]float64


// ErfinvPs: Compute the inverse error function of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := 1.0 / ERF(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_erfinv_ps'.
// Requires AVX512F.
func ErfinvPs(a M512) M512 {
	return M512(erfinvPs([16]float32(a)))
}

func erfinvPs(a [16]float32) [16]float32


// MaskErfinvPs: Compute the inverse error function of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := 1.0 / ERF(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_erfinv_ps'.
// Requires AVX512F.
func MaskErfinvPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskErfinvPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskErfinvPs(src [16]float32, k uint16, a [16]float32) [16]float32


// ExpPd: Compute the exponential value of 'e' raised to the power of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := e^(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_exp_pd'.
// Requires AVX512F.
func ExpPd(a M512d) M512d {
	return M512d(expPd([8]float64(a)))
}

func expPd(a [8]float64) [8]float64


// MaskExpPd: Compute the exponential value of 'e' raised to the power of
// packed double-precision (64-bit) floating-point elements in 'a', and store
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := e^(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_exp_pd'.
// Requires AVX512F.
func MaskExpPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskExpPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskExpPd(src [8]float64, k uint8, a [8]float64) [8]float64


// ExpPs: Compute the exponential value of 'e' raised to the power of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := e^(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_exp_ps'.
// Requires AVX512F.
func ExpPs(a M512) M512 {
	return M512(expPs([16]float32(a)))
}

func expPs(a [16]float32) [16]float32


// MaskExpPs: Compute the exponential value of 'e' raised to the power of
// packed single-precision (32-bit) floating-point elements in 'a', and store
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := e^(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_exp_ps'.
// Requires AVX512F.
func MaskExpPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskExpPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskExpPs(src [16]float32, k uint16, a [16]float32) [16]float32


// Exp10Pd: Compute the exponential value of 10 raised to the power of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := 10^(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_exp10_pd'.
// Requires AVX512F.
func Exp10Pd(a M512d) M512d {
	return M512d(exp10Pd([8]float64(a)))
}

func exp10Pd(a [8]float64) [8]float64


// MaskExp10Pd: Compute the exponential value of 10 raised to the power of
// packed double-precision (64-bit) floating-point elements in 'a', and store
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := 10^(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_exp10_pd'.
// Requires AVX512F.
func MaskExp10Pd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskExp10Pd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskExp10Pd(src [8]float64, k uint8, a [8]float64) [8]float64


// Exp10Ps: Compute the exponential value of 10 raised to the power of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := 10^(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_exp10_ps'.
// Requires AVX512F.
func Exp10Ps(a M512) M512 {
	return M512(exp10Ps([16]float32(a)))
}

func exp10Ps(a [16]float32) [16]float32


// MaskExp10Ps: Compute the exponential value of 10 raised to the power of
// packed single-precision (32-bit) floating-point elements in 'a', and store
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := 10^(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_exp10_ps'.
// Requires AVX512F.
func MaskExp10Ps(src M512, k Mmask16, a M512) M512 {
	return M512(maskExp10Ps([16]float32(src), uint16(k), [16]float32(a)))
}

func maskExp10Ps(src [16]float32, k uint16, a [16]float32) [16]float32


// Exp2Pd: Compute the exponential value of 2 raised to the power of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := 2^(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_exp2_pd'.
// Requires AVX512F.
func Exp2Pd(a M512d) M512d {
	return M512d(exp2Pd([8]float64(a)))
}

func exp2Pd(a [8]float64) [8]float64


// MaskExp2Pd: Compute the exponential value of 2 raised to the power of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := 2^(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_exp2_pd'.
// Requires AVX512F.
func MaskExp2Pd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskExp2Pd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskExp2Pd(src [8]float64, k uint8, a [8]float64) [8]float64


// Exp2Ps: Compute the exponential value of 2 raised to the power of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := 2^(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_exp2_ps'.
// Requires AVX512F.
func Exp2Ps(a M512) M512 {
	return M512(exp2Ps([16]float32(a)))
}

func exp2Ps(a [16]float32) [16]float32


// MaskExp2Ps: Compute the exponential value of 2 raised to the power of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := 2^(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_exp2_ps'.
// Requires AVX512F.
func MaskExp2Ps(src M512, k Mmask16, a M512) M512 {
	return M512(maskExp2Ps([16]float32(src), uint16(k), [16]float32(a)))
}

func maskExp2Ps(src [16]float32, k uint16, a [16]float32) [16]float32


// Exp223Ps: Approximates the base-2 exponent of the packed single-precision
// (32-bit) floating-point elements in 'v2' with eight bits for sign and
// magnitude and 24 bits for the fractional part. Results are stored in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := exp223(v2[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VEXP223PS'. Intrinsic: '_mm512_exp223_ps'.
// Requires KNCNI.
func Exp223Ps(v2 M512i) M512 {
	return M512(exp223Ps([64]byte(v2)))
}

func exp223Ps(v2 [64]byte) [16]float32


// MaskExp223Ps: Approximates the base-2 exponent of the packed
// single-precision (32-bit) floating-point elements in 'v2' with eight bits
// for sign and magnitude and 24 bits for the fractional part. Results are
// stored in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := exp223(v2[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VEXP223PS'. Intrinsic: '_mm512_mask_exp223_ps'.
// Requires KNCNI.
func MaskExp223Ps(src M512, k Mmask16, v2 M512i) M512 {
	return M512(maskExp223Ps([16]float32(src), uint16(k), [64]byte(v2)))
}

func maskExp223Ps(src [16]float32, k uint16, v2 [64]byte) [16]float32


// Exp2a23Pd: Compute the approximate exponential value of 2 raised to the
// power of packed double-precision (64-bit) floating-point elements in 'a',
// and store the results in 'dst'. The maximum relative error for this
// approximation is less than 2^-23. 
//
//		FOR j := 0 to 7
//			i := j*64;
//			dst[i+63:i] := EXP_2_23_DP(a[i+63:i]);
//		ENDFOR;
//		dst[MAX:512] := 0
//
// Instruction: 'VEXP2PD'. Intrinsic: '_mm512_exp2a23_pd'.
// Requires AVX512ER.
func Exp2a23Pd(a M512d) M512d {
	return M512d(exp2a23Pd([8]float64(a)))
}

func exp2a23Pd(a [8]float64) [8]float64


// MaskExp2a23Pd: Compute the approximate exponential value of 2 raised to the
// power of packed double-precision (64-bit) floating-point elements in 'a',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). The maximum relative
// error for this approximation is less than 2^-23. 
//
//		FOR j := 0 to 7
//			i := j*64;
//			IF k[j] THEN
//				dst[i+63:i] := EXP_2_23_DP(a[i+63:i]);
//			ELSE
//				dst[i+63:i] := src[i+63:i];
//			FI
//		ENDFOR;
//		dst[MAX:512] := 0
//
// Instruction: 'VEXP2PD'. Intrinsic: '_mm512_mask_exp2a23_pd'.
// Requires AVX512ER.
func MaskExp2a23Pd(a M512d, k Mmask8, src M512d) M512d {
	return M512d(maskExp2a23Pd([8]float64(a), uint8(k), [8]float64(src)))
}

func maskExp2a23Pd(a [8]float64, k uint8, src [8]float64) [8]float64


// MaskzExp2a23Pd: Compute the approximate exponential value of 2 raised to the
// power of packed double-precision (64-bit) floating-point elements in 'a',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). The maximum relative error for
// this approximation is less than 2^-23. 
//
//		FOR j := 0 to 7
//			i := j*64;
//			IF k[j] THEN
//				dst[i+63:i] := EXP_2_23_DP(a[i+63:i]);
//			ELSE
//				dst[i+63:i] := 0;
//			FI
//		ENDFOR;
//		dst[MAX:512] := 0
//
// Instruction: 'VEXP2PD'. Intrinsic: '_mm512_maskz_exp2a23_pd'.
// Requires AVX512ER.
func MaskzExp2a23Pd(k Mmask8, a M512d) M512d {
	return M512d(maskzExp2a23Pd(uint8(k), [8]float64(a)))
}

func maskzExp2a23Pd(k uint8, a [8]float64) [8]float64


// Exp2a23Ps: Compute the approximate exponential value of 2 raised to the
// power of packed single-precision (32-bit) floating-point elements in 'a',
// and store the results in 'dst'. The maximum relative error for this
// approximation is less than 2^-23. 
//
//		FOR j := 0 to 15
//			i := j*32;
//			dst[i+31:i] := EXP_2_23_SP(a[i+31:i]);
//		ENDFOR;
//		dst[MAX:512] := 0
//
// Instruction: 'VEXP2PS'. Intrinsic: '_mm512_exp2a23_ps'.
// Requires AVX512ER.
func Exp2a23Ps(a M512) M512 {
	return M512(exp2a23Ps([16]float32(a)))
}

func exp2a23Ps(a [16]float32) [16]float32


// MaskExp2a23Ps: Compute the approximate exponential value of 2 raised to the
// power of packed single-precision (32-bit) floating-point elements in 'a',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). The maximum relative
// error for this approximation is less than 2^-23. 
//
//		FOR j := 0 to 15
//			i := j*32;
//			IF k[j] THEN
//				dst[i+31:i] := EXP_2_23_SP(a[i+31:i]);
//			ELSE
//				dst[i*31:i] := src[i*31:i];
//			FI
//		ENDFOR;
//		dst[MAX:512] := 0
//
// Instruction: 'VEXP2PS'. Intrinsic: '_mm512_mask_exp2a23_ps'.
// Requires AVX512ER.
func MaskExp2a23Ps(src M512, k Mmask16, a M512) M512 {
	return M512(maskExp2a23Ps([16]float32(src), uint16(k), [16]float32(a)))
}

func maskExp2a23Ps(src [16]float32, k uint16, a [16]float32) [16]float32


// MaskzExp2a23Ps: Compute the approximate exponential value of 2 raised to the
// power of packed single-precision (32-bit) floating-point elements in 'a',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). The maximum relative error for
// this approximation is less than 2^-23. 
//
//		FOR j := 0 to 15
//			i := j*32;
//			IF k[j] THEN
//				dst[i+31:i] := EXP_2_23_SP(a[i+31:i]);
//			ELSE
//				dst[i*31:i] := 0;
//			FI
//		ENDFOR;
//		dst[MAX:512] := 0
//
// Instruction: 'VEXP2PS'. Intrinsic: '_mm512_maskz_exp2a23_ps'.
// Requires AVX512ER.
func MaskzExp2a23Ps(k Mmask16, a M512) M512 {
	return M512(maskzExp2a23Ps(uint16(k), [16]float32(a)))
}

func maskzExp2a23Ps(k uint16, a [16]float32) [16]float32


// Exp2a23RoundPd: Compute the approximate exponential value of 2 raised to the
// power of packed double-precision (64-bit) floating-point elements in 'a',
// and store the results in 'dst'. The maximum relative error for this
// approximation is less than 2^-23. Rounding is done according to the
// 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64;
//			dst[i+63:i] := EXP_2_23_DP(a[i+63:i]);
//		ENDFOR;
//		dst[MAX:512] := 0
//
// Instruction: 'VEXP2PD'. Intrinsic: '_mm512_exp2a23_round_pd'.
// Requires AVX512ER.
func Exp2a23RoundPd(a M512d, rounding int) M512d {
	return M512d(exp2a23RoundPd([8]float64(a), rounding))
}

func exp2a23RoundPd(a [8]float64, rounding int) [8]float64


// MaskExp2a23RoundPd: Compute the approximate exponential value of 2 raised to
// the power of packed double-precision (64-bit) floating-point elements in
// 'a', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). The maximum relative
// error for this approximation is less than 2^-23. Rounding is done according
// to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64;
//			IF k[j] THEN
//				dst[i+63:i] := EXP_2_23_DP(a[i+63:i]);
//			ELSE
//				dst[i+63:i] := src[i+63:i];
//			FI
//		ENDFOR;
//		dst[MAX:512] := 0
//
// Instruction: 'VEXP2PD'. Intrinsic: '_mm512_mask_exp2a23_round_pd'.
// Requires AVX512ER.
func MaskExp2a23RoundPd(a M512d, k Mmask8, src M512d, rounding int) M512d {
	return M512d(maskExp2a23RoundPd([8]float64(a), uint8(k), [8]float64(src), rounding))
}

func maskExp2a23RoundPd(a [8]float64, k uint8, src [8]float64, rounding int) [8]float64


// MaskzExp2a23RoundPd: Compute the approximate exponential value of 2 raised
// to the power of packed double-precision (64-bit) floating-point elements in
// 'a', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). The maximum relative error
// for this approximation is less than 2^-23. Rounding is done according to the
// 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64;
//			IF k[j] THEN
//				dst[i+63:i] := EXP_2_23_DP(a[i+63:i]);
//			ELSE
//				dst[i+63:i] := 0;
//			FI
//		ENDFOR;
//		dst[MAX:512] := 0
//
// Instruction: 'VEXP2PD'. Intrinsic: '_mm512_maskz_exp2a23_round_pd'.
// Requires AVX512ER.
func MaskzExp2a23RoundPd(k Mmask8, a M512d, rounding int) M512d {
	return M512d(maskzExp2a23RoundPd(uint8(k), [8]float64(a), rounding))
}

func maskzExp2a23RoundPd(k uint8, a [8]float64, rounding int) [8]float64


// Exp2a23RoundPs: Compute the approximate exponential value of 2 raised to the
// power of packed single-precision (32-bit) floating-point elements in 'a',
// and store the results in 'dst'. The maximum relative error for this
// approximation is less than 2^-23. Rounding is done according to the
// 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32;
//			dst[i+31:i] := EXP_2_23_SP(a[i+31:i]);
//		ENDFOR;
//		dst[MAX:512] := 0
//
// Instruction: 'VEXP2PS'. Intrinsic: '_mm512_exp2a23_round_ps'.
// Requires AVX512ER.
func Exp2a23RoundPs(a M512, rounding int) M512 {
	return M512(exp2a23RoundPs([16]float32(a), rounding))
}

func exp2a23RoundPs(a [16]float32, rounding int) [16]float32


// MaskExp2a23RoundPs: Compute the approximate exponential value of 2 raised to
// the power of packed single-precision (32-bit) floating-point elements in
// 'a', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). The maximum relative
// error for this approximation is less than 2^-23. Rounding is done according
// to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32;
//			IF k[j] THEN
//				dst[i+31:i] := EXP_2_23_SP(a[i+31:i]);
//			ELSE
//				dst[i*31:i] := src[i*31:i];
//			FI
//		ENDFOR;
//		dst[MAX:512] := 0
//
// Instruction: 'VEXP2PS'. Intrinsic: '_mm512_mask_exp2a23_round_ps'.
// Requires AVX512ER.
func MaskExp2a23RoundPs(src M512, k Mmask16, a M512, rounding int) M512 {
	return M512(maskExp2a23RoundPs([16]float32(src), uint16(k), [16]float32(a), rounding))
}

func maskExp2a23RoundPs(src [16]float32, k uint16, a [16]float32, rounding int) [16]float32


// MaskzExp2a23RoundPs: Compute the approximate exponential value of 2 raised
// to the power of packed single-precision (32-bit) floating-point elements in
// 'a', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). The maximum relative error
// for this approximation is less than 2^-23. Rounding is done according to the
// 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32;
//			IF k[j] THEN
//				dst[i+31:i] := EXP_2_23_SP(a[i+31:i]);
//			ELSE
//				dst[i*31:i] := 0;
//			FI
//		ENDFOR;
//		dst[MAX:512] := 0
//
// Instruction: 'VEXP2PS'. Intrinsic: '_mm512_maskz_exp2a23_round_ps'.
// Requires AVX512ER.
func MaskzExp2a23RoundPs(k Mmask16, a M512, rounding int) M512 {
	return M512(maskzExp2a23RoundPs(uint16(k), [16]float32(a), rounding))
}

func maskzExp2a23RoundPs(k uint16, a [16]float32, rounding int) [16]float32


// MaskExpandEpi32: Load contiguous active 32-bit integers from 'a' (those with
// their respective bit set in mask 'k'), and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		m := 0
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[m+31:m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPEXPANDD'. Intrinsic: '_mm512_mask_expand_epi32'.
// Requires AVX512F.
func MaskExpandEpi32(src M512i, k Mmask16, a M512i) M512i {
	return M512i(maskExpandEpi32([64]byte(src), uint16(k), [64]byte(a)))
}

func maskExpandEpi32(src [64]byte, k uint16, a [64]byte) [64]byte


// MaskzExpandEpi32: Load contiguous active 32-bit integers from 'a' (those
// with their respective bit set in mask 'k'), and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		m := 0
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[m+31:m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPEXPANDD'. Intrinsic: '_mm512_maskz_expand_epi32'.
// Requires AVX512F.
func MaskzExpandEpi32(k Mmask16, a M512i) M512i {
	return M512i(maskzExpandEpi32(uint16(k), [64]byte(a)))
}

func maskzExpandEpi32(k uint16, a [64]byte) [64]byte


// MaskExpandEpi64: Load contiguous active 64-bit integers from 'a' (those with
// their respective bit set in mask 'k'), and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[m+63:m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPEXPANDQ'. Intrinsic: '_mm512_mask_expand_epi64'.
// Requires AVX512F.
func MaskExpandEpi64(src M512i, k Mmask8, a M512i) M512i {
	return M512i(maskExpandEpi64([64]byte(src), uint8(k), [64]byte(a)))
}

func maskExpandEpi64(src [64]byte, k uint8, a [64]byte) [64]byte


// MaskzExpandEpi64: Load contiguous active 64-bit integers from 'a' (those
// with their respective bit set in mask 'k'), and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[m+63:m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPEXPANDQ'. Intrinsic: '_mm512_maskz_expand_epi64'.
// Requires AVX512F.
func MaskzExpandEpi64(k Mmask8, a M512i) M512i {
	return M512i(maskzExpandEpi64(uint8(k), [64]byte(a)))
}

func maskzExpandEpi64(k uint8, a [64]byte) [64]byte


// MaskExpandPd: Load contiguous active double-precision (64-bit)
// floating-point elements from 'a' (those with their respective bit set in
// mask 'k'), and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[m+63:m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VEXPANDPD'. Intrinsic: '_mm512_mask_expand_pd'.
// Requires AVX512F.
func MaskExpandPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskExpandPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskExpandPd(src [8]float64, k uint8, a [8]float64) [8]float64


// MaskzExpandPd: Load contiguous active double-precision (64-bit)
// floating-point elements from 'a' (those with their respective bit set in
// mask 'k'), and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[m+63:m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VEXPANDPD'. Intrinsic: '_mm512_maskz_expand_pd'.
// Requires AVX512F.
func MaskzExpandPd(k Mmask8, a M512d) M512d {
	return M512d(maskzExpandPd(uint8(k), [8]float64(a)))
}

func maskzExpandPd(k uint8, a [8]float64) [8]float64


// MaskExpandPs: Load contiguous active single-precision (32-bit)
// floating-point elements from 'a' (those with their respective bit set in
// mask 'k'), and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[m+31:m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VEXPANDPS'. Intrinsic: '_mm512_mask_expand_ps'.
// Requires AVX512F.
func MaskExpandPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskExpandPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskExpandPs(src [16]float32, k uint16, a [16]float32) [16]float32


// MaskzExpandPs: Load contiguous active single-precision (32-bit)
// floating-point elements from 'a' (those with their respective bit set in
// mask 'k'), and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[m+31:m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VEXPANDPS'. Intrinsic: '_mm512_maskz_expand_ps'.
// Requires AVX512F.
func MaskzExpandPs(k Mmask16, a M512) M512 {
	return M512(maskzExpandPs(uint16(k), [16]float32(a)))
}

func maskzExpandPs(k uint16, a [16]float32) [16]float32


// MaskExpandloaduEpi32: Load contiguous active 32-bit integers from unaligned
// memory at 'mem_addr' (those with their respective bit set in mask 'k'), and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPEXPANDD'. Intrinsic: '_mm512_mask_expandloadu_epi32'.
// Requires AVX512F.
func MaskExpandloaduEpi32(src M512i, k Mmask16, mem_addr uintptr) M512i {
	return M512i(maskExpandloaduEpi32([64]byte(src), uint16(k), uintptr(mem_addr)))
}

func maskExpandloaduEpi32(src [64]byte, k uint16, mem_addr uintptr) [64]byte


// MaskzExpandloaduEpi32: Load contiguous active 32-bit integers from unaligned
// memory at 'mem_addr' (those with their respective bit set in mask 'k'), and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPEXPANDD'. Intrinsic: '_mm512_maskz_expandloadu_epi32'.
// Requires AVX512F.
func MaskzExpandloaduEpi32(k Mmask16, mem_addr uintptr) M512i {
	return M512i(maskzExpandloaduEpi32(uint16(k), uintptr(mem_addr)))
}

func maskzExpandloaduEpi32(k uint16, mem_addr uintptr) [64]byte


// MaskExpandloaduEpi64: Load contiguous active 64-bit integers from unaligned
// memory at 'mem_addr' (those with their respective bit set in mask 'k'), and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPEXPANDQ'. Intrinsic: '_mm512_mask_expandloadu_epi64'.
// Requires AVX512F.
func MaskExpandloaduEpi64(src M512i, k Mmask8, mem_addr uintptr) M512i {
	return M512i(maskExpandloaduEpi64([64]byte(src), uint8(k), uintptr(mem_addr)))
}

func maskExpandloaduEpi64(src [64]byte, k uint8, mem_addr uintptr) [64]byte


// MaskzExpandloaduEpi64: Load contiguous active 64-bit integers from unaligned
// memory at 'mem_addr' (those with their respective bit set in mask 'k'), and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPEXPANDQ'. Intrinsic: '_mm512_maskz_expandloadu_epi64'.
// Requires AVX512F.
func MaskzExpandloaduEpi64(k Mmask8, mem_addr uintptr) M512i {
	return M512i(maskzExpandloaduEpi64(uint8(k), uintptr(mem_addr)))
}

func maskzExpandloaduEpi64(k uint8, mem_addr uintptr) [64]byte


// MaskExpandloaduPd: Load contiguous active double-precision (64-bit)
// floating-point elements from unaligned memory at 'mem_addr' (those with
// their respective bit set in mask 'k'), and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VEXPANDPD'. Intrinsic: '_mm512_mask_expandloadu_pd'.
// Requires AVX512F.
func MaskExpandloaduPd(src M512d, k Mmask8, mem_addr uintptr) M512d {
	return M512d(maskExpandloaduPd([8]float64(src), uint8(k), uintptr(mem_addr)))
}

func maskExpandloaduPd(src [8]float64, k uint8, mem_addr uintptr) [8]float64


// MaskzExpandloaduPd: Load contiguous active double-precision (64-bit)
// floating-point elements from unaligned memory at 'mem_addr' (those with
// their respective bit set in mask 'k'), and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VEXPANDPD'. Intrinsic: '_mm512_maskz_expandloadu_pd'.
// Requires AVX512F.
func MaskzExpandloaduPd(k Mmask8, mem_addr uintptr) M512d {
	return M512d(maskzExpandloaduPd(uint8(k), uintptr(mem_addr)))
}

func maskzExpandloaduPd(k uint8, mem_addr uintptr) [8]float64


// MaskExpandloaduPs: Load contiguous active single-precision (32-bit)
// floating-point elements from unaligned memory at 'mem_addr' (those with
// their respective bit set in mask 'k'), and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		m := 0
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VEXPANDPS'. Intrinsic: '_mm512_mask_expandloadu_ps'.
// Requires AVX512F.
func MaskExpandloaduPs(src M512, k Mmask16, mem_addr uintptr) M512 {
	return M512(maskExpandloaduPs([16]float32(src), uint16(k), uintptr(mem_addr)))
}

func maskExpandloaduPs(src [16]float32, k uint16, mem_addr uintptr) [16]float32


// MaskzExpandloaduPs: Load contiguous active single-precision (32-bit)
// floating-point elements from unaligned memory at 'mem_addr' (those with
// their respective bit set in mask 'k'), and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		m := 0
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VEXPANDPS'. Intrinsic: '_mm512_maskz_expandloadu_ps'.
// Requires AVX512F.
func MaskzExpandloaduPs(k Mmask16, mem_addr uintptr) M512 {
	return M512(maskzExpandloaduPs(uint16(k), uintptr(mem_addr)))
}

func maskzExpandloaduPs(k uint16, mem_addr uintptr) [16]float32


// Expm1Pd: Compute the exponential value of 'e' raised to the power of packed
// double-precision (64-bit) floating-point elements in 'a', subtract one from
// each element, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := e^(a[i+63:i]) - 1.0
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_expm1_pd'.
// Requires AVX512F.
func Expm1Pd(a M512d) M512d {
	return M512d(expm1Pd([8]float64(a)))
}

func expm1Pd(a [8]float64) [8]float64


// MaskExpm1Pd: Compute the exponential value of 'e' raised to the power of
// packed double-precision (64-bit) floating-point elements in 'a', subtract
// one from each element, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := e^(a[i+63:i]) - 1.0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_expm1_pd'.
// Requires AVX512F.
func MaskExpm1Pd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskExpm1Pd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskExpm1Pd(src [8]float64, k uint8, a [8]float64) [8]float64


// Expm1Ps: Compute the exponential value of 'e' raised to the power of packed
// single-precision (32-bit) floating-point elements in 'a', subtract one from
// each element, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := e^(a[i+31:i]) - 1.0
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_expm1_ps'.
// Requires AVX512F.
func Expm1Ps(a M512) M512 {
	return M512(expm1Ps([16]float32(a)))
}

func expm1Ps(a [16]float32) [16]float32


// MaskExpm1Ps: Compute the exponential value of 'e' raised to the power of
// packed single-precision (32-bit) floating-point elements in 'a', subtract
// one from each element, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := e^(a[i+31:i]) - 1.0
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_expm1_ps'.
// Requires AVX512F.
func MaskExpm1Ps(src M512, k Mmask16, a M512) M512 {
	return M512(maskExpm1Ps([16]float32(src), uint16(k), [16]float32(a)))
}

func maskExpm1Ps(src [16]float32, k uint16, a [16]float32) [16]float32


// ExtloadEpi32: Depending on 'bc', loads 1, 4, or 16 elements of type and size
// determined by 'conv' from memory address 'mt' and converts all elements to
// 32-bit integer elements, storing the results in 'dst'. 'hint' indicates to
// the processor whether the data is non-temporal. 
//
//		addr = MEM[mt]
//		FOR j := 0 to 15
//			i := j*32
//			CASE bc OF
//			_MM_BROADCAST32_NONE:
//				CASE conv OF
//				_MM_UPCONV_EPI32_NONE:
//					n	 := j*32
//					dst[i+31:i] := addr[n+31:n]
//				_MM_UPCONV_EPI32_UINT8:
//					n	 := j*8
//					dst[i+31:i] := UInt8ToInt32(addr[n+7:n])
//				_MM_UPCONV_EPI32_SINT8:
//					n	 := j*8
//					dst[i+31:i] := SInt8ToInt32(addr[n+7:n])
//				_MM_UPCONV_EPI32_UINT16:
//					n	 := j*16
//					dst[i+31:i] := UInt16ToInt32(addr[n+15:n])
//				_MM_UPCONV_EPI32_SINT16:
//					n	 := j*16
//					dst[i+31:i] := SInt16ToInt32(addr[n+15:n])
//				ESAC
//			_MM_BROADCAST_1X16:
//				CASE conv OF
//				_MM_UPCONV_EPI32_NONE:
//					n	 := j*32
//					dst[i+31:i] := addr[31:0]
//				_MM_UPCONV_EPI32_UINT8:
//					n	 := j*8
//					dst[i+31:i] := UInt8ToInt32(addr[7:0])
//				_MM_UPCONV_EPI32_SINT8:
//					n	 := j*8
//					dst[i+31:i] := SInt8ToInt32(addr[7:0])
//				_MM_UPCONV_EPI32_UINT16:
//					n	 := j*16
//					dst[i+31:i] := UInt16ToInt32(addr[15:0])
//				_MM_UPCONV_EPI32_SINT16:
//					n	 := j*16
//					dst[i+31:i] := SInt16ToInt32(addr[15:0])
//				ESAC
//			_MM_BROADCAST_4X16:
//				mod := j%4
//				CASE conv OF
//				_MM_UPCONV_EPI32_NONE:
//					n := mod*32
//					dst[i+31:i] := addr[n+31:n]
//				_MM_UPCONV_EPI32_UINT8:
//					n := mod*8
//					dst[i+31:i] := UInt8ToInt32(addr[n+7:n])
//				_MM_UPCONV_EPI32_SINT8:
//					n := mod*8
//					dst[i+31:i] := SInt8ToInt32(addr[n+7:n])
//				_MM_UPCONV_EPI32_UINT16:
//					n := mod*16
//					dst[i+31:i] := UInt16ToInt32(addr[n+15:n])
//				_MM_UPCONV_EPI32_SINT16:
//					n := mod*16
//					dst[i+31:i] := SInt16ToInt32(addr[n+15:n])
//				ESAC
//			ESAC
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDQA32, VBROADCASTI32X4, VPBROADCASTD'. Intrinsic: '_mm512_extload_epi32'.
// Requires KNCNI.
func ExtloadEpi32(mt uintptr, conv MMUPCONVEPI32ENUM, bc MMBROADCAST32ENUM, hint int) M512i {
	return M512i(extloadEpi32(uintptr(mt), conv, bc, hint))
}

func extloadEpi32(mt uintptr, conv MMUPCONVEPI32ENUM, bc MMBROADCAST32ENUM, hint int) [64]byte


// MaskExtloadEpi32: Depending on 'bc', loads 1, 4, or 16 elements of type and
// size determined by 'conv' from memory address 'mt' and converts all elements
// to 32-bit integer elements, storing the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set).
// 'hint' indicates to the processor whether the data is non-temporal. 
//
//		addr = MEM[mt]
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				CASE bc OF
//				_MM_BROADCAST32_NONE:
//					CASE conv OF
//					_MM_UPCONV_EPI32_NONE:
//						n	 := j*32
//						dst[i+31:i] := addr[n+31:n]
//					_MM_UPCONV_EPI32_UINT8:
//						n	 := j*8
//						dst[i+31:i] := UInt8ToInt32(addr[n+7:n])
//					_MM_UPCONV_EPI32_SINT8:
//						n	 := j*8
//						dst[i+31:i] := SInt8ToInt32(addr[n+7:n])
//					_MM_UPCONV_EPI32_UINT16:
//						n	 := j*16
//						dst[i+31:i] := UInt16ToInt32(addr[n+15:n])
//					_MM_UPCONV_EPI32_SINT16:
//						n	 := j*16
//						dst[i+31:i] := SInt16ToInt32(addr[n+15:n])
//					ESAC
//				_MM_BROADCAST_1X16:
//					CASE conv OF
//					_MM_UPCONV_EPI32_NONE:
//						n	 := j*32
//						dst[i+31:i] := addr[31:0]
//					_MM_UPCONV_EPI32_UINT8:
//						n	 := j*8
//						dst[i+31:i] := UInt8ToInt32(addr[7:0])
//					_MM_UPCONV_EPI32_SINT8:
//						n	 := j*8
//						dst[i+31:i] := SInt8ToInt32(addr[7:0])
//					_MM_UPCONV_EPI32_UINT16:
//						n	 := j*16
//						dst[i+31:i] := UInt16ToInt32(addr[15:0])
//					_MM_UPCONV_EPI32_SINT16:
//						n	 := j*16
//						dst[i+31:i] := SInt16ToInt32(addr[15:0])
//					ESAC
//				_MM_BROADCAST_4X16:
//					mod := j%4
//					CASE conv OF
//					_MM_UPCONV_EPI32_NONE:
//						n := mod*32
//						dst[i+31:i] := addr[n+31:n]
//					_MM_UPCONV_EPI32_UINT8:
//						n := mod*8
//						dst[i+31:i] := UInt8ToInt32(addr[n+7:n])
//					_MM_UPCONV_EPI32_SINT8:
//						n := mod*8
//						dst[i+31:i] := SInt8ToInt32(addr[n+7:n])
//					_MM_UPCONV_EPI32_UINT16:
//						n := mod*16
//						dst[i+31:i] := UInt16ToInt32(addr[n+15:n])
//					_MM_UPCONV_EPI32_SINT16:
//						n := mod*16
//						dst[i+31:i] := SInt16ToInt32(addr[n+15:n])
//					ESAC
//				ESAC
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDQA32, VBROADCASTI32X4, VPBROADCASTD'. Intrinsic: '_mm512_mask_extload_epi32'.
// Requires KNCNI.
func MaskExtloadEpi32(src M512i, k Mmask16, mt uintptr, conv MMUPCONVEPI32ENUM, bc MMBROADCAST32ENUM, hint int) M512i {
	return M512i(maskExtloadEpi32([64]byte(src), uint16(k), uintptr(mt), conv, bc, hint))
}

func maskExtloadEpi32(src [64]byte, k uint16, mt uintptr, conv MMUPCONVEPI32ENUM, bc MMBROADCAST32ENUM, hint int) [64]byte


// ExtloadEpi64: Depending on 'bc', loads 1, 4, or 8 elements of type and size
// determined by 'conv' from memory address 'mt' and converts all elements to
// 64-bit integer elements, storing the results in 'dst'. 'hint' indicates to
// the processor whether the data is non-temporal. 
//
//		addr = MEM[mt]
//		FOR j := 0 to 7
//			i := j*64
//			CASE bc OF
//			_MM_BROADCAST64_NONE:
//				CASE conv OF
//				_MM_UPCONV_EPI64_NONE:
//					n := j*64
//					dst[i+63:i] := addr[n+63:n]
//				ESAC
//			_MM_BROADCAST_1X8:
//				CASE conv OF
//				_MM_UPCONV_EPI64_NONE:
//					n := j*64
//					dst[i+63:i] := addr[63:0]
//				ESAC
//			_MM_BROADCAST_4X8:
//				mod := j%4
//				CASE conv OF
//				_MM_UPCONV_EPI64_NONE:
//					n := mod*64
//					dst[i+63:i] := addr[n+63:n]
//				ESAC
//			ESAC
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDQA64, VBROADCASTI64X4, VPBROADCASTQ'. Intrinsic: '_mm512_extload_epi64'.
// Requires KNCNI.
func ExtloadEpi64(mt uintptr, conv MMUPCONVEPI64ENUM, bc MMBROADCAST64ENUM, hint int) M512i {
	return M512i(extloadEpi64(uintptr(mt), conv, bc, hint))
}

func extloadEpi64(mt uintptr, conv MMUPCONVEPI64ENUM, bc MMBROADCAST64ENUM, hint int) [64]byte


// MaskExtloadEpi64: Depending on 'bc', loads 1, 4, or 8 elements of type and
// size determined by 'conv' from memory address 'mt' and converts all elements
// to 64-bit integer elements, storing the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set).
// 'hint' indicates to the processor whether the data is non-temporal. 
//
//		addr = MEM[mt]
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				CASE bc OF
//				_MM_BROADCAST64_NONE:
//					CASE conv OF
//					_MM_UPCONV_EPI64_NONE:
//						n := j*64
//						dst[i+63:i] := addr[n+63:n]
//					ESAC
//				_MM_BROADCAST_1X8:
//					CASE conv OF
//					_MM_UPCONV_EPI64_NONE:
//						n := j*64
//						dst[i+63:i] := addr[63:0]
//					ESAC
//				_MM_BROADCAST_4X8:
//					mod := j%4
//					CASE conv OF
//					_MM_UPCONV_EPI64_NONE:
//						n := mod*64
//						dst[i+63:i] := addr[n+63:n]
//					ESAC
//				ESAC
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDQA64, VBROADCASTI64X4, VPBROADCASTQ'. Intrinsic: '_mm512_mask_extload_epi64'.
// Requires KNCNI.
func MaskExtloadEpi64(src M512i, k Mmask8, mt uintptr, conv MMUPCONVEPI64ENUM, bc MMBROADCAST64ENUM, hint int) M512i {
	return M512i(maskExtloadEpi64([64]byte(src), uint8(k), uintptr(mt), conv, bc, hint))
}

func maskExtloadEpi64(src [64]byte, k uint8, mt uintptr, conv MMUPCONVEPI64ENUM, bc MMBROADCAST64ENUM, hint int) [64]byte


// ExtloadPd: Depending on 'bc', loads 1, 4, or 8 elements of type and size
// determined by 'conv' from memory address 'mt' and converts all elements to
// double-precision (64-bit) floating-point elements, storing the results in
// 'dst'. 'hint' indicates to the processor whether the data is non-temporal. 
//
//		addr = MEM[mt]
//		FOR j := 0 to 7
//			i := j*64
//			CASE bc OF
//			_MM_BROADCAST64_NONE:
//				CASE conv OF
//				_MM_UPCONV_PD_NONE:
//					n := j*64
//					dst[i+63:i] := addr[n+63:n]
//				ESAC
//			_MM_BROADCAST_1X8:
//				CASE conv OF
//				_MM_UPCONV_PD_NONE:
//					n := j*64
//					dst[i+63:i] := addr[63:0]
//				ESAC
//			_MM_BROADCAST_4X8:
//				mod := j%4
//				CASE conv OF
//				_MM_UPCONV_PD_NONE:
//					n := mod*64
//					dst[i+63:i] := addr[n+63:n]
//				ESAC
//			ESAC
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVAPD, VBROADCASTF64X4, VBROADCASTSD'. Intrinsic: '_mm512_extload_pd'.
// Requires KNCNI.
func ExtloadPd(mt uintptr, conv MMUPCONVPDENUM, bc MMBROADCAST64ENUM, hint int) M512d {
	return M512d(extloadPd(uintptr(mt), conv, bc, hint))
}

func extloadPd(mt uintptr, conv MMUPCONVPDENUM, bc MMBROADCAST64ENUM, hint int) [8]float64


// MaskExtloadPd: Depending on 'bc', loads 1, 4, or 8 elements of type and size
// determined by 'conv' from memory address 'mt' and converts all elements to
// double-precision (64-bit) floating-point elements, storing the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 'hint' indicates to the processor
// whether the data is non-temporal. 
//
//		addr = MEM[mt]
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				CASE bc OF
//				_MM_BROADCAST64_NONE:
//					CASE conv OF
//					_MM_UPCONV_PD_NONE:
//						n := j*64
//						dst[i+63:i] := addr[n+63:n]
//					ESAC
//				_MM_BROADCAST_1X8:
//					CASE conv OF
//					_MM_UPCONV_PD_NONE:
//						n := j*64
//						dst[i+63:i] := addr[63:0]
//					ESAC
//				_MM_BROADCAST_4X8:
//					mod := j%4
//					CASE conv OF
//					_MM_UPCONV_PD_NONE:
//						n := mod*64
//						dst[i+63:i] := addr[n+63:n]
//					ESAC
//				ESAC
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVAPD, VBROADCASTF64X4, VBROADCASTSD'. Intrinsic: '_mm512_mask_extload_pd'.
// Requires KNCNI.
func MaskExtloadPd(src M512d, k Mmask8, mt uintptr, conv MMUPCONVPDENUM, bc MMBROADCAST64ENUM, hint int) M512d {
	return M512d(maskExtloadPd([8]float64(src), uint8(k), uintptr(mt), conv, bc, hint))
}

func maskExtloadPd(src [8]float64, k uint8, mt uintptr, conv MMUPCONVPDENUM, bc MMBROADCAST64ENUM, hint int) [8]float64


// ExtloadPs: Depending on 'bc', loads 1, 4, or 16 elements of type and size
// determined by 'conv' from memory address 'mt' and converts all elements to
// single-precision (32-bit) floating-point elements, storing the results in
// 'dst'. 'hint' indicates to the processor whether the data is non-temporal. 
//
//		addr = MEM[mt]
//		FOR j := 0 to 15
//			i := j*32
//			CASE bc OF
//			_MM_BROADCAST32_NONE:
//				CASE conv OF
//				_MM_UPCONV_PS_NONE:
//					n	 := j*32
//					dst[i+31:i] := addr[n+31:n]
//				_MM_UPCONV_PS_FLOAT16:
//					n	 := j*16
//					dst[i+31:i] := Float16ToFloat32(addr[n+15:n])
//				_MM_UPCONV_PS_UINT8:
//					n	 := j*8
//					dst[i+31:i] := UInt8ToFloat32(addr[n+7:n])
//				_MM_UPCONV_PS_SINT8:
//					n	 := j*8
//					dst[i+31:i] := SInt8ToFloat32(addr[n+7:n])
//				_MM_UPCONV_PS_UINT16:
//					n	 := j*16
//					dst[i+31:i] := UInt16ToFloat32(addr[n+15:n])
//				_MM_UPCONV_PS_SINT16:
//					n	 := j*16
//					dst[i+31:i] := SInt16ToFloat32(addr[n+15:n])
//				ESAC
//			_MM_BROADCAST_1X16:
//				CASE conv OF
//				_MM_UPCONV_PS_NONE:
//					n	 := j*32
//					dst[i+31:i] := addr[31:0]
//				_MM_UPCONV_PS_FLOAT16:
//					n	 := j*16
//					dst[i+31:i] := Float16ToFloat32(addr[15:0])
//				_MM_UPCONV_PS_UINT8:
//					n	 := j*8
//					dst[i+31:i] := UInt8ToFloat32(addr[7:0])
//				_MM_UPCONV_PS_SINT8:
//					n	 := j*8
//					dst[i+31:i] := SInt8ToFloat32(addr[7:0])
//				_MM_UPCONV_PS_UINT16:
//					n	 := j*16
//					dst[i+31:i] := UInt16ToFloat32(addr[15:0])
//				_MM_UPCONV_PS_SINT16:
//					n	 := j*16
//					dst[i+31:i] := SInt16ToFloat32(addr[15:0])
//				ESAC
//			_MM_BROADCAST_4X16:
//				mod := j%4
//				CASE conv OF
//				_MM_UPCONV_PS_NONE:
//					n := mod*32
//					dst[i+31:i] := addr[n+31:n]
//				_MM_UPCONV_PS_FLOAT16:
//					n := mod*16
//					dst[i+31:i] := Float16ToFloat32(addr[n+15:n])
//				_MM_UPCONV_PS_UINT8:
//					n := mod*8
//					dst[i+31:i] := UInt8ToFloat32(addr[n+7:n])
//				_MM_UPCONV_PS_SINT8:
//					n := mod*8
//					dst[i+31:i] := SInt8ToFloat32(addr[n+7:n])
//				_MM_UPCONV_PS_UINT16:
//					n := mod*16
//					dst[i+31:i] := UInt16ToFloat32(addr[n+15:n])
//				_MM_UPCONV_PS_SINT16:
//					n := mod*16
//					dst[i+31:i] := SInt16ToFloat32(addr[n+15:n])
//				ESAC
//			ESAC
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVAPS, VBROADCASTF32X4, VBROADCASTSS'. Intrinsic: '_mm512_extload_ps'.
// Requires KNCNI.
func ExtloadPs(mt uintptr, conv MMUPCONVPSENUM, bc MMBROADCAST32ENUM, hint int) M512 {
	return M512(extloadPs(uintptr(mt), conv, bc, hint))
}

func extloadPs(mt uintptr, conv MMUPCONVPSENUM, bc MMBROADCAST32ENUM, hint int) [16]float32


// MaskExtloadPs: Depending on 'bc', loads 1, 4, or 16 elements of type and
// size determined by 'conv' from memory address 'mt' and converts all elements
// to single-precision (32-bit) floating-point elements, storing the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 'hint' indicates to the processor
// whether the data is non-temporal. 
//
//		addr = MEM[mt]
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				CASE bc OF
//				_MM_BROADCAST32_NONE:
//					CASE conv OF
//					_MM_UPCONV_PS_NONE:
//						n	 := j*32
//						dst[i+31:i] := addr[n+31:n]
//					_MM_UPCONV_PS_FLOAT16:
//						n	 := j*16
//						dst[i+31:i] := Float16ToFloat32(addr[n+15:n])
//					_MM_UPCONV_PS_UINT8:
//						n	 := j*8
//						dst[i+31:i] := UInt8ToFloat32(addr[n+7:n])
//					_MM_UPCONV_PS_SINT8:
//						n	 := j*8
//						dst[i+31:i] := SInt8ToFloat32(addr[n+7:n])
//					_MM_UPCONV_PS_UINT16:
//						n	 := j*16
//						dst[i+31:i] := UInt16ToFloat32(addr[n+15:n])
//					_MM_UPCONV_PS_SINT16:
//						n	 := j*16
//						dst[i+31:i] := SInt16ToFloat32(addr[n+15:n])
//					ESAC
//				_MM_BROADCAST_1X16:
//					CASE conv OF
//					_MM_UPCONV_PS_NONE:
//						n	 := j*32
//						dst[i+31:i] := addr[31:0]
//					_MM_UPCONV_PS_FLOAT16:
//						n	 := j*16
//						dst[i+31:i] := Float16ToFloat32(addr[15:0])
//					_MM_UPCONV_PS_UINT8:
//						n	 := j*8
//						dst[i+31:i] := UInt8ToFloat32(addr[7:0])
//					_MM_UPCONV_PS_SINT8:
//						n	 := j*8
//						dst[i+31:i] := SInt8ToFloat32(addr[7:0])
//					_MM_UPCONV_PS_UINT16:
//						n	 := j*16
//						dst[i+31:i] := UInt16ToFloat32(addr[15:0])
//					_MM_UPCONV_PS_SINT16:
//						n	 := j*16
//						dst[i+31:i] := SInt16ToFloat32(addr[15:0])
//					ESAC
//				_MM_BROADCAST_4X16:
//					mod := j%4
//					CASE conv OF
//					_MM_UPCONV_PS_NONE:
//						n := mod*32
//						dst[i+31:i] := addr[n+31:n]
//					_MM_UPCONV_PS_FLOAT16:
//						n := mod*16
//						dst[i+31:i] := Float16ToFloat32(addr[n+15:n])
//					_MM_UPCONV_PS_UINT8:
//						n := mod*8
//						dst[i+31:i] := UInt8ToFloat32(addr[n+7:n])
//					_MM_UPCONV_PS_SINT8:
//						n := mod*8
//						dst[i+31:i] := SInt8ToFloat32(addr[n+7:n])
//					_MM_UPCONV_PS_UINT16:
//						n := mod*16
//						dst[i+31:i] := UInt16ToFloat32(addr[n+15:n])
//					_MM_UPCONV_PS_SINT16:
//						n := mod*16
//						dst[i+31:i] := SInt16ToFloat32(addr[n+15:n])
//					ESAC
//				ESAC
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVAPS, VBROADCASTF32X4, VBROADCASTSS'. Intrinsic: '_mm512_mask_extload_ps'.
// Requires KNCNI.
func MaskExtloadPs(src M512, k Mmask16, mt uintptr, conv MMUPCONVPSENUM, bc MMBROADCAST32ENUM, hint int) M512 {
	return M512(maskExtloadPs([16]float32(src), uint16(k), uintptr(mt), conv, bc, hint))
}

func maskExtloadPs(src [16]float32, k uint16, mt uintptr, conv MMUPCONVPSENUM, bc MMBROADCAST32ENUM, hint int) [16]float32


// ExtloadunpackhiEpi32: Loads the high-64-byte-aligned portion of the
// byte/word/doubleword stream starting at element-aligned address mt-64,
// up-converted depending on the value of 'conv', and expanded into packed
// 32-bit integers in 'dst'. The initial values of 'dst' are copied from 'src'.
// Only those converted doublewords that occur at or after the first
// 64-byte-aligned address following (mt-64) are loaded. Elements in the
// resulting vector that do not map to those doublewords are taken from 'src'.
// 'hint' indicates to the processor whether the loaded data is non-temporal. 
//
//		UPCONVERT(address, offset, convertTo) {
//			CASE conv OF
//			_MM_UPCONV_EPI32_NONE:   RETURN MEM[addr + 4*offset]
//			_MM_UPCONV_EPI32_UINT8:  RETURN UInt8ToInt32(MEM[addr + offset])
//			_MM_UPCONV_EPI32_SINT8:  RETURN SInt8ToInt32(MEM[addr + offset])
//			_MM_UPCONV_EPI32_UINT16: RETURN UInt16ToInt32(MEM[addr + 2*offset])
//			_MM_UPCONV_EPI32_SINT16: RETURN SInt16ToInt32(MEM[addr + 2*offset])
//			ESAC
//		}
//		
//		UPCONVERTSIZE(convertTo) {
//			CASE conv OF
//			_MM_UPCONV_EPI32_NONE:   RETURN 4
//			_MM_UPCONV_EPI32_UINT8:  RETURN 1
//			_MM_UPCONV_EPI32_SINT8:  RETURN 1
//			_MM_UPCONV_EPI32_UINT16: RETURN 2
//			_MM_UPCONV_EPI32_SINT16: RETURN 2
//			ESAC
//		}
//		
//		dst[511:0] := src[511:0]
//		loadOffset := 0
//		foundNext64BytesBoundary := false
//		upSize := UPCONVERTSIZE(conv)
//		addr = mt-64
//		FOR j := 0 to 15
//			IF foundNext64BytesBoundary == false
//				IF (addr + (loadOffset + 1)*upSize % 64) == 0
//					foundNext64BytesBoundary := true
//				FI
//			ELSE
//				i := j*32
//				dst[i+31:i] := UPCONVERT(addr, loadOffset, conv)
//			FI
//			loadOffset := loadOffset + 1
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VLOADUNPACKHD'. Intrinsic: '_mm512_extloadunpackhi_epi32'.
// Requires KNCNI.
func ExtloadunpackhiEpi32(src M512i, mt uintptr, conv MMUPCONVEPI32ENUM, hint int) M512i {
	return M512i(extloadunpackhiEpi32([64]byte(src), uintptr(mt), conv, hint))
}

func extloadunpackhiEpi32(src [64]byte, mt uintptr, conv MMUPCONVEPI32ENUM, hint int) [64]byte


// MaskExtloadunpackhiEpi32: Loads the high-64-byte-aligned portion of the
// byte/word/doubleword stream starting at element-aligned address mt-64,
// up-converted depending on the value of 'conv', and expanded into packed
// 32-bit integers in 'dst'. The initial values of 'dst' are copied from 'src'.
// Only those converted doublewords that occur at or after the first
// 64-byte-aligned address following (mt-64) are loaded. Elements in the
// resulting vector that do not map to those doublewords are taken from 'src'.
// 'hint' indicates to the processor whether the loaded data is non-temporal.
// Elements are copied to 'dst' according to element selector 'k' (elements are
// skipped when the corresponding mask bit is not set). 
//
//		UPCONVERT(address, offset, convertTo) {
//			CASE conv OF
//			_MM_UPCONV_EPI32_NONE:   RETURN MEM[addr + 4*offset]
//			_MM_UPCONV_EPI32_UINT8:  RETURN UInt8ToInt32(MEM[addr + offset])
//			_MM_UPCONV_EPI32_SINT8:  RETURN SInt8ToInt32(MEM[addr + offset])
//			_MM_UPCONV_EPI32_UINT16: RETURN UInt16ToInt32(MEM[addr + 2*offset])
//			_MM_UPCONV_EPI32_SINT16: RETURN SInt16ToInt32(MEM[addr + 2*offset])
//			ESAC
//		}
//		
//		UPCONVERTSIZE(convertTo) {
//			CASE conv OF
//			_MM_UPCONV_EPI32_NONE:   RETURN 4
//			_MM_UPCONV_EPI32_UINT8:  RETURN 1
//			_MM_UPCONV_EPI32_SINT8:  RETURN 1
//			_MM_UPCONV_EPI32_UINT16: RETURN 2
//			_MM_UPCONV_EPI32_SINT16: RETURN 2
//			ESAC
//		}
//		
//		dst[511:0] := src[511:0]
//		loadOffset := 0
//		foundNext64BytesBoundary := false
//		upSize := UPCONVERTSIZE(conv)
//		addr = mt-64
//		FOR j := 0 to 15
//			IF k[j]
//				IF foundNext64BytesBoundary == false
//					IF (addr + (loadOffset + 1)*upSize % 64) == 0
//						foundNext64BytesBoundary := true
//					FI
//				ELSE
//					i := j*32
//					dst[i+31:i] := UPCONVERT(addr, loadOffset, conv)
//				FI
//				loadOffset := loadOffset + 1
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VLOADUNPACKHD'. Intrinsic: '_mm512_mask_extloadunpackhi_epi32'.
// Requires KNCNI.
func MaskExtloadunpackhiEpi32(src M512i, k Mmask16, mt uintptr, conv MMUPCONVEPI32ENUM, hint int) M512i {
	return M512i(maskExtloadunpackhiEpi32([64]byte(src), uint16(k), uintptr(mt), conv, hint))
}

func maskExtloadunpackhiEpi32(src [64]byte, k uint16, mt uintptr, conv MMUPCONVEPI32ENUM, hint int) [64]byte


// ExtloadunpackhiEpi64: Loads the high-64-byte-aligned portion of the quadword
// stream starting at element-aligned address mt-64, up-converted depending on
// the value of 'conv', and expanded into packed 64-bit integers in 'dst'. The
// initial values of 'dst' are copied from 'src'. Only those converted
// quadwords that occur at or after the first 64-byte-aligned address following
// (mt-64) are loaded. Elements in the resulting vector that do not map to
// those quadwords are taken from 'src'. 'hint' indicates to the processor
// whether the loaded data is non-temporal. 
//
//		UPCONVERT(address, offset, convertTo) {
//			CASE conv OF
//			_MM_UPCONV_EPI64_NONE:   RETURN MEM[addr + 8*offset]
//			ESAC
//		}
//		
//		UPCONVERTSIZE(convertTo) {
//			CASE conv OF
//			_MM_UPCONV_EPI64_NONE:   RETURN 8
//			ESAC
//		}
//		
//		dst[511:0] := src[511:0]
//		loadOffset := 0
//		foundNext64BytesBoundary := false
//		upSize := UPCONVERTSIZE(conv)
//		addr = mt-64
//		FOR j := 0 to 7
//			IF foundNext64BytesBoundary == false
//				IF (addr + (loadOffset + 1)*upSize) == 0
//					foundNext64BytesBoundary := true
//				FI
//			ELSE
//				i := j*64
//				dst[i+63:i] := UPCONVERT(addr, loadOffset, conv)
//			FI
//			loadOffset := loadOffset + 1
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VLOADUNPACKHQ'. Intrinsic: '_mm512_extloadunpackhi_epi64'.
// Requires KNCNI.
func ExtloadunpackhiEpi64(src M512i, mt uintptr, conv MMUPCONVEPI64ENUM, hint int) M512i {
	return M512i(extloadunpackhiEpi64([64]byte(src), uintptr(mt), conv, hint))
}

func extloadunpackhiEpi64(src [64]byte, mt uintptr, conv MMUPCONVEPI64ENUM, hint int) [64]byte


// MaskExtloadunpackhiEpi64: Loads the high-64-byte-aligned portion of the
// quadword stream starting at element-aligned address mt-64, up-converted
// depending on the value of 'conv', and expanded into packed 64-bit integers
// in 'dst'. The initial values of 'dst' are copied from 'src'. Only those
// converted quadwords that occur at or after the first 64-byte-aligned address
// following (mt-64) are loaded. Elements in the resulting vector that do not
// map to those quadwords are taken from 'src'. 'hint' indicates to the
// processor whether the loaded data is non-temporal. Elements are copied to
// 'dst' according to element selector 'k' (elements are skipped when the
// corresponding mask bit is not set). 
//
//		UPCONVERT(address, offset, convertTo) {
//			CASE conv OF
//			_MM_UPCONV_EPI64_NONE:   RETURN MEM[addr + 8*offset]
//			ESAC
//		}
//		
//		UPCONVERTSIZE(convertTo) {
//			CASE conv OF
//			_MM_UPCONV_EPI64_NONE:   RETURN 8
//			ESAC
//		}
//		
//		dst[511:0] := src[511:0]
//		loadOffset := 0
//		foundNext64BytesBoundary := false
//		upSize := UPCONVERTSIZE(conv)
//		addr = mt-64
//		FOR j := 0 to 7
//			IF k[j]
//				IF foundNext64BytesBoundary == false
//					IF (addr + (loadOffset + 1)*upSize) == 0
//						foundNext64BytesBoundary := true
//					FI
//				ELSE
//					i := j*64
//					dst[i+63:i] := UPCONVERT(addr, loadOffset, conv)
//				FI
//				loadOffset := loadOffset + 1
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VLOADUNPACKHQ'. Intrinsic: '_mm512_mask_extloadunpackhi_epi64'.
// Requires KNCNI.
func MaskExtloadunpackhiEpi64(src M512i, k Mmask8, mt uintptr, conv MMUPCONVEPI64ENUM, hint int) M512i {
	return M512i(maskExtloadunpackhiEpi64([64]byte(src), uint8(k), uintptr(mt), conv, hint))
}

func maskExtloadunpackhiEpi64(src [64]byte, k uint8, mt uintptr, conv MMUPCONVEPI64ENUM, hint int) [64]byte


// ExtloadunpackhiPd: Loads the high-64-byte-aligned portion of the quadword
// stream starting at element-aligned address mt-64, up-converted depending on
// the value of 'conv', and expanded into packed double-precision (64-bit)
// floating-point values in 'dst'. The initial values of 'dst' are copied from
// 'src'. Only those converted quadwords that occur at or after the first
// 64-byte-aligned address following (mt-64) are loaded. Elements in the
// resulting vector that do not map to those quadwords are taken from 'src'.
// 'hint' indicates to the processor whether the loaded data is non-temporal. 
//
//		UPCONVERT(address, offset, convertTo) {
//			CASE conv OF
//			_MM_UPCONV_PD_NONE: RETURN MEM[addr + 8*offset]
//			ESAC
//		}
//		
//		UPCONVERTSIZE(convertTo) {
//			CASE conv OF
//			_MM_UPCONV_PD_NONE: RETURN 8
//			ESAC
//		}
//		
//		dst[511:0] := src[511:0]
//		loadOffset := 0
//		foundNext64BytesBoundary := false
//		upSize := UPCONVERTSIZE(conv)
//		addr = mt-64
//		FOR j := 0 to 7
//			IF foundNext64BytesBoundary == false
//				IF (addr + (loadOffset + 1)*upSize) % 64 == 0
//					foundNext64BytesBoundary := true
//				FI
//			ELSE
//				i := j*64
//				dst[i+63:i] := UPCONVERT(addr, loadOffset, conv)
//			FI
//			loadOffset := loadOffset + 1
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VLOADUNPACKHPD'. Intrinsic: '_mm512_extloadunpackhi_pd'.
// Requires KNCNI.
func ExtloadunpackhiPd(src M512d, mt uintptr, conv MMUPCONVPDENUM, hint int) M512d {
	return M512d(extloadunpackhiPd([8]float64(src), uintptr(mt), conv, hint))
}

func extloadunpackhiPd(src [8]float64, mt uintptr, conv MMUPCONVPDENUM, hint int) [8]float64


// MaskExtloadunpackhiPd: Loads the high-64-byte-aligned portion of the
// quadword stream starting at element-aligned address mt-64, up-converted
// depending on the value of 'conv', and expanded into packed double-precision
// (64-bit) floating-point values in 'dst'. The initial values of 'dst' are
// copied from 'src'. Only those converted quadwords that occur at or after the
// first 64-byte-aligned address following (mt-64) are loaded. Elements in the
// resulting vector that do not map to those quadwords are taken from 'src'.
// 'hint' indicates to the processor whether the loaded data is non-temporal.
// Elements are copied to 'dst' according to element selector 'k' (elements are
// skipped when the corresponding mask bit is not set). 
//
//		UPCONVERT(address, offset, convertTo) {
//			CASE conv OF
//			_MM_UPCONV_PD_NONE: RETURN MEM[addr + 8*offset]
//			ESAC
//		}
//		
//		UPCONVERTSIZE(convertTo) {
//			CASE conv OF
//			_MM_UPCONV_PD_NONE: RETURN 8
//			ESAC
//		}
//		
//		dst[511:0] := src[511:0]
//		loadOffset := 0
//		foundNext64BytesBoundary := false
//		upSize := UPCONVERTSIZE(conv)
//		addr = mt-64
//		FOR j := 0 to 7
//			IF k[j]
//				IF foundNext64BytesBoundary == false
//					IF (addr + (loadOffset + 1)*upSize) % 64 == 0
//						foundNext64BytesBoundary := true
//					FI
//				ELSE
//					i := j*64
//					dst[i+63:i] := UPCONVERT(addr, loadOffset, conv)
//				FI
//				loadOffset := loadOffset + 1
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VLOADUNPACKHPD'. Intrinsic: '_mm512_mask_extloadunpackhi_pd'.
// Requires KNCNI.
func MaskExtloadunpackhiPd(src M512d, k Mmask8, mt uintptr, conv MMUPCONVPDENUM, hint int) M512d {
	return M512d(maskExtloadunpackhiPd([8]float64(src), uint8(k), uintptr(mt), conv, hint))
}

func maskExtloadunpackhiPd(src [8]float64, k uint8, mt uintptr, conv MMUPCONVPDENUM, hint int) [8]float64


// ExtloadunpackhiPs: Loads the high-64-byte-aligned portion of the
// byte/word/doubleword stream starting at element-aligned address mt-64,
// up-converted depending on the value of 'conv', and expanded into packed
// single-precision (32-bit) floating-point elements in 'dst'. The initial
// values of 'dst' are copied from 'src'. Only those converted quadwords that
// occur at or after the first 64-byte-aligned address following (mt-64) are
// loaded. Elements in the resulting vector that do not map to those quadwords
// are taken from 'src'. 'hint' indicates to the processor whether the loaded
// data is non-temporal. 
//
//		UPCONVERT(address, offset, convertTo) {
//			CASE conv OF
//			_MM_UPCONV_PS_NONE:	   RETURN MEM[addr + 4*offset]
//			_MM_UPCONV_PS_FLOAT16: RETURN Float16ToFloat32(MEM[addr + 4*offset])
//			_MM_UPCONV_PS_UINT8:   RETURN UInt8ToFloat32(MEM[addr + offset])
//			_MM_UPCONV_PS_SINT8:   RETURN SInt8ToFloat32(MEM[addr + offset])
//			_MM_UPCONV_PS_UINT16:  RETURN UInt16ToFloat32(MEM[addr + 2*offset])
//			_MM_UPCONV_PS_SINT16:  RETURN SInt16ToFloat32(MEM[addr + 2*offset])
//			ESAC
//		}
//		
//		UPCONVERTSIZE(convertTo) {
//			CASE conv OF
//			_MM_UPCONV_PS_NONE:	   RETURN 4
//			_MM_UPCONV_PS_FLOAT16: RETURN 2
//			_MM_UPCONV_PS_UINT8:   RETURN 1
//			_MM_UPCONV_PS_SINT8:   RETURN 1
//			_MM_UPCONV_PS_UINT16:  RETURN 2
//			_MM_UPCONV_PS_SINT16:  RETURN 2
//			ESAC
//		}
//		
//		dst[511:0] := src[511:0]
//		loadOffset := 0
//		foundNext64BytesBoundary := false
//		upSize := UPCONVERTSIZE(conv)
//		addr = mt-64
//		FOR j := 0 to 15
//			IF foundNext64BytesBoundary == false
//				IF (addr + (loadOffset + 1)*upSize % 64) == 0
//					foundNext64BytesBoundary := true
//				FI
//			ELSE
//				i := j*32
//				dst[i+31:i] := UPCONVERT(addr, loadOffset, conv)
//			FI
//			loadOffset := loadOffset + 1
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VLOADUNPACKHPS'. Intrinsic: '_mm512_extloadunpackhi_ps'.
// Requires KNCNI.
func ExtloadunpackhiPs(src M512, mt uintptr, conv MMUPCONVPSENUM, hint int) M512 {
	return M512(extloadunpackhiPs([16]float32(src), uintptr(mt), conv, hint))
}

func extloadunpackhiPs(src [16]float32, mt uintptr, conv MMUPCONVPSENUM, hint int) [16]float32


// MaskExtloadunpackhiPs: Loads the high-64-byte-aligned portion of the
// byte/word/doubleword stream starting at element-aligned address mt-64,
// up-converted depending on the value of 'conv', and expanded into packed
// single-precision (32-bit) floating-point elements in 'dst'. The initial
// values of 'dst' are copied from 'src'. Only those converted quadwords that
// occur at or after the first 64-byte-aligned address following (mt-64) are
// loaded. Elements in the resulting vector that do not map to those quadwords
// are taken from 'src'. 'hint' indicates to the processor whether the loaded
// data is non-temporal. Elements are copied to 'dst' according to element
// selector 'k' (elements are skipped when the corresponding mask bit is not
// set). 
//
//		UPCONVERT(address, offset, convertTo) {
//			CASE conv OF
//			_MM_UPCONV_PS_NONE:	   RETURN MEM[addr + 4*offset]
//			_MM_UPCONV_PS_FLOAT16: RETURN Float16ToFloat32(MEM[addr + 4*offset])
//			_MM_UPCONV_PS_UINT8:   RETURN UInt8ToFloat32(MEM[addr + offset])
//			_MM_UPCONV_PS_SINT8:   RETURN SInt8ToFloat32(MEM[addr + offset])
//			_MM_UPCONV_PS_UINT16:  RETURN UInt16ToFloat32(MEM[addr + 2*offset])
//			_MM_UPCONV_PS_SINT16:  RETURN SInt16ToFloat32(MEM[addr + 2*offset])
//			ESAC
//		}
//		
//		UPCONVERTSIZE(convertTo) {
//			CASE conv OF
//			_MM_UPCONV_PS_NONE:	   RETURN 4
//			_MM_UPCONV_PS_FLOAT16: RETURN 2
//			_MM_UPCONV_PS_UINT8:   RETURN 1
//			_MM_UPCONV_PS_SINT8:   RETURN 1
//			_MM_UPCONV_PS_UINT16:  RETURN 2
//			_MM_UPCONV_PS_SINT16:  RETURN 2
//			ESAC
//		}
//		
//		dst[511:0] := src[511:0]
//		loadOffset := 0
//		foundNext64BytesBoundary := false
//		upSize := UPCONVERTSIZE(conv)
//		addr = mt-64
//		FOR j := 0 to 15
//			IF k[j]
//				IF foundNext64BytesBoundary == false
//					IF (addr + (loadOffset + 1)*upSize % 64) == 0
//						foundNext64BytesBoundary := true
//					FI
//				ELSE
//					i := j*32
//					dst[i+31:i] := UPCONVERT(addr, loadOffset, conv)
//				FI
//				loadOffset := loadOffset + 1
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VLOADUNPACKHPS'. Intrinsic: '_mm512_mask_extloadunpackhi_ps'.
// Requires KNCNI.
func MaskExtloadunpackhiPs(src M512, k Mmask16, mt uintptr, conv MMUPCONVPSENUM, hint int) M512 {
	return M512(maskExtloadunpackhiPs([16]float32(src), uint16(k), uintptr(mt), conv, hint))
}

func maskExtloadunpackhiPs(src [16]float32, k uint16, mt uintptr, conv MMUPCONVPSENUM, hint int) [16]float32


// ExtloadunpackloEpi32: Loads the low-64-byte-aligned portion of the
// byte/word/doubleword stream starting at element-aligned address mt,
// up-converted depending on the value of 'conv', and expanded into packed
// 32-bit integers in 'dst'. The initial values of 'dst' are copied from 'src'.
// Only those converted doublewords that occur before first 64-byte-aligned
// address following 'mt' are loaded. Elements in the resulting vector that do
// not map to those doublewords are taken from 'src'. 'hint' indicates to the
// processor whether the loaded data is non-temporal. 
//
//		UPCONVERT(address, offset, convertTo) {
//			CASE conv OF
//			_MM_UPCONV_EPI32_NONE:   RETURN MEM[addr + 4*offset]
//			_MM_UPCONV_EPI32_UINT8:  RETURN UInt8ToInt32(MEM[addr + offset])
//			_MM_UPCONV_EPI32_SINT8:  RETURN SInt8ToInt32(MEM[addr + offset])
//			_MM_UPCONV_EPI32_UINT16: RETURN UInt16ToInt32(MEM[addr + 2*offset])
//			_MM_UPCONV_EPI32_SINT16: RETURN SInt16ToInt32(MEM[addr + 2*offset])
//			ESAC
//		}
//		
//		UPCONVERTSIZE(convertTo) {
//			CASE conv OF
//			_MM_UPCONV_EPI32_NONE:   RETURN 4
//			_MM_UPCONV_EPI32_UINT8:  RETURN 1
//			_MM_UPCONV_EPI32_SINT8:  RETURN 1
//			_MM_UPCONV_EPI32_UINT16: RETURN 2
//			_MM_UPCONV_EPI32_SINT16: RETURN 2
//			ESAC
//		}
//		
//		dst[511:0] := src[511:0]
//		loadOffset := 0
//		upSize := UPCONVERTSIZE(conv)
//		addr = mt
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := UPCONVERT(addr, loadOffset, conv)
//			loadOffset := loadOffset + 1
//			IF (mt + loadOffset * upSize) % 64 == 0
//				break
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VLOADUNPACKLD'. Intrinsic: '_mm512_extloadunpacklo_epi32'.
// Requires KNCNI.
func ExtloadunpackloEpi32(src M512i, mt uintptr, conv MMUPCONVEPI32ENUM, hint int) M512i {
	return M512i(extloadunpackloEpi32([64]byte(src), uintptr(mt), conv, hint))
}

func extloadunpackloEpi32(src [64]byte, mt uintptr, conv MMUPCONVEPI32ENUM, hint int) [64]byte


// MaskExtloadunpackloEpi32: Loads the low-64-byte-aligned portion of the
// byte/word/doubleword stream starting at element-aligned address mt,
// up-converted depending on the value of 'conv', and expanded into packed
// 32-bit integers in 'dst'. The initial values of 'dst' are copied from 'src'.
// Only those converted doublewords that occur before first 64-byte-aligned
// address following 'mt' are loaded. Elements in the resulting vector that do
// not map to those doublewords are taken from 'src'. 'hint' indicates to the
// processor whether the loaded data is non-temporal. Elements are copied to
// 'dst' according to element selector 'k' (elements are skipped when the
// corresponding mask bit is not set). 
//
//		UPCONVERT(address, offset, convertTo) {
//			CASE conv OF
//			_MM_UPCONV_EPI32_NONE:   RETURN MEM[addr + 4*offset]
//			_MM_UPCONV_EPI32_UINT8:  RETURN UInt8ToInt32(MEM[addr + offset])
//			_MM_UPCONV_EPI32_SINT8:  RETURN SInt8ToInt32(MEM[addr + offset])
//			_MM_UPCONV_EPI32_UINT16: RETURN UInt16ToInt32(MEM[addr + 2*offset])
//			_MM_UPCONV_EPI32_SINT16: RETURN SInt16ToInt32(MEM[addr + 2*offset])
//			ESAC
//		}
//		
//		UPCONVERTSIZE(convertTo) {
//			CASE conv OF
//			_MM_UPCONV_EPI32_NONE:   RETURN 4
//			_MM_UPCONV_EPI32_UINT8:  RETURN 1
//			_MM_UPCONV_EPI32_SINT8:  RETURN 1
//			_MM_UPCONV_EPI32_UINT16: RETURN 2
//			_MM_UPCONV_EPI32_SINT16: RETURN 2
//			ESAC
//		}
//		
//		dst[511:0] := src[511:0]
//		loadOffset := 0
//		upSize := UPCONVERTSIZE(conv)
//		addr = mt
//		FOR j := 0 to 15
//			IF k[j]
//				i := j*32
//				dst[i+31:i] := UPCONVERT(addr, loadOffset, conv)
//				loadOffset := loadOffset + 1
//				IF (mt + loadOffset * upSize) % 64 == 0
//					break
//				FI
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VLOADUNPACKLD'. Intrinsic: '_mm512_mask_extloadunpacklo_epi32'.
// Requires KNCNI.
func MaskExtloadunpackloEpi32(src M512i, k Mmask16, mt uintptr, conv MMUPCONVEPI32ENUM, hint int) M512i {
	return M512i(maskExtloadunpackloEpi32([64]byte(src), uint16(k), uintptr(mt), conv, hint))
}

func maskExtloadunpackloEpi32(src [64]byte, k uint16, mt uintptr, conv MMUPCONVEPI32ENUM, hint int) [64]byte


// ExtloadunpackloEpi64: Loads the low-64-byte-aligned portion of the quadword
// stream starting at element-aligned address mt, up-converted depending on the
// value of 'conv', and expanded into packed 64-bit integers in 'dst'. The
// initial values of 'dst' are copied from 'src'. Only those converted quad
// that occur before first 64-byte-aligned address following 'mt' are loaded.
// Elements in the resulting vector that do not map to those quadwords are
// taken from 'src'. 'hint' indicates to the processor whether the loaded data
// is non-temporal. 
//
//		UPCONVERT(address, offset, convertTo) {
//			CASE conv OF
//			_MM_UPCONV_EPI64_NONE:   RETURN MEM[addr + 8*offset]
//			ESAC
//		}
//		
//		UPCONVERTSIZE(convertTo) {
//			CASE conv OF
//			_MM_UPCONV_EPI64_NONE:   RETURN 8
//			ESAC
//		}
//		
//		dst[511:0] := src[511:0]
//		loadOffset := 0
//		upSize := UPCONVERTSIZE(conv)
//		addr = mt
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := UPCONVERT(addr, loadOffset, conv)
//			loadOffset := loadOffset + 1
//			IF (addr + loadOffset*upSize % 64) == 0
//				BREAK
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VLOADUNPACKLQ'. Intrinsic: '_mm512_extloadunpacklo_epi64'.
// Requires KNCNI.
func ExtloadunpackloEpi64(src M512i, mt uintptr, conv MMUPCONVEPI64ENUM, hint int) M512i {
	return M512i(extloadunpackloEpi64([64]byte(src), uintptr(mt), conv, hint))
}

func extloadunpackloEpi64(src [64]byte, mt uintptr, conv MMUPCONVEPI64ENUM, hint int) [64]byte


// MaskExtloadunpackloEpi64: Loads the low-64-byte-aligned portion of the
// quadword stream starting at element-aligned address mt, up-converted
// depending on the value of 'conv', and expanded into packed 64-bit integers
// in 'dst'. The initial values of 'dst' are copied from 'src'. Only those
// converted quadwords that occur before first 64-byte-aligned address
// following 'mt' are loaded. Elements in the resulting vector that do not map
// to those quadwords are taken from 'src'. 'hint' indicates to the processor
// whether the loaded data is non-temporal. Elements are copied to 'dst'
// according to element selector 'k' (elements are skipped when the
// corresponding mask bit is not set). 
//
//		UPCONVERT(address, offset, convertTo) {
//			CASE conv OF
//			_MM_UPCONV_EPI64_NONE:   RETURN MEM[addr + 8*offset]
//			ESAC
//		}
//		
//		UPCONVERTSIZE(convertTo) {
//			CASE conv OF
//			_MM_UPCONV_EPI64_NONE:   RETURN 8
//			ESAC
//		}
//		
//		dst[511:0] := src[511:0]
//		loadOffset := 0
//		upSize := UPCONVERTSIZE(conv)
//		addr = mt
//		FOR j := 0 to 7
//			IF k[j]
//				i := j*64
//				dst[i+63:i] := UPCONVERT(addr, loadOffset, conv)
//				loadOffset := loadOffset + 1
//				IF (addr + loadOffset*upSize % 64) == 0
//					BREAK
//				FI
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VLOADUNPACKLQ'. Intrinsic: '_mm512_mask_extloadunpacklo_epi64'.
// Requires KNCNI.
func MaskExtloadunpackloEpi64(src M512i, k Mmask8, mt uintptr, conv MMUPCONVEPI64ENUM, hint int) M512i {
	return M512i(maskExtloadunpackloEpi64([64]byte(src), uint8(k), uintptr(mt), conv, hint))
}

func maskExtloadunpackloEpi64(src [64]byte, k uint8, mt uintptr, conv MMUPCONVEPI64ENUM, hint int) [64]byte


// ExtloadunpackloPd: Loads the low-64-byte-aligned portion of the quadword
// stream starting at element-aligned address mt, up-converted depending on the
// value of 'conv', and expanded into packed double-precision (64-bit)
// floating-point elements in 'dst'. The initial values of 'dst' are copied
// from 'src'. Only those converted quad that occur before first
// 64-byte-aligned address following 'mt' are loaded. Elements in the resulting
// vector that do not map to those quadwords are taken from 'src'. 'hint'
// indicates to the processor whether the loaded data is non-temporal. 
//
//		UPCONVERT(address, offset, convertTo) {
//			CASE conv OF
//			_MM_UPCONV_PD_NONE:	RETURN MEM[addr + 8*offset]
//			ESAC
//		}
//		
//		UPCONVERTSIZE(convertTo) {
//			CASE conv OF
//			_MM_UPCONV_PD_NONE:	RETURN 8
//			ESAC
//		}
//		
//		dst[511:0] := src[511:0]
//		loadOffset := 0
//		upSize := UPCONVERTSIZE(conv)
//		addr = mt
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := UPCONVERT(addr, loadOffset, conv)
//			loadOffset := loadOffset + 1
//			IF (mt + loadOffset * upSize) % 64 == 0
//				break
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VLOADUNPACKLPD'. Intrinsic: '_mm512_extloadunpacklo_pd'.
// Requires KNCNI.
func ExtloadunpackloPd(src M512d, mt uintptr, conv MMUPCONVPDENUM, hint int) M512d {
	return M512d(extloadunpackloPd([8]float64(src), uintptr(mt), conv, hint))
}

func extloadunpackloPd(src [8]float64, mt uintptr, conv MMUPCONVPDENUM, hint int) [8]float64


// MaskExtloadunpackloPd: Loads the low-64-byte-aligned portion of the quadword
// stream starting at element-aligned address mt, up-converted depending on the
// value of 'conv', and expanded into packed double-precision (64-bit)
// floating-point elements in 'dst'. The initial values of 'dst' are copied
// from 'src'. Only those converted quad that occur before first
// 64-byte-aligned address following 'mt' are loaded. Elements in the resulting
// vector that do not map to those quadwords are taken from 'src'. 'hint'
// indicates to the processor whether the loaded data is non-temporal. Elements
// are copied to 'dst' according to element selector 'k' (elemenst are skipped
// when the corresponding mask bit is not set). 
//
//		UPCONVERT(address, offset, convertTo) {
//			CASE conv OF
//			_MM_UPCONV_PD_NONE:	RETURN MEM[addr + 8*offset]
//			ESAC
//		}
//		
//		UPCONVERTSIZE(convertTo) {
//			CASE conv OF
//			_MM_UPCONV_PD_NONE:	RETURN 8
//			ESAC
//		}
//		
//		dst[511:0] := src[511:0]
//		loadOffset := 0
//		upSize := UPCONVERTSIZE(conv)
//		addr = mt
//		FOR j := 0 to 7
//			IF k[j]
//				i := j*64
//				dst[i+63:i] := UPCONVERT(addr, loadOffset, conv)
//				loadOffset := loadOffset + 1
//				IF (mt + loadOffset * upSize) % 64 == 0
//					break
//				FI
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VLOADUNPACKLPD'. Intrinsic: '_mm512_mask_extloadunpacklo_pd'.
// Requires KNCNI.
func MaskExtloadunpackloPd(src M512d, k Mmask8, mt uintptr, conv MMUPCONVPDENUM, hint int) M512d {
	return M512d(maskExtloadunpackloPd([8]float64(src), uint8(k), uintptr(mt), conv, hint))
}

func maskExtloadunpackloPd(src [8]float64, k uint8, mt uintptr, conv MMUPCONVPDENUM, hint int) [8]float64


// ExtloadunpackloPs: Loads the low-64-byte-aligned portion of the
// byte/word/doubleword stream starting at element-aligned address mt,
// up-converted depending on the value of 'conv', and expanded into packed
// single-precision (32-bit) floating-point elements in 'dst'. The initial
// values of 'dst' are copied from 'src'. Only those converted doublewords that
// occur before first 64-byte-aligned address following 'mt' are loaded.
// Elements in the resulting vector that do not map to those doublewords are
// taken from 'src'. 'hint' indicates to the processor whether the loaded data
// is non-temporal. 
//
//		UPCONVERT(address, offset, convertTo) {
//			CASE conv OF
//			_MM_UPCONV_PS_NONE:	   RETURN MEM[addr + 4*offset]
//			_MM_UPCONV_PS_FLOAT16: RETURN Float16ToFloat32(MEM[addr + 4*offset])
//			_MM_UPCONV_PS_UINT8:   RETURN UInt8ToFloat32(MEM[addr + offset])
//			_MM_UPCONV_PS_SINT8:   RETURN SInt8ToFloat32(MEM[addr + offset])
//			_MM_UPCONV_PS_UINT16:  RETURN UInt16ToFloat32(MEM[addr + 2*offset])
//			_MM_UPCONV_PS_SINT16:  RETURN SInt16ToFloat32(MEM[addr + 2*offset])
//			ESAC
//		}
//		
//		UPCONVERTSIZE(convertTo) {
//			CASE conv OF
//			_MM_UPCONV_PS_NONE:	   RETURN 4
//			_MM_UPCONV_PS_FLOAT16: RETURN 2
//			_MM_UPCONV_PS_UINT8:   RETURN 1
//			_MM_UPCONV_PS_SINT8:   RETURN 1
//			_MM_UPCONV_PS_UINT16:  RETURN 2
//			_MM_UPCONV_PS_SINT16:  RETURN 2
//			ESAC
//		}
//		
//		dst[511:0] := src[511:0]
//		loadOffset := 0
//		upSize := UPCONVERTSIZE(conv)
//		addr = MEM[mt]
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := UPCONVERT(addr, loadOffset, conv)
//			loadOffset := loadOffset + 1
//			IF (mt + loadOffset * upSize) % 64 == 0
//				break
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VLOADUNPACKLPS'. Intrinsic: '_mm512_extloadunpacklo_ps'.
// Requires KNCNI.
func ExtloadunpackloPs(src M512, mt uintptr, conv MMUPCONVPSENUM, hint int) M512 {
	return M512(extloadunpackloPs([16]float32(src), uintptr(mt), conv, hint))
}

func extloadunpackloPs(src [16]float32, mt uintptr, conv MMUPCONVPSENUM, hint int) [16]float32


// MaskExtloadunpackloPs: Loads the low-64-byte-aligned portion of the
// byte/word/doubleword stream starting at element-aligned address mt,
// up-converted depending on the value of 'conv', and expanded into packed
// single-precision (32-bit) floating-point elements in 'dst'. The initial
// values of 'dst' are copied from 'src'. Only those converted doublewords that
// occur before first 64-byte-aligned address following 'mt' are loaded.
// Elements in the resulting vector that do not map to those doublewords are
// taken from 'src'. 'hint' indicates to the processor whether the loaded data
// is non-temporal. Elements are copied to 'dst' according to element selector
// 'k' (elements are skipped when the corresponding mask bit is not set). 
//
//		UPCONVERT(address, offset, convertTo) {
//			CASE conv OF
//			_MM_UPCONV_PS_NONE:    RETURN MEM[addr + 4*offset]
//			_MM_UPCONV_PS_FLOAT16: RETURN Float16ToFloat32(MEM[addr + 4*offset])
//			_MM_UPCONV_PS_UINT8:   RETURN UInt8ToFloat32(MEM[addr + offset])
//			_MM_UPCONV_PS_SINT8:   RETURN SInt8ToFloat32(MEM[addr + offset])
//			_MM_UPCONV_PS_UINT16:  RETURN UInt16ToFloat32(MEM[addr + 2*offset])
//			_MM_UPCONV_PS_SINT16:  RETURN SInt16ToFloat32(MEM[addr + 2*offset])
//			ESAC
//		}
//		
//		UPCONVERTSIZE(convertTo) {
//			CASE conv OF
//			_MM_UPCONV_PS_NONE:	   RETURN 4
//			_MM_UPCONV_PS_FLOAT16: RETURN 2
//			_MM_UPCONV_PS_UINT8:   RETURN 1
//			_MM_UPCONV_PS_SINT8:   RETURN 1
//			_MM_UPCONV_PS_UINT16:  RETURN 2
//			_MM_UPCONV_PS_SINT16:  RETURN 2
//			ESAC
//		}
//		
//		dst[511:0] := src[511:0]
//		loadOffset := 0
//		upSize := UPCONVERTSIZE(conv)
//		addr = MEM[mt]
//		FOR j := 0 to 15
//			IF k[j]
//				i := j*32
//				dst[i+31:i] := UPCONVERT(addr, loadOffset, conv)
//				loadOffset := loadOffset + 1
//				IF (mt + loadOffset * upSize) % 64 == 0
//					break
//				FI
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VLOADUNPACKLPS'. Intrinsic: '_mm512_mask_extloadunpacklo_ps'.
// Requires KNCNI.
func MaskExtloadunpackloPs(src M512, k Mmask16, mt uintptr, conv MMUPCONVPSENUM, hint int) M512 {
	return M512(maskExtloadunpackloPs([16]float32(src), uint16(k), uintptr(mt), conv, hint))
}

func maskExtloadunpackloPs(src [16]float32, k uint16, mt uintptr, conv MMUPCONVPSENUM, hint int) [16]float32


// ExtpackstorehiEpi32: Down-converts and stores packed 32-bit integer elements
// of 'v1' into a byte/word/doubleword stream according to 'conv' at a
// logically mapped starting address (mt-64), storing the high-64-byte elements
// of that stream (those elemetns of the stream that map at or after the first
// 64-byte-aligned address following (m5-64)). 'hint' indicates to the
// processor whether the data is non-temporal. 
//
//		DOWNCONVERT(element, convertTo) {
//			CASE converTo OF
//			_MM_UPCONV_EPI32_NONE:   RETURN element[i+31:i]
//			_MM_UPCONV_EPI32_UINT8:  RETURN UInt32ToUInt8(element[i+31:i])
//			_MM_UPCONV_EPI32_SINT8:  RETURN SInt32ToSInt8(element[i+31:i])
//			_MM_UPCONV_EPI32_UINT16: RETURN UInt32ToUInt16(element[i+31:i])
//			_MM_UPCONV_EPI32_SINT16: RETURN SInt32ToSInt16(element[i+31:i])
//			ESAC
//		}
//		
//		DOWNCONVERTSIZE(convertTo) {
//			CASE converTo OF
//			_MM_UPCONV_EPI32_NONE:   RETURN 4
//			_MM_UPCONV_EPI32_UINT8:  RETURN 1
//			_MM_UPCONV_EPI32_SINT8:  RETURN 1
//			_MM_UPCONV_EPI32_UINT16: RETURN 2
//			_MM_UPCONV_EPI32_SINT16: RETURN 2
//			ESAC
//		}
//		
//		storeOffset := 0
//		foundNext64BytesBoundary := false
//		downSize := DOWNCONVERTSIZE(conv)
//		addr = mt-64
//		FOR j := 0 to 15
//			IF foundNext64BytesBoundary == false
//				IF ((addr + (storeOffset + 1)*downSize) % 64) == 0
//					foundNext64BytesBoundary = true
//				FI
//			ELSE
//				i := j*32
//				tmp := DOWNCONVERT(v1[i+31:i], conv)
//				storeAddr := addr + storeOffset * downSize
//				CASE downSize OF
//				4: MEM[storeAddr] := tmp[31:0]
//				2: MEM[storeAddr] := tmp[15:0]
//				1: MEM[storeAddr] := tmp[7:0]
//				ESAC
//			FI
//			storeOffset := storeOffset + 1
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPACKSTOREHD'. Intrinsic: '_mm512_extpackstorehi_epi32'.
// Requires KNCNI.
func ExtpackstorehiEpi32(mt uintptr, v1 M512i, conv MMDOWNCONVEPI32ENUM, hint int)  {
	extpackstorehiEpi32(uintptr(mt), [64]byte(v1), conv, hint)
}

func extpackstorehiEpi32(mt uintptr, v1 [64]byte, conv MMDOWNCONVEPI32ENUM, hint int) 


// MaskExtpackstorehiEpi32: Down-converts and stores packed 32-bit integer
// elements of 'v1' into a byte/word/doubleword stream according to 'conv' at a
// logically mapped starting address (mt-64), storing the high-64-byte elements
// of that stream (those elemetns of the stream that map at or after the first
// 64-byte-aligned address following (m5-64)). 'hint' indicates to the
// processor whether the data is non-temporal. Elements are stored to memory
// according to element selector 'k' (elements are skipped when the
// corresonding mask bit is not set). 
//
//		DOWNCONVERT(element, convertTo) {
//			CASE converTo OF
//			_MM_UPCONV_EPI32_NONE:   RETURN element[i+31:i]
//			_MM_UPCONV_EPI32_UINT8:  RETURN UInt32ToUInt8(element[i+31:i])
//			_MM_UPCONV_EPI32_SINT8:  RETURN SInt32ToSInt8(element[i+31:i])
//			_MM_UPCONV_EPI32_UINT16: RETURN UInt32ToUInt16(element[i+31:i])
//			_MM_UPCONV_EPI32_SINT16: RETURN SInt32ToSInt16(element[i+31:i])
//			ESAC
//		}
//		
//		DOWNCONVERTSIZE(convertTo) {
//			CASE converTo OF
//			_MM_UPCONV_EPI32_NONE:   RETURN 4
//			_MM_UPCONV_EPI32_UINT8:  RETURN 1
//			_MM_UPCONV_EPI32_SINT8:  RETURN 1
//			_MM_UPCONV_EPI32_UINT16: RETURN 2
//			_MM_UPCONV_EPI32_SINT16: RETURN 2
//			ESAC
//		}
//		
//		storeOffset := 0
//		foundNext64BytesBoundary := false
//		downSize := DOWNCONVERTSIZE(conv)
//		addr = mt-64
//		FOR j := 0 to 15
//			IF k[j]
//				IF foundNext64BytesBoundary == false
//					IF ((addr + (storeOffset + 1)*downSize) % 64) == 0
//						foundNext64BytesBoundary = true
//					FI
//				ELSE
//					i := j*32
//					tmp := DOWNCONVERT(v1[i+31:i], conv)
//					storeAddr := addr + storeOffset * downSize
//					CASE downSize OF
//					4: MEM[storeAddr] := tmp[31:0]
//					2: MEM[storeAddr] := tmp[15:0]
//					1: MEM[storeAddr] := tmp[7:0]
//					ESAC
//				FI
//				storeOffset := storeOffset + 1
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPACKSTOREHD'. Intrinsic: '_mm512_mask_extpackstorehi_epi32'.
// Requires KNCNI.
func MaskExtpackstorehiEpi32(mt uintptr, k Mmask16, v1 M512i, conv MMDOWNCONVEPI32ENUM, hint int)  {
	maskExtpackstorehiEpi32(uintptr(mt), uint16(k), [64]byte(v1), conv, hint)
}

func maskExtpackstorehiEpi32(mt uintptr, k uint16, v1 [64]byte, conv MMDOWNCONVEPI32ENUM, hint int) 


// ExtpackstorehiEpi64: Down-converts and stores packed 64-bit integer elements
// of 'v1' into a quadword stream according to 'conv' at a logically mapped
// starting address (mt-64), storing the high-64-byte elements of that stream
// (those elemetns of the stream that map at or after the first 64-byte-aligned
// address following (m5-64)). 'hint' indicates to the processor whether the
// data is non-temporal. 
//
//		DOWNCONVERT(element, convertTo) {
//			CASE converTo OF
//			_MM_UPCONV_EPI64_NONE: RETURN element[i+63:i]
//			ESAC
//		}
//		
//		DOWNCONVERTSIZE(convertTo) {
//			CASE converTo OF
//			_MM_UPCONV_EPI64_NONE: RETURN 8
//			ESAC
//		}
//		
//		storeOffset := 0
//		foundNext64BytesBoundary := false
//		downSize := DOWNCONVERTSIZE(conv)
//		addr = mt-64
//		FOR j := 0 to 7
//			IF foundNext64BytesBoundary == false
//				IF ((addr + (storeOffset + 1)*downSize) % 64) == 0
//					foundNext64BytesBoundary = true
//				FI
//			ELSE
//				i := j*64
//				tmp := DOWNCONVERT(v1[i+63:i], conv)
//				storeAddr := addr + storeOffset * downSize
//				CASE downSize OF
//				8: MEM[storeAddr] := tmp[63:0]
//				ESAC
//			FI
//			storeOffset := storeOffset + 1
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPACKSTOREHQ'. Intrinsic: '_mm512_extpackstorehi_epi64'.
// Requires KNCNI.
func ExtpackstorehiEpi64(mt uintptr, v1 M512i, conv MMDOWNCONVEPI64ENUM, hint int)  {
	extpackstorehiEpi64(uintptr(mt), [64]byte(v1), conv, hint)
}

func extpackstorehiEpi64(mt uintptr, v1 [64]byte, conv MMDOWNCONVEPI64ENUM, hint int) 


// MaskExtpackstorehiEpi64: Down-converts and stores packed 64-bit integer
// elements of 'v1' into a quadword stream according to 'conv' at a logically
// mapped starting address (mt-64), storing the high-64-byte elements of that
// stream (those elemetns of the stream that map at or after the first
// 64-byte-aligned address following (mt-64)). 'hint' indicates to the
// processor whether the data is non-temporal. Elements are stored to memory
// according to element selector 'k' (elements are skipped when the
// corresonding mask bit is not set). 
//
//		DOWNCONVERT(element, convertTo) {
//			CASE converTo OF
//			_MM_UPCONV_EPI64_NONE: RETURN element[i+63:i]
//			ESAC
//		}
//		
//		DOWNCONVERTSIZE(convertTo) {
//			CASE converTo OF
//			_MM_UPCONV_EPI64_NONE: RETURN 8
//			ESAC
//		}
//		
//		storeOffset := 0
//		foundNext64BytesBoundary := false
//		downSize := DOWNCONVERTSIZE(conv)
//		addr = mt-64
//		FOR j := 0 to 7
//			IF k[j]
//				IF foundNext64BytesBoundary == false
//					IF ((addr + (storeOffset + 1)*downSize) % 64) == 0
//						foundNext64BytesBoundary = true
//					FI
//				ELSE
//					i := j*64
//					tmp := DOWNCONVERT(v1[i+63:i], conv)
//					storeAddr := addr + storeOffset * downSize
//					CASE downSize OF
//					8: MEM[storeAddr] := tmp[63:0]
//					ESAC
//				FI
//				storeOffset := storeOffset + 1
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPACKSTOREHQ'. Intrinsic: '_mm512_mask_extpackstorehi_epi64'.
// Requires KNCNI.
func MaskExtpackstorehiEpi64(mt uintptr, k Mmask8, v1 M512i, conv MMDOWNCONVEPI64ENUM, hint int)  {
	maskExtpackstorehiEpi64(uintptr(mt), uint8(k), [64]byte(v1), conv, hint)
}

func maskExtpackstorehiEpi64(mt uintptr, k uint8, v1 [64]byte, conv MMDOWNCONVEPI64ENUM, hint int) 


// ExtpackstorehiPd: Down-converts and stores packed double-precision (64-bit)
// floating-point elements of 'v1' into a quadword stream according to 'conv'
// at a logically mapped starting address (mt-64), storing the high-64-byte
// elements of that stream (those elemetns of the stream that map at or after
// the first 64-byte-aligned address following (m5-64)). 'hint' indicates to
// the processor whether the data is non-temporal. 
//
//		DOWNCONVERT(element, convertTo) {
//			CASE converTo OF
//			_MM_UPCONV_PD_NONE: RETURN element[i+63:i]
//			ESAC
//		}
//		
//		DOWNCONVERTSIZE(convertTo) {
//			CASE converTo OF
//			_MM_UPCONV_PD_NONE: RETURN 8
//			ESAC
//		}
//		
//		storeOffset := 0
//		foundNext64BytesBoundary := false
//		downSize := DOWNCONVERTSIZE(conv)
//		addr = mt-64
//		FOR j := 0 to 7
//			IF foundNext64BytesBoundary == false
//				IF ((addr + (storeOffset + 1)*downSize) % 64) == 0
//					foundNext64BytesBoundary = true
//				FI
//			ELSE
//				i := j*64
//				tmp := DOWNCONVERT(v1[i+63:i], conv)
//				storeAddr := addr + storeOffset * downSize
//				CASE downSize OF
//				8: MEM[storeAddr] := tmp[63:0]
//				ESAC
//			FI
//			storeOffset := storeOffset + 1
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPACKSTOREHPD'. Intrinsic: '_mm512_extpackstorehi_pd'.
// Requires KNCNI.
func ExtpackstorehiPd(mt uintptr, v1 M512d, conv MMDOWNCONVPDENUM, hint int)  {
	extpackstorehiPd(uintptr(mt), [8]float64(v1), conv, hint)
}

func extpackstorehiPd(mt uintptr, v1 [8]float64, conv MMDOWNCONVPDENUM, hint int) 


// MaskExtpackstorehiPd: Down-converts and stores packed double-precision
// (64-bit) floating-point elements of 'v1' into a quadword stream according to
// 'conv' at a logically mapped starting address (mt-64), storing the
// high-64-byte elements of that stream (those elemetns of the stream that map
// at or after the first 64-byte-aligned address following (m5-64)). 'hint'
// indicates to the processor whether the data is non-temporal. Elements are
// stored to memory according to element selector 'k' (elements are skipped
// when the corresponding mask bit is not set). 
//
//		DOWNCONVERT(element, convertTo) {
//			CASE converTo OF
//			_MM_UPCONV_PD_NONE: RETURN element[i+63:i]
//			ESAC
//		}
//		
//		DOWNCONVERTSIZE(convertTo) {
//			CASE converTo OF
//			_MM_UPCONV_PD_NONE: RETURN 8
//			ESAC
//		}
//		
//		storeOffset := 0
//		foundNext64BytesBoundary := false
//		downSize := DOWNCONVERTSIZE(conv)
//		addr = mt-64
//		FOR j := 0 to 7
//			IF k[j]
//				IF foundNext64BytesBoundary == false
//					IF ((addr + (storeOffset + 1)*downSize) % 64) == 0
//						foundNext64BytesBoundary = true
//					FI
//				ELSE
//					i := j*64
//					tmp := DOWNCONVERT(v1[i+63:i], conv)
//					storeAddr := addr + storeOffset * downSize
//					CASE downSize OF
//					8: MEM[storeAddr] := tmp[63:0]
//					ESAC
//				FI
//				storeOffset := storeOffset + 1
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPACKSTOREHPD'. Intrinsic: '_mm512_mask_extpackstorehi_pd'.
// Requires KNCNI.
func MaskExtpackstorehiPd(mt uintptr, k Mmask8, v1 M512d, conv MMDOWNCONVPDENUM, hint int)  {
	maskExtpackstorehiPd(uintptr(mt), uint8(k), [8]float64(v1), conv, hint)
}

func maskExtpackstorehiPd(mt uintptr, k uint8, v1 [8]float64, conv MMDOWNCONVPDENUM, hint int) 


// ExtpackstorehiPs: Down-converts and stores packed single-precision (32-bit)
// floating-point elements of 'v1' into a byte/word/doubleword stream according
// to 'conv' at a logically mapped starting address (mt-64), storing the
// high-64-byte elements of that stream (those elemetns of the stream that map
// at or after the first 64-byte-aligned address following (m5-64)). 'hint'
// indicates to the processor whether the data is non-temporal. 
//
//		DOWNCONVERT(element, convertTo) {
//			CASE converTo OF
//			_MM_UPCONV_PS_NONE:	   RETURN element[i+31:i]
//			_MM_UPCONV_PS_FLOAT16: RETURN Float32ToFloat16(element[i+31:i])
//			_MM_UPCONV_PS_UINT8:   RETURN UInt32ToUInt8(element[i+31:i])
//			_MM_UPCONV_PS_SINT8:   RETURN SInt32ToSInt8(element[i+31:i])
//			_MM_UPCONV_PS_UINT16:  RETURN UInt32ToUInt16(element[i+31:i])
//			_MM_UPCONV_PS_SINT16:  RETURN SInt32ToSInt16(element[i+31:i])
//			ESAC
//		}
//		
//		DOWNCONVERTSIZE(convertTo) {
//			CASE converTo OF
//			_MM_UPCONV_PS_NONE:	   RETURN 4
//			_MM_UPCONV_PS_FLOAT16: RETURN 2
//			_MM_UPCONV_PS_UINT8:   RETURN 1
//			_MM_UPCONV_PS_SINT8:   RETURN 1
//			_MM_UPCONV_PS_UINT16:  RETURN 2
//			_MM_UPCONV_PS_SINT16:  RETURN 2
//			ESAC
//		}
//		
//		storeOffset := 0
//		foundNext64BytesBoundary := false
//		downSize := DOWNCONVERTSIZE(conv)
//		addr = mt-64
//		FOR j := 0 to 15
//			IF foundNext64BytesBoundary == false
//				IF ((addr + (storeOffset + 1)*downSize) % 64) == 0
//					foundNext64BytesBoundary = true
//				FI
//			ELSE
//				i := j*32
//				tmp := DOWNCONVERT(v1[i+31:i], conv)
//				storeAddr := addr + storeOffset * downSize
//				CASE downSize OF
//				4: MEM[storeAddr] := tmp[31:0]
//				2: MEM[storeAddr] := tmp[15:0]
//				1: MEM[storeAddr] := tmp[7:0]
//				ESAC
//			FI
//			storeOffset := storeOffset + 1
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPACKSTOREHPS'. Intrinsic: '_mm512_extpackstorehi_ps'.
// Requires KNCNI.
func ExtpackstorehiPs(mt uintptr, v1 M512, conv MMDOWNCONVPSENUM, hint int)  {
	extpackstorehiPs(uintptr(mt), [16]float32(v1), conv, hint)
}

func extpackstorehiPs(mt uintptr, v1 [16]float32, conv MMDOWNCONVPSENUM, hint int) 


// MaskExtpackstorehiPs: Down-converts and stores packed single-precision
// (32-bit) floating-point elements of 'v1' into a byte/word/doubleword stream
// according to 'conv' at a logically mapped starting address (mt-64), storing
// the high-64-byte elements of that stream (those elemetns of the stream that
// map at or after the first 64-byte-aligned address following (m5-64)). 'hint'
// indicates to the processor whether the data is non-temporal. Elements are
// stored to memory according to element selector 'k' (elements are skipped
// when the corresponding mask bit is not set). 
//
//		DOWNCONVERT(element, convertTo) {
//			CASE converTo OF
//			_MM_UPCONV_PS_NONE:	   RETURN element[i+31:i]
//			_MM_UPCONV_PS_FLOAT16: RETURN Float32ToFloat16(element[i+31:i])
//			_MM_UPCONV_PS_UINT8:   RETURN UInt32ToUInt8(element[i+31:i])
//			_MM_UPCONV_PS_SINT8:   RETURN SInt32ToSInt8(element[i+31:i])
//			_MM_UPCONV_PS_UINT16:  RETURN UInt32ToUInt16(element[i+31:i])
//			_MM_UPCONV_PS_SINT16:  RETURN SInt32ToSInt16(element[i+31:i])
//			ESAC
//		}
//		
//		DOWNCONVERTSIZE(convertTo) {
//			CASE converTo OF
//			_MM_UPCONV_PS_NONE:    RETURN 4
//			_MM_UPCONV_PS_FLOAT16: RETURN 2
//			_MM_UPCONV_PS_UINT8:   RETURN 1
//			_MM_UPCONV_PS_SINT8:   RETURN 1
//			_MM_UPCONV_PS_UINT16:  RETURN 2
//			_MM_UPCONV_PS_SINT16:  RETURN 2
//			ESAC
//		}
//		
//		storeOffset := 0
//		foundNext64BytesBoundary := false
//		downSize := DOWNCONVERTSIZE(conv)
//		addr = mt-64
//		FOR j := 0 to 15
//			IF k[j]
//				IF foundNext64BytesBoundary == false
//					IF ((addr + (storeOffset + 1)*downSize) % 64) == 0
//						foundNext64BytesBoundary = true
//					FI
//				ELSE
//					i := j*32
//					tmp := DOWNCONVERT(v1[i+31:i], conv)
//					storeAddr := addr + storeOffset * downSize
//					CASE downSize OF
//					4: MEM[storeAddr] := tmp[31:0]
//					2: MEM[storeAddr] := tmp[15:0]
//					1: MEM[storeAddr] := tmp[7:0]
//					ESAC
//				FI
//				storeOffset := storeOffset + 1
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPACKSTOREHPS'. Intrinsic: '_mm512_mask_extpackstorehi_ps'.
// Requires KNCNI.
func MaskExtpackstorehiPs(mt uintptr, k Mmask16, v1 M512, conv MMDOWNCONVPSENUM, hint int)  {
	maskExtpackstorehiPs(uintptr(mt), uint16(k), [16]float32(v1), conv, hint)
}

func maskExtpackstorehiPs(mt uintptr, k uint16, v1 [16]float32, conv MMDOWNCONVPSENUM, hint int) 


// ExtpackstoreloEpi32: Down-converts and stores packed 32-bit integer elements
// of 'v1' into a byte/word/doubleword stream according to 'conv' at a
// logically mapped starting address 'mt', storing the low-64-byte elements of
// that stream (those elements of the stream that map before the first
// 64-byte-aligned address follwing 'mt'). 'hint' indicates to the processor
// whether the data is non-temporal. 
//
//		DOWNCONVERT(element, convertTo) {
//			CASE converTo OF
//			_MM_UPCONV_EPI32_NONE:   RETURN element[i+31:i]
//			_MM_UPCONV_EPI32_UINT8:  RETURN UInt32ToUInt8(element[i+31:i])
//			_MM_UPCONV_EPI32_SINT8:  RETURN SInt32ToSInt8(element[i+31:i])
//			_MM_UPCONV_EPI32_UINT16: RETURN UInt32ToUInt16(element[i+31:i])
//			_MM_UPCONV_EPI32_SINT16: RETURN SInt32ToSInt16(element[i+31:i])
//			ESAC
//		}
//		
//		DOWNCONVERTSIZE(convertTo) {
//			CASE converTo OF
//			_MM_UPCONV_EPI32_NONE:   RETURN 4
//			_MM_UPCONV_EPI32_UINT8:  RETURN 1
//			_MM_UPCONV_EPI32_SINT8:  RETURN 1
//			_MM_UPCONV_EPI32_UINT16: RETURN 2
//			_MM_UPCONV_EPI32_SINT16: RETURN 2
//			ESAC
//		}
//		
//		storeOffset := 0
//		downSize := DOWNCONVERTSIZE(conv)
//		addr = mt
//		FOR j := 0 to 15
//			i := j*32
//			tmp := DOWNCONVERT(v1[i+31:i], conv)
//			storeAddr := addr + storeOffset * downSize
//			CASE downSize OF
//			4: MEM[storeAddr] := tmp[31:0]
//			2: MEM[storeAddr] := tmp[15:0]
//			1: MEM[storeAddr] := tmp[7:0]
//			ESAC
//			storeOffset := storeOffset + 1
//			IF ((addr + storeOffset * downSize) % 64) == 0
//				BREAK
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPACKSTORELD'. Intrinsic: '_mm512_extpackstorelo_epi32'.
// Requires KNCNI.
func ExtpackstoreloEpi32(mt uintptr, v1 M512i, conv MMDOWNCONVEPI32ENUM, hint int)  {
	extpackstoreloEpi32(uintptr(mt), [64]byte(v1), conv, hint)
}

func extpackstoreloEpi32(mt uintptr, v1 [64]byte, conv MMDOWNCONVEPI32ENUM, hint int) 


// MaskExtpackstoreloEpi32: Down-converts and stores packed 32-bit integer
// elements of 'v1' into a byte/word/doubleword stream according to 'conv' at a
// logically mapped starting address 'mt', storing the low-64-byte elements of
// that stream (those elements of the stream that map before the first
// 64-byte-aligned address follwing 'mt'). 'hint' indicates to the processor
// whether the data is non-temporal. Elements are written to memory according
// to element selector 'k' (elements are skipped when the corresponding mask
// bit is not set). 
//
//		DOWNCONVERT(element, convertTo) {
//			CASE converTo OF
//			_MM_UPCONV_EPI32_NONE:   RETURN element[i+31:i]
//			_MM_UPCONV_EPI32_UINT8:  RETURN UInt32ToUInt8(element[i+31:i])
//			_MM_UPCONV_EPI32_SINT8:  RETURN SInt32ToSInt8(element[i+31:i])
//			_MM_UPCONV_EPI32_UINT16: RETURN UInt32ToUInt16(element[i+31:i])
//			_MM_UPCONV_EPI32_SINT16: RETURN SInt32ToSInt16(element[i+31:i])
//			ESAC
//		}
//		
//		DOWNCONVERTSIZE(convertTo) {
//			CASE converTo OF
//			_MM_UPCONV_EPI32_NONE:   RETURN 4
//			_MM_UPCONV_EPI32_UINT8:  RETURN 1
//			_MM_UPCONV_EPI32_SINT8:  RETURN 1
//			_MM_UPCONV_EPI32_UINT16: RETURN 2
//			_MM_UPCONV_EPI32_SINT16: RETURN 2
//			ESAC
//		}
//		
//		storeOffset := 0
//		downSize := DOWNCONVERTSIZE(conv)
//		addr = mt
//		FOR j := 0 to 15
//			IF k[j]
//				i := j*32
//				tmp := DOWNCONVERT(v1[i+31:i], conv)
//				storeAddr := addr + storeOffset * downSize
//				CASE downSize OF
//				4: MEM[storeAddr] := tmp[31:0]
//				2: MEM[storeAddr] := tmp[15:0]
//				1: MEM[storeAddr] := tmp[7:0]
//				ESAC
//				storeOffset := storeOffset + 1
//				IF ((addr + storeOffset * downSize) % 64) == 0
//					BREAK
//				FI
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPACKSTORELD'. Intrinsic: '_mm512_mask_extpackstorelo_epi32'.
// Requires KNCNI.
func MaskExtpackstoreloEpi32(mt uintptr, k Mmask16, v1 M512i, conv MMDOWNCONVEPI32ENUM, hint int)  {
	maskExtpackstoreloEpi32(uintptr(mt), uint16(k), [64]byte(v1), conv, hint)
}

func maskExtpackstoreloEpi32(mt uintptr, k uint16, v1 [64]byte, conv MMDOWNCONVEPI32ENUM, hint int) 


// ExtpackstoreloEpi64: Down-converts and stores packed 64-bit integer elements
// of 'v1' into a quadword stream according to 'conv' at a logically mapped
// starting address 'mt', storing the low-64-byte elements of that stream
// (those elements of the stream that map before the first 64-byte-aligned
// address follwing 'mt'). 'hint' indicates to the processor whether the data
// is non-temporal. 
//
//		DOWNCONVERT(element, convertTo) {
//			CASE converTo OF
//			_MM_UPCONV_EPI64_NONE: RETURN element[i+63:i]
//			ESAC
//		}
//		
//		DOWNCONVERTSIZE(convertTo) {
//			CASE converTo OF
//			_MM_UPCONV_EPI64_NONE: RETURN 8
//			ESAC
//		}
//		
//		storeOffset := 0
//		downSize := DOWNCONVERTSIZE(conv)
//		addr = mt
//		FOR j := 0 to 7
//			i := j*63
//			tmp := DOWNCONVERT(v1[i+63:i], conv)
//			storeAddr := addr + storeOffset * downSize
//			CASE downSize OF
//			8: MEM[storeAddr] := tmp[63:0]
//			ESAC
//			storeOffset := storeOffset + 1
//			IF ((addr + storeOffset * downSize) % 64) == 0
//				BREAK
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPACKSTORELQ'. Intrinsic: '_mm512_extpackstorelo_epi64'.
// Requires KNCNI.
func ExtpackstoreloEpi64(mt uintptr, v1 M512i, conv MMDOWNCONVEPI64ENUM, hint int)  {
	extpackstoreloEpi64(uintptr(mt), [64]byte(v1), conv, hint)
}

func extpackstoreloEpi64(mt uintptr, v1 [64]byte, conv MMDOWNCONVEPI64ENUM, hint int) 


// MaskExtpackstoreloEpi64: Down-converts and stores packed 64-bit integer
// elements of 'v1' into a quadword stream according to 'conv' at a logically
// mapped starting address 'mt', storing the low-64-byte elements of that
// stream (those elements of the stream that map before the first
// 64-byte-aligned address follwing 'mt'). 'hint' indicates to the processor
// whether the data is non-temporal. Elements are stored to memory according to
// element selector 'k' (elements are skipped whent he corresponding mask bit
// is not set). 
//
//		DOWNCONVERT(element, convertTo) {
//			CASE converTo OF
//			_MM_UPCONV_EPI64_NONE: RETURN element[i+63:i]
//			ESAC
//		}
//		
//		DOWNCONVERTSIZE(convertTo) {
//			CASE converTo OF
//			_MM_UPCONV_EPI64_NONE: RETURN 8
//			ESAC
//		}
//		
//		storeOffset := 0
//		downSize := DOWNCONVERTSIZE(conv)
//		addr = mt
//		FOR j := 0 to 7
//			IF k[j]
//				i := j*63
//				tmp := DOWNCONVERT(v1[i+63:i], conv)
//				storeAddr := addr + storeOffset * downSize
//				CASE downSize OF
//				8: MEM[storeAddr] := tmp[63:0]
//				ESAC
//				storeOffset := storeOffset + 1
//				IF ((addr + storeOffset * downSize) % 64) == 0
//					BREAK
//				FI
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPACKSTORELQ'. Intrinsic: '_mm512_mask_extpackstorelo_epi64'.
// Requires KNCNI.
func MaskExtpackstoreloEpi64(mt uintptr, k Mmask8, v1 M512i, conv MMDOWNCONVEPI64ENUM, hint int)  {
	maskExtpackstoreloEpi64(uintptr(mt), uint8(k), [64]byte(v1), conv, hint)
}

func maskExtpackstoreloEpi64(mt uintptr, k uint8, v1 [64]byte, conv MMDOWNCONVEPI64ENUM, hint int) 


// ExtpackstoreloPd: Down-converts and stores packed double-precision (64-bit)
// floating-point elements of 'v1' into a quadword stream according to 'conv'
// at a logically mapped starting address 'mt', storing the low-64-byte
// elements of that stream (those elements of the stream that map before the
// first 64-byte-aligned address follwing 'mt'). 'hint' indicates to the
// processor whether the data is non-temporal. 
//
//		DOWNCONVERT(element, convertTo) {
//			CASE converTo OF
//			_MM_UPCONV_PD_NONE: RETURN element[i+63:i]
//			ESAC
//		}
//		
//		DOWNCONVERTSIZE(convertTo) {
//			CASE converTo OF
//			_MM_UPCONV_PD_NONE: RETURN 8
//			ESAC
//		}
//		
//		storeOffset := 0
//		downSize := DOWNCONVERTSIZE(conv)
//		addr = mt
//		FOR j := 0 to 7
//			i := j*63
//			tmp := DOWNCONVERT(v1[i+63:i], conv)
//			storeAddr := addr + storeOffset * downSize
//			CASE downSize OF
//			8: MEM[storeAddr] := tmp[63:0]
//			ESAC
//			storeOffset := storeOffset + 1
//			IF ((addr + storeOffset * downSize) % 64) == 0
//				BREAK
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPACKSTORELPD'. Intrinsic: '_mm512_extpackstorelo_pd'.
// Requires KNCNI.
func ExtpackstoreloPd(mt uintptr, v1 M512d, conv MMDOWNCONVPDENUM, hint int)  {
	extpackstoreloPd(uintptr(mt), [8]float64(v1), conv, hint)
}

func extpackstoreloPd(mt uintptr, v1 [8]float64, conv MMDOWNCONVPDENUM, hint int) 


// MaskExtpackstoreloPd: Down-converts and stores packed double-precision
// (64-bit) floating-point elements of 'v1' into a quadword stream according to
// 'conv' at a logically mapped starting address 'mt', storing the low-64-byte
// elements of that stream (those elements of the stream that map before the
// first 64-byte-aligned address follwing 'mt'). 'hint' indicates to the
// processor whether the data is non-temporal. Elements are stored to memory
// according to element selector 'k' (elements are skipped when the
// corresponding mask bit is not set). 
//
//		DOWNCONVERT(element, convertTo) {
//			CASE converTo OF
//			_MM_UPCONV_PD_NONE: RETURN element[i+63:i]
//			ESAC
//		}
//		
//		DOWNCONVERTSIZE(convertTo) {
//			CASE converTo OF
//			_MM_UPCONV_PD_NONE: RETURN 8
//			ESAC
//		}
//		
//		storeOffset := 0
//		downSize := DOWNCONVERTSIZE(conv)
//		addr = mt
//		FOR j := 0 to 7
//			IF k[j]
//				i := j*63
//				tmp := DOWNCONVERT(v1[i+63:i], conv)
//				storeAddr := addr + storeOffset * downSize
//				CASE downSize OF
//				8: MEM[storeAddr] := tmp[63:0]
//				ESAC
//				storeOffset := storeOffset + 1
//				IF ((addr + storeOffset * downSize) % 64) == 0
//					BREAK
//				FI
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPACKSTORELPD'. Intrinsic: '_mm512_mask_extpackstorelo_pd'.
// Requires KNCNI.
func MaskExtpackstoreloPd(mt uintptr, k Mmask8, v1 M512d, conv MMDOWNCONVPDENUM, hint int)  {
	maskExtpackstoreloPd(uintptr(mt), uint8(k), [8]float64(v1), conv, hint)
}

func maskExtpackstoreloPd(mt uintptr, k uint8, v1 [8]float64, conv MMDOWNCONVPDENUM, hint int) 


// ExtpackstoreloPs: Down-converts and stores packed single-precision (32-bit)
// floating-point elements of 'v1' into a byte/word/doubleword stream according
// to 'conv' at a logically mapped starting address 'mt', storing the
// low-64-byte elements of that stream (those elements of the stream that map
// before the first 64-byte-aligned address follwing 'mt'). 'hint' indicates to
// the processor whether the data is non-temporal. 
//
//		DOWNCONVERT(element, convertTo) {
//			CASE converTo OF
//			_MM_UPCONV_PS_NONE:	   RETURN element[i+31:i]
//			_MM_UPCONV_PS_FLOAT16: RETURN Float32ToFloat16(element[i+31:i])
//			_MM_UPCONV_PS_UINT8:   RETURN UInt32ToUInt8(element[i+31:i])
//			_MM_UPCONV_PS_SINT8:   RETURN SInt32ToSInt8(element[i+31:i])
//			_MM_UPCONV_PS_UINT16:  RETURN UInt32ToUInt16(element[i+31:i])
//			_MM_UPCONV_PS_SINT16:  RETURN SInt32ToSInt16(element[i+31:i])
//			ESAC
//		}
//		
//		DOWNCONVERTSIZE(convertTo) {
//			CASE converTo OF
//			_MM_UPCONV_PS_NONE:	   RETURN 4
//			_MM_UPCONV_PS_FLOAT16: RETURN 2
//			_MM_UPCONV_PS_UINT8:   RETURN 1
//			_MM_UPCONV_PS_SINT8:   RETURN 1
//			_MM_UPCONV_PS_UINT16:  RETURN 2
//			_MM_UPCONV_PS_SINT16:  RETURN 2
//			ESAC
//		}
//		
//		storeOffset := 0
//		downSize := DOWNCONVERTSIZE(conv)
//		addr = mt
//		FOR j := 0 to 15
//			i := j*32
//			tmp := DOWNCONVERT(v1[i+31:i], conv)
//			storeAddr := addr + storeOffset * downSize
//			CASE downSize OF
//			4: MEM[storeAddr] := tmp[31:0]
//			2: MEM[storeAddr] := tmp[15:0]
//			1: MEM[storeAddr] := tmp[7:0]
//			ESAC
//			storeOffset := storeOffset + 1
//			IF ((addr + storeOffset * downSize) % 64) == 0
//				BREAK
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPACKSTORELPS'. Intrinsic: '_mm512_extpackstorelo_ps'.
// Requires KNCNI.
func ExtpackstoreloPs(mt uintptr, v1 M512, conv MMDOWNCONVPSENUM, hint int)  {
	extpackstoreloPs(uintptr(mt), [16]float32(v1), conv, hint)
}

func extpackstoreloPs(mt uintptr, v1 [16]float32, conv MMDOWNCONVPSENUM, hint int) 


// MaskExtpackstoreloPs: Down-converts and stores packed single-precision
// (32-bit) floating-point elements of 'v1' into a byte/word/doubleword stream
// according to 'conv' at a logically mapped starting address 'mt', storing the
// low-64-byte elements of that stream (those elements of the stream that map
// before the first 64-byte-aligned address follwing 'mt'). 'hint' indicates to
// the processor whether the data is non-temporal. Elements are stored to
// memory according to element selector 'k' (elements are skipped when the
// corresponding mask bit is not set). 
//
//		DOWNCONVERT(element, convertTo) {
//			CASE converTo OF
//			_MM_UPCONV_PS_NONE:	   RETURN element[i+31:i]
//			_MM_UPCONV_PS_FLOAT16: RETURN Float32ToFloat16(element[i+31:i])
//			_MM_UPCONV_PS_UINT8:   RETURN UInt32ToUInt8(element[i+31:i])
//			_MM_UPCONV_PS_SINT8:   RETURN SInt32ToSInt8(element[i+31:i])
//			_MM_UPCONV_PS_UINT16:  RETURN UInt32ToUInt16(element[i+31:i])
//			_MM_UPCONV_PS_SINT16:  RETURN SInt32ToSInt16(element[i+31:i])
//			ESAC
//		}
//		
//		DOWNCONVERTSIZE(convertTo) {
//			CASE converTo OF
//			_MM_UPCONV_PS_NONE:    RETURN 4
//			_MM_UPCONV_PS_FLOAT16: RETURN 2
//			_MM_UPCONV_PS_UINT8:   RETURN 1
//			_MM_UPCONV_PS_SINT8:   RETURN 1
//			_MM_UPCONV_PS_UINT16:  RETURN 2
//			_MM_UPCONV_PS_SINT16:  RETURN 2
//			ESAC
//		}
//		
//		storeOffset := 0
//		downSize := DOWNCONVERTSIZE(conv)
//		addr = mt
//		FOR j := 0 to 15
//			IF k[j]
//				i := j*32
//				tmp := DOWNCONVERT(v1[i+31:i], conv)
//				storeAddr := addr + storeOffset * downSize
//				CASE downSize OF
//				4: MEM[storeAddr] := tmp[31:0]
//				2: MEM[storeAddr] := tmp[15:0]
//				1: MEM[storeAddr] := tmp[7:0]
//				ESAC
//				storeOffset := storeOffset + 1
//				IF ((addr + storeOffset * downSize) % 64) == 0
//					BREAK
//				FI
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPACKSTORELPS'. Intrinsic: '_mm512_mask_extpackstorelo_ps'.
// Requires KNCNI.
func MaskExtpackstoreloPs(mt uintptr, k Mmask16, v1 M512, conv MMDOWNCONVPSENUM, hint int)  {
	maskExtpackstoreloPs(uintptr(mt), uint16(k), [16]float32(v1), conv, hint)
}

func maskExtpackstoreloPs(mt uintptr, k uint16, v1 [16]float32, conv MMDOWNCONVPSENUM, hint int) 


// Extractf32x4Ps: Extract 128 bits (composed of 4 packed single-precision
// (32-bit) floating-point elements) from 'a', selected with 'imm8', and store
// the result in 'dst'. 
//
//		CASE imm8[7:0] of
//		0: dst[127:0] := a[127:0]
//		1: dst[127:0] := a[255:128]
//		2: dst[127:0] := a[383:256]
//		3: dst[127:0] := a[511:384]
//		ESAC
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTF32X4'. Intrinsic: '_mm512_extractf32x4_ps'.
// Requires AVX512F.
func Extractf32x4Ps(a M512, imm8 int) M128 {
	return M128(extractf32x4Ps([16]float32(a), imm8))
}

func extractf32x4Ps(a [16]float32, imm8 int) [4]float32


// MaskExtractf32x4Ps: Extract 128 bits (composed of 4 packed single-precision
// (32-bit) floating-point elements) from 'a', selected with 'imm8', and store
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		CASE imm8[7:0] of
//		0: tmp[127:0] := a[127:0]
//		1: tmp[127:0] := a[255:128]
//		2: tmp[127:0] := a[383:256]
//		3: tmp[127:0] := a[511:384]
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTF32X4'. Intrinsic: '_mm512_mask_extractf32x4_ps'.
// Requires AVX512F.
func MaskExtractf32x4Ps(src M128, k Mmask8, a M512, imm8 int) M128 {
	return M128(maskExtractf32x4Ps([4]float32(src), uint8(k), [16]float32(a), imm8))
}

func maskExtractf32x4Ps(src [4]float32, k uint8, a [16]float32, imm8 int) [4]float32


// MaskzExtractf32x4Ps: Extract 128 bits (composed of 4 packed single-precision
// (32-bit) floating-point elements) from 'a', selected with 'imm8', and store
// the results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		CASE imm8[7:0] of
//		0: tmp[127:0] := a[127:0]
//		1: tmp[127:0] := a[255:128]
//		2: tmp[127:0] := a[383:256]
//		3: tmp[127:0] := a[511:384]
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTF32X4'. Intrinsic: '_mm512_maskz_extractf32x4_ps'.
// Requires AVX512F.
func MaskzExtractf32x4Ps(k Mmask8, a M512, imm8 int) M128 {
	return M128(maskzExtractf32x4Ps(uint8(k), [16]float32(a), imm8))
}

func maskzExtractf32x4Ps(k uint8, a [16]float32, imm8 int) [4]float32


// Extractf32x8Ps: Extract 256 bits (composed of 8 packed single-precision
// (32-bit) floating-point elements) from 'a', selected with 'imm8', and store
// the result in 'dst'. 
//
//		CASE imm8[7:0] of
//		0: dst[255:0] := a[255:0]
//		1: dst[255:0] := a[511:256]
//		ESAC
//		dst[MAX:256] := 0
//
// Instruction: 'VEXTRACTF32X8'. Intrinsic: '_mm512_extractf32x8_ps'.
// Requires AVX512DQ.
func Extractf32x8Ps(a M512, imm8 int) M256 {
	return M256(extractf32x8Ps([16]float32(a), imm8))
}

func extractf32x8Ps(a [16]float32, imm8 int) [8]float32


// MaskExtractf32x8Ps: Extract 256 bits (composed of 8 packed single-precision
// (32-bit) floating-point elements) from 'a', selected with 'imm8', and store
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		CASE imm8[7:0] of
//		0: tmp[255:0] := a[255:0]
//		1: tmp[255:0] := a[511:256]
//		ESAC
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VEXTRACTF32X8'. Intrinsic: '_mm512_mask_extractf32x8_ps'.
// Requires AVX512DQ.
func MaskExtractf32x8Ps(src M256, k Mmask8, a M512, imm8 int) M256 {
	return M256(maskExtractf32x8Ps([8]float32(src), uint8(k), [16]float32(a), imm8))
}

func maskExtractf32x8Ps(src [8]float32, k uint8, a [16]float32, imm8 int) [8]float32


// MaskzExtractf32x8Ps: Extract 256 bits (composed of 8 packed single-precision
// (32-bit) floating-point elements) from 'a', selected with 'imm8', and store
// the results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		CASE imm8[7:0] of
//		0: tmp[255:0] := a[255:0]
//		1: tmp[255:0] := a[511:256]
//		ESAC
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VEXTRACTF32X8'. Intrinsic: '_mm512_maskz_extractf32x8_ps'.
// Requires AVX512DQ.
func MaskzExtractf32x8Ps(k Mmask8, a M512, imm8 int) M256 {
	return M256(maskzExtractf32x8Ps(uint8(k), [16]float32(a), imm8))
}

func maskzExtractf32x8Ps(k uint8, a [16]float32, imm8 int) [8]float32


// Extractf64x2Pd: Extract 128 bits (composed of 2 packed double-precision
// (64-bit) floating-point elements) from 'a', selected with 'imm8', and store
// the result in 'dst'. 
//
//		CASE imm8[7:0] of
//		0: dst[127:0] := a[127:0]
//		1: dst[127:0] := a[255:128]
//		2: dst[127:0] := a[383:256]
//		3: dst[127:0] := a[511:384]
//		ESAC
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTF64X2'. Intrinsic: '_mm512_extractf64x2_pd'.
// Requires AVX512DQ.
func Extractf64x2Pd(a M512d, imm8 int) M128d {
	return M128d(extractf64x2Pd([8]float64(a), imm8))
}

func extractf64x2Pd(a [8]float64, imm8 int) [2]float64


// MaskExtractf64x2Pd: Extract 128 bits (composed of 2 packed double-precision
// (64-bit) floating-point elements) from 'a', selected with 'imm8', and store
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		CASE imm8[7:0] of
//		0: tmp[127:0] := a[127:0]
//		1: tmp[127:0] := a[255:128]
//		2: tmp[127:0] := a[383:256]
//		3: tmp[127:0] := a[511:384]
//		ESAC
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTF64X2'. Intrinsic: '_mm512_mask_extractf64x2_pd'.
// Requires AVX512DQ.
func MaskExtractf64x2Pd(src M128d, k Mmask8, a M512d, imm8 int) M128d {
	return M128d(maskExtractf64x2Pd([2]float64(src), uint8(k), [8]float64(a), imm8))
}

func maskExtractf64x2Pd(src [2]float64, k uint8, a [8]float64, imm8 int) [2]float64


// MaskzExtractf64x2Pd: Extract 128 bits (composed of 2 packed double-precision
// (64-bit) floating-point elements) from 'a', selected with 'imm8', and store
// the results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		CASE imm8[7:0] of
//		0: tmp[127:0] := a[127:0]
//		1: tmp[127:0] := a[255:128]
//		2: tmp[127:0] := a[383:256]
//		3: tmp[127:0] := a[511:384]
//		ESAC
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTF64X2'. Intrinsic: '_mm512_maskz_extractf64x2_pd'.
// Requires AVX512DQ.
func MaskzExtractf64x2Pd(k Mmask8, a M512d, imm8 int) M128d {
	return M128d(maskzExtractf64x2Pd(uint8(k), [8]float64(a), imm8))
}

func maskzExtractf64x2Pd(k uint8, a [8]float64, imm8 int) [2]float64


// Extractf64x4Pd: Extract 256 bits (composed of 4 packed double-precision
// (64-bit) floating-point elements) from 'a', selected with 'imm8', and store
// the result in 'dst'. 
//
//		CASE imm8[7:0] of
//		0: dst[255:0] := a[255:0]
//		1: dst[255:0] := a[511:256]
//		ESAC
//		dst[MAX:256] := 0
//
// Instruction: 'VEXTRACTF64X4'. Intrinsic: '_mm512_extractf64x4_pd'.
// Requires AVX512F.
func Extractf64x4Pd(a M512d, imm8 int) M256d {
	return M256d(extractf64x4Pd([8]float64(a), imm8))
}

func extractf64x4Pd(a [8]float64, imm8 int) [4]float64


// MaskExtractf64x4Pd: Extract 256 bits (composed of 4 packed double-precision
// (64-bit) floating-point elements) from 'a', selected with 'imm8', and store
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		CASE imm8[7:0] of
//		0: tmp[255:0] := a[255:0]
//		1: tmp[255:0] := a[511:256]
//		ESAC
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VEXTRACTF64X4'. Intrinsic: '_mm512_mask_extractf64x4_pd'.
// Requires AVX512F.
func MaskExtractf64x4Pd(src M256d, k Mmask8, a M512d, imm8 int) M256d {
	return M256d(maskExtractf64x4Pd([4]float64(src), uint8(k), [8]float64(a), imm8))
}

func maskExtractf64x4Pd(src [4]float64, k uint8, a [8]float64, imm8 int) [4]float64


// MaskzExtractf64x4Pd: Extract 256 bits (composed of 4 packed double-precision
// (64-bit) floating-point elements) from 'a', selected with 'imm8', and store
// the results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		CASE imm8[7:0] of
//		0: tmp[255:0] := a[255:0]
//		1: tmp[255:0] := a[511:256]
//		ESAC
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VEXTRACTF64X4'. Intrinsic: '_mm512_maskz_extractf64x4_pd'.
// Requires AVX512F.
func MaskzExtractf64x4Pd(k Mmask8, a M512d, imm8 int) M256d {
	return M256d(maskzExtractf64x4Pd(uint8(k), [8]float64(a), imm8))
}

func maskzExtractf64x4Pd(k uint8, a [8]float64, imm8 int) [4]float64


// Extracti32x4Epi32: Extract 128 bits (composed of 4 packed 32-bit integers)
// from 'a', selected with 'imm8', and store the result in 'dst'. 
//
//		CASE imm8[7:0] of
//		0: dst[127:0] := a[127:0]
//		1: dst[127:0] := a[255:128]
//		2: dst[127:0] := a[383:256]
//		3: dst[127:0] := a[511:384]
//		ESAC
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTI32X4'. Intrinsic: '_mm512_extracti32x4_epi32'.
// Requires AVX512F.
func Extracti32x4Epi32(a M512i, imm8 int) M128i {
	return M128i(extracti32x4Epi32([64]byte(a), imm8))
}

func extracti32x4Epi32(a [64]byte, imm8 int) [16]byte


// MaskExtracti32x4Epi32: Extract 128 bits (composed of 4 packed 32-bit
// integers) from 'a', selected with 'imm8', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		CASE imm8[7:0] of
//		0: dst[127:0] := a[127:0]
//		1: dst[127:0] := a[255:128]
//		2: dst[127:0] := a[383:256]
//		3: dst[127:0] := a[511:384]
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTI32X4'. Intrinsic: '_mm512_mask_extracti32x4_epi32'.
// Requires AVX512F.
func MaskExtracti32x4Epi32(src M128i, k Mmask8, a M512i, imm8 int) M128i {
	return M128i(maskExtracti32x4Epi32([16]byte(src), uint8(k), [64]byte(a), imm8))
}

func maskExtracti32x4Epi32(src [16]byte, k uint8, a [64]byte, imm8 int) [16]byte


// MaskzExtracti32x4Epi32: Extract 128 bits (composed of 4 packed 32-bit
// integers) from 'a', selected with 'imm8', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		CASE imm8[7:0] of
//		0: dst[127:0] := a[127:0]
//		1: dst[127:0] := a[255:128]
//		2: dst[127:0] := a[383:256]
//		3: dst[127:0] := a[511:384]
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTI32X4'. Intrinsic: '_mm512_maskz_extracti32x4_epi32'.
// Requires AVX512F.
func MaskzExtracti32x4Epi32(k Mmask8, a M512i, imm8 int) M128i {
	return M128i(maskzExtracti32x4Epi32(uint8(k), [64]byte(a), imm8))
}

func maskzExtracti32x4Epi32(k uint8, a [64]byte, imm8 int) [16]byte


// Extracti32x8Epi32: Extract 256 bits (composed of 8 packed 32-bit integers)
// from 'a', selected with 'imm8', and store the result in 'dst'. 
//
//		CASE imm8[7:0] of
//		0: dst[255:0] := a[255:0]
//		1: dst[255:0] := a[511:256]
//		ESAC
//		dst[MAX:256] := 0
//
// Instruction: 'VEXTRACTI32X8'. Intrinsic: '_mm512_extracti32x8_epi32'.
// Requires AVX512DQ.
func Extracti32x8Epi32(a M512i, imm8 int) M256i {
	return M256i(extracti32x8Epi32([64]byte(a), imm8))
}

func extracti32x8Epi32(a [64]byte, imm8 int) [32]byte


// MaskExtracti32x8Epi32: Extract 256 bits (composed of 8 packed 32-bit
// integers) from 'a', selected with 'imm8', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		CASE imm8[7:0] of
//		0: tmp[255:0] := a[255:0]
//		1: tmp[255:0] := a[511:256]
//		ESAC
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VEXTRACTI32X8'. Intrinsic: '_mm512_mask_extracti32x8_epi32'.
// Requires AVX512DQ.
func MaskExtracti32x8Epi32(src M256i, k Mmask8, a M512i, imm8 int) M256i {
	return M256i(maskExtracti32x8Epi32([32]byte(src), uint8(k), [64]byte(a), imm8))
}

func maskExtracti32x8Epi32(src [32]byte, k uint8, a [64]byte, imm8 int) [32]byte


// MaskzExtracti32x8Epi32: Extract 256 bits (composed of 8 packed 32-bit
// integers) from 'a', selected with 'imm8', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		CASE imm8[7:0] of
//		0: tmp[255:0] := a[255:0]
//		1: tmp[255:0] := a[511:256]
//		ESAC
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VEXTRACTI32X8'. Intrinsic: '_mm512_maskz_extracti32x8_epi32'.
// Requires AVX512DQ.
func MaskzExtracti32x8Epi32(k Mmask8, a M512i, imm8 int) M256i {
	return M256i(maskzExtracti32x8Epi32(uint8(k), [64]byte(a), imm8))
}

func maskzExtracti32x8Epi32(k uint8, a [64]byte, imm8 int) [32]byte


// Extracti64x2Epi64: Extract 128 bits (composed of 2 packed 64-bit integers)
// from 'a', selected with 'imm8', and store the result in 'dst'. 
//
//		CASE imm8[7:0] of
//		0: dst[127:0] := a[127:0]
//		1: dst[127:0] := a[255:128]
//		2: dst[127:0] := a[383:256]
//		3: dst[127:0] := a[511:384]
//		ESAC
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTI64X2'. Intrinsic: '_mm512_extracti64x2_epi64'.
// Requires AVX512DQ.
func Extracti64x2Epi64(a M512i, imm8 int) M128i {
	return M128i(extracti64x2Epi64([64]byte(a), imm8))
}

func extracti64x2Epi64(a [64]byte, imm8 int) [16]byte


// MaskExtracti64x2Epi64: Extract 128 bits (composed of 2 packed 64-bit
// integers) from 'a', selected with 'imm8', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		CASE imm8[7:0] of
//		0: tmp[127:0] := a[127:0]
//		1: tmp[127:0] := a[255:128]
//		2: tmp[127:0] := a[383:256]
//		3: tmp[127:0] := a[511:384]
//		ESAC
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTI64X2'. Intrinsic: '_mm512_mask_extracti64x2_epi64'.
// Requires AVX512DQ.
func MaskExtracti64x2Epi64(src M128i, k Mmask8, a M512i, imm8 int) M128i {
	return M128i(maskExtracti64x2Epi64([16]byte(src), uint8(k), [64]byte(a), imm8))
}

func maskExtracti64x2Epi64(src [16]byte, k uint8, a [64]byte, imm8 int) [16]byte


// MaskzExtracti64x2Epi64: Extract 128 bits (composed of 2 packed 64-bit
// integers) from 'a', selected with 'imm8', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		CASE imm8[7:0] of
//		0: tmp[127:0] := a[127:0]
//		1: tmp[127:0] := a[255:128]
//		2: tmp[127:0] := a[383:256]
//		3: tmp[127:0] := a[511:384]
//		ESAC
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTI64X2'. Intrinsic: '_mm512_maskz_extracti64x2_epi64'.
// Requires AVX512DQ.
func MaskzExtracti64x2Epi64(k Mmask8, a M512i, imm8 int) M128i {
	return M128i(maskzExtracti64x2Epi64(uint8(k), [64]byte(a), imm8))
}

func maskzExtracti64x2Epi64(k uint8, a [64]byte, imm8 int) [16]byte


// Extracti64x4Epi64: Extract 256 bits (composed of 4 packed 64-bit integers)
// from 'a', selected with 'imm8', and store the result in 'dst'. 
//
//		CASE imm8[7:0] of
//		0: dst[255:0] := a[255:0]
//		1: dst[255:0] := a[511:256]
//		ESAC
//		dst[MAX:256] := 0
//
// Instruction: 'VEXTRACTI64X4'. Intrinsic: '_mm512_extracti64x4_epi64'.
// Requires AVX512F.
func Extracti64x4Epi64(a M512i, imm8 int) M256i {
	return M256i(extracti64x4Epi64([64]byte(a), imm8))
}

func extracti64x4Epi64(a [64]byte, imm8 int) [32]byte


// MaskExtracti64x4Epi64: Extract 256 bits (composed of 4 packed 64-bit
// integers) from 'a', selected with 'imm8', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		CASE imm8[7:0] of
//		0: dst[255:0] := a[255:0]
//		1: dst[255:0] := a[511:256]
//		ESAC
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VEXTRACTI64X4'. Intrinsic: '_mm512_mask_extracti64x4_epi64'.
// Requires AVX512F.
func MaskExtracti64x4Epi64(src M256i, k Mmask8, a M512i, imm8 int) M256i {
	return M256i(maskExtracti64x4Epi64([32]byte(src), uint8(k), [64]byte(a), imm8))
}

func maskExtracti64x4Epi64(src [32]byte, k uint8, a [64]byte, imm8 int) [32]byte


// MaskzExtracti64x4Epi64: Extract 256 bits (composed of 4 packed 64-bit
// integers) from 'a', selected with 'imm8', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		CASE imm8[7:0] of
//		0: dst[255:0] := a[255:0]
//		1: dst[255:0] := a[511:256]
//		ESAC
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VEXTRACTI64X4'. Intrinsic: '_mm512_maskz_extracti64x4_epi64'.
// Requires AVX512F.
func MaskzExtracti64x4Epi64(k Mmask8, a M512i, imm8 int) M256i {
	return M256i(maskzExtracti64x4Epi64(uint8(k), [64]byte(a), imm8))
}

func maskzExtracti64x4Epi64(k uint8, a [64]byte, imm8 int) [32]byte


// ExtstoreEpi32: Downconverts packed 32-bit integer elements stored in 'v' to
// a smaller type depending on 'conv' and stores them in memory location 'mt'.
// 'hint' indicates to the processor whether the data is non-temporal. 
//
//		addr := MEM[mt]
//		FOR j := 0 to 15
//			i := j*32
//			CASE conv OF
//			_MM_DOWNCONV_EPI32_NONE:
//				addr[i+31:i] := v[i+31:i]
//			_MM_DOWNCONV_EPI32_UINT8:
//				n := j*8
//				addr[n+7:n] := Int32ToUInt8(v[i+31:i])
//			_MM_DOWNCONV_EPI32_SINT8:
//				n := j*8
//				addr[n+7:n] := Int32ToSInt8(v[i+31:i])
//			_MM_DOWNCONV_EPI32_UINT16:
//				n := j*16
//				addr[n+15:n] := Int32ToUInt16(v[i+31:i])
//			_MM_DOWNCONV_EPI32_SINT16:
//				n := j*16
//				addr[n+15:n] := Int32ToSInt16(v[i+31:i])
//			ESAC
//		ENDFOR
//
// Instruction: 'VMOVDQA32'. Intrinsic: '_mm512_extstore_epi32'.
// Requires KNCNI.
func ExtstoreEpi32(mt uintptr, v M512i, conv MMDOWNCONVEPI32ENUM, hint int)  {
	extstoreEpi32(uintptr(mt), [64]byte(v), conv, hint)
}

func extstoreEpi32(mt uintptr, v [64]byte, conv MMDOWNCONVEPI32ENUM, hint int) 


// MaskExtstoreEpi32: Downconverts packed 32-bit integer elements stored in 'v'
// to a smaller type depending on 'conv' and stores them in memory location
// 'mt' (elements in 'mt' are unaltered when the corresponding mask bit is not
// set). 'hint' indicates to the processor whether the data is non-temporal. 
//
//		addr := MEM[mt]
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				CASE conv OF
//				_MM_DOWNCONV_EPI32_NONE:
//					addr[i+31:i] := v[i+31:i]
//				_MM_DOWNCONV_EPI32_UINT8:
//					n := j*8
//					addr[n+7:n] := Int32ToUInt8(v[i+31:i])
//				_MM_DOWNCONV_EPI32_SINT8:
//					n := j*8
//					addr[n+7:n] := Int32ToSInt8(v[i+31:i])
//				_MM_DOWNCONV_EPI32_UINT16:
//					n := j*16
//					addr[n+15:n] := Int32ToUInt16(v[i+31:i])
//				_MM_DOWNCONV_EPI32_SINT16:
//					n := j*16
//					addr[n+15:n] := Int32ToSInt16(v[i+31:i])
//				ESAC
//			FI
//		ENDFOR
//
// Instruction: 'VMOVDQA32'. Intrinsic: '_mm512_mask_extstore_epi32'.
// Requires KNCNI.
func MaskExtstoreEpi32(mt uintptr, k Mmask16, v M512i, conv MMDOWNCONVEPI32ENUM, hint int)  {
	maskExtstoreEpi32(uintptr(mt), uint16(k), [64]byte(v), conv, hint)
}

func maskExtstoreEpi32(mt uintptr, k uint16, v [64]byte, conv MMDOWNCONVEPI32ENUM, hint int) 


// ExtstoreEpi64: Downconverts packed 64-bit integer elements stored in 'v' to
// a smaller type depending on 'conv' and stores them in memory location 'mt'.
// 'hint' indicates to the processor whether the data is non-temporal. 
//
//		addr := MEM[mt]
//		FOR j := 0 to 7
//			i := j*64
//			CASE conv OF
//			_MM_DOWNCONV_EPI64_NONE: addr[i+63:i] := v[i+63:i]
//			ESAC
//		ENDFOR
//
// Instruction: 'VMOVDQA64'. Intrinsic: '_mm512_extstore_epi64'.
// Requires KNCNI.
func ExtstoreEpi64(mt uintptr, v M512i, conv MMDOWNCONVEPI64ENUM, hint int)  {
	extstoreEpi64(uintptr(mt), [64]byte(v), conv, hint)
}

func extstoreEpi64(mt uintptr, v [64]byte, conv MMDOWNCONVEPI64ENUM, hint int) 


// MaskExtstoreEpi64: Downconverts packed 64-bit integer elements stored in 'v'
// to a smaller type depending on 'conv' and stores them in memory location
// 'mt' (elements in 'mt' are unaltered when the corresponding mask bit is not
// set). 'hint' indicates to the processor whether the data is non-temporal. 
//
//		addr := MEM[mt]
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				CASE conv OF
//				_MM_DOWNCONV_EPI64_NONE: addr[i+63:i] := v[i+63:i]
//				ESAC
//			FI
//		ENDFOR
//
// Instruction: 'VMOVDQA64'. Intrinsic: '_mm512_mask_extstore_epi64'.
// Requires KNCNI.
func MaskExtstoreEpi64(mt uintptr, k Mmask8, v M512i, conv MMDOWNCONVEPI64ENUM, hint int)  {
	maskExtstoreEpi64(uintptr(mt), uint8(k), [64]byte(v), conv, hint)
}

func maskExtstoreEpi64(mt uintptr, k uint8, v [64]byte, conv MMDOWNCONVEPI64ENUM, hint int) 


// ExtstorePd: Downconverts packed double-precision (64-bit) floating-point
// elements stored in 'v' to a smaller type depending on 'conv' and stores them
// in memory location 'mt'. 'hint' indicates to the processor whether the data
// is non-temporal. 
//
//		addr := MEM[mt]
//		FOR j := 0 to 7
//			i := j*64
//			CASE conv OF
//			_MM_DOWNCONV_PS_NONE:
//				addr[i+63:i] := v[i+63:i]
//			ESAC
//		ENDFOR
//
// Instruction: 'VMOVAPD'. Intrinsic: '_mm512_extstore_pd'.
// Requires KNCNI.
func ExtstorePd(mt uintptr, v M512d, conv MMDOWNCONVPDENUM, hint int)  {
	extstorePd(uintptr(mt), [8]float64(v), conv, hint)
}

func extstorePd(mt uintptr, v [8]float64, conv MMDOWNCONVPDENUM, hint int) 


// MaskExtstorePd: Downconverts packed double-precision (64-bit) floating-point
// elements stored in 'v' to a smaller type depending on 'conv' and stores them
// in memory location 'mt' (elements in 'mt' are unaltered when the
// corresponding mask bit is not set). 'hint' indicates to the processor
// whether the data is non-temporal. 
//
//		addr := MEM[mt]		
//		FOR j := 0 to 7
//			i := j*64
//			CASE conv OF
//			_MM_DOWNCONV_PD_NONE:
//				IF k[j]
//					mt[i+63:i] := v[i+63:i]
//				FI
//			ESAC
//		ENDFOR
//
// Instruction: 'VMOVAPD'. Intrinsic: '_mm512_mask_extstore_pd'.
// Requires KNCNI.
func MaskExtstorePd(mt uintptr, k Mmask8, v M512d, conv MMDOWNCONVPDENUM, hint int)  {
	maskExtstorePd(uintptr(mt), uint8(k), [8]float64(v), conv, hint)
}

func maskExtstorePd(mt uintptr, k uint8, v [8]float64, conv MMDOWNCONVPDENUM, hint int) 


// ExtstorePs: Downconverts packed single-precision (32-bit) floating-point
// elements stored in 'v' to a smaller type depending on 'conv' and stores them
// in memory location 'mt'. 'hint' indicates to the processor whether the data
// is non-temporal. 
//
//		addr := MEM[mt]		
//		FOR j := 0 to 15
//			i := j*32
//			CASE conv OF
//			_MM_DOWNCONV_PS_NONE:
//				addr[i+31:i] := v[i+31:i]
//			_MM_DOWNCONV_PS_FLOAT16:
//				n := j*16
//				addr[n+15:n] := Float32ToFloat16(v[i+31:i])
//			_MM_DOWNCONV_PS_UINT8:
//				n := j*8
//				addr[n+7:n] := Float32ToUInt8(v[i+31:i])
//			_MM_DOWNCONV_PS_SINT8:
//				n := j*8
//				addr[n+7:n] := Float32ToSInt8(v[i+31:i])
//			_MM_DOWNCONV_PS_UINT16:
//				n := j*16
//				addr[n+15:n] := Float32ToUInt16(v[i+31:i])
//			_MM_DOWNCONV_PS_SINT16:
//				n := j*16
//				addr[n+15:n] := Float32ToSInt16(v[i+31:i])
//			ESAC
//		ENDFOR
//
// Instruction: 'VMOVAPS'. Intrinsic: '_mm512_extstore_ps'.
// Requires KNCNI.
func ExtstorePs(mt uintptr, v M512, conv MMDOWNCONVPSENUM, hint int)  {
	extstorePs(uintptr(mt), [16]float32(v), conv, hint)
}

func extstorePs(mt uintptr, v [16]float32, conv MMDOWNCONVPSENUM, hint int) 


// MaskExtstorePs: Downconverts packed single-precision (32-bit) floating-point
// elements stored in 'v' to a smaller type depending on 'conv' and stores them
// in memory location 'mt' using writemask 'k' (elements are not written to
// memory when the corresponding mask bit is not set). 'hint' indicates to the
// processor whether the data is non-temporal. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				CASE conv OF
//				_MM_DOWNCONV_PS_NONE:
//					mt[i+31:i] := v[i+31:i]
//				_MM_DOWNCONV_PS_FLOAT16:
//					n := j*16
//					mt[n+15:n] := Float32ToFloat16(v[i+31:i])
//				_MM_DOWNCONV_PS_UINT8:
//					n := j*8
//					mt[n+7:n] := Float32ToUInt8(v[i+31:i])
//				_MM_DOWNCONV_PS_SINT8:
//					n := j*8
//					mt[n+7:n] := Float32ToSInt8(v[i+31:i])
//				_MM_DOWNCONV_PS_UINT16:
//					n := j*16
//					mt[n+15:n] := Float32ToUInt16(v[i+31:i])
//				_MM_DOWNCONV_PS_SINT16:
//					n := j*16
//					mt[n+15:n] := Float32ToSInt16(v[i+31:i])
//				ESAC
//			 FI
//		ENDFOR
//
// Instruction: 'VMOVAPS'. Intrinsic: '_mm512_mask_extstore_ps'.
// Requires KNCNI.
func MaskExtstorePs(mt uintptr, k Mmask16, v M512, conv MMDOWNCONVPSENUM, hint int)  {
	maskExtstorePs(uintptr(mt), uint16(k), [16]float32(v), conv, hint)
}

func maskExtstorePs(mt uintptr, k uint16, v [16]float32, conv MMDOWNCONVPSENUM, hint int) 


// FixupimmPd: Fix up packed double-precision (64-bit) floating-point elements
// in 'a' and 'b' using packed 64-bit integers in 'c', and store the results in
// 'dst'. 'imm8' is used to set the required flags reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFIXUPIMMPD'. Intrinsic: '_mm512_fixupimm_pd'.
// Requires AVX512F.
func FixupimmPd(a M512d, b M512d, c M512i, imm8 int) M512d {
	return M512d(fixupimmPd([8]float64(a), [8]float64(b), [64]byte(c), imm8))
}

func fixupimmPd(a [8]float64, b [8]float64, c [64]byte, imm8 int) [8]float64


// MaskFixupimmPd: Fix up packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' using packed 64-bit integers in 'c', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFIXUPIMMPD'. Intrinsic: '_mm512_mask_fixupimm_pd'.
// Requires AVX512F.
func MaskFixupimmPd(a M512d, k Mmask8, b M512d, c M512i, imm8 int) M512d {
	return M512d(maskFixupimmPd([8]float64(a), uint8(k), [8]float64(b), [64]byte(c), imm8))
}

func maskFixupimmPd(a [8]float64, k uint8, b [8]float64, c [64]byte, imm8 int) [8]float64


// MaskzFixupimmPd: Fix up packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' using packed 64-bit integers in 'c', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFIXUPIMMPD'. Intrinsic: '_mm512_maskz_fixupimm_pd'.
// Requires AVX512F.
func MaskzFixupimmPd(k Mmask8, a M512d, b M512d, c M512i, imm8 int) M512d {
	return M512d(maskzFixupimmPd(uint8(k), [8]float64(a), [8]float64(b), [64]byte(c), imm8))
}

func maskzFixupimmPd(k uint8, a [8]float64, b [8]float64, c [64]byte, imm8 int) [8]float64


// FixupimmPs: Fix up packed single-precision (32-bit) floating-point elements
// in 'a' and 'b' using packed 32-bit integers in 'c', and store the results in
// 'dst'. 'imm8' is used to set the required flags reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFIXUPIMMPS'. Intrinsic: '_mm512_fixupimm_ps'.
// Requires AVX512F.
func FixupimmPs(a M512, b M512, c M512i, imm8 int) M512 {
	return M512(fixupimmPs([16]float32(a), [16]float32(b), [64]byte(c), imm8))
}

func fixupimmPs(a [16]float32, b [16]float32, c [64]byte, imm8 int) [16]float32


// MaskFixupimmPs: Fix up packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' using packed 32-bit integers in 'c', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFIXUPIMMPS'. Intrinsic: '_mm512_mask_fixupimm_ps'.
// Requires AVX512F.
func MaskFixupimmPs(a M512, k Mmask16, b M512, c M512i, imm8 int) M512 {
	return M512(maskFixupimmPs([16]float32(a), uint16(k), [16]float32(b), [64]byte(c), imm8))
}

func maskFixupimmPs(a [16]float32, k uint16, b [16]float32, c [64]byte, imm8 int) [16]float32


// MaskzFixupimmPs: Fix up packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' using packed 32-bit integers in 'c', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFIXUPIMMPS'. Intrinsic: '_mm512_maskz_fixupimm_ps'.
// Requires AVX512F.
func MaskzFixupimmPs(k Mmask16, a M512, b M512, c M512i, imm8 int) M512 {
	return M512(maskzFixupimmPs(uint16(k), [16]float32(a), [16]float32(b), [64]byte(c), imm8))
}

func maskzFixupimmPs(k uint16, a [16]float32, b [16]float32, c [64]byte, imm8 int) [16]float32


// FixupimmRoundPd: Fix up packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' using packed 64-bit integers in 'c', and store the
// results in 'dst'. 'imm8' is used to set the required flags reporting.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFIXUPIMMPD'. Intrinsic: '_mm512_fixupimm_round_pd'.
// Requires AVX512F.
func FixupimmRoundPd(a M512d, b M512d, c M512i, imm8 int, rounding int) M512d {
	return M512d(fixupimmRoundPd([8]float64(a), [8]float64(b), [64]byte(c), imm8, rounding))
}

func fixupimmRoundPd(a [8]float64, b [8]float64, c [64]byte, imm8 int, rounding int) [8]float64


// MaskFixupimmRoundPd: Fix up packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' using packed 64-bit integers in 'c', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFIXUPIMMPD'. Intrinsic: '_mm512_mask_fixupimm_round_pd'.
// Requires AVX512F.
func MaskFixupimmRoundPd(a M512d, k Mmask8, b M512d, c M512i, imm8 int, rounding int) M512d {
	return M512d(maskFixupimmRoundPd([8]float64(a), uint8(k), [8]float64(b), [64]byte(c), imm8, rounding))
}

func maskFixupimmRoundPd(a [8]float64, k uint8, b [8]float64, c [64]byte, imm8 int, rounding int) [8]float64


// MaskzFixupimmRoundPd: Fix up packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' using packed 64-bit integers in 'c', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFIXUPIMMPD'. Intrinsic: '_mm512_maskz_fixupimm_round_pd'.
// Requires AVX512F.
func MaskzFixupimmRoundPd(k Mmask8, a M512d, b M512d, c M512i, imm8 int, rounding int) M512d {
	return M512d(maskzFixupimmRoundPd(uint8(k), [8]float64(a), [8]float64(b), [64]byte(c), imm8, rounding))
}

func maskzFixupimmRoundPd(k uint8, a [8]float64, b [8]float64, c [64]byte, imm8 int, rounding int) [8]float64


// FixupimmRoundPs: Fix up packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' using packed 32-bit integers in 'c', and store the
// results in 'dst'. 'imm8' is used to set the required flags reporting.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFIXUPIMMPS'. Intrinsic: '_mm512_fixupimm_round_ps'.
// Requires AVX512F.
func FixupimmRoundPs(a M512, b M512, c M512i, imm8 int, rounding int) M512 {
	return M512(fixupimmRoundPs([16]float32(a), [16]float32(b), [64]byte(c), imm8, rounding))
}

func fixupimmRoundPs(a [16]float32, b [16]float32, c [64]byte, imm8 int, rounding int) [16]float32


// MaskFixupimmRoundPs: Fix up packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' using packed 32-bit integers in 'c', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFIXUPIMMPS'. Intrinsic: '_mm512_mask_fixupimm_round_ps'.
// Requires AVX512F.
func MaskFixupimmRoundPs(a M512, k Mmask16, b M512, c M512i, imm8 int, rounding int) M512 {
	return M512(maskFixupimmRoundPs([16]float32(a), uint16(k), [16]float32(b), [64]byte(c), imm8, rounding))
}

func maskFixupimmRoundPs(a [16]float32, k uint16, b [16]float32, c [64]byte, imm8 int, rounding int) [16]float32


// MaskzFixupimmRoundPs: Fix up packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' using packed 32-bit integers in 'c', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFIXUPIMMPS'. Intrinsic: '_mm512_maskz_fixupimm_round_ps'.
// Requires AVX512F.
func MaskzFixupimmRoundPs(k Mmask16, a M512, b M512, c M512i, imm8 int, rounding int) M512 {
	return M512(maskzFixupimmRoundPs(uint16(k), [16]float32(a), [16]float32(b), [64]byte(c), imm8, rounding))
}

func maskzFixupimmRoundPs(k uint16, a [16]float32, b [16]float32, c [64]byte, imm8 int, rounding int) [16]float32


// FixupnanPd: Fixes up NaN's from packed double-precision (64-bit)
// floating-point elements in 'v1' and 'v2', storing the results in 'dst' and
// storing the quietized NaN's from 'v1' in 'v3'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := FixupNaNs(v1[i+63:i], v2[i+63:i])
//			v3[i+63:i] := QuietizeNaNs(v1[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFIXUPNANPD'. Intrinsic: '_mm512_fixupnan_pd'.
// Requires KNCNI.
func FixupnanPd(v1 M512d, v2 M512d, v3 M512i) M512d {
	return M512d(fixupnanPd([8]float64(v1), [8]float64(v2), [64]byte(v3)))
}

func fixupnanPd(v1 [8]float64, v2 [8]float64, v3 [64]byte) [8]float64


// MaskFixupnanPd: Fixes up NaN's from packed double-precision (64-bit)
// floating-point elements in 'v1' and 'v2', storing the results in 'dst' using
// writemask 'k' (only elements whose corresponding mask bit is set are used in
// the computation). Quietized NaN's from 'v1' are stored in 'v3'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := FixupNaNs(v1[i+63:i], v2[i+63:i])
//				v3[i+63:i] := QuietizeNaNs(v1[i+63:i])
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFIXUPNANPD'. Intrinsic: '_mm512_mask_fixupnan_pd'.
// Requires KNCNI.
func MaskFixupnanPd(v1 M512d, k Mmask8, v2 M512d, v3 M512i) M512d {
	return M512d(maskFixupnanPd([8]float64(v1), uint8(k), [8]float64(v2), [64]byte(v3)))
}

func maskFixupnanPd(v1 [8]float64, k uint8, v2 [8]float64, v3 [64]byte) [8]float64


// FixupnanPs: Fixes up NaN's from packed single-precision (32-bit)
// floating-point elements in 'v1' and 'v2', storing the results in 'dst' and
// storing the quietized NaN's from 'v1' in 'v3'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := FixupNaNs(v1[i+31:i], v2[i+31:i])
//			v3[i+31:i] := QuietizeNaNs(v1[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFIXUPNANPS'. Intrinsic: '_mm512_fixupnan_ps'.
// Requires KNCNI.
func FixupnanPs(v1 M512, v2 M512, v3 M512i) M512 {
	return M512(fixupnanPs([16]float32(v1), [16]float32(v2), [64]byte(v3)))
}

func fixupnanPs(v1 [16]float32, v2 [16]float32, v3 [64]byte) [16]float32


// MaskFixupnanPs: Fixes up NaN's from packed single-precision (32-bit)
// floating-point elements in 'v1' and 'v2', storing the results in 'dst' using
// writemask 'k' (only elements whose corresponding mask bit is set are used in
// the computation). Quietized NaN's from 'v1' are stored in 'v3'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := FixupNaNs(v1[i+31:i], v2[i+31:i])
//				v3[i+31:i] := QuietizeNaNs(v1[i+31:i])
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFIXUPNANPS'. Intrinsic: '_mm512_mask_fixupnan_ps'.
// Requires KNCNI.
func MaskFixupnanPs(v1 M512, k Mmask16, v2 M512, v3 M512i) M512 {
	return M512(maskFixupnanPs([16]float32(v1), uint16(k), [16]float32(v2), [64]byte(v3)))
}

func maskFixupnanPs(v1 [16]float32, k uint16, v2 [16]float32, v3 [64]byte) [16]float32


// FloorPd: Round the packed double-precision (64-bit) floating-point elements
// in 'a' down to an integer value, and store the results as packed
// double-precision floating-point elements in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := FLOOR(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_floor_pd'.
// Requires AVX512F.
func FloorPd(a M512d) M512d {
	return M512d(floorPd([8]float64(a)))
}

func floorPd(a [8]float64) [8]float64


// MaskFloorPd: Round the packed double-precision (64-bit) floating-point
// elements in 'a' down to an integer value, and store the results as packed
// double-precision floating-point elements in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := FLOOR(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_floor_pd'.
// Requires AVX512F.
func MaskFloorPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskFloorPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskFloorPd(src [8]float64, k uint8, a [8]float64) [8]float64


// FloorPs: Round the packed single-precision (32-bit) floating-point elements
// in 'a' down to an integer value, and store the results as packed
// single-precision floating-point elements in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := FLOOR(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_floor_ps'.
// Requires AVX512F.
func FloorPs(a M512) M512 {
	return M512(floorPs([16]float32(a)))
}

func floorPs(a [16]float32) [16]float32


// MaskFloorPs: Round the packed single-precision (32-bit) floating-point
// elements in 'a' down to an integer value, and store the results as packed
// single-precision floating-point elements in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := FLOOR(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_floor_ps'.
// Requires AVX512F.
func MaskFloorPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskFloorPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskFloorPs(src [16]float32, k uint16, a [16]float32) [16]float32


// FmaddEpi32: Multiply packed 32-bit integer elements in 'a' and 'b', add the
// intermediate result to packed elements in 'c' and store the results in
// 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMADD231D'. Intrinsic: '_mm512_fmadd_epi32'.
// Requires KNCNI.
func FmaddEpi32(a M512i, b M512i, c M512i) M512i {
	return M512i(fmaddEpi32([64]byte(a), [64]byte(b), [64]byte(c)))
}

func fmaddEpi32(a [64]byte, b [64]byte, c [64]byte) [64]byte


// MaskFmaddEpi32: Multiply packed 32-bit integer elements in 'a' and 'b', add
// the intermediate result to packed elements in 'c' and store the results in
// 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMADD231D'. Intrinsic: '_mm512_mask_fmadd_epi32'.
// Requires KNCNI.
func MaskFmaddEpi32(a M512i, k Mmask16, b M512i, c M512i) M512i {
	return M512i(maskFmaddEpi32([64]byte(a), uint16(k), [64]byte(b), [64]byte(c)))
}

func maskFmaddEpi32(a [64]byte, k uint16, b [64]byte, c [64]byte) [64]byte


// Mask3FmaddEpi32: Multiply packed 32-bit integer elements in 'a' and 'b', add
// the intermediate result to packed elements in 'c' and store the results in
// 'dst' using writemask 'k' (elements are copied from 'c' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMADD231D'. Intrinsic: '_mm512_mask3_fmadd_epi32'.
// Requires KNCNI.
func Mask3FmaddEpi32(a M512i, b M512i, c M512i, k Mmask16) M512i {
	return M512i(mask3FmaddEpi32([64]byte(a), [64]byte(b), [64]byte(c), uint16(k)))
}

func mask3FmaddEpi32(a [64]byte, b [64]byte, c [64]byte, k uint16) [64]byte


// FmaddPd: Multiply packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', add the intermediate result to packed elements in 'c', and
// store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADD132PD, VFMADD213PD, VFMADD231PD'. Intrinsic: '_mm512_fmadd_pd'.
// Requires KNCNI.
func FmaddPd(a M512d, b M512d, c M512d) M512d {
	return M512d(fmaddPd([8]float64(a), [8]float64(b), [8]float64(c)))
}

func fmaddPd(a [8]float64, b [8]float64, c [8]float64) [8]float64


// MaskFmaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADD132PD, VFMADD213PD, VFMADD231PD'. Intrinsic: '_mm512_mask_fmadd_pd'.
// Requires KNCNI.
func MaskFmaddPd(a M512d, k Mmask8, b M512d, c M512d) M512d {
	return M512d(maskFmaddPd([8]float64(a), uint8(k), [8]float64(b), [8]float64(c)))
}

func maskFmaddPd(a [8]float64, k uint8, b [8]float64, c [8]float64) [8]float64


// Mask3FmaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADD132PD, VFMADD213PD, VFMADD231PD'. Intrinsic: '_mm512_mask3_fmadd_pd'.
// Requires KNCNI.
func Mask3FmaddPd(a M512d, b M512d, c M512d, k Mmask8) M512d {
	return M512d(mask3FmaddPd([8]float64(a), [8]float64(b), [8]float64(c), uint8(k)))
}

func mask3FmaddPd(a [8]float64, b [8]float64, c [8]float64, k uint8) [8]float64


// MaskzFmaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADD132PD, VFMADD213PD, VFMADD231PD'. Intrinsic: '_mm512_maskz_fmadd_pd'.
// Requires AVX512F.
func MaskzFmaddPd(k Mmask8, a M512d, b M512d, c M512d) M512d {
	return M512d(maskzFmaddPd(uint8(k), [8]float64(a), [8]float64(b), [8]float64(c)))
}

func maskzFmaddPd(k uint8, a [8]float64, b [8]float64, c [8]float64) [8]float64


// FmaddPs: Multiply packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', add the intermediate result to packed elements in 'c', and
// store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADD132PS, VFMADD213PS, VFMADD231PS'. Intrinsic: '_mm512_fmadd_ps'.
// Requires KNCNI.
func FmaddPs(a M512, b M512, c M512) M512 {
	return M512(fmaddPs([16]float32(a), [16]float32(b), [16]float32(c)))
}

func fmaddPs(a [16]float32, b [16]float32, c [16]float32) [16]float32


// MaskFmaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADD132PS, VFMADD213PS, VFMADD231PS'. Intrinsic: '_mm512_mask_fmadd_ps'.
// Requires KNCNI.
func MaskFmaddPs(a M512, k Mmask16, b M512, c M512) M512 {
	return M512(maskFmaddPs([16]float32(a), uint16(k), [16]float32(b), [16]float32(c)))
}

func maskFmaddPs(a [16]float32, k uint16, b [16]float32, c [16]float32) [16]float32


// Mask3FmaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADD132PS, VFMADD213PS, VFMADD231PS'. Intrinsic: '_mm512_mask3_fmadd_ps'.
// Requires KNCNI.
func Mask3FmaddPs(a M512, b M512, c M512, k Mmask16) M512 {
	return M512(mask3FmaddPs([16]float32(a), [16]float32(b), [16]float32(c), uint16(k)))
}

func mask3FmaddPs(a [16]float32, b [16]float32, c [16]float32, k uint16) [16]float32


// MaskzFmaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADD132PS, VFMADD213PS, VFMADD231PS'. Intrinsic: '_mm512_maskz_fmadd_ps'.
// Requires AVX512F.
func MaskzFmaddPs(k Mmask16, a M512, b M512, c M512) M512 {
	return M512(maskzFmaddPs(uint16(k), [16]float32(a), [16]float32(b), [16]float32(c)))
}

func maskzFmaddPs(k uint16, a [16]float32, b [16]float32, c [16]float32) [16]float32


// FmaddRoundPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADD132PD, VFMADD213PD, VFMADD231PD'. Intrinsic: '_mm512_fmadd_round_pd'.
// Requires KNCNI.
func FmaddRoundPd(a M512d, b M512d, c M512d, rounding int) M512d {
	return M512d(fmaddRoundPd([8]float64(a), [8]float64(b), [8]float64(c), rounding))
}

func fmaddRoundPd(a [8]float64, b [8]float64, c [8]float64, rounding int) [8]float64


// MaskFmaddRoundPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'a' when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADD132PD, VFMADD213PD, VFMADD231PD'. Intrinsic: '_mm512_mask_fmadd_round_pd'.
// Requires KNCNI.
func MaskFmaddRoundPd(a M512d, k Mmask8, b M512d, c M512d, rounding int) M512d {
	return M512d(maskFmaddRoundPd([8]float64(a), uint8(k), [8]float64(b), [8]float64(c), rounding))
}

func maskFmaddRoundPd(a [8]float64, k uint8, b [8]float64, c [8]float64, rounding int) [8]float64


// Mask3FmaddRoundPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'c' when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE 
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADD132PD, VFMADD213PD, VFMADD231PD'. Intrinsic: '_mm512_mask3_fmadd_round_pd'.
// Requires KNCNI.
func Mask3FmaddRoundPd(a M512d, b M512d, c M512d, k Mmask8, rounding int) M512d {
	return M512d(mask3FmaddRoundPd([8]float64(a), [8]float64(b), [8]float64(c), uint8(k), rounding))
}

func mask3FmaddRoundPd(a [8]float64, b [8]float64, c [8]float64, k uint8, rounding int) [8]float64


// MaskzFmaddRoundPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADD132PD, VFMADD213PD, VFMADD231PD'. Intrinsic: '_mm512_maskz_fmadd_round_pd'.
// Requires AVX512F.
func MaskzFmaddRoundPd(k Mmask8, a M512d, b M512d, c M512d, rounding int) M512d {
	return M512d(maskzFmaddRoundPd(uint8(k), [8]float64(a), [8]float64(b), [8]float64(c), rounding))
}

func maskzFmaddRoundPd(k uint8, a [8]float64, b [8]float64, c [8]float64, rounding int) [8]float64


// FmaddRoundPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADD132PS, VFMADD213PS, VFMADD231PS'. Intrinsic: '_mm512_fmadd_round_ps'.
// Requires KNCNI.
func FmaddRoundPs(a M512, b M512, c M512, rounding int) M512 {
	return M512(fmaddRoundPs([16]float32(a), [16]float32(b), [16]float32(c), rounding))
}

func fmaddRoundPs(a [16]float32, b [16]float32, c [16]float32, rounding int) [16]float32


// MaskFmaddRoundPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'a' when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADD132PS, VFMADD213PS, VFMADD231PS'. Intrinsic: '_mm512_mask_fmadd_round_ps'.
// Requires KNCNI.
func MaskFmaddRoundPs(a M512, k Mmask16, b M512, c M512, rounding int) M512 {
	return M512(maskFmaddRoundPs([16]float32(a), uint16(k), [16]float32(b), [16]float32(c), rounding))
}

func maskFmaddRoundPs(a [16]float32, k uint16, b [16]float32, c [16]float32, rounding int) [16]float32


// Mask3FmaddRoundPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'c' when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADD132PS, VFMADD213PS, VFMADD231PS'. Intrinsic: '_mm512_mask3_fmadd_round_ps'.
// Requires KNCNI.
func Mask3FmaddRoundPs(a M512, b M512, c M512, k Mmask16, rounding int) M512 {
	return M512(mask3FmaddRoundPs([16]float32(a), [16]float32(b), [16]float32(c), uint16(k), rounding))
}

func mask3FmaddRoundPs(a [16]float32, b [16]float32, c [16]float32, k uint16, rounding int) [16]float32


// MaskzFmaddRoundPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'a' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				a[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADD132PS, VFMADD213PS, VFMADD231PS'. Intrinsic: '_mm512_maskz_fmadd_round_ps'.
// Requires AVX512F.
func MaskzFmaddRoundPs(k Mmask16, a M512, b M512, c M512, rounding int) M512 {
	return M512(maskzFmaddRoundPs(uint16(k), [16]float32(a), [16]float32(b), [16]float32(c), rounding))
}

func maskzFmaddRoundPs(k uint16, a [16]float32, b [16]float32, c [16]float32, rounding int) [16]float32


// Fmadd233Epi32: Multiply packed 32-bit integer elements in each 4-element set
// of 'a' and by element 1 of the corresponding 4-element set from 'b', add the
// intermediate result to element 0 of the corresponding 4-element set from
// 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			base := (j & ~0x3) * 32
//			scale[31:0] := b[base+63:base+32]
//			bias[31:0]  := b[base+31:base]
//			dst[i+31:i] := (a[i+31:i] * scale[31:0]) + bias[31:0]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMADD233D'. Intrinsic: '_mm512_fmadd233_epi32'.
// Requires KNCNI.
func Fmadd233Epi32(a M512i, b M512i) M512i {
	return M512i(fmadd233Epi32([64]byte(a), [64]byte(b)))
}

func fmadd233Epi32(a [64]byte, b [64]byte) [64]byte


// MaskFmadd233Epi32: Multiply packed 32-bit integer elements in each 4-element
// set of 'a' and by element 1 of the corresponding 4-element set from 'b', add
// the intermediate result to element 0 of the corresponding 4-element set from
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				base := (j & ~0x3) * 32
//				scale[31:0] := b[base+63:base+32]
//				bias[31:0]  := b[base+31:base]
//				dst[i+31:i] := (a[i+31:i] * scale[31:0]) + bias[31:0]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMADD233D'. Intrinsic: '_mm512_mask_fmadd233_epi32'.
// Requires KNCNI.
func MaskFmadd233Epi32(src M512i, k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskFmadd233Epi32([64]byte(src), uint16(k), [64]byte(a), [64]byte(b)))
}

func maskFmadd233Epi32(src [64]byte, k uint16, a [64]byte, b [64]byte) [64]byte


// Fmadd233Ps: Performs multiplication between single-precision (32-bit)
// floating-point elements in 'a' and 'b' and adds the result to the elements
// in 'b', storing the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + b[i+31:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADD233PS'. Intrinsic: '_mm512_fmadd233_ps'.
// Requires KNCNI.
func Fmadd233Ps(a M512, b M512) M512 {
	return M512(fmadd233Ps([16]float32(a), [16]float32(b)))
}

func fmadd233Ps(a [16]float32, b [16]float32) [16]float32


// MaskFmadd233Ps: Performs multiplication between single-precision (32-bit)
// floating-point elements in 'a' and 'b' and adds the result to the elements
// in 'b', storing the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADD233PS'. Intrinsic: '_mm512_mask_fmadd233_ps'.
// Requires KNCNI.
func MaskFmadd233Ps(src M512, k Mmask16, a M512, b M512) M512 {
	return M512(maskFmadd233Ps([16]float32(src), uint16(k), [16]float32(a), [16]float32(b)))
}

func maskFmadd233Ps(src [16]float32, k uint16, a [16]float32, b [16]float32) [16]float32


// Fmadd233RoundPs: Multiply packed single-precision (32-bit) floating-point
// elements in each 4-element set of 'a' and by element 1 of the corresponding
// 4-element set from 'b', add the intermediate result to element 0 of the
// corresponding 4-element set from 'b', and store the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			base := (j & ~0x3) * 32
//			scale[31:0] := b[base+63:base+32]
//			bias[31:0]  := b[base+31:base]
//			dst[i+31:i] := (a[i+31:i] * scale[31:0]) + bias[31:0]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADD233PS'. Intrinsic: '_mm512_fmadd233_round_ps'.
// Requires KNCNI.
func Fmadd233RoundPs(a M512, b M512, rounding int) M512 {
	return M512(fmadd233RoundPs([16]float32(a), [16]float32(b), rounding))
}

func fmadd233RoundPs(a [16]float32, b [16]float32, rounding int) [16]float32


// MaskFmadd233RoundPs: Multiply packed single-precision (32-bit)
// floating-point elements in each 4-element set of 'a' and by element 1 of the
// corresponding 4-element set from 'b', add the intermediate result to element
// 0 of the corresponding 4-element set from 'b', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				base := (j & ~0x3) * 32
//				scale[31:0] := b[base+63:base+32]
//				bias[31:0]  := b[base+31:base]
//				dst[i+31:i] := (a[i+31:i] * scale[31:0]) + bias[31:0]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADD233PS'. Intrinsic: '_mm512_mask_fmadd233_round_ps'.
// Requires KNCNI.
func MaskFmadd233RoundPs(src M512, k Mmask16, a M512, b M512, rounding int) M512 {
	return M512(maskFmadd233RoundPs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b), rounding))
}

func maskFmadd233RoundPs(src [16]float32, k uint16, a [16]float32, b [16]float32, rounding int) [16]float32


// FmaddsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF (j is even) 
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm512_fmaddsub_pd'.
// Requires AVX512F.
func FmaddsubPd(a M512d, b M512d, c M512d) M512d {
	return M512d(fmaddsubPd([8]float64(a), [8]float64(b), [8]float64(c)))
}

func fmaddsubPd(a [8]float64, b [8]float64, c [8]float64) [8]float64


// MaskFmaddsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm512_mask_fmaddsub_pd'.
// Requires AVX512F.
func MaskFmaddsubPd(a M512d, k Mmask8, b M512d, c M512d) M512d {
	return M512d(maskFmaddsubPd([8]float64(a), uint8(k), [8]float64(b), [8]float64(c)))
}

func maskFmaddsubPd(a [8]float64, k uint8, b [8]float64, c [8]float64) [8]float64


// Mask3FmaddsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'c' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm512_mask3_fmaddsub_pd'.
// Requires AVX512F.
func Mask3FmaddsubPd(a M512d, b M512d, c M512d, k Mmask8) M512d {
	return M512d(mask3FmaddsubPd([8]float64(a), [8]float64(b), [8]float64(c), uint8(k)))
}

func mask3FmaddsubPd(a [8]float64, b [8]float64, c [8]float64, k uint8) [8]float64


// MaskzFmaddsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm512_maskz_fmaddsub_pd'.
// Requires AVX512F.
func MaskzFmaddsubPd(k Mmask8, a M512d, b M512d, c M512d) M512d {
	return M512d(maskzFmaddsubPd(uint8(k), [8]float64(a), [8]float64(b), [8]float64(c)))
}

func maskzFmaddsubPd(k uint8, a [8]float64, b [8]float64, c [8]float64) [8]float64


// FmaddsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF (j is even) 
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm512_fmaddsub_ps'.
// Requires AVX512F.
func FmaddsubPs(a M512, b M512, c M512) M512 {
	return M512(fmaddsubPs([16]float32(a), [16]float32(b), [16]float32(c)))
}

func fmaddsubPs(a [16]float32, b [16]float32, c [16]float32) [16]float32


// MaskFmaddsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm512_mask_fmaddsub_ps'.
// Requires AVX512F.
func MaskFmaddsubPs(a M512, k Mmask16, b M512, c M512) M512 {
	return M512(maskFmaddsubPs([16]float32(a), uint16(k), [16]float32(b), [16]float32(c)))
}

func maskFmaddsubPs(a [16]float32, k uint16, b [16]float32, c [16]float32) [16]float32


// Mask3FmaddsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'c' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm512_mask3_fmaddsub_ps'.
// Requires AVX512F.
func Mask3FmaddsubPs(a M512, b M512, c M512, k Mmask16) M512 {
	return M512(mask3FmaddsubPs([16]float32(a), [16]float32(b), [16]float32(c), uint16(k)))
}

func mask3FmaddsubPs(a [16]float32, b [16]float32, c [16]float32, k uint16) [16]float32


// MaskzFmaddsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm512_maskz_fmaddsub_ps'.
// Requires AVX512F.
func MaskzFmaddsubPs(k Mmask16, a M512, b M512, c M512) M512 {
	return M512(maskzFmaddsubPs(uint16(k), [16]float32(a), [16]float32(b), [16]float32(c)))
}

func maskzFmaddsubPs(k uint16, a [16]float32, b [16]float32, c [16]float32) [16]float32


// FmaddsubRoundPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF (j is even) 
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm512_fmaddsub_round_pd'.
// Requires AVX512F.
func FmaddsubRoundPd(a M512d, b M512d, c M512d, rounding int) M512d {
	return M512d(fmaddsubRoundPd([8]float64(a), [8]float64(b), [8]float64(c), rounding))
}

func fmaddsubRoundPd(a [8]float64, b [8]float64, c [8]float64, rounding int) [8]float64


// MaskFmaddsubRoundPd: Multiply packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', alternatively add and subtract
// packed elements in 'c' to/from the intermediate result, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). Rounding is done according to the
// 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm512_mask_fmaddsub_round_pd'.
// Requires AVX512F.
func MaskFmaddsubRoundPd(a M512d, k Mmask8, b M512d, c M512d, rounding int) M512d {
	return M512d(maskFmaddsubRoundPd([8]float64(a), uint8(k), [8]float64(b), [8]float64(c), rounding))
}

func maskFmaddsubRoundPd(a [8]float64, k uint8, b [8]float64, c [8]float64, rounding int) [8]float64


// Mask3FmaddsubRoundPd: Multiply packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', alternatively add and subtract
// packed elements in 'c' to/from the intermediate result, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'c' when the
// corresponding mask bit is not set).  Rounding is done according to the
// 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				FI
//			ELSE 
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm512_mask3_fmaddsub_round_pd'.
// Requires AVX512F.
func Mask3FmaddsubRoundPd(a M512d, b M512d, c M512d, k Mmask8, rounding int) M512d {
	return M512d(mask3FmaddsubRoundPd([8]float64(a), [8]float64(b), [8]float64(c), uint8(k), rounding))
}

func mask3FmaddsubRoundPd(a [8]float64, b [8]float64, c [8]float64, k uint8, rounding int) [8]float64


// MaskzFmaddsubRoundPd: Multiply packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', alternatively add and subtract
// packed elements in 'c' to/from the intermediate result, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm512_maskz_fmaddsub_round_pd'.
// Requires AVX512F.
func MaskzFmaddsubRoundPd(k Mmask8, a M512d, b M512d, c M512d, rounding int) M512d {
	return M512d(maskzFmaddsubRoundPd(uint8(k), [8]float64(a), [8]float64(b), [8]float64(c), rounding))
}

func maskzFmaddsubRoundPd(k uint8, a [8]float64, b [8]float64, c [8]float64, rounding int) [8]float64


// FmaddsubRoundPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF (j is even) 
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm512_fmaddsub_round_ps'.
// Requires AVX512F.
func FmaddsubRoundPs(a M512, b M512, c M512, rounding int) M512 {
	return M512(fmaddsubRoundPs([16]float32(a), [16]float32(b), [16]float32(c), rounding))
}

func fmaddsubRoundPs(a [16]float32, b [16]float32, c [16]float32, rounding int) [16]float32


// MaskFmaddsubRoundPs: Multiply packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', alternatively add and subtract
// packed elements in 'c' to/from the intermediate result, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). Rounding is done according to the
// 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm512_mask_fmaddsub_round_ps'.
// Requires AVX512F.
func MaskFmaddsubRoundPs(a M512, k Mmask16, b M512, c M512, rounding int) M512 {
	return M512(maskFmaddsubRoundPs([16]float32(a), uint16(k), [16]float32(b), [16]float32(c), rounding))
}

func maskFmaddsubRoundPs(a [16]float32, k uint16, b [16]float32, c [16]float32, rounding int) [16]float32


// Mask3FmaddsubRoundPs: Multiply packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', alternatively add and subtract
// packed elements in 'c' to/from the intermediate result, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'c' when the
// corresponding mask bit is not set).  Rounding is done according to the
// 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm512_mask3_fmaddsub_round_ps'.
// Requires AVX512F.
func Mask3FmaddsubRoundPs(a M512, b M512, c M512, k Mmask16, rounding int) M512 {
	return M512(mask3FmaddsubRoundPs([16]float32(a), [16]float32(b), [16]float32(c), uint16(k), rounding))
}

func mask3FmaddsubRoundPs(a [16]float32, b [16]float32, c [16]float32, k uint16, rounding int) [16]float32


// MaskzFmaddsubRoundPs: Multiply packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', alternatively add and subtract
// packed elements in 'c' to/from the intermediate result, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm512_maskz_fmaddsub_round_ps'.
// Requires AVX512F.
func MaskzFmaddsubRoundPs(k Mmask16, a M512, b M512, c M512, rounding int) M512 {
	return M512(maskzFmaddsubRoundPs(uint16(k), [16]float32(a), [16]float32(b), [16]float32(c), rounding))
}

func maskzFmaddsubRoundPs(k uint16, a [16]float32, b [16]float32, c [16]float32, rounding int) [16]float32


// FmsubPd: Multiply packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', subtract packed elements in 'c' from the intermediate
// result, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUB132PD, VFMSUB213PD, VFMSUB231PD'. Intrinsic: '_mm512_fmsub_pd'.
// Requires KNCNI.
func FmsubPd(a M512d, b M512d, c M512d) M512d {
	return M512d(fmsubPd([8]float64(a), [8]float64(b), [8]float64(c)))
}

func fmsubPd(a [8]float64, b [8]float64, c [8]float64) [8]float64


// MaskFmsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUB132PD, VFMSUB213PD, VFMSUB231PD'. Intrinsic: '_mm512_mask_fmsub_pd'.
// Requires KNCNI.
func MaskFmsubPd(a M512d, k Mmask8, b M512d, c M512d) M512d {
	return M512d(maskFmsubPd([8]float64(a), uint8(k), [8]float64(b), [8]float64(c)))
}

func maskFmsubPd(a [8]float64, k uint8, b [8]float64, c [8]float64) [8]float64


// Mask3FmsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUB132PD, VFMSUB213PD, VFMSUB231PD'. Intrinsic: '_mm512_mask3_fmsub_pd'.
// Requires KNCNI.
func Mask3FmsubPd(a M512d, b M512d, c M512d, k Mmask8) M512d {
	return M512d(mask3FmsubPd([8]float64(a), [8]float64(b), [8]float64(c), uint8(k)))
}

func mask3FmsubPd(a [8]float64, b [8]float64, c [8]float64, k uint8) [8]float64


// MaskzFmsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUB132PD, VFMSUB213PD, VFMSUB231PD'. Intrinsic: '_mm512_maskz_fmsub_pd'.
// Requires AVX512F.
func MaskzFmsubPd(k Mmask8, a M512d, b M512d, c M512d) M512d {
	return M512d(maskzFmsubPd(uint8(k), [8]float64(a), [8]float64(b), [8]float64(c)))
}

func maskzFmsubPd(k uint8, a [8]float64, b [8]float64, c [8]float64) [8]float64


// FmsubPs: Multiply packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', subtract packed elements in 'c' from the intermediate
// result, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUB132PS, VFMSUB213PS, VFMSUB231PS'. Intrinsic: '_mm512_fmsub_ps'.
// Requires KNCNI.
func FmsubPs(a M512, b M512, c M512) M512 {
	return M512(fmsubPs([16]float32(a), [16]float32(b), [16]float32(c)))
}

func fmsubPs(a [16]float32, b [16]float32, c [16]float32) [16]float32


// MaskFmsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUB132PS, VFMSUB213PS, VFMSUB231PS'. Intrinsic: '_mm512_mask_fmsub_ps'.
// Requires KNCNI.
func MaskFmsubPs(a M512, k Mmask16, b M512, c M512) M512 {
	return M512(maskFmsubPs([16]float32(a), uint16(k), [16]float32(b), [16]float32(c)))
}

func maskFmsubPs(a [16]float32, k uint16, b [16]float32, c [16]float32) [16]float32


// Mask3FmsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUB132PS, VFMSUB213PS, VFMSUB231PS'. Intrinsic: '_mm512_mask3_fmsub_ps'.
// Requires KNCNI.
func Mask3FmsubPs(a M512, b M512, c M512, k Mmask16) M512 {
	return M512(mask3FmsubPs([16]float32(a), [16]float32(b), [16]float32(c), uint16(k)))
}

func mask3FmsubPs(a [16]float32, b [16]float32, c [16]float32, k uint16) [16]float32


// MaskzFmsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUB132PS, VFMSUB213PS, VFMSUB231PS'. Intrinsic: '_mm512_maskz_fmsub_ps'.
// Requires AVX512F.
func MaskzFmsubPs(k Mmask16, a M512, b M512, c M512) M512 {
	return M512(maskzFmsubPs(uint16(k), [16]float32(a), [16]float32(b), [16]float32(c)))
}

func maskzFmsubPs(k uint16, a [16]float32, b [16]float32, c [16]float32) [16]float32


// FmsubRoundPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUB132PD, VFMSUB213PD, VFMSUB231PD'. Intrinsic: '_mm512_fmsub_round_pd'.
// Requires KNCNI.
func FmsubRoundPd(a M512d, b M512d, c M512d, rounding int) M512d {
	return M512d(fmsubRoundPd([8]float64(a), [8]float64(b), [8]float64(c), rounding))
}

func fmsubRoundPd(a [8]float64, b [8]float64, c [8]float64, rounding int) [8]float64


// MaskFmsubRoundPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set).
// Rounding is done according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUB132PD, VFMSUB213PD, VFMSUB231PD'. Intrinsic: '_mm512_mask_fmsub_round_pd'.
// Requires KNCNI.
func MaskFmsubRoundPd(a M512d, k Mmask8, b M512d, c M512d, rounding int) M512d {
	return M512d(maskFmsubRoundPd([8]float64(a), uint8(k), [8]float64(b), [8]float64(c), rounding))
}

func maskFmsubRoundPd(a [8]float64, k uint8, b [8]float64, c [8]float64, rounding int) [8]float64


// Mask3FmsubRoundPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set).
// Rounding is done according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUB132PD, VFMSUB213PD, VFMSUB231PD'. Intrinsic: '_mm512_mask3_fmsub_round_pd'.
// Requires KNCNI.
func Mask3FmsubRoundPd(a M512d, b M512d, c M512d, k Mmask8, rounding int) M512d {
	return M512d(mask3FmsubRoundPd([8]float64(a), [8]float64(b), [8]float64(c), uint8(k), rounding))
}

func mask3FmsubRoundPd(a [8]float64, b [8]float64, c [8]float64, k uint8, rounding int) [8]float64


// MaskzFmsubRoundPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUB132PD, VFMSUB213PD, VFMSUB231PD'. Intrinsic: '_mm512_maskz_fmsub_round_pd'.
// Requires AVX512F.
func MaskzFmsubRoundPd(k Mmask8, a M512d, b M512d, c M512d, rounding int) M512d {
	return M512d(maskzFmsubRoundPd(uint8(k), [8]float64(a), [8]float64(b), [8]float64(c), rounding))
}

func maskzFmsubRoundPd(k uint8, a [8]float64, b [8]float64, c [8]float64, rounding int) [8]float64


// FmsubRoundPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUB132PS, VFMSUB213PS, VFMSUB231PS'. Intrinsic: '_mm512_fmsub_round_ps'.
// Requires KNCNI.
func FmsubRoundPs(a M512, b M512, c M512, rounding int) M512 {
	return M512(fmsubRoundPs([16]float32(a), [16]float32(b), [16]float32(c), rounding))
}

func fmsubRoundPs(a [16]float32, b [16]float32, c [16]float32, rounding int) [16]float32


// MaskFmsubRoundPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set).
// Rounding is done according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUB132PS, VFMSUB213PS, VFMSUB231PS'. Intrinsic: '_mm512_mask_fmsub_round_ps'.
// Requires KNCNI.
func MaskFmsubRoundPs(a M512, k Mmask16, b M512, c M512, rounding int) M512 {
	return M512(maskFmsubRoundPs([16]float32(a), uint16(k), [16]float32(b), [16]float32(c), rounding))
}

func maskFmsubRoundPs(a [16]float32, k uint16, b [16]float32, c [16]float32, rounding int) [16]float32


// Mask3FmsubRoundPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set).
// Rounding is done according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUB132PS, VFMSUB213PS, VFMSUB231PS'. Intrinsic: '_mm512_mask3_fmsub_round_ps'.
// Requires KNCNI.
func Mask3FmsubRoundPs(a M512, b M512, c M512, k Mmask16, rounding int) M512 {
	return M512(mask3FmsubRoundPs([16]float32(a), [16]float32(b), [16]float32(c), uint16(k), rounding))
}

func mask3FmsubRoundPs(a [16]float32, b [16]float32, c [16]float32, k uint16, rounding int) [16]float32


// MaskzFmsubRoundPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUB132PS, VFMSUB213PS, VFMSUB231PS'. Intrinsic: '_mm512_maskz_fmsub_round_ps'.
// Requires AVX512F.
func MaskzFmsubRoundPs(k Mmask16, a M512, b M512, c M512, rounding int) M512 {
	return M512(maskzFmsubRoundPs(uint16(k), [16]float32(a), [16]float32(b), [16]float32(c), rounding))
}

func maskzFmsubRoundPs(k uint16, a [16]float32, b [16]float32, c [16]float32, rounding int) [16]float32


// FmsubaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF (j is even) 
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm512_fmsubadd_pd'.
// Requires AVX512F.
func FmsubaddPd(a M512d, b M512d, c M512d) M512d {
	return M512d(fmsubaddPd([8]float64(a), [8]float64(b), [8]float64(c)))
}

func fmsubaddPd(a [8]float64, b [8]float64, c [8]float64) [8]float64


// MaskFmsubaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm512_mask_fmsubadd_pd'.
// Requires AVX512F.
func MaskFmsubaddPd(a M512d, k Mmask8, b M512d, c M512d) M512d {
	return M512d(maskFmsubaddPd([8]float64(a), uint8(k), [8]float64(b), [8]float64(c)))
}

func maskFmsubaddPd(a [8]float64, k uint8, b [8]float64, c [8]float64) [8]float64


// Mask3FmsubaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'c' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm512_mask3_fmsubadd_pd'.
// Requires AVX512F.
func Mask3FmsubaddPd(a M512d, b M512d, c M512d, k Mmask8) M512d {
	return M512d(mask3FmsubaddPd([8]float64(a), [8]float64(b), [8]float64(c), uint8(k)))
}

func mask3FmsubaddPd(a [8]float64, b [8]float64, c [8]float64, k uint8) [8]float64


// MaskzFmsubaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm512_maskz_fmsubadd_pd'.
// Requires AVX512F.
func MaskzFmsubaddPd(k Mmask8, a M512d, b M512d, c M512d) M512d {
	return M512d(maskzFmsubaddPd(uint8(k), [8]float64(a), [8]float64(b), [8]float64(c)))
}

func maskzFmsubaddPd(k uint8, a [8]float64, b [8]float64, c [8]float64) [8]float64


// FmsubaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF (j is even) 
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm512_fmsubadd_ps'.
// Requires AVX512F.
func FmsubaddPs(a M512, b M512, c M512) M512 {
	return M512(fmsubaddPs([16]float32(a), [16]float32(b), [16]float32(c)))
}

func fmsubaddPs(a [16]float32, b [16]float32, c [16]float32) [16]float32


// MaskFmsubaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm512_mask_fmsubadd_ps'.
// Requires AVX512F.
func MaskFmsubaddPs(a M512, k Mmask16, b M512, c M512) M512 {
	return M512(maskFmsubaddPs([16]float32(a), uint16(k), [16]float32(b), [16]float32(c)))
}

func maskFmsubaddPs(a [16]float32, k uint16, b [16]float32, c [16]float32) [16]float32


// Mask3FmsubaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'c' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm512_mask3_fmsubadd_ps'.
// Requires AVX512F.
func Mask3FmsubaddPs(a M512, b M512, c M512, k Mmask16) M512 {
	return M512(mask3FmsubaddPs([16]float32(a), [16]float32(b), [16]float32(c), uint16(k)))
}

func mask3FmsubaddPs(a [16]float32, b [16]float32, c [16]float32, k uint16) [16]float32


// MaskzFmsubaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm512_maskz_fmsubadd_ps'.
// Requires AVX512F.
func MaskzFmsubaddPs(k Mmask16, a M512, b M512, c M512) M512 {
	return M512(maskzFmsubaddPs(uint16(k), [16]float32(a), [16]float32(b), [16]float32(c)))
}

func maskzFmsubaddPs(k uint16, a [16]float32, b [16]float32, c [16]float32) [16]float32


// FmsubaddRoundPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF (j is even) 
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm512_fmsubadd_round_pd'.
// Requires AVX512F.
func FmsubaddRoundPd(a M512d, b M512d, c M512d, rounding int) M512d {
	return M512d(fmsubaddRoundPd([8]float64(a), [8]float64(b), [8]float64(c), rounding))
}

func fmsubaddRoundPd(a [8]float64, b [8]float64, c [8]float64, rounding int) [8]float64


// MaskFmsubaddRoundPd: Multiply packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', alternatively subtract and add
// packed elements in 'c' from/to the intermediate result, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). Rounding is done according to the
// 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm512_mask_fmsubadd_round_pd'.
// Requires AVX512F.
func MaskFmsubaddRoundPd(a M512d, k Mmask8, b M512d, c M512d, rounding int) M512d {
	return M512d(maskFmsubaddRoundPd([8]float64(a), uint8(k), [8]float64(b), [8]float64(c), rounding))
}

func maskFmsubaddRoundPd(a [8]float64, k uint8, b [8]float64, c [8]float64, rounding int) [8]float64


// Mask3FmsubaddRoundPd: Multiply packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', alternatively subtract and add
// packed elements in 'c' from/to the intermediate result, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'c' when the
// corresponding mask bit is not set).  Rounding is done according to the
// 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm512_mask3_fmsubadd_round_pd'.
// Requires AVX512F.
func Mask3FmsubaddRoundPd(a M512d, b M512d, c M512d, k Mmask8, rounding int) M512d {
	return M512d(mask3FmsubaddRoundPd([8]float64(a), [8]float64(b), [8]float64(c), uint8(k), rounding))
}

func mask3FmsubaddRoundPd(a [8]float64, b [8]float64, c [8]float64, k uint8, rounding int) [8]float64


// MaskzFmsubaddRoundPd: Multiply packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', alternatively subtract and add
// packed elements in 'c' from/to the intermediate result, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm512_maskz_fmsubadd_round_pd'.
// Requires AVX512F.
func MaskzFmsubaddRoundPd(k Mmask8, a M512d, b M512d, c M512d, rounding int) M512d {
	return M512d(maskzFmsubaddRoundPd(uint8(k), [8]float64(a), [8]float64(b), [8]float64(c), rounding))
}

func maskzFmsubaddRoundPd(k uint8, a [8]float64, b [8]float64, c [8]float64, rounding int) [8]float64


// FmsubaddRoundPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF (j is even) 
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm512_fmsubadd_round_ps'.
// Requires AVX512F.
func FmsubaddRoundPs(a M512, b M512, c M512, rounding int) M512 {
	return M512(fmsubaddRoundPs([16]float32(a), [16]float32(b), [16]float32(c), rounding))
}

func fmsubaddRoundPs(a [16]float32, b [16]float32, c [16]float32, rounding int) [16]float32


// MaskFmsubaddRoundPs: Multiply packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', alternatively subtract and add
// packed elements in 'c' from/to the intermediate result, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm512_mask_fmsubadd_round_ps'.
// Requires AVX512F.
func MaskFmsubaddRoundPs(a M512, k Mmask16, b M512, c M512, rounding int) M512 {
	return M512(maskFmsubaddRoundPs([16]float32(a), uint16(k), [16]float32(b), [16]float32(c), rounding))
}

func maskFmsubaddRoundPs(a [16]float32, k uint16, b [16]float32, c [16]float32, rounding int) [16]float32


// Mask3FmsubaddRoundPs: Multiply packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', alternatively subtract and add
// packed elements in 'c' from/to the intermediate result, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'c' when the
// corresponding mask bit is not set).  Rounding is done according to the
// 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm512_mask3_fmsubadd_round_ps'.
// Requires AVX512F.
func Mask3FmsubaddRoundPs(a M512, b M512, c M512, k Mmask16, rounding int) M512 {
	return M512(mask3FmsubaddRoundPs([16]float32(a), [16]float32(b), [16]float32(c), uint16(k), rounding))
}

func mask3FmsubaddRoundPs(a [16]float32, b [16]float32, c [16]float32, k uint16, rounding int) [16]float32


// MaskzFmsubaddRoundPs: Multiply packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', alternatively subtract and add
// packed elements in 'c' from/to the intermediate result, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm512_maskz_fmsubadd_round_ps'.
// Requires AVX512F.
func MaskzFmsubaddRoundPs(k Mmask16, a M512, b M512, c M512, rounding int) M512 {
	return M512(maskzFmsubaddRoundPs(uint16(k), [16]float32(a), [16]float32(b), [16]float32(c), rounding))
}

func maskzFmsubaddRoundPs(k uint16, a [16]float32, b [16]float32, c [16]float32, rounding int) [16]float32


// FnmaddPd: Multiply packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', add the negated intermediate result to packed elements in
// 'c', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMADD132PD, VFNMADD213PD, VFNMADD231PD'. Intrinsic: '_mm512_fnmadd_pd'.
// Requires KNCNI.
func FnmaddPd(a M512d, b M512d, c M512d) M512d {
	return M512d(fnmaddPd([8]float64(a), [8]float64(b), [8]float64(c)))
}

func fnmaddPd(a [8]float64, b [8]float64, c [8]float64) [8]float64


// MaskFnmaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMADD132PD, VFNMADD213PD, VFNMADD231PD'. Intrinsic: '_mm512_mask_fnmadd_pd'.
// Requires KNCNI.
func MaskFnmaddPd(a M512d, k Mmask8, b M512d, c M512d) M512d {
	return M512d(maskFnmaddPd([8]float64(a), uint8(k), [8]float64(b), [8]float64(c)))
}

func maskFnmaddPd(a [8]float64, k uint8, b [8]float64, c [8]float64) [8]float64


// Mask3FnmaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMADD132PD, VFNMADD213PD, VFNMADD231PD'. Intrinsic: '_mm512_mask3_fnmadd_pd'.
// Requires KNCNI.
func Mask3FnmaddPd(a M512d, b M512d, c M512d, k Mmask8) M512d {
	return M512d(mask3FnmaddPd([8]float64(a), [8]float64(b), [8]float64(c), uint8(k)))
}

func mask3FnmaddPd(a [8]float64, b [8]float64, c [8]float64, k uint8) [8]float64


// MaskzFnmaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMADD132PD, VFNMADD213PD, VFNMADD231PD'. Intrinsic: '_mm512_maskz_fnmadd_pd'.
// Requires AVX512F.
func MaskzFnmaddPd(k Mmask8, a M512d, b M512d, c M512d) M512d {
	return M512d(maskzFnmaddPd(uint8(k), [8]float64(a), [8]float64(b), [8]float64(c)))
}

func maskzFnmaddPd(k uint8, a [8]float64, b [8]float64, c [8]float64) [8]float64


// FnmaddPs: Multiply packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', add the negated intermediate result to packed elements in
// 'c', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			a[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMADD132PS, VFNMADD213PS, VFNMADD231PS'. Intrinsic: '_mm512_fnmadd_ps'.
// Requires KNCNI.
func FnmaddPs(a M512, b M512, c M512) M512 {
	return M512(fnmaddPs([16]float32(a), [16]float32(b), [16]float32(c)))
}

func fnmaddPs(a [16]float32, b [16]float32, c [16]float32) [16]float32


// MaskFnmaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMADD132PS, VFNMADD213PS, VFNMADD231PS'. Intrinsic: '_mm512_mask_fnmadd_ps'.
// Requires KNCNI.
func MaskFnmaddPs(a M512, k Mmask16, b M512, c M512) M512 {
	return M512(maskFnmaddPs([16]float32(a), uint16(k), [16]float32(b), [16]float32(c)))
}

func maskFnmaddPs(a [16]float32, k uint16, b [16]float32, c [16]float32) [16]float32


// Mask3FnmaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMADD132PS, VFNMADD213PS, VFNMADD231PS'. Intrinsic: '_mm512_mask3_fnmadd_ps'.
// Requires KNCNI.
func Mask3FnmaddPs(a M512, b M512, c M512, k Mmask16) M512 {
	return M512(mask3FnmaddPs([16]float32(a), [16]float32(b), [16]float32(c), uint16(k)))
}

func mask3FnmaddPs(a [16]float32, b [16]float32, c [16]float32, k uint16) [16]float32


// MaskzFnmaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMADD132PS, VFNMADD213PS, VFNMADD231PS'. Intrinsic: '_mm512_maskz_fnmadd_ps'.
// Requires AVX512F.
func MaskzFnmaddPs(k Mmask16, a M512, b M512, c M512) M512 {
	return M512(maskzFnmaddPs(uint16(k), [16]float32(a), [16]float32(b), [16]float32(c)))
}

func maskzFnmaddPs(k uint16, a [16]float32, b [16]float32, c [16]float32) [16]float32


// FnmaddRoundPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst'.
// 	 Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMADD132PD, VFNMADD213PD, VFNMADD231PD'. Intrinsic: '_mm512_fnmadd_round_pd'.
// Requires KNCNI.
func FnmaddRoundPd(a M512d, b M512d, c M512d, rounding int) M512d {
	return M512d(fnmaddRoundPd([8]float64(a), [8]float64(b), [8]float64(c), rounding))
}

func fnmaddRoundPd(a [8]float64, b [8]float64, c [8]float64, rounding int) [8]float64


// MaskFnmaddRoundPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set).
// Rounding is done according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMADD132PD, VFNMADD213PD, VFNMADD231PD'. Intrinsic: '_mm512_mask_fnmadd_round_pd'.
// Requires KNCNI.
func MaskFnmaddRoundPd(a M512d, k Mmask8, b M512d, c M512d, rounding int) M512d {
	return M512d(maskFnmaddRoundPd([8]float64(a), uint8(k), [8]float64(b), [8]float64(c), rounding))
}

func maskFnmaddRoundPd(a [8]float64, k uint8, b [8]float64, c [8]float64, rounding int) [8]float64


// Mask3FnmaddRoundPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set).
// Rounding is done according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMADD132PD, VFNMADD213PD, VFNMADD231PD'. Intrinsic: '_mm512_mask3_fnmadd_round_pd'.
// Requires KNCNI.
func Mask3FnmaddRoundPd(a M512d, b M512d, c M512d, k Mmask8, rounding int) M512d {
	return M512d(mask3FnmaddRoundPd([8]float64(a), [8]float64(b), [8]float64(c), uint8(k), rounding))
}

func mask3FnmaddRoundPd(a [8]float64, b [8]float64, c [8]float64, k uint8, rounding int) [8]float64


// MaskzFnmaddRoundPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). Rounding is done
// according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMADD132PD, VFNMADD213PD, VFNMADD231PD'. Intrinsic: '_mm512_maskz_fnmadd_round_pd'.
// Requires AVX512F.
func MaskzFnmaddRoundPd(k Mmask8, a M512d, b M512d, c M512d, rounding int) M512d {
	return M512d(maskzFnmaddRoundPd(uint8(k), [8]float64(a), [8]float64(b), [8]float64(c), rounding))
}

func maskzFnmaddRoundPd(k uint8, a [8]float64, b [8]float64, c [8]float64, rounding int) [8]float64


// FnmaddRoundPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst'.  
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMADD132PS, VFNMADD213PS, VFNMADD231PS'. Intrinsic: '_mm512_fnmadd_round_ps'.
// Requires KNCNI.
func FnmaddRoundPs(a M512, b M512, c M512, rounding int) M512 {
	return M512(fnmaddRoundPs([16]float32(a), [16]float32(b), [16]float32(c), rounding))
}

func fnmaddRoundPs(a [16]float32, b [16]float32, c [16]float32, rounding int) [16]float32


// MaskFnmaddRoundPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set).
// Rounding is done according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMADD132PS, VFNMADD213PS, VFNMADD231PS'. Intrinsic: '_mm512_mask_fnmadd_round_ps'.
// Requires KNCNI.
func MaskFnmaddRoundPs(a M512, k Mmask16, b M512, c M512, rounding int) M512 {
	return M512(maskFnmaddRoundPs([16]float32(a), uint16(k), [16]float32(b), [16]float32(c), rounding))
}

func maskFnmaddRoundPs(a [16]float32, k uint16, b [16]float32, c [16]float32, rounding int) [16]float32


// Mask3FnmaddRoundPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set).
// Rounding is done according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMADD132PS, VFNMADD213PS, VFNMADD231PS'. Intrinsic: '_mm512_mask3_fnmadd_round_ps'.
// Requires KNCNI.
func Mask3FnmaddRoundPs(a M512, b M512, c M512, k Mmask16, rounding int) M512 {
	return M512(mask3FnmaddRoundPs([16]float32(a), [16]float32(b), [16]float32(c), uint16(k), rounding))
}

func mask3FnmaddRoundPs(a [16]float32, b [16]float32, c [16]float32, k uint16, rounding int) [16]float32


// MaskzFnmaddRoundPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). Rounding is done
// according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMADD132PS, VFNMADD213PS, VFNMADD231PS'. Intrinsic: '_mm512_maskz_fnmadd_round_ps'.
// Requires AVX512F.
func MaskzFnmaddRoundPs(k Mmask16, a M512, b M512, c M512, rounding int) M512 {
	return M512(maskzFnmaddRoundPs(uint16(k), [16]float32(a), [16]float32(b), [16]float32(c), rounding))
}

func maskzFnmaddRoundPs(k uint16, a [16]float32, b [16]float32, c [16]float32, rounding int) [16]float32


// FnmsubPd: Multiply packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD'. Intrinsic: '_mm512_fnmsub_pd'.
// Requires KNCNI.
func FnmsubPd(a M512d, b M512d, c M512d) M512d {
	return M512d(fnmsubPd([8]float64(a), [8]float64(b), [8]float64(c)))
}

func fnmsubPd(a [8]float64, b [8]float64, c [8]float64) [8]float64


// MaskFnmsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD'. Intrinsic: '_mm512_mask_fnmsub_pd'.
// Requires KNCNI.
func MaskFnmsubPd(a M512d, k Mmask8, b M512d, c M512d) M512d {
	return M512d(maskFnmsubPd([8]float64(a), uint8(k), [8]float64(b), [8]float64(c)))
}

func maskFnmsubPd(a [8]float64, k uint8, b [8]float64, c [8]float64) [8]float64


// Mask3FnmsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD'. Intrinsic: '_mm512_mask3_fnmsub_pd'.
// Requires KNCNI.
func Mask3FnmsubPd(a M512d, b M512d, c M512d, k Mmask8) M512d {
	return M512d(mask3FnmsubPd([8]float64(a), [8]float64(b), [8]float64(c), uint8(k)))
}

func mask3FnmsubPd(a [8]float64, b [8]float64, c [8]float64, k uint8) [8]float64


// MaskzFnmsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD'. Intrinsic: '_mm512_maskz_fnmsub_pd'.
// Requires AVX512F.
func MaskzFnmsubPd(k Mmask8, a M512d, b M512d, c M512d) M512d {
	return M512d(maskzFnmsubPd(uint8(k), [8]float64(a), [8]float64(b), [8]float64(c)))
}

func maskzFnmsubPd(k uint8, a [8]float64, b [8]float64, c [8]float64) [8]float64


// FnmsubPs: Multiply packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS'. Intrinsic: '_mm512_fnmsub_ps'.
// Requires KNCNI.
func FnmsubPs(a M512, b M512, c M512) M512 {
	return M512(fnmsubPs([16]float32(a), [16]float32(b), [16]float32(c)))
}

func fnmsubPs(a [16]float32, b [16]float32, c [16]float32) [16]float32


// MaskFnmsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS'. Intrinsic: '_mm512_mask_fnmsub_ps'.
// Requires KNCNI.
func MaskFnmsubPs(a M512, k Mmask16, b M512, c M512) M512 {
	return M512(maskFnmsubPs([16]float32(a), uint16(k), [16]float32(b), [16]float32(c)))
}

func maskFnmsubPs(a [16]float32, k uint16, b [16]float32, c [16]float32) [16]float32


// Mask3FnmsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS'. Intrinsic: '_mm512_mask3_fnmsub_ps'.
// Requires KNCNI.
func Mask3FnmsubPs(a M512, b M512, c M512, k Mmask16) M512 {
	return M512(mask3FnmsubPs([16]float32(a), [16]float32(b), [16]float32(c), uint16(k)))
}

func mask3FnmsubPs(a [16]float32, b [16]float32, c [16]float32, k uint16) [16]float32


// MaskzFnmsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS'. Intrinsic: '_mm512_maskz_fnmsub_ps'.
// Requires AVX512F.
func MaskzFnmsubPs(k Mmask16, a M512, b M512, c M512) M512 {
	return M512(maskzFnmsubPs(uint16(k), [16]float32(a), [16]float32(b), [16]float32(c)))
}

func maskzFnmsubPs(k uint16, a [16]float32, b [16]float32, c [16]float32) [16]float32


// FnmsubRoundPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst'.  
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD'. Intrinsic: '_mm512_fnmsub_round_pd'.
// Requires KNCNI.
func FnmsubRoundPd(a M512d, b M512d, c M512d, rounding int) M512d {
	return M512d(fnmsubRoundPd([8]float64(a), [8]float64(b), [8]float64(c), rounding))
}

func fnmsubRoundPd(a [8]float64, b [8]float64, c [8]float64, rounding int) [8]float64


// MaskFnmsubRoundPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD'. Intrinsic: '_mm512_mask_fnmsub_round_pd'.
// Requires KNCNI.
func MaskFnmsubRoundPd(a M512d, k Mmask8, b M512d, c M512d, rounding int) M512d {
	return M512d(maskFnmsubRoundPd([8]float64(a), uint8(k), [8]float64(b), [8]float64(c), rounding))
}

func maskFnmsubRoundPd(a [8]float64, k uint8, b [8]float64, c [8]float64, rounding int) [8]float64


// Mask3FnmsubRoundPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set).
// Rounding is done according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD'. Intrinsic: '_mm512_mask3_fnmsub_round_pd'.
// Requires KNCNI.
func Mask3FnmsubRoundPd(a M512d, b M512d, c M512d, k Mmask8, rounding int) M512d {
	return M512d(mask3FnmsubRoundPd([8]float64(a), [8]float64(b), [8]float64(c), uint8(k), rounding))
}

func mask3FnmsubRoundPd(a [8]float64, b [8]float64, c [8]float64, k uint8, rounding int) [8]float64


// MaskzFnmsubRoundPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set).
// Rounding is done according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD'. Intrinsic: '_mm512_maskz_fnmsub_round_pd'.
// Requires AVX512F.
func MaskzFnmsubRoundPd(k Mmask8, a M512d, b M512d, c M512d, rounding int) M512d {
	return M512d(maskzFnmsubRoundPd(uint8(k), [8]float64(a), [8]float64(b), [8]float64(c), rounding))
}

func maskzFnmsubRoundPd(k uint8, a [8]float64, b [8]float64, c [8]float64, rounding int) [8]float64


// FnmsubRoundPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS'. Intrinsic: '_mm512_fnmsub_round_ps'.
// Requires KNCNI.
func FnmsubRoundPs(a M512, b M512, c M512, rounding int) M512 {
	return M512(fnmsubRoundPs([16]float32(a), [16]float32(b), [16]float32(c), rounding))
}

func fnmsubRoundPs(a [16]float32, b [16]float32, c [16]float32, rounding int) [16]float32


// MaskFnmsubRoundPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS'. Intrinsic: '_mm512_mask_fnmsub_round_ps'.
// Requires KNCNI.
func MaskFnmsubRoundPs(a M512, k Mmask16, b M512, c M512, rounding int) M512 {
	return M512(maskFnmsubRoundPs([16]float32(a), uint16(k), [16]float32(b), [16]float32(c), rounding))
}

func maskFnmsubRoundPs(a [16]float32, k uint16, b [16]float32, c [16]float32, rounding int) [16]float32


// Mask3FnmsubRoundPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set).
// Rounding is done according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS'. Intrinsic: '_mm512_mask3_fnmsub_round_ps'.
// Requires KNCNI.
func Mask3FnmsubRoundPs(a M512, b M512, c M512, k Mmask16, rounding int) M512 {
	return M512(mask3FnmsubRoundPs([16]float32(a), [16]float32(b), [16]float32(c), uint16(k), rounding))
}

func mask3FnmsubRoundPs(a [16]float32, b [16]float32, c [16]float32, k uint16, rounding int) [16]float32


// MaskzFnmsubRoundPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS'. Intrinsic: '_mm512_maskz_fnmsub_round_ps'.
// Requires AVX512F.
func MaskzFnmsubRoundPs(k Mmask16, a M512, b M512, c M512, rounding int) M512 {
	return M512(maskzFnmsubRoundPs(uint16(k), [16]float32(a), [16]float32(b), [16]float32(c), rounding))
}

func maskzFnmsubRoundPs(k uint16, a [16]float32, b [16]float32, c [16]float32, rounding int) [16]float32


// FpclassPdMask: Test packed double-precision (64-bit) floating-point elements
// in 'a' for special categories specified by 'imm8', and store the results in
// mask vector 'k'.
// 	'imm' can be a combination of:
//     0x01 // QNaN
//     0x02 // Positive Zero
//     0x04 // Negative Zero
//     0x08 // Positive Infinity
//     0x10 // Negative Infinity
//     0x20 // Denormal
//     0x40 // Negative
//     0x80 // SNaN 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := CheckFPClass_FP64(a[i+63:i], imm8[7:0])
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VFPCLASSPD'. Intrinsic: '_mm512_fpclass_pd_mask'.
// Requires AVX512DQ.
func FpclassPdMask(a M512d, imm8 int) Mmask8 {
	return Mmask8(fpclassPdMask([8]float64(a), imm8))
}

func fpclassPdMask(a [8]float64, imm8 int) uint8


// MaskFpclassPdMask: Test packed double-precision (64-bit) floating-point
// elements in 'a' for special categories specified by 'imm8', and store the
// results in mask vector 'k' using zeromask 'k1' (elements are zeroed out when
// the corresponding mask bit is not set).
// 	'imm' can be a combination of:
//     0x01 // QNaN
//     0x02 // Positive Zero
//     0x04 // Negative Zero
//     0x08 // Positive Infinity
//     0x10 // Negative Infinity
//     0x20 // Denormal
//     0x40 // Negative
//     0x80 // SNaN 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := CheckFPClass_FP64(a[i+63:i], imm8[7:0])
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VFPCLASSPD'. Intrinsic: '_mm512_mask_fpclass_pd_mask'.
// Requires AVX512DQ.
func MaskFpclassPdMask(k1 Mmask8, a M512d, imm8 int) Mmask8 {
	return Mmask8(maskFpclassPdMask(uint8(k1), [8]float64(a), imm8))
}

func maskFpclassPdMask(k1 uint8, a [8]float64, imm8 int) uint8


// FpclassPsMask: Test packed single-precision (32-bit) floating-point elements
// in 'a' for special categories specified by 'imm8', and store the results in
// mask vector 'k'.
// 	'imm' can be a combination of:
//     0x01 // QNaN
//     0x02 // Positive Zero
//     0x04 // Negative Zero
//     0x08 // Positive Infinity
//     0x10 // Negative Infinity
//     0x20 // Denormal
//     0x40 // Negative
//     0x80 // SNaN 
//
//		FOR j := 0 to 15
//			i := j*32
//			k[j] := CheckFPClass_FP32(a[i+31:i], imm8[7:0])
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VFPCLASSPS'. Intrinsic: '_mm512_fpclass_ps_mask'.
// Requires AVX512DQ.
func FpclassPsMask(a M512, imm8 int) Mmask16 {
	return Mmask16(fpclassPsMask([16]float32(a), imm8))
}

func fpclassPsMask(a [16]float32, imm8 int) uint16


// MaskFpclassPsMask: Test packed single-precision (32-bit) floating-point
// elements in 'a' for special categories specified by 'imm8', and store the
// results in mask vector 'k' using zeromask 'k1' (elements are zeroed out when
// the corresponding mask bit is not set).
// 	'imm' can be a combination of:
//     0x01 // QNaN
//     0x02 // Positive Zero
//     0x04 // Negative Zero
//     0x08 // Positive Infinity
//     0x10 // Negative Infinity
//     0x20 // Denormal
//     0x40 // Negative
//     0x80 // SNaN 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k1[j]
//				k[j] := CheckFPClass_FP32(a[i+31:i], imm8[7:0])
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VFPCLASSPS'. Intrinsic: '_mm512_mask_fpclass_ps_mask'.
// Requires AVX512DQ.
func MaskFpclassPsMask(k1 Mmask16, a M512, imm8 int) Mmask16 {
	return Mmask16(maskFpclassPsMask(uint16(k1), [16]float32(a), imm8))
}

func maskFpclassPsMask(k1 uint16, a [16]float32, imm8 int) uint16


// GetexpPd: Convert the exponent of each packed double-precision (64-bit)
// floating-point element in 'a' to a double-precision (64-bit) floating-point
// number representing the integer exponent, and store the results in 'dst'.
// This intrinsic essentially calculates 'floor(log2(x))' for each element. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ConvertExpFP64(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGETEXPPD'. Intrinsic: '_mm512_getexp_pd'.
// Requires KNCNI.
func GetexpPd(a M512d) M512d {
	return M512d(getexpPd([8]float64(a)))
}

func getexpPd(a [8]float64) [8]float64


// MaskGetexpPd: Convert the exponent of each packed double-precision (64-bit)
// floating-point element in 'a' to a double-precision (64-bit) floating-point
// number representing the integer exponent, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). This intrinsic essentially calculates 'floor(log2(x))'
// for each element. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ConvertExpFP64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGETEXPPD'. Intrinsic: '_mm512_mask_getexp_pd'.
// Requires KNCNI.
func MaskGetexpPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskGetexpPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskGetexpPd(src [8]float64, k uint8, a [8]float64) [8]float64


// MaskzGetexpPd: Convert the exponent of each packed double-precision (64-bit)
// floating-point element in 'a' to a double-precision (64-bit) floating-point
// number representing the integer exponent, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). This intrinsic essentially calculates 'floor(log2(x))' for each
// element. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ConvertExpFP64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGETEXPPD'. Intrinsic: '_mm512_maskz_getexp_pd'.
// Requires AVX512F.
func MaskzGetexpPd(k Mmask8, a M512d) M512d {
	return M512d(maskzGetexpPd(uint8(k), [8]float64(a)))
}

func maskzGetexpPd(k uint8, a [8]float64) [8]float64


// GetexpPs: Convert the exponent of each packed single-precision (32-bit)
// floating-point element in 'a' to a single-precision (32-bit) floating-point
// number representing the integer exponent, and store the results in 'dst'.
// This intrinsic essentially calculates 'floor(log2(x))' for each element. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ConvertExpFP32(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGETEXPPS'. Intrinsic: '_mm512_getexp_ps'.
// Requires KNCNI.
func GetexpPs(a M512) M512 {
	return M512(getexpPs([16]float32(a)))
}

func getexpPs(a [16]float32) [16]float32


// MaskGetexpPs: Convert the exponent of each packed single-precision (32-bit)
// floating-point element in 'a' to a single-precision (32-bit) floating-point
// number representing the integer exponent, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). This intrinsic essentially calculates 'floor(log2(x))'
// for each element. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ConvertExpFP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGETEXPPS'. Intrinsic: '_mm512_mask_getexp_ps'.
// Requires KNCNI.
func MaskGetexpPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskGetexpPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskGetexpPs(src [16]float32, k uint16, a [16]float32) [16]float32


// MaskzGetexpPs: Convert the exponent of each packed single-precision (32-bit)
// floating-point element in 'a' to a single-precision (32-bit) floating-point
// number representing the integer exponent, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). This intrinsic essentially calculates 'floor(log2(x))' for each
// element. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ConvertExpFP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGETEXPPS'. Intrinsic: '_mm512_maskz_getexp_ps'.
// Requires AVX512F.
func MaskzGetexpPs(k Mmask16, a M512) M512 {
	return M512(maskzGetexpPs(uint16(k), [16]float32(a)))
}

func maskzGetexpPs(k uint16, a [16]float32) [16]float32


// GetexpRoundPd: Convert the exponent of each packed double-precision (64-bit)
// floating-point element in 'a' to a double-precision (64-bit) floating-point
// number representing the integer exponent, and store the results in 'dst'.
// This intrinsic essentially calculates 'floor(log2(x))' for each element.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ConvertExpFP64(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGETEXPPD'. Intrinsic: '_mm512_getexp_round_pd'.
// Requires KNCNI.
func GetexpRoundPd(a M512d, rounding int) M512d {
	return M512d(getexpRoundPd([8]float64(a), rounding))
}

func getexpRoundPd(a [8]float64, rounding int) [8]float64


// MaskGetexpRoundPd: Convert the exponent of each packed double-precision
// (64-bit) floating-point element in 'a' to a double-precision (64-bit)
// floating-point number representing the integer exponent, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). This intrinsic essentially
// calculates 'floor(log2(x))' for each element.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ConvertExpFP64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGETEXPPD'. Intrinsic: '_mm512_mask_getexp_round_pd'.
// Requires KNCNI.
func MaskGetexpRoundPd(src M512d, k Mmask8, a M512d, rounding int) M512d {
	return M512d(maskGetexpRoundPd([8]float64(src), uint8(k), [8]float64(a), rounding))
}

func maskGetexpRoundPd(src [8]float64, k uint8, a [8]float64, rounding int) [8]float64


// MaskzGetexpRoundPd: Convert the exponent of each packed double-precision
// (64-bit) floating-point element in 'a' to a double-precision (64-bit)
// floating-point number representing the integer exponent, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). This intrinsic essentially calculates
// 'floor(log2(x))' for each element.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ConvertExpFP64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGETEXPPD'. Intrinsic: '_mm512_maskz_getexp_round_pd'.
// Requires AVX512F.
func MaskzGetexpRoundPd(k Mmask8, a M512d, rounding int) M512d {
	return M512d(maskzGetexpRoundPd(uint8(k), [8]float64(a), rounding))
}

func maskzGetexpRoundPd(k uint8, a [8]float64, rounding int) [8]float64


// GetexpRoundPs: Convert the exponent of each packed single-precision (32-bit)
// floating-point element in 'a' to a single-precision (32-bit) floating-point
// number representing the integer exponent, and store the results in 'dst'.
// This intrinsic essentially calculates 'floor(log2(x))' for each element.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ConvertExpFP32(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGETEXPPS'. Intrinsic: '_mm512_getexp_round_ps'.
// Requires KNCNI.
func GetexpRoundPs(a M512, rounding int) M512 {
	return M512(getexpRoundPs([16]float32(a), rounding))
}

func getexpRoundPs(a [16]float32, rounding int) [16]float32


// MaskGetexpRoundPs: Convert the exponent of each packed single-precision
// (32-bit) floating-point element in 'a' to a single-precision (32-bit)
// floating-point number representing the integer exponent, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). This intrinsic essentially
// calculates 'floor(log2(x))' for each element.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ConvertExpFP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGETEXPPS'. Intrinsic: '_mm512_mask_getexp_round_ps'.
// Requires KNCNI.
func MaskGetexpRoundPs(src M512, k Mmask16, a M512, rounding int) M512 {
	return M512(maskGetexpRoundPs([16]float32(src), uint16(k), [16]float32(a), rounding))
}

func maskGetexpRoundPs(src [16]float32, k uint16, a [16]float32, rounding int) [16]float32


// MaskzGetexpRoundPs: Convert the exponent of each packed single-precision
// (32-bit) floating-point element in 'a' to a single-precision (32-bit)
// floating-point number representing the integer exponent, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). This intrinsic essentially calculates
// 'floor(log2(x))' for each element.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ConvertExpFP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGETEXPPS'. Intrinsic: '_mm512_maskz_getexp_round_ps'.
// Requires AVX512F.
func MaskzGetexpRoundPs(k Mmask16, a M512, rounding int) M512 {
	return M512(maskzGetexpRoundPs(uint16(k), [16]float32(a), rounding))
}

func maskzGetexpRoundPs(k uint16, a [16]float32, rounding int) [16]float32


// GetmantPd: Normalize the mantissas of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. This
// intrinsic essentially calculates '(2^k)*|x.significand|', where 'k'
// depends on the interval range defined by 'interv' and the sign depends on
// 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGETMANTPD'. Intrinsic: '_mm512_getmant_pd'.
// Requires KNCNI.
func GetmantPd(a M512d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M512d {
	return M512d(getmantPd([8]float64(a), interv, sc))
}

func getmantPd(a [8]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [8]float64


// MaskGetmantPd: Normalize the mantissas of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). This intrinsic essentially calculates
// '(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGETMANTPD'. Intrinsic: '_mm512_mask_getmant_pd'.
// Requires KNCNI.
func MaskGetmantPd(src M512d, k Mmask8, a M512d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M512d {
	return M512d(maskGetmantPd([8]float64(src), uint8(k), [8]float64(a), interv, sc))
}

func maskGetmantPd(src [8]float64, k uint8, a [8]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [8]float64


// MaskzGetmantPd: Normalize the mantissas of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). This intrinsic essentially calculates '(2^k)*|x.significand|', where
// 'k' depends on the interval range defined by 'interv' and the sign depends
// on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGETMANTPD'. Intrinsic: '_mm512_maskz_getmant_pd'.
// Requires AVX512F.
func MaskzGetmantPd(k Mmask8, a M512d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M512d {
	return M512d(maskzGetmantPd(uint8(k), [8]float64(a), interv, sc))
}

func maskzGetmantPd(k uint8, a [8]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [8]float64


// GetmantPs: Normalize the mantissas of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. This
// intrinsic essentially calculates '(2^k)*|x.significand|', where 'k'
// depends on the interval range defined by 'interv' and the sign depends on
// 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGETMANTPS'. Intrinsic: '_mm512_getmant_ps'.
// Requires KNCNI.
func GetmantPs(a M512, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M512 {
	return M512(getmantPs([16]float32(a), interv, sc))
}

func getmantPs(a [16]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [16]float32


// MaskGetmantPs: Normalize the mantissas of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). This intrinsic essentially calculates
// '(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGETMANTPS'. Intrinsic: '_mm512_mask_getmant_ps'.
// Requires KNCNI.
func MaskGetmantPs(src M512, k Mmask16, a M512, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M512 {
	return M512(maskGetmantPs([16]float32(src), uint16(k), [16]float32(a), interv, sc))
}

func maskGetmantPs(src [16]float32, k uint16, a [16]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [16]float32


// MaskzGetmantPs: Normalize the mantissas of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). This intrinsic essentially calculates '(2^k)*|x.significand|', where
// 'k' depends on the interval range defined by 'interv' and the sign depends
// on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGETMANTPS'. Intrinsic: '_mm512_maskz_getmant_ps'.
// Requires AVX512F.
func MaskzGetmantPs(k Mmask16, a M512, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M512 {
	return M512(maskzGetmantPs(uint16(k), [16]float32(a), interv, sc))
}

func maskzGetmantPs(k uint16, a [16]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [16]float32


// GetmantRoundPd: Normalize the mantissas of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. This
// intrinsic essentially calculates '(2^k)*|x.significand|', where 'k'
// depends on the interval range defined by 'interv' and the sign depends on
// 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1Rounding is done according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGETMANTPD'. Intrinsic: '_mm512_getmant_round_pd'.
// Requires KNCNI.
func GetmantRoundPd(a M512d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) M512d {
	return M512d(getmantRoundPd([8]float64(a), interv, sc, rounding))
}

func getmantRoundPd(a [8]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) [8]float64


// MaskGetmantRoundPd: Normalize the mantissas of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). This intrinsic essentially calculates
// '(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1Rounding is done according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGETMANTPD'. Intrinsic: '_mm512_mask_getmant_round_pd'.
// Requires KNCNI.
func MaskGetmantRoundPd(src M512d, k Mmask8, a M512d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) M512d {
	return M512d(maskGetmantRoundPd([8]float64(src), uint8(k), [8]float64(a), interv, sc, rounding))
}

func maskGetmantRoundPd(src [8]float64, k uint8, a [8]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) [8]float64


// MaskzGetmantRoundPd: Normalize the mantissas of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). This intrinsic essentially calculates
// '(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1Rounding is done according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGETMANTPD'. Intrinsic: '_mm512_maskz_getmant_round_pd'.
// Requires AVX512F.
func MaskzGetmantRoundPd(k Mmask8, a M512d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) M512d {
	return M512d(maskzGetmantRoundPd(uint8(k), [8]float64(a), interv, sc, rounding))
}

func maskzGetmantRoundPd(k uint8, a [8]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) [8]float64


// GetmantRoundPs: Normalize the mantissas of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. This
// intrinsic essentially calculates '(2^k)*|x.significand|', where 'k'
// depends on the interval range defined by 'interv' and the sign depends on
// 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1Rounding is done according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGETMANTPS'. Intrinsic: '_mm512_getmant_round_ps'.
// Requires KNCNI.
func GetmantRoundPs(a M512, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) M512 {
	return M512(getmantRoundPs([16]float32(a), interv, sc, rounding))
}

func getmantRoundPs(a [16]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) [16]float32


// MaskGetmantRoundPs: Normalize the mantissas of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). This intrinsic essentially calculates
// '(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1Rounding is done according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGETMANTPS'. Intrinsic: '_mm512_mask_getmant_round_ps'.
// Requires KNCNI.
func MaskGetmantRoundPs(src M512, k Mmask16, a M512, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) M512 {
	return M512(maskGetmantRoundPs([16]float32(src), uint16(k), [16]float32(a), interv, sc, rounding))
}

func maskGetmantRoundPs(src [16]float32, k uint16, a [16]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) [16]float32


// MaskzGetmantRoundPs: Normalize the mantissas of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). This intrinsic essentially calculates
// '(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1Rounding is done according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGETMANTPS'. Intrinsic: '_mm512_maskz_getmant_round_ps'.
// Requires AVX512F.
func MaskzGetmantRoundPs(k Mmask16, a M512, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) M512 {
	return M512(maskzGetmantRoundPs(uint16(k), [16]float32(a), interv, sc, rounding))
}

func maskzGetmantRoundPs(k uint16, a [16]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) [16]float32


// GmaxPd: Determines the maximum of each pair of corresponding elements in
// packed double-precision (64-bit) floating-point elements in 'a' and 'b',
// storing the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := FpMax(a[i+63:i], b[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGMAXPD'. Intrinsic: '_mm512_gmax_pd'.
// Requires KNCNI.
func GmaxPd(a M512d, b M512d) M512d {
	return M512d(gmaxPd([8]float64(a), [8]float64(b)))
}

func gmaxPd(a [8]float64, b [8]float64) [8]float64


// MaskGmaxPd: Determines the maximum of each pair of corresponding elements of
// packed double-precision (64-bit) floating-point elements in 'a' and 'b',
// storing the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := FpMax(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGMAXPD'. Intrinsic: '_mm512_mask_gmax_pd'.
// Requires KNCNI.
func MaskGmaxPd(src M512d, k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskGmaxPd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b)))
}

func maskGmaxPd(src [8]float64, k uint8, a [8]float64, b [8]float64) [8]float64


// GmaxPs: Determines the maximum of each pair of corresponding elements in
// packed single-precision (32-bit) floating-point elements in 'a' and 'b',
// storing the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := FpMax(a[i+31:i], b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGMAXPS'. Intrinsic: '_mm512_gmax_ps'.
// Requires KNCNI.
func GmaxPs(a M512, b M512) M512 {
	return M512(gmaxPs([16]float32(a), [16]float32(b)))
}

func gmaxPs(a [16]float32, b [16]float32) [16]float32


// MaskGmaxPs: Determines the maximum of each pair of corresponding elements of
// packed single-precision (32-bit) floating-point elements in 'a' and 'b',
// storing the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := FpMax(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGMAXPS'. Intrinsic: '_mm512_mask_gmax_ps'.
// Requires KNCNI.
func MaskGmaxPs(src M512, k Mmask16, a M512, b M512) M512 {
	return M512(maskGmaxPs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b)))
}

func maskGmaxPs(src [16]float32, k uint16, a [16]float32, b [16]float32) [16]float32


// GmaxabsPs: Determines the maximum of the absolute elements of each pair of
// corresponding elements of packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', storing the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := FpMax(Abs(a[i+31:i]), Abs(b[i+31:i]))
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGMAXABSPS'. Intrinsic: '_mm512_gmaxabs_ps'.
// Requires KNCNI.
func GmaxabsPs(a M512, b M512) M512 {
	return M512(gmaxabsPs([16]float32(a), [16]float32(b)))
}

func gmaxabsPs(a [16]float32, b [16]float32) [16]float32


// MaskGmaxabsPs: Determines the maximum of the absolute elements of each pair
// of corresponding elements of packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', storing the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := FpMax(Abs(a[i+31:i]), Abs(b[i+31:i]))
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGMAXABSPS'. Intrinsic: '_mm512_mask_gmaxabs_ps'.
// Requires KNCNI.
func MaskGmaxabsPs(src M512, k Mmask16, a M512, b M512) M512 {
	return M512(maskGmaxabsPs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b)))
}

func maskGmaxabsPs(src [16]float32, k uint16, a [16]float32, b [16]float32) [16]float32


// GminPd: Determines the minimum of each pair of corresponding elements in
// packed double-precision (64-bit) floating-point elements in 'a' and 'b',
// storing the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := FpMin(a[i+63:i], b[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGMINPD'. Intrinsic: '_mm512_gmin_pd'.
// Requires KNCNI.
func GminPd(a M512d, b M512d) M512d {
	return M512d(gminPd([8]float64(a), [8]float64(b)))
}

func gminPd(a [8]float64, b [8]float64) [8]float64


// MaskGminPd: Determines the maximum of each pair of corresponding elements of
// packed double-precision (64-bit) floating-point elements in 'a' and 'b',
// storing the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := FpMin(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGMINPD'. Intrinsic: '_mm512_mask_gmin_pd'.
// Requires KNCNI.
func MaskGminPd(src M512d, k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskGminPd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b)))
}

func maskGminPd(src [8]float64, k uint8, a [8]float64, b [8]float64) [8]float64


// GminPs: Determines the minimum of each pair of corresponding elements in
// packed single-precision (32-bit) floating-point elements in 'a' and 'b',
// storing the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := FpMin(a[i+31:i], b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGMINPS'. Intrinsic: '_mm512_gmin_ps'.
// Requires KNCNI.
func GminPs(a M512, b M512) M512 {
	return M512(gminPs([16]float32(a), [16]float32(b)))
}

func gminPs(a [16]float32, b [16]float32) [16]float32


// MaskGminPs: Determines the maximum of each pair of corresponding elements of
// packed single-precision (32-bit) floating-point elements in 'a' and 'b',
// storing the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := FpMin(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGMINPS'. Intrinsic: '_mm512_mask_gmin_ps'.
// Requires KNCNI.
func MaskGminPs(src M512, k Mmask16, a M512, b M512) M512 {
	return M512(maskGminPs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b)))
}

func maskGminPs(src [16]float32, k uint16, a [16]float32, b [16]float32) [16]float32


// HypotPd: Compute the length of the hypotenous of a right triangle, with the
// lengths of the other two sides of the triangle stored as packed
// double-precision (64-bit) floating-point elements in 'a' and 'b', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := SQRT(a[i+63:i]^2 + b[i+63:i]^2)
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_hypot_pd'.
// Requires AVX512F.
func HypotPd(a M512d, b M512d) M512d {
	return M512d(hypotPd([8]float64(a), [8]float64(b)))
}

func hypotPd(a [8]float64, b [8]float64) [8]float64


// MaskHypotPd: Compute the length of the hypotenous of a right triangle, with
// the lengths of the other two sides of the triangle stored as packed
// double-precision (64-bit) floating-point elements in 'a' and 'b', and store
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SQRT(a[i+63:i]^2 + b[i+63:i]^2)
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_hypot_pd'.
// Requires AVX512F.
func MaskHypotPd(src M512d, k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskHypotPd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b)))
}

func maskHypotPd(src [8]float64, k uint8, a [8]float64, b [8]float64) [8]float64


// HypotPs: Compute the length of the hypotenous of a right triangle, with the
// lengths of the other two sides of the triangle stored as packed
// single-precision (32-bit) floating-point elements in 'a' and 'b', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := SQRT(a[i+31:i]^2 + b[i+31:i]^2)
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_hypot_ps'.
// Requires AVX512F.
func HypotPs(a M512, b M512) M512 {
	return M512(hypotPs([16]float32(a), [16]float32(b)))
}

func hypotPs(a [16]float32, b [16]float32) [16]float32


// MaskHypotPs: Compute the length of the hypotenous of a right triangle, with
// the lengths of the other two sides of the triangle stored as packed
// single-precision (32-bit) floating-point elements in 'a' and 'b', and store
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SQRT(a[i+31:i]^2 + b[i+31:i]^2)
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_hypot_ps'.
// Requires AVX512F.
func MaskHypotPs(src M512, k Mmask16, a M512, b M512) M512 {
	return M512(maskHypotPs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b)))
}

func maskHypotPs(src [16]float32, k uint16, a [16]float32, b [16]float32) [16]float32


// I32extgatherEpi32: Up-converts 16 memory locations starting at location 'mv'
// at packed 32-bit integer indices stored in 'index' scaled by 'scale' using
// 'conv' to 32-bit integer elements and stores them in 'dst'. 
//
//		FOR j := 0 to 15
//			addr := MEM[mv + index[j] * scale]
//			i := j*32
//			CASE conv OF
//			_MM_UPCONV_EPI32_NONE:
//				dst[i+31:i] := addr[i+31:i]
//			_MM_UPCONV_EPI32_UINT8:
//				n := j*7
//				dst[i+31:i] := UInt8ToUInt32(addr[n+7:n])
//			_MM_UPCONV_EPI32_SINT8:
//				n := j*7
//				dst[i+31:i] := Int8ToInt32(addr[n+7:n])
//			_MM_UPCONV_EPI32_UINT16:
//				n := j*16
//				dst[i+31:i] := UInt16ToUInt32(addr[n+15:n])
//			_MM_UPCONV_EPI32_SINT16:
//				n := j*16
//				dst[i+31:i] := Int16ToInt32(addr[n+15:n])
//			ESAC
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPGATHERDD'. Intrinsic: '_mm512_i32extgather_epi32'.
// Requires KNCNI.
func I32extgatherEpi32(index M512i, mv uintptr, conv MMUPCONVEPI32ENUM, scale int, hint int) M512i {
	return M512i(i32extgatherEpi32([64]byte(index), uintptr(mv), conv, scale, hint))
}

func i32extgatherEpi32(index [64]byte, mv uintptr, conv MMUPCONVEPI32ENUM, scale int, hint int) [64]byte


// MaskI32extgatherEpi32: Up-converts 16 single-precision (32-bit) memory
// locations starting at location 'mv' at packed 32-bit integer indices stored
// in 'index' scaled by 'scale' using 'conv' to 32-bit integer elements and
// stores them in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			addr := MEM[mv + index[j] * scale]
//			i := j*32
//			IF k[j]
//				CASE conv OF
//				_MM_UPCONV_EPI32_NONE:
//					dst[i+31:i] := addr[i+31:i]
//				_MM_UPCONV_EPI32_UINT8:
//					n := j*7
//					dst[i+31:i] := UInt8ToUInt32(addr[n+7:n])
//				_MM_UPCONV_EPI32_SINT8:
//					n := j*7
//					dst[i+31:i] := Int8ToInt32(addr[n+7:n])
//				_MM_UPCONV_EPI32_UINT16:
//					n := j*16
//					dst[i+31:i] := UInt16ToUInt32(addr[n+15:n])
//				_MM_UPCONV_EPI32_SINT16:
//					n := j*16
//					dst[i+31:i] := Int16ToInt32(addr[n+15:n])
//				ESAC
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPGATHERDD'. Intrinsic: '_mm512_mask_i32extgather_epi32'.
// Requires KNCNI.
func MaskI32extgatherEpi32(src M512i, k Mmask16, index M512i, mv uintptr, conv MMUPCONVEPI32ENUM, scale int, hint int) M512i {
	return M512i(maskI32extgatherEpi32([64]byte(src), uint16(k), [64]byte(index), uintptr(mv), conv, scale, hint))
}

func maskI32extgatherEpi32(src [64]byte, k uint16, index [64]byte, mv uintptr, conv MMUPCONVEPI32ENUM, scale int, hint int) [64]byte


// I32extgatherPs: Up-converts 16 memory locations starting at location 'mv' at
// packed 32-bit integer indices stored in 'index' scaled by 'scale' using
// 'conv' to single-precision (32-bit) floating-point elements and stores them
// in 'dst'. 
//
//		FOR j := 0 to 15
//			addr := MEM[mv + index[j] * scale]
//			i := j*32
//			CASE conv OF
//			_MM_UPCONV_PS_NONE:
//				dst[i+31:i] := addr[i+31:i]
//			_MM_UPCONV_PS_FLOAT16:
//				n := j*16
//				dst[i+31:i] := Float16ToFloat32(addr[n+15:n])
//			_MM_UPCONV_PS_UINT8:
//				n := j*8
//				dst[i+31:i] := UInt8ToFloat32(addr[n+7:n])
//			_MM_UPCONV_PS_SINT8:
//				n := j*8
//				dst[i+31:i] := SInt8ToFloat32(addr[n+7:n])
//			_MM_UPCONV_PS_UINT16:
//				n := j*16
//				dst[i+31:i] := UInt16ToFloat32(addr[n+15:n])
//			_MM_UPCONV_PS_SINT16:
//				n := j*16
//				dst[i+31:i] := SInt16ToFloat32(addr[n+15:n])
//			ESAC
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGATHERDPS'. Intrinsic: '_mm512_i32extgather_ps'.
// Requires KNCNI.
func I32extgatherPs(index M512i, mv uintptr, conv MMUPCONVPSENUM, scale int, hint int) M512 {
	return M512(i32extgatherPs([64]byte(index), uintptr(mv), conv, scale, hint))
}

func i32extgatherPs(index [64]byte, mv uintptr, conv MMUPCONVPSENUM, scale int, hint int) [16]float32


// MaskI32extgatherPs: Up-converts 16 single-precision (32-bit) memory
// locations starting at location 'mv' at packed 32-bit integer indices stored
// in 'index' scaled by 'scale' using 'conv' to single-precision (32-bit)
// floating-point elements and stores them in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			addr := MEM[mv + index[j] * scale]
//			i := j*32
//			IF k[j]
//				CASE conv OF
//				_MM_UPCONV_PS_NONE:
//					dst[i+31:i] := addr[i+31:i]
//				_MM_UPCONV_PS_FLOAT16:
//					n := j*16
//					dst[i+31:i] := Float16ToFloat32(addr[n+15:n])
//				_MM_UPCONV_PS_UINT8:
//					n := j*8
//					dst[i+31:i] := UInt8ToFloat32(addr[n+7:n])
//				_MM_UPCONV_PS_SINT8:
//					n := j*8
//					dst[i+31:i] := SInt8ToFloat32(addr[n+7:n])
//				_MM_UPCONV_PS_UINT16:
//					n := j*16
//					dst[i+31:i] := UInt16ToFloat32(addr[n+15:n])
//				_MM_UPCONV_PS_SINT16:
//					n := j*16
//					dst[i+31:i] := SInt16ToFloat32(addr[n+15:n])
//				ESAC
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGATHERDPS'. Intrinsic: '_mm512_mask_i32extgather_ps'.
// Requires KNCNI.
func MaskI32extgatherPs(src M512, k Mmask16, index M512i, mv uintptr, conv MMUPCONVPSENUM, scale int, hint int) M512 {
	return M512(maskI32extgatherPs([16]float32(src), uint16(k), [64]byte(index), uintptr(mv), conv, scale, hint))
}

func maskI32extgatherPs(src [16]float32, k uint16, index [64]byte, mv uintptr, conv MMUPCONVPSENUM, scale int, hint int) [16]float32


// I32extscatterEpi32: Down-converts 16 packed 32-bit integer elements in 'v1'
// using 'conv' and stores them in memory locations starting at location 'mv'
// at packed 32-bit integer indices stored in 'index' scaled by 'scale'. 'hint'
// indicates to the processor whether the data is non-temporal. 
//
//		FOR j := 0 to 15
//			addr := MEM[mv + index[j] * scale]
//			i := j*32
//			CASE conv OF
//			_MM_DOWNCONV_EPI32_NONE:
//				addr[i+31:i] := v1[i+31:i]
//			_MM_DOWNCONV_EPI32_UINT8:
//				n := j*8
//				addr[n+7:n] := UInt32ToUInt8(v1[i+31:i])
//			_MM_DOWNCONV_EPI32_SINT8:
//				n := j*8
//				addr[n+7:n] := SInt32ToSInt8(v1[i+31:i])
//			_MM_DOWNCONV_EPI32_UINT16:
//				n := j*16
//				addr[n+15:n] := UInt32ToUInt16(v1[i+31:i])
//			_MM_DOWNCONV_EPI32_SINT16:
//				n := j*16
//				addr[n+15:n] := SInt32ToSInt16(v1[n+15:n])
//			ESAC
//		ENDFOR
//
// Instruction: 'VPSCATTERDD'. Intrinsic: '_mm512_i32extscatter_epi32'.
// Requires KNCNI.
func I32extscatterEpi32(mv uintptr, index M512i, v1 M512i, conv MMDOWNCONVEPI32ENUM, scale int, hint int)  {
	i32extscatterEpi32(uintptr(mv), [64]byte(index), [64]byte(v1), conv, scale, hint)
}

func i32extscatterEpi32(mv uintptr, index [64]byte, v1 [64]byte, conv MMDOWNCONVEPI32ENUM, scale int, hint int) 


// MaskI32extscatterEpi32: Down-converts 16 packed 32-bit integer elements in
// 'v1' using 'conv' and stores them in memory locations starting at location
// 'mv' at packed 32-bit integer indices stored in 'index' scaled by 'scale'.
// Elements are written using writemask 'k' (elements are only written when the
// corresponding mask bit is set; otherwise, elements are left unchanged in
// memory). 'hint' indicates to the processor whether the data is non-temporal. 
//
//		FOR j := 0 to 15
//			addr := MEM[mv + index[j] * scale]
//			i := j*32
//			IF k[j]
//				CASE conv OF
//				_MM_DOWNCONV_EPI32_NONE:
//					addr[i+31:i] := v1[i+31:i]
//				_MM_DOWNCONV_EPI32_UINT8:
//					n := j*8
//					addr[n+7:n] := UInt32ToUInt8(v1[i+31:i])
//				_MM_DOWNCONV_EPI32_SINT8:
//					n := j*8
//					addr[n+7:n] := SInt32ToSInt8(v1[i+31:i])
//				_MM_DOWNCONV_EPI32_UINT16:
//					n := j*16
//					addr[n+15:n] := UInt32ToUInt16(v1[i+31:i])
//				_MM_DOWNCONV_EPI32_SINT16:
//					n := j*16
//					addr[n+15:n] := SInt32ToSInt16(v1[n+15:n])
//				ESAC
//			FI 
//		ENDFOR
//
// Instruction: 'VPSCATTERDD'. Intrinsic: '_mm512_mask_i32extscatter_epi32'.
// Requires KNCNI.
func MaskI32extscatterEpi32(mv uintptr, k Mmask16, index M512i, v1 M512i, conv MMDOWNCONVEPI32ENUM, scale int, hint int)  {
	maskI32extscatterEpi32(uintptr(mv), uint16(k), [64]byte(index), [64]byte(v1), conv, scale, hint)
}

func maskI32extscatterEpi32(mv uintptr, k uint16, index [64]byte, v1 [64]byte, conv MMDOWNCONVEPI32ENUM, scale int, hint int) 


// I32extscatterPs: Down-converts 16 packed single-precision (32-bit)
// floating-point elements in 'v1' and stores them in memory locations starting
// at location 'mv' at packed 32-bit integer indices stored in 'index' scaled
// by 'scale' using 'conv'. 
//
//		FOR j := 0 to 15
//			addr := MEM[mv + index[j] * scale]
//			i := j*32
//			CASE conv OF
//			_MM_DOWNCONV_PS_NONE:
//				n := j*32
//				addr[i+31:i] := v1[n+31:n]
//			_MM_DOWNCONV_PS_FLOAT16:
//				i := j*16
//				addr[i+15:i] := Float32ToFloat16(v1[n+31:n])
//			_MM_DOWNCONV_PS_UINT8:
//				i := j*8
//				addr[i+7:i] := Float32ToUInt8(v1[n+31:n])
//			_MM_DOWNCONV_PS_SINT8:
//				i := j*8
//				addr[i+7:i] := Float32ToSInt8(v1[n+31:n])
//			_MM_DOWNCONV_PS_UINT16:
//				i := j*8
//				addr[i+15:i] := Float32ToUInt16(v1[n+31:n])
//			_MM_DOWNCONV_PS_SINT16:
//				i := j*8
//				addr[i+15:i] := Float32ToSInt16(v1[n+31:n])
//			ESAC
//		ENDFOR
//
// Instruction: 'VSCATTERDPS'. Intrinsic: '_mm512_i32extscatter_ps'.
// Requires KNCNI.
func I32extscatterPs(mv uintptr, index M512i, v1 M512, conv MMDOWNCONVPSENUM, scale int)  {
	i32extscatterPs(uintptr(mv), [64]byte(index), [16]float32(v1), conv, scale)
}

func i32extscatterPs(mv uintptr, index [64]byte, v1 [16]float32, conv MMDOWNCONVPSENUM, scale int) 


// MaskI32extscatterPs: Down-converts 16 packed single-precision (32-bit)
// floating-point elements in 'v1' according to 'conv' and stores them in
// memory locations starting at location 'mv' at packed 32-bit integer indices
// stored in 'index' scaled by 'scale' using writemask 'k' (elements are
// written only when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				addr := MEM[mv + index[j] * scale]
//				CASE conv OF
//				_MM_DOWNCONV_PS_NONE:
//					n := j*32
//					addr[i+31:i] := v1[n+31:n]
//				_MM_DOWNCONV_PS_FLOAT16:
//					i := j*16
//					addr[i+15:i] := Float32ToFloat16(v1[n+31:n])
//				_MM_DOWNCONV_PS_UINT8:
//					i := j*8
//					addr[i+7:i] := Float32ToUInt8(v1[n+31:n])
//				_MM_DOWNCONV_PS_SINT8:
//					i := j*8
//					addr[i+7:i] := Float32ToSInt8(v1[n+31:n])
//				_MM_DOWNCONV_PS_UINT16:
//					i := j*8
//					addr[i+15:i] := Float32ToUInt16(v1[n+31:n])
//				_MM_DOWNCONV_PS_SINT16:
//					i := j*8
//					addr[i+15:i] := Float32ToSInt16(v1[n+31:n])
//				ESAC
//			FI
//		ENDFOR
//
// Instruction: 'VSCATTERDPS'. Intrinsic: '_mm512_mask_i32extscatter_ps'.
// Requires KNCNI.
func MaskI32extscatterPs(mv uintptr, k Mmask16, index M512i, v1 M512, conv MMDOWNCONVPSENUM, scale int, hint int)  {
	maskI32extscatterPs(uintptr(mv), uint16(k), [64]byte(index), [16]float32(v1), conv, scale, hint)
}

func maskI32extscatterPs(mv uintptr, k uint16, index [64]byte, v1 [16]float32, conv MMDOWNCONVPSENUM, scale int, hint int) 


// I32gatherEpi32: Gather 32-bit integers from memory using 32-bit indices.
// 32-bit elements are loaded from addresses starting at 'base_addr' and offset
// by each 32-bit element in 'vindex' (each index is scaled by the factor in
// 'scale'). Gathered elements are merged into 'dst'. 'scale' should be 1, 2, 4
// or 8. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+31:i])*scale]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPGATHERDD'. Intrinsic: '_mm512_i32gather_epi32'.
// Requires KNCNI.
func I32gatherEpi32(vindex M512i, base_addr uintptr, scale int) M512i {
	return M512i(i32gatherEpi32([64]byte(vindex), uintptr(base_addr), scale))
}

func i32gatherEpi32(vindex [64]byte, base_addr uintptr, scale int) [64]byte


// MaskI32gatherEpi32: Gather 32-bit integers from memory using 32-bit indices.
// 32-bit elements are loaded from addresses starting at 'base_addr' and offset
// by each 32-bit element in 'vindex' (each index is scaled by the factor in
// 'scale'). Gathered elements are merged into 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+31:i])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//		dst[MAX:512] := 0
//
// Instruction: 'VPGATHERDD'. Intrinsic: '_mm512_mask_i32gather_epi32'.
// Requires KNCNI.
func MaskI32gatherEpi32(src M512i, k Mmask16, vindex M512i, base_addr uintptr, scale int) M512i {
	return M512i(maskI32gatherEpi32([64]byte(src), uint16(k), [64]byte(vindex), uintptr(base_addr), scale))
}

func maskI32gatherEpi32(src [64]byte, k uint16, vindex [64]byte, base_addr uintptr, scale int) [64]byte


// I32gatherEpi64: Gather 64-bit integers from memory using 32-bit indices.
// 64-bit elements are loaded from addresses starting at 'base_addr' and offset
// by each 32-bit element in 'vindex' (each index is scaled by the factor in
// 'scale'). Gathered elements are merged into 'dst'. 'scale' should be 1, 2, 4
// or 8. 
//
//		FOR j := 0 to 7
//			i := j*64
//			m := j*32
//			dst[i+63:i] := MEM[base_addr + SignExtend(vindex[m+31:m])*scale]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPGATHERDQ'. Intrinsic: '_mm512_i32gather_epi64'.
// Requires AVX512F.
func I32gatherEpi64(vindex M256i, base_addr uintptr, scale int) M512i {
	return M512i(i32gatherEpi64([32]byte(vindex), uintptr(base_addr), scale))
}

func i32gatherEpi64(vindex [32]byte, base_addr uintptr, scale int) [64]byte


// MaskI32gatherEpi64: Gather 64-bit integers from memory using 32-bit indices.
// 64-bit elements are loaded from addresses starting at 'base_addr' and offset
// by each 32-bit element in 'vindex' (each index is scaled by the factor in
// 'scale'). Gathered elements are merged into 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*64
//			m := j*32
//			IF k[j]
//				dst[i+63:i] := MEM[base_addr + SignExtend(vindex[m+31:m])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//		dst[MAX:512] := 0
//
// Instruction: 'VPGATHERDQ'. Intrinsic: '_mm512_mask_i32gather_epi64'.
// Requires AVX512F.
func MaskI32gatherEpi64(src M512i, k Mmask8, vindex M256i, base_addr uintptr, scale int) M512i {
	return M512i(maskI32gatherEpi64([64]byte(src), uint8(k), [32]byte(vindex), uintptr(base_addr), scale))
}

func maskI32gatherEpi64(src [64]byte, k uint8, vindex [32]byte, base_addr uintptr, scale int) [64]byte


// I32gatherPd: Gather double-precision (64-bit) floating-point elements from
// memory using 32-bit indices. 64-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 32-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst'. 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*64
//			m := j*32
//			dst[i+63:i] := MEM[base_addr + SignExtend(vindex[m+31:m])*scale]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGATHERDPD'. Intrinsic: '_mm512_i32gather_pd'.
// Requires AVX512F.
func I32gatherPd(vindex M256i, base_addr uintptr, scale int) M512d {
	return M512d(i32gatherPd([32]byte(vindex), uintptr(base_addr), scale))
}

func i32gatherPd(vindex [32]byte, base_addr uintptr, scale int) [8]float64


// MaskI32gatherPd: Gather double-precision (64-bit) floating-point elements
// from memory using 32-bit indices. 64-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 32-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*64
//			m := j*32
//			IF k[j]
//				dst[i+63:i] := MEM[base_addr + SignExtend(vindex[m+31:m])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//		dst[MAX:512] := 0
//
// Instruction: 'VGATHERDPD'. Intrinsic: '_mm512_mask_i32gather_pd'.
// Requires AVX512F.
func MaskI32gatherPd(src M512d, k Mmask8, vindex M256i, base_addr uintptr, scale int) M512d {
	return M512d(maskI32gatherPd([8]float64(src), uint8(k), [32]byte(vindex), uintptr(base_addr), scale))
}

func maskI32gatherPd(src [8]float64, k uint8, vindex [32]byte, base_addr uintptr, scale int) [8]float64


// I32gatherPs: Gather single-precision (32-bit) floating-point elements from
// memory using 32-bit indices. 32-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 32-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst'. 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+31:i])*scale]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGATHERDPS'. Intrinsic: '_mm512_i32gather_ps'.
// Requires KNCNI.
func I32gatherPs(vindex M512i, base_addr uintptr, scale int) M512 {
	return M512(i32gatherPs([64]byte(vindex), uintptr(base_addr), scale))
}

func i32gatherPs(vindex [64]byte, base_addr uintptr, scale int) [16]float32


// MaskI32gatherPs: Gather single-precision (32-bit) floating-point elements
// from memory using 32-bit indices. 32-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 32-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+31:i])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//		dst[MAX:512] := 0
//
// Instruction: 'VGATHERDPS'. Intrinsic: '_mm512_mask_i32gather_ps'.
// Requires KNCNI.
func MaskI32gatherPs(src M512, k Mmask16, vindex M512i, base_addr uintptr, scale int) M512 {
	return M512(maskI32gatherPs([16]float32(src), uint16(k), [64]byte(vindex), uintptr(base_addr), scale))
}

func maskI32gatherPs(src [16]float32, k uint16, vindex [64]byte, base_addr uintptr, scale int) [16]float32


// I32loextgatherEpi64: Up-converts 8 double-precision (64-bit) memory
// locations starting at location 'mv' at packed 32-bit integer indices stored
// in the lower half of 'index' scaled by 'scale' using 'conv' to 64-bit
// integer elements and stores them in 'dst'. 
//
//		FOR j := 0 to 7
//			addr := MEM[mv + index[j] * scale]
//			i := j*64
//			CASE conv OF
//			_MM_UPCONV_EPI64_NONE: dst[i+63:i] := addr[i+63:i]
//			ESAC
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPGATHERDQ'. Intrinsic: '_mm512_i32loextgather_epi64'.
// Requires KNCNI.
func I32loextgatherEpi64(index M512i, mv uintptr, conv MMUPCONVEPI64ENUM, scale int, hint int) M512i {
	return M512i(i32loextgatherEpi64([64]byte(index), uintptr(mv), conv, scale, hint))
}

func i32loextgatherEpi64(index [64]byte, mv uintptr, conv MMUPCONVEPI64ENUM, scale int, hint int) [64]byte


// MaskI32loextgatherEpi64: Up-converts 8 double-precision (64-bit) memory
// locations starting at location 'mv' at packed 32-bit integer indices stored
// in the lower half of 'index' scaled by 'scale' using 'conv' to 64-bit
// integer elements and stores them in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			addr := MEM[mv + index[j] * scale]
//			i := j*64
//			IF k[j]
//				CASE conv OF
//				_MM_UPCONV_EPI64_NONE: dst[i+63:i] := addr[i+63:i]
//				ESAC
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPGATHERDQ'. Intrinsic: '_mm512_mask_i32loextgather_epi64'.
// Requires KNCNI.
func MaskI32loextgatherEpi64(src M512i, k Mmask8, index M512i, mv uintptr, conv MMUPCONVEPI64ENUM, scale int, hint int) M512i {
	return M512i(maskI32loextgatherEpi64([64]byte(src), uint8(k), [64]byte(index), uintptr(mv), conv, scale, hint))
}

func maskI32loextgatherEpi64(src [64]byte, k uint8, index [64]byte, mv uintptr, conv MMUPCONVEPI64ENUM, scale int, hint int) [64]byte


// I32loextgatherPd: Up-converts 8 double-precision (64-bit) floating-point
// elements in memory locations starting at location 'mv' at packed 32-bit
// integer indices stored in the lower half of 'index' scaled by 'scale' using
// 'conv' to 64-bit floating-point elements and stores them in 'dst'. 
//
//		FOR j := 0 to 7
//			addr := MEM[mv + index[j] * scale]
//			i := j*64
//			CASE conv OF
//			_MM_UPCONV_PD_NONE: dst[i+63:i] := addr[i+63:i]
//			ESAC
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGATHERDPD'. Intrinsic: '_mm512_i32loextgather_pd'.
// Requires KNCNI.
func I32loextgatherPd(index M512i, mv uintptr, conv MMUPCONVPDENUM, scale int, hint int) M512d {
	return M512d(i32loextgatherPd([64]byte(index), uintptr(mv), conv, scale, hint))
}

func i32loextgatherPd(index [64]byte, mv uintptr, conv MMUPCONVPDENUM, scale int, hint int) [8]float64


// MaskI32loextgatherPd: Up-converts 8 double-precision (64-bit) floating-point
// elements in memory locations starting at location 'mv' at packed 32-bit
// integer indices stored in the lower half of 'index' scaled by 'scale' using
// 'conv' to 64-bit floating-point elements and stores them in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			addr := MEM[mv + index[j] * scale]
//			i := j*64
//			IF k[j]
//				CASE conv OF
//				_MM_UPCONV_PD_NONE: dst[i+63:i] := addr[i+63:i]
//				ESAC
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGATHERDPD'. Intrinsic: '_mm512_mask_i32loextgather_pd'.
// Requires KNCNI.
func MaskI32loextgatherPd(src M512d, k Mmask8, index M512i, mv uintptr, conv MMUPCONVPDENUM, scale int, hint int) M512d {
	return M512d(maskI32loextgatherPd([8]float64(src), uint8(k), [64]byte(index), uintptr(mv), conv, scale, hint))
}

func maskI32loextgatherPd(src [8]float64, k uint8, index [64]byte, mv uintptr, conv MMUPCONVPDENUM, scale int, hint int) [8]float64


// I32loextscatterEpi64: Down-converts 8 packed 64-bit integer elements in 'v1'
// and stores them in memory locations starting at location 'mv' at packed
// 32-bit integer indices stored in 'index' scaled by 'scale' using 'conv'. 
//
//		FOR j := 0 to 7
//			addr := MEM[mv + index[j] * scale]
//			i := j*64
//			CASE conv OF
//			_MM_DOWNCONV_EPI64_NONE: addr[i+63:i] := v1[i+63:i]
//			ESAC
//		ENDFOR
//
// Instruction: 'VPSCATTERDQ'. Intrinsic: '_mm512_i32loextscatter_epi64'.
// Requires KNCNI.
func I32loextscatterEpi64(mv uintptr, index M512i, v1 M512i, conv MMDOWNCONVEPI64ENUM, scale int, hint int)  {
	i32loextscatterEpi64(uintptr(mv), [64]byte(index), [64]byte(v1), conv, scale, hint)
}

func i32loextscatterEpi64(mv uintptr, index [64]byte, v1 [64]byte, conv MMDOWNCONVEPI64ENUM, scale int, hint int) 


// MaskI32loextscatterEpi64: Down-converts 8 packed 64-bit integer elements in
// 'v1' and stores them in memory locations starting at location 'mv' at packed
// 32-bit integer indices stored in 'index' scaled by 'scale' using 'conv'.
// Only those elements whose corresponding mask bit is set in writemask 'k' are
// written to memory. 
//
//		FOR j := 0 to 7
//			IF k[j]
//				addr := MEM[mv + index[j] * scale]
//				i := j*64
//				CASE conv OF
//				_MM_DOWNCONV_EPI64_NONE: addr[i+63:i] := v1[i+63:i]
//				ESAC
//			FI
//		ENDFOR
//
// Instruction: 'VPSCATTERDQ'. Intrinsic: '_mm512_mask_i32loextscatter_epi64'.
// Requires KNCNI.
func MaskI32loextscatterEpi64(mv uintptr, k Mmask8, index M512i, v1 M512i, conv MMDOWNCONVEPI64ENUM, scale int, hint int)  {
	maskI32loextscatterEpi64(uintptr(mv), uint8(k), [64]byte(index), [64]byte(v1), conv, scale, hint)
}

func maskI32loextscatterEpi64(mv uintptr, k uint8, index [64]byte, v1 [64]byte, conv MMDOWNCONVEPI64ENUM, scale int, hint int) 


// I32loextscatterPd: Down-converts 8 packed double-precision (64-bit)
// floating-point elements in 'v1' and stores them in memory locations starting
// at location 'mv' at packed 32-bit integer indices stored in 'index' scaled
// by 'scale' using 'conv'. 
//
//		FOR j := 0 to 7
//			addr := MEM[mv + index[j] * scale]
//			i := j*64
//			CASE conv OF
//			_MM_DOWNCONV_PD_NONE: addr[i+63:i] := v1[i+63:i]
//			ESAC
//		ENDFOR
//
// Instruction: 'VSCATTERDPD'. Intrinsic: '_mm512_i32loextscatter_pd'.
// Requires KNCNI.
func I32loextscatterPd(mv uintptr, index M512i, v1 M512d, conv MMDOWNCONVPDENUM, scale int, hint int)  {
	i32loextscatterPd(uintptr(mv), [64]byte(index), [8]float64(v1), conv, scale, hint)
}

func i32loextscatterPd(mv uintptr, index [64]byte, v1 [8]float64, conv MMDOWNCONVPDENUM, scale int, hint int) 


// MaskI32loextscatterPd: Down-converts 8 packed double-precision (64-bit)
// floating-point elements in 'v1' and stores them in memory locations starting
// at location 'mv' at packed 32-bit integer indices stored in 'index' scaled
// by 'scale' using 'conv'. Only those elements whose corresponding mask bit is
// set in writemask 'k' are written to memory. 
//
//		FOR j := 0 to 7
//			IF k[j]
//				addr := MEM[mv + index[j] * scale]
//				i := j*64
//				CASE conv OF
//				_MM_DOWNCONV_PD_NONE: addr[i+63:i] := v1[i+63:i]
//				ESAC
//			FI
//		ENDFOR
//
// Instruction: 'VSCATTERDPD'. Intrinsic: '_mm512_mask_i32loextscatter_pd'.
// Requires KNCNI.
func MaskI32loextscatterPd(mv uintptr, k Mmask8, index M512i, v1 M512d, conv MMDOWNCONVPDENUM, scale int, hint int)  {
	maskI32loextscatterPd(uintptr(mv), uint8(k), [64]byte(index), [8]float64(v1), conv, scale, hint)
}

func maskI32loextscatterPd(mv uintptr, k uint8, index [64]byte, v1 [8]float64, conv MMDOWNCONVPDENUM, scale int, hint int) 


// I32logatherEpi64: Loads 8 64-bit integer elements from memory starting at
// location 'mv' at packed 32-bit integer indices stored in the lower half of
// 'index' scaled by 'scale' and stores them in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			addr := MEM[mv + index[j] * scale]
//			dst[i+63:i] := addr[i+63:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPGATHERDQ'. Intrinsic: '_mm512_i32logather_epi64'.
// Requires KNCNI.
func I32logatherEpi64(index M512i, mv uintptr, scale int) M512i {
	return M512i(i32logatherEpi64([64]byte(index), uintptr(mv), scale))
}

func i32logatherEpi64(index [64]byte, mv uintptr, scale int) [64]byte


// MaskI32logatherEpi64: Loads 8 64-bit integer elements from memory starting
// at location 'mv' at packed 32-bit integer indices stored in the lower half
// of 'index' scaled by 'scale' and stores them in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				addr := MEM[mv + index[j] * scale]
//				dst[i+63:i] := addr[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPGATHERDQ'. Intrinsic: '_mm512_mask_i32logather_epi64'.
// Requires KNCNI.
func MaskI32logatherEpi64(src M512i, k Mmask8, index M512i, mv uintptr, scale int) M512i {
	return M512i(maskI32logatherEpi64([64]byte(src), uint8(k), [64]byte(index), uintptr(mv), scale))
}

func maskI32logatherEpi64(src [64]byte, k uint8, index [64]byte, mv uintptr, scale int) [64]byte


// I32logatherPd: Loads 8 double-precision (64-bit) floating-point elements
// stored at memory locations starting at location 'mv' at packed 32-bit
// integer indices stored in the lower half of 'index' scaled by 'scale' them
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			addr := MEM[mv + index[j] * scale]
//			dst[i+63:i] := addr[i+63:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGATHERDPD'. Intrinsic: '_mm512_i32logather_pd'.
// Requires KNCNI.
func I32logatherPd(index M512i, mv uintptr, scale int) M512d {
	return M512d(i32logatherPd([64]byte(index), uintptr(mv), scale))
}

func i32logatherPd(index [64]byte, mv uintptr, scale int) [8]float64


// MaskI32logatherPd: Loads 8 double-precision (64-bit) floating-point elements
// from memory starting at location 'mv' at packed 32-bit integer indices
// stored in the lower half of 'index' scaled by 'scale' into 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				addr := MEM[mv + index[j] * scale]
//				dst[i+63:i] := addr[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGATHERDPD'. Intrinsic: '_mm512_mask_i32logather_pd'.
// Requires KNCNI.
func MaskI32logatherPd(src M512d, k Mmask8, index M512i, mv uintptr, scale int) M512d {
	return M512d(maskI32logatherPd([8]float64(src), uint8(k), [64]byte(index), uintptr(mv), scale))
}

func maskI32logatherPd(src [8]float64, k uint8, index [64]byte, mv uintptr, scale int) [8]float64


// I32loscatterEpi64: Stores 8 packed 64-bit integer elements located in 'v1'
// and stores them in memory locations starting at location 'mv' at packed
// 32-bit integer indices stored in 'index' scaled by 'scale'. 
//
//		FOR j := 0 to 7
//			addr := MEM[mv + index[j] * scale]
//			i := j*64
//			addr[i+63:i] := v1[k+63:j]
//		ENDFOR
//
// Instruction: 'VPSCATTERDQ'. Intrinsic: '_mm512_i32loscatter_epi64'.
// Requires KNCNI.
func I32loscatterEpi64(mv uintptr, index M512i, v1 M512i, scale int)  {
	i32loscatterEpi64(uintptr(mv), [64]byte(index), [64]byte(v1), scale)
}

func i32loscatterEpi64(mv uintptr, index [64]byte, v1 [64]byte, scale int) 


// MaskI32loscatterEpi64: Stores 8 packed 64-bit integer elements located in
// 'v1' and stores them in memory locations starting at location 'mv' at packed
// 32-bit integer indices stored in 'index' scaled by 'scale' using writemask
// 'k' (elements whose corresponding mask bit is not set are not written to
// memory). 
//
//		FOR j := 0 to 7
//			IF k[j]
//				addr := MEM[mv + index[j] * scale]
//				addr[i+63:i] := v1[i+63:i]
//			FI
//		ENDFOR
//
// Instruction: 'VPSCATTERDQ'. Intrinsic: '_mm512_mask_i32loscatter_epi64'.
// Requires KNCNI.
func MaskI32loscatterEpi64(mv uintptr, k Mmask8, index M512i, v1 M512i, scale int)  {
	maskI32loscatterEpi64(uintptr(mv), uint8(k), [64]byte(index), [64]byte(v1), scale)
}

func maskI32loscatterEpi64(mv uintptr, k uint8, index [64]byte, v1 [64]byte, scale int) 


// I32loscatterPd: Stores 8 packed double-precision (64-bit) floating-point
// elements in 'v1' and to memory locations starting at location 'mv' at packed
// 32-bit integer indices stored in 'index' scaled by 'scale'. 
//
//		FOR j := 0 to 7
//			addr := MEM[mv + index[j] * scale]
//			i := j*64
//			addr[i+63:i] := v1[k+63:j]
//		ENDFOR
//
// Instruction: 'VSCATTERDPD'. Intrinsic: '_mm512_i32loscatter_pd'.
// Requires KNCNI.
func I32loscatterPd(mv uintptr, index M512i, v1 M512d, scale int)  {
	i32loscatterPd(uintptr(mv), [64]byte(index), [8]float64(v1), scale)
}

func i32loscatterPd(mv uintptr, index [64]byte, v1 [8]float64, scale int) 


// MaskI32loscatterPd: Stores 8 packed double-precision (64-bit) floating-point
// elements in 'v1' to memory locations starting at location 'mv' at packed
// 32-bit integer indices stored in 'index' scaled by 'scale'. Only those
// elements whose corresponding mask bit is set in writemask 'k' are written to
// memory. 
//
//		FOR j := 0 to 7
//			IF k[j]
//				addr := MEM[mv + index[j] * scale]
//				i := j*64
//				addr[i+63:i] := v1[k+63:j]
//			FI
//		ENDFOR
//
// Instruction: 'VSCATTERDPD'. Intrinsic: '_mm512_mask_i32loscatter_pd'.
// Requires KNCNI.
func MaskI32loscatterPd(mv uintptr, k Mmask8, index M512i, v1 M512d, scale int)  {
	maskI32loscatterPd(uintptr(mv), uint8(k), [64]byte(index), [8]float64(v1), scale)
}

func maskI32loscatterPd(mv uintptr, k uint8, index [64]byte, v1 [8]float64, scale int) 


// I32scatterEpi32: Scatter 32-bit integers from 'a' into memory using 32-bit
// indices. 32-bit elements are stored at addresses starting at 'base_addr' and
// offset by each 32-bit element in 'vindex' (each index is scaled by the
// factor in 'scale'). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 15
//			i := j*32
//			MEM[base_addr + SignExtend(vindex[i+31:i])*scale] := a[i+31:i]
//		ENDFOR
//
// Instruction: 'VPSCATTERDD'. Intrinsic: '_mm512_i32scatter_epi32'.
// Requires KNCNI.
func I32scatterEpi32(base_addr uintptr, vindex M512i, a M512i, scale int)  {
	i32scatterEpi32(uintptr(base_addr), [64]byte(vindex), [64]byte(a), scale)
}

func i32scatterEpi32(base_addr uintptr, vindex [64]byte, a [64]byte, scale int) 


// MaskI32scatterEpi32: Scatter 32-bit integers from 'a' into memory using
// 32-bit indices. 32-bit elements are stored at addresses starting at
// 'base_addr' and offset by each 32-bit element in 'vindex' (each index is
// scaled by the factor in 'scale') subject to mask 'k' (elements are not
// stored when the corresponding mask bit is not set). 'scale' should be 1, 2,
// 4 or 8. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[i+31:i])*scale] := a[i+31:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPSCATTERDD'. Intrinsic: '_mm512_mask_i32scatter_epi32'.
// Requires KNCNI.
func MaskI32scatterEpi32(base_addr uintptr, k Mmask16, vindex M512i, a M512i, scale int)  {
	maskI32scatterEpi32(uintptr(base_addr), uint16(k), [64]byte(vindex), [64]byte(a), scale)
}

func maskI32scatterEpi32(base_addr uintptr, k uint16, vindex [64]byte, a [64]byte, scale int) 


// I32scatterEpi64: Scatter 64-bit integers from 'a' into memory using 32-bit
// indices. 64-bit elements are stored at addresses starting at 'base_addr' and
// offset by each 32-bit element in 'vindex' (each index is scaled by the
// factor in 'scale'). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			MEM[base_addr + SignExtend(vindex[l+31:l])*scale] := a[i+63:i]
//		ENDFOR
//
// Instruction: 'VPSCATTERDQ'. Intrinsic: '_mm512_i32scatter_epi64'.
// Requires AVX512F.
func I32scatterEpi64(base_addr uintptr, vindex M256i, a M512i, scale int)  {
	i32scatterEpi64(uintptr(base_addr), [32]byte(vindex), [64]byte(a), scale)
}

func i32scatterEpi64(base_addr uintptr, vindex [32]byte, a [64]byte, scale int) 


// MaskI32scatterEpi64: Scatter 64-bit integers from 'a' into memory using
// 32-bit indices. 64-bit elements are stored at addresses starting at
// 'base_addr' and offset by each 32-bit element in 'vindex' (each index is
// scaled by the factor in 'scale') subject to mask 'k' (elements are not
// stored when the corresponding mask bit is not set). 'scale' should be 1, 2,
// 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[l+31:l])*scale] := a[i+63:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPSCATTERDQ'. Intrinsic: '_mm512_mask_i32scatter_epi64'.
// Requires AVX512F.
func MaskI32scatterEpi64(base_addr uintptr, k Mmask8, vindex M256i, a M512i, scale int)  {
	maskI32scatterEpi64(uintptr(base_addr), uint8(k), [32]byte(vindex), [64]byte(a), scale)
}

func maskI32scatterEpi64(base_addr uintptr, k uint8, vindex [32]byte, a [64]byte, scale int) 


// I32scatterPd: Scatter double-precision (64-bit) floating-point elements from
// 'a' into memory using 32-bit indices. 64-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 32-bit element in
// 'vindex' (each index is scaled by the factor in 'scale'). 'scale' should be
// 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			MEM[base_addr + SignExtend(vindex[l+31:l])*scale] := a[i+63:i]
//		ENDFOR
//
// Instruction: 'VSCATTERDPD'. Intrinsic: '_mm512_i32scatter_pd'.
// Requires AVX512F.
func I32scatterPd(base_addr uintptr, vindex M256i, a M512d, scale int)  {
	i32scatterPd(uintptr(base_addr), [32]byte(vindex), [8]float64(a), scale)
}

func i32scatterPd(base_addr uintptr, vindex [32]byte, a [8]float64, scale int) 


// MaskI32scatterPd: Scatter double-precision (64-bit) floating-point elements
// from 'a' into memory using 32-bit indices. 64-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 32-bit element in
// 'vindex' (each index is scaled by the factor in 'scale') subject to mask 'k'
// (elements are not stored when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[l+31:l])*scale] := a[i+63:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VSCATTERDPD'. Intrinsic: '_mm512_mask_i32scatter_pd'.
// Requires AVX512F.
func MaskI32scatterPd(base_addr uintptr, k Mmask8, vindex M256i, a M512d, scale int)  {
	maskI32scatterPd(uintptr(base_addr), uint8(k), [32]byte(vindex), [8]float64(a), scale)
}

func maskI32scatterPd(base_addr uintptr, k uint8, vindex [32]byte, a [8]float64, scale int) 


// I32scatterPs: Scatter single-precision (32-bit) floating-point elements from
// 'a' into memory using 32-bit indices. 32-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 32-bit element in
// 'vindex' (each index is scaled by the factor in 'scale'). 'scale' should be
// 1, 2, 4 or 8. 
//
//		FOR j := 0 to 15
//			i := j*32
//			MEM[base_addr + SignExtend(vindex[i+31:i])*scale] := a[i+31:i]
//		ENDFOR
//
// Instruction: 'VSCATTERDPS'. Intrinsic: '_mm512_i32scatter_ps'.
// Requires KNCNI.
func I32scatterPs(base_addr uintptr, vindex M512i, a M512, scale int)  {
	i32scatterPs(uintptr(base_addr), [64]byte(vindex), [16]float32(a), scale)
}

func i32scatterPs(base_addr uintptr, vindex [64]byte, a [16]float32, scale int) 


// MaskI32scatterPs: Scatter single-precision (32-bit) floating-point elements
// from 'a' into memory using 32-bit indices. 32-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 32-bit element in
// 'vindex' (each index is scaled by the factor in 'scale') subject to mask 'k'
// (elements are not stored when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[i+31:i])*scale] := a[i+31:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VSCATTERDPS'. Intrinsic: '_mm512_mask_i32scatter_ps'.
// Requires KNCNI.
func MaskI32scatterPs(base_addr uintptr, k Mmask16, vindex M512i, a M512, scale int)  {
	maskI32scatterPs(uintptr(base_addr), uint16(k), [64]byte(vindex), [16]float32(a), scale)
}

func maskI32scatterPs(base_addr uintptr, k uint16, vindex [64]byte, a [16]float32, scale int) 


// I64extgatherEpi32lo: Up-converts 8 single-precision (32-bit) memory
// locations starting at location 'mv' at packed 64-bit integer indices stored
// in 'index' scaled by 'scale' using 'conv' to 32-bit integer elements and
// stores them in 'dst'. 'hint' indicates to the processor whether the data is
// non-temporal. 
//
//		FOR j := 0 to 7
//			addr := MEM[mv + index[j] * scale]
//			i := j*32
//			CASE conv OF
//			_MM_UPCONV_EPI32_NONE:
//				dst[i+31:i] := addr[i+31:i]
//			_MM_UPCONV_EPI32_UINT8:
//				n := j*8
//				dst[i+31:i] := UInt8ToInt32(addr[n+7:n])
//			_MM_UPCONV_EPI32_SINT8:
//				n := j*8
//				dst[i+31:i] := SInt8ToInt32(addr[n+7:n])
//			_MM_UPCONV_EPI32_UINT16:
//				n := j*16
//				dst[i+31:i] := UInt16ToInt32(addr[n+15:n])
//			_MM_UPCONV_EPI32_SINT16:
//				n := j*16
//				dst[i+31:i] := SInt16ToInt32(addr[n+15:n])
//			ESAC
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_i64extgather_epi32lo'.
// Requires KNCNI.
func I64extgatherEpi32lo(index M512i, mv uintptr, conv MMUPCONVEPI32ENUM, scale int, hint int) M512i {
	return M512i(i64extgatherEpi32lo([64]byte(index), uintptr(mv), conv, scale, hint))
}

func i64extgatherEpi32lo(index [64]byte, mv uintptr, conv MMUPCONVEPI32ENUM, scale int, hint int) [64]byte


// MaskI64extgatherEpi32lo: Up-converts 8 single-precision (32-bit) memory
// locations starting at location 'mv' at packed 64-bit integer indices stored
// in 'index' scaled by 'scale' using 'conv' to 32-bit integer elements and
// stores them in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 'hint' indicates to the
// processor whether the data is non-temporal. 
//
//		FOR j := 0 to 7
//			addr := MEM[mv + index[j] * scale]
//			i := j*32
//			IF k[j]
//				CASE conv OF
//				_MM_UPCONV_EPI32_NONE:
//					dst[i+31:i] := addr[i+31:i]
//				_MM_UPCONV_EPI32_UINT8:
//					n := j*8
//					dst[i+31:i] := UInt8ToInt32(addr[n+7:n])
//				_MM_UPCONV_EPI32_SINT8:
//					n := j*8
//					dst[i+31:i] := SInt8ToInt32(addr[n+7:n])
//				_MM_UPCONV_EPI32_UINT16:
//					n := j*16
//					dst[i+31:i] := UInt16ToInt32(addr[n+15:n])
//				_MM_UPCONV_EPI32_SINT16:
//					n := j*16
//					dst[i+31:i] := SInt16ToInt32(addr[n+15:n])
//				ESAC
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_i64extgather_epi32lo'.
// Requires KNCNI.
func MaskI64extgatherEpi32lo(src M512i, k Mmask8, index M512i, mv uintptr, conv MMUPCONVEPI32ENUM, scale int, hint int) M512i {
	return M512i(maskI64extgatherEpi32lo([64]byte(src), uint8(k), [64]byte(index), uintptr(mv), conv, scale, hint))
}

func maskI64extgatherEpi32lo(src [64]byte, k uint8, index [64]byte, mv uintptr, conv MMUPCONVEPI32ENUM, scale int, hint int) [64]byte


// I64extgatherEpi64: Up-converts 8 double-precision (64-bit) memory locations
// starting at location 'mv' at packed 64-bit integer indices stored in 'index'
// scaled by 'scale' using 'conv' to 64-bit integer elements and stores them in
// 'dst'. 'hint' indicates to the processor whether the load is non-temporal. 
//
//		FOR j := 0 to 7
//			i := j*64
//			addr := MEM[mv + index[j] * scale]
//			CASE conv OF
//			_MM_UPCONV_EPI64_NONE: dst[i+63:i] := addr[i+63:i]
//			ESAC
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_i64extgather_epi64'.
// Requires KNCNI.
func I64extgatherEpi64(index M512i, mv uintptr, conv MMUPCONVEPI64ENUM, scale int, hint int) M512i {
	return M512i(i64extgatherEpi64([64]byte(index), uintptr(mv), conv, scale, hint))
}

func i64extgatherEpi64(index [64]byte, mv uintptr, conv MMUPCONVEPI64ENUM, scale int, hint int) [64]byte


// MaskI64extgatherEpi64: Up-converts 8 double-precision (64-bit) memory
// locations starting at location 'mv' at packed 64-bit integer indices stored
// in 'index' scaled by 'scale' using 'conv' to 64-bit integer elements and
// stores them in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 'hint' indicates to the
// processor whether the load is non-temporal. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				addr := MEM[mv + index[j] * scale]
//				CASE conv OF
//				_MM_UPCONV_EPI64_NONE: dst[i+63:i] := addr[i+63:i]
//				ESAC
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_i64extgather_epi64'.
// Requires KNCNI.
func MaskI64extgatherEpi64(src M512i, k Mmask8, index M512i, mv uintptr, conv MMUPCONVEPI64ENUM, scale int, hint int) M512i {
	return M512i(maskI64extgatherEpi64([64]byte(src), uint8(k), [64]byte(index), uintptr(mv), conv, scale, hint))
}

func maskI64extgatherEpi64(src [64]byte, k uint8, index [64]byte, mv uintptr, conv MMUPCONVEPI64ENUM, scale int, hint int) [64]byte


// I64extgatherPd: Up-converts 8 double-precision (64-bit) floating-point
// elements stored in memory starting at location 'mv' at packed 64-bit integer
// indices stored in 'index' scaled by 'scale' using 'conv' to 64-bit
// floating-point elements and stores them in 'dst'. 'hint' indicates to the
// processor whether the data is non-temporal. 
//
//		FOR j := 0 to 7
//			addr := MEM[mv + index[j] * scale]
//			i := j*64
//			CASE conv OF
//			_MM_UPCONV_PD_NONE: dst[i+63:i] := addr[i+63:i]
//			ESAC
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_i64extgather_pd'.
// Requires KNCNI.
func I64extgatherPd(index M512i, mv uintptr, conv MMUPCONVPDENUM, scale int, hint int) M512d {
	return M512d(i64extgatherPd([64]byte(index), uintptr(mv), conv, scale, hint))
}

func i64extgatherPd(index [64]byte, mv uintptr, conv MMUPCONVPDENUM, scale int, hint int) [8]float64


// MaskI64extgatherPd: Up-converts 8 double-precision (64-bit) floating-point
// elements stored in memory starting at location 'mv' at packed 64-bit integer
// indices stored in 'index' scaled by 'scale' using 'conv' to 64-bit
// floating-point elements and stores them in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set).
// 'hint' indicates to the processor whether the data is non-temporal. 
//
//		FOR j := 0 to 7
//			addr := MEM[mv + index[j] * scale]
//			i := j*64
//			IF k[j]
//				CASE conv OF
//				_MM_UPCONV_PD_NONE: dst[i+63:i] := addr[i+63:i]
//				ESAC
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_i64extgather_pd'.
// Requires KNCNI.
func MaskI64extgatherPd(src M512d, k Mmask8, index M512i, mv uintptr, conv MMUPCONVPDENUM, scale int, hint int) M512d {
	return M512d(maskI64extgatherPd([8]float64(src), uint8(k), [64]byte(index), uintptr(mv), conv, scale, hint))
}

func maskI64extgatherPd(src [8]float64, k uint8, index [64]byte, mv uintptr, conv MMUPCONVPDENUM, scale int, hint int) [8]float64


// I64extgatherPslo: Up-converts 8 memory locations starting at location 'mv'
// at packed 64-bit integer indices stored in 'index' scaled by 'scale' using
// 'conv' to single-precision (32-bit) floating-point elements and stores them
// in the lower half of 'dst'. 'hint' indicates to the processor whether the
// load is non-temporal. 
//
//		FOR j := 0 to 7
//			addr := MEM[mv + index[j] * scale]
//			i := j*32
//			CASE conv OF
//			_MM_UPCONV_PS_NONE:
//				dst[i+31:i] := addr[i+31:i]
//			_MM_UPCONV_PS_FLOAT16:
//				n := j*16
//				dst[i+31:i] := Float16ToFloat32(addr[n+15:n])
//			_MM_UPCONV_PS_UINT8:
//				n := j*8
//				dst[i+31:i] := UInt8ToFloat32(addr[n+7:n])
//			_MM_UPCONV_PS_SINT8:
//				n := j*8
//				dst[i+31:i] := SInt8ToFloat32(addr[n+7:n])
//			_MM_UPCONV_PS_UINT16:
//				n := j*16
//				dst[i+31:i] := UInt16ToFloat32(addr[n+15:n])
//			_MM_UPCONV_PS_SINT16:
//				n := j*16
//				dst[i+31:i] := SInt16ToFloat32(addr[n+15:n])
//			ESAC
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_i64extgather_pslo'.
// Requires KNCNI.
func I64extgatherPslo(index M512i, mv uintptr, conv MMUPCONVPSENUM, scale int, hint int) M512 {
	return M512(i64extgatherPslo([64]byte(index), uintptr(mv), conv, scale, hint))
}

func i64extgatherPslo(index [64]byte, mv uintptr, conv MMUPCONVPSENUM, scale int, hint int) [16]float32


// MaskI64extgatherPslo: Up-converts 8 memory locations starting at location
// 'mv' at packed 64-bit integer indices stored in 'index' scaled by 'scale'
// using 'conv' to single-precision (32-bit) floating-point elements and stores
// them in the lower half of 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 'hint' indicates to
// the processor whether the load is non-temporal. 
//
//		FOR j := 0 to 7
//			addr := MEM[mv + index[j] * scale]
//			i := j*32
//			IF k[j]
//				CASE conv OF
//				_MM_UPCONV_PS_NONE:
//					dst[i+31:i] := addr[i+31:i]
//				_MM_UPCONV_PS_FLOAT16:
//					n := j*16
//					dst[i+31:i] := Float16ToFloat32(addr[n+15:n])
//				_MM_UPCONV_PS_UINT8:
//					n := j*8
//					dst[i+31:i] := UInt8ToFloat32(addr[n+7:n])
//				_MM_UPCONV_PS_SINT8:
//					n := j*8
//					dst[i+31:i] := SInt8ToFloat32(addr[n+7:n])
//				_MM_UPCONV_PS_UINT16:
//					n := j*16
//					dst[i+31:i] := UInt16ToFloat32(addr[n+15:n])
//				_MM_UPCONV_PS_SINT16:
//					n := j*16
//					dst[i+31:i] := SInt16ToFloat32(addr[n+15:n])
//				ESAC
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_i64extgather_pslo'.
// Requires KNCNI.
func MaskI64extgatherPslo(src M512, k Mmask8, index M512i, mv uintptr, conv MMUPCONVPSENUM, scale int, hint int) M512 {
	return M512(maskI64extgatherPslo([16]float32(src), uint8(k), [64]byte(index), uintptr(mv), conv, scale, hint))
}

func maskI64extgatherPslo(src [16]float32, k uint8, index [64]byte, mv uintptr, conv MMUPCONVPSENUM, scale int, hint int) [16]float32


// I64extscatterEpi32lo: Down-converts the low 8 packed 32-bit integer elements
// in 'v1' using 'conv' and stores them in memory locations starting at
// location 'mv' at packed 64-bit integer indices stored in 'index' scaled by
// 'scale'. 'hint' indicates to the processor whether the data is non-temporal. 
//
//		FOR j := 0 to 7
//			addr := MEM[mv + index[j] * scale]
//			i := j*64
//			CASE conv OF
//			_MM_DOWNCONV_EPI32_NONE:
//				addr[i+31:i] := v1[i+31:i]
//			_MM_DOWNCONV_EPI32_UINT8:
//				n := j*8
//				addr[n+7:n] := UInt32ToUInt8(v1[i+31:i])
//			_MM_DOWNCONV_EPI32_SINT8:
//				n := j*8
//				addr[n+7:n] := SInt32ToSInt8(v1[i+31:i])
//			_MM_DOWNCONV_EPI32_UINT16:
//				n := j*16
//				addr[n+15:n] := UInt32ToUInt16(v1[i+31:i])
//			_MM_DOWNCONV_EPI32_SINT16:
//				n := j*16
//				addr[n+15:n] := SInt32ToSInt16(v1[n+15:n])
//			ESAC
//		ENDFOR
//
// Instruction: '...'. Intrinsic: '_mm512_i64extscatter_epi32lo'.
// Requires KNCNI.
func I64extscatterEpi32lo(mv uintptr, index M512i, v1 M512i, conv MMDOWNCONVEPI32ENUM, scale int, hint int)  {
	i64extscatterEpi32lo(uintptr(mv), [64]byte(index), [64]byte(v1), conv, scale, hint)
}

func i64extscatterEpi32lo(mv uintptr, index [64]byte, v1 [64]byte, conv MMDOWNCONVEPI32ENUM, scale int, hint int) 


// MaskI64extscatterEpi32lo: Down-converts the low 8 packed 32-bit integer
// elements in 'v1' using 'conv' and stores them in memory locations starting
// at location 'mv' at packed 64-bit integer indices stored in 'index' scaled
// by 'scale'. Elements are written to memory using writemask 'k' (elements are
// only written when the corresponding mask bit is set; otherwise, the memory
// location is left unchanged). 'hint' indicates to the processor whether the
// data is non-temporal. 
//
//		FOR j := 0 to 7
//			addr := MEM[mv + index[j] * scale]
//			i := j*64
//			IF k[j]
//				CASE conv OF
//				_MM_DOWNCONV_EPI32_NONE:
//					addr[i+31:i] := v1[i+31:i]
//				_MM_DOWNCONV_EPI32_UINT8:
//					n := j*8
//					addr[n+7:n] := UInt32ToUInt8(v1[i+31:i])
//				_MM_DOWNCONV_EPI32_SINT8:
//					n := j*8
//					addr[n+7:n] := SInt32ToSInt8(v1[i+31:i])
//				_MM_DOWNCONV_EPI32_UINT16:
//					n := j*16
//					addr[n+15:n] := UInt32ToUInt16(v1[i+31:i])
//				_MM_DOWNCONV_EPI32_SINT16:
//					n := j*16
//					addr[n+15:n] := SInt32ToSInt16(v1[n+15:n])
//				ESAC
//			FI
//		ENDFOR
//
// Instruction: '...'. Intrinsic: '_mm512_mask_i64extscatter_epi32lo'.
// Requires KNCNI.
func MaskI64extscatterEpi32lo(mv uintptr, k Mmask8, index M512i, v1 M512i, conv MMDOWNCONVEPI32ENUM, scale int, hint int)  {
	maskI64extscatterEpi32lo(uintptr(mv), uint8(k), [64]byte(index), [64]byte(v1), conv, scale, hint)
}

func maskI64extscatterEpi32lo(mv uintptr, k uint8, index [64]byte, v1 [64]byte, conv MMDOWNCONVEPI32ENUM, scale int, hint int) 


// I64extscatterEpi64: Down-converts 8 packed 64-bit integer elements in 'v1'
// using 'conv' and stores them in memory locations starting at location 'mv'
// at packed 64-bit integer indices stored in 'index' scaled by 'scale'. 'hint'
// indicates to the processor whether the load is non-temporal. 
//
//		FOR j := 0 to 7
//			addr := MEM[mv + index[j] * scale]
//			i := j*64
//			CASE conv OF
//			_MM_DOWNCONV_EPI64_NONE: addr[i+63:i] := v1[i+63:i]
//			ESAC
//		ENDFOR
//
// Instruction: '...'. Intrinsic: '_mm512_i64extscatter_epi64'.
// Requires KNCNI.
func I64extscatterEpi64(mv uintptr, index M512i, v1 M512i, conv MMDOWNCONVEPI64ENUM, scale int, hint int)  {
	i64extscatterEpi64(uintptr(mv), [64]byte(index), [64]byte(v1), conv, scale, hint)
}

func i64extscatterEpi64(mv uintptr, index [64]byte, v1 [64]byte, conv MMDOWNCONVEPI64ENUM, scale int, hint int) 


// MaskI64extscatterEpi64: Down-converts 8 packed 64-bit integer elements in
// 'v1' using 'conv' and stores them in memory locations starting at location
// 'mv' at packed 64-bit integer indices stored in 'index' scaled by 'scale'.
// Only those elements whose corresponding mask bit is set in writemask 'k' are
// written to memory. 
//
//		FOR j := 0 to 7
//			IF k[j]
//				addr := MEM[mv + index[j] * scale]
//				i := j*64
//				CASE conv OF
//				_MM_DOWNCONV_EPI64_NONE: addr[i+63:i] := v1[i+63:i]
//				ESAC
//			FI
//		ENDFOR
//
// Instruction: '...'. Intrinsic: '_mm512_mask_i64extscatter_epi64'.
// Requires KNCNI.
func MaskI64extscatterEpi64(mv uintptr, k Mmask8, index M512i, v1 M512i, conv MMDOWNCONVEPI64ENUM, scale int, hint int)  {
	maskI64extscatterEpi64(uintptr(mv), uint8(k), [64]byte(index), [64]byte(v1), conv, scale, hint)
}

func maskI64extscatterEpi64(mv uintptr, k uint8, index [64]byte, v1 [64]byte, conv MMDOWNCONVEPI64ENUM, scale int, hint int) 


// I64extscatterPd: Down-converts 8 packed double-precision (64-bit)
// floating-point elements in 'v1' using 'conv' and stores them in memory
// locations starting at location 'mv' at packed 64-bit integer indices stored
// in 'index' scaled by 'scale'. 'hint' indicates to the processor whether the
// data is non-temporal. 
//
//		FOR j := 0 to 7
//			addr := MEM[mv + index[j] * scale]
//			i := j*64
//			CASE conv OF
//			_MM_DOWNCONV_EPI64_NONE:
//				addr[i+63:i] := v1[i+63:i]
//			ESAC
//		ENDFOR
//
// Instruction: '...'. Intrinsic: '_mm512_i64extscatter_pd'.
// Requires KNCNI.
func I64extscatterPd(mv uintptr, index M512i, v1 M512d, conv MMDOWNCONVPDENUM, scale int, hint int)  {
	i64extscatterPd(uintptr(mv), [64]byte(index), [8]float64(v1), conv, scale, hint)
}

func i64extscatterPd(mv uintptr, index [64]byte, v1 [8]float64, conv MMDOWNCONVPDENUM, scale int, hint int) 


// MaskI64extscatterPd: Down-converts 8 packed double-precision (64-bit)
// floating-point elements in 'v1' using 'conv' and stores them in memory
// locations starting at location 'mv' at packed 64-bit integer indices stored
// in 'index' scaled by 'scale'. Elements are written to memory using writemask
// 'k' (elements are not stored to memory when the corresponding mask bit is
// not set; the memory location is left unchagned). 'hint' indicates to the
// processor whether the data is non-temporal. 
//
//		FOR j := 0 to 7
//			addr := MEM[mv + index[j] * scale]
//			i := j*64
//			IF k[j]
//				CASE conv OF
//				_MM_DOWNCONV_EPI64_NONE:
//					addr[i+63:i] := v1[i+63:i]
//				ESAC
//			FI
//		ENDFOR
//
// Instruction: '...'. Intrinsic: '_mm512_mask_i64extscatter_pd'.
// Requires KNCNI.
func MaskI64extscatterPd(mv uintptr, k Mmask8, index M512i, v1 M512d, conv MMDOWNCONVPDENUM, scale int, hint int)  {
	maskI64extscatterPd(uintptr(mv), uint8(k), [64]byte(index), [8]float64(v1), conv, scale, hint)
}

func maskI64extscatterPd(mv uintptr, k uint8, index [64]byte, v1 [8]float64, conv MMDOWNCONVPDENUM, scale int, hint int) 


// I64extscatterPslo: Down-converts 8 packed single-precision (32-bit)
// floating-point elements in 'v1' using 'conv' and stores them in memory
// locations starting at location 'mv' at packed 64-bit integer indices stored
// in 'index' scaled by 'scale'. 'hint' indicates to the processor whether the
// data is non-temporal. 
//
//		FOR j := 0 to 7
//			addr := MEM[mv + index[j] * scale]
//			i := j*32
//			CASE conv OF
//			_MM_DOWNCONV_PS_NONE:
//				addr[i+31:i] := v1[i+31:i]
//			_MM_DOWNCONV_PS_FLOAT16:
//				n := j*16
//				addr[n+15:n] := Float32ToFloat16(v1[i+31:i])
//			_MM_DOWNCONV_PS_UINT8:
//				n := j*8
//				addr[n+7:n] := Float32ToUInt8(v1[i+31:i])
//			_MM_DOWNCONV_PS_SINT8:
//				n := j*8
//				addr[n+7:n] := Float32ToSInt8(v1[i+31:i])
//			_MM_DOWNCONV_PS_UINT16:
//				n := j*16
//				addr[n+15:n] := Float32ToUInt16(v1[i+31:i])
//			_MM_DOWNCONV_PS_SINT16:
//				n := j*16
//				addr[n+15:n] := Float32ToSInt16(v1[i+31:i])
//			ESAC
//		ENDFOR
//
// Instruction: '...'. Intrinsic: '_mm512_i64extscatter_pslo'.
// Requires KNCNI.
func I64extscatterPslo(mv uintptr, index M512i, v1 M512, conv MMDOWNCONVPSENUM, scale int, hint int)  {
	i64extscatterPslo(uintptr(mv), [64]byte(index), [16]float32(v1), conv, scale, hint)
}

func i64extscatterPslo(mv uintptr, index [64]byte, v1 [16]float32, conv MMDOWNCONVPSENUM, scale int, hint int) 


// MaskI64extscatterPslo: Down-converts 8 packed single-precision (32-bit)
// floating-point elements in 'v1' using 'conv' and stores them in memory
// locations starting at location 'mv' at packed 64-bit integer indices stored
// in 'index' scaled by 'scale'. Elements are only written when the
// corresponding mask bit is set in 'k'; otherwise, elements are unchanged in
// memory. 'hint' indicates to the processor whether the data is non-temporal. 
//
//		FOR j := 0 to 7
//			addr := MEM[mv + index[j] * scale]
//			i := j*32
//			IF k[j]
//				CASE conv OF
//				_MM_DOWNCONV_PS_NONE:
//					addr[i+31:i] := v[i+31:i]
//				_MM_DOWNCONV_PS_FLOAT16:
//					n := j*16
//					addr[n+15:n] := Float32ToFloat16(v1[i+31:i])
//				_MM_DOWNCONV_PS_UINT8:
//					n := j*8
//					addr[n+7:n] := Float32ToUInt8(v1[i+31:i])
//				_MM_DOWNCONV_PS_SINT8:
//					n := j*8
//					addr[n+7:n] := Float32ToSInt8(v1[i+31:i])
//				_MM_DOWNCONV_PS_UINT16:
//					n := j*16
//					addr[n+15:n] := Float32ToUInt16(v1[i+31:i])
//				_MM_DOWNCONV_PS_SINT16:
//					n := j*16
//					addr[n+15:n] := Float32ToSInt16(v1[i+31:i])
//				ESAC
//			FI
//		ENDFOR
//
// Instruction: '...'. Intrinsic: '_mm512_mask_i64extscatter_pslo'.
// Requires KNCNI.
func MaskI64extscatterPslo(mv uintptr, k Mmask8, index M512i, v1 M512, conv MMDOWNCONVPSENUM, scale int, hint int)  {
	maskI64extscatterPslo(uintptr(mv), uint8(k), [64]byte(index), [16]float32(v1), conv, scale, hint)
}

func maskI64extscatterPslo(mv uintptr, k uint8, index [64]byte, v1 [16]float32, conv MMDOWNCONVPSENUM, scale int, hint int) 


// I64gatherEpi32: Gather 32-bit integers from memory using 64-bit indices.
// 32-bit elements are loaded from addresses starting at 'base_addr' and offset
// by each 64-bit element in 'vindex' (each index is scaled by the factor in
// 'scale'). Gathered elements are merged into 'dst'. 'scale' should be 1, 2, 4
// or 8. 
//
//		FOR j := 0 to 7
//			i := j*32
//			m := j*64
//			dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPGATHERQD'. Intrinsic: '_mm512_i64gather_epi32'.
// Requires AVX512F.
func I64gatherEpi32(vindex M512i, base_addr uintptr, scale int) M256i {
	return M256i(i64gatherEpi32([64]byte(vindex), uintptr(base_addr), scale))
}

func i64gatherEpi32(vindex [64]byte, base_addr uintptr, scale int) [32]byte


// MaskI64gatherEpi32: Gather 32-bit integers from memory using 64-bit indices.
// 32-bit elements are loaded from addresses starting at 'base_addr' and offset
// by each 64-bit element in 'vindex' (each index is scaled by the factor in
// 'scale'). Gathered elements are merged into 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*32
//			m := j*64
//			IF k[j]
//				dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VPGATHERQD'. Intrinsic: '_mm512_mask_i64gather_epi32'.
// Requires AVX512F.
func MaskI64gatherEpi32(src M256i, k Mmask8, vindex M512i, base_addr uintptr, scale int) M256i {
	return M256i(maskI64gatherEpi32([32]byte(src), uint8(k), [64]byte(vindex), uintptr(base_addr), scale))
}

func maskI64gatherEpi32(src [32]byte, k uint8, vindex [64]byte, base_addr uintptr, scale int) [32]byte


// I64gatherEpi32lo: Loads 8 32-bit integer memory locations starting at
// location 'mv' at packed 64-bit integer indices stored in 'index' scaled by
// 'scale' to 'dst'. 
//
//		FOR j := 0 to 7
//			addr := MEM[mv + index[j] * scale]
//			i := j*32
//			dst[i+31:i] := addr[i+31:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_i64gather_epi32lo'.
// Requires KNCNI.
func I64gatherEpi32lo(index M512i, mv uintptr, scale int) M512i {
	return M512i(i64gatherEpi32lo([64]byte(index), uintptr(mv), scale))
}

func i64gatherEpi32lo(index [64]byte, mv uintptr, scale int) [64]byte


// MaskI64gatherEpi32lo: Loads 8 32-bit integer memory locations starting at
// location 'mv' at packed 64-bit integer indices stored in 'index' scaled by
// 'scale' to 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				addr := MEM[mv + index[j] * scale]
//				dst[i+31:i] := addr[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_i64gather_epi32lo'.
// Requires KNCNI.
func MaskI64gatherEpi32lo(src M512i, k Mmask8, index M512i, mv uintptr, scale int) M512i {
	return M512i(maskI64gatherEpi32lo([64]byte(src), uint8(k), [64]byte(index), uintptr(mv), scale))
}

func maskI64gatherEpi32lo(src [64]byte, k uint8, index [64]byte, mv uintptr, scale int) [64]byte


// I64gatherEpi64: Gather 64-bit integers from memory using 64-bit indices.
// 64-bit elements are loaded from addresses starting at 'base_addr' and offset
// by each 64-bit element in 'vindex' (each index is scaled by the factor in
// 'scale'). Gathered elements are merged into 'dst'. 'scale' should be 1, 2, 4
// or 8. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPGATHERQQ'. Intrinsic: '_mm512_i64gather_epi64'.
// Requires AVX512F.
func I64gatherEpi64(vindex M512i, base_addr uintptr, scale int) M512i {
	return M512i(i64gatherEpi64([64]byte(vindex), uintptr(base_addr), scale))
}

func i64gatherEpi64(vindex [64]byte, base_addr uintptr, scale int) [64]byte


// MaskI64gatherEpi64: Gather 64-bit integers from memory using 64-bit indices.
// 64-bit elements are loaded from addresses starting at 'base_addr' and offset
// by each 64-bit element in 'vindex' (each index is scaled by the factor in
// 'scale'). Gathered elements are merged into 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//		dst[MAX:512] := 0
//
// Instruction: 'VPGATHERQQ'. Intrinsic: '_mm512_mask_i64gather_epi64'.
// Requires AVX512F.
func MaskI64gatherEpi64(src M512i, k Mmask8, vindex M512i, base_addr uintptr, scale int) M512i {
	return M512i(maskI64gatherEpi64([64]byte(src), uint8(k), [64]byte(vindex), uintptr(base_addr), scale))
}

func maskI64gatherEpi64(src [64]byte, k uint8, vindex [64]byte, base_addr uintptr, scale int) [64]byte


// I64gatherPd: Gather double-precision (64-bit) floating-point elements from
// memory using 64-bit indices. 64-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 64-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst'. 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGATHERQPD'. Intrinsic: '_mm512_i64gather_pd'.
// Requires AVX512F.
func I64gatherPd(vindex M512i, base_addr uintptr, scale int) M512d {
	return M512d(i64gatherPd([64]byte(vindex), uintptr(base_addr), scale))
}

func i64gatherPd(vindex [64]byte, base_addr uintptr, scale int) [8]float64


// MaskI64gatherPd: Gather double-precision (64-bit) floating-point elements
// from memory using 64-bit indices. 64-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 64-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//		dst[MAX:512] := 0
//
// Instruction: 'VGATHERQPD'. Intrinsic: '_mm512_mask_i64gather_pd'.
// Requires AVX512F.
func MaskI64gatherPd(src M512d, k Mmask8, vindex M512i, base_addr uintptr, scale int) M512d {
	return M512d(maskI64gatherPd([8]float64(src), uint8(k), [64]byte(vindex), uintptr(base_addr), scale))
}

func maskI64gatherPd(src [8]float64, k uint8, vindex [64]byte, base_addr uintptr, scale int) [8]float64


// I64gatherPs: Gather single-precision (32-bit) floating-point elements from
// memory using 64-bit indices. 32-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 64-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst'. 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*32
//			m := j*64
//			dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGATHERQPS'. Intrinsic: '_mm512_i64gather_ps'.
// Requires AVX512F.
func I64gatherPs(vindex M512i, base_addr uintptr, scale int) M256 {
	return M256(i64gatherPs([64]byte(vindex), uintptr(base_addr), scale))
}

func i64gatherPs(vindex [64]byte, base_addr uintptr, scale int) [8]float32


// MaskI64gatherPs: Gather single-precision (32-bit) floating-point elements
// from memory using 64-bit indices. 32-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 64-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*32
//			m := j*64
//			IF k[j]
//				dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VGATHERQPS'. Intrinsic: '_mm512_mask_i64gather_ps'.
// Requires AVX512F.
func MaskI64gatherPs(src M256, k Mmask8, vindex M512i, base_addr uintptr, scale int) M256 {
	return M256(maskI64gatherPs([8]float32(src), uint8(k), [64]byte(vindex), uintptr(base_addr), scale))
}

func maskI64gatherPs(src [8]float32, k uint8, vindex [64]byte, base_addr uintptr, scale int) [8]float32


// I64gatherPslo: Loads 8 single-precision (32-bit) floating-point memory
// locations starting at location 'mv' at packed 64-bit integer indices stored
// in 'index' scaled by 'scale' to 'dst'. 
//
//		FOR j := 0 to 7
//			addr := MEM[mv + index[j] * scale]
//			i := j*32
//			dst[i+31:i] := addr[i+31:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_i64gather_pslo'.
// Requires KNCNI.
func I64gatherPslo(index M512i, mv uintptr, scale int) M512 {
	return M512(i64gatherPslo([64]byte(index), uintptr(mv), scale))
}

func i64gatherPslo(index [64]byte, mv uintptr, scale int) [16]float32


// MaskI64gatherPslo: Loads 8 single-precision (32-bit) floating-point memory
// locations starting at location 'mv' at packed 64-bit integer indices stored
// in 'index' scaled by 'scale' to 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				addr := MEM[mv + index[j] * scale]
//				dst[i+31:i] := addr[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_i64gather_pslo'.
// Requires KNCNI.
func MaskI64gatherPslo(src M512, k Mmask8, index M512i, mv uintptr, scale int) M512 {
	return M512(maskI64gatherPslo([16]float32(src), uint8(k), [64]byte(index), uintptr(mv), scale))
}

func maskI64gatherPslo(src [16]float32, k uint8, index [64]byte, mv uintptr, scale int) [16]float32


// I64scatterEpi32: Scatter 32-bit integers from 'a' into memory using 64-bit
// indices. 32-bit elements are stored at addresses starting at 'base_addr' and
// offset by each 64-bit element in 'vindex' (each index is scaled by the
// factor in 'scale'). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*32
//			l := j*64
//			MEM[base_addr + SignExtend(vindex[l+63:l])*scale] := a[i+31:i]
//		ENDFOR
//
// Instruction: 'VPSCATTERQD'. Intrinsic: '_mm512_i64scatter_epi32'.
// Requires AVX512F.
func I64scatterEpi32(base_addr uintptr, vindex M512i, a M256i, scale int)  {
	i64scatterEpi32(uintptr(base_addr), [64]byte(vindex), [32]byte(a), scale)
}

func i64scatterEpi32(base_addr uintptr, vindex [64]byte, a [32]byte, scale int) 


// MaskI64scatterEpi32: Scatter 32-bit integers from 'a' into memory using
// 64-bit indices. 32-bit elements are stored at addresses starting at
// 'base_addr' and offset by each 64-bit element in 'vindex' (each index is
// scaled by the factor in 'scale') subject to mask 'k' (elements are not
// stored when the corresponding mask bit is not set). 'scale' should be 1, 2,
// 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*32
//			l := j*64
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[l+63:l])*scale] := a[i+31:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPSCATTERQD'. Intrinsic: '_mm512_mask_i64scatter_epi32'.
// Requires AVX512F.
func MaskI64scatterEpi32(base_addr uintptr, k Mmask8, vindex M512i, a M256i, scale int)  {
	maskI64scatterEpi32(uintptr(base_addr), uint8(k), [64]byte(vindex), [32]byte(a), scale)
}

func maskI64scatterEpi32(base_addr uintptr, k uint8, vindex [64]byte, a [32]byte, scale int) 


// I64scatterEpi32lo: Stores 8 packed 32-bit integer elements in 'v1' in memory
// locations starting at location 'mv' at packed 64-bit integer indices stored
// in 'index' scaled by 'scale'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			addr := MEM[mv + index[j] * scale]
//			addr[i+31:i] := v1[i+31:i]
//		ENDFOR
//
// Instruction: '...'. Intrinsic: '_mm512_i64scatter_epi32lo'.
// Requires KNCNI.
func I64scatterEpi32lo(mv uintptr, index M512i, v1 M512i, scale int)  {
	i64scatterEpi32lo(uintptr(mv), [64]byte(index), [64]byte(v1), scale)
}

func i64scatterEpi32lo(mv uintptr, index [64]byte, v1 [64]byte, scale int) 


// MaskI64scatterEpi32lo: Stores 8 packed 32-bit integer elements in 'v1' in
// memory locations starting at location 'mv' at packed 64-bit integer indices
// stored in 'index' scaled by 'scale' using writemask 'k' (elements are only
// written to memory when the corresponding mask bit is set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				addr := MEM[mv + index[j] * scale]
//				addr[i+31:i] := v1[i+31:i]
//			FI	
//		ENDFOR
//
// Instruction: '...'. Intrinsic: '_mm512_mask_i64scatter_epi32lo'.
// Requires KNCNI.
func MaskI64scatterEpi32lo(mv uintptr, k Mmask8, index M512i, v1 M512i, scale int)  {
	maskI64scatterEpi32lo(uintptr(mv), uint8(k), [64]byte(index), [64]byte(v1), scale)
}

func maskI64scatterEpi32lo(mv uintptr, k uint8, index [64]byte, v1 [64]byte, scale int) 


// I64scatterEpi64: Scatter 64-bit integers from 'a' into memory using 64-bit
// indices. 64-bit elements are stored at addresses starting at 'base_addr' and
// offset by each 64-bit element in 'vindex' (each index is scaled by the
// factor in 'scale'). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*64
//			MEM[base_addr + SignExtend(vindex[i+63:i])*scale] := a[i+63:i]
//		ENDFOR
//
// Instruction: 'VPSCATTERQQ'. Intrinsic: '_mm512_i64scatter_epi64'.
// Requires AVX512F.
func I64scatterEpi64(base_addr uintptr, vindex M512i, a M512i, scale int)  {
	i64scatterEpi64(uintptr(base_addr), [64]byte(vindex), [64]byte(a), scale)
}

func i64scatterEpi64(base_addr uintptr, vindex [64]byte, a [64]byte, scale int) 


// MaskI64scatterEpi64: Scatter 64-bit integers from 'a' into memory using
// 64-bit indices. 64-bit elements are stored at addresses starting at
// 'base_addr' and offset by each 64-bit element in 'vindex' (each index is
// scaled by the factor in 'scale') subject to mask 'k' (elements are not
// stored when the corresponding mask bit is not set). 'scale' should be 1, 2,
// 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[i+63:i])*scale] := a[i+63:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPSCATTERQQ'. Intrinsic: '_mm512_mask_i64scatter_epi64'.
// Requires AVX512F.
func MaskI64scatterEpi64(base_addr uintptr, k Mmask8, vindex M512i, a M512i, scale int)  {
	maskI64scatterEpi64(uintptr(base_addr), uint8(k), [64]byte(vindex), [64]byte(a), scale)
}

func maskI64scatterEpi64(base_addr uintptr, k uint8, vindex [64]byte, a [64]byte, scale int) 


// I64scatterPd: Scatter double-precision (64-bit) floating-point elements from
// 'a' into memory using 64-bit indices. 64-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 64-bit element in
// 'vindex' (each index is scaled by the factor in 'scale'). 'scale' should be
// 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*64
//			MEM[base_addr + SignExtend(vindex[i+63:i])*scale] := a[i+63:i]
//		ENDFOR
//
// Instruction: 'VSCATTERQPD'. Intrinsic: '_mm512_i64scatter_pd'.
// Requires AVX512F.
func I64scatterPd(base_addr uintptr, vindex M512i, a M512d, scale int)  {
	i64scatterPd(uintptr(base_addr), [64]byte(vindex), [8]float64(a), scale)
}

func i64scatterPd(base_addr uintptr, vindex [64]byte, a [8]float64, scale int) 


// MaskI64scatterPd: Scatter double-precision (64-bit) floating-point elements
// from 'a' into memory using 64-bit indices. 64-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 64-bit element in
// 'vindex' (each index is scaled by the factor in 'scale') subject to mask 'k'
// (elements are not stored when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[i+63:i])*scale] := a[i+63:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VSCATTERQPD'. Intrinsic: '_mm512_mask_i64scatter_pd'.
// Requires AVX512F.
func MaskI64scatterPd(base_addr uintptr, k Mmask8, vindex M512i, a M512d, scale int)  {
	maskI64scatterPd(uintptr(base_addr), uint8(k), [64]byte(vindex), [8]float64(a), scale)
}

func maskI64scatterPd(base_addr uintptr, k uint8, vindex [64]byte, a [8]float64, scale int) 


// I64scatterPs: Scatter single-precision (32-bit) floating-point elements from
// 'a' into memory using 64-bit indices. 32-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 64-bit element in
// 'vindex' (each index is scaled by the factor in 'scale') subject to mask 'k'
// (elements are not stored when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*32
//			l := j*64
//			MEM[base_addr + SignExtend(vindex[l+63:l])*scale] := a[i+31:i]
//		ENDFOR
//
// Instruction: 'VSCATTERQPS'. Intrinsic: '_mm512_i64scatter_ps'.
// Requires AVX512F.
func I64scatterPs(base_addr uintptr, vindex M512i, a M256, scale int)  {
	i64scatterPs(uintptr(base_addr), [64]byte(vindex), [8]float32(a), scale)
}

func i64scatterPs(base_addr uintptr, vindex [64]byte, a [8]float32, scale int) 


// MaskI64scatterPs: Scatter single-precision (32-bit) floating-point elements
// from 'a' into memory using 64-bit indices. 32-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 64-bit element in
// 'vindex' (each index is scaled by the factor in 'scale') subject to mask 'k'
// (elements are not stored when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*32
//			l := j*64
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[l+63:l])*scale] := a[i+31:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VSCATTERQPS'. Intrinsic: '_mm512_mask_i64scatter_ps'.
// Requires AVX512F.
func MaskI64scatterPs(base_addr uintptr, k Mmask8, vindex M512i, a M256, scale int)  {
	maskI64scatterPs(uintptr(base_addr), uint8(k), [64]byte(vindex), [8]float32(a), scale)
}

func maskI64scatterPs(base_addr uintptr, k uint8, vindex [64]byte, a [8]float32, scale int) 


// I64scatterPslo: Stores 8 packed single-precision (32-bit) floating-point
// elements in 'v' in memory locations starting at location 'mv' at packed
// 64-bit integer indices stored in 'index' scaled by 'scale'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			addr := MEM[mv + index[j] * scale]
//			addr[i+31:i] := v[i+31:i]
//		ENDFOR
//
// Instruction: '...'. Intrinsic: '_mm512_i64scatter_pslo'.
// Requires KNCNI.
func I64scatterPslo(mv uintptr, index M512i, v M512, scale int)  {
	i64scatterPslo(uintptr(mv), [64]byte(index), [16]float32(v), scale)
}

func i64scatterPslo(mv uintptr, index [64]byte, v [16]float32, scale int) 


// MaskI64scatterPslo: Stores 8 packed single-precision (32-bit) floating-point
// elements in 'v1' in memory locations starting at location 'mv' at packed
// 64-bit integer indices stored in 'index' scaled by 'scale' using writemask
// 'k' (elements are only written to memory when the corresponding mask bit is
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				addr := MEM[mv + index[j] * scale]
//				addr[i+31:i] := v1[i+31:i]
//			FI	
//		ENDFOR
//
// Instruction: '...'. Intrinsic: '_mm512_mask_i64scatter_pslo'.
// Requires KNCNI.
func MaskI64scatterPslo(mv uintptr, k Mmask8, index M512i, v1 M512, scale int)  {
	maskI64scatterPslo(uintptr(mv), uint8(k), [64]byte(index), [16]float32(v1), scale)
}

func maskI64scatterPslo(mv uintptr, k uint8, index [64]byte, v1 [16]float32, scale int) 


// Insertf32x4: Copy 'a' to 'dst', then insert 128 bits (composed of 4 packed
// single-precision (32-bit) floating-point elements) from 'b' into 'dst' at
// the location specified by 'imm8'. 
//
//		dst[511:0] := a[511:0]
//		CASE (imm8[1:0]) of
//		0: dst[127:0] := b[127:0]
//		1: dst[255:128] := b[127:0]
//		2: dst[383:256] := b[127:0]
//		3: dst[511:384] := b[127:0]
//		ESAC
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTF32X4'. Intrinsic: '_mm512_insertf32x4'.
// Requires AVX512F.
func Insertf32x4(a M512, b M128, imm8 int) M512 {
	return M512(insertf32x4([16]float32(a), [4]float32(b), imm8))
}

func insertf32x4(a [16]float32, b [4]float32, imm8 int) [16]float32


// MaskInsertf32x4: Copy 'a' to 'tmp', then insert 128 bits (composed of 4
// packed single-precision (32-bit) floating-point elements) from 'b' into
// 'tmp' at the location specified by 'imm8'.  Store 'tmp' to 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp[511:0] := a[511:0]
//		CASE (imm8[1:0]) of
//		0: tmp[127:0] := b[127:0]
//		1: tmp[255:128] := b[127:0]
//		2: tmp[383:256] := b[127:0]
//		3: tmp[511:384] := b[127:0]
//		ESAC
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTF32X4'. Intrinsic: '_mm512_mask_insertf32x4'.
// Requires AVX512F.
func MaskInsertf32x4(src M512, k Mmask16, a M512, b M128, imm8 int) M512 {
	return M512(maskInsertf32x4([16]float32(src), uint16(k), [16]float32(a), [4]float32(b), imm8))
}

func maskInsertf32x4(src [16]float32, k uint16, a [16]float32, b [4]float32, imm8 int) [16]float32


// MaskzInsertf32x4: Copy 'a' to 'tmp', then insert 128 bits (composed of 4
// packed single-precision (32-bit) floating-point elements) from 'b' into
// 'tmp' at the location specified by 'imm8'.  Store 'tmp' to 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp[511:0] := a[511:0]
//		CASE (imm8[1:0]) of
//		0: tmp[127:0] := b[127:0]
//		1: tmp[255:128] := b[127:0]
//		2: tmp[383:256] := b[127:0]
//		3: tmp[511:384] := b[127:0]
//		ESAC
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTF32X4'. Intrinsic: '_mm512_maskz_insertf32x4'.
// Requires AVX512F.
func MaskzInsertf32x4(k Mmask16, a M512, b M128, imm8 int) M512 {
	return M512(maskzInsertf32x4(uint16(k), [16]float32(a), [4]float32(b), imm8))
}

func maskzInsertf32x4(k uint16, a [16]float32, b [4]float32, imm8 int) [16]float32


// Insertf32x8: Copy 'a' to 'dst', then insert 256 bits (composed of 8 packed
// single-precision (32-bit) floating-point elements) from 'b' into 'dst' at
// the location specified by 'imm8'. 
//
//		dst[511:0] := a[511:0]
//		CASE (imm8[7:0]) OF
//		0: dst[255:0] := b[255:0]
//		1: dst[511:256] := b[255:0]
//		ESAC
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTF32X8'. Intrinsic: '_mm512_insertf32x8'.
// Requires AVX512DQ.
func Insertf32x8(a M512, b M256, imm8 int) M512 {
	return M512(insertf32x8([16]float32(a), [8]float32(b), imm8))
}

func insertf32x8(a [16]float32, b [8]float32, imm8 int) [16]float32


// MaskInsertf32x8: Copy 'a' to 'tmp', then insert 256 bits (composed of 8
// packed single-precision (32-bit) floating-point elements) from 'b' into
// 'tmp' at the location specified by 'imm8'.  Store 'tmp' to 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp[511:0] := a[511:0]
//		CASE (imm8[7:0]) OF
//		0: tmp[255:0] := b[255:0]
//		1: tmp[511:256] := b[255:0]
//		ESAC
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTF32X8'. Intrinsic: '_mm512_mask_insertf32x8'.
// Requires AVX512DQ.
func MaskInsertf32x8(src M512, k Mmask16, a M512, b M256, imm8 int) M512 {
	return M512(maskInsertf32x8([16]float32(src), uint16(k), [16]float32(a), [8]float32(b), imm8))
}

func maskInsertf32x8(src [16]float32, k uint16, a [16]float32, b [8]float32, imm8 int) [16]float32


// MaskzInsertf32x8: Copy 'a' to 'tmp', then insert 256 bits (composed of 8
// packed single-precision (32-bit) floating-point elements) from 'b' into
// 'tmp' at the location specified by 'imm8'.  Store 'tmp' to 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp[511:0] := a[511:0]
//		CASE (imm8[7:0]) OF
//		0: tmp[255:0] := b[255:0]
//		1: tmp[511:256] := b[255:0]
//		ESAC
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTF32X8'. Intrinsic: '_mm512_maskz_insertf32x8'.
// Requires AVX512DQ.
func MaskzInsertf32x8(k Mmask16, a M512, b M256, imm8 int) M512 {
	return M512(maskzInsertf32x8(uint16(k), [16]float32(a), [8]float32(b), imm8))
}

func maskzInsertf32x8(k uint16, a [16]float32, b [8]float32, imm8 int) [16]float32


// Insertf64x2: Copy 'a' to 'dst', then insert 128 bits (composed of 2 packed
// double-precision (64-bit) floating-point elements) from 'b' into 'dst' at
// the location specified by 'imm8'. 
//
//		dst[511:0] := a[511:0]
//		CASE imm8[7:0] of
//		0: dst[127:0] := b[127:0]
//		1: dst[255:128] := b[127:0]
//		2: dst[383:256] := b[127:0]
//		3: dst[511:384] := b[127:0]
//		ESAC
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTF64X2'. Intrinsic: '_mm512_insertf64x2'.
// Requires AVX512DQ.
func Insertf64x2(a M512d, b M128d, imm8 int) M512d {
	return M512d(insertf64x2([8]float64(a), [2]float64(b), imm8))
}

func insertf64x2(a [8]float64, b [2]float64, imm8 int) [8]float64


// MaskInsertf64x2: Copy 'a' to 'tmp', then insert 128 bits (composed of 2
// packed double-precision (64-bit) floating-point elements) from 'b' into
// 'tmp' at the location specified by 'imm8'.  Store 'tmp' to 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp[511:0] := a[511:0]
//		CASE (imm8[1:0]) of
//		0: tmp[127:0] := b[127:0]
//		1: tmp[255:128] := b[127:0]
//		2: tmp[383:256] := b[127:0]
//		3: tmp[511:384] := b[127:0]
//		ESAC
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTF64X2'. Intrinsic: '_mm512_mask_insertf64x2'.
// Requires AVX512DQ.
func MaskInsertf64x2(src M512d, k Mmask8, a M512d, b M128d, imm8 int) M512d {
	return M512d(maskInsertf64x2([8]float64(src), uint8(k), [8]float64(a), [2]float64(b), imm8))
}

func maskInsertf64x2(src [8]float64, k uint8, a [8]float64, b [2]float64, imm8 int) [8]float64


// MaskzInsertf64x2: Copy 'a' to 'tmp', then insert 128 bits (composed of 2
// packed double-precision (64-bit) floating-point elements) from 'b' into
// 'tmp' at the location specified by 'imm8'.  Store 'tmp' to 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp[511:0] := a[511:0]
//		CASE (imm8[1:0]) of
//		0: tmp[127:0] := b[127:0]
//		1: tmp[255:128] := b[127:0]
//		2: tmp[383:256] := b[127:0]
//		3: tmp[511:384] := b[127:0]
//		ESAC
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTF64X2'. Intrinsic: '_mm512_maskz_insertf64x2'.
// Requires AVX512DQ.
func MaskzInsertf64x2(k Mmask8, a M512d, b M128d, imm8 int) M512d {
	return M512d(maskzInsertf64x2(uint8(k), [8]float64(a), [2]float64(b), imm8))
}

func maskzInsertf64x2(k uint8, a [8]float64, b [2]float64, imm8 int) [8]float64


// Insertf64x4: Copy 'a' to 'dst', then insert 256 bits (composed of 4 packed
// double-precision (64-bit) floating-point elements) from 'b' into 'dst' at
// the location specified by 'imm8'. 
//
//		dst[511:0] := a[511:0]
//		CASE (imm8[0]) of
//		0: dst[255:0] := b[255:0]
//		1: dst[511:256] := b[255:0]
//		ESAC
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTF64X4'. Intrinsic: '_mm512_insertf64x4'.
// Requires AVX512F.
func Insertf64x4(a M512d, b M256d, imm8 int) M512d {
	return M512d(insertf64x4([8]float64(a), [4]float64(b), imm8))
}

func insertf64x4(a [8]float64, b [4]float64, imm8 int) [8]float64


// MaskInsertf64x4: Copy 'a' to 'tmp', then insert 256 bits (composed of 4
// packed double-precision (64-bit) floating-point elements) from 'b' into
// 'tmp' at the location specified by 'imm8'.  Store 'tmp' to 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp[511:0] := a[511:0]
//		CASE (imm8[0]) of
//		0: tmp[255:0] := b[255:0]
//		1: tmp[511:256] := b[255:0]
//		ESAC
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTF64X4'. Intrinsic: '_mm512_mask_insertf64x4'.
// Requires AVX512F.
func MaskInsertf64x4(src M512d, k Mmask8, a M512d, b M256d, imm8 int) M512d {
	return M512d(maskInsertf64x4([8]float64(src), uint8(k), [8]float64(a), [4]float64(b), imm8))
}

func maskInsertf64x4(src [8]float64, k uint8, a [8]float64, b [4]float64, imm8 int) [8]float64


// MaskzInsertf64x4: Copy 'a' to 'tmp', then insert 256 bits (composed of 4
// packed double-precision (64-bit) floating-point elements) from 'b' into
// 'tmp' at the location specified by 'imm8'.  Store 'tmp' to 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp[511:0] := a[511:0]
//		CASE (imm8[0]) of
//		0: tmp[255:0] := b[255:0]
//		1: tmp[511:256] := b[255:0]
//		ESAC
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTF64X4'. Intrinsic: '_mm512_maskz_insertf64x4'.
// Requires AVX512F.
func MaskzInsertf64x4(k Mmask8, a M512d, b M256d, imm8 int) M512d {
	return M512d(maskzInsertf64x4(uint8(k), [8]float64(a), [4]float64(b), imm8))
}

func maskzInsertf64x4(k uint8, a [8]float64, b [4]float64, imm8 int) [8]float64


// Inserti32x4: Copy 'a' to 'dst', then insert 128 bits (composed of 4 packed
// 32-bit integers) from 'b' into 'dst' at the location specified by 'imm8'. 
//
//		dst[511:0] := a[511:0]
//		CASE (imm8[1:0]) of
//		0: dst[127:0] := b[127:0]
//		1: dst[255:128] := b[127:0]
//		2: dst[383:256] := b[127:0]
//		3: dst[511:384] := b[127:0]
//		ESAC
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTI32X4'. Intrinsic: '_mm512_inserti32x4'.
// Requires AVX512F.
func Inserti32x4(a M512i, b M128i, imm8 int) M512i {
	return M512i(inserti32x4([64]byte(a), [16]byte(b), imm8))
}

func inserti32x4(a [64]byte, b [16]byte, imm8 int) [64]byte


// MaskInserti32x4: Copy 'a' to 'tmp', then insert 128 bits (composed of 4
// packed 32-bit integers) from 'b' into 'tmp' at the location specified by
// 'imm8'.  Store 'tmp' to 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		tmp[511:0] := a[511:0]
//		CASE (imm8[1:0]) of
//		0: tmp[127:0] := b[127:0]
//		1: tmp[255:128] := b[127:0]
//		2: tmp[383:256] := b[127:0]
//		3: tmp[511:384] := b[127:0]
//		ESAC
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTI32X4'. Intrinsic: '_mm512_mask_inserti32x4'.
// Requires AVX512F.
func MaskInserti32x4(src M512i, k Mmask16, a M512i, b M128i, imm8 int) M512i {
	return M512i(maskInserti32x4([64]byte(src), uint16(k), [64]byte(a), [16]byte(b), imm8))
}

func maskInserti32x4(src [64]byte, k uint16, a [64]byte, b [16]byte, imm8 int) [64]byte


// MaskzInserti32x4: Copy 'a' to 'tmp', then insert 128 bits (composed of 4
// packed 32-bit integers) from 'b' into 'tmp' at the location specified by
// 'imm8'.  Store 'tmp' to 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		tmp[511:0] := a[511:0]
//		CASE (imm8[1:0]) of
//		0: tmp[127:0] := b[127:0]
//		1: tmp[255:128] := b[127:0]
//		2: tmp[383:256] := b[127:0]
//		3: tmp[511:384] := b[127:0]
//		ESAC
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTI32X4'. Intrinsic: '_mm512_maskz_inserti32x4'.
// Requires AVX512F.
func MaskzInserti32x4(k Mmask16, a M512i, b M128i, imm8 int) M512i {
	return M512i(maskzInserti32x4(uint16(k), [64]byte(a), [16]byte(b), imm8))
}

func maskzInserti32x4(k uint16, a [64]byte, b [16]byte, imm8 int) [64]byte


// Inserti32x8: Copy 'a' to 'dst', then insert 256 bits (composed of 8 packed
// 32-bit integers) from 'b' into 'dst' at the location specified by 'imm8'. 
//
//		dst[511:0] := a[511:0]
//		CASE imm8[7:0] of
//		0: dst[255:0] := b[255:0]
//		1: dst[511:256] := b[255:0]
//		ESAC
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTI32X8'. Intrinsic: '_mm512_inserti32x8'.
// Requires AVX512DQ.
func Inserti32x8(a M512i, b M256i, imm8 int) M512i {
	return M512i(inserti32x8([64]byte(a), [32]byte(b), imm8))
}

func inserti32x8(a [64]byte, b [32]byte, imm8 int) [64]byte


// MaskInserti32x8: Copy 'a' to 'tmp', then insert 256 bits (composed of 8
// packed 32-bit integers) from 'b' into 'tmp' at the location specified by
// 'imm8'.  Store 'tmp' to 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		tmp[511:0] := a[511:0]
//		CASE (imm8[7:0]) OF
//		0: tmp[255:0] := b[255:0]
//		1: tmp[511:256] := b[255:0]
//		ESAC
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTI32X8'. Intrinsic: '_mm512_mask_inserti32x8'.
// Requires AVX512DQ.
func MaskInserti32x8(src M512i, k Mmask16, a M512i, b M256i, imm8 int) M512i {
	return M512i(maskInserti32x8([64]byte(src), uint16(k), [64]byte(a), [32]byte(b), imm8))
}

func maskInserti32x8(src [64]byte, k uint16, a [64]byte, b [32]byte, imm8 int) [64]byte


// MaskzInserti32x8: Copy 'a' to 'tmp', then insert 256 bits (composed of 8
// packed 32-bit integers) from 'b' into 'tmp' at the location specified by
// 'imm8'.  Store 'tmp' to 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		tmp[511:0] := a[511:0]
//		CASE (imm8[7:0]) OF
//		0: tmp[255:0] := b[255:0]
//		1: tmp[511:256] := b[255:0]
//		ESAC
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTI32X8'. Intrinsic: '_mm512_maskz_inserti32x8'.
// Requires AVX512DQ.
func MaskzInserti32x8(k Mmask16, a M512i, b M256i, imm8 int) M512i {
	return M512i(maskzInserti32x8(uint16(k), [64]byte(a), [32]byte(b), imm8))
}

func maskzInserti32x8(k uint16, a [64]byte, b [32]byte, imm8 int) [64]byte


// Inserti64x2: Copy 'a' to 'dst', then insert 128 bits (composed of 2 packed
// 64-bit integers) from 'b' into 'dst' at the location specified by 'imm8'. 
//
//		dst[511:0] := a[511:0]
//		CASE imm8[7:0] of
//		0: dst[127:0] := b[127:0]
//		1: dst[255:128] := b[127:0]
//		2: dst[383:256] := b[127:0]
//		3: dst[511:384] := b[127:0]
//		ESAC
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTI64X2'. Intrinsic: '_mm512_inserti64x2'.
// Requires AVX512DQ.
func Inserti64x2(a M512i, b M128i, imm8 int) M512i {
	return M512i(inserti64x2([64]byte(a), [16]byte(b), imm8))
}

func inserti64x2(a [64]byte, b [16]byte, imm8 int) [64]byte


// MaskInserti64x2: Copy 'a' to 'tmp', then insert 128 bits (composed of 2
// packed 64-bit integers) from 'b' into 'tmp' at the location specified by
// 'imm8'.  Store 'tmp' to 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		tmp[511:0] := a[511:0]
//		CASE (imm8[1:0]) of
//		0: tmp[127:0] := b[127:0]
//		1: tmp[255:128] := b[127:0]
//		2: tmp[383:256] := b[127:0]
//		3: tmp[511:384] := b[127:0]
//		ESAC
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTI64X2'. Intrinsic: '_mm512_mask_inserti64x2'.
// Requires AVX512DQ.
func MaskInserti64x2(src M512i, k Mmask8, a M512i, b M128i, imm8 int) M512i {
	return M512i(maskInserti64x2([64]byte(src), uint8(k), [64]byte(a), [16]byte(b), imm8))
}

func maskInserti64x2(src [64]byte, k uint8, a [64]byte, b [16]byte, imm8 int) [64]byte


// MaskzInserti64x2: Copy 'a' to 'tmp', then insert 128 bits (composed of 2
// packed 64-bit integers) from 'b' into 'tmp' at the location specified by
// 'imm8'.  Store 'tmp' to 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		tmp[511:0] := a[511:0]
//		CASE (imm8[1:0]) of
//		0: tmp[127:0] := b[127:0]
//		1: tmp[255:128] := b[127:0]
//		2: tmp[383:256] := b[127:0]
//		3: tmp[511:384] := b[127:0]
//		ESAC
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTI64X2'. Intrinsic: '_mm512_maskz_inserti64x2'.
// Requires AVX512DQ.
func MaskzInserti64x2(k Mmask8, a M512i, b M128i, imm8 int) M512i {
	return M512i(maskzInserti64x2(uint8(k), [64]byte(a), [16]byte(b), imm8))
}

func maskzInserti64x2(k uint8, a [64]byte, b [16]byte, imm8 int) [64]byte


// Inserti64x4: Copy 'a' to 'dst', then insert 256 bits (composed of 4 packed
// 64-bit integers) from 'b' into 'dst' at the location specified by 'imm8'. 
//
//		dst[511:0] := a[511:0]
//		CASE (imm8[7:0]) OF
//		0: dst[255:0] := b[255:0]
//		1: dst[511:256] := b[255:0]
//		ESAC
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTI64X4'. Intrinsic: '_mm512_inserti64x4'.
// Requires AVX512F.
func Inserti64x4(a M512i, b M256i, imm8 int) M512i {
	return M512i(inserti64x4([64]byte(a), [32]byte(b), imm8))
}

func inserti64x4(a [64]byte, b [32]byte, imm8 int) [64]byte


// MaskInserti64x4: Copy 'a' to 'tmp', then insert 256 bits (composed of 4
// packed 64-bit integers) from 'b' into 'tmp' at the location specified by
// 'imm8'.  Store 'tmp' to 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		tmp[511:0] := a[511:0]
//		CASE (imm8[0]) of
//		0: tmp[255:0] := b[255:0]
//		1: tmp[511:256] := b[255:0]
//		ESAC
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTI64X4'. Intrinsic: '_mm512_mask_inserti64x4'.
// Requires AVX512F.
func MaskInserti64x4(src M512i, k Mmask8, a M512i, b M256i, imm8 int) M512i {
	return M512i(maskInserti64x4([64]byte(src), uint8(k), [64]byte(a), [32]byte(b), imm8))
}

func maskInserti64x4(src [64]byte, k uint8, a [64]byte, b [32]byte, imm8 int) [64]byte


// MaskzInserti64x4: Copy 'a' to 'tmp', then insert 256 bits (composed of 4
// packed 64-bit integers) from 'b' into 'tmp' at the location specified by
// 'imm8'.  Store 'tmp' to 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		tmp[511:0] := a[511:0]
//		CASE (imm8[0]) of
//		0: tmp[255:0] := b[255:0]
//		1: tmp[511:256] := b[255:0]
//		ESAC
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTI64X4'. Intrinsic: '_mm512_maskz_inserti64x4'.
// Requires AVX512F.
func MaskzInserti64x4(k Mmask8, a M512i, b M256i, imm8 int) M512i {
	return M512i(maskzInserti64x4(uint8(k), [64]byte(a), [32]byte(b), imm8))
}

func maskzInserti64x4(k uint8, a [64]byte, b [32]byte, imm8 int) [64]byte


// Int2mask: Converts integer 'mask' into bitmask, storing the result in 'dst'. 
//
//		dst := mask[15:0]
//
// Instruction: 'KMOV'. Intrinsic: '_mm512_int2mask'.
// Requires KNCNI.
func Int2mask(mask int) Mmask16 {
	return Mmask16(int2mask(mask))
}

func int2mask(mask int) uint16


// InvsqrtPd: Compute the inverse square root of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := InvSQRT(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_invsqrt_pd'.
// Requires AVX512F.
func InvsqrtPd(a M512d) M512d {
	return M512d(invsqrtPd([8]float64(a)))
}

func invsqrtPd(a [8]float64) [8]float64


// MaskInvsqrtPd: Compute the inverse square root of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := InvSQRT(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_invsqrt_pd'.
// Requires AVX512F.
func MaskInvsqrtPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskInvsqrtPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskInvsqrtPd(src [8]float64, k uint8, a [8]float64) [8]float64


// InvsqrtPs: Compute the inverse square root of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := InvSQRT(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_invsqrt_ps'.
// Requires AVX512F.
func InvsqrtPs(a M512) M512 {
	return M512(invsqrtPs([16]float32(a)))
}

func invsqrtPs(a [16]float32) [16]float32


// MaskInvsqrtPs: Compute the inverse square root of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := InvSQRT(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_invsqrt_ps'.
// Requires AVX512F.
func MaskInvsqrtPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskInvsqrtPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskInvsqrtPs(src [16]float32, k uint16, a [16]float32) [16]float32


// Kand: Compute the bitwise AND of 16-bit masks 'a' and 'b', and store the
// result in 'k'. 
//
//		k[15:0] := a[15:0] AND b[15:0]
//		k[MAX:16] := 0
//
// Instruction: 'KANDW'. Intrinsic: '_mm512_kand'.
// Requires AVX512F.
func Kand(a Mmask16, b Mmask16) Mmask16 {
	return Mmask16(kand(uint16(a), uint16(b)))
}

func kand(a uint16, b uint16) uint16


// Kand1: Compute the bitwise AND of 16-bit masks 'a' and 'b', and store the
// result in 'k'. 
//
//		k[15:0] := a[15:0] AND b[15:0]
//		k[MAX:16] := 0
//
// Instruction: 'KAND'. Intrinsic: '_mm512_kand'.
// Requires KNCNI.
func Kand1(a Mmask16, b Mmask16) Mmask16 {
	return Mmask16(kand1(uint16(a), uint16(b)))
}

func kand1(a uint16, b uint16) uint16


// Kandn: Compute the bitwise AND NOT of 16-bit masks 'a' and 'b', and store
// the result in 'k'. 
//
//		k[15:0] := (NOT a[15:0]) AND b[15:0]
//		k[MAX:16] := 0
//
// Instruction: 'KANDN'. Intrinsic: '_mm512_kandn'.
// Requires KNCNI.
func Kandn(a Mmask16, b Mmask16) Mmask16 {
	return Mmask16(kandn(uint16(a), uint16(b)))
}

func kandn(a uint16, b uint16) uint16


// Kandn1: Compute the bitwise AND NOT of 16-bit masks 'a' and 'b', and store
// the result in 'k'. 
//
//		k[15:0] := (NOT a[15:0]) AND b[15:0]
//		k[MAX:16] := 0
//
// Instruction: 'KANDNW'. Intrinsic: '_mm512_kandn'.
// Requires AVX512F.
func Kandn1(a Mmask16, b Mmask16) Mmask16 {
	return Mmask16(kandn1(uint16(a), uint16(b)))
}

func kandn1(a uint16, b uint16) uint16


// Kandnr: Performs a bitwise AND operation between NOT of 'k2' and 'k1',
// storing the result in 'dst'. 
//
//		dst[15:0] := NOT(k2[15:0]) & k1[15:0]
//
// Instruction: 'KANDNR'. Intrinsic: '_mm512_kandnr'.
// Requires KNCNI.
func Kandnr(k1 Mmask16, k2 Mmask16) Mmask16 {
	return Mmask16(kandnr(uint16(k1), uint16(k2)))
}

func kandnr(k1 uint16, k2 uint16) uint16


// Kconcathi64: Packs masks 'k1' and 'k2' into the high 32 bits of 'dst'. The
// rest of 'dst' is set to 0. 
//
//		dst[63:48] := k1[15:0]
//		dst[47:32] := k2[15:0]
//		dst[31:0]  := 0
//
// Instruction: 'KCONCATH'. Intrinsic: '_mm512_kconcathi_64'.
// Requires KNCNI.
func Kconcathi64(k1 Mmask16, k2 Mmask16) int64 {
	return int64(kconcathi64(uint16(k1), uint16(k2)))
}

func kconcathi64(k1 uint16, k2 uint16) int64


// Kconcatlo64: Packs masks 'k1' and 'k2' into the low 32 bits of 'dst'. The
// rest of 'dst' is set to 0. 
//
//		dst[31:16] := k1[15:0]
//		dst[15:0]  := k2[15:0]
//		dst[63:32] := 0
//
// Instruction: 'KCONCATL'. Intrinsic: '_mm512_kconcatlo_64'.
// Requires KNCNI.
func Kconcatlo64(k1 Mmask16, k2 Mmask16) int64 {
	return int64(kconcatlo64(uint16(k1), uint16(k2)))
}

func kconcatlo64(k1 uint16, k2 uint16) int64


// Kextract64: Extracts 16-bit value 'b' from 64-bit integer 'a', storing the
// result in 'dst'. 
//
//		CASE b of
//		0: dst[15:0] := a[63:48]
//		1: dst[15:0] := a[47:32]
//		2: dst[15:0] := a[31:16]
//		3: dst[15:0] := a[15:0]
//		ESAC
//		dst[MAX:15] := 0
//
// Instruction: 'KEXTRACT'. Intrinsic: '_mm512_kextract_64'.
// Requires KNCNI.
func Kextract64(a int64, b int) Mmask16 {
	return Mmask16(kextract64(a, b))
}

func kextract64(a int64, b int) uint16


// Kmerge2l1h: Move the high element from 'k1' to the low element of 'k1', and
// insert the low element of 'k2' into the high element of 'k1'. 
//
//		tmp[7:0] := k1[15:8]
//		k1[15:8] := k2[7:0]
//		k1[7:0]  := tmp[7:0]
//
// Instruction: 'KMERGE2L1H'. Intrinsic: '_mm512_kmerge2l1h'.
// Requires KNCNI.
func Kmerge2l1h(k1 Mmask16, k2 Mmask16) Mmask16 {
	return Mmask16(kmerge2l1h(uint16(k1), uint16(k2)))
}

func kmerge2l1h(k1 uint16, k2 uint16) uint16


// Kmerge2l1l: Insert the low element of 'k2' into the high element of 'k1'. 
//
//		k1[15:8] := k2[7:0]
//
// Instruction: 'KMERGE2L1L'. Intrinsic: '_mm512_kmerge2l1l'.
// Requires KNCNI.
func Kmerge2l1l(k1 Mmask16, k2 Mmask16) Mmask16 {
	return Mmask16(kmerge2l1l(uint16(k1), uint16(k2)))
}

func kmerge2l1l(k1 uint16, k2 uint16) uint16


// Kmov: Copy 16-bit mask 'a' to 'k'. 
//
//		k[15:0] := a[15:0]
//		k[MAX:16] := 0
//
// Instruction: 'KMOVW'. Intrinsic: '_mm512_kmov'.
// Requires AVX512F.
func Kmov(a Mmask16) Mmask16 {
	return Mmask16(kmov(uint16(a)))
}

func kmov(a uint16) uint16


// Kmov1: Copy 16-bit mask 'a' to 'k'. 
//
//		k[15:0] := a[15:0]
//		k[MAX:16] := 0
//
// Instruction: 'KMOV'. Intrinsic: '_mm512_kmov'.
// Requires KNCNI.
func Kmov1(a Mmask16) Mmask16 {
	return Mmask16(kmov1(uint16(a)))
}

func kmov1(a uint16) uint16


// Kmovlhb: Inserts the low byte of mask 'k2' into the high byte of 'dst', and
// copies the low byte of 'k1' to the low byte of 'dst'. 
//
//		dst[7:0] := k1[7:0]
//		dst[15:8] := k2[7:0]
//
// Instruction: 'KMERGE2L1L'. Intrinsic: '_mm512_kmovlhb'.
// Requires KNCNI.
func Kmovlhb(k1 Mmask16, k2 Mmask16) Mmask16 {
	return Mmask16(kmovlhb(uint16(k1), uint16(k2)))
}

func kmovlhb(k1 uint16, k2 uint16) uint16


// Knot: Compute the bitwise NOT of 16-bit mask 'a', and store the result in
// 'k'. 
//
//		k[15:0] := NOT a[15:0]
//		k[MAX:16] := 0
//
// Instruction: 'KNOT'. Intrinsic: '_mm512_knot'.
// Requires KNCNI.
func Knot(a Mmask16) Mmask16 {
	return Mmask16(knot(uint16(a)))
}

func knot(a uint16) uint16


// Knot1: Compute the bitwise NOT of 16-bit mask 'a', and store the result in
// 'k'. 
//
//		k[15:0] := NOT a[15:0]
//		k[MAX:16] := 0
//
// Instruction: 'KNOTW'. Intrinsic: '_mm512_knot'.
// Requires AVX512F.
func Knot1(a Mmask16) Mmask16 {
	return Mmask16(knot1(uint16(a)))
}

func knot1(a uint16) uint16


// Kor: Compute the bitwise OR of 16-bit masks 'a' and 'b', and store the
// result in 'k'. 
//
//		k[15:0] := a[15:0] OR b[15:0]
//		k[MAX:16] := 0
//
// Instruction: 'KOR'. Intrinsic: '_mm512_kor'.
// Requires KNCNI.
func Kor(a Mmask16, b Mmask16) Mmask16 {
	return Mmask16(kor(uint16(a), uint16(b)))
}

func kor(a uint16, b uint16) uint16


// Kor1: Compute the bitwise OR of 16-bit masks 'a' and 'b', and store the
// result in 'k'. 
//
//		k[15:0] := a[15:0] OR b[15:0]
//		k[MAX:16] := 0
//
// Instruction: 'KORW'. Intrinsic: '_mm512_kor'.
// Requires AVX512F.
func Kor1(a Mmask16, b Mmask16) Mmask16 {
	return Mmask16(kor1(uint16(a), uint16(b)))
}

func kor1(a uint16, b uint16) uint16


// Kortestc: Performs bitwise OR between 'k1' and 'k2', storing the result in
// 'dst'. CF flag is set if 'dst' consists of all 1's. 
//
//		dst[15:0] := k1[15:0] | k2[15:0]
//		IF PopCount(dst[15:0]) = 16
//			SetCF()
//		FI
//
// Instruction: 'KORTEST'. Intrinsic: '_mm512_kortestc'.
// Requires KNCNI.
func Kortestc(k1 Mmask16, k2 Mmask16) int {
	return int(kortestc(uint16(k1), uint16(k2)))
}

func kortestc(k1 uint16, k2 uint16) int


// Kortestc1: Performs bitwise OR between 'k1' and 'k2', storing the result in
// 'dst'. CF flag is set if 'dst' consists of all 1's. 
//
//		dst[15:0] := k1[15:0] | k2[15:0]
//		IF PopCount(dst[15:0]) = 16
//			SetCF()
//		FI
//
// Instruction: 'KORTESTW'. Intrinsic: '_mm512_kortestc'.
// Requires AVX512F.
func Kortestc1(k1 Mmask16, k2 Mmask16) int {
	return int(kortestc1(uint16(k1), uint16(k2)))
}

func kortestc1(k1 uint16, k2 uint16) int


// Kortestz: Performs bitwise OR between 'k1' and 'k2', storing the result in
// 'dst'. ZF flag is set if 'dst' is 0. 
//
//		dst[15:0] := k1[15:0] | k2[15:0]
//		IF dst = 0
//			SetZF()
//		FI
//
// Instruction: 'KORTESTW'. Intrinsic: '_mm512_kortestz'.
// Requires AVX512F.
func Kortestz(k1 Mmask16, k2 Mmask16) int {
	return int(kortestz(uint16(k1), uint16(k2)))
}

func kortestz(k1 uint16, k2 uint16) int


// Kortestz1: Performs bitwise OR between 'k1' and 'k2', storing the result in
// 'dst'. ZF flag is set if 'dst' is 0. 
//
//		dst[15:0] := k1[15:0] | k2[15:0]
//		IF dst = 0
//			SetZF()
//		FI
//
// Instruction: 'KORTEST'. Intrinsic: '_mm512_kortestz'.
// Requires KNCNI.
func Kortestz1(k1 Mmask16, k2 Mmask16) int {
	return int(kortestz1(uint16(k1), uint16(k2)))
}

func kortestz1(k1 uint16, k2 uint16) int


// Kswapb: Moves high byte from 'k2' to low byte of 'k1', and moves low byte of
// 'k2' to high byte of 'k1'. 
//
//		tmp[7:0] := k2[15:8]
//		k2[15:8] := k1[7:0]
//		k1[7:0]  := tmp[7:0]
//		
//		tmp[7:0] := k2[7:0]
//		k2[7:0]  := k1[15:8]
//		k1[15:8] := tmp[7:0]
//
// Instruction: 'KMERGE2L1H'. Intrinsic: '_mm512_kswapb'.
// Requires KNCNI.
func Kswapb(k1 Mmask16, k2 Mmask16) Mmask16 {
	return Mmask16(kswapb(uint16(k1), uint16(k2)))
}

func kswapb(k1 uint16, k2 uint16) uint16


// Kunpackb: Unpack and interleave 8 bits from masks 'a' and 'b', and store the
// 16-bit result in 'k'. 
//
//		k[7:0] := b[7:0]
//		k[15:8] := a[7:0]
//		k[MAX:16] := 0
//
// Instruction: 'KUNPCKBW'. Intrinsic: '_mm512_kunpackb'.
// Requires AVX512F.
func Kunpackb(a Mmask16, b Mmask16) Mmask16 {
	return Mmask16(kunpackb(uint16(a), uint16(b)))
}

func kunpackb(a uint16, b uint16) uint16


// Kunpackd: Unpack and interleave 32 bits from masks 'a' and 'b', and store
// the 64-bit result in 'k'. 
//
//		k[31:0] := a[31:0]
//		k[63:32] := b[31:0]
//		k[MAX:64] := 0
//
// Instruction: 'KUNPCKDQ'. Intrinsic: '_mm512_kunpackd'.
// Requires AVX512BW.
func Kunpackd(a Mmask64, b Mmask64) Mmask64 {
	return Mmask64(kunpackd(uint64(a), uint64(b)))
}

func kunpackd(a uint64, b uint64) uint64


// Kunpackw: Unpack and interleave 16 bits from masks 'a' and 'b', and store
// the 32-bit result in 'k'. 
//
//		k[15:0] := a[15:0]
//		k[31:16] := b[15:0]
//		k[MAX:32] := 0
//
// Instruction: 'KUNPCKWD'. Intrinsic: '_mm512_kunpackw'.
// Requires AVX512BW.
func Kunpackw(a Mmask32, b Mmask32) Mmask32 {
	return Mmask32(kunpackw(uint32(a), uint32(b)))
}

func kunpackw(a uint32, b uint32) uint32


// Kxnor: Compute the bitwise XNOR of 16-bit masks 'a' and 'b', and store the
// result in 'k'. 
//
//		k[15:0] := NOT (a[15:0] XOR b[15:0])
//		k[MAX:16] := 0
//
// Instruction: 'KXNORW'. Intrinsic: '_mm512_kxnor'.
// Requires AVX512F.
func Kxnor(a Mmask16, b Mmask16) Mmask16 {
	return Mmask16(kxnor(uint16(a), uint16(b)))
}

func kxnor(a uint16, b uint16) uint16


// Kxnor1: Compute the bitwise XNOR of 16-bit masks 'a' and 'b', and store the
// result in 'k'. 
//
//		k[15:0] := NOT (a[15:0] XOR b[15:0])
//		k[MAX:16] := 0
//
// Instruction: 'KXNOR'. Intrinsic: '_mm512_kxnor'.
// Requires KNCNI.
func Kxnor1(a Mmask16, b Mmask16) Mmask16 {
	return Mmask16(kxnor1(uint16(a), uint16(b)))
}

func kxnor1(a uint16, b uint16) uint16


// Kxor: Compute the bitwise XOR of 16-bit masks 'a' and 'b', and store the
// result in 'k'. 
//
//		k[15:0] := a[15:0] XOR b[15:0]
//		k[MAX:16] := 0
//
// Instruction: 'KXOR'. Intrinsic: '_mm512_kxor'.
// Requires KNCNI.
func Kxor(a Mmask16, b Mmask16) Mmask16 {
	return Mmask16(kxor(uint16(a), uint16(b)))
}

func kxor(a uint16, b uint16) uint16


// Kxor1: Compute the bitwise XOR of 16-bit masks 'a' and 'b', and store the
// result in 'k'. 
//
//		k[15:0] := a[15:0] XOR b[15:0]
//		k[MAX:16] := 0
//
// Instruction: 'KXORW'. Intrinsic: '_mm512_kxor'.
// Requires AVX512F.
func Kxor1(a Mmask16, b Mmask16) Mmask16 {
	return Mmask16(kxor1(uint16(a), uint16(b)))
}

func kxor1(a uint16, b uint16) uint16


// LoadEpi32: Load 512-bits (composed of 16 packed 32-bit integers) from memory
// into 'dst'. 
// 	'mem_addr' must be aligned on a 64-byte boundary or a general-protection
// exception may be generated. 
//
//		dst[511:0] := MEM[mem_addr+511:mem_addr]
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDQA32'. Intrinsic: '_mm512_load_epi32'.
// Requires KNCNI.
func LoadEpi32(mem_addr uintptr) M512i {
	return M512i(loadEpi32(uintptr(mem_addr)))
}

func loadEpi32(mem_addr uintptr) [64]byte


// MaskLoadEpi32: Load packed 32-bit integers from memory into 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set).
// 	'mem_addr' must be aligned on a 64-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDQA32'. Intrinsic: '_mm512_mask_load_epi32'.
// Requires KNCNI.
func MaskLoadEpi32(src M512i, k Mmask16, mem_addr uintptr) M512i {
	return M512i(maskLoadEpi32([64]byte(src), uint16(k), uintptr(mem_addr)))
}

func maskLoadEpi32(src [64]byte, k uint16, mem_addr uintptr) [64]byte


// MaskzLoadEpi32: Load packed 32-bit integers from memory into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
// 	'mem_addr' must be aligned on a 64-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDQA32'. Intrinsic: '_mm512_maskz_load_epi32'.
// Requires AVX512F.
func MaskzLoadEpi32(k Mmask16, mem_addr uintptr) M512i {
	return M512i(maskzLoadEpi32(uint16(k), uintptr(mem_addr)))
}

func maskzLoadEpi32(k uint16, mem_addr uintptr) [64]byte


// LoadEpi64: Load 512-bits (composed of 8 packed 64-bit integers) from memory
// into 'dst'. 
// 	'mem_addr' must be aligned on a 64-byte boundary or a general-protection
// exception may be generated. 
//
//		dst[511:0] := MEM[mem_addr+511:mem_addr]
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDQA64'. Intrinsic: '_mm512_load_epi64'.
// Requires KNCNI.
func LoadEpi64(mem_addr uintptr) M512i {
	return M512i(loadEpi64(uintptr(mem_addr)))
}

func loadEpi64(mem_addr uintptr) [64]byte


// MaskLoadEpi64: Load packed 64-bit integers from memory into 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
// 	'mem_addr' must be aligned on a 64-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDQA64'. Intrinsic: '_mm512_mask_load_epi64'.
// Requires KNCNI.
func MaskLoadEpi64(src M512i, k Mmask8, mem_addr uintptr) M512i {
	return M512i(maskLoadEpi64([64]byte(src), uint8(k), uintptr(mem_addr)))
}

func maskLoadEpi64(src [64]byte, k uint8, mem_addr uintptr) [64]byte


// MaskzLoadEpi64: Load packed 64-bit integers from memory into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
// 	'mem_addr' must be aligned on a 64-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDQA64'. Intrinsic: '_mm512_maskz_load_epi64'.
// Requires AVX512F.
func MaskzLoadEpi64(k Mmask8, mem_addr uintptr) M512i {
	return M512i(maskzLoadEpi64(uint8(k), uintptr(mem_addr)))
}

func maskzLoadEpi64(k uint8, mem_addr uintptr) [64]byte


// LoadPd: Load 512-bits (composed of 8 packed double-precision (64-bit)
// floating-point elements) from memory into 'dst'. 
// 	'mem_addr' must be aligned on a 64-byte boundary or a general-protection
// exception may be generated. 
//
//		dst[511:0] := MEM[mem_addr+511:mem_addr]
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVAPD'. Intrinsic: '_mm512_load_pd'.
// Requires KNCNI.
func LoadPd(mem_addr uintptr) M512d {
	return M512d(loadPd(uintptr(mem_addr)))
}

func loadPd(mem_addr uintptr) [8]float64


// MaskLoadPd: Load packed double-precision (64-bit) floating-point elements
// from memory into 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 'mem_addr' must be aligned on a
// 64-byte boundary or a general-protection exception may be generated. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVAPD'. Intrinsic: '_mm512_mask_load_pd'.
// Requires KNCNI.
func MaskLoadPd(src M512d, k Mmask8, mem_addr uintptr) M512d {
	return M512d(maskLoadPd([8]float64(src), uint8(k), uintptr(mem_addr)))
}

func maskLoadPd(src [8]float64, k uint8, mem_addr uintptr) [8]float64


// MaskzLoadPd: Load packed double-precision (64-bit) floating-point elements
// from memory into 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 'mem_addr' must be aligned on a 64-byte
// boundary or a general-protection exception may be generated. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVAPD'. Intrinsic: '_mm512_maskz_load_pd'.
// Requires AVX512F.
func MaskzLoadPd(k Mmask8, mem_addr uintptr) M512d {
	return M512d(maskzLoadPd(uint8(k), uintptr(mem_addr)))
}

func maskzLoadPd(k uint8, mem_addr uintptr) [8]float64


// LoadPs: Load 512-bits (composed of 16 packed single-precision (32-bit)
// floating-point elements) from memory into 'dst'. 
// 	'mem_addr' must be aligned on a 64-byte boundary or a general-protection
// exception may be generated. 
//
//		dst[511:0] := MEM[mem_addr+511:mem_addr]
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVAPS'. Intrinsic: '_mm512_load_ps'.
// Requires KNCNI.
func LoadPs(mem_addr uintptr) M512 {
	return M512(loadPs(uintptr(mem_addr)))
}

func loadPs(mem_addr uintptr) [16]float32


// MaskLoadPs: Load packed single-precision (32-bit) floating-point elements
// from memory into 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 'mem_addr' must be aligned on a
// 64-byte boundary or a general-protection exception may be generated. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVAPS'. Intrinsic: '_mm512_mask_load_ps'.
// Requires KNCNI.
func MaskLoadPs(src M512, k Mmask16, mem_addr uintptr) M512 {
	return M512(maskLoadPs([16]float32(src), uint16(k), uintptr(mem_addr)))
}

func maskLoadPs(src [16]float32, k uint16, mem_addr uintptr) [16]float32


// MaskzLoadPs: Load packed single-precision (32-bit) floating-point elements
// from memory into 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 'mem_addr' must be aligned on a 64-byte
// boundary or a general-protection exception may be generated. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVAPS'. Intrinsic: '_mm512_maskz_load_ps'.
// Requires AVX512F.
func MaskzLoadPs(k Mmask16, mem_addr uintptr) M512 {
	return M512(maskzLoadPs(uint16(k), uintptr(mem_addr)))
}

func maskzLoadPs(k uint16, mem_addr uintptr) [16]float32


// LoadSi512: Load 512-bits of integer data from memory into 'dst'. 
// 	'mem_addr' must be aligned on a 64-byte boundary or a general-protection
// exception may be generated. 
//
//		dst[511:0] := MEM[mem_addr+511:mem_addr]
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDQA32'. Intrinsic: '_mm512_load_si512'.
// Requires KNCNI.
func LoadSi512(mem_addr uintptr) M512i {
	return M512i(loadSi512(uintptr(mem_addr)))
}

func loadSi512(mem_addr uintptr) [64]byte


// MaskLoaduEpi16: Load packed 16-bit integers from memory into 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := MEM[mem_addr+i+15:mem_addr+i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDQU16'. Intrinsic: '_mm512_mask_loadu_epi16'.
// Requires AVX512BW.
func MaskLoaduEpi16(src M512i, k Mmask32, mem_addr uintptr) M512i {
	return M512i(maskLoaduEpi16([64]byte(src), uint32(k), uintptr(mem_addr)))
}

func maskLoaduEpi16(src [64]byte, k uint32, mem_addr uintptr) [64]byte


// MaskzLoaduEpi16: Load packed 16-bit integers from memory into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := MEM[mem_addr+i+15:mem_addr+i]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDQU16'. Intrinsic: '_mm512_maskz_loadu_epi16'.
// Requires AVX512BW.
func MaskzLoaduEpi16(k Mmask32, mem_addr uintptr) M512i {
	return M512i(maskzLoaduEpi16(uint32(k), uintptr(mem_addr)))
}

func maskzLoaduEpi16(k uint32, mem_addr uintptr) [64]byte


// MaskLoaduEpi32: Load packed 32-bit integers from memory into 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDQU32'. Intrinsic: '_mm512_mask_loadu_epi32'.
// Requires AVX512F.
func MaskLoaduEpi32(src M512i, k Mmask16, mem_addr uintptr) M512i {
	return M512i(maskLoaduEpi32([64]byte(src), uint16(k), uintptr(mem_addr)))
}

func maskLoaduEpi32(src [64]byte, k uint16, mem_addr uintptr) [64]byte


// MaskzLoaduEpi32: Load packed 32-bit integers from memory into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDQU32'. Intrinsic: '_mm512_maskz_loadu_epi32'.
// Requires AVX512F.
func MaskzLoaduEpi32(k Mmask16, mem_addr uintptr) M512i {
	return M512i(maskzLoaduEpi32(uint16(k), uintptr(mem_addr)))
}

func maskzLoaduEpi32(k uint16, mem_addr uintptr) [64]byte


// MaskLoaduEpi64: Load packed 64-bit integers from memory into 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDQU64'. Intrinsic: '_mm512_mask_loadu_epi64'.
// Requires AVX512F.
func MaskLoaduEpi64(src M512i, k Mmask8, mem_addr uintptr) M512i {
	return M512i(maskLoaduEpi64([64]byte(src), uint8(k), uintptr(mem_addr)))
}

func maskLoaduEpi64(src [64]byte, k uint8, mem_addr uintptr) [64]byte


// MaskzLoaduEpi64: Load packed 64-bit integers from memory into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDQU64'. Intrinsic: '_mm512_maskz_loadu_epi64'.
// Requires AVX512F.
func MaskzLoaduEpi64(k Mmask8, mem_addr uintptr) M512i {
	return M512i(maskzLoaduEpi64(uint8(k), uintptr(mem_addr)))
}

func maskzLoaduEpi64(k uint8, mem_addr uintptr) [64]byte


// MaskLoaduEpi8: Load packed 8-bit integers from memory into 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := MEM[mem_addr+i+7:mem_addr+i]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDQU8'. Intrinsic: '_mm512_mask_loadu_epi8'.
// Requires AVX512BW.
func MaskLoaduEpi8(src M512i, k Mmask64, mem_addr uintptr) M512i {
	return M512i(maskLoaduEpi8([64]byte(src), uint64(k), uintptr(mem_addr)))
}

func maskLoaduEpi8(src [64]byte, k uint64, mem_addr uintptr) [64]byte


// MaskzLoaduEpi8: Load packed 8-bit integers from memory into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := MEM[mem_addr+i+7:mem_addr+i]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDQU8'. Intrinsic: '_mm512_maskz_loadu_epi8'.
// Requires AVX512BW.
func MaskzLoaduEpi8(k Mmask64, mem_addr uintptr) M512i {
	return M512i(maskzLoaduEpi8(uint64(k), uintptr(mem_addr)))
}

func maskzLoaduEpi8(k uint64, mem_addr uintptr) [64]byte


// LoaduPd: Load 512-bits (composed of 8 packed double-precision (64-bit)
// floating-point elements) from memory into 'dst'. 
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		dst[511:0] := MEM[mem_addr+511:mem_addr]
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVUPD'. Intrinsic: '_mm512_loadu_pd'.
// Requires AVX512F.
func LoaduPd(mem_addr uintptr) M512d {
	return M512d(loaduPd(uintptr(mem_addr)))
}

func loaduPd(mem_addr uintptr) [8]float64


// MaskLoaduPd: Load packed double-precision (64-bit) floating-point elements
// from memoy into 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVUPD'. Intrinsic: '_mm512_mask_loadu_pd'.
// Requires AVX512F.
func MaskLoaduPd(src M512d, k Mmask8, mem_addr uintptr) M512d {
	return M512d(maskLoaduPd([8]float64(src), uint8(k), uintptr(mem_addr)))
}

func maskLoaduPd(src [8]float64, k uint8, mem_addr uintptr) [8]float64


// MaskzLoaduPd: Load packed double-precision (64-bit) floating-point elements
// from memoy into 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVUPD'. Intrinsic: '_mm512_maskz_loadu_pd'.
// Requires AVX512F.
func MaskzLoaduPd(k Mmask8, mem_addr uintptr) M512d {
	return M512d(maskzLoaduPd(uint8(k), uintptr(mem_addr)))
}

func maskzLoaduPd(k uint8, mem_addr uintptr) [8]float64


// LoaduPs: Load 512-bits (composed of 16 packed single-precision (32-bit)
// floating-point elements) from memory into 'dst'. 
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		dst[511:0] := MEM[mem_addr+511:mem_addr]
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVUPS'. Intrinsic: '_mm512_loadu_ps'.
// Requires AVX512F.
func LoaduPs(mem_addr uintptr) M512 {
	return M512(loaduPs(uintptr(mem_addr)))
}

func loaduPs(mem_addr uintptr) [16]float32


// MaskLoaduPs: Load packed single-precision (32-bit) floating-point elements
// from memory into 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVUPS'. Intrinsic: '_mm512_mask_loadu_ps'.
// Requires AVX512F.
func MaskLoaduPs(src M512, k Mmask16, mem_addr uintptr) M512 {
	return M512(maskLoaduPs([16]float32(src), uint16(k), uintptr(mem_addr)))
}

func maskLoaduPs(src [16]float32, k uint16, mem_addr uintptr) [16]float32


// MaskzLoaduPs: Load packed single-precision (32-bit) floating-point elements
// from memory into 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVUPS'. Intrinsic: '_mm512_maskz_loadu_ps'.
// Requires AVX512F.
func MaskzLoaduPs(k Mmask16, mem_addr uintptr) M512 {
	return M512(maskzLoaduPs(uint16(k), uintptr(mem_addr)))
}

func maskzLoaduPs(k uint16, mem_addr uintptr) [16]float32


// LoaduSi512: Load 512-bits of integer data from memory into 'dst'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		dst[511:0] := MEM[mem_addr+511:mem_addr]
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDQU32'. Intrinsic: '_mm512_loadu_si512'.
// Requires AVX512F.
func LoaduSi512(mem_addr uintptr) M512i {
	return M512i(loaduSi512(uintptr(mem_addr)))
}

func loaduSi512(mem_addr uintptr) [64]byte


// LoadunpackhiEpi32: Loads the high-64-byte-aligned portion of the
// byte/word/doubleword stream starting at element-aligned address mt-64 and
// expands them into packed 32-bit integers in 'dst'. The initial values of
// 'dst' are copied from 'src'. Only those converted doublewords that occur at
// or after the first 64-byte-aligned address following (mt-64) are loaded.
// Elements in the resulting vector that do not map to those doublewords are
// taken from 'src'. 
//
//		dst[511:0] := src[511:0]
//		loadOffset := 0
//		foundNext64BytesBoundary := false
//		addr = mt-64
//		FOR j := 0 to 15
//			IF foundNext64BytesBoundary == false
//				IF (addr + (loadOffset + 1)*4 % 64) == 0
//					foundNext64BytesBoundary := true
//				FI
//			ELSE
//				i := j*32
//				tmp := MEM[addr + loadOffset*4]
//				dst[i+31:i] := tmp[i+31:i]
//			FI
//			loadOffset := loadOffset + 1
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VLOADUNPACKHD'. Intrinsic: '_mm512_loadunpackhi_epi32'.
// Requires KNCNI.
func LoadunpackhiEpi32(src M512i, mt uintptr) M512i {
	return M512i(loadunpackhiEpi32([64]byte(src), uintptr(mt)))
}

func loadunpackhiEpi32(src [64]byte, mt uintptr) [64]byte


// MaskLoadunpackhiEpi32: Loads the high-64-byte-aligned portion of the
// byte/word/doubleword stream starting at element-aligned address mt-64 and
// expands them into packed 32-bit integers in 'dst'. The initial values of
// 'dst' are copied from 'src'. Only those converted doublewords that occur at
// or after the first 64-byte-aligned address following (mt-64) are loaded.
// Elements in the resulting vector that do not map to those doublewords are
// taken from 'src'. Elements are loaded from memory according to element
// selector 'k' (elements are skipped when the corresponding mask bit is not
// set). 
//
//		dst[511:0] := src[511:0]
//		loadOffset := 0
//		foundNext64BytesBoundary := false
//		addr = mt-64
//		FOR j := 0 to 15
//			IF k[j]
//				IF foundNext64BytesBoundary == false
//					IF (addr + (loadOffset + 1)*4 % 64) == 0
//						foundNext64BytesBoundary := true
//					FI
//				ELSE
//					i := j*32
//					tmp := MEM[addr + loadOffset*4]
//					dst[i+31:i] := tmp[i+31:i]
//				FI
//				loadOffset := loadOffset + 1
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VLOADUNPACKHD'. Intrinsic: '_mm512_mask_loadunpackhi_epi32'.
// Requires KNCNI.
func MaskLoadunpackhiEpi32(src M512i, k Mmask16, mt uintptr) M512i {
	return M512i(maskLoadunpackhiEpi32([64]byte(src), uint16(k), uintptr(mt)))
}

func maskLoadunpackhiEpi32(src [64]byte, k uint16, mt uintptr) [64]byte


// LoadunpackhiEpi64: Loads the high-64-byte-aligned portion of the quadword
// stream starting at element-aligned address mt-64 and expands them into
// packed 64-bit integers in 'dst'. The initial values of 'dst' are copied from
// 'src'. Only those converted quadwords that occur at or after the first
// 64-byte-aligned address following (mt-64) are loaded. Elements in the
// resulting vector that do not map to those quadwords are taken from 'src'. 
//
//		dst[511:0] := src[511:0]
//		loadOffset := 0
//		foundNext64BytesBoundary := false
//		addr = mt-64
//		FOR j := 0 to 7
//			IF foundNext64BytesBoundary == false
//				IF (addr + (loadOffset + 1)*8) == 0
//					foundNext64BytesBoundary := true
//				FI
//			ELSE
//				i := j*64
//				tmp := MEM[addr + loadOffset*8]
//				dst[i+63:i] := tmp[i+63:i]
//			FI
//			loadOffset := loadOffset + 1
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VLOADUNPACKHQ'. Intrinsic: '_mm512_loadunpackhi_epi64'.
// Requires KNCNI.
func LoadunpackhiEpi64(src M512i, mt uintptr) M512i {
	return M512i(loadunpackhiEpi64([64]byte(src), uintptr(mt)))
}

func loadunpackhiEpi64(src [64]byte, mt uintptr) [64]byte


// MaskLoadunpackhiEpi64: Loads the high-64-byte-aligned portion of the
// quadword stream starting at element-aligned address mt-64 and expands them
// into packed 64-bit integers in 'dst'. The initial values of 'dst' are copied
// from 'src'. Only those converted quadwords that occur at or after the first
// 64-byte-aligned address following (mt-64) are loaded. Elements in the
// resulting vector that do not map to those quadwords are taken from 'src'.
// Elements are loaded from memory according to element selector 'k' (elements
// are skipped when the corresponding mask bit is not set). 
//
//		dst[511:0] := src[511:0]
//		loadOffset := 0
//		foundNext64BytesBoundary := false
//		addr = mt-64
//		FOR j := 0 to 7
//			IF k[j]
//				IF foundNext64BytesBoundary == false
//					IF (addr + (loadOffset + 1)*8) == 0
//						foundNext64BytesBoundary := true
//					FI
//				ELSE
//					i := j*64
//					tmp := MEM[addr + loadOffset*8]
//					dst[i+63:i] := tmp[i+63:i]
//				FI
//				loadOffset := loadOffset + 1
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VLOADUNPACKHQ'. Intrinsic: '_mm512_mask_loadunpackhi_epi64'.
// Requires KNCNI.
func MaskLoadunpackhiEpi64(src M512i, k Mmask8, mt uintptr) M512i {
	return M512i(maskLoadunpackhiEpi64([64]byte(src), uint8(k), uintptr(mt)))
}

func maskLoadunpackhiEpi64(src [64]byte, k uint8, mt uintptr) [64]byte


// LoadunpackhiPd: Loads the high-64-byte-aligned portion of the quadword
// stream starting at element-aligned address mt-64 and expands them into
// packed double-precision (64-bit) floating-point values in 'dst'. The initial
// values of 'dst' are copied from 'src'. Only those converted quadwords that
// occur at or after the first 64-byte-aligned address following (mt-64) are
// loaded. Elements in the resulting vector that do not map to those quadwords
// are taken from 'src'. 
//
//		dst[511:0] := src[511:0]
//		loadOffset := 0
//		foundNext64BytesBoundary := false
//		addr = mt-64
//		FOR j := 0 to 7
//			IF foundNext64BytesBoundary == false
//				IF (addr + (loadOffset + 1)*8) % 64 == 0
//					foundNext64BytesBoundary := true
//				FI
//			ELSE
//				i := j*64
//				tmp := MEM[addr + loadOffset*8]
//				dst[i+63:i] := tmp[i+63:i]
//			FI
//			loadOffset := loadOffset + 1
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VLOADUNPACKHPD'. Intrinsic: '_mm512_loadunpackhi_pd'.
// Requires KNCNI.
func LoadunpackhiPd(src M512d, mt uintptr) M512d {
	return M512d(loadunpackhiPd([8]float64(src), uintptr(mt)))
}

func loadunpackhiPd(src [8]float64, mt uintptr) [8]float64


// MaskLoadunpackhiPd: Loads the high-64-byte-aligned portion of the quadword
// stream starting at element-aligned address mt-64 and expands them into
// packed double-precision (64-bit) floating-point values in 'dst'. The initial
// values of 'dst' are copied from 'src'. Only those converted quadwords that
// occur at or after the first 64-byte-aligned address following (mt-64) are
// loaded. Elements in the resulting vector that do not map to those quadwords
// are taken from 'src'. Elements are loaded from memory according to element
// selector 'k' (elements are skipped when the corresponding mask bit is not
// set). 
//
//		dst[511:0] := src[511:0]
//		loadOffset := 0
//		foundNext64BytesBoundary := false
//		addr = mt-64
//		FOR j := 0 to 7
//			IF k[j]
//				IF foundNext64BytesBoundary == false
//					IF (addr + (loadOffset + 1)*8) % 64 == 0
//						foundNext64BytesBoundary := true
//					FI
//				ELSE
//					i := j*64
//					tmp := MEM[addr + loadOffset*8]
//					dst[i+63:i] := tmp[i+63:i]
//				FI
//				loadOffset := loadOffset + 1
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VLOADUNPACKHPD'. Intrinsic: '_mm512_mask_loadunpackhi_pd'.
// Requires KNCNI.
func MaskLoadunpackhiPd(src M512d, k Mmask8, mt uintptr) M512d {
	return M512d(maskLoadunpackhiPd([8]float64(src), uint8(k), uintptr(mt)))
}

func maskLoadunpackhiPd(src [8]float64, k uint8, mt uintptr) [8]float64


// LoadunpackhiPs: Loads the high-64-byte-aligned portion of the
// byte/word/doubleword stream starting at element-aligned address mt-64 and
// expands them into packed single-precision (32-bit) floating-point elements
// in 'dst'. The initial values of 'dst' are copied from 'src'. Only those
// converted quadwords that occur at or after the first 64-byte-aligned address
// following (mt-64) are loaded. Elements in the resulting vector that do not
// map to those quadwords are taken from 'src'. 
//
//		dst[511:0] := src[511:0]
//		loadOffset := 0
//		foundNext64BytesBoundary := false
//		addr = mt-64
//		FOR j := 0 to 15
//			IF foundNext64BytesBoundary == false
//				IF (addr + (loadOffset + 1)*4 % 64) == 0
//					foundNext64BytesBoundary := true
//				FI
//			ELSE
//				i := j*32
//				tmp := MEM[addr + loadOffset*4]
//				dst[i+31:i] := tmp[i+31:i]
//			FI
//			loadOffset := loadOffset + 1
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VLOADUNPACKHPS'. Intrinsic: '_mm512_loadunpackhi_ps'.
// Requires KNCNI.
func LoadunpackhiPs(src M512, mt uintptr) M512 {
	return M512(loadunpackhiPs([16]float32(src), uintptr(mt)))
}

func loadunpackhiPs(src [16]float32, mt uintptr) [16]float32


// MaskLoadunpackhiPs: Loads the high-64-byte-aligned portion of the doubleword
// stream starting at element-aligned address mt-64 and expands them into
// packed single-precision (32-bit) floating-point elements in 'dst'. The
// initial values of 'dst' are copied from 'src'. Only those converted
// quadwords that occur at or after the first 64-byte-aligned address following
// (mt-64) are loaded. Elements in the resulting vector that do not map to
// those quadwords are taken from 'src'. Elements are loaded from memory
// according to element selector 'k' (elements are skipped when the
// corresponding mask bit is not set). 
//
//		dst[511:0] := src[511:0]
//		loadOffset := 0
//		foundNext64BytesBoundary := false
//		addr = mt-64
//		FOR j := 0 to 15
//			IF k[j]
//				IF foundNext64BytesBoundary == false
//					IF (addr + (loadOffset + 1)*4 % 64) == 0
//						foundNext64BytesBoundary := true
//					FI
//				ELSE
//					i := j*32
//					tmp := MEM[addr + loadOffset*4]
//					dst[i+31:i] := tmp[i+31:i]
//				FI
//				loadOffset := loadOffset + 1
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VLOADUNPACKHPS'. Intrinsic: '_mm512_mask_loadunpackhi_ps'.
// Requires KNCNI.
func MaskLoadunpackhiPs(src M512, k Mmask16, mt uintptr) M512 {
	return M512(maskLoadunpackhiPs([16]float32(src), uint16(k), uintptr(mt)))
}

func maskLoadunpackhiPs(src [16]float32, k uint16, mt uintptr) [16]float32


// LoadunpackloEpi32: Loads the low-64-byte-aligned portion of the
// byte/word/doubleword stream starting at element-aligned address mt and
// expanded into packed 32-bit integers in 'dst'. The initial values of 'dst'
// are copied from 'src'. Only those converted doublewords that occur before
// first 64-byte-aligned address following 'mt' are loaded. Elements in the
// resulting vector that do not map to those doublewords are taken from 'src'. 
//
//		dst[511:0] := src[511:0]
//		loadOffset := 0
//		addr = mt
//		FOR j := 0 to 15
//			i := j*32
//			tmp := MEM[addr + loadOffset*4]
//			dst[i+31:i] := tmp[i+31:i]
//			loadOffset := loadOffset + 1
//			IF (mt + loadOffset * 4) % 64 == 0
//				break
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VLOADUNPACKLD'. Intrinsic: '_mm512_loadunpacklo_epi32'.
// Requires KNCNI.
func LoadunpackloEpi32(src M512i, mt uintptr) M512i {
	return M512i(loadunpackloEpi32([64]byte(src), uintptr(mt)))
}

func loadunpackloEpi32(src [64]byte, mt uintptr) [64]byte


// MaskLoadunpackloEpi32: Loads the low-64-byte-aligned portion of the
// byte/word/doubleword stream starting at element-aligned address mt and
// expands them into packed 32-bit integers in 'dst'. The initial values of
// 'dst' are copied from 'src'. Only those converted doublewords that occur
// before first 64-byte-aligned address following 'mt' are loaded. Elements in
// the resulting vector that do not map to those doublewords are taken from
// 'src'. Elements are loaded from memory according to element selector 'k'
// (elements are skipped when the corresponding mask bit is not set). 
//
//		dst[511:0] := src[511:0]
//		loadOffset := 0
//		addr = mt
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				tmp := MEM[addr + loadOffset*4]
//				dst[i+31:i] := tmp[i+31:i]
//				loadOffset := loadOffset + 1
//				IF (mt + loadOffset * 4) % 64 == 0
//					break
//				FI
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VLOADUNPACKLD'. Intrinsic: '_mm512_mask_loadunpacklo_epi32'.
// Requires KNCNI.
func MaskLoadunpackloEpi32(src M512i, k Mmask16, mt uintptr) M512i {
	return M512i(maskLoadunpackloEpi32([64]byte(src), uint16(k), uintptr(mt)))
}

func maskLoadunpackloEpi32(src [64]byte, k uint16, mt uintptr) [64]byte


// LoadunpackloEpi64: Loads the low-64-byte-aligned portion of the quadword
// stream starting at element-aligned address mt and expands them into packed
// 64-bit integers in 'dst'. The initial values of 'dst' are copied from 'src'.
// Only those converted quad that occur before first 64-byte-aligned address
// following 'mt' are loaded. Elements in the resulting vector that do not map
// to those quadwords are taken from 'src'. 
//
//		dst[511:0] := src[511:0]
//		loadOffset := 0
//		addr = mt
//		FOR j := 0 to 7
//			i := j*64
//			tmp := MEM[addr + loadOffset*8]
//			dst[i+63:i] := tmp[i+63:i]
//			loadOffset := loadOffset + 1
//			IF (addr + loadOffset*8 % 64) == 0
//				break
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VLOADUNPACKLQ'. Intrinsic: '_mm512_loadunpacklo_epi64'.
// Requires KNCNI.
func LoadunpackloEpi64(src M512i, mt uintptr) M512i {
	return M512i(loadunpackloEpi64([64]byte(src), uintptr(mt)))
}

func loadunpackloEpi64(src [64]byte, mt uintptr) [64]byte


// MaskLoadunpackloEpi64: Loads the low-64-byte-aligned portion of the quadword
// stream starting at element-aligned address mt and expands them into packed
// 64-bit integers in 'dst'. The initial values of 'dst' are copied from 'src'.
// Only those converted quad that occur before first 64-byte-aligned address
// following 'mt' are loaded. Elements in the resulting vector that do not map
// to those quadwords are taken from 'src'. Elements are loaded from memory
// according to element selector 'k' (elements are skipped when the
// corresponding mask bit is not set). 
//
//		dst[511:0] := src[511:0]
//		loadOffset := 0
//		addr = mt
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				tmp := MEM[addr + loadOffset*8]
//				dst[i+63:i] := tmp[i+63:i]
//				loadOffset := loadOffset + 1
//				IF (addr + loadOffset*8 % 64) == 0
//					break
//				FI
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VLOADUNPACKLQ'. Intrinsic: '_mm512_mask_loadunpacklo_epi64'.
// Requires KNCNI.
func MaskLoadunpackloEpi64(src M512i, k Mmask8, mt uintptr) M512i {
	return M512i(maskLoadunpackloEpi64([64]byte(src), uint8(k), uintptr(mt)))
}

func maskLoadunpackloEpi64(src [64]byte, k uint8, mt uintptr) [64]byte


// LoadunpackloPd: Loads the low-64-byte-aligned portion of the quadword stream
// starting at element-aligned address mt and expands them into packed
// double-precision (64-bit) floating-point elements in 'dst'. The initial
// values of 'dst' are copied from 'src'. Only those converted quad that occur
// before first 64-byte-aligned address following 'mt' are loaded. Elements in
// the resulting vector that do not map to those quadwords are taken from
// 'src'. 
//
//		dst[511:0] := src[511:0]
//		loadOffset := 0
//		addr = mt
//		FOR j := 0 to 7
//			i := j*64
//			tmp := MEM[addr + loadOffset*8]
//			dst[i+63:i] := tmp[i+63:i]
//			loadOffset := loadOffset + 1
//			IF ((addr + 8*loadOffset) % 64) == 0
//				BREAK
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VLOADUNPACKLPD'. Intrinsic: '_mm512_loadunpacklo_pd'.
// Requires KNCNI.
func LoadunpackloPd(src M512d, mt uintptr) M512d {
	return M512d(loadunpackloPd([8]float64(src), uintptr(mt)))
}

func loadunpackloPd(src [8]float64, mt uintptr) [8]float64


// MaskLoadunpackloPd: Loads the low-64-byte-aligned portion of the quadword
// stream starting at element-aligned address mt and expands them into packed
// double-precision (64-bit) floating-point values in 'dst'. The initial values
// of 'dst' are copied from 'src'. Only those converted quad that occur before
// first 64-byte-aligned address following 'mt' are loaded. Elements in the
// resulting vector that do not map to those quadwords are taken from 'src'.
// Elements are loaded from memory according to element selector 'k' (elements
// are skipped when the corresponding mask bit is not set). 
//
//		dst[511:0] := src[511:0]
//		loadOffset := 0
//		addr = mt
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				tmp := MEM[addr + loadOffset*8]
//				dst[i+63:i] := tmp[i+63:i]
//				loadOffset := loadOffset + 1
//				IF ((addr + 8*loadOffset) % 64) == 0
//					BREAK
//				FI
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VLOADUNPACKLPD'. Intrinsic: '_mm512_mask_loadunpacklo_pd'.
// Requires KNCNI.
func MaskLoadunpackloPd(src M512d, k Mmask8, mt uintptr) M512d {
	return M512d(maskLoadunpackloPd([8]float64(src), uint8(k), uintptr(mt)))
}

func maskLoadunpackloPd(src [8]float64, k uint8, mt uintptr) [8]float64


// LoadunpackloPs: Loads the low-64-byte-aligned portion of the doubleword
// stream starting at element-aligned address mt and expanded into packed
// single-precision (32-bit) floating-point elements in 'dst'. The initial
// values of 'dst' are copied from 'src'. Only those converted doublewords that
// occur before first 64-byte-aligned address following 'mt' are loaded.
// Elements in the resulting vector that do not map to those doublewords are
// taken from 'src'. 
//
//		dst[511:0] := src[511:0]
//		loadOffset := 0
//		addr = mt
//		FOR j := 0 to 15
//			i := j*32
//			tmp := MEM[addr + loadOffset*4]
//			dst[i+31:i] := tmp[i+31:i]
//			loadOffset := loadOffset + 1
//			IF (mt + loadOffset * 4) % 64 == 0
//				BREAK
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VLOADUNPACKLPS'. Intrinsic: '_mm512_loadunpacklo_ps'.
// Requires KNCNI.
func LoadunpackloPs(src M512, mt uintptr) M512 {
	return M512(loadunpackloPs([16]float32(src), uintptr(mt)))
}

func loadunpackloPs(src [16]float32, mt uintptr) [16]float32


// MaskLoadunpackloPs: Loads the low-64-byte-aligned portion of the doubleword
// stream starting at element-aligned address mt and expanded into packed
// single-precision (32-bit) floating-point elements in 'dst'. The initial
// values of 'dst' are copied from 'src'. Only those converted doublewords that
// occur before first 64-byte-aligned address following 'mt' are loaded.
// Elements in the resulting vector that do not map to those doublewords are
// taken from 'src'. Elements are loaded from memory according to element
// selector 'k' (elements are skipped when the corresponding mask bit is not
// set). 
//
//		dst[511:0] := src[511:0]
//		loadOffset := 0
//		addr = mt
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				tmp := MEM[addr + loadOffset*4]
//				dst[i+31:i] := tmp[i+31:i]
//				loadOffset := loadOffset + 1
//				IF (mt + loadOffset * 4) % 64 == 0
//					break
//				FI
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VLOADUNPACKLPS'. Intrinsic: '_mm512_mask_loadunpacklo_ps'.
// Requires KNCNI.
func MaskLoadunpackloPs(src M512, k Mmask16, mt uintptr) M512 {
	return M512(maskLoadunpackloPs([16]float32(src), uint16(k), uintptr(mt)))
}

func maskLoadunpackloPs(src [16]float32, k uint16, mt uintptr) [16]float32


// LogPd: Compute the natural logarithm of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ln(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_log_pd'.
// Requires AVX512F.
func LogPd(a M512d) M512d {
	return M512d(logPd([8]float64(a)))
}

func logPd(a [8]float64) [8]float64


// MaskLogPd: Compute the natural logarithm of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ln(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_log_pd'.
// Requires AVX512F.
func MaskLogPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskLogPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskLogPd(src [8]float64, k uint8, a [8]float64) [8]float64


// LogPs: Compute the natural logarithm of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ln(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_log_ps'.
// Requires AVX512F.
func LogPs(a M512) M512 {
	return M512(logPs([16]float32(a)))
}

func logPs(a [16]float32) [16]float32


// MaskLogPs: Compute the natural logarithm of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ln(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_log_ps'.
// Requires AVX512F.
func MaskLogPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskLogPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskLogPs(src [16]float32, k uint16, a [16]float32) [16]float32


// Log10Pd: Compute the base-10 logarithm of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := log10(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_log10_pd'.
// Requires AVX512F.
func Log10Pd(a M512d) M512d {
	return M512d(log10Pd([8]float64(a)))
}

func log10Pd(a [8]float64) [8]float64


// MaskLog10Pd: Compute the base-10 logarithm of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := log10(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_log10_pd'.
// Requires AVX512F.
func MaskLog10Pd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskLog10Pd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskLog10Pd(src [8]float64, k uint8, a [8]float64) [8]float64


// Log10Ps: Compute the base-10 logarithm of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := log10(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_log10_ps'.
// Requires AVX512F.
func Log10Ps(a M512) M512 {
	return M512(log10Ps([16]float32(a)))
}

func log10Ps(a [16]float32) [16]float32


// MaskLog10Ps: Compute the base-10 logarithm of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := log10(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_log10_ps'.
// Requires AVX512F.
func MaskLog10Ps(src M512, k Mmask16, a M512) M512 {
	return M512(maskLog10Ps([16]float32(src), uint16(k), [16]float32(a)))
}

func maskLog10Ps(src [16]float32, k uint16, a [16]float32) [16]float32


// Log1pPd: Compute the natural logarithm of one plus packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ln(1.0 + a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_log1p_pd'.
// Requires AVX512F.
func Log1pPd(a M512d) M512d {
	return M512d(log1pPd([8]float64(a)))
}

func log1pPd(a [8]float64) [8]float64


// MaskLog1pPd: Compute the natural logarithm of one plus packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ln(1.0 + a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_log1p_pd'.
// Requires AVX512F.
func MaskLog1pPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskLog1pPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskLog1pPd(src [8]float64, k uint8, a [8]float64) [8]float64


// Log1pPs: Compute the natural logarithm of one plus packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ln(1.0 + a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_log1p_ps'.
// Requires AVX512F.
func Log1pPs(a M512) M512 {
	return M512(log1pPs([16]float32(a)))
}

func log1pPs(a [16]float32) [16]float32


// MaskLog1pPs: Compute the natural logarithm of one plus packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ln(1.0 + a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_log1p_ps'.
// Requires AVX512F.
func MaskLog1pPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskLog1pPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskLog1pPs(src [16]float32, k uint16, a [16]float32) [16]float32


// Log2Pd: Compute the base-2 logarithm of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := log2(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_log2_pd'.
// Requires AVX512F.
func Log2Pd(a M512d) M512d {
	return M512d(log2Pd([8]float64(a)))
}

func log2Pd(a [8]float64) [8]float64


// MaskLog2Pd: Compute the base-2 logarithm of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := log2(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_log2_pd'.
// Requires AVX512F.
func MaskLog2Pd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskLog2Pd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskLog2Pd(src [8]float64, k uint8, a [8]float64) [8]float64


// Log2Ps: Compute the base-2 logarithm of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := log2(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VLOG2PS'. Intrinsic: '_mm512_log2_ps'.
// Requires KNCNI.
func Log2Ps(a M512) M512 {
	return M512(log2Ps([16]float32(a)))
}

func log2Ps(a [16]float32) [16]float32


// MaskLog2Ps: Compute the base-2 logarithm of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := log2(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VLOG2PS'. Intrinsic: '_mm512_mask_log2_ps'.
// Requires KNCNI.
func MaskLog2Ps(src M512, k Mmask16, a M512) M512 {
	return M512(maskLog2Ps([16]float32(src), uint16(k), [16]float32(a)))
}

func maskLog2Ps(src [16]float32, k uint16, a [16]float32) [16]float32


// Log2ae23Ps: Compute the base-2 logarithm of packed single-precision (32-bit)
// floating-point elements in 'a' with absolute error of 2^(-23) and store the
// results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := Log2ae23(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VLOG2PS'. Intrinsic: '_mm512_log2ae23_ps'.
// Requires KNCNI.
func Log2ae23Ps(a M512) M512 {
	return M512(log2ae23Ps([16]float32(a)))
}

func log2ae23Ps(a [16]float32) [16]float32


// MaskLog2ae23Ps: Compute the base-2 logarithm of packed single-precision
// (32-bit) floating-point elements in 'a' with absolute error of 2^(-23) and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := Log2ae23(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VLOG2PS'. Intrinsic: '_mm512_mask_log2ae23_ps'.
// Requires KNCNI.
func MaskLog2ae23Ps(src M512, k Mmask16, a M512) M512 {
	return M512(maskLog2ae23Ps([16]float32(src), uint16(k), [16]float32(a)))
}

func maskLog2ae23Ps(src [16]float32, k uint16, a [16]float32) [16]float32


// LogbPd: Convert the exponent of each packed double-precision (64-bit)
// floating-point element in 'a' to a double-precision floating-point number
// representing the integer exponent, and store the results in 'dst'. This
// intrinsic essentially calculates 'floor(log2(x))' for each element. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ConvertExpFP64(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_logb_pd'.
// Requires AVX512F.
func LogbPd(a M512d) M512d {
	return M512d(logbPd([8]float64(a)))
}

func logbPd(a [8]float64) [8]float64


// MaskLogbPd: Convert the exponent of each packed double-precision (64-bit)
// floating-point element in 'a' to a double-precision floating-point number
// representing the integer exponent, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). This intrinsic essentially calculates 'floor(log2(x))' for
// each element. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ConvertExpFP64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_logb_pd'.
// Requires AVX512F.
func MaskLogbPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskLogbPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskLogbPd(src [8]float64, k uint8, a [8]float64) [8]float64


// LogbPs: Convert the exponent of each packed single-precision (32-bit)
// floating-point element in 'a' to a single-precision floating-point number
// representing the integer exponent, and store the results in 'dst'. This
// intrinsic essentially calculates 'floor(log2(x))' for each element. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ConvertExpFP32(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_logb_ps'.
// Requires AVX512F.
func LogbPs(a M512) M512 {
	return M512(logbPs([16]float32(a)))
}

func logbPs(a [16]float32) [16]float32


// MaskLogbPs: Convert the exponent of each packed single-precision (32-bit)
// floating-point element in 'a' to a single-precision floating-point number
// representing the integer exponent, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). This intrinsic essentially calculates 'floor(log2(x))' for
// each element. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ConvertExpFP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_logb_ps'.
// Requires AVX512F.
func MaskLogbPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskLogbPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskLogbPs(src [16]float32, k uint16, a [16]float32) [16]float32


// LzcntEpi32: Counts the number of leading zero bits in each packed 32-bit
// integer in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			tmp := 31
//			dst[i+31:i] := 0
//			DO WHILE (tmp >= 0 AND a[i+tmp] == 0)
//				tmp := tmp - 1
//				dst[i+31:i] := dst[i+31:i] + 1
//			OD
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPLZCNTD'. Intrinsic: '_mm512_lzcnt_epi32'.
// Requires AVX512CD.
func LzcntEpi32(a M512i) M512i {
	return M512i(lzcntEpi32([64]byte(a)))
}

func lzcntEpi32(a [64]byte) [64]byte


// MaskLzcntEpi32: Counts the number of leading zero bits in each packed 32-bit
// integer in 'a', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				tmp := 31
//				dst[i+31:i] := 0
//				DO WHILE (tmp >= 0 AND a[i+tmp] == 0)
//					tmp := tmp - 1
//					dst[i+31:i] := dst[i+31:i] + 1
//				OD
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPLZCNTD'. Intrinsic: '_mm512_mask_lzcnt_epi32'.
// Requires AVX512CD.
func MaskLzcntEpi32(src M512i, k Mmask16, a M512i) M512i {
	return M512i(maskLzcntEpi32([64]byte(src), uint16(k), [64]byte(a)))
}

func maskLzcntEpi32(src [64]byte, k uint16, a [64]byte) [64]byte


// MaskzLzcntEpi32: Counts the number of leading zero bits in each packed
// 32-bit integer in 'a', and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				tmp := 31
//				dst[i+31:i] := 0
//				DO WHILE (tmp >= 0 AND a[i+tmp] == 0)
//					tmp := tmp - 1
//					dst[i+31:i] := dst[i+31:i] + 1
//				OD
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPLZCNTD'. Intrinsic: '_mm512_maskz_lzcnt_epi32'.
// Requires AVX512CD.
func MaskzLzcntEpi32(k Mmask16, a M512i) M512i {
	return M512i(maskzLzcntEpi32(uint16(k), [64]byte(a)))
}

func maskzLzcntEpi32(k uint16, a [64]byte) [64]byte


// LzcntEpi64: Counts the number of leading zero bits in each packed 64-bit
// integer in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			tmp := 63
//			dst[i+63:i] := 0
//			DO WHILE (tmp >= 0 AND a[i+tmp] == 0)
//				tmp := tmp - 1
//				dst[i+63:i] := dst[i+63:i] + 1
//			OD
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPLZCNTQ'. Intrinsic: '_mm512_lzcnt_epi64'.
// Requires AVX512CD.
func LzcntEpi64(a M512i) M512i {
	return M512i(lzcntEpi64([64]byte(a)))
}

func lzcntEpi64(a [64]byte) [64]byte


// MaskLzcntEpi64: Counts the number of leading zero bits in each packed 64-bit
// integer in 'a', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				tmp := 63
//				dst[i+63:i] := 0
//				DO WHILE (tmp >= 0 AND a[i+tmp] == 0)
//					tmp := tmp - 1
//					dst[i+63:i] := dst[i+63:i] + 1
//				OD
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPLZCNTQ'. Intrinsic: '_mm512_mask_lzcnt_epi64'.
// Requires AVX512CD.
func MaskLzcntEpi64(src M512i, k Mmask8, a M512i) M512i {
	return M512i(maskLzcntEpi64([64]byte(src), uint8(k), [64]byte(a)))
}

func maskLzcntEpi64(src [64]byte, k uint8, a [64]byte) [64]byte


// MaskzLzcntEpi64: Counts the number of leading zero bits in each packed
// 64-bit integer in 'a', and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				tmp := 63
//				dst[i+63:i] := 0
//				DO WHILE (tmp >= 0 AND a[i+tmp] == 0)
//					tmp := tmp - 1
//					dst[i+63:i] := dst[i+63:i] + 1
//				OD
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPLZCNTQ'. Intrinsic: '_mm512_maskz_lzcnt_epi64'.
// Requires AVX512CD.
func MaskzLzcntEpi64(k Mmask8, a M512i) M512i {
	return M512i(maskzLzcntEpi64(uint8(k), [64]byte(a)))
}

func maskzLzcntEpi64(k uint8, a [64]byte) [64]byte


// MaddEpi16: Multiply packed 16-bit integers in 'a' and 'b', producing
// intermediate 32-bit integers. Horizontally add adjacent pairs of
// intermediate 32-bit integers, and pack the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			st[i+31:i] := a[i+31:i+16]*b[i+31:i+16] + a[i+15:i]*b[i+15:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMADDWD'. Intrinsic: '_mm512_madd_epi16'.
// Requires AVX512BW.
func MaddEpi16(a M512i, b M512i) M512i {
	return M512i(maddEpi16([64]byte(a), [64]byte(b)))
}

func maddEpi16(a [64]byte, b [64]byte) [64]byte


// MaskMaddEpi16: Multiply packed 16-bit integers in 'a' and 'b', producing
// intermediate 32-bit integers. Horizontally add adjacent pairs of
// intermediate 32-bit integers, and pack the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i+16]*b[i+31:i+16] + a[i+15:i]*b[i+15:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMADDWD'. Intrinsic: '_mm512_mask_madd_epi16'.
// Requires AVX512BW.
func MaskMaddEpi16(src M512i, k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskMaddEpi16([64]byte(src), uint16(k), [64]byte(a), [64]byte(b)))
}

func maskMaddEpi16(src [64]byte, k uint16, a [64]byte, b [64]byte) [64]byte


// MaskzMaddEpi16: Multiply packed 16-bit integers in 'a' and 'b', producing
// intermediate 32-bit integers. Horizontally add adjacent pairs of
// intermediate 32-bit integers, and pack the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i+16]*b[i+31:i+16] + a[i+15:i]*b[i+15:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMADDWD'. Intrinsic: '_mm512_maskz_madd_epi16'.
// Requires AVX512BW.
func MaskzMaddEpi16(k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskzMaddEpi16(uint16(k), [64]byte(a), [64]byte(b)))
}

func maskzMaddEpi16(k uint16, a [64]byte, b [64]byte) [64]byte


// Madd52hiEpu64: Multiply packed unsigned 52-bit integers in each 64-bit
// element of 'b' and 'c' to form a 104-bit intermediate result. Add the high
// 52-bit unsigned integer from the intermediate result with the corresponding
// unsigned 64-bit integer in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
//			dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[103:52])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMADD52HUQ'. Intrinsic: '_mm512_madd52hi_epu64'.
// Requires AVX512IFMA52.
func Madd52hiEpu64(a M512i, b M512i, c M512i) M512i {
	return M512i(madd52hiEpu64([64]byte(a), [64]byte(b), [64]byte(c)))
}

func madd52hiEpu64(a [64]byte, b [64]byte, c [64]byte) [64]byte


// MaskMadd52hiEpu64: Multiply packed unsigned 52-bit integers in each 64-bit
// element of 'b' and 'c' to form a 104-bit intermediate result. Add the high
// 52-bit unsigned integer from the intermediate result with the corresponding
// unsigned 64-bit integer in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
//				dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[103:52])
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMADD52HUQ'. Intrinsic: '_mm512_mask_madd52hi_epu64'.
// Requires AVX512IFMA52.
func MaskMadd52hiEpu64(a M512i, k Mmask8, b M512i, c M512i) M512i {
	return M512i(maskMadd52hiEpu64([64]byte(a), uint8(k), [64]byte(b), [64]byte(c)))
}

func maskMadd52hiEpu64(a [64]byte, k uint8, b [64]byte, c [64]byte) [64]byte


// MaskzMadd52hiEpu64: Multiply packed unsigned 52-bit integers in each 64-bit
// element of 'b' and 'c' to form a 104-bit intermediate result. Add the high
// 52-bit unsigned integer from the intermediate result with the corresponding
// unsigned 64-bit integer in 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
//				dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[103:52])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMADD52HUQ'. Intrinsic: '_mm512_maskz_madd52hi_epu64'.
// Requires AVX512IFMA52.
func MaskzMadd52hiEpu64(k Mmask8, a M512i, b M512i, c M512i) M512i {
	return M512i(maskzMadd52hiEpu64(uint8(k), [64]byte(a), [64]byte(b), [64]byte(c)))
}

func maskzMadd52hiEpu64(k uint8, a [64]byte, b [64]byte, c [64]byte) [64]byte


// Madd52loEpu64: Multiply packed unsigned 52-bit integers in each 64-bit
// element of 'b' and 'c' to form a 104-bit intermediate result. Add the low
// 52-bit unsigned integer from the intermediate result with the corresponding
// unsigned 64-bit integer in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
//			dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[51:0])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMADD52LUQ'. Intrinsic: '_mm512_madd52lo_epu64'.
// Requires AVX512IFMA52.
func Madd52loEpu64(a M512i, b M512i, c M512i) M512i {
	return M512i(madd52loEpu64([64]byte(a), [64]byte(b), [64]byte(c)))
}

func madd52loEpu64(a [64]byte, b [64]byte, c [64]byte) [64]byte


// MaskMadd52loEpu64: Multiply packed unsigned 52-bit integers in each 64-bit
// element of 'b' and 'c' to form a 104-bit intermediate result. Add the low
// 52-bit unsigned integer from the intermediate result with the corresponding
// unsigned 64-bit integer in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
//				dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[51:0])
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMADD52LUQ'. Intrinsic: '_mm512_mask_madd52lo_epu64'.
// Requires AVX512IFMA52.
func MaskMadd52loEpu64(a M512i, k Mmask8, b M512i, c M512i) M512i {
	return M512i(maskMadd52loEpu64([64]byte(a), uint8(k), [64]byte(b), [64]byte(c)))
}

func maskMadd52loEpu64(a [64]byte, k uint8, b [64]byte, c [64]byte) [64]byte


// MaskzMadd52loEpu64: Multiply packed unsigned 52-bit integers in each 64-bit
// element of 'b' and 'c' to form a 104-bit intermediate result. Add the low
// 52-bit unsigned integer from the intermediate result with the corresponding
// unsigned 64-bit integer in 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
//				dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[51:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMADD52LUQ'. Intrinsic: '_mm512_maskz_madd52lo_epu64'.
// Requires AVX512IFMA52.
func MaskzMadd52loEpu64(k Mmask8, a M512i, b M512i, c M512i) M512i {
	return M512i(maskzMadd52loEpu64(uint8(k), [64]byte(a), [64]byte(b), [64]byte(c)))
}

func maskzMadd52loEpu64(k uint8, a [64]byte, b [64]byte, c [64]byte) [64]byte


// MaddubsEpi16: Vertically multiply each unsigned 8-bit integer from 'a' with
// the corresponding signed 8-bit integer from 'b', producing intermediate
// signed 16-bit integers. Horizontally add adjacent pairs of intermediate
// signed 16-bit integers, and pack the saturated results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			dst[i+15:i] := Saturate_To_Int16( a[i+15:i+8]*b[i+15:i+8] + a[i+7:i]*b[i+7:i] )
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMADDUBSW'. Intrinsic: '_mm512_maddubs_epi16'.
// Requires AVX512BW.
func MaddubsEpi16(a M512i, b M512i) M512i {
	return M512i(maddubsEpi16([64]byte(a), [64]byte(b)))
}

func maddubsEpi16(a [64]byte, b [64]byte) [64]byte


// MaskMaddubsEpi16: Multiply packed unsigned 8-bit integers in 'a' by packed
// signed 8-bit integers in 'b', producing intermediate signed 16-bit integers.
// Horizontally add adjacent pairs of intermediate signed 16-bit integers, and
// pack the saturated results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := Saturate_To_Int16( a[i+15:i+8]*b[i+15:i+8] + a[i+7:i]*b[i+7:i] )
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMADDUBSW'. Intrinsic: '_mm512_mask_maddubs_epi16'.
// Requires AVX512BW.
func MaskMaddubsEpi16(src M512i, k Mmask32, a M512i, b M512i) M512i {
	return M512i(maskMaddubsEpi16([64]byte(src), uint32(k), [64]byte(a), [64]byte(b)))
}

func maskMaddubsEpi16(src [64]byte, k uint32, a [64]byte, b [64]byte) [64]byte


// MaskzMaddubsEpi16: Multiply packed unsigned 8-bit integers in 'a' by packed
// signed 8-bit integers in 'b', producing intermediate signed 16-bit integers.
// Horizontally add adjacent pairs of intermediate signed 16-bit integers, and
// pack the saturated results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := Saturate_To_Int16( a[i+15:i+8]*b[i+15:i+8] + a[i+7:i]*b[i+7:i] )
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMADDUBSW'. Intrinsic: '_mm512_maskz_maddubs_epi16'.
// Requires AVX512BW.
func MaskzMaddubsEpi16(k Mmask32, a M512i, b M512i) M512i {
	return M512i(maskzMaddubsEpi16(uint32(k), [64]byte(a), [64]byte(b)))
}

func maskzMaddubsEpi16(k uint32, a [64]byte, b [64]byte) [64]byte


// Mask2int: Converts bit mask 'k1' into an integer value, storing the results
// in 'dst'. 
//
//		dst := SignExtend(k1)
//
// Instruction: 'KMOV'. Intrinsic: '_mm512_mask2int'.
// Requires KNCNI.
func Mask2int(k1 Mmask16) int {
	return int(mask2int(uint16(k1)))
}

func mask2int(k1 uint16) int


// MaskMaxEpi16: Compare packed 16-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				IF a[i+15:i] > b[i+15:i]
//					dst[i+15:i] := a[i+15:i]
//				ELSE
//					dst[i+15:i] := b[i+15:i]
//				FI
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMAXSW'. Intrinsic: '_mm512_mask_max_epi16'.
// Requires AVX512BW.
func MaskMaxEpi16(src M512i, k Mmask32, a M512i, b M512i) M512i {
	return M512i(maskMaxEpi16([64]byte(src), uint32(k), [64]byte(a), [64]byte(b)))
}

func maskMaxEpi16(src [64]byte, k uint32, a [64]byte, b [64]byte) [64]byte


// MaskzMaxEpi16: Compare packed 16-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				IF a[i+15:i] > b[i+15:i]
//					dst[i+15:i] := a[i+15:i]
//				ELSE
//					dst[i+15:i] := b[i+15:i]
//				FI
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMAXSW'. Intrinsic: '_mm512_maskz_max_epi16'.
// Requires AVX512BW.
func MaskzMaxEpi16(k Mmask32, a M512i, b M512i) M512i {
	return M512i(maskzMaxEpi16(uint32(k), [64]byte(a), [64]byte(b)))
}

func maskzMaxEpi16(k uint32, a [64]byte, b [64]byte) [64]byte


// MaxEpi16: Compare packed 16-bit integers in 'a' and 'b', and store packed
// maximum values in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF a[i+15:i] > b[i+15:i]
//				dst[i+15:i] := a[i+15:i]
//			ELSE
//				dst[i+15:i] := b[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMAXSW'. Intrinsic: '_mm512_max_epi16'.
// Requires AVX512BW.
func MaxEpi16(a M512i, b M512i) M512i {
	return M512i(maxEpi16([64]byte(a), [64]byte(b)))
}

func maxEpi16(a [64]byte, b [64]byte) [64]byte


// MaskMaxEpi32: Compare packed 32-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] > b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMAXSD'. Intrinsic: '_mm512_mask_max_epi32'.
// Requires KNCNI.
func MaskMaxEpi32(src M512i, k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskMaxEpi32([64]byte(src), uint16(k), [64]byte(a), [64]byte(b)))
}

func maskMaxEpi32(src [64]byte, k uint16, a [64]byte, b [64]byte) [64]byte


// MaskzMaxEpi32: Compare packed 32-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] > b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0 
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMAXSD'. Intrinsic: '_mm512_maskz_max_epi32'.
// Requires AVX512F.
func MaskzMaxEpi32(k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskzMaxEpi32(uint16(k), [64]byte(a), [64]byte(b)))
}

func maskzMaxEpi32(k uint16, a [64]byte, b [64]byte) [64]byte


// MaxEpi32: Compare packed 32-bit integers in 'a' and 'b', and store packed
// maximum values in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF a[i+31:i] > b[i+31:i]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := b[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMAXSD'. Intrinsic: '_mm512_max_epi32'.
// Requires KNCNI.
func MaxEpi32(a M512i, b M512i) M512i {
	return M512i(maxEpi32([64]byte(a), [64]byte(b)))
}

func maxEpi32(a [64]byte, b [64]byte) [64]byte


// MaskMaxEpi64: Compare packed 64-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] > b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMAXSQ'. Intrinsic: '_mm512_mask_max_epi64'.
// Requires AVX512F.
func MaskMaxEpi64(src M512i, k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskMaxEpi64([64]byte(src), uint8(k), [64]byte(a), [64]byte(b)))
}

func maskMaxEpi64(src [64]byte, k uint8, a [64]byte, b [64]byte) [64]byte


// MaskzMaxEpi64: Compare packed 64-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] > b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMAXSQ'. Intrinsic: '_mm512_maskz_max_epi64'.
// Requires AVX512F.
func MaskzMaxEpi64(k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskzMaxEpi64(uint8(k), [64]byte(a), [64]byte(b)))
}

func maskzMaxEpi64(k uint8, a [64]byte, b [64]byte) [64]byte


// MaxEpi64: Compare packed 64-bit integers in 'a' and 'b', and store packed
// maximum values in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF a[i+63:i] > b[i+63:i]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := b[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMAXSQ'. Intrinsic: '_mm512_max_epi64'.
// Requires AVX512F.
func MaxEpi64(a M512i, b M512i) M512i {
	return M512i(maxEpi64([64]byte(a), [64]byte(b)))
}

func maxEpi64(a [64]byte, b [64]byte) [64]byte


// MaskMaxEpi8: Compare packed 8-bit integers in 'a' and 'b', and store packed
// maximum values in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				IF a[i+7:i] > b[i+7:i]
//					dst[i+7:i] := a[i+7:i]
//				ELSE
//					dst[i+7:i] := b[i+7:i]
//				FI
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMAXSB'. Intrinsic: '_mm512_mask_max_epi8'.
// Requires AVX512BW.
func MaskMaxEpi8(src M512i, k Mmask64, a M512i, b M512i) M512i {
	return M512i(maskMaxEpi8([64]byte(src), uint64(k), [64]byte(a), [64]byte(b)))
}

func maskMaxEpi8(src [64]byte, k uint64, a [64]byte, b [64]byte) [64]byte


// MaskzMaxEpi8: Compare packed 8-bit integers in 'a' and 'b', and store packed
// maximum values in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				IF a[i+7:i] > b[i+7:i]
//					dst[i+7:i] := a[i+7:i]
//				ELSE
//					dst[i+7:i] := b[i+7:i]
//				FI
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMAXSB'. Intrinsic: '_mm512_maskz_max_epi8'.
// Requires AVX512BW.
func MaskzMaxEpi8(k Mmask64, a M512i, b M512i) M512i {
	return M512i(maskzMaxEpi8(uint64(k), [64]byte(a), [64]byte(b)))
}

func maskzMaxEpi8(k uint64, a [64]byte, b [64]byte) [64]byte


// MaxEpi8: Compare packed 8-bit integers in 'a' and 'b', and store packed
// maximum values in 'dst'. 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF a[i+7:i] > b[i+7:i]
//				dst[i+7:i] := a[i+7:i]
//			ELSE
//				dst[i+7:i] := b[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMAXSB'. Intrinsic: '_mm512_max_epi8'.
// Requires AVX512BW.
func MaxEpi8(a M512i, b M512i) M512i {
	return M512i(maxEpi8([64]byte(a), [64]byte(b)))
}

func maxEpi8(a [64]byte, b [64]byte) [64]byte


// MaskMaxEpu16: Compare packed unsigned 16-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				IF a[i+15:i] > b[i+15:i]
//					dst[i+15:i] := a[i+15:i]
//				ELSE
//					dst[i+15:i] := b[i+15:i]
//				FI
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMAXUW'. Intrinsic: '_mm512_mask_max_epu16'.
// Requires AVX512BW.
func MaskMaxEpu16(src M512i, k Mmask32, a M512i, b M512i) M512i {
	return M512i(maskMaxEpu16([64]byte(src), uint32(k), [64]byte(a), [64]byte(b)))
}

func maskMaxEpu16(src [64]byte, k uint32, a [64]byte, b [64]byte) [64]byte


// MaskzMaxEpu16: Compare packed unsigned 16-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				IF a[i+15:i] > b[i+15:i]
//					dst[i+15:i] := a[i+15:i]
//				ELSE
//					dst[i+15:i] := b[i+15:i]
//				FI
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMAXUW'. Intrinsic: '_mm512_maskz_max_epu16'.
// Requires AVX512BW.
func MaskzMaxEpu16(k Mmask32, a M512i, b M512i) M512i {
	return M512i(maskzMaxEpu16(uint32(k), [64]byte(a), [64]byte(b)))
}

func maskzMaxEpu16(k uint32, a [64]byte, b [64]byte) [64]byte


// MaxEpu16: Compare packed unsigned 16-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF a[i+15:i] > b[i+15:i]
//				dst[i+15:i] := a[i+15:i]
//			ELSE
//				dst[i+15:i] := b[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMAXUW'. Intrinsic: '_mm512_max_epu16'.
// Requires AVX512BW.
func MaxEpu16(a M512i, b M512i) M512i {
	return M512i(maxEpu16([64]byte(a), [64]byte(b)))
}

func maxEpu16(a [64]byte, b [64]byte) [64]byte


// MaskMaxEpu32: Compare packed unsigned 32-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] > b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMAXUD'. Intrinsic: '_mm512_mask_max_epu32'.
// Requires KNCNI.
func MaskMaxEpu32(src M512i, k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskMaxEpu32([64]byte(src), uint16(k), [64]byte(a), [64]byte(b)))
}

func maskMaxEpu32(src [64]byte, k uint16, a [64]byte, b [64]byte) [64]byte


// MaskzMaxEpu32: Compare packed unsigned 32-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] > b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMAXUD'. Intrinsic: '_mm512_maskz_max_epu32'.
// Requires AVX512F.
func MaskzMaxEpu32(k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskzMaxEpu32(uint16(k), [64]byte(a), [64]byte(b)))
}

func maskzMaxEpu32(k uint16, a [64]byte, b [64]byte) [64]byte


// MaxEpu32: Compare packed unsigned 32-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF a[i+31:i] > b[i+31:i]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := b[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMAXUD'. Intrinsic: '_mm512_max_epu32'.
// Requires KNCNI.
func MaxEpu32(a M512i, b M512i) M512i {
	return M512i(maxEpu32([64]byte(a), [64]byte(b)))
}

func maxEpu32(a [64]byte, b [64]byte) [64]byte


// MaskMaxEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] > b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMAXUQ'. Intrinsic: '_mm512_mask_max_epu64'.
// Requires AVX512F.
func MaskMaxEpu64(src M512i, k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskMaxEpu64([64]byte(src), uint8(k), [64]byte(a), [64]byte(b)))
}

func maskMaxEpu64(src [64]byte, k uint8, a [64]byte, b [64]byte) [64]byte


// MaskzMaxEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] > b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMAXUQ'. Intrinsic: '_mm512_maskz_max_epu64'.
// Requires AVX512F.
func MaskzMaxEpu64(k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskzMaxEpu64(uint8(k), [64]byte(a), [64]byte(b)))
}

func maskzMaxEpu64(k uint8, a [64]byte, b [64]byte) [64]byte


// MaxEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF a[i+63:i] > b[i+63:i]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := b[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMAXUQ'. Intrinsic: '_mm512_max_epu64'.
// Requires AVX512F.
func MaxEpu64(a M512i, b M512i) M512i {
	return M512i(maxEpu64([64]byte(a), [64]byte(b)))
}

func maxEpu64(a [64]byte, b [64]byte) [64]byte


// MaskMaxEpu8: Compare packed unsigned 8-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				IF a[i+7:i] > b[i+7:i]
//					dst[i+7:i] := a[i+7:i]
//				ELSE
//					dst[i+7:i] := b[i+7:i]
//				FI
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMAXUB'. Intrinsic: '_mm512_mask_max_epu8'.
// Requires AVX512BW.
func MaskMaxEpu8(src M512i, k Mmask64, a M512i, b M512i) M512i {
	return M512i(maskMaxEpu8([64]byte(src), uint64(k), [64]byte(a), [64]byte(b)))
}

func maskMaxEpu8(src [64]byte, k uint64, a [64]byte, b [64]byte) [64]byte


// MaskzMaxEpu8: Compare packed unsigned 8-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				IF a[i+7:i] > b[i+7:i]
//					dst[i+7:i] := a[i+7:i]
//				ELSE
//					dst[i+7:i] := b[i+7:i]
//				FI
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMAXUB'. Intrinsic: '_mm512_maskz_max_epu8'.
// Requires AVX512BW.
func MaskzMaxEpu8(k Mmask64, a M512i, b M512i) M512i {
	return M512i(maskzMaxEpu8(uint64(k), [64]byte(a), [64]byte(b)))
}

func maskzMaxEpu8(k uint64, a [64]byte, b [64]byte) [64]byte


// MaxEpu8: Compare packed unsigned 8-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst'. 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF a[i+7:i] > b[i+7:i]
//				dst[i+7:i] := a[i+7:i]
//			ELSE
//				dst[i+7:i] := b[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMAXUB'. Intrinsic: '_mm512_max_epu8'.
// Requires AVX512BW.
func MaxEpu8(a M512i, b M512i) M512i {
	return M512i(maxEpu8([64]byte(a), [64]byte(b)))
}

func maxEpu8(a [64]byte, b [64]byte) [64]byte


// MaskMaxPd: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', and store packed maximum values in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMAXPD'. Intrinsic: '_mm512_mask_max_pd'.
// Requires AVX512F.
func MaskMaxPd(src M512d, k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskMaxPd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b)))
}

func maskMaxPd(src [8]float64, k uint8, a [8]float64, b [8]float64) [8]float64


// MaskzMaxPd: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', and store packed maximum values in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMAXPD'. Intrinsic: '_mm512_maskz_max_pd'.
// Requires AVX512F.
func MaskzMaxPd(k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskzMaxPd(uint8(k), [8]float64(a), [8]float64(b)))
}

func maskzMaxPd(k uint8, a [8]float64, b [8]float64) [8]float64


// MaxPd: Compare packed double-precision (64-bit) floating-point elements in
// 'a' and 'b', and store packed maximum values in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMAXPD'. Intrinsic: '_mm512_max_pd'.
// Requires AVX512F.
func MaxPd(a M512d, b M512d) M512d {
	return M512d(maxPd([8]float64(a), [8]float64(b)))
}

func maxPd(a [8]float64, b [8]float64) [8]float64


// MaskMaxPs: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', and store packed maximum values in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMAXPS'. Intrinsic: '_mm512_mask_max_ps'.
// Requires AVX512F.
func MaskMaxPs(src M512, k Mmask16, a M512, b M512) M512 {
	return M512(maskMaxPs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b)))
}

func maskMaxPs(src [16]float32, k uint16, a [16]float32, b [16]float32) [16]float32


// MaskzMaxPs: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', and store packed maximum values in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMAXPS'. Intrinsic: '_mm512_maskz_max_ps'.
// Requires AVX512F.
func MaskzMaxPs(k Mmask16, a M512, b M512) M512 {
	return M512(maskzMaxPs(uint16(k), [16]float32(a), [16]float32(b)))
}

func maskzMaxPs(k uint16, a [16]float32, b [16]float32) [16]float32


// MaxPs: Compare packed single-precision (32-bit) floating-point elements in
// 'a' and 'b', and store packed maximum values in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMAXPS'. Intrinsic: '_mm512_max_ps'.
// Requires AVX512F.
func MaxPs(a M512, b M512) M512 {
	return M512(maxPs([16]float32(a), [16]float32(b)))
}

func maxPs(a [16]float32, b [16]float32) [16]float32


// MaskMaxRoundPd: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store packed maximum values in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set).  
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMAXPD'. Intrinsic: '_mm512_mask_max_round_pd'.
// Requires AVX512F.
func MaskMaxRoundPd(src M512d, k Mmask8, a M512d, b M512d, sae int) M512d {
	return M512d(maskMaxRoundPd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b), sae))
}

func maskMaxRoundPd(src [8]float64, k uint8, a [8]float64, b [8]float64, sae int) [8]float64


// MaskzMaxRoundPd: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store packed maximum values in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMAXPD'. Intrinsic: '_mm512_maskz_max_round_pd'.
// Requires AVX512F.
func MaskzMaxRoundPd(k Mmask8, a M512d, b M512d, sae int) M512d {
	return M512d(maskzMaxRoundPd(uint8(k), [8]float64(a), [8]float64(b), sae))
}

func maskzMaxRoundPd(k uint8, a [8]float64, b [8]float64, sae int) [8]float64


// MaxRoundPd: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', and store packed maximum values in 'dst'. 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMAXPD'. Intrinsic: '_mm512_max_round_pd'.
// Requires AVX512F.
func MaxRoundPd(a M512d, b M512d, sae int) M512d {
	return M512d(maxRoundPd([8]float64(a), [8]float64(b), sae))
}

func maxRoundPd(a [8]float64, b [8]float64, sae int) [8]float64


// MaskMaxRoundPs: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store packed maximum values in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set).  
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMAXPS'. Intrinsic: '_mm512_mask_max_round_ps'.
// Requires AVX512F.
func MaskMaxRoundPs(src M512, k Mmask16, a M512, b M512, sae int) M512 {
	return M512(maskMaxRoundPs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b), sae))
}

func maskMaxRoundPs(src [16]float32, k uint16, a [16]float32, b [16]float32, sae int) [16]float32


// MaskzMaxRoundPs: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store packed maximum values in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMAXPS'. Intrinsic: '_mm512_maskz_max_round_ps'.
// Requires AVX512F.
func MaskzMaxRoundPs(k Mmask16, a M512, b M512, sae int) M512 {
	return M512(maskzMaxRoundPs(uint16(k), [16]float32(a), [16]float32(b), sae))
}

func maskzMaxRoundPs(k uint16, a [16]float32, b [16]float32, sae int) [16]float32


// MaxRoundPs: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', and store packed maximum values in 'dst'. 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMAXPS'. Intrinsic: '_mm512_max_round_ps'.
// Requires AVX512F.
func MaxRoundPs(a M512, b M512, sae int) M512 {
	return M512(maxRoundPs([16]float32(a), [16]float32(b), sae))
}

func maxRoundPs(a [16]float32, b [16]float32, sae int) [16]float32


// MaskMaxabsPs: Determines the maximum of the absolute elements of each pair
// of corresponding elements of packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', storing the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := FpMax(Abs(a[i+31:i]), Abs(b[i+31:i]))
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGMAXABSPS'. Intrinsic: '_mm512_mask_maxabs_ps'.
// Requires KNCNI.
func MaskMaxabsPs(src M512, k Mmask16, a M512, b M512) M512 {
	return M512(maskMaxabsPs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b)))
}

func maskMaxabsPs(src [16]float32, k uint16, a [16]float32, b [16]float32) [16]float32


// MaxabsPs: Determines the maximum of the absolute elements of each pair of
// corresponding elements of packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', storing the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := FpMax(Abs(a[i+31:i]), Abs(b[i+31:i]))
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGMAXABSPS'. Intrinsic: '_mm512_maxabs_ps'.
// Requires KNCNI.
func MaxabsPs(a M512, b M512) M512 {
	return M512(maxabsPs([16]float32(a), [16]float32(b)))
}

func maxabsPs(a [16]float32, b [16]float32) [16]float32


// MaskMinEpi16: Compare packed 16-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				IF a[i+15:i] < b[i+15:i]
//					dst[i+15:i] := a[i+15:i]
//				ELSE
//					dst[i+15:i] := b[i+15:i]
//				FI
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMINSW'. Intrinsic: '_mm512_mask_min_epi16'.
// Requires AVX512BW.
func MaskMinEpi16(src M512i, k Mmask32, a M512i, b M512i) M512i {
	return M512i(maskMinEpi16([64]byte(src), uint32(k), [64]byte(a), [64]byte(b)))
}

func maskMinEpi16(src [64]byte, k uint32, a [64]byte, b [64]byte) [64]byte


// MaskzMinEpi16: Compare packed 16-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				IF a[i+15:i] < b[i+15:i]
//					dst[i+15:i] := a[i+15:i]
//				ELSE
//					dst[i+15:i] := b[i+15:i]
//				FI
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMINSW'. Intrinsic: '_mm512_maskz_min_epi16'.
// Requires AVX512BW.
func MaskzMinEpi16(k Mmask32, a M512i, b M512i) M512i {
	return M512i(maskzMinEpi16(uint32(k), [64]byte(a), [64]byte(b)))
}

func maskzMinEpi16(k uint32, a [64]byte, b [64]byte) [64]byte


// MinEpi16: Compare packed 16-bit integers in 'a' and 'b', and store packed
// minimum values in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF a[i+15:i] < b[i+15:i]
//				dst[i+15:i] := a[i+15:i]
//			ELSE
//				dst[i+15:i] := b[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMINSW'. Intrinsic: '_mm512_min_epi16'.
// Requires AVX512BW.
func MinEpi16(a M512i, b M512i) M512i {
	return M512i(minEpi16([64]byte(a), [64]byte(b)))
}

func minEpi16(a [64]byte, b [64]byte) [64]byte


// MaskMinEpi32: Compare packed 32-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] < b[i+31:i]
//						dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMINSD'. Intrinsic: '_mm512_mask_min_epi32'.
// Requires KNCNI.
func MaskMinEpi32(src M512i, k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskMinEpi32([64]byte(src), uint16(k), [64]byte(a), [64]byte(b)))
}

func maskMinEpi32(src [64]byte, k uint16, a [64]byte, b [64]byte) [64]byte


// MaskzMinEpi32: Compare packed 32-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] < b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMINSD'. Intrinsic: '_mm512_maskz_min_epi32'.
// Requires AVX512F.
func MaskzMinEpi32(k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskzMinEpi32(uint16(k), [64]byte(a), [64]byte(b)))
}

func maskzMinEpi32(k uint16, a [64]byte, b [64]byte) [64]byte


// MinEpi32: Compare packed 32-bit integers in 'a' and 'b', and store packed
// minimum values in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF a[i+31:i] < b[i+31:i]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := b[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMINSD'. Intrinsic: '_mm512_min_epi32'.
// Requires KNCNI.
func MinEpi32(a M512i, b M512i) M512i {
	return M512i(minEpi32([64]byte(a), [64]byte(b)))
}

func minEpi32(a [64]byte, b [64]byte) [64]byte


// MaskMinEpi64: Compare packed 64-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] < b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMINSQ'. Intrinsic: '_mm512_mask_min_epi64'.
// Requires AVX512F.
func MaskMinEpi64(src M512i, k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskMinEpi64([64]byte(src), uint8(k), [64]byte(a), [64]byte(b)))
}

func maskMinEpi64(src [64]byte, k uint8, a [64]byte, b [64]byte) [64]byte


// MaskzMinEpi64: Compare packed 64-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] < b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMINSQ'. Intrinsic: '_mm512_maskz_min_epi64'.
// Requires AVX512F.
func MaskzMinEpi64(k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskzMinEpi64(uint8(k), [64]byte(a), [64]byte(b)))
}

func maskzMinEpi64(k uint8, a [64]byte, b [64]byte) [64]byte


// MinEpi64: Compare packed 64-bit integers in 'a' and 'b', and store packed
// minimum values in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF a[i+63:i] < b[i+63:i]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := b[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMINSQ'. Intrinsic: '_mm512_min_epi64'.
// Requires AVX512F.
func MinEpi64(a M512i, b M512i) M512i {
	return M512i(minEpi64([64]byte(a), [64]byte(b)))
}

func minEpi64(a [64]byte, b [64]byte) [64]byte


// MaskMinEpi8: Compare packed 8-bit integers in 'a' and 'b', and store packed
// minimum values in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				IF a[i+7:i] < b[i+7:i]
//					dst[i+7:i] := a[i+7:i]
//				ELSE
//					dst[i+7:i] := b[i+7:i]
//				FI
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMINSB'. Intrinsic: '_mm512_mask_min_epi8'.
// Requires AVX512BW.
func MaskMinEpi8(src M512i, k Mmask64, a M512i, b M512i) M512i {
	return M512i(maskMinEpi8([64]byte(src), uint64(k), [64]byte(a), [64]byte(b)))
}

func maskMinEpi8(src [64]byte, k uint64, a [64]byte, b [64]byte) [64]byte


// MaskzMinEpi8: Compare packed 8-bit integers in 'a' and 'b', and store packed
// minimum values in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				IF a[i+7:i] < b[i+7:i]
//					dst[i+7:i] := a[i+7:i]
//				ELSE
//					dst[i+7:i] := b[i+7:i]
//				FI
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMINSB'. Intrinsic: '_mm512_maskz_min_epi8'.
// Requires AVX512BW.
func MaskzMinEpi8(k Mmask64, a M512i, b M512i) M512i {
	return M512i(maskzMinEpi8(uint64(k), [64]byte(a), [64]byte(b)))
}

func maskzMinEpi8(k uint64, a [64]byte, b [64]byte) [64]byte


// MinEpi8: Compare packed 8-bit integers in 'a' and 'b', and store packed
// minimum values in 'dst'. 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF a[i+7:i] < b[i+7:i]
//				dst[i+7:i] := a[i+7:i]
//			ELSE
//				dst[i+7:i] := b[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMINSB'. Intrinsic: '_mm512_min_epi8'.
// Requires AVX512BW.
func MinEpi8(a M512i, b M512i) M512i {
	return M512i(minEpi8([64]byte(a), [64]byte(b)))
}

func minEpi8(a [64]byte, b [64]byte) [64]byte


// MaskMinEpu16: Compare packed unsigned 16-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				IF a[i+15:i] < b[i+15:i]
//					dst[i+15:i] := a[i+15:i]
//				ELSE
//					dst[i+15:i] := b[i+15:i]
//				FI
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMINUW'. Intrinsic: '_mm512_mask_min_epu16'.
// Requires AVX512BW.
func MaskMinEpu16(src M512i, k Mmask32, a M512i, b M512i) M512i {
	return M512i(maskMinEpu16([64]byte(src), uint32(k), [64]byte(a), [64]byte(b)))
}

func maskMinEpu16(src [64]byte, k uint32, a [64]byte, b [64]byte) [64]byte


// MaskzMinEpu16: Compare packed unsigned 16-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				IF a[i+15:i] < b[i+15:i]
//					dst[i+15:i] := a[i+15:i]
//				ELSE
//					dst[i+15:i] := b[i+15:i]
//				FI
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMINUW'. Intrinsic: '_mm512_maskz_min_epu16'.
// Requires AVX512BW.
func MaskzMinEpu16(k Mmask32, a M512i, b M512i) M512i {
	return M512i(maskzMinEpu16(uint32(k), [64]byte(a), [64]byte(b)))
}

func maskzMinEpu16(k uint32, a [64]byte, b [64]byte) [64]byte


// MinEpu16: Compare packed unsigned 16-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF a[i+15:i] < b[i+15:i]
//				dst[i+15:i] := a[i+15:i]
//			ELSE
//				dst[i+15:i] := b[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMINUW'. Intrinsic: '_mm512_min_epu16'.
// Requires AVX512BW.
func MinEpu16(a M512i, b M512i) M512i {
	return M512i(minEpu16([64]byte(a), [64]byte(b)))
}

func minEpu16(a [64]byte, b [64]byte) [64]byte


// MaskMinEpu32: Compare packed unsigned 32-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] < b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMINUD'. Intrinsic: '_mm512_mask_min_epu32'.
// Requires KNCNI.
func MaskMinEpu32(src M512i, k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskMinEpu32([64]byte(src), uint16(k), [64]byte(a), [64]byte(b)))
}

func maskMinEpu32(src [64]byte, k uint16, a [64]byte, b [64]byte) [64]byte


// MaskzMinEpu32: Compare packed unsigned 32-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] < b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMINUD'. Intrinsic: '_mm512_maskz_min_epu32'.
// Requires AVX512F.
func MaskzMinEpu32(k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskzMinEpu32(uint16(k), [64]byte(a), [64]byte(b)))
}

func maskzMinEpu32(k uint16, a [64]byte, b [64]byte) [64]byte


// MinEpu32: Compare packed unsigned 32-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF a[i+31:i] < b[i+31:i]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := b[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMINUD'. Intrinsic: '_mm512_min_epu32'.
// Requires KNCNI.
func MinEpu32(a M512i, b M512i) M512i {
	return M512i(minEpu32([64]byte(a), [64]byte(b)))
}

func minEpu32(a [64]byte, b [64]byte) [64]byte


// MaskMinEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] < b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMINUQ'. Intrinsic: '_mm512_mask_min_epu64'.
// Requires AVX512F.
func MaskMinEpu64(src M512i, k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskMinEpu64([64]byte(src), uint8(k), [64]byte(a), [64]byte(b)))
}

func maskMinEpu64(src [64]byte, k uint8, a [64]byte, b [64]byte) [64]byte


// MaskzMinEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] < b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMINUQ'. Intrinsic: '_mm512_maskz_min_epu64'.
// Requires AVX512F.
func MaskzMinEpu64(k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskzMinEpu64(uint8(k), [64]byte(a), [64]byte(b)))
}

func maskzMinEpu64(k uint8, a [64]byte, b [64]byte) [64]byte


// MinEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF a[i+63:i] < b[i+63:i]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := b[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMINUQ'. Intrinsic: '_mm512_min_epu64'.
// Requires AVX512F.
func MinEpu64(a M512i, b M512i) M512i {
	return M512i(minEpu64([64]byte(a), [64]byte(b)))
}

func minEpu64(a [64]byte, b [64]byte) [64]byte


// MaskMinEpu8: Compare packed unsigned 8-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				IF a[i+7:i] < b[i+7:i]
//					dst[i+7:i] := a[i+7:i]
//				ELSE
//					dst[i+7:i] := b[i+7:i]
//				FI
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMINUB'. Intrinsic: '_mm512_mask_min_epu8'.
// Requires AVX512BW.
func MaskMinEpu8(src M512i, k Mmask64, a M512i, b M512i) M512i {
	return M512i(maskMinEpu8([64]byte(src), uint64(k), [64]byte(a), [64]byte(b)))
}

func maskMinEpu8(src [64]byte, k uint64, a [64]byte, b [64]byte) [64]byte


// MaskzMinEpu8: Compare packed unsigned 8-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				IF a[i+7:i] < b[i+7:i]
//					dst[i+7:i] := a[i+7:i]
//				ELSE
//					dst[i+7:i] := b[i+7:i]
//				FI
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMINUB'. Intrinsic: '_mm512_maskz_min_epu8'.
// Requires AVX512BW.
func MaskzMinEpu8(k Mmask64, a M512i, b M512i) M512i {
	return M512i(maskzMinEpu8(uint64(k), [64]byte(a), [64]byte(b)))
}

func maskzMinEpu8(k uint64, a [64]byte, b [64]byte) [64]byte


// MinEpu8: Compare packed unsigned 8-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst'. 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF a[i+7:i] < b[i+7:i]
//				dst[i+7:i] := a[i+7:i]
//			ELSE
//				dst[i+7:i] := b[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMINUB'. Intrinsic: '_mm512_min_epu8'.
// Requires AVX512BW.
func MinEpu8(a M512i, b M512i) M512i {
	return M512i(minEpu8([64]byte(a), [64]byte(b)))
}

func minEpu8(a [64]byte, b [64]byte) [64]byte


// MaskMinPd: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', and store packed minimum values in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMINPD'. Intrinsic: '_mm512_mask_min_pd'.
// Requires AVX512F.
func MaskMinPd(src M512d, k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskMinPd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b)))
}

func maskMinPd(src [8]float64, k uint8, a [8]float64, b [8]float64) [8]float64


// MaskzMinPd: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', and store packed minimum values in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMINPD'. Intrinsic: '_mm512_maskz_min_pd'.
// Requires AVX512F.
func MaskzMinPd(k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskzMinPd(uint8(k), [8]float64(a), [8]float64(b)))
}

func maskzMinPd(k uint8, a [8]float64, b [8]float64) [8]float64


// MinPd: Compare packed double-precision (64-bit) floating-point elements in
// 'a' and 'b', and store packed minimum values in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMINPD'. Intrinsic: '_mm512_min_pd'.
// Requires AVX512F.
func MinPd(a M512d, b M512d) M512d {
	return M512d(minPd([8]float64(a), [8]float64(b)))
}

func minPd(a [8]float64, b [8]float64) [8]float64


// MaskMinPs: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', and store packed minimum values in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMINPS'. Intrinsic: '_mm512_mask_min_ps'.
// Requires AVX512F.
func MaskMinPs(src M512, k Mmask16, a M512, b M512) M512 {
	return M512(maskMinPs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b)))
}

func maskMinPs(src [16]float32, k uint16, a [16]float32, b [16]float32) [16]float32


// MaskzMinPs: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', and store packed minimum values in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMINPS'. Intrinsic: '_mm512_maskz_min_ps'.
// Requires AVX512F.
func MaskzMinPs(k Mmask16, a M512, b M512) M512 {
	return M512(maskzMinPs(uint16(k), [16]float32(a), [16]float32(b)))
}

func maskzMinPs(k uint16, a [16]float32, b [16]float32) [16]float32


// MinPs: Compare packed single-precision (32-bit) floating-point elements in
// 'a' and 'b', and store packed minimum values in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMINPS'. Intrinsic: '_mm512_min_ps'.
// Requires AVX512F.
func MinPs(a M512, b M512) M512 {
	return M512(minPs([16]float32(a), [16]float32(b)))
}

func minPs(a [16]float32, b [16]float32) [16]float32


// MaskMinRoundPd: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store packed minimum values in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set).  
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMINPD'. Intrinsic: '_mm512_mask_min_round_pd'.
// Requires AVX512F.
func MaskMinRoundPd(src M512d, k Mmask8, a M512d, b M512d, sae int) M512d {
	return M512d(maskMinRoundPd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b), sae))
}

func maskMinRoundPd(src [8]float64, k uint8, a [8]float64, b [8]float64, sae int) [8]float64


// MaskzMinRoundPd: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store packed minimum values in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMINPD'. Intrinsic: '_mm512_maskz_min_round_pd'.
// Requires AVX512F.
func MaskzMinRoundPd(k Mmask8, a M512d, b M512d, sae int) M512d {
	return M512d(maskzMinRoundPd(uint8(k), [8]float64(a), [8]float64(b), sae))
}

func maskzMinRoundPd(k uint8, a [8]float64, b [8]float64, sae int) [8]float64


// MinRoundPd: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', and store packed minimum values in 'dst'. 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMINPD'. Intrinsic: '_mm512_min_round_pd'.
// Requires AVX512F.
func MinRoundPd(a M512d, b M512d, sae int) M512d {
	return M512d(minRoundPd([8]float64(a), [8]float64(b), sae))
}

func minRoundPd(a [8]float64, b [8]float64, sae int) [8]float64


// MaskMinRoundPs: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store packed minimum values in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set).  
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMINPS'. Intrinsic: '_mm512_mask_min_round_ps'.
// Requires AVX512F.
func MaskMinRoundPs(src M512, k Mmask16, a M512, b M512, sae int) M512 {
	return M512(maskMinRoundPs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b), sae))
}

func maskMinRoundPs(src [16]float32, k uint16, a [16]float32, b [16]float32, sae int) [16]float32


// MaskzMinRoundPs: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store packed minimum values in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMINPS'. Intrinsic: '_mm512_maskz_min_round_ps'.
// Requires AVX512F.
func MaskzMinRoundPs(k Mmask16, a M512, b M512, sae int) M512 {
	return M512(maskzMinRoundPs(uint16(k), [16]float32(a), [16]float32(b), sae))
}

func maskzMinRoundPs(k uint16, a [16]float32, b [16]float32, sae int) [16]float32


// MinRoundPs: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', and store packed minimum values in 'dst'. 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMINPS'. Intrinsic: '_mm512_min_round_ps'.
// Requires AVX512F.
func MinRoundPs(a M512, b M512, sae int) M512 {
	return M512(minRoundPs([16]float32(a), [16]float32(b), sae))
}

func minRoundPs(a [16]float32, b [16]float32, sae int) [16]float32


// MaskMovEpi16: Move packed 16-bit integers from 'a' into 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := a[i+15:i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDQU16'. Intrinsic: '_mm512_mask_mov_epi16'.
// Requires AVX512BW.
func MaskMovEpi16(src M512i, k Mmask32, a M512i) M512i {
	return M512i(maskMovEpi16([64]byte(src), uint32(k), [64]byte(a)))
}

func maskMovEpi16(src [64]byte, k uint32, a [64]byte) [64]byte


// MaskzMovEpi16: Move packed 16-bit integers from 'a' into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := a[i+15:i]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDQU16'. Intrinsic: '_mm512_maskz_mov_epi16'.
// Requires AVX512BW.
func MaskzMovEpi16(k Mmask32, a M512i) M512i {
	return M512i(maskzMovEpi16(uint32(k), [64]byte(a)))
}

func maskzMovEpi16(k uint32, a [64]byte) [64]byte


// MaskMovEpi32: Move packed 32-bit integers from 'a' to 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDQA32'. Intrinsic: '_mm512_mask_mov_epi32'.
// Requires KNCNI.
func MaskMovEpi32(src M512i, k Mmask16, a M512i) M512i {
	return M512i(maskMovEpi32([64]byte(src), uint16(k), [64]byte(a)))
}

func maskMovEpi32(src [64]byte, k uint16, a [64]byte) [64]byte


// MaskzMovEpi32: Move packed 32-bit integers from 'a' into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDQA32'. Intrinsic: '_mm512_maskz_mov_epi32'.
// Requires AVX512F.
func MaskzMovEpi32(k Mmask16, a M512i) M512i {
	return M512i(maskzMovEpi32(uint16(k), [64]byte(a)))
}

func maskzMovEpi32(k uint16, a [64]byte) [64]byte


// MaskMovEpi64: Move packed 64-bit integers from 'a' to 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDQA64'. Intrinsic: '_mm512_mask_mov_epi64'.
// Requires KNCNI.
func MaskMovEpi64(src M512i, k Mmask8, a M512i) M512i {
	return M512i(maskMovEpi64([64]byte(src), uint8(k), [64]byte(a)))
}

func maskMovEpi64(src [64]byte, k uint8, a [64]byte) [64]byte


// MaskzMovEpi64: Move packed 64-bit integers from 'a' into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDQA64'. Intrinsic: '_mm512_maskz_mov_epi64'.
// Requires AVX512F.
func MaskzMovEpi64(k Mmask8, a M512i) M512i {
	return M512i(maskzMovEpi64(uint8(k), [64]byte(a)))
}

func maskzMovEpi64(k uint8, a [64]byte) [64]byte


// MaskMovEpi8: Move packed 8-bit integers from 'a' into 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := a[i+7:i]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDQU8'. Intrinsic: '_mm512_mask_mov_epi8'.
// Requires AVX512BW.
func MaskMovEpi8(src M512i, k Mmask64, a M512i) M512i {
	return M512i(maskMovEpi8([64]byte(src), uint64(k), [64]byte(a)))
}

func maskMovEpi8(src [64]byte, k uint64, a [64]byte) [64]byte


// MaskzMovEpi8: Move packed 8-bit integers from 'a' into 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := a[i+7:i]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDQU8'. Intrinsic: '_mm512_maskz_mov_epi8'.
// Requires AVX512BW.
func MaskzMovEpi8(k Mmask64, a M512i) M512i {
	return M512i(maskzMovEpi8(uint64(k), [64]byte(a)))
}

func maskzMovEpi8(k uint64, a [64]byte) [64]byte


// MaskMovPd: Move packed double-precision (64-bit) floating-point elements
// from 'a' to 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVAPD'. Intrinsic: '_mm512_mask_mov_pd'.
// Requires KNCNI.
func MaskMovPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskMovPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskMovPd(src [8]float64, k uint8, a [8]float64) [8]float64


// MaskzMovPd: Move packed double-precision (64-bit) floating-point elements
// from 'a' into 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVAPD'. Intrinsic: '_mm512_maskz_mov_pd'.
// Requires AVX512F.
func MaskzMovPd(k Mmask8, a M512d) M512d {
	return M512d(maskzMovPd(uint8(k), [8]float64(a)))
}

func maskzMovPd(k uint8, a [8]float64) [8]float64


// MaskMovPs: Move packed single-precision (32-bit) floating-point elements
// from 'a' to 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVAPS'. Intrinsic: '_mm512_mask_mov_ps'.
// Requires KNCNI.
func MaskMovPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskMovPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskMovPs(src [16]float32, k uint16, a [16]float32) [16]float32


// MaskzMovPs: Move packed single-precision (32-bit) floating-point elements
// from 'a' into 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVAPS'. Intrinsic: '_mm512_maskz_mov_ps'.
// Requires AVX512F.
func MaskzMovPs(k Mmask16, a M512) M512 {
	return M512(maskzMovPs(uint16(k), [16]float32(a)))
}

func maskzMovPs(k uint16, a [16]float32) [16]float32


// MaskMovedupPd: Duplicate even-indexed double-precision (64-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp[63:0] := a[63:0]
//		tmp[127:64] := a[63:0]
//		tmp[191:128] := a[191:128]
//		tmp[255:192] := a[191:128]
//		tmp[319:256] := a[319:256] 
//		tmp[383:320] := a[319:256] 
//		tmp[447:384] := a[447:384]
//		tmp[511:448] := a[447:384]
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDDUP'. Intrinsic: '_mm512_mask_movedup_pd'.
// Requires AVX512F.
func MaskMovedupPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskMovedupPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskMovedupPd(src [8]float64, k uint8, a [8]float64) [8]float64


// MaskzMovedupPd: Duplicate even-indexed double-precision (64-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp[63:0] := a[63:0]
//		tmp[127:64] := a[63:0]
//		tmp[191:128] := a[191:128]
//		tmp[255:192] := a[191:128]
//		tmp[319:256] := a[319:256] 
//		tmp[383:320] := a[319:256] 
//		tmp[447:384] := a[447:384]
//		tmp[511:448] := a[447:384]
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDDUP'. Intrinsic: '_mm512_maskz_movedup_pd'.
// Requires AVX512F.
func MaskzMovedupPd(k Mmask8, a M512d) M512d {
	return M512d(maskzMovedupPd(uint8(k), [8]float64(a)))
}

func maskzMovedupPd(k uint8, a [8]float64) [8]float64


// MovedupPd: Duplicate even-indexed double-precision (64-bit) floating-point
// elements from 'a', and store the results in 'dst'. 
//
//		tmp[63:0] := a[63:0]
//		tmp[127:64] := a[63:0]
//		tmp[191:128] := a[191:128]
//		tmp[255:192] := a[191:128]
//		tmp[319:256] := a[319:256] 
//		tmp[383:320] := a[319:256] 
//		tmp[447:384] := a[447:384]
//		tmp[511:448] := a[447:384]
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDDUP'. Intrinsic: '_mm512_movedup_pd'.
// Requires AVX512F.
func MovedupPd(a M512d) M512d {
	return M512d(movedupPd([8]float64(a)))
}

func movedupPd(a [8]float64) [8]float64


// MaskMovehdupPs: Duplicate odd-indexed single-precision (32-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp[31:0] := a[63:32] 
//		tmp[63:32] := a[63:32] 
//		tmp[95:64] := a[127:96] 
//		tmp[127:96] := a[127:96]
//		tmp[159:128] := a[191:160] 
//		tmp[191:160] := a[191:160] 
//		tmp[223:192] := a[255:224] 
//		tmp[255:224] := a[255:224]
//		tmp[287:256] := a[319:288] 
//		tmp[319:288] := a[319:288] 
//		tmp[351:320] := a[383:352] 
//		tmp[383:352] := a[383:352] 
//		tmp[415:384] := a[447:416] 
//		tmp[447:416] := a[447:416] 
//		tmp[479:448] := a[511:480]
//		tmp[511:480] := a[511:480]
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVSHDUP'. Intrinsic: '_mm512_mask_movehdup_ps'.
// Requires AVX512F.
func MaskMovehdupPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskMovehdupPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskMovehdupPs(src [16]float32, k uint16, a [16]float32) [16]float32


// MaskzMovehdupPs: Duplicate odd-indexed single-precision (32-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp[31:0] := a[63:32] 
//		tmp[63:32] := a[63:32] 
//		tmp[95:64] := a[127:96] 
//		tmp[127:96] := a[127:96]
//		tmp[159:128] := a[191:160] 
//		tmp[191:160] := a[191:160] 
//		tmp[223:192] := a[255:224] 
//		tmp[255:224] := a[255:224]
//		tmp[287:256] := a[319:288] 
//		tmp[319:288] := a[319:288] 
//		tmp[351:320] := a[383:352] 
//		tmp[383:352] := a[383:352] 
//		tmp[415:384] := a[447:416] 
//		tmp[447:416] := a[447:416] 
//		tmp[479:448] := a[511:480]
//		tmp[511:480] := a[511:480]
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVSHDUP'. Intrinsic: '_mm512_maskz_movehdup_ps'.
// Requires AVX512F.
func MaskzMovehdupPs(k Mmask16, a M512) M512 {
	return M512(maskzMovehdupPs(uint16(k), [16]float32(a)))
}

func maskzMovehdupPs(k uint16, a [16]float32) [16]float32


// MovehdupPs: Duplicate odd-indexed single-precision (32-bit) floating-point
// elements from 'a', and store the results in 'dst'. 
//
//		dst[31:0] := a[63:32] 
//		dst[63:32] := a[63:32] 
//		dst[95:64] := a[127:96] 
//		dst[127:96] := a[127:96]
//		dst[159:128] := a[191:160] 
//		dst[191:160] := a[191:160] 
//		dst[223:192] := a[255:224] 
//		dst[255:224] := a[255:224]
//		dst[287:256] := a[319:288] 
//		dst[319:288] := a[319:288] 
//		dst[351:320] := a[383:352] 
//		dst[383:352] := a[383:352] 
//		dst[415:384] := a[447:416] 
//		dst[447:416] := a[447:416] 
//		dst[479:448] := a[511:480]
//		dst[511:480] := a[511:480]
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVSHDUP'. Intrinsic: '_mm512_movehdup_ps'.
// Requires AVX512F.
func MovehdupPs(a M512) M512 {
	return M512(movehdupPs([16]float32(a)))
}

func movehdupPs(a [16]float32) [16]float32


// MaskMoveldupPs: Duplicate even-indexed single-precision (32-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp[31:0] := a[31:0] 
//		tmp[63:32] := a[31:0] 
//		tmp[95:64] := a[95:64] 
//		tmp[127:96] := a[95:64]
//		tmp[159:128] := a[159:128] 
//		tmp[191:160] := a[159:128] 
//		tmp[223:192] := a[223:192] 
//		tmp[255:224] := a[223:192]
//		tmp[287:256] := a[287:256] 
//		tmp[319:288] := a[287:256] 
//		tmp[351:320] := a[351:320] 
//		tmp[383:352] := a[351:320] 
//		tmp[415:384] := a[415:384] 
//		tmp[447:416] := a[415:384] 
//		tmp[479:448] := a[479:448]
//		tmp[511:480] := a[479:448]
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVSLDUP'. Intrinsic: '_mm512_mask_moveldup_ps'.
// Requires AVX512F.
func MaskMoveldupPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskMoveldupPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskMoveldupPs(src [16]float32, k uint16, a [16]float32) [16]float32


// MaskzMoveldupPs: Duplicate even-indexed single-precision (32-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp[31:0] := a[31:0] 
//		tmp[63:32] := a[31:0] 
//		tmp[95:64] := a[95:64] 
//		tmp[127:96] := a[95:64]
//		tmp[159:128] := a[159:128] 
//		tmp[191:160] := a[159:128] 
//		tmp[223:192] := a[223:192] 
//		tmp[255:224] := a[223:192]
//		tmp[287:256] := a[287:256] 
//		tmp[319:288] := a[287:256] 
//		tmp[351:320] := a[351:320] 
//		tmp[383:352] := a[351:320] 
//		tmp[415:384] := a[415:384] 
//		tmp[447:416] := a[415:384] 
//		tmp[479:448] := a[479:448]
//		tmp[511:480] := a[479:448]
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVSLDUP'. Intrinsic: '_mm512_maskz_moveldup_ps'.
// Requires AVX512F.
func MaskzMoveldupPs(k Mmask16, a M512) M512 {
	return M512(maskzMoveldupPs(uint16(k), [16]float32(a)))
}

func maskzMoveldupPs(k uint16, a [16]float32) [16]float32


// MoveldupPs: Duplicate even-indexed single-precision (32-bit) floating-point
// elements from 'a', and store the results in 'dst'. 
//
//		dst[31:0] := a[31:0] 
//		dst[63:32] := a[31:0] 
//		dst[95:64] := a[95:64] 
//		dst[127:96] := a[95:64]
//		dst[159:128] := a[159:128] 
//		dst[191:160] := a[159:128] 
//		dst[223:192] := a[223:192] 
//		dst[255:224] := a[223:192]
//		dst[287:256] := a[287:256] 
//		dst[319:288] := a[287:256] 
//		dst[351:320] := a[351:320] 
//		dst[383:352] := a[351:320] 
//		dst[415:384] := a[415:384] 
//		dst[447:416] := a[415:384] 
//		dst[479:448] := a[479:448]
//		dst[511:480] := a[479:448]
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVSLDUP'. Intrinsic: '_mm512_moveldup_ps'.
// Requires AVX512F.
func MoveldupPs(a M512) M512 {
	return M512(moveldupPs([16]float32(a)))
}

func moveldupPs(a [16]float32) [16]float32


// Movepi16Mask: Set each bit of mask register 'k' based on the most
// significant bit of the corresponding packed 16-bit integer in 'a'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF a[i+15]
//				k[j] := 1
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPMOVW2M'. Intrinsic: '_mm_movepi16_mask'.
// Requires AVX512BW.
func Movepi16Mask(a M128i) Mmask8 {
	return Mmask8(movepi16Mask([16]byte(a)))
}

func movepi16Mask(a [16]byte) uint8


// Movepi16Mask1: Set each bit of mask register 'k' based on the most
// significant bit of the corresponding packed 16-bit integer in 'a'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF a[i+15]
//				k[j] := 1
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPMOVW2M'. Intrinsic: '_mm512_movepi16_mask'.
// Requires AVX512BW.
func Movepi16Mask1(a M512i) Mmask32 {
	return Mmask32(movepi16Mask1([64]byte(a)))
}

func movepi16Mask1(a [64]byte) uint32


// Movepi32Mask: Set each bit of mask register 'k' based on the most
// significant bit of the corresponding packed 32-bit integer in 'a'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF a[i+31]
//				k[j] := 1
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPMOVD2M'. Intrinsic: '_mm_movepi32_mask'.
// Requires AVX512DQ.
func Movepi32Mask(a M128i) Mmask8 {
	return Mmask8(movepi32Mask([16]byte(a)))
}

func movepi32Mask(a [16]byte) uint8


// Movepi32Mask1: Set each bit of mask register 'k' based on the most
// significant bit of the corresponding packed 32-bit integer in 'a'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF a[i+31]
//				k[j] := 1
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPMOVD2M'. Intrinsic: '_mm512_movepi32_mask'.
// Requires AVX512DQ.
func Movepi32Mask1(a M512i) Mmask16 {
	return Mmask16(movepi32Mask1([64]byte(a)))
}

func movepi32Mask1(a [64]byte) uint16


// Movepi64Mask: Set each bit of mask register 'k' based on the most
// significant bit of the corresponding packed 64-bit integer in 'a'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF a[i+63]
//				k[j] := 1
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPMOVQ2M'. Intrinsic: '_mm_movepi64_mask'.
// Requires AVX512DQ.
func Movepi64Mask(a M128i) Mmask8 {
	return Mmask8(movepi64Mask([16]byte(a)))
}

func movepi64Mask(a [16]byte) uint8


// Movepi64Mask1: Set each bit of mask register 'k' based on the most
// significant bit of the corresponding packed 64-bit integer in 'a'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF a[i+63]
//				k[j] := 1
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPMOVQ2M'. Intrinsic: '_mm512_movepi64_mask'.
// Requires AVX512DQ.
func Movepi64Mask1(a M512i) Mmask8 {
	return Mmask8(movepi64Mask1([64]byte(a)))
}

func movepi64Mask1(a [64]byte) uint8


// Movepi8Mask: Set each bit of mask register 'k' based on the most significant
// bit of the corresponding packed 8-bit integer in 'a'. 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF a[i+7]
//				k[j] := 1
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPMOVB2M'. Intrinsic: '_mm_movepi8_mask'.
// Requires AVX512BW.
func Movepi8Mask(a M128i) Mmask16 {
	return Mmask16(movepi8Mask([16]byte(a)))
}

func movepi8Mask(a [16]byte) uint16


// Movepi8Mask1: Set each bit of mask register 'k' based on the most
// significant bit of the corresponding packed 8-bit integer in 'a'. 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF a[i+7]
//				k[j] := 1
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:64] := 0
//
// Instruction: 'VPMOVB2M'. Intrinsic: '_mm512_movepi8_mask'.
// Requires AVX512BW.
func Movepi8Mask1(a M512i) Mmask64 {
	return Mmask64(movepi8Mask1([64]byte(a)))
}

func movepi8Mask1(a [64]byte) uint64


// MovmEpi16: Set each packed 16-bit integer in 'dst' to all ones or all zeros
// based on the value of the corresponding bit in 'k'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := 0xFFFF
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVM2W'. Intrinsic: '_mm512_movm_epi16'.
// Requires AVX512BW.
func MovmEpi16(k Mmask32) M512i {
	return M512i(movmEpi16(uint32(k)))
}

func movmEpi16(k uint32) [64]byte


// MovmEpi32: Set each packed 32-bit integer in 'dst' to all ones or all zeros
// based on the value of the corresponding bit in 'k'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := 0xFFFFFFFF
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVM2D'. Intrinsic: '_mm512_movm_epi32'.
// Requires AVX512DQ.
func MovmEpi32(k Mmask16) M512i {
	return M512i(movmEpi32(uint16(k)))
}

func movmEpi32(k uint16) [64]byte


// MovmEpi64: Set each packed 64-bit integer in 'dst' to all ones or all zeros
// based on the value of the corresponding bit in 'k'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := 0xFFFFFFFFffffffff
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVM2Q'. Intrinsic: '_mm512_movm_epi64'.
// Requires AVX512DQ.
func MovmEpi64(k Mmask8) M512i {
	return M512i(movmEpi64(uint8(k)))
}

func movmEpi64(k uint8) [64]byte


// MovmEpi8: Set each packed 8-bit integer in 'dst' to all ones or all zeros
// based on the value of the corresponding bit in 'k'. 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := 0xFF
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVM2B'. Intrinsic: '_mm512_movm_epi8'.
// Requires AVX512BW.
func MovmEpi8(k Mmask64) M512i {
	return M512i(movmEpi8(uint64(k)))
}

func movmEpi8(k uint64) [64]byte


// MaskMulEpi32: Multiply the low 32-bit integers from each packed 64-bit
// element in 'a' and 'b', and store the signed 64-bit results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMULDQ'. Intrinsic: '_mm512_mask_mul_epi32'.
// Requires AVX512F.
func MaskMulEpi32(src M512i, k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskMulEpi32([64]byte(src), uint8(k), [64]byte(a), [64]byte(b)))
}

func maskMulEpi32(src [64]byte, k uint8, a [64]byte, b [64]byte) [64]byte


// MaskzMulEpi32: Multiply the low 32-bit integers from each packed 64-bit
// element in 'a' and 'b', and store the signed 64-bit results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMULDQ'. Intrinsic: '_mm512_maskz_mul_epi32'.
// Requires AVX512F.
func MaskzMulEpi32(k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskzMulEpi32(uint8(k), [64]byte(a), [64]byte(b)))
}

func maskzMulEpi32(k uint8, a [64]byte, b [64]byte) [64]byte


// MulEpi32: Multiply the low 32-bit integers from each packed 64-bit element
// in 'a' and 'b', and store the signed 64-bit results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := a[i+31:i] * b[i+31:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMULDQ'. Intrinsic: '_mm512_mul_epi32'.
// Requires AVX512F.
func MulEpi32(a M512i, b M512i) M512i {
	return M512i(mulEpi32([64]byte(a), [64]byte(b)))
}

func mulEpi32(a [64]byte, b [64]byte) [64]byte


// MaskMulEpu32: Multiply the low unsigned 32-bit integers from each packed
// 64-bit element in 'a' and 'b', and store the unsigned 64-bit results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMULUDQ'. Intrinsic: '_mm512_mask_mul_epu32'.
// Requires AVX512F.
func MaskMulEpu32(src M512i, k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskMulEpu32([64]byte(src), uint8(k), [64]byte(a), [64]byte(b)))
}

func maskMulEpu32(src [64]byte, k uint8, a [64]byte, b [64]byte) [64]byte


// MaskzMulEpu32: Multiply the low unsigned 32-bit integers from each packed
// 64-bit element in 'a' and 'b', and store the unsigned 64-bit results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMULUDQ'. Intrinsic: '_mm512_maskz_mul_epu32'.
// Requires AVX512F.
func MaskzMulEpu32(k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskzMulEpu32(uint8(k), [64]byte(a), [64]byte(b)))
}

func maskzMulEpu32(k uint8, a [64]byte, b [64]byte) [64]byte


// MulEpu32: Multiply the low unsigned 32-bit integers from each packed 64-bit
// element in 'a' and 'b', and store the unsigned 64-bit results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := a[i+31:i] * b[i+31:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMULUDQ'. Intrinsic: '_mm512_mul_epu32'.
// Requires AVX512F.
func MulEpu32(a M512i, b M512i) M512i {
	return M512i(mulEpu32([64]byte(a), [64]byte(b)))
}

func mulEpu32(a [64]byte, b [64]byte) [64]byte


// MaskMulPd: Multiply packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set).  RM. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] * b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMULPD'. Intrinsic: '_mm512_mask_mul_pd'.
// Requires KNCNI.
func MaskMulPd(src M512d, k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskMulPd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b)))
}

func maskMulPd(src [8]float64, k uint8, a [8]float64, b [8]float64) [8]float64


// MaskzMulPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] * b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMULPD'. Intrinsic: '_mm512_maskz_mul_pd'.
// Requires AVX512F.
func MaskzMulPd(k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskzMulPd(uint8(k), [8]float64(a), [8]float64(b)))
}

func maskzMulPd(k uint8, a [8]float64, b [8]float64) [8]float64


// MulPd: Multiply packed double-precision (64-bit) floating-point elements in
// 'a' and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := a[i+63:i] * b[i+63:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMULPD'. Intrinsic: '_mm512_mul_pd'.
// Requires KNCNI.
func MulPd(a M512d, b M512d) M512d {
	return M512d(mulPd([8]float64(a), [8]float64(b)))
}

func mulPd(a [8]float64, b [8]float64) [8]float64


// MaskMulPs: Multiply packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set).  RM. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMULPS'. Intrinsic: '_mm512_mask_mul_ps'.
// Requires KNCNI.
func MaskMulPs(src M512, k Mmask16, a M512, b M512) M512 {
	return M512(maskMulPs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b)))
}

func maskMulPs(src [16]float32, k uint16, a [16]float32, b [16]float32) [16]float32


// MaskzMulPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMULPS'. Intrinsic: '_mm512_maskz_mul_ps'.
// Requires AVX512F.
func MaskzMulPs(k Mmask16, a M512, b M512) M512 {
	return M512(maskzMulPs(uint16(k), [16]float32(a), [16]float32(b)))
}

func maskzMulPs(k uint16, a [16]float32, b [16]float32) [16]float32


// MulPs: Multiply packed single-precision (32-bit) floating-point elements in
// 'a' and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := a[i+31:i] * b[i+31:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMULPS'. Intrinsic: '_mm512_mul_ps'.
// Requires KNCNI.
func MulPs(a M512, b M512) M512 {
	return M512(mulPs([16]float32(a), [16]float32(b)))
}

func mulPs(a [16]float32, b [16]float32) [16]float32


// MaskMulRoundPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] * b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMULPD'. Intrinsic: '_mm512_mask_mul_round_pd'.
// Requires KNCNI.
func MaskMulRoundPd(src M512d, k Mmask8, a M512d, b M512d, rounding int) M512d {
	return M512d(maskMulRoundPd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b), rounding))
}

func maskMulRoundPd(src [8]float64, k uint8, a [8]float64, b [8]float64, rounding int) [8]float64


// MaskzMulRoundPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] * b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMULPD'. Intrinsic: '_mm512_maskz_mul_round_pd'.
// Requires AVX512F.
func MaskzMulRoundPd(k Mmask8, a M512d, b M512d, rounding int) M512d {
	return M512d(maskzMulRoundPd(uint8(k), [8]float64(a), [8]float64(b), rounding))
}

func maskzMulRoundPd(k uint8, a [8]float64, b [8]float64, rounding int) [8]float64


// MulRoundPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store the results in 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := a[i+63:i] * b[i+63:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMULPD'. Intrinsic: '_mm512_mul_round_pd'.
// Requires KNCNI.
func MulRoundPd(a M512d, b M512d, rounding int) M512d {
	return M512d(mulRoundPd([8]float64(a), [8]float64(b), rounding))
}

func mulRoundPd(a [8]float64, b [8]float64, rounding int) [8]float64


// MaskMulRoundPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set).
// 	 Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMULPS'. Intrinsic: '_mm512_mask_mul_round_ps'.
// Requires KNCNI.
func MaskMulRoundPs(src M512, k Mmask16, a M512, b M512, rounding int) M512 {
	return M512(maskMulRoundPs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b), rounding))
}

func maskMulRoundPs(src [16]float32, k uint16, a [16]float32, b [16]float32, rounding int) [16]float32


// MaskzMulRoundPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMULPS'. Intrinsic: '_mm512_maskz_mul_round_ps'.
// Requires AVX512F.
func MaskzMulRoundPs(k Mmask16, a M512, b M512, rounding int) M512 {
	return M512(maskzMulRoundPs(uint16(k), [16]float32(a), [16]float32(b), rounding))
}

func maskzMulRoundPs(k uint16, a [16]float32, b [16]float32, rounding int) [16]float32


// MulRoundPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store the results in 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := a[i+31:i] * b[i+31:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMULPS'. Intrinsic: '_mm512_mul_round_ps'.
// Requires KNCNI.
func MulRoundPs(a M512, b M512, rounding int) M512 {
	return M512(mulRoundPs([16]float32(a), [16]float32(b), rounding))
}

func mulRoundPs(a [16]float32, b [16]float32, rounding int) [16]float32


// MaskMulhiEpi16: Multiply the packed 16-bit integers in 'a' and 'b',
// producing intermediate 32-bit integers, and store the high 16 bits of the
// intermediate integers in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				tmp[31:0] := a[i+15:i] * b[i+15:i]
//				dst[i+15:i] := tmp[31:16]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMULHW'. Intrinsic: '_mm512_mask_mulhi_epi16'.
// Requires AVX512BW.
func MaskMulhiEpi16(src M512i, k Mmask32, a M512i, b M512i) M512i {
	return M512i(maskMulhiEpi16([64]byte(src), uint32(k), [64]byte(a), [64]byte(b)))
}

func maskMulhiEpi16(src [64]byte, k uint32, a [64]byte, b [64]byte) [64]byte


// MaskzMulhiEpi16: Multiply the packed 16-bit integers in 'a' and 'b',
// producing intermediate 32-bit integers, and store the high 16 bits of the
// intermediate integers in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				tmp[31:0] := a[i+15:i] * b[i+15:i]
//				dst[i+15:i] := tmp[31:16]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMULHW'. Intrinsic: '_mm512_maskz_mulhi_epi16'.
// Requires AVX512BW.
func MaskzMulhiEpi16(k Mmask32, a M512i, b M512i) M512i {
	return M512i(maskzMulhiEpi16(uint32(k), [64]byte(a), [64]byte(b)))
}

func maskzMulhiEpi16(k uint32, a [64]byte, b [64]byte) [64]byte


// MulhiEpi16: Multiply the packed 16-bit integers in 'a' and 'b', producing
// intermediate 32-bit integers, and store the high 16 bits of the intermediate
// integers in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			tmp[31:0] := a[i+15:i] * b[i+15:i]
//			dst[i+15:i] := tmp[31:16]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMULHW'. Intrinsic: '_mm512_mulhi_epi16'.
// Requires AVX512BW.
func MulhiEpi16(a M512i, b M512i) M512i {
	return M512i(mulhiEpi16([64]byte(a), [64]byte(b)))
}

func mulhiEpi16(a [64]byte, b [64]byte) [64]byte


// MaskMulhiEpi32: Performs element-by-element multiplication between packed
// 32-bit integer elements in 'a' and 'b' and stores the high 32 bits of each
// result into 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) >> 32
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMULHD'. Intrinsic: '_mm512_mask_mulhi_epi32'.
// Requires KNCNI.
func MaskMulhiEpi32(src M512i, k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskMulhiEpi32([64]byte(src), uint16(k), [64]byte(a), [64]byte(b)))
}

func maskMulhiEpi32(src [64]byte, k uint16, a [64]byte, b [64]byte) [64]byte


// MulhiEpi32: Performs element-by-element multiplication between packed 32-bit
// integer elements in 'a' and 'b' and stores the high 32 bits of each result
// into 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) >> 32
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMULHD'. Intrinsic: '_mm512_mulhi_epi32'.
// Requires KNCNI.
func MulhiEpi32(a M512i, b M512i) M512i {
	return M512i(mulhiEpi32([64]byte(a), [64]byte(b)))
}

func mulhiEpi32(a [64]byte, b [64]byte) [64]byte


// MaskMulhiEpu16: Multiply the packed unsigned 16-bit integers in 'a' and 'b',
// producing intermediate 32-bit integers, and store the high 16 bits of the
// intermediate integers in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				tmp[31:0] := a[i+15:i] * b[i+15:i]
//				dst[i+15:i] := tmp[31:16]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMULHUW'. Intrinsic: '_mm512_mask_mulhi_epu16'.
// Requires AVX512BW.
func MaskMulhiEpu16(src M512i, k Mmask32, a M512i, b M512i) M512i {
	return M512i(maskMulhiEpu16([64]byte(src), uint32(k), [64]byte(a), [64]byte(b)))
}

func maskMulhiEpu16(src [64]byte, k uint32, a [64]byte, b [64]byte) [64]byte


// MaskzMulhiEpu16: Multiply the packed unsigned 16-bit integers in 'a' and
// 'b', producing intermediate 32-bit integers, and store the high 16 bits of
// the intermediate integers in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				tmp[31:0] := a[i+15:i] * b[i+15:i]
//				dst[i+15:i] := tmp[31:16]
//			ELSE
//				dst[i+15:i] := o
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMULHUW'. Intrinsic: '_mm512_maskz_mulhi_epu16'.
// Requires AVX512BW.
func MaskzMulhiEpu16(k Mmask32, a M512i, b M512i) M512i {
	return M512i(maskzMulhiEpu16(uint32(k), [64]byte(a), [64]byte(b)))
}

func maskzMulhiEpu16(k uint32, a [64]byte, b [64]byte) [64]byte


// MulhiEpu16: Multiply the packed unsigned 16-bit integers in 'a' and 'b',
// producing intermediate 32-bit integers, and store the high 16 bits of the
// intermediate integers in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			tmp[31:0] := a[i+15:i] * b[i+15:i]
//			dst[i+15:i] := tmp[31:16]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMULHUW'. Intrinsic: '_mm512_mulhi_epu16'.
// Requires AVX512BW.
func MulhiEpu16(a M512i, b M512i) M512i {
	return M512i(mulhiEpu16([64]byte(a), [64]byte(b)))
}

func mulhiEpu16(a [64]byte, b [64]byte) [64]byte


// MaskMulhiEpu32: Performs element-by-element multiplication between packed
// unsigned 32-bit integer elements in 'a' and 'b' and stores the high 32 bits
// of each result into 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) >> 32
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMULHUD'. Intrinsic: '_mm512_mask_mulhi_epu32'.
// Requires KNCNI.
func MaskMulhiEpu32(src M512i, k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskMulhiEpu32([64]byte(src), uint16(k), [64]byte(a), [64]byte(b)))
}

func maskMulhiEpu32(src [64]byte, k uint16, a [64]byte, b [64]byte) [64]byte


// MulhiEpu32: Performs element-by-element multiplication between packed
// unsigned 32-bit integer elements in 'a' and 'b' and stores the high 32 bits
// of each result into 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) >> 32
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMULHUD'. Intrinsic: '_mm512_mulhi_epu32'.
// Requires KNCNI.
func MulhiEpu32(a M512i, b M512i) M512i {
	return M512i(mulhiEpu32([64]byte(a), [64]byte(b)))
}

func mulhiEpu32(a [64]byte, b [64]byte) [64]byte


// MaskMulhrsEpi16: Multiply packed 16-bit integers in 'a' and 'b', producing
// intermediate signed 32-bit integers. Truncate each intermediate integer to
// the 18 most significant bits, round by adding 1, and store bits [16:1] to
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				tmp[31:0] := ((a[i+15:i] * b[i+15:i]) >> 14) + 1
//				dst[i+15:i] := tmp[16:1]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMULHRSW'. Intrinsic: '_mm512_mask_mulhrs_epi16'.
// Requires AVX512BW.
func MaskMulhrsEpi16(src M512i, k Mmask32, a M512i, b M512i) M512i {
	return M512i(maskMulhrsEpi16([64]byte(src), uint32(k), [64]byte(a), [64]byte(b)))
}

func maskMulhrsEpi16(src [64]byte, k uint32, a [64]byte, b [64]byte) [64]byte


// MaskzMulhrsEpi16: Multiply packed 16-bit integers in 'a' and 'b', producing
// intermediate signed 32-bit integers. Truncate each intermediate integer to
// the 18 most significant bits, round by adding 1, and store bits [16:1] to
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				tmp[31:0] := ((a[i+15:i] * b[i+15:i]) >> 14) + 1
//				dst[i+15:i] := tmp[16:1]
//			ELSE
//				dst[i+15:i] := 9
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMULHRSW'. Intrinsic: '_mm512_maskz_mulhrs_epi16'.
// Requires AVX512BW.
func MaskzMulhrsEpi16(k Mmask32, a M512i, b M512i) M512i {
	return M512i(maskzMulhrsEpi16(uint32(k), [64]byte(a), [64]byte(b)))
}

func maskzMulhrsEpi16(k uint32, a [64]byte, b [64]byte) [64]byte


// MulhrsEpi16: Multiply packed 16-bit integers in 'a' and 'b', producing
// intermediate signed 32-bit integers. Truncate each intermediate integer to
// the 18 most significant bits, round by adding 1, and store bits [16:1] to
// 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			tmp[31:0] := ((a[i+15:i] * b[i+15:i]) >> 14) + 1
//			dst[i+15:i] := tmp[16:1]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMULHRSW'. Intrinsic: '_mm512_mulhrs_epi16'.
// Requires AVX512BW.
func MulhrsEpi16(a M512i, b M512i) M512i {
	return M512i(mulhrsEpi16([64]byte(a), [64]byte(b)))
}

func mulhrsEpi16(a [64]byte, b [64]byte) [64]byte


// MaskMulloEpi16: Multiply the packed 16-bit integers in 'a' and 'b',
// producing intermediate 32-bit integers, and store the low 16 bits of the
// intermediate integers in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				tmp[31:0] := a[i+15:i] * b[i+15:i]
//				dst[i+15:i] := tmp[15:0]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMULLW'. Intrinsic: '_mm512_mask_mullo_epi16'.
// Requires AVX512BW.
func MaskMulloEpi16(src M512i, k Mmask32, a M512i, b M512i) M512i {
	return M512i(maskMulloEpi16([64]byte(src), uint32(k), [64]byte(a), [64]byte(b)))
}

func maskMulloEpi16(src [64]byte, k uint32, a [64]byte, b [64]byte) [64]byte


// MaskzMulloEpi16: Multiply the packed 16-bit integers in 'a' and 'b',
// producing intermediate 32-bit integers, and store the low 16 bits of the
// intermediate integers in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				tmp[31:0] := a[i+15:i] * b[i+15:i]
//				dst[i+15:i] := tmp[15:0]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMULLW'. Intrinsic: '_mm512_maskz_mullo_epi16'.
// Requires AVX512BW.
func MaskzMulloEpi16(k Mmask32, a M512i, b M512i) M512i {
	return M512i(maskzMulloEpi16(uint32(k), [64]byte(a), [64]byte(b)))
}

func maskzMulloEpi16(k uint32, a [64]byte, b [64]byte) [64]byte


// MulloEpi16: Multiply the packed 16-bit integers in 'a' and 'b', producing
// intermediate 32-bit integers, and store the low 16 bits of the intermediate
// integers in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			tmp[31:0] := a[i+15:i] * b[i+15:i]
//			dst[i+15:i] := tmp[15:0]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMULLW'. Intrinsic: '_mm512_mullo_epi16'.
// Requires AVX512BW.
func MulloEpi16(a M512i, b M512i) M512i {
	return M512i(mulloEpi16([64]byte(a), [64]byte(b)))
}

func mulloEpi16(a [64]byte, b [64]byte) [64]byte


// MaskMulloEpi32: Multiply the packed 32-bit integers in 'a' and 'b',
// producing intermediate 64-bit integers, and store the low 32 bits of the
// intermediate integers in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				tmp[63:0] := a[i+31:i] * b[i+31:i]
//				dst[i+31:i] := tmp[31:0]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMULLD'. Intrinsic: '_mm512_mask_mullo_epi32'.
// Requires KNCNI.
func MaskMulloEpi32(src M512i, k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskMulloEpi32([64]byte(src), uint16(k), [64]byte(a), [64]byte(b)))
}

func maskMulloEpi32(src [64]byte, k uint16, a [64]byte, b [64]byte) [64]byte


// MaskzMulloEpi32: Multiply the packed 32-bit integers in 'a' and 'b',
// producing intermediate 64-bit integers, and store the low 32 bits of the
// intermediate integers in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				tmp[63:0] := a[i+31:i] * b[i+31:i]
//				dst[i+31:i] := tmp[31:0]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMULLD'. Intrinsic: '_mm512_maskz_mullo_epi32'.
// Requires AVX512F.
func MaskzMulloEpi32(k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskzMulloEpi32(uint16(k), [64]byte(a), [64]byte(b)))
}

func maskzMulloEpi32(k uint16, a [64]byte, b [64]byte) [64]byte


// MulloEpi32: Multiply the packed 32-bit integers in 'a' and 'b', producing
// intermediate 64-bit integers, and store the low 32 bits of the intermediate
// integers in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			tmp[63:0] := a[i+31:i] * b[i+31:i]
//			dst[i+31:i] := tmp[31:0]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMULLD'. Intrinsic: '_mm512_mullo_epi32'.
// Requires KNCNI.
func MulloEpi32(a M512i, b M512i) M512i {
	return M512i(mulloEpi32([64]byte(a), [64]byte(b)))
}

func mulloEpi32(a [64]byte, b [64]byte) [64]byte


// MaskMulloEpi64: Multiply the packed 64-bit integers in 'a' and 'b',
// producing intermediate 128-bit integers, and store the low 64 bits of the
// intermediate integers in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				tmp[127:0] := a[i+63:i] * b[i+63:i]
//				dst[i+63:i] := tmp[63:0]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMULLQ'. Intrinsic: '_mm512_mask_mullo_epi64'.
// Requires AVX512DQ.
func MaskMulloEpi64(src M512i, k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskMulloEpi64([64]byte(src), uint8(k), [64]byte(a), [64]byte(b)))
}

func maskMulloEpi64(src [64]byte, k uint8, a [64]byte, b [64]byte) [64]byte


// MaskzMulloEpi64: Multiply the packed 64-bit integers in 'a' and 'b',
// producing intermediate 128-bit integers, and store the low 64 bits of the
// intermediate integers in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				tmp[127:0] := a[i+63:i] * b[i+63:i]
//				dst[i+63:i] := tmp[63:0]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMULLQ'. Intrinsic: '_mm512_maskz_mullo_epi64'.
// Requires AVX512DQ.
func MaskzMulloEpi64(k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskzMulloEpi64(uint8(k), [64]byte(a), [64]byte(b)))
}

func maskzMulloEpi64(k uint8, a [64]byte, b [64]byte) [64]byte


// MulloEpi64: Multiply the packed 64-bit integers in 'a' and 'b', producing
// intermediate 128-bit integers, and store the low 64 bits of the intermediate
// integers in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			tmp[127:0] := a[i+63:i] * b[i+63:i]
//			dst[i+63:i] := tmp[63:0]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMULLQ'. Intrinsic: '_mm512_mullo_epi64'.
// Requires AVX512DQ.
func MulloEpi64(a M512i, b M512i) M512i {
	return M512i(mulloEpi64([64]byte(a), [64]byte(b)))
}

func mulloEpi64(a [64]byte, b [64]byte) [64]byte


// MaskMulloxEpi64: Multiplies elements in packed 64-bit integer vectors 'a'
// and 'b' together, storing the lower 64 bits of the result in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] * b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_mullox_epi64'.
// Requires AVX512F.
func MaskMulloxEpi64(src M512i, k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskMulloxEpi64([64]byte(src), uint8(k), [64]byte(a), [64]byte(b)))
}

func maskMulloxEpi64(src [64]byte, k uint8, a [64]byte, b [64]byte) [64]byte


// MulloxEpi64: Multiplies elements in packed 64-bit integer vectors 'a' and
// 'b' together, storing the lower 64 bits of the result in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := a[i+63:i] * b[i+63:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mullox_epi64'.
// Requires AVX512F.
func MulloxEpi64(a M512i, b M512i) M512i {
	return M512i(mulloxEpi64([64]byte(a), [64]byte(b)))
}

func mulloxEpi64(a [64]byte, b [64]byte) [64]byte


// MaskMultishiftEpi64Epi8: For each 64-bit element in 'b', select 8 unaligned
// bytes using a byte-granular shift control within the corresponding 64-bit
// element of 'a', and store the 8 assembled bytes to the corresponding 64-bit
// element of 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR i := 0 to 7
//			q := i * 64
//			FOR j := 0 to 7
//				tmp8 := 0
//				ctrl := a[q+j*8+7:q+j*8] & 63
//				FOR l := 0 to 7
//					tmp8[k] := b[q+((ctrl+k) & 63)]
//				ENDFOR
//				IF k[i*8+j]
//					dst[q+j*8+7:q+j*8] := tmp8[7:0]
//				ELSE
//					dst[q+j*8+7:q+j*8] := src[q+j*8+7:q+j*8]
//				FI
//			ENDFOR
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMULTISHIFTQB'. Intrinsic: '_mm512_mask_multishift_epi64_epi8'.
// Requires AVX512VBMI.
func MaskMultishiftEpi64Epi8(src M512i, k Mmask64, a M512i, b M512i) M512i {
	return M512i(maskMultishiftEpi64Epi8([64]byte(src), uint64(k), [64]byte(a), [64]byte(b)))
}

func maskMultishiftEpi64Epi8(src [64]byte, k uint64, a [64]byte, b [64]byte) [64]byte


// MaskzMultishiftEpi64Epi8: For each 64-bit element in 'b', select 8 unaligned
// bytes using a byte-granular shift control within the corresponding 64-bit
// element of 'a', and store the 8 assembled bytes to the corresponding 64-bit
// element of 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR i := 0 to 7
//			q := i * 64
//			FOR j := 0 to 7
//				tmp8 := 0
//				ctrl := a[q+j*8+7:q+j*8] & 63
//				FOR l := 0 to 7
//					tmp8[k] := b[q+((ctrl+k) & 63)]
//				ENDFOR
//				IF k[i*8+j]
//					dst[q+j*8+7:q+j*8] := tmp8[7:0]
//				ELSE
//					dst[q+j*8+7:q+j*8] := 0
//				FI
//			ENDFOR
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMULTISHIFTQB'. Intrinsic: '_mm512_maskz_multishift_epi64_epi8'.
// Requires AVX512VBMI.
func MaskzMultishiftEpi64Epi8(k Mmask64, a M512i, b M512i) M512i {
	return M512i(maskzMultishiftEpi64Epi8(uint64(k), [64]byte(a), [64]byte(b)))
}

func maskzMultishiftEpi64Epi8(k uint64, a [64]byte, b [64]byte) [64]byte


// MultishiftEpi64Epi8: For each 64-bit element in 'b', select 8 unaligned
// bytes using a byte-granular shift control within the corresponding 64-bit
// element of 'a', and store the 8 assembled bytes to the corresponding 64-bit
// element of 'dst'. 
//
//		FOR i := 0 to 7
//			q := i * 64
//			FOR j := 0 to 7
//				tmp8 := 0
//				ctrl := a[q+j*8+7:q+j*8] & 63
//				FOR l := 0 to 7
//					tmp8[k] := b[q+((ctrl+k) & 63)]
//				ENDFOR
//				dst[q+j*8+7:q+j*8] := tmp8[7:0]
//			ENDFOR
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMULTISHIFTQB'. Intrinsic: '_mm512_multishift_epi64_epi8'.
// Requires AVX512VBMI.
func MultishiftEpi64Epi8(a M512i, b M512i) M512i {
	return M512i(multishiftEpi64Epi8([64]byte(a), [64]byte(b)))
}

func multishiftEpi64Epi8(a [64]byte, b [64]byte) [64]byte


// MaskNearbyintPd: Rounds each packed double-precision (64-bit) floating-point
// element in 'a' to the nearest integer value and stores the results as packed
// double-precision floating-point elements in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := NearbyInt(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_nearbyint_pd'.
// Requires AVX512F.
func MaskNearbyintPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskNearbyintPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskNearbyintPd(src [8]float64, k uint8, a [8]float64) [8]float64


// NearbyintPd: Rounds each packed double-precision (64-bit) floating-point
// element in 'a' to the nearest integer value and stores the results as packed
// double-precision floating-point elements in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := NearbyInt(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_nearbyint_pd'.
// Requires AVX512F.
func NearbyintPd(a M512d) M512d {
	return M512d(nearbyintPd([8]float64(a)))
}

func nearbyintPd(a [8]float64) [8]float64


// MaskNearbyintPs: Rounds each packed single-precision (32-bit) floating-point
// element in 'a' to the nearest integer value and stores the results as packed
// double-precision floating-point elements in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := NearbyInt(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_nearbyint_ps'.
// Requires AVX512F.
func MaskNearbyintPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskNearbyintPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskNearbyintPs(src [16]float32, k uint16, a [16]float32) [16]float32


// NearbyintPs: Rounds each packed single-precision (32-bit) floating-point
// element in 'a' to the nearest integer value and stores the results as packed
// double-precision floating-point elements in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := NearbyInt(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_nearbyint_ps'.
// Requires AVX512F.
func NearbyintPs(a M512) M512 {
	return M512(nearbyintPs([16]float32(a)))
}

func nearbyintPs(a [16]float32) [16]float32


// MaskOrEpi32: Compute the bitwise OR of packed 32-bit integers in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] OR b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPORD'. Intrinsic: '_mm512_mask_or_epi32'.
// Requires KNCNI.
func MaskOrEpi32(src M512i, k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskOrEpi32([64]byte(src), uint16(k), [64]byte(a), [64]byte(b)))
}

func maskOrEpi32(src [64]byte, k uint16, a [64]byte, b [64]byte) [64]byte


// MaskzOrEpi32: Compute the bitwise OR of packed 32-bit integers in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] OR b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPORD'. Intrinsic: '_mm512_maskz_or_epi32'.
// Requires AVX512F.
func MaskzOrEpi32(k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskzOrEpi32(uint16(k), [64]byte(a), [64]byte(b)))
}

func maskzOrEpi32(k uint16, a [64]byte, b [64]byte) [64]byte


// OrEpi32: Compute the bitwise OR of packed 32-bit integers in 'a' and 'b',
// and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := a[i+31:i] OR b[i+31:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPORD'. Intrinsic: '_mm512_or_epi32'.
// Requires KNCNI.
func OrEpi32(a M512i, b M512i) M512i {
	return M512i(orEpi32([64]byte(a), [64]byte(b)))
}

func orEpi32(a [64]byte, b [64]byte) [64]byte


// MaskOrEpi64: Compute the bitwise OR of packed 64-bit integers in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] OR b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPORQ'. Intrinsic: '_mm512_mask_or_epi64'.
// Requires KNCNI.
func MaskOrEpi64(src M512i, k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskOrEpi64([64]byte(src), uint8(k), [64]byte(a), [64]byte(b)))
}

func maskOrEpi64(src [64]byte, k uint8, a [64]byte, b [64]byte) [64]byte


// MaskzOrEpi64: Compute the bitwise OR of packed 64-bit integers in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] OR b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPORQ'. Intrinsic: '_mm512_maskz_or_epi64'.
// Requires AVX512F.
func MaskzOrEpi64(k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskzOrEpi64(uint8(k), [64]byte(a), [64]byte(b)))
}

func maskzOrEpi64(k uint8, a [64]byte, b [64]byte) [64]byte


// OrEpi64: Compute the bitwise OR of packed 64-bit integers in 'a' and 'b',
// and store the resut in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := a[i+63:i] OR b[i+63:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPORQ'. Intrinsic: '_mm512_or_epi64'.
// Requires KNCNI.
func OrEpi64(a M512i, b M512i) M512i {
	return M512i(orEpi64([64]byte(a), [64]byte(b)))
}

func orEpi64(a [64]byte, b [64]byte) [64]byte


// MaskOrPd: Compute the bitwise OR of packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] BITWISE OR b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VORPD'. Intrinsic: '_mm512_mask_or_pd'.
// Requires AVX512DQ.
func MaskOrPd(src M512d, k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskOrPd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b)))
}

func maskOrPd(src [8]float64, k uint8, a [8]float64, b [8]float64) [8]float64


// MaskzOrPd: Compute the bitwise OR of packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] BITWISE OR b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VORPD'. Intrinsic: '_mm512_maskz_or_pd'.
// Requires AVX512DQ.
func MaskzOrPd(k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskzOrPd(uint8(k), [8]float64(a), [8]float64(b)))
}

func maskzOrPd(k uint8, a [8]float64, b [8]float64) [8]float64


// OrPd: Compute the bitwise OR of packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := a[i+63:i] BITWISE OR b[i+63:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VORPD'. Intrinsic: '_mm512_or_pd'.
// Requires AVX512DQ.
func OrPd(a M512d, b M512d) M512d {
	return M512d(orPd([8]float64(a), [8]float64(b)))
}

func orPd(a [8]float64, b [8]float64) [8]float64


// MaskOrPs: Compute the bitwise OR of packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] BITWISE OR b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VORPS'. Intrinsic: '_mm512_mask_or_ps'.
// Requires AVX512DQ.
func MaskOrPs(src M512, k Mmask16, a M512, b M512) M512 {
	return M512(maskOrPs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b)))
}

func maskOrPs(src [16]float32, k uint16, a [16]float32, b [16]float32) [16]float32


// MaskzOrPs: Compute the bitwise OR of packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] BITWISE OR b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VORPS'. Intrinsic: '_mm512_maskz_or_ps'.
// Requires AVX512DQ.
func MaskzOrPs(k Mmask16, a M512, b M512) M512 {
	return M512(maskzOrPs(uint16(k), [16]float32(a), [16]float32(b)))
}

func maskzOrPs(k uint16, a [16]float32, b [16]float32) [16]float32


// OrPs: Compute the bitwise OR of packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := a[i+31:i] BITWISE OR b[i+31:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VORPS'. Intrinsic: '_mm512_or_ps'.
// Requires AVX512DQ.
func OrPs(a M512, b M512) M512 {
	return M512(orPs([16]float32(a), [16]float32(b)))
}

func orPs(a [16]float32, b [16]float32) [16]float32


// OrSi512: Compute the bitwise OR of 512 bits (representing integer data) in
// 'a' and 'b', and store the result in 'dst'. 
//
//		dst[511:0] := (a[511:0] OR b[511:0])
//		dst[MAX:512] := 0
//
// Instruction: 'VPORD'. Intrinsic: '_mm512_or_si512'.
// Requires KNCNI.
func OrSi512(a M512i, b M512i) M512i {
	return M512i(orSi512([64]byte(a), [64]byte(b)))
}

func orSi512(a [64]byte, b [64]byte) [64]byte


// MaskPacksEpi16: Convert packed 16-bit integers from 'a' and 'b' to packed
// 8-bit integers using signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp_dst[7:0] := Saturate_Int16_To_Int8 (a[15:0])
//		tmp_dst[15:8] := Saturate_Int16_To_Int8 (a[31:16])
//		tmp_dst[23:16] := Saturate_Int16_To_Int8 (a[47:32])
//		tmp_dst[31:24] := Saturate_Int16_To_Int8 (a[63:48])
//		tmp_dst[39:32] := Saturate_Int16_To_Int8 (a[79:64])
//		tmp_dst[47:40] := Saturate_Int16_To_Int8 (a[95:80])
//		tmp_dst[55:48] := Saturate_Int16_To_Int8 (a[111:96])
//		tmp_dst[63:56] := Saturate_Int16_To_Int8 (a[127:112])
//		tmp_dst[71:64] := Saturate_Int16_To_Int8 (b[15:0])
//		tmp_dst[79:72] := Saturate_Int16_To_Int8 (b[31:16])
//		tmp_dst[87:80] := Saturate_Int16_To_Int8 (b[47:32])
//		tmp_dst[95:88] := Saturate_Int16_To_Int8 (b[63:48])
//		tmp_dst[103:96] := Saturate_Int16_To_Int8 (b[79:64])
//		tmp_dst[111:104] := Saturate_Int16_To_Int8 (b[95:80])
//		tmp_dst[119:112] := Saturate_Int16_To_Int8 (b[111:96])
//		tmp_dst[127:120] := Saturate_Int16_To_Int8 (b[127:112])
//		tmp_dst[135:128] := Saturate_Int16_To_Int8 (a[143:128])
//		tmp_dst[143:136] := Saturate_Int16_To_Int8 (a[159:144])
//		tmp_dst[151:144] := Saturate_Int16_To_Int8 (a[175:160])
//		tmp_dst[159:152] := Saturate_Int16_To_Int8 (a[191:176])
//		tmp_dst[167:160] := Saturate_Int16_To_Int8 (a[207:192])
//		tmp_dst[175:168] := Saturate_Int16_To_Int8 (a[223:208])
//		tmp_dst[183:176] := Saturate_Int16_To_Int8 (a[239:224])
//		tmp_dst[191:184] := Saturate_Int16_To_Int8 (a[255:240])
//		tmp_dst[199:192] := Saturate_Int16_To_Int8 (b[143:128])
//		tmp_dst[207:200] := Saturate_Int16_To_Int8 (b[159:144])
//		tmp_dst[215:208] := Saturate_Int16_To_Int8 (b[175:160])
//		tmp_dst[223:216] := Saturate_Int16_To_Int8 (b[191:176])
//		tmp_dst[231:224] := Saturate_Int16_To_Int8 (b[207:192])
//		tmp_dst[239:232] := Saturate_Int16_To_Int8 (b[223:208])
//		tmp_dst[247:240] := Saturate_Int16_To_Int8 (b[239:224])
//		tmp_dst[255:248] := Saturate_Int16_To_Int8 (b[255:240])
//		tmp_dst[263:256] := Saturate_Int16_To_Int8 (a[271:256])
//		tmp_dst[271:264] := Saturate_Int16_To_Int8 (a[287:272])
//		tmp_dst[279:272] := Saturate_Int16_To_Int8 (a[303:288])
//		tmp_dst[287:280] := Saturate_Int16_To_Int8 (a[319:304])
//		tmp_dst[295:288] := Saturate_Int16_To_Int8 (a[335:320])
//		tmp_dst[303:296] := Saturate_Int16_To_Int8 (a[351:336])
//		tmp_dst[311:304] := Saturate_Int16_To_Int8 (a[367:352])
//		tmp_dst[319:312] := Saturate_Int16_To_Int8 (a[383:368])
//		tmp_dst[327:320] := Saturate_Int16_To_Int8 (b[271:256])
//		tmp_dst[335:328] := Saturate_Int16_To_Int8 (b[287:272])
//		tmp_dst[343:336] := Saturate_Int16_To_Int8 (b[303:288])
//		tmp_dst[351:344] := Saturate_Int16_To_Int8 (b[319:304])
//		tmp_dst[359:352] := Saturate_Int16_To_Int8 (b[335:320])
//		tmp_dst[367:360] := Saturate_Int16_To_Int8 (b[351:336])
//		tmp_dst[375:368] := Saturate_Int16_To_Int8 (b[367:352])
//		tmp_dst[383:376] := Saturate_Int16_To_Int8 (b[383:368])
//		tmp_dst[391:384] := Saturate_Int16_To_Int8 (a[399:384])
//		tmp_dst[399:392] := Saturate_Int16_To_Int8 (a[415:400])
//		tmp_dst[407:400] := Saturate_Int16_To_Int8 (a[431:416])
//		tmp_dst[415:408] := Saturate_Int16_To_Int8 (a[447:432])
//		tmp_dst[423:416] := Saturate_Int16_To_Int8 (a[463:448])
//		tmp_dst[431:424] := Saturate_Int16_To_Int8 (a[479:464])
//		tmp_dst[439:432] := Saturate_Int16_To_Int8 (a[495:480])
//		tmp_dst[447:440] := Saturate_Int16_To_Int8 (a[511:496])
//		tmp_dst[455:448] := Saturate_Int16_To_Int8 (b[399:384])
//		tmp_dst[463:456] := Saturate_Int16_To_Int8 (b[415:400])
//		tmp_dst[471:464] := Saturate_Int16_To_Int8 (b[431:416])
//		tmp_dst[479:472] := Saturate_Int16_To_Int8 (b[447:432])
//		tmp_dst[487:480] := Saturate_Int16_To_Int8 (b[463:448])
//		tmp_dst[495:488] := Saturate_Int16_To_Int8 (b[479:464])
//		tmp_dst[503:496] := Saturate_Int16_To_Int8 (b[495:480])
//		tmp_dst[511:504] := Saturate_Int16_To_Int8 (b[511:496])
//		
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := tmp_dst[i+7:i]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPACKSSWB'. Intrinsic: '_mm512_mask_packs_epi16'.
// Requires AVX512BW.
func MaskPacksEpi16(src M512i, k Mmask64, a M512i, b M512i) M512i {
	return M512i(maskPacksEpi16([64]byte(src), uint64(k), [64]byte(a), [64]byte(b)))
}

func maskPacksEpi16(src [64]byte, k uint64, a [64]byte, b [64]byte) [64]byte


// MaskzPacksEpi16: Convert packed 16-bit integers from 'a' and 'b' to packed
// 8-bit integers using signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp_dst[7:0] := Saturate_Int16_To_Int8 (a[15:0])
//		tmp_dst[15:8] := Saturate_Int16_To_Int8 (a[31:16])
//		tmp_dst[23:16] := Saturate_Int16_To_Int8 (a[47:32])
//		tmp_dst[31:24] := Saturate_Int16_To_Int8 (a[63:48])
//		tmp_dst[39:32] := Saturate_Int16_To_Int8 (a[79:64])
//		tmp_dst[47:40] := Saturate_Int16_To_Int8 (a[95:80])
//		tmp_dst[55:48] := Saturate_Int16_To_Int8 (a[111:96])
//		tmp_dst[63:56] := Saturate_Int16_To_Int8 (a[127:112])
//		tmp_dst[71:64] := Saturate_Int16_To_Int8 (b[15:0])
//		tmp_dst[79:72] := Saturate_Int16_To_Int8 (b[31:16])
//		tmp_dst[87:80] := Saturate_Int16_To_Int8 (b[47:32])
//		tmp_dst[95:88] := Saturate_Int16_To_Int8 (b[63:48])
//		tmp_dst[103:96] := Saturate_Int16_To_Int8 (b[79:64])
//		tmp_dst[111:104] := Saturate_Int16_To_Int8 (b[95:80])
//		tmp_dst[119:112] := Saturate_Int16_To_Int8 (b[111:96])
//		tmp_dst[127:120] := Saturate_Int16_To_Int8 (b[127:112])
//		tmp_dst[135:128] := Saturate_Int16_To_Int8 (a[143:128])
//		tmp_dst[143:136] := Saturate_Int16_To_Int8 (a[159:144])
//		tmp_dst[151:144] := Saturate_Int16_To_Int8 (a[175:160])
//		tmp_dst[159:152] := Saturate_Int16_To_Int8 (a[191:176])
//		tmp_dst[167:160] := Saturate_Int16_To_Int8 (a[207:192])
//		tmp_dst[175:168] := Saturate_Int16_To_Int8 (a[223:208])
//		tmp_dst[183:176] := Saturate_Int16_To_Int8 (a[239:224])
//		tmp_dst[191:184] := Saturate_Int16_To_Int8 (a[255:240])
//		tmp_dst[199:192] := Saturate_Int16_To_Int8 (b[143:128])
//		tmp_dst[207:200] := Saturate_Int16_To_Int8 (b[159:144])
//		tmp_dst[215:208] := Saturate_Int16_To_Int8 (b[175:160])
//		tmp_dst[223:216] := Saturate_Int16_To_Int8 (b[191:176])
//		tmp_dst[231:224] := Saturate_Int16_To_Int8 (b[207:192])
//		tmp_dst[239:232] := Saturate_Int16_To_Int8 (b[223:208])
//		tmp_dst[247:240] := Saturate_Int16_To_Int8 (b[239:224])
//		tmp_dst[255:248] := Saturate_Int16_To_Int8 (b[255:240])
//		tmp_dst[263:256] := Saturate_Int16_To_Int8 (a[271:256])
//		tmp_dst[271:264] := Saturate_Int16_To_Int8 (a[287:272])
//		tmp_dst[279:272] := Saturate_Int16_To_Int8 (a[303:288])
//		tmp_dst[287:280] := Saturate_Int16_To_Int8 (a[319:304])
//		tmp_dst[295:288] := Saturate_Int16_To_Int8 (a[335:320])
//		tmp_dst[303:296] := Saturate_Int16_To_Int8 (a[351:336])
//		tmp_dst[311:304] := Saturate_Int16_To_Int8 (a[367:352])
//		tmp_dst[319:312] := Saturate_Int16_To_Int8 (a[383:368])
//		tmp_dst[327:320] := Saturate_Int16_To_Int8 (b[271:256])
//		tmp_dst[335:328] := Saturate_Int16_To_Int8 (b[287:272])
//		tmp_dst[343:336] := Saturate_Int16_To_Int8 (b[303:288])
//		tmp_dst[351:344] := Saturate_Int16_To_Int8 (b[319:304])
//		tmp_dst[359:352] := Saturate_Int16_To_Int8 (b[335:320])
//		tmp_dst[367:360] := Saturate_Int16_To_Int8 (b[351:336])
//		tmp_dst[375:368] := Saturate_Int16_To_Int8 (b[367:352])
//		tmp_dst[383:376] := Saturate_Int16_To_Int8 (b[383:368])
//		tmp_dst[391:384] := Saturate_Int16_To_Int8 (a[399:384])
//		tmp_dst[399:392] := Saturate_Int16_To_Int8 (a[415:400])
//		tmp_dst[407:400] := Saturate_Int16_To_Int8 (a[431:416])
//		tmp_dst[415:408] := Saturate_Int16_To_Int8 (a[447:432])
//		tmp_dst[423:416] := Saturate_Int16_To_Int8 (a[463:448])
//		tmp_dst[431:424] := Saturate_Int16_To_Int8 (a[479:464])
//		tmp_dst[439:432] := Saturate_Int16_To_Int8 (a[495:480])
//		tmp_dst[447:440] := Saturate_Int16_To_Int8 (a[511:496])
//		tmp_dst[455:448] := Saturate_Int16_To_Int8 (b[399:384])
//		tmp_dst[463:456] := Saturate_Int16_To_Int8 (b[415:400])
//		tmp_dst[471:464] := Saturate_Int16_To_Int8 (b[431:416])
//		tmp_dst[479:472] := Saturate_Int16_To_Int8 (b[447:432])
//		tmp_dst[487:480] := Saturate_Int16_To_Int8 (b[463:448])
//		tmp_dst[495:488] := Saturate_Int16_To_Int8 (b[479:464])
//		tmp_dst[503:496] := Saturate_Int16_To_Int8 (b[495:480])
//		tmp_dst[511:504] := Saturate_Int16_To_Int8 (b[511:496])
//		
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := tmp_dst[i+7:i]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPACKSSWB'. Intrinsic: '_mm512_maskz_packs_epi16'.
// Requires AVX512BW.
func MaskzPacksEpi16(k Mmask64, a M512i, b M512i) M512i {
	return M512i(maskzPacksEpi16(uint64(k), [64]byte(a), [64]byte(b)))
}

func maskzPacksEpi16(k uint64, a [64]byte, b [64]byte) [64]byte


// PacksEpi16: Convert packed 16-bit integers from 'a' and 'b' to packed 8-bit
// integers using signed saturation, and store the results in 'dst'. 
//
//		dst[7:0] := Saturate_Int16_To_Int8 (a[15:0])
//		dst[15:8] := Saturate_Int16_To_Int8 (a[31:16])
//		dst[23:16] := Saturate_Int16_To_Int8 (a[47:32])
//		dst[31:24] := Saturate_Int16_To_Int8 (a[63:48])
//		dst[39:32] := Saturate_Int16_To_Int8 (a[79:64])
//		dst[47:40] := Saturate_Int16_To_Int8 (a[95:80])
//		dst[55:48] := Saturate_Int16_To_Int8 (a[111:96])
//		dst[63:56] := Saturate_Int16_To_Int8 (a[127:112])
//		dst[71:64] := Saturate_Int16_To_Int8 (b[15:0])
//		dst[79:72] := Saturate_Int16_To_Int8 (b[31:16])
//		dst[87:80] := Saturate_Int16_To_Int8 (b[47:32])
//		dst[95:88] := Saturate_Int16_To_Int8 (b[63:48])
//		dst[103:96] := Saturate_Int16_To_Int8 (b[79:64])
//		dst[111:104] := Saturate_Int16_To_Int8 (b[95:80])
//		dst[119:112] := Saturate_Int16_To_Int8 (b[111:96])
//		dst[127:120] := Saturate_Int16_To_Int8 (b[127:112])
//		dst[135:128] := Saturate_Int16_To_Int8 (a[143:128])
//		dst[143:136] := Saturate_Int16_To_Int8 (a[159:144])
//		dst[151:144] := Saturate_Int16_To_Int8 (a[175:160])
//		dst[159:152] := Saturate_Int16_To_Int8 (a[191:176])
//		dst[167:160] := Saturate_Int16_To_Int8 (a[207:192])
//		dst[175:168] := Saturate_Int16_To_Int8 (a[223:208])
//		dst[183:176] := Saturate_Int16_To_Int8 (a[239:224])
//		dst[191:184] := Saturate_Int16_To_Int8 (a[255:240])
//		dst[199:192] := Saturate_Int16_To_Int8 (b[143:128])
//		dst[207:200] := Saturate_Int16_To_Int8 (b[159:144])
//		dst[215:208] := Saturate_Int16_To_Int8 (b[175:160])
//		dst[223:216] := Saturate_Int16_To_Int8 (b[191:176])
//		dst[231:224] := Saturate_Int16_To_Int8 (b[207:192])
//		dst[239:232] := Saturate_Int16_To_Int8 (b[223:208])
//		dst[247:240] := Saturate_Int16_To_Int8 (b[239:224])
//		dst[255:248] := Saturate_Int16_To_Int8 (b[255:240])
//		dst[263:256] := Saturate_Int16_To_Int8 (a[271:256])
//		dst[271:264] := Saturate_Int16_To_Int8 (a[287:272])
//		dst[279:272] := Saturate_Int16_To_Int8 (a[303:288])
//		dst[287:280] := Saturate_Int16_To_Int8 (a[319:304])
//		dst[295:288] := Saturate_Int16_To_Int8 (a[335:320])
//		dst[303:296] := Saturate_Int16_To_Int8 (a[351:336])
//		dst[311:304] := Saturate_Int16_To_Int8 (a[367:352])
//		dst[319:312] := Saturate_Int16_To_Int8 (a[383:368])
//		dst[327:320] := Saturate_Int16_To_Int8 (b[271:256])
//		dst[335:328] := Saturate_Int16_To_Int8 (b[287:272])
//		dst[343:336] := Saturate_Int16_To_Int8 (b[303:288])
//		dst[351:344] := Saturate_Int16_To_Int8 (b[319:304])
//		dst[359:352] := Saturate_Int16_To_Int8 (b[335:320])
//		dst[367:360] := Saturate_Int16_To_Int8 (b[351:336])
//		dst[375:368] := Saturate_Int16_To_Int8 (b[367:352])
//		dst[383:376] := Saturate_Int16_To_Int8 (b[383:368])
//		dst[391:384] := Saturate_Int16_To_Int8 (a[399:384])
//		dst[399:392] := Saturate_Int16_To_Int8 (a[415:400])
//		dst[407:400] := Saturate_Int16_To_Int8 (a[431:416])
//		dst[415:408] := Saturate_Int16_To_Int8 (a[447:432])
//		dst[423:416] := Saturate_Int16_To_Int8 (a[463:448])
//		dst[431:424] := Saturate_Int16_To_Int8 (a[479:464])
//		dst[439:432] := Saturate_Int16_To_Int8 (a[495:480])
//		dst[447:440] := Saturate_Int16_To_Int8 (a[511:496])
//		dst[455:448] := Saturate_Int16_To_Int8 (b[399:384])
//		dst[463:456] := Saturate_Int16_To_Int8 (b[415:400])
//		dst[471:464] := Saturate_Int16_To_Int8 (b[431:416])
//		dst[479:472] := Saturate_Int16_To_Int8 (b[447:432])
//		dst[487:480] := Saturate_Int16_To_Int8 (b[463:448])
//		dst[495:488] := Saturate_Int16_To_Int8 (b[479:464])
//		dst[503:496] := Saturate_Int16_To_Int8 (b[495:480])
//		dst[511:504] := Saturate_Int16_To_Int8 (b[511:496])
//		dst[MAX:512] := 0
//
// Instruction: 'VPACKSSWB'. Intrinsic: '_mm512_packs_epi16'.
// Requires AVX512BW.
func PacksEpi16(a M512i, b M512i) M512i {
	return M512i(packsEpi16([64]byte(a), [64]byte(b)))
}

func packsEpi16(a [64]byte, b [64]byte) [64]byte


// MaskPacksEpi32: Convert packed 32-bit integers from 'a' and 'b' to packed
// 16-bit integers using signed saturation, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		tmp_dst[15:0] := Saturate_Int32_To_Int16 (a[31:0])
//		tmp_dst[31:16] := Saturate_Int32_To_Int16 (a[63:32])
//		tmp_dst[47:32] := Saturate_Int32_To_Int16 (a[95:64])
//		tmp_dst[63:48] := Saturate_Int32_To_Int16 (a[127:96])
//		tmp_dst[79:64] := Saturate_Int32_To_Int16 (b[31:0])
//		tmp_dst[95:80] := Saturate_Int32_To_Int16 (b[63:32])
//		tmp_dst[111:96] := Saturate_Int32_To_Int16 (b[95:64])
//		tmp_dst[127:112] := Saturate_Int32_To_Int16 (b[127:96])
//		tmp_dst[143:128] := Saturate_Int32_To_Int16 (a[159:128])
//		tmp_dst[159:144] := Saturate_Int32_To_Int16 (a[191:160])
//		tmp_dst[175:160] := Saturate_Int32_To_Int16 (a[223:192])
//		tmp_dst[191:176] := Saturate_Int32_To_Int16 (a[255:224])
//		tmp_dst[207:192] := Saturate_Int32_To_Int16 (b[159:128])
//		tmp_dst[223:208] := Saturate_Int32_To_Int16 (b[191:160])
//		tmp_dst[239:224] := Saturate_Int32_To_Int16 (b[223:192])
//		tmp_dst[255:240] := Saturate_Int32_To_Int16 (b[255:224])
//		tmp_dst[271:256] := Saturate_Int32_To_Int16 (a[287:256])
//		tmp_dst[287:272] := Saturate_Int32_To_Int16 (a[319:288])
//		tmp_dst[303:288] := Saturate_Int32_To_Int16 (a[351:320])
//		tmp_dst[319:304] := Saturate_Int32_To_Int16 (a[383:352])
//		tmp_dst[335:320] := Saturate_Int32_To_Int16 (b[287:256])
//		tmp_dst[351:336] := Saturate_Int32_To_Int16 (b[319:288])
//		tmp_dst[367:352] := Saturate_Int32_To_Int16 (b[351:320])
//		tmp_dst[383:368] := Saturate_Int32_To_Int16 (b[383:352])
//		tmp_dst[399:384] := Saturate_Int32_To_Int16 (a[415:384])
//		tmp_dst[415:400] := Saturate_Int32_To_Int16 (a[447:416])
//		tmp_dst[431:416] := Saturate_Int32_To_Int16 (a[479:448])
//		tmp_dst[447:432] := Saturate_Int32_To_Int16 (a[511:480])
//		tmp_dst[463:448] := Saturate_Int32_To_Int16 (b[415:384])
//		tmp_dst[479:464] := Saturate_Int32_To_Int16 (b[447:416])
//		tmp_dst[495:480] := Saturate_Int32_To_Int16 (b[479:448])
//		tmp_dst[511:496] := Saturate_Int32_To_Int16 (b[511:480])
//		
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPACKSSDW'. Intrinsic: '_mm512_mask_packs_epi32'.
// Requires AVX512BW.
func MaskPacksEpi32(src M512i, k Mmask32, a M512i, b M512i) M512i {
	return M512i(maskPacksEpi32([64]byte(src), uint32(k), [64]byte(a), [64]byte(b)))
}

func maskPacksEpi32(src [64]byte, k uint32, a [64]byte, b [64]byte) [64]byte


// MaskzPacksEpi32: Convert packed 32-bit integers from 'a' and 'b' to packed
// 16-bit integers using signed saturation, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		tmp_dst[15:0] := Saturate_Int32_To_Int16 (a[31:0])
//		tmp_dst[31:16] := Saturate_Int32_To_Int16 (a[63:32])
//		tmp_dst[47:32] := Saturate_Int32_To_Int16 (a[95:64])
//		tmp_dst[63:48] := Saturate_Int32_To_Int16 (a[127:96])
//		tmp_dst[79:64] := Saturate_Int32_To_Int16 (b[31:0])
//		tmp_dst[95:80] := Saturate_Int32_To_Int16 (b[63:32])
//		tmp_dst[111:96] := Saturate_Int32_To_Int16 (b[95:64])
//		tmp_dst[127:112] := Saturate_Int32_To_Int16 (b[127:96])
//		tmp_dst[143:128] := Saturate_Int32_To_Int16 (a[159:128])
//		tmp_dst[159:144] := Saturate_Int32_To_Int16 (a[191:160])
//		tmp_dst[175:160] := Saturate_Int32_To_Int16 (a[223:192])
//		tmp_dst[191:176] := Saturate_Int32_To_Int16 (a[255:224])
//		tmp_dst[207:192] := Saturate_Int32_To_Int16 (b[159:128])
//		tmp_dst[223:208] := Saturate_Int32_To_Int16 (b[191:160])
//		tmp_dst[239:224] := Saturate_Int32_To_Int16 (b[223:192])
//		tmp_dst[255:240] := Saturate_Int32_To_Int16 (b[255:224])
//		tmp_dst[271:256] := Saturate_Int32_To_Int16 (a[287:256])
//		tmp_dst[287:272] := Saturate_Int32_To_Int16 (a[319:288])
//		tmp_dst[303:288] := Saturate_Int32_To_Int16 (a[351:320])
//		tmp_dst[319:304] := Saturate_Int32_To_Int16 (a[383:352])
//		tmp_dst[335:320] := Saturate_Int32_To_Int16 (b[287:256])
//		tmp_dst[351:336] := Saturate_Int32_To_Int16 (b[319:288])
//		tmp_dst[367:352] := Saturate_Int32_To_Int16 (b[351:320])
//		tmp_dst[383:368] := Saturate_Int32_To_Int16 (b[383:352])
//		tmp_dst[399:384] := Saturate_Int32_To_Int16 (a[415:384])
//		tmp_dst[415:400] := Saturate_Int32_To_Int16 (a[447:416])
//		tmp_dst[431:416] := Saturate_Int32_To_Int16 (a[479:448])
//		tmp_dst[447:432] := Saturate_Int32_To_Int16 (a[511:480])
//		tmp_dst[463:448] := Saturate_Int32_To_Int16 (b[415:384])
//		tmp_dst[479:464] := Saturate_Int32_To_Int16 (b[447:416])
//		tmp_dst[495:480] := Saturate_Int32_To_Int16 (b[479:448])
//		tmp_dst[511:496] := Saturate_Int32_To_Int16 (b[511:480])
//		
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPACKSSDW'. Intrinsic: '_mm512_maskz_packs_epi32'.
// Requires AVX512BW.
func MaskzPacksEpi32(k Mmask32, a M512i, b M512i) M512i {
	return M512i(maskzPacksEpi32(uint32(k), [64]byte(a), [64]byte(b)))
}

func maskzPacksEpi32(k uint32, a [64]byte, b [64]byte) [64]byte


// PacksEpi32: Convert packed 32-bit integers from 'a' and 'b' to packed 16-bit
// integers using signed saturation, and store the results in 'dst'. 
//
//		dst[15:0] := Saturate_Int32_To_Int16 (a[31:0])
//		dst[31:16] := Saturate_Int32_To_Int16 (a[63:32])
//		dst[47:32] := Saturate_Int32_To_Int16 (a[95:64])
//		dst[63:48] := Saturate_Int32_To_Int16 (a[127:96])
//		dst[79:64] := Saturate_Int32_To_Int16 (b[31:0])
//		dst[95:80] := Saturate_Int32_To_Int16 (b[63:32])
//		dst[111:96] := Saturate_Int32_To_Int16 (b[95:64])
//		dst[127:112] := Saturate_Int32_To_Int16 (b[127:96])
//		dst[143:128] := Saturate_Int32_To_Int16 (a[159:128])
//		dst[159:144] := Saturate_Int32_To_Int16 (a[191:160])
//		dst[175:160] := Saturate_Int32_To_Int16 (a[223:192])
//		dst[191:176] := Saturate_Int32_To_Int16 (a[255:224])
//		dst[207:192] := Saturate_Int32_To_Int16 (b[159:128])
//		dst[223:208] := Saturate_Int32_To_Int16 (b[191:160])
//		dst[239:224] := Saturate_Int32_To_Int16 (b[223:192])
//		dst[255:240] := Saturate_Int32_To_Int16 (b[255:224])
//		dst[271:256] := Saturate_Int32_To_Int16 (a[287:256])
//		dst[287:272] := Saturate_Int32_To_Int16 (a[319:288])
//		dst[303:288] := Saturate_Int32_To_Int16 (a[351:320])
//		dst[319:304] := Saturate_Int32_To_Int16 (a[383:352])
//		dst[335:320] := Saturate_Int32_To_Int16 (b[287:256])
//		dst[351:336] := Saturate_Int32_To_Int16 (b[319:288])
//		dst[367:352] := Saturate_Int32_To_Int16 (b[351:320])
//		dst[383:368] := Saturate_Int32_To_Int16 (b[383:352])
//		dst[399:384] := Saturate_Int32_To_Int16 (a[415:384])
//		dst[415:400] := Saturate_Int32_To_Int16 (a[447:416])
//		dst[431:416] := Saturate_Int32_To_Int16 (a[479:448])
//		dst[447:432] := Saturate_Int32_To_Int16 (a[511:480])
//		dst[463:448] := Saturate_Int32_To_Int16 (b[415:384])
//		dst[479:464] := Saturate_Int32_To_Int16 (b[447:416])
//		dst[495:480] := Saturate_Int32_To_Int16 (b[479:448])
//		dst[511:496] := Saturate_Int32_To_Int16 (b[511:480])
//		dst[MAX:512] := 0
//
// Instruction: 'VPACKSSDW'. Intrinsic: '_mm512_packs_epi32'.
// Requires AVX512BW.
func PacksEpi32(a M512i, b M512i) M512i {
	return M512i(packsEpi32([64]byte(a), [64]byte(b)))
}

func packsEpi32(a [64]byte, b [64]byte) [64]byte


// MaskPackstorehiEpi32: Stores packed 32-bit integer elements of 'v1' into a
// doubleword stream at a logically mapped starting address (mt-64), storing
// the high-64-byte elements of that stream (those elements of the stream that
// map at or after the first 64-byte-aligned address following (m5-64)).
// Elements are loaded from memory according to element selector 'k' (elements
// are skipped when the corresponding mask bit is not set). 
//
//		storeOffset := 0
//		foundNext64BytesBoundary := false
//		addr = mt-64
//		FOR j := 0 to 15
//			IF k[j]
//				IF foundNext64BytesBoundary == false
//					IF ((addr + (storeOffset + 1)*4) % 64) == 0
//						foundNext64BytesBoundary = true
//					FI
//				ELSE
//					i := j*32
//					MEM[addr + storeOffset*4] := v1[i+31:i]
//				FI
//				storeOffset := storeOffset + 1
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPACKSTOREHD'. Intrinsic: '_mm512_mask_packstorehi_epi32'.
// Requires KNCNI.
func MaskPackstorehiEpi32(mt uintptr, k Mmask16, v1 M512i)  {
	maskPackstorehiEpi32(uintptr(mt), uint16(k), [64]byte(v1))
}

func maskPackstorehiEpi32(mt uintptr, k uint16, v1 [64]byte) 


// PackstorehiEpi32: Stores packed 32-bit integer elements of 'v1' into a
// doubleword stream at a logically mapped starting address (mt-64), storing
// the high-64-byte elements of that stream (those elements of the stream that
// map at or after the first 64-byte-aligned address following (m5-64)). 
//
//		storeOffset := 0
//		foundNext64BytesBoundary := false
//		addr = mt-64
//		FOR j := 0 to 15
//			IF foundNext64BytesBoundary == false
//				IF ((addr + (storeOffset + 1)*4) % 64) == 0
//					foundNext64BytesBoundary = true
//				FI
//			ELSE
//				i := j*32
//				MEM[addr + storeOffset*4] := v1[i+31:i]
//			FI
//			storeOffset := storeOffset + 1
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPACKSTOREHD'. Intrinsic: '_mm512_packstorehi_epi32'.
// Requires KNCNI.
func PackstorehiEpi32(mt uintptr, v1 M512i)  {
	packstorehiEpi32(uintptr(mt), [64]byte(v1))
}

func packstorehiEpi32(mt uintptr, v1 [64]byte) 


// MaskPackstorehiEpi64: Stores packed 64-bit integer elements of 'v1' into a
// quadword stream at a logically mapped starting address (mt-64), storing the
// high-64-byte elements of that stream (those elemetns of the stream that map
// at or after the first 64-byte-aligned address following (m5-64)). Elements
// are loaded from memory according to element selector 'k' (elements are
// skipped when the corresponding mask bit is not set). 
//
//		storeOffset := 0
//		foundNext64BytesBoundary := false
//		addr = mt-64
//		FOR j := 0 to 7
//			IF k[j]
//				IF foundNext64BytesBoundary == false
//					IF ((addr + (storeOffset + 1)*8) % 64) == 0
//						foundNext64BytesBoundary = true
//					FI
//				ELSE
//					i := j*64
//					MEM[addr + storeOffset*8] := v1[i+63:i]
//				FI
//				storeOffset := storeOffset + 1
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPACKSTOREHQ'. Intrinsic: '_mm512_mask_packstorehi_epi64'.
// Requires KNCNI.
func MaskPackstorehiEpi64(mt uintptr, k Mmask8, v1 M512i)  {
	maskPackstorehiEpi64(uintptr(mt), uint8(k), [64]byte(v1))
}

func maskPackstorehiEpi64(mt uintptr, k uint8, v1 [64]byte) 


// PackstorehiEpi64: Stores packed 64-bit integer elements of 'v1' into a
// quadword stream at a logically mapped starting address (mt-64), storing the
// high-64-byte elements of that stream (those elemetns of the stream that map
// at or after the first 64-byte-aligned address following (m5-64)). 
//
//		storeOffset := 0
//		foundNext64BytesBoundary := false
//		addr = mt-64
//		FOR j := 0 to 7
//			IF foundNext64BytesBoundary == false
//				IF ((addr + (storeOffset + 1)*8) % 64) == 0
//					foundNext64BytesBoundary = true
//				FI
//			ELSE
//				i := j*64
//				MEM[addr + storeOffset*8] := v1[i+63:i]
//			FI
//			storeOffset := storeOffset + 1
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPACKSTOREHQ'. Intrinsic: '_mm512_packstorehi_epi64'.
// Requires KNCNI.
func PackstorehiEpi64(mt uintptr, v1 M512i)  {
	packstorehiEpi64(uintptr(mt), [64]byte(v1))
}

func packstorehiEpi64(mt uintptr, v1 [64]byte) 


// MaskPackstorehiPd: Stores packed double-precision (64-bit) floating-point
// elements of 'v1' into a quadword stream at a logically mapped starting
// address (mt-64), storing the high-64-byte elements of that stream (those
// elemetns of the stream that map at or after the first 64-byte-aligned
// address following (m5-64)). Elements are loaded from memory according to
// element selector 'k' (elements are skipped when the corresponding mask bit
// is not set). 
//
//		storeOffset := 0
//		foundNext64BytesBoundary := false
//		addr = mt-64
//		FOR j := 0 to 7
//			IF k[j]
//				IF foundNext64BytesBoundary == false
//					IF ((addr + (storeOffset + 1)*8) % 64) == 0
//						foundNext64BytesBoundary = true
//					FI
//				ELSE
//					i := j*64
//					MEM[addr + storeOffset*4] := v1[i+63:i]
//				FI
//				storeOffset := storeOffset + 1
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPACKSTOREHPD'. Intrinsic: '_mm512_mask_packstorehi_pd'.
// Requires KNCNI.
func MaskPackstorehiPd(mt uintptr, k Mmask8, v1 M512d)  {
	maskPackstorehiPd(uintptr(mt), uint8(k), [8]float64(v1))
}

func maskPackstorehiPd(mt uintptr, k uint8, v1 [8]float64) 


// PackstorehiPd: Stores packed double-precision (64-bit) floating-point
// elements of 'v1' into a quadword stream at a logically mapped starting
// address (mt-64), storing the high-64-byte elements of that stream (those
// elemetns of the stream that map at or after the first 64-byte-aligned
// address following (m5-64)). 
//
//		storeOffset := 0
//		foundNext64BytesBoundary := false
//		addr = mt-64
//		FOR j := 0 to 7
//			IF foundNext64BytesBoundary == false
//				IF ((addr + (storeOffset + 1)*8) % 64) == 0
//					foundNext64BytesBoundary = true
//				FI
//			ELSE
//				i := j*64
//				MEM[addr + storeOffset*4] := v1[i+63:i]
//			FI
//			storeOffset := storeOffset + 1
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPACKSTOREHPD'. Intrinsic: '_mm512_packstorehi_pd'.
// Requires KNCNI.
func PackstorehiPd(mt uintptr, v1 M512d)  {
	packstorehiPd(uintptr(mt), [8]float64(v1))
}

func packstorehiPd(mt uintptr, v1 [8]float64) 


// MaskPackstorehiPs: Stores packed single-precision (32-bit) floating-point
// elements of 'v1' into a doubleword stream at a logically mapped starting
// address (mt-64), storing the high-64-byte elements of that stream (those
// elemetns of the stream that map at or after the first 64-byte-aligned
// address following (m5-64)). Elements are loaded from memory according to
// element selector 'k' (elements are skipped when the corresponding mask bit
// is not set). 
//
//		storeOffset := 0
//		foundNext64BytesBoundary := false
//		addr = mt-64
//		FOR j := 0 to 15
//			IF k[j]
//				IF foundNext64BytesBoundary == false
//					IF ((addr + (storeOffset + 1)*4) % 64) == 0
//						foundNext64BytesBoundary = true
//					FI
//				ELSE
//					i := j*32
//					MEM[addr + storeOffset*4] := v1[i+31:i]
//				FI
//				storeOffset := storeOffset + 1
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPACKSTOREHPS'. Intrinsic: '_mm512_mask_packstorehi_ps'.
// Requires KNCNI.
func MaskPackstorehiPs(mt uintptr, k Mmask16, v1 M512)  {
	maskPackstorehiPs(uintptr(mt), uint16(k), [16]float32(v1))
}

func maskPackstorehiPs(mt uintptr, k uint16, v1 [16]float32) 


// PackstorehiPs: Stores packed single-precision (32-bit) floating-point
// elements of 'v1' into a doubleword stream at a logically mapped starting
// address (mt-64), storing the high-64-byte elements of that stream (those
// elemetns of the stream that map at or after the first 64-byte-aligned
// address following (m5-64)). 
//
//		storeOffset := 0
//		foundNext64BytesBoundary := false
//		addr = mt-64
//		FOR j := 0 to 15
//			IF foundNext64BytesBoundary == false
//				IF ((addr + (storeOffset + 1)*4) % 64) == 0
//					foundNext64BytesBoundary = true
//				FI
//			ELSE
//				i := j*32
//				MEM[addr + storeOffset*4] := v1[i+31:i]
//			FI
//			storeOffset := storeOffset + 1
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPACKSTOREHPS'. Intrinsic: '_mm512_packstorehi_ps'.
// Requires KNCNI.
func PackstorehiPs(mt uintptr, v1 M512)  {
	packstorehiPs(uintptr(mt), [16]float32(v1))
}

func packstorehiPs(mt uintptr, v1 [16]float32) 


// MaskPackstoreloEpi32: Stores packed 32-bit integer elements of 'v1' into a
// doubleword stream at a logically mapped starting address 'mt', storing the
// low-64-byte elements of that stream (those elements of the stream that map
// before the first 64-byte-aligned address follwing 'mt'). Elements are loaded
// from memory according to element selector 'k' (elements are skipped when the
// corresponding mask bit is not set). 
//
//		storeOffset := 0
//		addr = mt
//		FOR j := 0 to 15
//			IF k[j]
//				i := j*32
//				MEM[addr + storeOffset*4] := v1[i+31:i]
//				storeOffset := storeOffset + 1
//				IF ((addr + storeOffset*4) % 64) == 0
//					BREAK
//				FI
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPACKSTORELD'. Intrinsic: '_mm512_mask_packstorelo_epi32'.
// Requires KNCNI.
func MaskPackstoreloEpi32(mt uintptr, k Mmask16, v1 M512i)  {
	maskPackstoreloEpi32(uintptr(mt), uint16(k), [64]byte(v1))
}

func maskPackstoreloEpi32(mt uintptr, k uint16, v1 [64]byte) 


// PackstoreloEpi32: Stores packed 32-bit integer elements of 'v1' into a
// doubleword stream at a logically mapped starting address 'mt', storing the
// low-64-byte elements of that stream (those elements of the stream that map
// before the first 64-byte-aligned address follwing 'mt'). 
//
//		storeOffset := 0
//		addr = mt
//		FOR j := 0 to 15
//			i := j*32
//			MEM[addr + storeOffset*4] := v1[i+31:i]
//			storeOffset := storeOffset + 1
//			IF ((addr + storeOffset*4) % 64) == 0
//				BREAK
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPACKSTORELD'. Intrinsic: '_mm512_packstorelo_epi32'.
// Requires KNCNI.
func PackstoreloEpi32(mt uintptr, v1 M512i)  {
	packstoreloEpi32(uintptr(mt), [64]byte(v1))
}

func packstoreloEpi32(mt uintptr, v1 [64]byte) 


// MaskPackstoreloEpi64: Stores packed 64-bit integer elements of 'v1' into a
// quadword stream at a logically mapped starting address 'mt', storing the
// low-64-byte elements of that stream (those elements of the stream that map
// before the first 64-byte-aligned address follwing 'mt'). Elements are loaded
// from memory according to element selector 'k' (elements are skipped when the
// corresponding mask bit is not set). 
//
//		storeOffset := 0
//		addr = mt
//		FOR j := 0 to 7
//			IF k[j]
//				i := j*64
//				MEM[addr + storeOffset*8] := v1[i+63:i]
//				storeOffset := storeOffset + 1
//				IF ((addr + storeOffset*8) % 64) == 0
//					BREAK
//				FI
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPACKSTORELQ'. Intrinsic: '_mm512_mask_packstorelo_epi64'.
// Requires KNCNI.
func MaskPackstoreloEpi64(mt uintptr, k Mmask8, v1 M512i)  {
	maskPackstoreloEpi64(uintptr(mt), uint8(k), [64]byte(v1))
}

func maskPackstoreloEpi64(mt uintptr, k uint8, v1 [64]byte) 


// PackstoreloEpi64: Stores packed 64-bit integer elements of 'v1' into a
// quadword stream at a logically mapped starting address 'mt', storing the
// low-64-byte elements of that stream (those elements of the stream that map
// before the first 64-byte-aligned address follwing 'mt'). 
//
//		storeOffset := 0
//		addr = mt
//		FOR j := 0 to 7
//			i := j*64
//			MEM[addr + storeOffset*8] := v1[i+63:i]
//			storeOffset := storeOffset + 1
//			IF ((addr + storeOffset*8) % 64) == 0
//				BREAK
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPACKSTORELQ'. Intrinsic: '_mm512_packstorelo_epi64'.
// Requires KNCNI.
func PackstoreloEpi64(mt uintptr, v1 M512i)  {
	packstoreloEpi64(uintptr(mt), [64]byte(v1))
}

func packstoreloEpi64(mt uintptr, v1 [64]byte) 


// MaskPackstoreloPd: Stores packed double-precision (64-bit) floating-point
// elements of 'v1' into a quadword stream at a logically mapped starting
// address 'mt', storing the low-64-byte elements of that stream (those
// elements of the stream that map before the first 64-byte-aligned address
// follwing 'mt'). Elements are loaded from memory according to element
// selector 'k' (elements are skipped when the corresponding mask bit is not
// set). 
//
//		storeOffset := 0
//		addr = mt
//		FOR j := 0 to 7
//			IF k[j]
//				i := j*64
//				MEM[addr + storeOffset*8] := v1[i+63:i]
//				storeOffset := storeOffset + 1
//				IF ((addr + storeOffset*8) % 64) == 0
//					BREAK
//				FI
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPACKSTORELPD'. Intrinsic: '_mm512_mask_packstorelo_pd'.
// Requires KNCNI.
func MaskPackstoreloPd(mt uintptr, k Mmask8, v1 M512d)  {
	maskPackstoreloPd(uintptr(mt), uint8(k), [8]float64(v1))
}

func maskPackstoreloPd(mt uintptr, k uint8, v1 [8]float64) 


// PackstoreloPd: Stores packed double-precision (64-bit) floating-point
// elements of 'v1' into a quadword stream at a logically mapped starting
// address 'mt', storing the low-64-byte elements of that stream (those
// elements of the stream that map before the first 64-byte-aligned address
// follwing 'mt'). 
//
//		storeOffset := 0
//		addr = mt
//		FOR j := 0 to 7
//			i := j*64
//			MEM[addr + storeOffset*8] := v1[i+63:i]
//			storeOffset := storeOffset + 1
//			IF ((addr + storeOffset*8) % 64) == 0
//				BREAK
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPACKSTORELPD'. Intrinsic: '_mm512_packstorelo_pd'.
// Requires KNCNI.
func PackstoreloPd(mt uintptr, v1 M512d)  {
	packstoreloPd(uintptr(mt), [8]float64(v1))
}

func packstoreloPd(mt uintptr, v1 [8]float64) 


// MaskPackstoreloPs: Stores packed single-precision (32-bit) floating-point
// elements of 'v1' into a doubleword stream at a logically mapped starting
// address 'mt', storing the low-64-byte elements of that stream (those
// elements of the stream that map before the first 64-byte-aligned address
// follwing 'mt'). Elements are loaded from memory according to element
// selector 'k' (elements are skipped when the corresponding mask bit is not
// set). 
//
//		storeOffset := 0
//		addr = mt
//		FOR j := 0 to 15
//			IF k[j]
//				i := j*32
//				MEM[addr + storeOffset*4] := v1[i+31:i]
//				storeOffset := storeOffset + 1
//				IF ((addr + storeOffset*4) % 64) == 0
//					BREAK
//				FI
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPACKSTORELPS'. Intrinsic: '_mm512_mask_packstorelo_ps'.
// Requires KNCNI.
func MaskPackstoreloPs(mt uintptr, k Mmask16, v1 M512)  {
	maskPackstoreloPs(uintptr(mt), uint16(k), [16]float32(v1))
}

func maskPackstoreloPs(mt uintptr, k uint16, v1 [16]float32) 


// PackstoreloPs: Stores packed single-precision (32-bit) floating-point
// elements of 'v1' into a doubleword stream at a logically mapped starting
// address 'mt', storing the low-64-byte elements of that stream (those
// elements of the stream that map before the first 64-byte-aligned address
// follwing 'mt'). 
//
//		storeOffset := 0
//		addr = mt
//		FOR j := 0 to 15
//			i := j*32
//			MEM[addr + storeOffset*4] := v1[i+31:i]
//			storeOffset := storeOffset + 1
//			IF ((addr + storeOffset*4) % 64) == 0
//				BREAK
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPACKSTORELPS'. Intrinsic: '_mm512_packstorelo_ps'.
// Requires KNCNI.
func PackstoreloPs(mt uintptr, v1 M512)  {
	packstoreloPs(uintptr(mt), [16]float32(v1))
}

func packstoreloPs(mt uintptr, v1 [16]float32) 


// MaskPackusEpi16: Convert packed 16-bit integers from 'a' and 'b' to packed
// 8-bit integers using unsigned saturation, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		tmp_dst[7:0] := Saturate_Int16_To_UnsignedInt8 (a[15:0])
//		tmp_dst[15:8] := Saturate_Int16_To_UnsignedInt8 (a[31:16])
//		tmp_dst[23:16] := Saturate_Int16_To_UnsignedInt8 (a[47:32])
//		tmp_dst[31:24] := Saturate_Int16_To_UnsignedInt8 (a[63:48])
//		tmp_dst[39:32] := Saturate_Int16_To_UnsignedInt8 (a[79:64])
//		tmp_dst[47:40] := Saturate_Int16_To_UnsignedInt8 (a[95:80])
//		tmp_dst[55:48] := Saturate_Int16_To_UnsignedInt8 (a[111:96])
//		tmp_dst[63:56] := Saturate_Int16_To_UnsignedInt8 (a[127:112])
//		tmp_dst[71:64] := Saturate_Int16_To_UnsignedInt8 (b[15:0])
//		tmp_dst[79:72] := Saturate_Int16_To_UnsignedInt8 (b[31:16])
//		tmp_dst[87:80] := Saturate_Int16_To_UnsignedInt8 (b[47:32])
//		tmp_dst[95:88] := Saturate_Int16_To_UnsignedInt8 (b[63:48])
//		tmp_dst[103:96] := Saturate_Int16_To_UnsignedInt8 (b[79:64])
//		tmp_dst[111:104] := Saturate_Int16_To_UnsignedInt8 (b[95:80])
//		tmp_dst[119:112] := Saturate_Int16_To_UnsignedInt8 (b[111:96])
//		tmp_dst[127:120] := Saturate_Int16_To_UnsignedInt8 (b[127:112])
//		tmp_dst[135:128] := Saturate_Int16_To_UnsignedInt8 (a[143:128])
//		tmp_dst[143:136] := Saturate_Int16_To_UnsignedInt8 (a[159:144])
//		tmp_dst[151:144] := Saturate_Int16_To_UnsignedInt8 (a[175:160])
//		tmp_dst[159:152] := Saturate_Int16_To_UnsignedInt8 (a[191:176])
//		tmp_dst[167:160] := Saturate_Int16_To_UnsignedInt8 (a[207:192])
//		tmp_dst[175:168] := Saturate_Int16_To_UnsignedInt8 (a[223:208])
//		tmp_dst[183:176] := Saturate_Int16_To_UnsignedInt8 (a[239:224])
//		tmp_dst[191:184] := Saturate_Int16_To_UnsignedInt8 (a[255:240])
//		tmp_dst[199:192] := Saturate_Int16_To_UnsignedInt8 (b[143:128])
//		tmp_dst[207:200] := Saturate_Int16_To_UnsignedInt8 (b[159:144])
//		tmp_dst[215:208] := Saturate_Int16_To_UnsignedInt8 (b[175:160])
//		tmp_dst[223:216] := Saturate_Int16_To_UnsignedInt8 (b[191:176])
//		tmp_dst[231:224] := Saturate_Int16_To_UnsignedInt8 (b[207:192])
//		tmp_dst[239:232] := Saturate_Int16_To_UnsignedInt8 (b[223:208])
//		tmp_dst[247:240] := Saturate_Int16_To_UnsignedInt8 (b[239:224])
//		tmp_dst[255:248] := Saturate_Int16_To_UnsignedInt8 (b[255:240])
//		tmp_dst[263:256] := Saturate_Int16_To_UnsignedInt8 (a[271:256])
//		tmp_dst[271:264] := Saturate_Int16_To_UnsignedInt8 (a[287:272])
//		tmp_dst[279:272] := Saturate_Int16_To_UnsignedInt8 (a[303:288])
//		tmp_dst[287:280] := Saturate_Int16_To_UnsignedInt8 (a[319:304])
//		tmp_dst[295:288] := Saturate_Int16_To_UnsignedInt8 (a[335:320])
//		tmp_dst[303:296] := Saturate_Int16_To_UnsignedInt8 (a[351:336])
//		tmp_dst[311:304] := Saturate_Int16_To_UnsignedInt8 (a[367:352])
//		tmp_dst[319:312] := Saturate_Int16_To_UnsignedInt8 (a[383:368])
//		tmp_dst[327:320] := Saturate_Int16_To_UnsignedInt8 (b[271:256])
//		tmp_dst[335:328] := Saturate_Int16_To_UnsignedInt8 (b[287:272])
//		tmp_dst[343:336] := Saturate_Int16_To_UnsignedInt8 (b[303:288])
//		tmp_dst[351:344] := Saturate_Int16_To_UnsignedInt8 (b[319:304])
//		tmp_dst[359:352] := Saturate_Int16_To_UnsignedInt8 (b[335:320])
//		tmp_dst[367:360] := Saturate_Int16_To_UnsignedInt8 (b[351:336])
//		tmp_dst[375:368] := Saturate_Int16_To_UnsignedInt8 (b[367:352])
//		tmp_dst[383:376] := Saturate_Int16_To_UnsignedInt8 (b[383:368])
//		tmp_dst[391:384] := Saturate_Int16_To_UnsignedInt8 (a[399:384])
//		tmp_dst[399:392] := Saturate_Int16_To_UnsignedInt8 (a[415:400])
//		tmp_dst[407:400] := Saturate_Int16_To_UnsignedInt8 (a[431:416])
//		tmp_dst[415:408] := Saturate_Int16_To_UnsignedInt8 (a[447:432])
//		tmp_dst[423:416] := Saturate_Int16_To_UnsignedInt8 (a[463:448])
//		tmp_dst[431:424] := Saturate_Int16_To_UnsignedInt8 (a[479:464])
//		tmp_dst[439:432] := Saturate_Int16_To_UnsignedInt8 (a[495:480])
//		tmp_dst[447:440] := Saturate_Int16_To_UnsignedInt8 (a[511:496])
//		tmp_dst[455:448] := Saturate_Int16_To_UnsignedInt8 (b[399:384])
//		tmp_dst[463:456] := Saturate_Int16_To_UnsignedInt8 (b[415:400])
//		tmp_dst[471:464] := Saturate_Int16_To_UnsignedInt8 (b[431:416])
//		tmp_dst[479:472] := Saturate_Int16_To_UnsignedInt8 (b[447:432])
//		tmp_dst[487:480] := Saturate_Int16_To_UnsignedInt8 (b[463:448])
//		tmp_dst[495:488] := Saturate_Int16_To_UnsignedInt8 (b[479:464])
//		tmp_dst[503:496] := Saturate_Int16_To_UnsignedInt8 (b[495:480])
//		tmp_dst[511:504] := Saturate_Int16_To_UnsignedInt8 (b[511:496])
//		
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := tmp_dst[i+7:i]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPACKUSWB'. Intrinsic: '_mm512_mask_packus_epi16'.
// Requires AVX512BW.
func MaskPackusEpi16(src M512i, k Mmask64, a M512i, b M512i) M512i {
	return M512i(maskPackusEpi16([64]byte(src), uint64(k), [64]byte(a), [64]byte(b)))
}

func maskPackusEpi16(src [64]byte, k uint64, a [64]byte, b [64]byte) [64]byte


// MaskzPackusEpi16: Convert packed 16-bit integers from 'a' and 'b' to packed
// 8-bit integers using unsigned saturation, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		tmp_dst[7:0] := Saturate_Int16_To_UnsignedInt8 (a[15:0])
//		tmp_dst[15:8] := Saturate_Int16_To_UnsignedInt8 (a[31:16])
//		tmp_dst[23:16] := Saturate_Int16_To_UnsignedInt8 (a[47:32])
//		tmp_dst[31:24] := Saturate_Int16_To_UnsignedInt8 (a[63:48])
//		tmp_dst[39:32] := Saturate_Int16_To_UnsignedInt8 (a[79:64])
//		tmp_dst[47:40] := Saturate_Int16_To_UnsignedInt8 (a[95:80])
//		tmp_dst[55:48] := Saturate_Int16_To_UnsignedInt8 (a[111:96])
//		tmp_dst[63:56] := Saturate_Int16_To_UnsignedInt8 (a[127:112])
//		tmp_dst[71:64] := Saturate_Int16_To_UnsignedInt8 (b[15:0])
//		tmp_dst[79:72] := Saturate_Int16_To_UnsignedInt8 (b[31:16])
//		tmp_dst[87:80] := Saturate_Int16_To_UnsignedInt8 (b[47:32])
//		tmp_dst[95:88] := Saturate_Int16_To_UnsignedInt8 (b[63:48])
//		tmp_dst[103:96] := Saturate_Int16_To_UnsignedInt8 (b[79:64])
//		tmp_dst[111:104] := Saturate_Int16_To_UnsignedInt8 (b[95:80])
//		tmp_dst[119:112] := Saturate_Int16_To_UnsignedInt8 (b[111:96])
//		tmp_dst[127:120] := Saturate_Int16_To_UnsignedInt8 (b[127:112])
//		tmp_dst[135:128] := Saturate_Int16_To_UnsignedInt8 (a[143:128])
//		tmp_dst[143:136] := Saturate_Int16_To_UnsignedInt8 (a[159:144])
//		tmp_dst[151:144] := Saturate_Int16_To_UnsignedInt8 (a[175:160])
//		tmp_dst[159:152] := Saturate_Int16_To_UnsignedInt8 (a[191:176])
//		tmp_dst[167:160] := Saturate_Int16_To_UnsignedInt8 (a[207:192])
//		tmp_dst[175:168] := Saturate_Int16_To_UnsignedInt8 (a[223:208])
//		tmp_dst[183:176] := Saturate_Int16_To_UnsignedInt8 (a[239:224])
//		tmp_dst[191:184] := Saturate_Int16_To_UnsignedInt8 (a[255:240])
//		tmp_dst[199:192] := Saturate_Int16_To_UnsignedInt8 (b[143:128])
//		tmp_dst[207:200] := Saturate_Int16_To_UnsignedInt8 (b[159:144])
//		tmp_dst[215:208] := Saturate_Int16_To_UnsignedInt8 (b[175:160])
//		tmp_dst[223:216] := Saturate_Int16_To_UnsignedInt8 (b[191:176])
//		tmp_dst[231:224] := Saturate_Int16_To_UnsignedInt8 (b[207:192])
//		tmp_dst[239:232] := Saturate_Int16_To_UnsignedInt8 (b[223:208])
//		tmp_dst[247:240] := Saturate_Int16_To_UnsignedInt8 (b[239:224])
//		tmp_dst[255:248] := Saturate_Int16_To_UnsignedInt8 (b[255:240])
//		tmp_dst[263:256] := Saturate_Int16_To_UnsignedInt8 (a[271:256])
//		tmp_dst[271:264] := Saturate_Int16_To_UnsignedInt8 (a[287:272])
//		tmp_dst[279:272] := Saturate_Int16_To_UnsignedInt8 (a[303:288])
//		tmp_dst[287:280] := Saturate_Int16_To_UnsignedInt8 (a[319:304])
//		tmp_dst[295:288] := Saturate_Int16_To_UnsignedInt8 (a[335:320])
//		tmp_dst[303:296] := Saturate_Int16_To_UnsignedInt8 (a[351:336])
//		tmp_dst[311:304] := Saturate_Int16_To_UnsignedInt8 (a[367:352])
//		tmp_dst[319:312] := Saturate_Int16_To_UnsignedInt8 (a[383:368])
//		tmp_dst[327:320] := Saturate_Int16_To_UnsignedInt8 (b[271:256])
//		tmp_dst[335:328] := Saturate_Int16_To_UnsignedInt8 (b[287:272])
//		tmp_dst[343:336] := Saturate_Int16_To_UnsignedInt8 (b[303:288])
//		tmp_dst[351:344] := Saturate_Int16_To_UnsignedInt8 (b[319:304])
//		tmp_dst[359:352] := Saturate_Int16_To_UnsignedInt8 (b[335:320])
//		tmp_dst[367:360] := Saturate_Int16_To_UnsignedInt8 (b[351:336])
//		tmp_dst[375:368] := Saturate_Int16_To_UnsignedInt8 (b[367:352])
//		tmp_dst[383:376] := Saturate_Int16_To_UnsignedInt8 (b[383:368])
//		tmp_dst[391:384] := Saturate_Int16_To_UnsignedInt8 (a[399:384])
//		tmp_dst[399:392] := Saturate_Int16_To_UnsignedInt8 (a[415:400])
//		tmp_dst[407:400] := Saturate_Int16_To_UnsignedInt8 (a[431:416])
//		tmp_dst[415:408] := Saturate_Int16_To_UnsignedInt8 (a[447:432])
//		tmp_dst[423:416] := Saturate_Int16_To_UnsignedInt8 (a[463:448])
//		tmp_dst[431:424] := Saturate_Int16_To_UnsignedInt8 (a[479:464])
//		tmp_dst[439:432] := Saturate_Int16_To_UnsignedInt8 (a[495:480])
//		tmp_dst[447:440] := Saturate_Int16_To_UnsignedInt8 (a[511:496])
//		tmp_dst[455:448] := Saturate_Int16_To_UnsignedInt8 (b[399:384])
//		tmp_dst[463:456] := Saturate_Int16_To_UnsignedInt8 (b[415:400])
//		tmp_dst[471:464] := Saturate_Int16_To_UnsignedInt8 (b[431:416])
//		tmp_dst[479:472] := Saturate_Int16_To_UnsignedInt8 (b[447:432])
//		tmp_dst[487:480] := Saturate_Int16_To_UnsignedInt8 (b[463:448])
//		tmp_dst[495:488] := Saturate_Int16_To_UnsignedInt8 (b[479:464])
//		tmp_dst[503:496] := Saturate_Int16_To_UnsignedInt8 (b[495:480])
//		tmp_dst[511:504] := Saturate_Int16_To_UnsignedInt8 (b[511:496])
//		
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := tmp_dst[i+7:i]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPACKUSWB'. Intrinsic: '_mm512_maskz_packus_epi16'.
// Requires AVX512BW.
func MaskzPackusEpi16(k Mmask64, a M512i, b M512i) M512i {
	return M512i(maskzPackusEpi16(uint64(k), [64]byte(a), [64]byte(b)))
}

func maskzPackusEpi16(k uint64, a [64]byte, b [64]byte) [64]byte


// PackusEpi16: Convert packed 16-bit integers from 'a' and 'b' to packed 8-bit
// integers using unsigned saturation, and store the results in 'dst'. 
//
//		dst[7:0] := Saturate_Int16_To_UnsignedInt8 (a[15:0])
//		dst[15:8] := Saturate_Int16_To_UnsignedInt8 (a[31:16])
//		dst[23:16] := Saturate_Int16_To_UnsignedInt8 (a[47:32])
//		dst[31:24] := Saturate_Int16_To_UnsignedInt8 (a[63:48])
//		dst[39:32] := Saturate_Int16_To_UnsignedInt8 (a[79:64])
//		dst[47:40] := Saturate_Int16_To_UnsignedInt8 (a[95:80])
//		dst[55:48] := Saturate_Int16_To_UnsignedInt8 (a[111:96])
//		dst[63:56] := Saturate_Int16_To_UnsignedInt8 (a[127:112])
//		dst[71:64] := Saturate_Int16_To_UnsignedInt8 (b[15:0])
//		dst[79:72] := Saturate_Int16_To_UnsignedInt8 (b[31:16])
//		dst[87:80] := Saturate_Int16_To_UnsignedInt8 (b[47:32])
//		dst[95:88] := Saturate_Int16_To_UnsignedInt8 (b[63:48])
//		dst[103:96] := Saturate_Int16_To_UnsignedInt8 (b[79:64])
//		dst[111:104] := Saturate_Int16_To_UnsignedInt8 (b[95:80])
//		dst[119:112] := Saturate_Int16_To_UnsignedInt8 (b[111:96])
//		dst[127:120] := Saturate_Int16_To_UnsignedInt8 (b[127:112])
//		dst[135:128] := Saturate_Int16_To_UnsignedInt8 (a[143:128])
//		dst[143:136] := Saturate_Int16_To_UnsignedInt8 (a[159:144])
//		dst[151:144] := Saturate_Int16_To_UnsignedInt8 (a[175:160])
//		dst[159:152] := Saturate_Int16_To_UnsignedInt8 (a[191:176])
//		dst[167:160] := Saturate_Int16_To_UnsignedInt8 (a[207:192])
//		dst[175:168] := Saturate_Int16_To_UnsignedInt8 (a[223:208])
//		dst[183:176] := Saturate_Int16_To_UnsignedInt8 (a[239:224])
//		dst[191:184] := Saturate_Int16_To_UnsignedInt8 (a[255:240])
//		dst[199:192] := Saturate_Int16_To_UnsignedInt8 (b[143:128])
//		dst[207:200] := Saturate_Int16_To_UnsignedInt8 (b[159:144])
//		dst[215:208] := Saturate_Int16_To_UnsignedInt8 (b[175:160])
//		dst[223:216] := Saturate_Int16_To_UnsignedInt8 (b[191:176])
//		dst[231:224] := Saturate_Int16_To_UnsignedInt8 (b[207:192])
//		dst[239:232] := Saturate_Int16_To_UnsignedInt8 (b[223:208])
//		dst[247:240] := Saturate_Int16_To_UnsignedInt8 (b[239:224])
//		dst[255:248] := Saturate_Int16_To_UnsignedInt8 (b[255:240])
//		dst[263:256] := Saturate_Int16_To_UnsignedInt8 (a[271:256])
//		dst[271:264] := Saturate_Int16_To_UnsignedInt8 (a[287:272])
//		dst[279:272] := Saturate_Int16_To_UnsignedInt8 (a[303:288])
//		dst[287:280] := Saturate_Int16_To_UnsignedInt8 (a[319:304])
//		dst[295:288] := Saturate_Int16_To_UnsignedInt8 (a[335:320])
//		dst[303:296] := Saturate_Int16_To_UnsignedInt8 (a[351:336])
//		dst[311:304] := Saturate_Int16_To_UnsignedInt8 (a[367:352])
//		dst[319:312] := Saturate_Int16_To_UnsignedInt8 (a[383:368])
//		dst[327:320] := Saturate_Int16_To_UnsignedInt8 (b[271:256])
//		dst[335:328] := Saturate_Int16_To_UnsignedInt8 (b[287:272])
//		dst[343:336] := Saturate_Int16_To_UnsignedInt8 (b[303:288])
//		dst[351:344] := Saturate_Int16_To_UnsignedInt8 (b[319:304])
//		dst[359:352] := Saturate_Int16_To_UnsignedInt8 (b[335:320])
//		dst[367:360] := Saturate_Int16_To_UnsignedInt8 (b[351:336])
//		dst[375:368] := Saturate_Int16_To_UnsignedInt8 (b[367:352])
//		dst[383:376] := Saturate_Int16_To_UnsignedInt8 (b[383:368])
//		dst[391:384] := Saturate_Int16_To_UnsignedInt8 (a[399:384])
//		dst[399:392] := Saturate_Int16_To_UnsignedInt8 (a[415:400])
//		dst[407:400] := Saturate_Int16_To_UnsignedInt8 (a[431:416])
//		dst[415:408] := Saturate_Int16_To_UnsignedInt8 (a[447:432])
//		dst[423:416] := Saturate_Int16_To_UnsignedInt8 (a[463:448])
//		dst[431:424] := Saturate_Int16_To_UnsignedInt8 (a[479:464])
//		dst[439:432] := Saturate_Int16_To_UnsignedInt8 (a[495:480])
//		dst[447:440] := Saturate_Int16_To_UnsignedInt8 (a[511:496])
//		dst[455:448] := Saturate_Int16_To_UnsignedInt8 (b[399:384])
//		dst[463:456] := Saturate_Int16_To_UnsignedInt8 (b[415:400])
//		dst[471:464] := Saturate_Int16_To_UnsignedInt8 (b[431:416])
//		dst[479:472] := Saturate_Int16_To_UnsignedInt8 (b[447:432])
//		dst[487:480] := Saturate_Int16_To_UnsignedInt8 (b[463:448])
//		dst[495:488] := Saturate_Int16_To_UnsignedInt8 (b[479:464])
//		dst[503:496] := Saturate_Int16_To_UnsignedInt8 (b[495:480])
//		dst[511:504] := Saturate_Int16_To_UnsignedInt8 (b[511:496])
//		dst[MAX:512] := 0
//
// Instruction: 'VPACKUSWB'. Intrinsic: '_mm512_packus_epi16'.
// Requires AVX512BW.
func PackusEpi16(a M512i, b M512i) M512i {
	return M512i(packusEpi16([64]byte(a), [64]byte(b)))
}

func packusEpi16(a [64]byte, b [64]byte) [64]byte


// MaskPackusEpi32: Convert packed 32-bit integers from 'a' and 'b' to packed
// 16-bit integers using unsigned saturation, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		tmp_dst[15:0] := Saturate_Int32_To_UnsignedInt16 (a[31:0])
//		tmp_dst[31:16] := Saturate_Int32_To_UnsignedInt16 (a[63:32])
//		tmp_dst[47:32] := Saturate_Int32_To_UnsignedInt16 (a[95:64])
//		tmp_dst[63:48] := Saturate_Int32_To_UnsignedInt16 (a[127:96])
//		tmp_dst[79:64] := Saturate_Int32_To_UnsignedInt16 (b[31:0])
//		tmp_dst[95:80] := Saturate_Int32_To_UnsignedInt16 (b[63:32])
//		tmp_dst[111:96] := Saturate_Int32_To_UnsignedInt16 (b[95:64])
//		tmp_dst[127:112] := Saturate_Int32_To_UnsignedInt16 (b[127:96])
//		tmp_dst[143:128] := Saturate_Int32_To_UnsignedInt16 (a[159:128])
//		tmp_dst[159:144] := Saturate_Int32_To_UnsignedInt16 (a[191:160])
//		tmp_dst[175:160] := Saturate_Int32_To_UnsignedInt16 (a[223:192])
//		tmp_dst[191:176] := Saturate_Int32_To_UnsignedInt16 (a[255:224])
//		tmp_dst[207:192] := Saturate_Int32_To_UnsignedInt16 (b[159:128])
//		tmp_dst[223:208] := Saturate_Int32_To_UnsignedInt16 (b[191:160])
//		tmp_dst[239:224] := Saturate_Int32_To_UnsignedInt16 (b[223:192])
//		tmp_dst[255:240] := Saturate_Int32_To_UnsignedInt16 (b[255:224])
//		tmp_dst[271:256] := Saturate_Int32_To_UnsignedInt16 (a[287:256])
//		tmp_dst[287:272] := Saturate_Int32_To_UnsignedInt16 (a[319:288])
//		tmp_dst[303:288] := Saturate_Int32_To_UnsignedInt16 (a[351:320])
//		tmp_dst[319:304] := Saturate_Int32_To_UnsignedInt16 (a[383:352])
//		tmp_dst[335:320] := Saturate_Int32_To_UnsignedInt16 (b[287:256])
//		tmp_dst[351:336] := Saturate_Int32_To_UnsignedInt16 (b[319:288])
//		tmp_dst[367:352] := Saturate_Int32_To_UnsignedInt16 (b[351:320])
//		tmp_dst[383:368] := Saturate_Int32_To_UnsignedInt16 (b[383:352])
//		tmp_dst[399:384] := Saturate_Int32_To_UnsignedInt16 (a[415:384])
//		tmp_dst[415:400] := Saturate_Int32_To_UnsignedInt16 (a[447:416])
//		tmp_dst[431:416] := Saturate_Int32_To_UnsignedInt16 (a[479:448])
//		tmp_dst[447:432] := Saturate_Int32_To_UnsignedInt16 (a[511:480])
//		tmp_dst[463:448] := Saturate_Int32_To_UnsignedInt16 (b[415:384])
//		tmp_dst[479:464] := Saturate_Int32_To_UnsignedInt16 (b[447:416])
//		tmp_dst[495:480] := Saturate_Int32_To_UnsignedInt16 (b[479:448])
//		tmp_dst[511:496] := Saturate_Int32_To_UnsignedInt16 (b[511:480])
//		
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPACKUSDW'. Intrinsic: '_mm512_mask_packus_epi32'.
// Requires AVX512BW.
func MaskPackusEpi32(src M512i, k Mmask32, a M512i, b M512i) M512i {
	return M512i(maskPackusEpi32([64]byte(src), uint32(k), [64]byte(a), [64]byte(b)))
}

func maskPackusEpi32(src [64]byte, k uint32, a [64]byte, b [64]byte) [64]byte


// MaskzPackusEpi32: Convert packed 32-bit integers from 'a' and 'b' to packed
// 16-bit integers using unsigned saturation, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		tmp_dst[15:0] := Saturate_Int32_To_UnsignedInt16 (a[31:0])
//		tmp_dst[31:16] := Saturate_Int32_To_UnsignedInt16 (a[63:32])
//		tmp_dst[47:32] := Saturate_Int32_To_UnsignedInt16 (a[95:64])
//		tmp_dst[63:48] := Saturate_Int32_To_UnsignedInt16 (a[127:96])
//		tmp_dst[79:64] := Saturate_Int32_To_UnsignedInt16 (b[31:0])
//		tmp_dst[95:80] := Saturate_Int32_To_UnsignedInt16 (b[63:32])
//		tmp_dst[111:96] := Saturate_Int32_To_UnsignedInt16 (b[95:64])
//		tmp_dst[127:112] := Saturate_Int32_To_UnsignedInt16 (b[127:96])
//		tmp_dst[143:128] := Saturate_Int32_To_UnsignedInt16 (a[159:128])
//		tmp_dst[159:144] := Saturate_Int32_To_UnsignedInt16 (a[191:160])
//		tmp_dst[175:160] := Saturate_Int32_To_UnsignedInt16 (a[223:192])
//		tmp_dst[191:176] := Saturate_Int32_To_UnsignedInt16 (a[255:224])
//		tmp_dst[207:192] := Saturate_Int32_To_UnsignedInt16 (b[159:128])
//		tmp_dst[223:208] := Saturate_Int32_To_UnsignedInt16 (b[191:160])
//		tmp_dst[239:224] := Saturate_Int32_To_UnsignedInt16 (b[223:192])
//		tmp_dst[255:240] := Saturate_Int32_To_UnsignedInt16 (b[255:224])
//		tmp_dst[271:256] := Saturate_Int32_To_UnsignedInt16 (a[287:256])
//		tmp_dst[287:272] := Saturate_Int32_To_UnsignedInt16 (a[319:288])
//		tmp_dst[303:288] := Saturate_Int32_To_UnsignedInt16 (a[351:320])
//		tmp_dst[319:304] := Saturate_Int32_To_UnsignedInt16 (a[383:352])
//		tmp_dst[335:320] := Saturate_Int32_To_UnsignedInt16 (b[287:256])
//		tmp_dst[351:336] := Saturate_Int32_To_UnsignedInt16 (b[319:288])
//		tmp_dst[367:352] := Saturate_Int32_To_UnsignedInt16 (b[351:320])
//		tmp_dst[383:368] := Saturate_Int32_To_UnsignedInt16 (b[383:352])
//		tmp_dst[399:384] := Saturate_Int32_To_UnsignedInt16 (a[415:384])
//		tmp_dst[415:400] := Saturate_Int32_To_UnsignedInt16 (a[447:416])
//		tmp_dst[431:416] := Saturate_Int32_To_UnsignedInt16 (a[479:448])
//		tmp_dst[447:432] := Saturate_Int32_To_UnsignedInt16 (a[511:480])
//		tmp_dst[463:448] := Saturate_Int32_To_UnsignedInt16 (b[415:384])
//		tmp_dst[479:464] := Saturate_Int32_To_UnsignedInt16 (b[447:416])
//		tmp_dst[495:480] := Saturate_Int32_To_UnsignedInt16 (b[479:448])
//		tmp_dst[511:496] := Saturate_Int32_To_UnsignedInt16 (b[511:480])
//		
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPACKUSDW'. Intrinsic: '_mm512_maskz_packus_epi32'.
// Requires AVX512BW.
func MaskzPackusEpi32(k Mmask32, a M512i, b M512i) M512i {
	return M512i(maskzPackusEpi32(uint32(k), [64]byte(a), [64]byte(b)))
}

func maskzPackusEpi32(k uint32, a [64]byte, b [64]byte) [64]byte


// PackusEpi32: Convert packed 32-bit integers from 'a' and 'b' to packed
// 16-bit integers using unsigned saturation, and store the results in 'dst'. 
//
//		dst[15:0] := Saturate_Int32_To_UnsignedInt16 (a[31:0])
//		dst[31:16] := Saturate_Int32_To_UnsignedInt16 (a[63:32])
//		dst[47:32] := Saturate_Int32_To_UnsignedInt16 (a[95:64])
//		dst[63:48] := Saturate_Int32_To_UnsignedInt16 (a[127:96])
//		dst[79:64] := Saturate_Int32_To_UnsignedInt16 (b[31:0])
//		dst[95:80] := Saturate_Int32_To_UnsignedInt16 (b[63:32])
//		dst[111:96] := Saturate_Int32_To_UnsignedInt16 (b[95:64])
//		dst[127:112] := Saturate_Int32_To_UnsignedInt16 (b[127:96])
//		dst[143:128] := Saturate_Int32_To_UnsignedInt16 (a[159:128])
//		dst[159:144] := Saturate_Int32_To_UnsignedInt16 (a[191:160])
//		dst[175:160] := Saturate_Int32_To_UnsignedInt16 (a[223:192])
//		dst[191:176] := Saturate_Int32_To_UnsignedInt16 (a[255:224])
//		dst[207:192] := Saturate_Int32_To_UnsignedInt16 (b[159:128])
//		dst[223:208] := Saturate_Int32_To_UnsignedInt16 (b[191:160])
//		dst[239:224] := Saturate_Int32_To_UnsignedInt16 (b[223:192])
//		dst[255:240] := Saturate_Int32_To_UnsignedInt16 (b[255:224])
//		dst[271:256] := Saturate_Int32_To_UnsignedInt16 (a[287:256])
//		dst[287:272] := Saturate_Int32_To_UnsignedInt16 (a[319:288])
//		dst[303:288] := Saturate_Int32_To_UnsignedInt16 (a[351:320])
//		dst[319:304] := Saturate_Int32_To_UnsignedInt16 (a[383:352])
//		dst[335:320] := Saturate_Int32_To_UnsignedInt16 (b[287:256])
//		dst[351:336] := Saturate_Int32_To_UnsignedInt16 (b[319:288])
//		dst[367:352] := Saturate_Int32_To_UnsignedInt16 (b[351:320])
//		dst[383:368] := Saturate_Int32_To_UnsignedInt16 (b[383:352])
//		dst[399:384] := Saturate_Int32_To_UnsignedInt16 (a[415:384])
//		dst[415:400] := Saturate_Int32_To_UnsignedInt16 (a[447:416])
//		dst[431:416] := Saturate_Int32_To_UnsignedInt16 (a[479:448])
//		dst[447:432] := Saturate_Int32_To_UnsignedInt16 (a[511:480])
//		dst[463:448] := Saturate_Int32_To_UnsignedInt16 (b[415:384])
//		dst[479:464] := Saturate_Int32_To_UnsignedInt16 (b[447:416])
//		dst[495:480] := Saturate_Int32_To_UnsignedInt16 (b[479:448])
//		dst[511:496] := Saturate_Int32_To_UnsignedInt16 (b[511:480])
//		dst[MAX:512] := 0
//
// Instruction: 'VPACKUSDW'. Intrinsic: '_mm512_packus_epi32'.
// Requires AVX512BW.
func PackusEpi32(a M512i, b M512i) M512i {
	return M512i(packusEpi32([64]byte(a), [64]byte(b)))
}

func packusEpi32(a [64]byte, b [64]byte) [64]byte


// MaskPermutePd: Shuffle double-precision (64-bit) floating-point elements in
// 'a' within 128-bit lanes using the control in 'imm8', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		IF (imm8[0] == 0) tmp_dst[63:0] := a[63:0]
//		IF (imm8[0] == 1) tmp_dst[63:0] := a[127:64]
//		IF (imm8[1] == 0) tmp_dst[127:64] := a[63:0]
//		IF (imm8[1] == 1) tmp_dst[127:64] := a[127:64]
//		IF (imm8[2] == 0) tmp_dst[191:128] := a[191:128]
//		IF (imm8[2] == 1) tmp_dst[191:128] := a[255:192]
//		IF (imm8[3] == 0) tmp_dst[255:192] := a[191:128]
//		IF (imm8[3] == 1) tmp_dst[255:192] := a[255:192]
//		IF (imm8[4] == 0) tmp_dst[319:256] := a[319:256]
//		IF (imm8[4] == 1) tmp_dst[319:256] := a[383:320]
//		IF (imm8[5] == 0) tmp_dst[383:320] := a[319:256]
//		IF (imm8[5] == 1) tmp_dst[383:320] := a[383:320]
//		IF (imm8[6] == 0) tmp_dst[447:384] := a[447:384]
//		IF (imm8[6] == 1) tmp_dst[447:384] := a[511:448]
//		IF (imm8[7] == 0) tmp_dst[511:448] := a[447:384]
//		IF (imm8[7] == 1) tmp_dst[511:448] := a[511:448]
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm512_mask_permute_pd'.
// Requires AVX512F.
func MaskPermutePd(src M512d, k Mmask8, a M512d, imm8 int) M512d {
	return M512d(maskPermutePd([8]float64(src), uint8(k), [8]float64(a), imm8))
}

func maskPermutePd(src [8]float64, k uint8, a [8]float64, imm8 int) [8]float64


// MaskzPermutePd: Shuffle double-precision (64-bit) floating-point elements in
// 'a' within 128-bit lanes using the control in 'imm8', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		IF (imm8[0] == 0) tmp_dst[63:0] := a[63:0]
//		IF (imm8[0] == 1) tmp_dst[63:0] := a[127:64]
//		IF (imm8[1] == 0) tmp_dst[127:64] := a[63:0]
//		IF (imm8[1] == 1) tmp_dst[127:64] := a[127:64]
//		IF (imm8[2] == 0) tmp_dst[191:128] := a[191:128]
//		IF (imm8[2] == 1) tmp_dst[191:128] := a[255:192]
//		IF (imm8[3] == 0) tmp_dst[255:192] := a[191:128]
//		IF (imm8[3] == 1) tmp_dst[255:192] := a[255:192]
//		IF (imm8[4] == 0) tmp_dst[319:256] := a[319:256]
//		IF (imm8[4] == 1) tmp_dst[319:256] := a[383:320]
//		IF (imm8[5] == 0) tmp_dst[383:320] := a[319:256]
//		IF (imm8[5] == 1) tmp_dst[383:320] := a[383:320]
//		IF (imm8[6] == 0) tmp_dst[447:384] := a[447:384]
//		IF (imm8[6] == 1) tmp_dst[447:384] := a[511:448]
//		IF (imm8[7] == 0) tmp_dst[511:448] := a[447:384]
//		IF (imm8[7] == 1) tmp_dst[511:448] := a[511:448]
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm512_maskz_permute_pd'.
// Requires AVX512F.
func MaskzPermutePd(k Mmask8, a M512d, imm8 int) M512d {
	return M512d(maskzPermutePd(uint8(k), [8]float64(a), imm8))
}

func maskzPermutePd(k uint8, a [8]float64, imm8 int) [8]float64


// PermutePd: Shuffle double-precision (64-bit) floating-point elements in 'a'
// within 128-bit lanes using the control in 'imm8', and store the results in
// 'dst'. 
//
//		IF (imm8[0] == 0) dst[63:0] := a[63:0]
//		IF (imm8[0] == 1) dst[63:0] := a[127:64]
//		IF (imm8[1] == 0) dst[127:64] := a[63:0]
//		IF (imm8[1] == 1) dst[127:64] := a[127:64]
//		IF (imm8[2] == 0) dst[191:128] := a[191:128]
//		IF (imm8[2] == 1) dst[191:128] := a[255:192]
//		IF (imm8[3] == 0) dst[255:192] := a[191:128]
//		IF (imm8[3] == 1) dst[255:192] := a[255:192]
//		IF (imm8[4] == 0) dst[319:256] := a[319:256]
//		IF (imm8[4] == 1) dst[319:256] := a[383:320]
//		IF (imm8[5] == 0) dst[383:320] := a[319:256]
//		IF (imm8[5] == 1) dst[383:320] := a[383:320]
//		IF (imm8[6] == 0) dst[447:384] := a[447:384]
//		IF (imm8[6] == 1) dst[447:384] := a[511:448]
//		IF (imm8[7] == 0) dst[511:448] := a[447:384]
//		IF (imm8[7] == 1) dst[511:448] := a[511:448]
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm512_permute_pd'.
// Requires AVX512F.
func PermutePd(a M512d, imm8 int) M512d {
	return M512d(permutePd([8]float64(a), imm8))
}

func permutePd(a [8]float64, imm8 int) [8]float64


// MaskPermutePs: Shuffle single-precision (32-bit) floating-point elements in
// 'a' within 128-bit lanes using the control in 'imm8', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		tmp_dst[223:192] := SELECT4(a[255:128], imm8[5:4])
//		tmp_dst[255:224] := SELECT4(a[255:128], imm8[7:6])
//		tmp_dst[287:256] := SELECT4(a[383:256], imm8[1:0])
//		tmp_dst[319:288] := SELECT4(a[383:256], imm8[3:2])
//		tmp_dst[351:320] := SELECT4(a[383:256], imm8[5:4])
//		tmp_dst[383:352] := SELECT4(a[383:256], imm8[7:6])
//		tmp_dst[415:384] := SELECT4(a[511:384], imm8[1:0])
//		tmp_dst[447:416] := SELECT4(a[511:384], imm8[3:2])
//		tmp_dst[479:448] := SELECT4(a[511:384], imm8[5:4])
//		tmp_dst[511:480] := SELECT4(a[511:384], imm8[7:6])
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm512_mask_permute_ps'.
// Requires AVX512F.
func MaskPermutePs(src M512, k Mmask16, a M512, imm8 int) M512 {
	return M512(maskPermutePs([16]float32(src), uint16(k), [16]float32(a), imm8))
}

func maskPermutePs(src [16]float32, k uint16, a [16]float32, imm8 int) [16]float32


// MaskzPermutePs: Shuffle single-precision (32-bit) floating-point elements in
// 'a' within 128-bit lanes using the control in 'imm8', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		tmp_dst[223:192] := SELECT4(a[255:128], imm8[5:4])
//		tmp_dst[255:224] := SELECT4(a[255:128], imm8[7:6])
//		tmp_dst[287:256] := SELECT4(a[383:256], imm8[1:0])
//		tmp_dst[319:288] := SELECT4(a[383:256], imm8[3:2])
//		tmp_dst[351:320] := SELECT4(a[383:256], imm8[5:4])
//		tmp_dst[383:352] := SELECT4(a[383:256], imm8[7:6])
//		tmp_dst[415:384] := SELECT4(a[511:384], imm8[1:0])
//		tmp_dst[447:416] := SELECT4(a[511:384], imm8[3:2])
//		tmp_dst[479:448] := SELECT4(a[511:384], imm8[5:4])
//		tmp_dst[511:480] := SELECT4(a[511:384], imm8[7:6])
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm512_maskz_permute_ps'.
// Requires AVX512F.
func MaskzPermutePs(k Mmask16, a M512, imm8 int) M512 {
	return M512(maskzPermutePs(uint16(k), [16]float32(a), imm8))
}

func maskzPermutePs(k uint16, a [16]float32, imm8 int) [16]float32


// PermutePs: Shuffle single-precision (32-bit) floating-point elements in 'a'
// within 128-bit lanes using the control in 'imm8', and store the results in
// 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		dst[223:192] := SELECT4(a[255:128], imm8[5:4])
//		dst[255:224] := SELECT4(a[255:128], imm8[7:6])
//		dst[287:256] := SELECT4(a[383:256], imm8[1:0])
//		dst[319:288] := SELECT4(a[383:256], imm8[3:2])
//		dst[351:320] := SELECT4(a[383:256], imm8[5:4])
//		dst[383:352] := SELECT4(a[383:256], imm8[7:6])
//		dst[415:384] := SELECT4(a[511:384], imm8[1:0])
//		dst[447:416] := SELECT4(a[511:384], imm8[3:2])
//		dst[479:448] := SELECT4(a[511:384], imm8[5:4])
//		dst[511:480] := SELECT4(a[511:384], imm8[7:6])
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm512_permute_ps'.
// Requires AVX512F.
func PermutePs(a M512, imm8 int) M512 {
	return M512(permutePs([16]float32(a), imm8))
}

func permutePs(a [16]float32, imm8 int) [16]float32


// MaskPermute4f128Epi32: Permutes 128-bit blocks of the packed 32-bit integer
// vector 'a' using constant 'imm8'. The results are stored in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		SELECT4(src, control) {
//			CASE control[1:0] OF
//			0: tmp[127:0] := src[127:0]
//			1: tmp[127:0] := src[255:128]
//			2: tmp[127:0] := src[383:256]
//			3: tmp[127:0] := src[511:384]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp[511:0] := 0
//		FOR j := 0 to 4
//			i := j*128
//			n := j*2
//			tmp[i+127:i] := SELECT4(a[511:0], imm8[n+1:n])
//		ENDFOR
//		FOR j := 0 to 15
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMF32X4'. Intrinsic: '_mm512_mask_permute4f128_epi32'.
// Requires KNCNI.
func MaskPermute4f128Epi32(src M512i, k Mmask16, a M512i, imm8 MMPERMENUM) M512i {
	return M512i(maskPermute4f128Epi32([64]byte(src), uint16(k), [64]byte(a), imm8))
}

func maskPermute4f128Epi32(src [64]byte, k uint16, a [64]byte, imm8 MMPERMENUM) [64]byte


// Permute4f128Epi32: Permutes 128-bit blocks of the packed 32-bit integer
// vector 'a' using constant 'imm8'. The results are stored in 'dst'. 
//
//		SELECT4(src, control) {
//			CASE control[1:0] OF
//			0: tmp[127:0] := src[127:0]
//			1: tmp[127:0] := src[255:128]
//			2: tmp[127:0] := src[383:256]
//			3: tmp[127:0] := src[511:384]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*128
//			n := j*2
//			dst[i+127:i] := SELECT4(a[511:0], imm8[n+1:n])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMF32X4'. Intrinsic: '_mm512_permute4f128_epi32'.
// Requires KNCNI.
func Permute4f128Epi32(a M512i, imm8 MMPERMENUM) M512i {
	return M512i(permute4f128Epi32([64]byte(a), imm8))
}

func permute4f128Epi32(a [64]byte, imm8 MMPERMENUM) [64]byte


// MaskPermute4f128Ps: Permutes 128-bit blocks of the packed single-precision
// (32-bit) floating-point elements in 'a' using constant 'imm8'. The results
// are stored in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		SELECT4(src, control) {
//			CASE control[1:0] OF
//			0: tmp[127:0] := src[127:0]
//			1: tmp[127:0] := src[255:128]
//			2: tmp[127:0] := src[383:256]
//			3: tmp[127:0] := src[511:384]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp[511:0] := 0
//		FOR j := 0 to 4
//			i := j*128
//			n := j*2
//			tmp[i+127:i] := SELECT4(a[511:0], imm8[n+1:n])
//		ENDFOR
//		FOR j := 0 to 15
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMF32X4'. Intrinsic: '_mm512_mask_permute4f128_ps'.
// Requires KNCNI.
func MaskPermute4f128Ps(src M512, k Mmask16, a M512, imm8 MMPERMENUM) M512 {
	return M512(maskPermute4f128Ps([16]float32(src), uint16(k), [16]float32(a), imm8))
}

func maskPermute4f128Ps(src [16]float32, k uint16, a [16]float32, imm8 MMPERMENUM) [16]float32


// Permute4f128Ps: Permutes 128-bit blocks of the packed single-precision
// (32-bit) floating-point elements in 'a' using constant 'imm8'. The results
// are stored in 'dst'. 
//
//		SELECT4(src, control) {
//			CASE control[1:0] OF
//			0: tmp[127:0] := src[127:0]
//			1: tmp[127:0] := src[255:128]
//			2: tmp[127:0] := src[383:256]
//			3: tmp[127:0] := src[511:384]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*128
//			n := j*2
//			dst[i+127:i] := SELECT4(a[511:0], imm8[n+1:n])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMF32X4'. Intrinsic: '_mm512_permute4f128_ps'.
// Requires KNCNI.
func Permute4f128Ps(a M512, imm8 MMPERMENUM) M512 {
	return M512(permute4f128Ps([16]float32(a), imm8))
}

func permute4f128Ps(a [16]float32, imm8 MMPERMENUM) [16]float32


// MaskPermutevarEpi32: Shuffle 32-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). Note that this intrinsic shuffles across 128-bit lanes, unlike past
// intrinsics that use the 'permutevar' name. This intrinsic is identical to
// '_mm512_mask_permutexvar_epi32', and it is recommended that you use that
// intrinsic name. 
//
//		FOR j := 0 to 15
//			i := j*32
//			id := idx[i+3:i]*32
//			IF k[j]
//				dst[i+31:i] := a[id+31:id]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMD'. Intrinsic: '_mm512_mask_permutevar_epi32'.
// Requires KNCNI.
func MaskPermutevarEpi32(src M512i, k Mmask16, idx M512i, a M512i) M512i {
	return M512i(maskPermutevarEpi32([64]byte(src), uint16(k), [64]byte(idx), [64]byte(a)))
}

func maskPermutevarEpi32(src [64]byte, k uint16, idx [64]byte, a [64]byte) [64]byte


// PermutevarEpi32: Shuffle 32-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst'. Note that this
// intrinsic shuffles across 128-bit lanes, unlike past intrinsics that use the
// 'permutevar' name. This intrinsic is identical to
// '_mm512_permutexvar_epi32', and it is recommended that you use that
// intrinsic name. 
//
//		FOR j := 0 to 15
//			i := j*32
//			id := idx[i+3:i]*32
//			dst[i+31:i] := a[id+31:id]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMD'. Intrinsic: '_mm512_permutevar_epi32'.
// Requires KNCNI.
func PermutevarEpi32(idx M512i, a M512i) M512i {
	return M512i(permutevarEpi32([64]byte(idx), [64]byte(a)))
}

func permutevarEpi32(idx [64]byte, a [64]byte) [64]byte


// MaskPermutevarPd: Shuffle double-precision (64-bit) floating-point elements
// in 'a' within 128-bit lanes using the control in 'b', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		IF (b[1] == 0) tmp_dst[63:0] := a[63:0]
//		IF (b[1] == 1) tmp_dst[63:0] := a[127:64]
//		IF (b[65] == 0) tmp_dst[127:64] := a[63:0]
//		IF (b[65] == 1) tmp_dst[127:64] := a[127:64]
//		IF (b[129] == 0) tmp_dst[191:128] := a[191:128]
//		IF (b[129] == 1) tmp_dst[191:128] := a[255:192]
//		IF (b[193] == 0) tmp_dst[255:192] := a[191:128]
//		IF (b[193] == 1) tmp_dst[255:192] := a[255:192]
//		IF (b[257] == 0) tmp_dst[319:256] := a[319:256]
//		IF (b[257] == 1) tmp_dst[319:256] := a[383:320]
//		IF (b[321] == 0) tmp_dst[383:320] := a[319:256]
//		IF (b[321] == 1) tmp_dst[383:320] := a[383:320]
//		IF (b[385] == 0) tmp_dst[447:384] := a[447:384]
//		IF (b[385] == 1) tmp_dst[447:384] := a[511:448]
//		IF (b[449] == 0) tmp_dst[511:448] := a[447:384]
//		IF (b[449] == 1) tmp_dst[511:448] := a[511:448]
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm512_mask_permutevar_pd'.
// Requires AVX512F.
func MaskPermutevarPd(src M512d, k Mmask8, a M512d, b M512i) M512d {
	return M512d(maskPermutevarPd([8]float64(src), uint8(k), [8]float64(a), [64]byte(b)))
}

func maskPermutevarPd(src [8]float64, k uint8, a [8]float64, b [64]byte) [8]float64


// MaskzPermutevarPd: Shuffle double-precision (64-bit) floating-point elements
// in 'a' within 128-bit lanes using the control in 'b', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		IF (b[1] == 0) tmp_dst[63:0] := a[63:0]
//		IF (b[1] == 1) tmp_dst[63:0] := a[127:64]
//		IF (b[65] == 0) tmp_dst[127:64] := a[63:0]
//		IF (b[65] == 1) tmp_dst[127:64] := a[127:64]
//		IF (b[129] == 0) tmp_dst[191:128] := a[191:128]
//		IF (b[129] == 1) tmp_dst[191:128] := a[255:192]
//		IF (b[193] == 0) tmp_dst[255:192] := a[191:128]
//		IF (b[193] == 1) tmp_dst[255:192] := a[255:192]
//		IF (b[257] == 0) tmp_dst[319:256] := a[319:256]
//		IF (b[257] == 1) tmp_dst[319:256] := a[383:320]
//		IF (b[321] == 0) tmp_dst[383:320] := a[319:256]
//		IF (b[321] == 1) tmp_dst[383:320] := a[383:320]
//		IF (b[385] == 0) tmp_dst[447:384] := a[447:384]
//		IF (b[385] == 1) tmp_dst[447:384] := a[511:448]
//		IF (b[449] == 0) tmp_dst[511:448] := a[447:384]
//		IF (b[449] == 1) tmp_dst[511:448] := a[511:448]
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm512_maskz_permutevar_pd'.
// Requires AVX512F.
func MaskzPermutevarPd(k Mmask8, a M512d, b M512i) M512d {
	return M512d(maskzPermutevarPd(uint8(k), [8]float64(a), [64]byte(b)))
}

func maskzPermutevarPd(k uint8, a [8]float64, b [64]byte) [8]float64


// PermutevarPd: Shuffle double-precision (64-bit) floating-point elements in
// 'a' within 128-bit lanes using the control in 'b', and store the results in
// 'dst'. 
//
//		IF (b[1] == 0) dst[63:0] := a[63:0]
//		IF (b[1] == 1) dst[63:0] := a[127:64]
//		IF (b[65] == 0) dst[127:64] := a[63:0]
//		IF (b[65] == 1) dst[127:64] := a[127:64]
//		IF (b[129] == 0) dst[191:128] := a[191:128]
//		IF (b[129] == 1) dst[191:128] := a[255:192]
//		IF (b[193] == 0) dst[255:192] := a[191:128]
//		IF (b[193] == 1) dst[255:192] := a[255:192]
//		IF (b[257] == 0) dst[319:256] := a[319:256]
//		IF (b[257] == 1) dst[319:256] := a[383:320]
//		IF (b[321] == 0) dst[383:320] := a[319:256]
//		IF (b[321] == 1) dst[383:320] := a[383:320]
//		IF (b[385] == 0) dst[447:384] := a[447:384]
//		IF (b[385] == 1) dst[447:384] := a[511:448]
//		IF (b[449] == 0) dst[511:448] := a[447:384]
//		IF (b[449] == 1) dst[511:448] := a[511:448]
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm512_permutevar_pd'.
// Requires AVX512F.
func PermutevarPd(a M512d, b M512i) M512d {
	return M512d(permutevarPd([8]float64(a), [64]byte(b)))
}

func permutevarPd(a [8]float64, b [64]byte) [8]float64


// MaskPermutevarPs: Shuffle single-precision (32-bit) floating-point elements
// in 'a' within 128-bit lanes using the control in 'b', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], b[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], b[33:32])
//		tmp_dst[95:64] := SELECT4(a[127:0], b[65:64])
//		tmp_dst[127:96] := SELECT4(a[127:0], b[97:96])
//		tmp_dst[159:128] := SELECT4(a[255:128], b[129:128])
//		tmp_dst[191:160] := SELECT4(a[255:128], b[161:160])
//		tmp_dst[223:192] := SELECT4(a[255:128], b[193:192])
//		tmp_dst[255:224] := SELECT4(a[255:128], b[225:224])
//		tmp_dst[287:256] := SELECT4(a[383:256], b[257:256])
//		tmp_dst[319:288] := SELECT4(a[383:256], b[289:288])
//		tmp_dst[351:320] := SELECT4(a[383:256], b[321:320])
//		tmp_dst[383:352] := SELECT4(a[383:256], b[353:352])
//		tmp_dst[415:384] := SELECT4(a[511:384], b[385:384])
//		tmp_dst[447:416] := SELECT4(a[511:384], b[417:416])
//		tmp_dst[479:448] := SELECT4(a[511:384], b[449:448])
//		tmp_dst[511:480] := SELECT4(a[511:384], b[481:480])
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm512_mask_permutevar_ps'.
// Requires AVX512F.
func MaskPermutevarPs(src M512, k Mmask16, a M512, b M512i) M512 {
	return M512(maskPermutevarPs([16]float32(src), uint16(k), [16]float32(a), [64]byte(b)))
}

func maskPermutevarPs(src [16]float32, k uint16, a [16]float32, b [64]byte) [16]float32


// MaskzPermutevarPs: Shuffle single-precision (32-bit) floating-point elements
// in 'a' within 128-bit lanes using the control in 'b', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], b[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], b[33:32])
//		tmp_dst[95:64] := SELECT4(a[127:0], b[65:64])
//		tmp_dst[127:96] := SELECT4(a[127:0], b[97:96])
//		tmp_dst[159:128] := SELECT4(a[255:128], b[129:128])
//		tmp_dst[191:160] := SELECT4(a[255:128], b[161:160])
//		tmp_dst[223:192] := SELECT4(a[255:128], b[193:192])
//		tmp_dst[255:224] := SELECT4(a[255:128], b[225:224])
//		tmp_dst[287:256] := SELECT4(a[383:256], b[257:256])
//		tmp_dst[319:288] := SELECT4(a[383:256], b[289:288])
//		tmp_dst[351:320] := SELECT4(a[383:256], b[321:320])
//		tmp_dst[383:352] := SELECT4(a[383:256], b[353:352])
//		tmp_dst[415:384] := SELECT4(a[511:384], b[385:384])
//		tmp_dst[447:416] := SELECT4(a[511:384], b[417:416])
//		tmp_dst[479:448] := SELECT4(a[511:384], b[449:448])
//		tmp_dst[511:480] := SELECT4(a[511:384], b[481:480])
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm512_maskz_permutevar_ps'.
// Requires AVX512F.
func MaskzPermutevarPs(k Mmask16, a M512, b M512i) M512 {
	return M512(maskzPermutevarPs(uint16(k), [16]float32(a), [64]byte(b)))
}

func maskzPermutevarPs(k uint16, a [16]float32, b [64]byte) [16]float32


// PermutevarPs: Shuffle single-precision (32-bit) floating-point elements in
// 'a' within 128-bit lanes using the control in 'b', and store the results in
// 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		dst[31:0] := SELECT4(a[127:0], b[1:0])
//		dst[63:32] := SELECT4(a[127:0], b[33:32])
//		dst[95:64] := SELECT4(a[127:0], b[65:64])
//		dst[127:96] := SELECT4(a[127:0], b[97:96])
//		dst[159:128] := SELECT4(a[255:128], b[129:128])
//		dst[191:160] := SELECT4(a[255:128], b[161:160])
//		dst[223:192] := SELECT4(a[255:128], b[193:192])
//		dst[255:224] := SELECT4(a[255:128], b[225:224])
//		dst[287:256] := SELECT4(a[383:256], b[257:256])
//		dst[319:288] := SELECT4(a[383:256], b[289:288])
//		dst[351:320] := SELECT4(a[383:256], b[321:320])
//		dst[383:352] := SELECT4(a[383:256], b[353:352])
//		dst[415:384] := SELECT4(a[511:384], b[385:384])
//		dst[447:416] := SELECT4(a[511:384], b[417:416])
//		dst[479:448] := SELECT4(a[511:384], b[449:448])
//		dst[511:480] := SELECT4(a[511:384], b[481:480])
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm512_permutevar_ps'.
// Requires AVX512F.
func PermutevarPs(a M512, b M512i) M512 {
	return M512(permutevarPs([16]float32(a), [64]byte(b)))
}

func permutevarPs(a [16]float32, b [64]byte) [16]float32


// MaskPermutexEpi64: Shuffle 64-bit integers in 'a' within 256-bit lanes using
// the control in 'imm8', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[63:0] := src[63:0]
//			1:	tmp[63:0] := src[127:64]
//			2:	tmp[63:0] := src[191:128]
//			3:	tmp[63:0] := src[255:192]
//			ESAC
//			RETURN tmp[63:0]
//		}
//		
//		tmp_dst[63:0] := SELECT4(a[255:0], imm8[1:0])
//		tmp_dst[127:64] := SELECT4(a[255:0], imm8[3:2])
//		tmp_dst[191:128] := SELECT4(a[255:0], imm8[5:4])
//		tmp_dst[255:192] := SELECT4(a[255:0], imm8[7:6])
//		tmp_dst[319:256] := SELECT4(a[511:256], imm8[1:0])
//		tmp_dst[383:320] := SELECT4(a[511:256], imm8[3:2])
//		tmp_dst[447:384] := SELECT4(a[511:256], imm8[5:4])
//		tmp_dst[511:448] := SELECT4(a[511:256], imm8[7:6])
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMQ'. Intrinsic: '_mm512_mask_permutex_epi64'.
// Requires AVX512F.
func MaskPermutexEpi64(src M512i, k Mmask8, a M512i, imm8 int) M512i {
	return M512i(maskPermutexEpi64([64]byte(src), uint8(k), [64]byte(a), imm8))
}

func maskPermutexEpi64(src [64]byte, k uint8, a [64]byte, imm8 int) [64]byte


// MaskzPermutexEpi64: Shuffle 64-bit integers in 'a' within 256-bit lanes
// using the control in 'imm8', and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[63:0] := src[63:0]
//			1:	tmp[63:0] := src[127:64]
//			2:	tmp[63:0] := src[191:128]
//			3:	tmp[63:0] := src[255:192]
//			ESAC
//			RETURN tmp[63:0]
//		}
//		
//		tmp_dst[63:0] := SELECT4(a[255:0], imm8[1:0])
//		tmp_dst[127:64] := SELECT4(a[255:0], imm8[3:2])
//		tmp_dst[191:128] := SELECT4(a[255:0], imm8[5:4])
//		tmp_dst[255:192] := SELECT4(a[255:0], imm8[7:6])
//		tmp_dst[319:256] := SELECT4(a[511:256], imm8[1:0])
//		tmp_dst[383:320] := SELECT4(a[511:256], imm8[3:2])
//		tmp_dst[447:384] := SELECT4(a[511:256], imm8[5:4])
//		tmp_dst[511:448] := SELECT4(a[511:256], imm8[7:6])
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMQ'. Intrinsic: '_mm512_maskz_permutex_epi64'.
// Requires AVX512F.
func MaskzPermutexEpi64(k Mmask8, a M512i, imm8 int) M512i {
	return M512i(maskzPermutexEpi64(uint8(k), [64]byte(a), imm8))
}

func maskzPermutexEpi64(k uint8, a [64]byte, imm8 int) [64]byte


// PermutexEpi64: Shuffle 64-bit integers in 'a' within 256-bit lanes using the
// control in 'imm8', and store the results in 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[63:0] := src[63:0]
//			1:	tmp[63:0] := src[127:64]
//			2:	tmp[63:0] := src[191:128]
//			3:	tmp[63:0] := src[255:192]
//			ESAC
//			RETURN tmp[63:0]
//		}
//		
//		dst[63:0] := SELECT4(a[255:0], imm8[1:0])
//		dst[127:64] := SELECT4(a[255:0], imm8[3:2])
//		dst[191:128] := SELECT4(a[255:0], imm8[5:4])
//		dst[255:192] := SELECT4(a[255:0], imm8[7:6])
//		dst[319:256] := SELECT4(a[511:256], imm8[1:0])
//		dst[383:320] := SELECT4(a[511:256], imm8[3:2])
//		dst[447:384] := SELECT4(a[511:256], imm8[5:4])
//		dst[511:448] := SELECT4(a[511:256], imm8[7:6])
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMQ'. Intrinsic: '_mm512_permutex_epi64'.
// Requires AVX512F.
func PermutexEpi64(a M512i, imm8 int) M512i {
	return M512i(permutexEpi64([64]byte(a), imm8))
}

func permutexEpi64(a [64]byte, imm8 int) [64]byte


// MaskPermutexPd: Shuffle double-precision (64-bit) floating-point elements in
// 'a' within 256-bit lanes using the control in 'imm8', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[63:0] := src[63:0]
//			1:	tmp[63:0] := src[127:64]
//			2:	tmp[63:0] := src[191:128]
//			3:	tmp[63:0] := src[255:192]
//			ESAC
//			RETURN tmp[63:0]
//		}
//		
//		tmp_dst[63:0] := SELECT4(a[255:0], imm8[1:0])
//		tmp_dst[127:64] := SELECT4(a[255:0], imm8[3:2])
//		tmp_dst[191:128] := SELECT4(a[255:0], imm8[5:4])
//		tmp_dst[255:192] := SELECT4(a[255:0], imm8[7:6])
//		tmp_dst[319:256] := SELECT4(a[511:256], imm8[1:0])
//		tmp_dst[383:320] := SELECT4(a[511:256], imm8[3:2])
//		tmp_dst[447:384] := SELECT4(a[511:256], imm8[5:4])
//		tmp_dst[511:448] := SELECT4(a[511:256], imm8[7:6])
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMPD'. Intrinsic: '_mm512_mask_permutex_pd'.
// Requires AVX512F.
func MaskPermutexPd(src M512d, k Mmask8, a M512d, imm8 int) M512d {
	return M512d(maskPermutexPd([8]float64(src), uint8(k), [8]float64(a), imm8))
}

func maskPermutexPd(src [8]float64, k uint8, a [8]float64, imm8 int) [8]float64


// MaskzPermutexPd: Shuffle double-precision (64-bit) floating-point elements
// in 'a' within 256-bit lanes using the control in 'imm8', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[63:0] := src[63:0]
//			1:	tmp[63:0] := src[127:64]
//			2:	tmp[63:0] := src[191:128]
//			3:	tmp[63:0] := src[255:192]
//			ESAC
//			RETURN tmp[63:0]
//		}
//		
//		tmp_dst[63:0] := SELECT4(a[255:0], imm8[1:0])
//		tmp_dst[127:64] := SELECT4(a[255:0], imm8[3:2])
//		tmp_dst[191:128] := SELECT4(a[255:0], imm8[5:4])
//		tmp_dst[255:192] := SELECT4(a[255:0], imm8[7:6])
//		tmp_dst[319:256] := SELECT4(a[511:256], imm8[1:0])
//		tmp_dst[383:320] := SELECT4(a[511:256], imm8[3:2])
//		tmp_dst[447:384] := SELECT4(a[511:256], imm8[5:4])
//		tmp_dst[511:448] := SELECT4(a[511:256], imm8[7:6])
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMPD'. Intrinsic: '_mm512_maskz_permutex_pd'.
// Requires AVX512F.
func MaskzPermutexPd(k Mmask8, a M512d, imm8 int) M512d {
	return M512d(maskzPermutexPd(uint8(k), [8]float64(a), imm8))
}

func maskzPermutexPd(k uint8, a [8]float64, imm8 int) [8]float64


// PermutexPd: Shuffle double-precision (64-bit) floating-point elements in 'a'
// within 256-bit lanes using the control in 'imm8', and store the results in
// 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[63:0] := src[63:0]
//			1:	tmp[63:0] := src[127:64]
//			2:	tmp[63:0] := src[191:128]
//			3:	tmp[63:0] := src[255:192]
//			ESAC
//			RETURN tmp[63:0]
//		}
//		
//		dst[63:0] := SELECT4(a[255:0], imm8[1:0])
//		dst[127:64] := SELECT4(a[255:0], imm8[3:2])
//		dst[191:128] := SELECT4(a[255:0], imm8[5:4])
//		dst[255:192] := SELECT4(a[255:0], imm8[7:6])
//		dst[319:256] := SELECT4(a[511:256], imm8[1:0])
//		dst[383:320] := SELECT4(a[511:256], imm8[3:2])
//		dst[447:384] := SELECT4(a[511:256], imm8[5:4])
//		dst[511:448] := SELECT4(a[511:256], imm8[7:6])
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMPD'. Intrinsic: '_mm512_permutex_pd'.
// Requires AVX512F.
func PermutexPd(a M512d, imm8 int) M512d {
	return M512d(permutexPd([8]float64(a), imm8))
}

func permutexPd(a [8]float64, imm8 int) [8]float64


// MaskPermutex2varEpi16: Shuffle 16-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				off := 16*idx[i+4:i]
//				dst[i+15:i] := idx[i+5] ? b[off+15:off] : a[off+15:off]
//			ELSE
//				dst[i+15:i] := a[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMT2W'. Intrinsic: '_mm512_mask_permutex2var_epi16'.
// Requires AVX512BW.
func MaskPermutex2varEpi16(a M512i, k Mmask32, idx M512i, b M512i) M512i {
	return M512i(maskPermutex2varEpi16([64]byte(a), uint32(k), [64]byte(idx), [64]byte(b)))
}

func maskPermutex2varEpi16(a [64]byte, k uint32, idx [64]byte, b [64]byte) [64]byte


// Mask2Permutex2varEpi16: Shuffle 16-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'idx' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				off := 16*idx[i+4:i]
//				dst[i+15:i] := idx[i+5] ? b[off+15:off] : a[off+15:off]
//			ELSE
//				dst[i+15:i] := idx[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMI2W'. Intrinsic: '_mm512_mask2_permutex2var_epi16'.
// Requires AVX512BW.
func Mask2Permutex2varEpi16(a M512i, idx M512i, k Mmask32, b M512i) M512i {
	return M512i(mask2Permutex2varEpi16([64]byte(a), [64]byte(idx), uint32(k), [64]byte(b)))
}

func mask2Permutex2varEpi16(a [64]byte, idx [64]byte, k uint32, b [64]byte) [64]byte


// MaskzPermutex2varEpi16: Shuffle 16-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				off := 16*idx[i+4:i]
//				dst[i+15:i] := idx[i+5] ? b[off+15:off] : a[off+15:off]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMI2W, VPERMT2W'. Intrinsic: '_mm512_maskz_permutex2var_epi16'.
// Requires AVX512BW.
func MaskzPermutex2varEpi16(k Mmask32, a M512i, idx M512i, b M512i) M512i {
	return M512i(maskzPermutex2varEpi16(uint32(k), [64]byte(a), [64]byte(idx), [64]byte(b)))
}

func maskzPermutex2varEpi16(k uint32, a [64]byte, idx [64]byte, b [64]byte) [64]byte


// Permutex2varEpi16: Shuffle 16-bit integers in 'a' and 'b' across lanes using
// the corresponding selector and index in 'idx', and store the results in
// 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			off := 16*idx[i+4:i]
//			dst[i+15:i] := idx[i+5] ? b[off+15:off] : a[off+15:off]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMI2W, VPERMT2W'. Intrinsic: '_mm512_permutex2var_epi16'.
// Requires AVX512BW.
func Permutex2varEpi16(a M512i, idx M512i, b M512i) M512i {
	return M512i(permutex2varEpi16([64]byte(a), [64]byte(idx), [64]byte(b)))
}

func permutex2varEpi16(a [64]byte, idx [64]byte, b [64]byte) [64]byte


// MaskPermutex2varEpi32: Shuffle 32-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			off := idx[i+3:i]*32
//			IF k[j]
//				dst[i+31:i] := idx[i+4] ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMT2D'. Intrinsic: '_mm512_mask_permutex2var_epi32'.
// Requires AVX512F.
func MaskPermutex2varEpi32(a M512i, k Mmask16, idx M512i, b M512i) M512i {
	return M512i(maskPermutex2varEpi32([64]byte(a), uint16(k), [64]byte(idx), [64]byte(b)))
}

func maskPermutex2varEpi32(a [64]byte, k uint16, idx [64]byte, b [64]byte) [64]byte


// Mask2Permutex2varEpi32: Shuffle 32-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'idx' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			off := idx[i+3:i]*32
//			IF k[j]
//				dst[i+31:i] := idx[i+4] ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := idx[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMI2D'. Intrinsic: '_mm512_mask2_permutex2var_epi32'.
// Requires AVX512F.
func Mask2Permutex2varEpi32(a M512i, idx M512i, k Mmask16, b M512i) M512i {
	return M512i(mask2Permutex2varEpi32([64]byte(a), [64]byte(idx), uint16(k), [64]byte(b)))
}

func mask2Permutex2varEpi32(a [64]byte, idx [64]byte, k uint16, b [64]byte) [64]byte


// MaskzPermutex2varEpi32: Shuffle 32-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			off := idx[i+3:i]*32
//			IF k[j]
//				dst[i+31:i] := (idx[i+4]) ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMI2D, VPERMT2D'. Intrinsic: '_mm512_maskz_permutex2var_epi32'.
// Requires AVX512F.
func MaskzPermutex2varEpi32(k Mmask16, a M512i, idx M512i, b M512i) M512i {
	return M512i(maskzPermutex2varEpi32(uint16(k), [64]byte(a), [64]byte(idx), [64]byte(b)))
}

func maskzPermutex2varEpi32(k uint16, a [64]byte, idx [64]byte, b [64]byte) [64]byte


// Permutex2varEpi32: Shuffle 32-bit integers in 'a' and 'b' across lanes using
// the corresponding selector and index in 'idx', and store the results in
// 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			off := idx[i+3:i]*32
//			dst[i+31:i] := idx[i+4] ? b[off+31:off] : a[off+31:off]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMI2D, VPERMT2D'. Intrinsic: '_mm512_permutex2var_epi32'.
// Requires AVX512F.
func Permutex2varEpi32(a M512i, idx M512i, b M512i) M512i {
	return M512i(permutex2varEpi32([64]byte(a), [64]byte(idx), [64]byte(b)))
}

func permutex2varEpi32(a [64]byte, idx [64]byte, b [64]byte) [64]byte


// MaskPermutex2varEpi64: Shuffle 64-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			off := idx[i+2:i]*64
//			IF k[j]
//				dst[i+63:i] := idx[i+3] ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMT2Q'. Intrinsic: '_mm512_mask_permutex2var_epi64'.
// Requires AVX512F.
func MaskPermutex2varEpi64(a M512i, k Mmask8, idx M512i, b M512i) M512i {
	return M512i(maskPermutex2varEpi64([64]byte(a), uint8(k), [64]byte(idx), [64]byte(b)))
}

func maskPermutex2varEpi64(a [64]byte, k uint8, idx [64]byte, b [64]byte) [64]byte


// Mask2Permutex2varEpi64: Shuffle 64-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'idx' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			off := idx[i+2:i]*64
//			IF k[j]
//				dst[i+63:i] := idx[i+3] ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := idx[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMI2Q'. Intrinsic: '_mm512_mask2_permutex2var_epi64'.
// Requires AVX512F.
func Mask2Permutex2varEpi64(a M512i, idx M512i, k Mmask8, b M512i) M512i {
	return M512i(mask2Permutex2varEpi64([64]byte(a), [64]byte(idx), uint8(k), [64]byte(b)))
}

func mask2Permutex2varEpi64(a [64]byte, idx [64]byte, k uint8, b [64]byte) [64]byte


// MaskzPermutex2varEpi64: Shuffle 64-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			off := idx[i+2:i]*64
//			IF k[j]
//				dst[i+63:i] := (idx[i+3]) ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMI2Q, VPERMT2Q'. Intrinsic: '_mm512_maskz_permutex2var_epi64'.
// Requires AVX512F.
func MaskzPermutex2varEpi64(k Mmask8, a M512i, idx M512i, b M512i) M512i {
	return M512i(maskzPermutex2varEpi64(uint8(k), [64]byte(a), [64]byte(idx), [64]byte(b)))
}

func maskzPermutex2varEpi64(k uint8, a [64]byte, idx [64]byte, b [64]byte) [64]byte


// Permutex2varEpi64: Shuffle 64-bit integers in 'a' and 'b' across lanes using
// the corresponding selector and index in 'idx', and store the results in
// 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			off := idx[i+2:i]*64
//			dst[i+63:i] := idx[i+3] ? b[off+63:off] : a[off+63:off]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMI2Q, VPERMT2Q'. Intrinsic: '_mm512_permutex2var_epi64'.
// Requires AVX512F.
func Permutex2varEpi64(a M512i, idx M512i, b M512i) M512i {
	return M512i(permutex2varEpi64([64]byte(a), [64]byte(idx), [64]byte(b)))
}

func permutex2varEpi64(a [64]byte, idx [64]byte, b [64]byte) [64]byte


// MaskPermutex2varEpi8: Shuffle 8-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				off := 8*idx[i+5:i]
//				dst[i+7:i] := idx[i+6] ? b[off+7:off] : a[off+7:off]
//			ELSE
//				dst[i+7:i] := a[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMT2B'. Intrinsic: '_mm512_mask_permutex2var_epi8'.
// Requires AVX512VBMI.
func MaskPermutex2varEpi8(a M512i, k Mmask64, idx M512i, b M512i) M512i {
	return M512i(maskPermutex2varEpi8([64]byte(a), uint64(k), [64]byte(idx), [64]byte(b)))
}

func maskPermutex2varEpi8(a [64]byte, k uint64, idx [64]byte, b [64]byte) [64]byte


// Mask2Permutex2varEpi8: Shuffle 8-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				off := 8*idx[i+5:i]
//				dst[i+7:i] := idx[i+6] ? b[off+7:off] : a[off+7:off]
//			ELSE
//				dst[i+7:i] := a[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMI2B'. Intrinsic: '_mm512_mask2_permutex2var_epi8'.
// Requires AVX512VBMI.
func Mask2Permutex2varEpi8(a M512i, idx M512i, k Mmask64, b M512i) M512i {
	return M512i(mask2Permutex2varEpi8([64]byte(a), [64]byte(idx), uint64(k), [64]byte(b)))
}

func mask2Permutex2varEpi8(a [64]byte, idx [64]byte, k uint64, b [64]byte) [64]byte


// MaskzPermutex2varEpi8: Shuffle 8-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				off := 8*idx[i+5:i]
//				dst[i+7:i] := idx[i+6] ? b[off+7:off] : a[off+7:off]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMI2B, VPERMT2B'. Intrinsic: '_mm512_maskz_permutex2var_epi8'.
// Requires AVX512VBMI.
func MaskzPermutex2varEpi8(k Mmask64, a M512i, idx M512i, b M512i) M512i {
	return M512i(maskzPermutex2varEpi8(uint64(k), [64]byte(a), [64]byte(idx), [64]byte(b)))
}

func maskzPermutex2varEpi8(k uint64, a [64]byte, idx [64]byte, b [64]byte) [64]byte


// Permutex2varEpi8: Shuffle 8-bit integers in 'a' and 'b' across lanes using
// the corresponding selector and index in 'idx', and store the results in
// 'dst'. 
//
//		FOR j := 0 to 63
//			i := j*8
//			off := 8*idx[i+5:i]
//			dst[i+7:i] := idx[i+6] ? b[off+7:off] : a[off+7:off]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMI2B'. Intrinsic: '_mm512_permutex2var_epi8'.
// Requires AVX512VBMI.
func Permutex2varEpi8(a M512i, idx M512i, b M512i) M512i {
	return M512i(permutex2varEpi8([64]byte(a), [64]byte(idx), [64]byte(b)))
}

func permutex2varEpi8(a [64]byte, idx [64]byte, b [64]byte) [64]byte


// MaskPermutex2varPd: Shuffle double-precision (64-bit) floating-point
// elements in 'a' and 'b' across lanes using the corresponding selector and
// index in 'idx', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			off := idx[i+2:i]*64
//			IF k[j]
//				dst[i+63:i] := idx[i+3] ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMT2PD'. Intrinsic: '_mm512_mask_permutex2var_pd'.
// Requires AVX512F.
func MaskPermutex2varPd(a M512d, k Mmask8, idx M512i, b M512d) M512d {
	return M512d(maskPermutex2varPd([8]float64(a), uint8(k), [64]byte(idx), [8]float64(b)))
}

func maskPermutex2varPd(a [8]float64, k uint8, idx [64]byte, b [8]float64) [8]float64


// Mask2Permutex2varPd: Shuffle double-precision (64-bit) floating-point
// elements in 'a' and 'b' across lanes using the corresponding selector and
// index in 'idx', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'idx' when the corresponding mask bit is not set) 
//
//		FOR j := 0 to 7
//			i := j*64
//			off := idx[i+2:i]*64
//			IF k[j]
//				dst[i+63:i] := idx[i+3] ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := idx[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMI2PD'. Intrinsic: '_mm512_mask2_permutex2var_pd'.
// Requires AVX512F.
func Mask2Permutex2varPd(a M512d, idx M512i, k Mmask8, b M512d) M512d {
	return M512d(mask2Permutex2varPd([8]float64(a), [64]byte(idx), uint8(k), [8]float64(b)))
}

func mask2Permutex2varPd(a [8]float64, idx [64]byte, k uint8, b [8]float64) [8]float64


// MaskzPermutex2varPd: Shuffle double-precision (64-bit) floating-point
// elements in 'a' and 'b' across lanes using the corresponding selector and
// index in 'idx', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			off := idx[i+2:i]*64
//			IF k[j]
//				dst[i+63:i] := (idx[i+3]) ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMI2PD, VPERMT2PD'. Intrinsic: '_mm512_maskz_permutex2var_pd'.
// Requires AVX512F.
func MaskzPermutex2varPd(k Mmask8, a M512d, idx M512i, b M512d) M512d {
	return M512d(maskzPermutex2varPd(uint8(k), [8]float64(a), [64]byte(idx), [8]float64(b)))
}

func maskzPermutex2varPd(k uint8, a [8]float64, idx [64]byte, b [8]float64) [8]float64


// Permutex2varPd: Shuffle double-precision (64-bit) floating-point elements in
// 'a' and 'b' across lanes using the corresponding selector and index in
// 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			off := idx[i+2:i]*64
//			dst[i+63:i] := idx[i+3] ? b[off+63:off] : a[off+63:off]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMI2PD, VPERMT2PD'. Intrinsic: '_mm512_permutex2var_pd'.
// Requires AVX512F.
func Permutex2varPd(a M512d, idx M512i, b M512d) M512d {
	return M512d(permutex2varPd([8]float64(a), [64]byte(idx), [8]float64(b)))
}

func permutex2varPd(a [8]float64, idx [64]byte, b [8]float64) [8]float64


// MaskPermutex2varPs: Shuffle single-precision (32-bit) floating-point
// elements in 'a' and 'b' across lanes using the corresponding selector and
// index in 'idx', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			off := idx[i+3:i]*32
//			IF k[j]
//				dst[i+31:i] := idx[i+4] ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMT2PS'. Intrinsic: '_mm512_mask_permutex2var_ps'.
// Requires AVX512F.
func MaskPermutex2varPs(a M512, k Mmask16, idx M512i, b M512) M512 {
	return M512(maskPermutex2varPs([16]float32(a), uint16(k), [64]byte(idx), [16]float32(b)))
}

func maskPermutex2varPs(a [16]float32, k uint16, idx [64]byte, b [16]float32) [16]float32


// Mask2Permutex2varPs: Shuffle single-precision (32-bit) floating-point
// elements in 'a' and 'b' across lanes using the corresponding selector and
// index in 'idx', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'idx' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			off := idx[i+3:i]*32
//			IF k[j]
//				dst[i+31:i] := idx[i+4] ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := idx[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMI2PS'. Intrinsic: '_mm512_mask2_permutex2var_ps'.
// Requires AVX512F.
func Mask2Permutex2varPs(a M512, idx M512i, k Mmask16, b M512) M512 {
	return M512(mask2Permutex2varPs([16]float32(a), [64]byte(idx), uint16(k), [16]float32(b)))
}

func mask2Permutex2varPs(a [16]float32, idx [64]byte, k uint16, b [16]float32) [16]float32


// MaskzPermutex2varPs: Shuffle single-precision (32-bit) floating-point
// elements in 'a' and 'b' across lanes using the corresponding selector and
// index in 'idx', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			off := idx[i+3:i]*32
//			IF k[j]
//				dst[i+31:i] := (idx[i+4]) ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMI2PS, VPERMT2PS'. Intrinsic: '_mm512_maskz_permutex2var_ps'.
// Requires AVX512F.
func MaskzPermutex2varPs(k Mmask16, a M512, idx M512i, b M512) M512 {
	return M512(maskzPermutex2varPs(uint16(k), [16]float32(a), [64]byte(idx), [16]float32(b)))
}

func maskzPermutex2varPs(k uint16, a [16]float32, idx [64]byte, b [16]float32) [16]float32


// Permutex2varPs: Shuffle single-precision (32-bit) floating-point elements in
// 'a' and 'b' across lanes using the corresponding selector and index in
// 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			off := idx[i+3:i]*32
//			dst[i+31:i] := idx[i+4] ? b[off+31:off] : a[off+31:off]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMI2PS, VPERMT2PS'. Intrinsic: '_mm512_permutex2var_ps'.
// Requires AVX512F.
func Permutex2varPs(a M512, idx M512i, b M512) M512 {
	return M512(permutex2varPs([16]float32(a), [64]byte(idx), [16]float32(b)))
}

func permutex2varPs(a [16]float32, idx [64]byte, b [16]float32) [16]float32


// MaskPermutexvarEpi16: Shuffle 16-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			id := idx[i+4:i]*16
//			IF k[j]
//				dst[i+15:i] := a[id+15:id]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMW'. Intrinsic: '_mm512_mask_permutexvar_epi16'.
// Requires AVX512BW.
func MaskPermutexvarEpi16(src M512i, k Mmask32, idx M512i, a M512i) M512i {
	return M512i(maskPermutexvarEpi16([64]byte(src), uint32(k), [64]byte(idx), [64]byte(a)))
}

func maskPermutexvarEpi16(src [64]byte, k uint32, idx [64]byte, a [64]byte) [64]byte


// MaskzPermutexvarEpi16: Shuffle 16-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			id := idx[i+4:i]*16
//			IF k[j]
//				dst[i+15:i] := a[id+15:id]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMW'. Intrinsic: '_mm512_maskz_permutexvar_epi16'.
// Requires AVX512BW.
func MaskzPermutexvarEpi16(k Mmask32, idx M512i, a M512i) M512i {
	return M512i(maskzPermutexvarEpi16(uint32(k), [64]byte(idx), [64]byte(a)))
}

func maskzPermutexvarEpi16(k uint32, idx [64]byte, a [64]byte) [64]byte


// PermutexvarEpi16: Shuffle 16-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			id := idx[i+4:i]*16
//			dst[i+15:i] := a[id+15:id]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMW'. Intrinsic: '_mm512_permutexvar_epi16'.
// Requires AVX512BW.
func PermutexvarEpi16(idx M512i, a M512i) M512i {
	return M512i(permutexvarEpi16([64]byte(idx), [64]byte(a)))
}

func permutexvarEpi16(idx [64]byte, a [64]byte) [64]byte


// MaskPermutexvarEpi32: Shuffle 32-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			id := idx[i+3:i]*32
//			IF k[j]
//				dst[i+31:i] := a[id+31:id]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMD'. Intrinsic: '_mm512_mask_permutexvar_epi32'.
// Requires AVX512F.
func MaskPermutexvarEpi32(src M512i, k Mmask16, idx M512i, a M512i) M512i {
	return M512i(maskPermutexvarEpi32([64]byte(src), uint16(k), [64]byte(idx), [64]byte(a)))
}

func maskPermutexvarEpi32(src [64]byte, k uint16, idx [64]byte, a [64]byte) [64]byte


// MaskzPermutexvarEpi32: Shuffle 32-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			id := idx[i+3:i]*32
//			IF k[j]
//				dst[i+31:i] := a[id+31:id]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMD'. Intrinsic: '_mm512_maskz_permutexvar_epi32'.
// Requires AVX512F.
func MaskzPermutexvarEpi32(k Mmask16, idx M512i, a M512i) M512i {
	return M512i(maskzPermutexvarEpi32(uint16(k), [64]byte(idx), [64]byte(a)))
}

func maskzPermutexvarEpi32(k uint16, idx [64]byte, a [64]byte) [64]byte


// PermutexvarEpi32: Shuffle 32-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			id := idx[i+3:i]*32
//			dst[i+31:i] := a[id+31:id]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMD'. Intrinsic: '_mm512_permutexvar_epi32'.
// Requires AVX512F.
func PermutexvarEpi32(idx M512i, a M512i) M512i {
	return M512i(permutexvarEpi32([64]byte(idx), [64]byte(a)))
}

func permutexvarEpi32(idx [64]byte, a [64]byte) [64]byte


// MaskPermutexvarEpi64: Shuffle 64-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			id := idx[i+2:i]*64
//			IF k[j]
//				dst[i+63:i] := a[id+63:id]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMQ'. Intrinsic: '_mm512_mask_permutexvar_epi64'.
// Requires AVX512F.
func MaskPermutexvarEpi64(src M512i, k Mmask8, idx M512i, a M512i) M512i {
	return M512i(maskPermutexvarEpi64([64]byte(src), uint8(k), [64]byte(idx), [64]byte(a)))
}

func maskPermutexvarEpi64(src [64]byte, k uint8, idx [64]byte, a [64]byte) [64]byte


// MaskzPermutexvarEpi64: Shuffle 64-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			id := idx[i+2:i]*64
//			IF k[j]
//				dst[i+63:i] := a[id+63:id]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMQ'. Intrinsic: '_mm512_maskz_permutexvar_epi64'.
// Requires AVX512F.
func MaskzPermutexvarEpi64(k Mmask8, idx M512i, a M512i) M512i {
	return M512i(maskzPermutexvarEpi64(uint8(k), [64]byte(idx), [64]byte(a)))
}

func maskzPermutexvarEpi64(k uint8, idx [64]byte, a [64]byte) [64]byte


// PermutexvarEpi64: Shuffle 64-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			id := idx[i+2:i]*64
//			dst[i+63:i] := a[id+63:id]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMQ'. Intrinsic: '_mm512_permutexvar_epi64'.
// Requires AVX512F.
func PermutexvarEpi64(idx M512i, a M512i) M512i {
	return M512i(permutexvarEpi64([64]byte(idx), [64]byte(a)))
}

func permutexvarEpi64(idx [64]byte, a [64]byte) [64]byte


// MaskPermutexvarEpi8: Shuffle 8-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			id := idx[i+5:i]*8
//			IF k[j]
//				dst[i+7:i] := a[id+7:id]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMB'. Intrinsic: '_mm512_mask_permutexvar_epi8'.
// Requires AVX512VBMI.
func MaskPermutexvarEpi8(src M512i, k Mmask64, idx M512i, a M512i) M512i {
	return M512i(maskPermutexvarEpi8([64]byte(src), uint64(k), [64]byte(idx), [64]byte(a)))
}

func maskPermutexvarEpi8(src [64]byte, k uint64, idx [64]byte, a [64]byte) [64]byte


// MaskzPermutexvarEpi8: Shuffle 8-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			id := idx[i+5:i]*8
//			IF k[j]
//				dst[i+7:i] := a[id+7:id]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMB'. Intrinsic: '_mm512_maskz_permutexvar_epi8'.
// Requires AVX512VBMI.
func MaskzPermutexvarEpi8(k Mmask64, idx M512i, a M512i) M512i {
	return M512i(maskzPermutexvarEpi8(uint64(k), [64]byte(idx), [64]byte(a)))
}

func maskzPermutexvarEpi8(k uint64, idx [64]byte, a [64]byte) [64]byte


// PermutexvarEpi8: Shuffle 8-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 63
//			i := j*8
//			id := idx[i+5:i]*8
//			dst[i+7:i] := a[id+7:id]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMB'. Intrinsic: '_mm512_permutexvar_epi8'.
// Requires AVX512VBMI.
func PermutexvarEpi8(idx M512i, a M512i) M512i {
	return M512i(permutexvarEpi8([64]byte(idx), [64]byte(a)))
}

func permutexvarEpi8(idx [64]byte, a [64]byte) [64]byte


// MaskPermutexvarPd: Shuffle double-precision (64-bit) floating-point elements
// in 'a' across lanes using the corresponding index in 'idx', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			id := idx[i+2:i]*64
//			IF k[j]
//				dst[i+63:i] := a[id+63:id]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMPD'. Intrinsic: '_mm512_mask_permutexvar_pd'.
// Requires AVX512F.
func MaskPermutexvarPd(src M512d, k Mmask8, idx M512i, a M512d) M512d {
	return M512d(maskPermutexvarPd([8]float64(src), uint8(k), [64]byte(idx), [8]float64(a)))
}

func maskPermutexvarPd(src [8]float64, k uint8, idx [64]byte, a [8]float64) [8]float64


// MaskzPermutexvarPd: Shuffle double-precision (64-bit) floating-point
// elements in 'a' across lanes using the corresponding index in 'idx', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			id := idx[i+2:i]*64
//			IF k[j]
//				dst[i+63:i] := a[id+63:id]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMPD'. Intrinsic: '_mm512_maskz_permutexvar_pd'.
// Requires AVX512F.
func MaskzPermutexvarPd(k Mmask8, idx M512i, a M512d) M512d {
	return M512d(maskzPermutexvarPd(uint8(k), [64]byte(idx), [8]float64(a)))
}

func maskzPermutexvarPd(k uint8, idx [64]byte, a [8]float64) [8]float64


// PermutexvarPd: Shuffle double-precision (64-bit) floating-point elements in
// 'a' across lanes using the corresponding index in 'idx', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			id := idx[i+2:i]*64
//			dst[i+63:i] := a[id+63:id]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMPD'. Intrinsic: '_mm512_permutexvar_pd'.
// Requires AVX512F.
func PermutexvarPd(idx M512i, a M512d) M512d {
	return M512d(permutexvarPd([64]byte(idx), [8]float64(a)))
}

func permutexvarPd(idx [64]byte, a [8]float64) [8]float64


// MaskPermutexvarPs: Shuffle single-precision (32-bit) floating-point elements
// in 'a' across lanes using the corresponding index in 'idx', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			id := idx[i+3:i]*32
//			IF k[j]
//				dst[i+31:i] := a[id+31:id]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMPS'. Intrinsic: '_mm512_mask_permutexvar_ps'.
// Requires AVX512F.
func MaskPermutexvarPs(src M512, k Mmask16, idx M512i, a M512) M512 {
	return M512(maskPermutexvarPs([16]float32(src), uint16(k), [64]byte(idx), [16]float32(a)))
}

func maskPermutexvarPs(src [16]float32, k uint16, idx [64]byte, a [16]float32) [16]float32


// MaskzPermutexvarPs: Shuffle single-precision (32-bit) floating-point
// elements in 'a' across lanes using the corresponding index in 'idx', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			id := idx[i+3:i]*32
//			IF k[j]
//				dst[i+31:i] := a[id+31:id]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMPS'. Intrinsic: '_mm512_maskz_permutexvar_ps'.
// Requires AVX512F.
func MaskzPermutexvarPs(k Mmask16, idx M512i, a M512) M512 {
	return M512(maskzPermutexvarPs(uint16(k), [64]byte(idx), [16]float32(a)))
}

func maskzPermutexvarPs(k uint16, idx [64]byte, a [16]float32) [16]float32


// PermutexvarPs: Shuffle single-precision (32-bit) floating-point elements in
// 'a' across lanes using the corresponding index in 'idx'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			id := idx[i+3:i]*32
//			dst[i+31:i] := a[id+31:id]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMPS'. Intrinsic: '_mm512_permutexvar_ps'.
// Requires AVX512F.
func PermutexvarPs(idx M512i, a M512) M512 {
	return M512(permutexvarPs([64]byte(idx), [16]float32(a)))
}

func permutexvarPs(idx [64]byte, a [16]float32) [16]float32


// MaskPowPd: Compute the exponential value of packed double-precision (64-bit)
// floating-point elements in 'a' raised by packed elements in 'b', and store
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i])^(b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_pow_pd'.
// Requires AVX512F.
func MaskPowPd(src M512d, k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskPowPd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b)))
}

func maskPowPd(src [8]float64, k uint8, a [8]float64, b [8]float64) [8]float64


// PowPd: Compute the exponential value of packed double-precision (64-bit)
// floating-point elements in 'a' raised by packed elements in 'b', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := (a[i+63:i])^(b[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_pow_pd'.
// Requires AVX512F.
func PowPd(a M512d, b M512d) M512d {
	return M512d(powPd([8]float64(a), [8]float64(b)))
}

func powPd(a [8]float64, b [8]float64) [8]float64


// MaskPowPs: Compute the exponential value of packed single-precision (32-bit)
// floating-point elements in 'a' raised by packed elements in 'b', and store
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i])^(b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_pow_ps'.
// Requires AVX512F.
func MaskPowPs(src M512, k Mmask16, a M512, b M512) M512 {
	return M512(maskPowPs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b)))
}

func maskPowPs(src [16]float32, k uint16, a [16]float32, b [16]float32) [16]float32


// PowPs: Compute the exponential value of packed single-precision (32-bit)
// floating-point elements in 'a' raised by packed elements in 'b', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := (a[i+31:i])^(b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_pow_ps'.
// Requires AVX512F.
func PowPs(a M512, b M512) M512 {
	return M512(powPs([16]float32(a), [16]float32(b)))
}

func powPs(a [16]float32, b [16]float32) [16]float32


// MaskPrefetchI32extgatherPs: Prefetches a set of 16 single-precision (32-bit)
// memory locations pointed by base address 'mv' and 32-bit integer index
// vector 'index' with scale 'scale' to L1 or L2 level of cache depending on
// the value of 'hint'. Gathered elements are merged in cache using writemask
// 'k' (elements are brought into cache only when their corresponding mask bits
// are set). The 'hint' parameter may be 1 (_MM_HINT_T0) for prefetching to L1
// cache, or 2 (_MM_HINT_T1) for prefetching to L2 cache.
// The 'conv' parameter specifies the granularity used by compilers to better
// encode the instruction. It should be the same as the 'conv' parameter
// specified for the subsequent gather intrinsic. 
//
//		FOR j := 0 to 15
//			addr := MEM[mv + index[j] * scale]
//			i := j*32
//			IF k[j] THEN
//				CASE hint OF
//				_MM_HINT_T0: PrefetchL1WithT0Hint(addr[i+31:i])
//				_MM_HINT_T1: PrefetchL2WithT1Hint(addr[i+31:i])
//				ESAC
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGATHERPF0DPS, VGATHERPF1DPS'. Intrinsic: '_mm512_mask_prefetch_i32extgather_ps'.
// Requires KNCNI.
func MaskPrefetchI32extgatherPs(index M512i, k Mmask16, mv uintptr, conv MMUPCONVPSENUM, scale int, hint int)  {
	maskPrefetchI32extgatherPs([64]byte(index), uint16(k), uintptr(mv), conv, scale, hint)
}

func maskPrefetchI32extgatherPs(index [64]byte, k uint16, mv uintptr, conv MMUPCONVPSENUM, scale int, hint int) 


// PrefetchI32extgatherPs: Prefetches a set of 16 single-precision (32-bit)
// memory locations pointed by base address 'mv' and 32-bit integer index
// vector 'index' with scale 'scale' to L1 or L2 level of cache depending on
// the value of 'hint'. The 'hint' parameter may be 1 (_MM_HINT_T0) for
// prefetching to L1 cache, or 2 (_MM_HINT_T1) for prefetching to L2 cache.
// The 'conv' parameter specifies the granularity used by compilers to better
// encode the instruction. It should be the same as the 'conv' parameter
// specified for the subsequent gather intrinsic. 
//
//		FOR j := 0 to 15
//			addr := MEM[mv + index[j] * scale]
//			i := j*32
//			CASE hint OF
//			_MM_HINT_T0: PrefetchL1WithT0Hint(addr[i+31:i])
//			_MM_HINT_T1: PrefetchL2WithT1Hint(addr[i+31:i])
//			ESAC
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGATHERPF0DPS, VGATHERPF1DPS'. Intrinsic: '_mm512_prefetch_i32extgather_ps'.
// Requires KNCNI.
func PrefetchI32extgatherPs(index M512i, mv uintptr, conv MMUPCONVPSENUM, scale int, hint int)  {
	prefetchI32extgatherPs([64]byte(index), uintptr(mv), conv, scale, hint)
}

func prefetchI32extgatherPs(index [64]byte, mv uintptr, conv MMUPCONVPSENUM, scale int, hint int) 


// MaskPrefetchI32extscatterPs: Prefetches a set of 16 single-precision
// (32-bit) memory locations pointed by base address 'mv' and 32-bit integer
// index vector 'index' with scale 'scale' to L1 or L2 level of cache depending
// on the value of 'hint'. The 'hint' parameter may be 1 (_MM_HINT_T0) for
// prefetching to L1 cache, or 2 (_MM_HINT_T1) for prefetching to L2 cache.
// The 'conv' parameter specifies the granularity used by compilers to better
// encode the instruction. It should be the same as the 'conv' parameter
// specified for the subsequent gather intrinsic. Only those elements whose
// corresponding mask bit in 'k' is set are loaded into cache. 
//
//		cachev := 0
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				addr := MEM[mv + index[j] * scale]
//				CASE hint OF
//				_MM_HINT_T0: PrefetchL1WithT0Hint(addr[i+31:i])
//				_MM_HINT_T1: PrefetchL2WithT1Hint(addr[i+31:i])
//				ESAC
//			FI
//		ENDFOR
//
// Instruction: 'VSCATTERPF0DPS, VSCATTERPF1DPS'. Intrinsic: '_mm512_mask_prefetch_i32extscatter_ps'.
// Requires KNCNI.
func MaskPrefetchI32extscatterPs(mv uintptr, k Mmask16, index M512i, conv MMUPCONVPSENUM, scale int, hint int)  {
	maskPrefetchI32extscatterPs(uintptr(mv), uint16(k), [64]byte(index), conv, scale, hint)
}

func maskPrefetchI32extscatterPs(mv uintptr, k uint16, index [64]byte, conv MMUPCONVPSENUM, scale int, hint int) 


// PrefetchI32extscatterPs: Prefetches a set of 16 single-precision (32-bit)
// memory locations pointed by base address 'mv' and 32-bit integer index
// vector 'index' with scale 'scale' to L1 or L2 level of cache depending on
// the value of 'hint', with a request for exclusive ownership. The 'hint'
// parameter may be one of the following: _MM_HINT_T0 = 1 for prefetching to L1
// cache, _MM_HINT_T1 = 2 for prefetching to L2 cache, _MM_HINT_T2 = 3 for
// prefetching to L2 cache non-temporal, _MM_HINT_NTA = 0 for prefetching to L1
// cache non-temporal. The 'conv' parameter specifies the granularity used by
// compilers to better encode the instruction. It should be the same as the
// 'conv' parameter specified for the subsequent scatter intrinsic. 
//
//		cachev := 0
//		FOR j := 0 to 15
//			i := j*32
//			addr := MEM[mv + index[j] * scale]
//			CASE hint OF
//			_MM_HINT_T0: PrefetchL1WithT0Hint(addr[i+31:i])
//			_MM_HINT_T1: PrefetchL2WithT1Hint(addr[i+31:i])
//			_MM_HINT_T2: PrefetchL2WithT1HintNonTemporal(addr[i+31:i])
//			_MM_HINT_NTA: PrefetchL1WithT0HintNonTemporal(addr[i+31:i])
//			ESAC
//		ENDFOR
//
// Instruction: 'VSCATTERPF0DPS, VSCATTERPF1DPS'. Intrinsic: '_mm512_prefetch_i32extscatter_ps'.
// Requires KNCNI.
func PrefetchI32extscatterPs(mv uintptr, index M512i, conv MMUPCONVPSENUM, scale int, hint int)  {
	prefetchI32extscatterPs(uintptr(mv), [64]byte(index), conv, scale, hint)
}

func prefetchI32extscatterPs(mv uintptr, index [64]byte, conv MMUPCONVPSENUM, scale int, hint int) 


// MaskPrefetchI32gatherPd: Prefetch double-precision (64-bit) floating-point
// elements from memory using 32-bit indices. 64-bit elements are loaded from
// addresses starting at 'base_addr' and offset by each 32-bit element in
// 'vindex' (each index is scaled by the factor in 'scale'). Gathered elements
// are merged in cache using writemask 'k' (elements are brought into cache
// only when their corresponding mask bits are set). 'scale' should be 1, 2, 4
// or 8. The 'hint' parameter may be 1 (_MM_HINT_T0) for prefetching to L1
// cache, or 2 (_MM_HINT_T1) for prefetching to L2 cache. 
//
//		FOR j := 0 to 7
//			i := j*32;
//			IF mask[j] THEN
//				Prefetch([base_addr + SignExtend(vindex[i*31:i]) * scale], hint, RFO=0);
//			FI
//		ENDFOR;
//
// Instruction: 'VGATHERPF0DPD, VGATHERPF1DPD'. Intrinsic: '_mm512_mask_prefetch_i32gather_pd'.
// Requires AVX512PF.
func MaskPrefetchI32gatherPd(vindex M256i, mask Mmask8, base_addr uintptr, scale int, hint int)  {
	maskPrefetchI32gatherPd([32]byte(vindex), uint8(mask), uintptr(base_addr), scale, hint)
}

func maskPrefetchI32gatherPd(vindex [32]byte, mask uint8, base_addr uintptr, scale int, hint int) 


// PrefetchI32gatherPd: Prefetch double-precision (64-bit) floating-point
// elements from memory using 32-bit indices. 64-bit elements are loaded from
// addresses starting at 'base_addr' and offset by each 32-bit element in
// 'vindex' (each index is scaled by the factor in 'scale'). Gathered elements
// are merged in cache. 'scale' should be 1, 2, 4 or 8. The 'hint' parameter
// may be 1 (_MM_HINT_T0) for prefetching to L1 cache, or 2 (_MM_HINT_T1) for
// prefetching to L2 cache. 
//
//		FOR j := 0 to 7
//			i := j*32;
//			Prefetch([base_addr + SignExtend(vindex[i*31:i]) * scale], hint, RFO=0);
//		ENDFOR;
//
// Instruction: 'VGATHERPF0DPD, VGATHERPF1DPD'. Intrinsic: '_mm512_prefetch_i32gather_pd'.
// Requires AVX512PF.
func PrefetchI32gatherPd(vindex M256i, base_addr uintptr, scale int, hint int)  {
	prefetchI32gatherPd([32]byte(vindex), uintptr(base_addr), scale, hint)
}

func prefetchI32gatherPd(vindex [32]byte, base_addr uintptr, scale int, hint int) 


// MaskPrefetchI32gatherPs: Prefetch single-precision (32-bit) floating-point
// elements from memory using 32-bit indices. 32-bit elements are loaded from
// addresses starting at 'base_addr' and offset by each 32-bit element in
// 'vindex' (each index is scaled by the factor in 'scale'). Gathered elements
// are merged in cache using writemask 'k' (elements are brought into cache
// only when their corresponding mask bits are set). 'scale' should be 1, 2, 4
// or 8. The 'hint' parameter may be 1 (_MM_HINT_T0) for prefetching to L1
// cache, or 2 (_MM_HINT_T1) for prefetching to L2 cache. 
//
//		FOR j := 0 to 15
//			i := j*16;
//			IF mask[j] THEN
//				Prefetch([base_addr + SignExtend(vindex[i*31:i]) * scale], hint, RFO=0);
//			FI
//		ENDFOR;
//
// Instruction: 'VGATHERPF0DPS, VGATHERPF1DPS'. Intrinsic: '_mm512_mask_prefetch_i32gather_ps'.
// Requires KNCNI.
func MaskPrefetchI32gatherPs(vindex M512i, mask Mmask16, base_addr uintptr, scale int, hint int)  {
	maskPrefetchI32gatherPs([64]byte(vindex), uint16(mask), uintptr(base_addr), scale, hint)
}

func maskPrefetchI32gatherPs(vindex [64]byte, mask uint16, base_addr uintptr, scale int, hint int) 


// PrefetchI32gatherPs: Prefetches 16 single-precision (32-bit) floating-point
// elements in memory starting at location 'mv' at packed 32-bit integer
// indices stored in 'index' scaled by 'scale'. The 'hint' parameter may be 1
// (_MM_HINT_T0) for prefetching to L1 cache, or 2 (_MM_HINT_T1) for
// prefetching to L2 cache. 
//
//		cachev := 0
//		FOR j := 0 to 15
//			i := j*32
//			addr := MEM[mv + index[j] * scale]
//			cachev[i+31:i] := addr[i+63:i]
//		ENDFOR
//
// Instruction: 'VGATHERPF0DPS, VGATHERPF1DPS'. Intrinsic: '_mm512_prefetch_i32gather_ps'.
// Requires KNCNI.
func PrefetchI32gatherPs(index M512i, mv uintptr, scale int, hint int)  {
	prefetchI32gatherPs([64]byte(index), uintptr(mv), scale, hint)
}

func prefetchI32gatherPs(index [64]byte, mv uintptr, scale int, hint int) 


// MaskPrefetchI32scatterPd: Prefetch double-precision (64-bit) floating-point
// elements with intent to write using 32-bit indices. The 'hint' parameter may
// be 1 (_MM_HINT_T0) for prefetching to L1 cache, or 2 (_MM_HINT_T1) for
// prefetching to L2 cache. 64-bit elements are brought into cache from
// addresses starting at 'base_addr' and offset by each 32-bit element in
// 'vindex' (each index is scaled by the factor in 'scale') subject to mask 'k'
// (elements are not brought into cache when the corresponding mask bit is not
// set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 TO 7
//			i := j*32;
//			IF mask[j] THEN
//				Prefetch(base_addr + SignExtend(vindex[i+31:i]) * scale], Level=hint, RFO=1);
//			FI
//		ENDFOR;
//
// Instruction: 'VSCATTERPF0DPD, VSCATTERPF1DPD'. Intrinsic: '_mm512_mask_prefetch_i32scatter_pd'.
// Requires AVX512PF.
func MaskPrefetchI32scatterPd(base_addr uintptr, mask Mmask8, vinde M256i, scale int, hint int)  {
	maskPrefetchI32scatterPd(uintptr(base_addr), uint8(mask), [32]byte(vinde), scale, hint)
}

func maskPrefetchI32scatterPd(base_addr uintptr, mask uint8, vinde [32]byte, scale int, hint int) 


// PrefetchI32scatterPd: Prefetch double-precision (64-bit) floating-point
// elements with intent to write using 32-bit indices. The 'hint' parameter may
// be 1 (_MM_HINT_T0) for prefetching to L1 cache, or 2 (_MM_HINT_T1) for
// prefetching to L2 cache. 64-bit elements are brought into cache from
// addresses starting at 'base_addr' and offset by each 32-bit element in
// 'vindex' (each index is scaled by the factor in 'scale'). 'scale' should be
// 1, 2, 4 or 8. 
//
//		FOR j := 0 TO 7
//			i := j*32;
//			Prefetch(base_addr + SignExtend(vindex[i+31:i]) * scale], Level=hint, RFO=1);
//		ENDFOR;
//
// Instruction: 'VSCATTERPF0DPD, VSCATTERPF1DPD'. Intrinsic: '_mm512_prefetch_i32scatter_pd'.
// Requires AVX512PF.
func PrefetchI32scatterPd(base_addr uintptr, vindex M256i, scale int, hint int)  {
	prefetchI32scatterPd(uintptr(base_addr), [32]byte(vindex), scale, hint)
}

func prefetchI32scatterPd(base_addr uintptr, vindex [32]byte, scale int, hint int) 


// MaskPrefetchI32scatterPs: Prefetches 16 single-precision (32-bit)
// floating-point elements in memory starting at location 'mv' at packed 32-bit
// integer indices stored in 'index' scaled by 'scale'. The 'hint' parameter
// may be 1 (_MM_HINT_T0) for prefetching to L1 cache, or 2 (_MM_HINT_T1) for
// prefetching to L2 cache. Only those elements whose corresponding mask bit in
// 'k' is set are loaded into the desired cache. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				addr := MEM[mv + index[j] * scale]
//				CASE hint OF
//				_MM_HINT_T0: PrefetchL1WithT0Hint(addr[i+31:i])
//				_MM_HINT_T1: PrefetchL2WithT1Hint(addr[i+31:i])
//				_MM_HINT_T2: PrefetchL2WithT1HintNonTemporal(addr[i+31:i])
//				_MM_HINT_NTA: PrefetchL1WithT0HintNonTemporal(addr[i+31:i])
//			FI
//		ENDFOR
//
// Instruction: 'VSCATTERPF0DPS, VSCATTERPF1DPS'. Intrinsic: '_mm512_mask_prefetch_i32scatter_ps'.
// Requires KNCNI.
func MaskPrefetchI32scatterPs(mv uintptr, k Mmask16, index M512i, scale int, hint int)  {
	maskPrefetchI32scatterPs(uintptr(mv), uint16(k), [64]byte(index), scale, hint)
}

func maskPrefetchI32scatterPs(mv uintptr, k uint16, index [64]byte, scale int, hint int) 


// PrefetchI32scatterPs: Prefetches 16 single-precision (32-bit) floating-point
// elements in memory starting at location 'mv' at packed 32-bit integer
// indices stored in 'index' scaled by 'scale'. The 'hint' parameter may be 1
// (_MM_HINT_T0) for prefetching to L1 cache, or 2 (_MM_HINT_T1) for
// prefetching to L2 cache. 
//
//		FOR j := 0 to 15
//			i := j*32
//			addr := MEM[mv + index[j] * scale]
//			CASE hint OF
//			_MM_HINT_T0: PrefetchL1WithT0Hint(addr[i+31:i])
//			_MM_HINT_T1: PrefetchL2WithT1Hint(addr[i+31:i])
//			_MM_HINT_T2: PrefetchL2WithT1HintNonTemporal(addr[i+31:i])
//			_MM_HINT_NTA: PrefetchL1WithT0HintNonTemporal(addr[i+31:i])
//			ESAC
//		ENDFOR
//
// Instruction: 'VSCATTERPF0DPS, VSCATTERPF1DPS'. Intrinsic: '_mm512_prefetch_i32scatter_ps'.
// Requires KNCNI.
func PrefetchI32scatterPs(mv uintptr, index M512i, scale int, hint int)  {
	prefetchI32scatterPs(uintptr(mv), [64]byte(index), scale, hint)
}

func prefetchI32scatterPs(mv uintptr, index [64]byte, scale int, hint int) 


// MaskPrefetchI64gatherPd: Prefetch double-precision (64-bit) floating-point
// elements from memory into cache level specified by 'hint' using 64-bit
// indices. 64-bit elements are loaded from addresses starting at 'base_addr'
// and offset by each 64-bit element in 'vindex' (each index is scaled by the
// factor in 'scale'). Prefetched elements are merged in cache using writemask
// 'k' (elements are copied from memory when the corresponding mask bit is
// set). 'scale' should be 1, 2, 4 or 8. The 'hint' parameter may be 1
// (_MM_HINT_T0) for prefetching to L1 cache, or 2 (_MM_HINT_T1) for
// prefetching to L2 cache. 
//
//		FOR j := 0 to 7
//			i := j*64;
//			IF mask[j] THEN
//				Prefetch([base_addr + SignExtend(vindex[i*63:i] * scale]), Level=hint, RFO=0);
//			FI
//		ENDFOR;
//
// Instruction: 'VGATHERPF0QPD, VGATHERPF1QPD'. Intrinsic: '_mm512_mask_prefetch_i64gather_pd'.
// Requires AVX512PF.
func MaskPrefetchI64gatherPd(vindex M512i, mask Mmask8, base_addr uintptr, scale int, hint int)  {
	maskPrefetchI64gatherPd([64]byte(vindex), uint8(mask), uintptr(base_addr), scale, hint)
}

func maskPrefetchI64gatherPd(vindex [64]byte, mask uint8, base_addr uintptr, scale int, hint int) 


// PrefetchI64gatherPd: Prefetch double-precision (64-bit) floating-point
// elements from memory into cache level specified by 'hint' using 64-bit
// indices. 64-bit elements are loaded from addresses starting at 'base_addr'
// and offset by each 64-bit element in 'vindex' (each index is scaled by the
// factor in 'scale'). 'scale' should be 1, 2, 4 or 8. The 'hint' parameter may
// be 1 (_MM_HINT_T0) for prefetching to L1 cache, or 2 (_MM_HINT_T1) for
// prefetching to L2 cache. 
//
//		FOR j := 0 to 7
//			i := j*64;
//			Prefetch([base_addr + SignExtend(vindex[i*63:i] * scale]), Level=hint, RFO=0);
//		ENDFOR;
//
// Instruction: 'VGATHERPF0QPD, VGATHERPF1QPD'. Intrinsic: '_mm512_prefetch_i64gather_pd'.
// Requires AVX512PF.
func PrefetchI64gatherPd(vindex M512i, base_addr uintptr, scale int, hint int)  {
	prefetchI64gatherPd([64]byte(vindex), uintptr(base_addr), scale, hint)
}

func prefetchI64gatherPd(vindex [64]byte, base_addr uintptr, scale int, hint int) 


// MaskPrefetchI64gatherPs: Prefetch single-precision (32-bit) floating-point
// elements from memory using 64-bit indices. 32-bit elements are loaded from
// addresses starting at 'base_addr' and offset by each 64-bit element in
// 'vindex' (each index is scaled by the factor in 'scale'). Gathered elements
// are merged in cache using writemask 'k' (elements are only brought into
// cache when their corresponding mask bit is set). 'scale' should be 1, 2, 4
// or 8.. The 'hint' parameter may be 1 (_MM_HINT_T0) for prefetching to L1
// cache, or 2 (_MM_HINT_T1) for prefetching to L2 cache. 
//
//		FOR j:= 0 to 7
//			i := j*64;
//			IF mask[j] THEN
//				Prefetch([base_addr + SignExtend(vindex[i+63:i]) * scale], hint, RFO=0);
//			FI
//		ENDFOR;
//
// Instruction: 'VGATHERPF0QPS, VGATHERPF1QPS'. Intrinsic: '_mm512_mask_prefetch_i64gather_ps'.
// Requires AVX512PF.
func MaskPrefetchI64gatherPs(vindex M512i, mask Mmask8, base_addr uintptr, scale int, hint int)  {
	maskPrefetchI64gatherPs([64]byte(vindex), uint8(mask), uintptr(base_addr), scale, hint)
}

func maskPrefetchI64gatherPs(vindex [64]byte, mask uint8, base_addr uintptr, scale int, hint int) 


// PrefetchI64gatherPs: Prefetch single-precision (32-bit) floating-point
// elements from memory using 64-bit indices. 32-bit elements are loaded from
// addresses starting at 'base_addr' and offset by each 64-bit element in
// 'vindex' (each index is scaled by the factor in 'scale'). Gathered elements
// are merged in cache. 'scale' should be 1, 2, 4 or 8. The 'hint' parameter
// may be 1 (_MM_HINT_T0) for prefetching to L1 cache, or 2 (_MM_HINT_T1) for
// prefetching to L2 cache. 
//
//		FOR j:= 0 to 7
//			i := j*64;
//			Prefetch([base_addr + SignExtend(vindex[i+63:i]) * scale], hint, RFO=0);
//		ENDFOR;
//
// Instruction: 'VGATHERPF0QPS, VGATHERPF1QPS'. Intrinsic: '_mm512_prefetch_i64gather_ps'.
// Requires AVX512PF.
func PrefetchI64gatherPs(vindex M512i, base_addr uintptr, scale int, hint int)  {
	prefetchI64gatherPs([64]byte(vindex), uintptr(base_addr), scale, hint)
}

func prefetchI64gatherPs(vindex [64]byte, base_addr uintptr, scale int, hint int) 


// MaskPrefetchI64scatterPd: Prefetch double-precision (64-bit) floating-point
// elements with intent to write into memory using 64-bit indices. The 'hint'
// parameter may be 1 (_MM_HINT_T0) for prefetching to L1 cache, or 2
// (_MM_HINT_T1) for prefetching to L2 cache. 64-bit elements are brought into
// cache from addresses starting at 'base_addr' and offset by each 64-bit
// element in 'vindex' (each index is scaled by the factor in 'scale') subject
// to mask 'k' (elements are not brought into cache when the corresponding mask
// bit is not set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*64;
//			IF mask[j] THEN
//				Prefetch([base_addr + SignExtend(vindex[i+63:i]) * scale], Level=hint, RFO=1);
//			FI
//		ENDFOR;
//
// Instruction: 'VSCATTERPF0QPD, VSCATTERPF1QPD'. Intrinsic: '_mm512_mask_prefetch_i64scatter_pd'.
// Requires AVX512PF.
func MaskPrefetchI64scatterPd(base_addr uintptr, mask Mmask8, vindex M512i, scale int, hint int)  {
	maskPrefetchI64scatterPd(uintptr(base_addr), uint8(mask), [64]byte(vindex), scale, hint)
}

func maskPrefetchI64scatterPd(base_addr uintptr, mask uint8, vindex [64]byte, scale int, hint int) 


// PrefetchI64scatterPd: Prefetch double-precision (64-bit) floating-point
// elements with intent to write into memory using 64-bit indices. The 'hint'
// parameter may be 1 (_MM_HINT_T0) for prefetching to L1 cache, or 2
// (_MM_HINT_T1) for prefetching to L2 cache. 64-bit elements are brought into
// cache from addresses starting at 'base_addr' and offset by each 64-bit
// element in 'vindex' (each index is scaled by the factor in 'scale'). 'scale'
// should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*64;
//			Prefetch([base_addr + SignExtend(vindex[i+63:i]) * scale], Level=hint, RFO=1);
//		ENDFOR;
//
// Instruction: 'VSCATTERPF0QPD, VSCATTERPF1QPD'. Intrinsic: '_mm512_prefetch_i64scatter_pd'.
// Requires AVX512PF.
func PrefetchI64scatterPd(base_addr uintptr, vindex M512i, scale int, hint int)  {
	prefetchI64scatterPd(uintptr(base_addr), [64]byte(vindex), scale, hint)
}

func prefetchI64scatterPd(base_addr uintptr, vindex [64]byte, scale int, hint int) 


// MaskPrefetchI64scatterPs: Prefetch single-precision (32-bit) floating-point
// elements with intent to write into memory using 64-bit indices. The 'hint'
// parameter may be 1 (_MM_HINT_T0) for prefetching to L1 cache, or 2
// (_MM_HINT_T1) for prefetching to L2 cache. 32-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 64-bit element in
// 'vindex' (each index is scaled by the factor in 'scale') subject to mask 'k'
// (elements are not brought into cache when the corresponding mask bit is not
// set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*64;
//			IF mask[j] THEN
//				Prefetch([base_addr + SignExtend(vindex[i+63:i]) * scale], Level=hint, RFO=1);
//			FI
//		ENDFOR;
//
// Instruction: 'VSCATTERPF0QPS, VSCATTERPF1QPS'. Intrinsic: '_mm512_mask_prefetch_i64scatter_ps'.
// Requires AVX512PF.
func MaskPrefetchI64scatterPs(base_addr uintptr, mask Mmask8, vindex M512i, scale int, hint int)  {
	maskPrefetchI64scatterPs(uintptr(base_addr), uint8(mask), [64]byte(vindex), scale, hint)
}

func maskPrefetchI64scatterPs(base_addr uintptr, mask uint8, vindex [64]byte, scale int, hint int) 


// PrefetchI64scatterPs: Prefetch single-precision (32-bit) floating-point
// elements with intent to write into memory using 64-bit indices. Elements are
// prefetched into cache level 'hint', where 'hint' is 0 or 1. 32-bit elements
// are stored at addresses starting at 'base_addr' and offset by each 64-bit
// element in 'vindex' (each index is scaled by the factor in 'scale'). 'scale'
// should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*64;
//			Prefetch([base_addr + SignExtend(vindex[i+63:i]) * scale], Level=hint, RFO=1);
//		ENDFOR;
//
// Instruction: 'VSCATTERPF0QPS, VSCATTERPF1QPS'. Intrinsic: '_mm512_prefetch_i64scatter_ps'.
// Requires AVX512PF.
func PrefetchI64scatterPs(base_addr uintptr, vindex M512i, scale int, hint int)  {
	prefetchI64scatterPs(uintptr(base_addr), [64]byte(vindex), scale, hint)
}

func prefetchI64scatterPs(base_addr uintptr, vindex [64]byte, scale int, hint int) 


// MaskRangePd: Calculate the max, min, absolute max, or absolute min
// (depending on control in 'imm8') for packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set).
// 	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 =
// absolute max, 11 = absolute min.
// 	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from
// compare result, 10 = clear sign bit, 11 = set sign bit. 
//
//		RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0])
//		{
//			CASE opCtl[1:0]
//			0: tmp[63:0] := (src1[63:0] <= src2[63:0]) ? src1[63:0] : src2[63:0]
//			1: tmp[63:0] := (src1[63:0] <= src2[63:0]) ? src2[63:0] : src1[63:0]
//			2: tmp[63:0] := (ABS(src1[63:0]) <= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
//			3: tmp[63:0] := (ABS(src1[63:0]) <= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
//			ESAC
//			
//			CASE signSelCtl[1:0]
//			0: dst[63:0] := (src1[63] << 63) OR (tmp[62:0])
//			1: dst[63:0] := tmp[63:0]
//			2: dst[63:0] := (0 << 63) OR (tmp[62:0])
//			3: dst[63:0] := (1 << 63) OR (tmp[62:0])
//			ESAC
//			
//			RETURN dst
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RANGE(a[i+63:i], b[i+63:i], imm8[1:0], imm8[3:2])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRANGEPD'. Intrinsic: '_mm512_mask_range_pd'.
// Requires AVX512DQ.
func MaskRangePd(src M512d, k Mmask8, a M512d, b M512d, imm8 int) M512d {
	return M512d(maskRangePd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b), imm8))
}

func maskRangePd(src [8]float64, k uint8, a [8]float64, b [8]float64, imm8 int) [8]float64


// MaskzRangePd: Calculate the max, min, absolute max, or absolute min
// (depending on control in 'imm8') for packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set).
// 	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 =
// absolute max, 11 = absolute min.
// 	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from
// compare result, 10 = clear sign bit, 11 = set sign bit. 
//
//		RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0])
//		{
//			CASE opCtl[1:0]
//			0: tmp[63:0] := (src1[63:0] <= src2[63:0]) ? src1[63:0] : src2[63:0]
//			1: tmp[63:0] := (src1[63:0] <= src2[63:0]) ? src2[63:0] : src1[63:0]
//			2: tmp[63:0] := (ABS(src1[63:0]) <= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
//			3: tmp[63:0] := (ABS(src1[63:0]) <= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
//			ESAC
//			
//			CASE signSelCtl[1:0]
//			0: dst[63:0] := (src1[63] << 63) OR (tmp[62:0])
//			1: dst[63:0] := tmp[63:0]
//			2: dst[63:0] := (0 << 63) OR (tmp[62:0])
//			3: dst[63:0] := (1 << 63) OR (tmp[62:0])
//			ESAC
//			
//			RETURN dst
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RANGE(a[i+63:i], b[i+63:i], imm8[1:0], imm8[3:2])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRANGEPD'. Intrinsic: '_mm512_maskz_range_pd'.
// Requires AVX512DQ.
func MaskzRangePd(k Mmask8, a M512d, b M512d, imm8 int) M512d {
	return M512d(maskzRangePd(uint8(k), [8]float64(a), [8]float64(b), imm8))
}

func maskzRangePd(k uint8, a [8]float64, b [8]float64, imm8 int) [8]float64


// RangePd: Calculate the max, min, absolute max, or absolute min (depending on
// control in 'imm8') for packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store the results in 'dst'.
// 	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 =
// absolute max, 11 = absolute min.
// 	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from
// compare result, 10 = clear sign bit, 11 = set sign bit. 
//
//		RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0])
//		{
//			CASE opCtl[1:0]
//			0: tmp[63:0] := (src1[63:0] <= src2[63:0]) ? src1[63:0] : src2[63:0]
//			1: tmp[63:0] := (src1[63:0] <= src2[63:0]) ? src2[63:0] : src1[63:0]
//			2: tmp[63:0] := (ABS(src1[63:0]) <= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
//			3: tmp[63:0] := (ABS(src1[63:0]) <= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
//			ESAC
//			
//			CASE signSelCtl[1:0]
//			0: dst[63:0] := (src1[63] << 63) OR (tmp[62:0])
//			1: dst[63:0] := tmp[63:0]
//			2: dst[63:0] := (0 << 63) OR (tmp[62:0])
//			3: dst[63:0] := (1 << 63) OR (tmp[62:0])
//			ESAC
//			
//			RETURN dst
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := RANGE(a[i+63:i], b[i+63:i], imm8[1:0], imm8[3:2])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRANGEPD'. Intrinsic: '_mm512_range_pd'.
// Requires AVX512DQ.
func RangePd(a M512d, b M512d, imm8 int) M512d {
	return M512d(rangePd([8]float64(a), [8]float64(b), imm8))
}

func rangePd(a [8]float64, b [8]float64, imm8 int) [8]float64


// MaskRangePs: Calculate the max, min, absolute max, or absolute min
// (depending on control in 'imm8') for packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set).
// 	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 =
// absolute max, 11 = absolute min.
// 	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from
// compare result, 10 = clear sign bit, 11 = set sign bit. 
//
//		RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0])
//		{
//			CASE opCtl[1:0]
//			0: tmp[31:0] := (src1[31:0] <= src2[31:0]) ? src1[31:0] : src2[31:0]
//			1: tmp[31:0] := (src1[31:0] <= src2[31:0]) ? src2[31:0] : src1[31:0]
//			2: tmp[31:0] := (ABS(src1[31:0]) <= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
//			3: tmp[31:0] := (ABS(src1[31:0]) <= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
//			ESAC
//			
//			CASE signSelCtl[1:0]
//			0: dst[31:0] := (src1[31] << 31) OR (tmp[30:0])
//			1: dst[31:0] := tmp[63:0]
//			2: dst[31:0] := (0 << 31) OR (tmp[30:0])
//			3: dst[31:0] := (1 << 31) OR (tmp[30:0])
//			ESAC
//			
//			RETURN dst
//		}
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RANGE(a[i+31:i], b[i+31:i], imm8[1:0], imm8[3:2])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRANGEPS'. Intrinsic: '_mm512_mask_range_ps'.
// Requires AVX512DQ.
func MaskRangePs(src M512, k Mmask16, a M512, b M512, imm8 int) M512 {
	return M512(maskRangePs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b), imm8))
}

func maskRangePs(src [16]float32, k uint16, a [16]float32, b [16]float32, imm8 int) [16]float32


// MaskzRangePs: Calculate the max, min, absolute max, or absolute min
// (depending on control in 'imm8') for packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set).
// 	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 =
// absolute max, 11 = absolute min.
// 	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from
// compare result, 10 = clear sign bit, 11 = set sign bit. 
//
//		RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0])
//		{
//			CASE opCtl[1:0]
//			0: tmp[31:0] := (src1[31:0] <= src2[31:0]) ? src1[31:0] : src2[31:0]
//			1: tmp[31:0] := (src1[31:0] <= src2[31:0]) ? src2[31:0] : src1[31:0]
//			2: tmp[31:0] := (ABS(src1[31:0]) <= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
//			3: tmp[31:0] := (ABS(src1[31:0]) <= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
//			ESAC
//			
//			CASE signSelCtl[1:0]
//			0: dst[31:0] := (src1[31] << 31) OR (tmp[30:0])
//			1: dst[31:0] := tmp[63:0]
//			2: dst[31:0] := (0 << 31) OR (tmp[30:0])
//			3: dst[31:0] := (1 << 31) OR (tmp[30:0])
//			ESAC
//			
//			RETURN dst
//		}
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RANGE(a[i+31:i], b[i+31:i], imm8[1:0], imm8[3:2])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRANGEPS'. Intrinsic: '_mm512_maskz_range_ps'.
// Requires AVX512DQ.
func MaskzRangePs(k Mmask16, a M512, b M512, imm8 int) M512 {
	return M512(maskzRangePs(uint16(k), [16]float32(a), [16]float32(b), imm8))
}

func maskzRangePs(k uint16, a [16]float32, b [16]float32, imm8 int) [16]float32


// RangePs: Calculate the max, min, absolute max, or absolute min (depending on
// control in 'imm8') for packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store the results in 'dst'.
// 	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 =
// absolute max, 11 = absolute min.
// 	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from
// compare result, 10 = clear sign bit, 11 = set sign bit. 
//
//		RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0])
//		{
//			CASE opCtl[1:0]
//			0: tmp[31:0] := (src1[31:0] <= src2[31:0]) ? src1[31:0] : src2[31:0]
//			1: tmp[31:0] := (src1[31:0] <= src2[31:0]) ? src2[31:0] : src1[31:0]
//			2: tmp[31:0] := (ABS(src1[31:0]) <= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
//			3: tmp[31:0] := (ABS(src1[31:0]) <= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
//			ESAC
//			
//			CASE signSelCtl[1:0]
//			0: dst[31:0] := (src1[31] << 31) OR (tmp[30:0])
//			1: dst[31:0] := tmp[63:0]
//			2: dst[31:0] := (0 << 31) OR (tmp[30:0])
//			3: dst[31:0] := (1 << 31) OR (tmp[30:0])
//			ESAC
//			
//			RETURN dst
//		}
//		
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := RANGE(a[i+31:i], b[i+31:i], imm8[1:0], imm8[3:2])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRANGEPS'. Intrinsic: '_mm512_range_ps'.
// Requires AVX512DQ.
func RangePs(a M512, b M512, imm8 int) M512 {
	return M512(rangePs([16]float32(a), [16]float32(b), imm8))
}

func rangePs(a [16]float32, b [16]float32, imm8 int) [16]float32


// MaskRangeRoundPd: Calculate the max, min, absolute max, or absolute min
// (depending on control in 'imm8') for packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set).
// 	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 =
// absolute max, 11 = absolute min.
// 	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from
// compare result, 10 = clear sign bit, 11 = set sign bit.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0])
//		{
//			CASE opCtl[1:0]
//			0: tmp[63:0] := (src1[63:0] <= src2[63:0]) ? src1[63:0] : src2[63:0]
//			1: tmp[63:0] := (src1[63:0] <= src2[63:0]) ? src2[63:0] : src1[63:0]
//			2: tmp[63:0] := (ABS(src1[63:0]) <= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
//			3: tmp[63:0] := (ABS(src1[63:0]) <= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
//			ESAC
//			
//			CASE signSelCtl[1:0]
//			0: dst[63:0] := (src1[63] << 63) OR (tmp[62:0])
//			1: dst[63:0] := tmp[63:0]
//			2: dst[63:0] := (0 << 63) OR (tmp[62:0])
//			3: dst[63:0] := (1 << 63) OR (tmp[62:0])
//			ESAC
//			
//			RETURN dst
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RANGE(a[i+63:i], b[i+63:i], imm8[1:0], imm8[3:2])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRANGEPD'. Intrinsic: '_mm512_mask_range_round_pd'.
// Requires AVX512DQ.
func MaskRangeRoundPd(src M512d, k Mmask8, a M512d, b M512d, imm8 int, rounding int) M512d {
	return M512d(maskRangeRoundPd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b), imm8, rounding))
}

func maskRangeRoundPd(src [8]float64, k uint8, a [8]float64, b [8]float64, imm8 int, rounding int) [8]float64


// MaskzRangeRoundPd: Calculate the max, min, absolute max, or absolute min
// (depending on control in 'imm8') for packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set).
// 	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 =
// absolute max, 11 = absolute min.
// 	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from
// compare result, 10 = clear sign bit, 11 = set sign bit.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0])
//		{
//			CASE opCtl[1:0]
//			0: tmp[63:0] := (src1[63:0] <= src2[63:0]) ? src1[63:0] : src2[63:0]
//			1: tmp[63:0] := (src1[63:0] <= src2[63:0]) ? src2[63:0] : src1[63:0]
//			2: tmp[63:0] := (ABS(src1[63:0]) <= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
//			3: tmp[63:0] := (ABS(src1[63:0]) <= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
//			ESAC
//			
//			CASE signSelCtl[1:0]
//			0: dst[63:0] := (src1[63] << 63) OR (tmp[62:0])
//			1: dst[63:0] := tmp[63:0]
//			2: dst[63:0] := (0 << 63) OR (tmp[62:0])
//			3: dst[63:0] := (1 << 63) OR (tmp[62:0])
//			ESAC
//			
//			RETURN dst
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RANGE(a[i+63:i], b[i+63:i], imm8[1:0], imm8[3:2])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRANGEPD'. Intrinsic: '_mm512_maskz_range_round_pd'.
// Requires AVX512DQ.
func MaskzRangeRoundPd(k Mmask8, a M512d, b M512d, imm8 int, rounding int) M512d {
	return M512d(maskzRangeRoundPd(uint8(k), [8]float64(a), [8]float64(b), imm8, rounding))
}

func maskzRangeRoundPd(k uint8, a [8]float64, b [8]float64, imm8 int, rounding int) [8]float64


// RangeRoundPd: Calculate the max, min, absolute max, or absolute min
// (depending on control in 'imm8') for packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst'.
// 	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 =
// absolute max, 11 = absolute min.
// 	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from
// compare result, 10 = clear sign bit, 11 = set sign bit.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0])
//		{
//			CASE opCtl[1:0]
//			0: tmp[63:0] := (src1[63:0] <= src2[63:0]) ? src1[63:0] : src2[63:0]
//			1: tmp[63:0] := (src1[63:0] <= src2[63:0]) ? src2[63:0] : src1[63:0]
//			2: tmp[63:0] := (ABS(src1[63:0]) <= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
//			3: tmp[63:0] := (ABS(src1[63:0]) <= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
//			ESAC
//			
//			CASE signSelCtl[1:0]
//			0: dst[63:0] := (src1[63] << 63) OR (tmp[62:0])
//			1: dst[63:0] := tmp[63:0]
//			2: dst[63:0] := (0 << 63) OR (tmp[62:0])
//			3: dst[63:0] := (1 << 63) OR (tmp[62:0])
//			ESAC
//			
//			RETURN dst
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := RANGE(a[i+63:i], b[i+63:i], imm8[1:0], imm8[3:2])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRANGEPD'. Intrinsic: '_mm512_range_round_pd'.
// Requires AVX512DQ.
func RangeRoundPd(a M512d, b M512d, imm8 int, rounding int) M512d {
	return M512d(rangeRoundPd([8]float64(a), [8]float64(b), imm8, rounding))
}

func rangeRoundPd(a [8]float64, b [8]float64, imm8 int, rounding int) [8]float64


// MaskRangeRoundPs: Calculate the max, min, absolute max, or absolute min
// (depending on control in 'imm8') for packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set).
// 	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 =
// absolute max, 11 = absolute min.
// 	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from
// compare result, 10 = clear sign bit, 11 = set sign bit.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0])
//		{
//			CASE opCtl[1:0]
//			0: tmp[31:0] := (src1[31:0] <= src2[31:0]) ? src1[31:0] : src2[31:0]
//			1: tmp[31:0] := (src1[31:0] <= src2[31:0]) ? src2[31:0] : src1[31:0]
//			2: tmp[31:0] := (ABS(src1[31:0]) <= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
//			3: tmp[31:0] := (ABS(src1[31:0]) <= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
//			ESAC
//			
//			CASE signSelCtl[1:0]
//			0: dst[31:0] := (src1[31] << 31) OR (tmp[30:0])
//			1: dst[31:0] := tmp[63:0]
//			2: dst[31:0] := (0 << 31) OR (tmp[30:0])
//			3: dst[31:0] := (1 << 31) OR (tmp[30:0])
//			ESAC
//			
//			RETURN dst
//		}
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RANGE(a[i+31:i], b[i+31:i], imm8[1:0], imm8[3:2])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRANGEPS'. Intrinsic: '_mm512_mask_range_round_ps'.
// Requires AVX512DQ.
func MaskRangeRoundPs(src M512, k Mmask16, a M512, b M512, imm8 int, rounding int) M512 {
	return M512(maskRangeRoundPs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b), imm8, rounding))
}

func maskRangeRoundPs(src [16]float32, k uint16, a [16]float32, b [16]float32, imm8 int, rounding int) [16]float32


// MaskzRangeRoundPs: Calculate the max, min, absolute max, or absolute min
// (depending on control in 'imm8') for packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set).
// 	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 =
// absolute max, 11 = absolute min.
// 	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from
// compare result, 10 = clear sign bit, 11 = set sign bit.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0])
//		{
//			CASE opCtl[1:0]
//			0: tmp[31:0] := (src1[31:0] <= src2[31:0]) ? src1[31:0] : src2[31:0]
//			1: tmp[31:0] := (src1[31:0] <= src2[31:0]) ? src2[31:0] : src1[31:0]
//			2: tmp[31:0] := (ABS(src1[31:0]) <= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
//			3: tmp[31:0] := (ABS(src1[31:0]) <= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
//			ESAC
//			
//			CASE signSelCtl[1:0]
//			0: dst[31:0] := (src1[31] << 31) OR (tmp[30:0])
//			1: dst[31:0] := tmp[63:0]
//			2: dst[31:0] := (0 << 31) OR (tmp[30:0])
//			3: dst[31:0] := (1 << 31) OR (tmp[30:0])
//			ESAC
//			
//			RETURN dst
//		}
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RANGE(a[i+31:i], b[i+31:i], imm8[1:0], imm8[3:2])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRANGEPS'. Intrinsic: '_mm512_maskz_range_round_ps'.
// Requires AVX512DQ.
func MaskzRangeRoundPs(k Mmask16, a M512, b M512, imm8 int, rounding int) M512 {
	return M512(maskzRangeRoundPs(uint16(k), [16]float32(a), [16]float32(b), imm8, rounding))
}

func maskzRangeRoundPs(k uint16, a [16]float32, b [16]float32, imm8 int, rounding int) [16]float32


// RangeRoundPs: Calculate the max, min, absolute max, or absolute min
// (depending on control in 'imm8') for packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst'.
// 	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 =
// absolute max, 11 = absolute min.
// 	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from
// compare result, 10 = clear sign bit, 11 = set sign bit.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0])
//		{
//			CASE opCtl[1:0]
//			0: tmp[31:0] := (src1[31:0] <= src2[31:0]) ? src1[31:0] : src2[31:0]
//			1: tmp[31:0] := (src1[31:0] <= src2[31:0]) ? src2[31:0] : src1[31:0]
//			2: tmp[31:0] := (ABS(src1[31:0]) <= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
//			3: tmp[31:0] := (ABS(src1[31:0]) <= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
//			ESAC
//			
//			CASE signSelCtl[1:0]
//			0: dst[31:0] := (src1[31] << 31) OR (tmp[30:0])
//			1: dst[31:0] := tmp[63:0]
//			2: dst[31:0] := (0 << 31) OR (tmp[30:0])
//			3: dst[31:0] := (1 << 31) OR (tmp[30:0])
//			ESAC
//			
//			RETURN dst
//		}
//		
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := RANGE(a[i+31:i], b[i+31:i], imm8[1:0], imm8[3:2])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRANGEPS'. Intrinsic: '_mm512_range_round_ps'.
// Requires AVX512DQ.
func RangeRoundPs(a M512, b M512, imm8 int, rounding int) M512 {
	return M512(rangeRoundPs([16]float32(a), [16]float32(b), imm8, rounding))
}

func rangeRoundPs(a [16]float32, b [16]float32, imm8 int, rounding int) [16]float32


// MaskRcp14Pd: Compute the approximate reciprocal of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). The maximum relative error for this approximation is
// less than 2^-14. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := APPROXIMATE(1.0/a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRCP14PD'. Intrinsic: '_mm512_mask_rcp14_pd'.
// Requires AVX512F.
func MaskRcp14Pd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskRcp14Pd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskRcp14Pd(src [8]float64, k uint8, a [8]float64) [8]float64


// MaskzRcp14Pd: Compute the approximate reciprocal of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). The maximum relative error for this approximation is less than
// 2^-14. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := APPROXIMATE(1.0/a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRCP14PD'. Intrinsic: '_mm512_maskz_rcp14_pd'.
// Requires AVX512F.
func MaskzRcp14Pd(k Mmask8, a M512d) M512d {
	return M512d(maskzRcp14Pd(uint8(k), [8]float64(a)))
}

func maskzRcp14Pd(k uint8, a [8]float64) [8]float64


// Rcp14Pd: Compute the approximate reciprocal of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'. The
// maximum relative error for this approximation is less than 2^-14. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := APPROXIMATE(1.0/a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRCP14PD'. Intrinsic: '_mm512_rcp14_pd'.
// Requires AVX512F.
func Rcp14Pd(a M512d) M512d {
	return M512d(rcp14Pd([8]float64(a)))
}

func rcp14Pd(a [8]float64) [8]float64


// MaskRcp14Ps: Compute the approximate reciprocal of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). The maximum relative error for this approximation is
// less than 2^-14. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRCP14PS'. Intrinsic: '_mm512_mask_rcp14_ps'.
// Requires AVX512F.
func MaskRcp14Ps(src M512, k Mmask16, a M512) M512 {
	return M512(maskRcp14Ps([16]float32(src), uint16(k), [16]float32(a)))
}

func maskRcp14Ps(src [16]float32, k uint16, a [16]float32) [16]float32


// MaskzRcp14Ps: Compute the approximate reciprocal of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). The maximum relative error for this approximation is less than
// 2^-14. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRCP14PS'. Intrinsic: '_mm512_maskz_rcp14_ps'.
// Requires AVX512F.
func MaskzRcp14Ps(k Mmask16, a M512) M512 {
	return M512(maskzRcp14Ps(uint16(k), [16]float32(a)))
}

func maskzRcp14Ps(k uint16, a [16]float32) [16]float32


// Rcp14Ps: Compute the approximate reciprocal of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'. The
// maximum relative error for this approximation is less than 2^-14. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRCP14PS'. Intrinsic: '_mm512_rcp14_ps'.
// Requires AVX512F.
func Rcp14Ps(a M512) M512 {
	return M512(rcp14Ps([16]float32(a)))
}

func rcp14Ps(a [16]float32) [16]float32


// MaskRcp23Ps: Approximates the reciprocals of packed single-precision
// (32-bit) floating-point elements in 'a' to 23 bits of precision, storing the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRCP23PS'. Intrinsic: '_mm512_mask_rcp23_ps'.
// Requires KNCNI.
func MaskRcp23Ps(src M512, k Mmask16, a M512) M512 {
	return M512(maskRcp23Ps([16]float32(src), uint16(k), [16]float32(a)))
}

func maskRcp23Ps(src [16]float32, k uint16, a [16]float32) [16]float32


// Rcp23Ps: Approximates the reciprocals of packed single-precision (32-bit)
// floating-point elements in 'a' to 23 bits of precision, storing the results
// in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRCP23PS'. Intrinsic: '_mm512_rcp23_ps'.
// Requires KNCNI.
func Rcp23Ps(a M512) M512 {
	return M512(rcp23Ps([16]float32(a)))
}

func rcp23Ps(a [16]float32) [16]float32


// MaskRcp28Pd: Compute the approximate reciprocal of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). The maximum relative error for this approximation is
// less than 2^-28. 
//
//		FOR j := 0 to 7
//			i := j*64;
//			IF k[j] THEN
//				dst[i+63:i] := RCP_28_SP(1.0/a[i+63:i];
//			ELSE
//				dst[i+63:i] := src[i+63:i];
//			FI
//		ENDFOR;
//
// Instruction: 'VRCP28PD'. Intrinsic: '_mm512_mask_rcp28_pd'.
// Requires AVX512ER.
func MaskRcp28Pd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskRcp28Pd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskRcp28Pd(src [8]float64, k uint8, a [8]float64) [8]float64


// MaskzRcp28Pd: Compute the approximate reciprocal of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). The maximum relative error for this approximation is less than
// 2^-28. 
//
//		FOR j := 0 to 7
//			i := j*64;
//			IF k[j] THEN
//				dst[i+63:i] := RCP_28_SP(1.0/a[i+63:i];
//			ELSE
//				dst[i+63:i] := 0;
//			FI
//		ENDFOR;
//
// Instruction: 'VRCP28PD'. Intrinsic: '_mm512_maskz_rcp28_pd'.
// Requires AVX512ER.
func MaskzRcp28Pd(k Mmask8, a M512d) M512d {
	return M512d(maskzRcp28Pd(uint8(k), [8]float64(a)))
}

func maskzRcp28Pd(k uint8, a [8]float64) [8]float64


// Rcp28Pd: Compute the approximate reciprocal of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'. The
// maximum relative error for this approximation is less than 2^-28. 
//
//		FOR j := 0 to 7
//			i := j*64;
//			dst[i+63:i] := RCP_28_SP(1.0/a[i+63:i];
//		ENDFOR;
//
// Instruction: 'VRCP28PD'. Intrinsic: '_mm512_rcp28_pd'.
// Requires AVX512ER.
func Rcp28Pd(a M512d) M512d {
	return M512d(rcp28Pd([8]float64(a)))
}

func rcp28Pd(a [8]float64) [8]float64


// MaskRcp28Ps: Compute the approximate reciprocal of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). The maximum relative error for this approximation is
// less than 2^-28. 
//
//		FOR j := 0 to 15
//			i := j*32;
//			IF k[j] THEN
//				dst[i+31:i] := RCP_28_SP(1.0/a[i+31:i];
//			ELSE
//				dst[i+31:i] := src[i+31:i];
//			FI
//		ENDFOR;
//
// Instruction: 'VRCP28PS'. Intrinsic: '_mm512_mask_rcp28_ps'.
// Requires AVX512ER.
func MaskRcp28Ps(src M512, k Mmask16, a M512) M512 {
	return M512(maskRcp28Ps([16]float32(src), uint16(k), [16]float32(a)))
}

func maskRcp28Ps(src [16]float32, k uint16, a [16]float32) [16]float32


// MaskzRcp28Ps: Compute the approximate reciprocal of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). The maximum relative error for this approximation is less than
// 2^-28. 
//
//		FOR j := 0 to 15
//			i := j*32;
//			IF k[j] THEN
//				dst[i+31:i] := RCP_28_SP(1.0/a[i+31:i];
//			ELSE
//				dst[i+31:i] := 0;
//			FI
//		ENDFOR;
//
// Instruction: 'VRCP28PS'. Intrinsic: '_mm512_maskz_rcp28_ps'.
// Requires AVX512ER.
func MaskzRcp28Ps(k Mmask16, a M512) M512 {
	return M512(maskzRcp28Ps(uint16(k), [16]float32(a)))
}

func maskzRcp28Ps(k uint16, a [16]float32) [16]float32


// Rcp28Ps: Compute the approximate reciprocal of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'. The
// maximum relative error for this approximation is less than 2^-28. 
//
//		FOR j := 0 to 15
//			i := j*32;
//			dst[i+31:i] := RCP_28_SP(1.0/a[i+31:i];
//		ENDFOR;
//
// Instruction: 'VRCP28PS'. Intrinsic: '_mm512_rcp28_ps'.
// Requires AVX512ER.
func Rcp28Ps(a M512) M512 {
	return M512(rcp28Ps([16]float32(a)))
}

func rcp28Ps(a [16]float32) [16]float32


// MaskRcp28RoundPd: Compute the approximate reciprocal of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-28. Rounding is done according to the
// 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64;
//			IF k[j] THEN
//				dst[i+63:i] := RCP_28_SP(1.0/a[i+63:i];
//			ELSE
//				dst[i+63:i] := src[i+63:i];
//			FI
//		ENDFOR;
//
// Instruction: 'VRCP28PD'. Intrinsic: '_mm512_mask_rcp28_round_pd'.
// Requires AVX512ER.
func MaskRcp28RoundPd(src M512d, k Mmask8, a M512d, rounding int) M512d {
	return M512d(maskRcp28RoundPd([8]float64(src), uint8(k), [8]float64(a), rounding))
}

func maskRcp28RoundPd(src [8]float64, k uint8, a [8]float64, rounding int) [8]float64


// MaskzRcp28RoundPd: Compute the approximate reciprocal of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-28. Rounding is done according to the
// 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64;
//			IF k[j] THEN
//				dst[i+63:i] := RCP_28_SP(1.0/a[i+63:i];
//			ELSE
//				dst[i+63:i] := 0;
//			FI
//		ENDFOR;
//
// Instruction: 'VRCP28PD'. Intrinsic: '_mm512_maskz_rcp28_round_pd'.
// Requires AVX512ER.
func MaskzRcp28RoundPd(k Mmask8, a M512d, rounding int) M512d {
	return M512d(maskzRcp28RoundPd(uint8(k), [8]float64(a), rounding))
}

func maskzRcp28RoundPd(k uint8, a [8]float64, rounding int) [8]float64


// Rcp28RoundPd: Compute the approximate reciprocal of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'. The
// maximum relative error for this approximation is less than 2^-28. Rounding
// is done according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64;
//			dst[i+63:i] := RCP_28_SP(1.0/a[i+63:i];
//		ENDFOR;
//
// Instruction: 'VRCP28PD'. Intrinsic: '_mm512_rcp28_round_pd'.
// Requires AVX512ER.
func Rcp28RoundPd(a M512d, rounding int) M512d {
	return M512d(rcp28RoundPd([8]float64(a), rounding))
}

func rcp28RoundPd(a [8]float64, rounding int) [8]float64


// MaskRcp28RoundPs: Compute the approximate reciprocal of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-28. Rounding is done according to the
// 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32;
//			IF k[j] THEN
//				dst[i+31:i] := RCP_28_SP(1.0/a[i+31:i];
//			ELSE
//				dst[i+31:i] := src[i+31:i];
//			FI
//		ENDFOR;
//
// Instruction: 'VRCP28PS'. Intrinsic: '_mm512_mask_rcp28_round_ps'.
// Requires AVX512ER.
func MaskRcp28RoundPs(src M512, k Mmask16, a M512, rounding int) M512 {
	return M512(maskRcp28RoundPs([16]float32(src), uint16(k), [16]float32(a), rounding))
}

func maskRcp28RoundPs(src [16]float32, k uint16, a [16]float32, rounding int) [16]float32


// MaskzRcp28RoundPs: Compute the approximate reciprocal of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-28. Rounding is done according to the
// 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32;
//			IF k[j] THEN
//				dst[i+31:i] := RCP_28_SP(1.0/a[i+31:i];
//			ELSE
//				dst[i+31:i] := 0;
//			FI
//		ENDFOR;
//
// Instruction: 'VRCP28PS'. Intrinsic: '_mm512_maskz_rcp28_round_ps'.
// Requires AVX512ER.
func MaskzRcp28RoundPs(k Mmask16, a M512, rounding int) M512 {
	return M512(maskzRcp28RoundPs(uint16(k), [16]float32(a), rounding))
}

func maskzRcp28RoundPs(k uint16, a [16]float32, rounding int) [16]float32


// Rcp28RoundPs: Compute the approximate reciprocal of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'. The
// maximum relative error for this approximation is less than 2^-28. Rounding
// is done according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32;
//			dst[i+31:i] := RCP_28_SP(1.0/a[i+31:i];
//		ENDFOR;
//
// Instruction: 'VRCP28PS'. Intrinsic: '_mm512_rcp28_round_ps'.
// Requires AVX512ER.
func Rcp28RoundPs(a M512, rounding int) M512 {
	return M512(rcp28RoundPs([16]float32(a), rounding))
}

func rcp28RoundPs(a [16]float32, rounding int) [16]float32


// MaskRecipPd: Computes the reciprocal of packed double-precision (64-bit)
// floating-point elements in 'a', storing the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (1 / a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_recip_pd'.
// Requires AVX512F.
func MaskRecipPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskRecipPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskRecipPd(src [8]float64, k uint8, a [8]float64) [8]float64


// RecipPd: Computes the reciprocal of packed double-precision (64-bit)
// floating-point elements in 'a', storing the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := (1 / a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_recip_pd'.
// Requires AVX512F.
func RecipPd(a M512d) M512d {
	return M512d(recipPd([8]float64(a)))
}

func recipPd(a [8]float64) [8]float64


// MaskRecipPs: Computes the reciprocal of packed single-precision (32-bit)
// floating-point elements in 'a', storing the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (1 / a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_recip_ps'.
// Requires AVX512F.
func MaskRecipPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskRecipPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskRecipPs(src [16]float32, k uint16, a [16]float32) [16]float32


// RecipPs: Computes the reciprocal of packed single-precision (32-bit)
// floating-point elements in 'a', storing the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := (1 / a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_recip_ps'.
// Requires AVX512F.
func RecipPs(a M512) M512 {
	return M512(recipPs([16]float32(a)))
}

func recipPs(a [16]float32) [16]float32


// MaskReduceAddEpi32: Reduce the packed 32-bit integers in 'a' by addition
// using mask 'k'. Returns the sum of all active elements in 'a'. 
//
//		sum[31:0] := 0
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				sum[31:0] := sum[31:0] + a[i+31:i]
//			FI
//		ENDFOR
//		RETURN sum[31:0]
//
// Instruction: '...'. Intrinsic: '_mm512_mask_reduce_add_epi32'.
// Requires KNCNI.
func MaskReduceAddEpi32(k Mmask16, a M512i) int {
	return int(maskReduceAddEpi32(uint16(k), [64]byte(a)))
}

func maskReduceAddEpi32(k uint16, a [64]byte) int


// ReduceAddEpi32: Reduce the packed 32-bit integers in 'a' by addition.
// Returns the sum of all elements in 'a'. 
//
//		sum[31:0] := 0
//		FOR j := 0 to 15
//			i := j*32
//			sum[31:0] := sum[31:0] + a[i+31:i]
//		ENDFOR
//		RETURN sum[63:0]
//
// Instruction: '...'. Intrinsic: '_mm512_reduce_add_epi32'.
// Requires KNCNI.
func ReduceAddEpi32(a M512i) int {
	return int(reduceAddEpi32([64]byte(a)))
}

func reduceAddEpi32(a [64]byte) int


// MaskReduceAddEpi64: Reduce the packed 64-bit integers in 'a' by addition
// using mask 'k'. Returns the sum of all active elements in 'a'. 
//
//		sum[63:0] := 0
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				sum[63:0] := sum[63:0] + a[i+63:i]
//			FI
//		ENDFOR
//		RETURN sum[63:0]
//
// Instruction: '...'. Intrinsic: '_mm512_mask_reduce_add_epi64'.
// Requires KNCNI.
func MaskReduceAddEpi64(k Mmask8, a M512i) int64 {
	return int64(maskReduceAddEpi64(uint8(k), [64]byte(a)))
}

func maskReduceAddEpi64(k uint8, a [64]byte) int64


// ReduceAddEpi64: Reduce the packed 64-bit integers in 'a' by addition.
// Returns the sum of all elements in 'a'. 
//
//		sum[63:0] := 0
//		FOR j := 0 to 7
//			i := j*64
//			sum[63:0] := sum[63:0] + a[i+63:i]
//		ENDFOR
//		RETURN sum[63:0]
//
// Instruction: '...'. Intrinsic: '_mm512_reduce_add_epi64'.
// Requires KNCNI.
func ReduceAddEpi64(a M512i) int64 {
	return int64(reduceAddEpi64([64]byte(a)))
}

func reduceAddEpi64(a [64]byte) int64


// MaskReduceAddPd: Reduce the packed double-precision (64-bit) floating-point
// elements in 'a' by addition using mask 'k'. Returns the sum of all active
// elements in 'a'. 
//
//		sum[63:0] := 0
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				sum[63:0] := sum[63:0] + a[i+63:i]
//			FI
//		ENDFOR
//		RETURN sum[63:0]
//
// Instruction: '...'. Intrinsic: '_mm512_mask_reduce_add_pd'.
// Requires KNCNI.
func MaskReduceAddPd(k Mmask8, a M512d) float64 {
	return float64(maskReduceAddPd(uint8(k), [8]float64(a)))
}

func maskReduceAddPd(k uint8, a [8]float64) float64


// ReduceAddPd: Reduce the packed double-precision (64-bit) floating-point
// elements in 'a' by addition. Returns the sum of all elements in 'a'. 
//
//		sum[63:0] := 0
//		FOR j := 0 to 7
//			i := j*64
//			sum[63:0] := sum[63:0] + a[i+63:i]
//		ENDFOR
//		RETURN sum[63:0]
//
// Instruction: '...'. Intrinsic: '_mm512_reduce_add_pd'.
// Requires KNCNI.
func ReduceAddPd(a M512d) float64 {
	return float64(reduceAddPd([8]float64(a)))
}

func reduceAddPd(a [8]float64) float64


// MaskReduceAddPs: Reduce the packed single-precision (32-bit) floating-point
// elements in 'a' by addition using mask 'k'. Returns the sum of all active
// elements in 'a'. 
//
//		sum[31:0] := 0
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				sum[31:0] := sum[31:0] + a[i+31:i]
//			FI
//		ENDFOR
//		RETURN sum[31:0]
//
// Instruction: '...'. Intrinsic: '_mm512_mask_reduce_add_ps'.
// Requires KNCNI.
func MaskReduceAddPs(k Mmask16, a M512) float32 {
	return float32(maskReduceAddPs(uint16(k), [16]float32(a)))
}

func maskReduceAddPs(k uint16, a [16]float32) float32


// ReduceAddPs: Reduce the packed single-precision (32-bit) floating-point
// elements in 'a' by addition. Returns the sum of all elements in 'a'. 
//
//		sum[31:0] := 0
//		FOR j := 0 to 15
//			i := j*32
//			sum[31:0] := sum[31:0] + a[i+31:i]
//		ENDFOR
//		RETURN sum[63:0]
//
// Instruction: '...'. Intrinsic: '_mm512_reduce_add_ps'.
// Requires KNCNI.
func ReduceAddPs(a M512) float32 {
	return float32(reduceAddPs([16]float32(a)))
}

func reduceAddPs(a [16]float32) float32


// MaskReduceAndEpi32: Reduce the packed 32-bit integers in 'a' by bitwise AND
// using mask 'k'. Returns the bitwise AND of all active elements in 'a'. 
//
//		reduced[31:0] := 0xFFFFFFFF
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				reduced[31:0] := reduced[31:0] AND a[i+31:i]
//			FI
//		ENDFOR
//		RETURN reduced[31:0]
//
// Instruction: '...'. Intrinsic: '_mm512_mask_reduce_and_epi32'.
// Requires KNCNI.
func MaskReduceAndEpi32(k Mmask16, a M512i) int {
	return int(maskReduceAndEpi32(uint16(k), [64]byte(a)))
}

func maskReduceAndEpi32(k uint16, a [64]byte) int


// ReduceAndEpi32: Reduce the packed 32-bit integers in 'a' by bitwise AND.
// Returns the bitwise AND of all elements in 'a'. 
//
//		reduced[31:0] := 0xFFFFFFFF
//		FOR j := 0 to 15
//			i := j*32
//			reduced[31:0] := reduced[31:0] AND a[i+31:i]
//		ENDFOR
//		RETURN reduced[31:0]
//
// Instruction: '...'. Intrinsic: '_mm512_reduce_and_epi32'.
// Requires KNCNI.
func ReduceAndEpi32(a M512i) int {
	return int(reduceAndEpi32([64]byte(a)))
}

func reduceAndEpi32(a [64]byte) int


// MaskReduceAndEpi64: Reduce the packed 64-bit integers in 'a' by bitwise AND
// using mask 'k'. Returns the bitwise AND of all active elements in 'a'. 
//
//		reduced[63:0] := 0xFFFFFFFFFFFFFFFF
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				reduced[63:0] := reduced[63:0] AND a[i+63:i]
//			FI
//		ENDFOR
//		RETURN reduced[63:0]
//
// Instruction: '...'. Intrinsic: '_mm512_mask_reduce_and_epi64'.
// Requires KNCNI.
func MaskReduceAndEpi64(k Mmask8, a M512i) int64 {
	return int64(maskReduceAndEpi64(uint8(k), [64]byte(a)))
}

func maskReduceAndEpi64(k uint8, a [64]byte) int64


// ReduceAndEpi64: Reduce the packed 64-bit integers in 'a' by bitwise AND.
// Returns the bitwise AND of all elements in 'a'. 
//
//		reduced[63:0] := 0xFFFFFFFFFFFFFFFF
//		FOR j := 0 to 7
//			i := j*64
//			reduced[63:0] := reduced[63:0] AND a[i+63:i]
//		ENDFOR
//		RETURN reduced[63:0]
//
// Instruction: '...'. Intrinsic: '_mm512_reduce_and_epi64'.
// Requires KNCNI.
func ReduceAndEpi64(a M512i) int64 {
	return int64(reduceAndEpi64([64]byte(a)))
}

func reduceAndEpi64(a [64]byte) int64


// MaskReduceGmaxPd: Determines the maximum element of the packed
// double-precision (64-bit) floating-point elements stored in 'a' and stores
// the result in 'dst'. Bitmask 'k' is used to exclude certain elements
// (elements are ignored when the corresponding mask bit is not set). 
//
//		max = a[63:0]
//		FOR j := 1 to 7
//			i := j*64
//			IF k[j]
//				CONTINUE
//			ELSE
//				dst = FpMax(max, a[i+63:i])
//			FI
//		ENDFOR
//		dst := max
//
// Instruction: '...'. Intrinsic: '_mm512_mask_reduce_gmax_pd'.
// Requires KNCNI.
func MaskReduceGmaxPd(k Mmask8, a M512d) float64 {
	return float64(maskReduceGmaxPd(uint8(k), [8]float64(a)))
}

func maskReduceGmaxPd(k uint8, a [8]float64) float64


// ReduceGmaxPd: Determines the maximum element of the packed double-precision
// (64-bit) floating-point elements stored in 'a' and stores the result in
// 'dst'. 
//
//		max = a[63:0]
//		FOR j := 1 to 7
//			i := j*64
//			dst = FpMax(max, a[i+63:i])
//		ENDFOR
//		dst := max
//
// Instruction: '...'. Intrinsic: '_mm512_reduce_gmax_pd'.
// Requires KNCNI.
func ReduceGmaxPd(a M512d) float64 {
	return float64(reduceGmaxPd([8]float64(a)))
}

func reduceGmaxPd(a [8]float64) float64


// MaskReduceGmaxPs: Determines the maximum element of the packed
// single-precision (32-bit) floating-point elements stored in 'a' and stores
// the result in 'dst'. Bitmask 'k' is used to exclude certain elements
// (elements are ignored when the corresponding mask bit is not set). 
//
//		max = a[31:0]
//		FOR j := 1 to 15
//			i := j*32
//			IF k[j]
//				CONTINUE
//			ELSE
//				dst = FpMax(max, a[i+31:i])
//			FI
//		ENDFOR
//		dst := max
//
// Instruction: '...'. Intrinsic: '_mm512_mask_reduce_gmax_ps'.
// Requires KNCNI.
func MaskReduceGmaxPs(k Mmask16, a M512) float32 {
	return float32(maskReduceGmaxPs(uint16(k), [16]float32(a)))
}

func maskReduceGmaxPs(k uint16, a [16]float32) float32


// ReduceGmaxPs: Determines the maximum element of the packed single-precision
// (32-bit) floating-point elements stored in 'a' and stores the result in
// 'dst'. 
//
//		max = a[31:0]
//		FOR j := 1 to 15
//			i := j*32
//			dst = FpMax(max, a[i+31:i])
//		ENDFOR
//		dst := max
//
// Instruction: '...'. Intrinsic: '_mm512_reduce_gmax_ps'.
// Requires KNCNI.
func ReduceGmaxPs(a M512) float32 {
	return float32(reduceGmaxPs([16]float32(a)))
}

func reduceGmaxPs(a [16]float32) float32


// MaskReduceGminPd: Determines the minimum element of the packed
// double-precision (64-bit) floating-point elements stored in 'a' and stores
// the result in 'dst'. Bitmask 'k' is used to exclude certain elements
// (elements are ignored when the corresponding mask bit is not set). 
//
//		min = a[63:0]
//		FOR j := 1 to 7
//			i := j*64
//			IF k[j]
//				CONTINUE
//			ELSE
//				dst = FpMin(min, a[i+63:i])
//			FI
//		ENDFOR
//		dst := min
//
// Instruction: '...'. Intrinsic: '_mm512_mask_reduce_gmin_pd'.
// Requires KNCNI.
func MaskReduceGminPd(k Mmask8, a M512d) float64 {
	return float64(maskReduceGminPd(uint8(k), [8]float64(a)))
}

func maskReduceGminPd(k uint8, a [8]float64) float64


// ReduceGminPd: Determines the minimum element of the packed double-precision
// (64-bit) floating-point elements stored in 'a' and stores the result in
// 'dst'. 
//
//		min = a[63:0]
//		FOR j := 1 to 7
//			i := j*64
//			dst = FpMin(min, a[i+63:i])
//		ENDFOR
//		dst := min
//
// Instruction: '...'. Intrinsic: '_mm512_reduce_gmin_pd'.
// Requires KNCNI.
func ReduceGminPd(a M512d) float64 {
	return float64(reduceGminPd([8]float64(a)))
}

func reduceGminPd(a [8]float64) float64


// MaskReduceGminPs: Determines the minimum element of the packed
// single-precision (32-bit) floating-point elements stored in 'a' and stores
// the result in 'dst' using writemask 'k' (elements are ignored when the
// corresponding mask bit is not set). 
//
//		min = a[31:0]
//		FOR j := 1 to 15
//			i := j*32
//			IF k[j]
//				CONTINUE
//			ELSE
//				dst = FpMin(min, a[i+31:i])
//			FI
//		ENDFOR
//		dst := min
//
// Instruction: '...'. Intrinsic: '_mm512_mask_reduce_gmin_ps'.
// Requires KNCNI.
func MaskReduceGminPs(k Mmask16, a M512) float32 {
	return float32(maskReduceGminPs(uint16(k), [16]float32(a)))
}

func maskReduceGminPs(k uint16, a [16]float32) float32


// ReduceGminPs: Determines the minimum element of the packed single-precision
// (32-bit) floating-point elements stored in 'a' and stores the result in
// 'dst'. 
//
//		min = a[31:0]
//		FOR j := 1 to 15
//			i := j*32
//			dst = FpMin(min, a[i+31:i])
//		ENDFOR
//		dst := min
//
// Instruction: '...'. Intrinsic: '_mm512_reduce_gmin_ps'.
// Requires KNCNI.
func ReduceGminPs(a M512) float32 {
	return float32(reduceGminPs([16]float32(a)))
}

func reduceGminPs(a [16]float32) float32


// MaskReduceMaxEpi32: Reduce the packed 32-bit integers in 'a' by maximum
// using mask 'k'. Returns the maximum of all active elements in 'a'. 
//
//		max[31:0] := MIN_INT
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				max[31:0] := MAXIMUM(max[31:0], a[i+31:i])
//			FI
//		ENDFOR
//		RETURN max[31:0]
//
// Instruction: '...'. Intrinsic: '_mm512_mask_reduce_max_epi32'.
// Requires KNCNI.
func MaskReduceMaxEpi32(k Mmask16, a M512i) int {
	return int(maskReduceMaxEpi32(uint16(k), [64]byte(a)))
}

func maskReduceMaxEpi32(k uint16, a [64]byte) int


// ReduceMaxEpi32: Reduce the packed 32-bit integers in 'a' by maximum. Returns
// the maximum of all elements in 'a'. 
//
//		max[31:0] := MIN_INT
//		FOR j := 0 to 15
//			i := j*32
//			max[31:0] := MAXIMUM(max[31:0], a[i+31:i])
//		ENDFOR
//		RETURN max[31:0]
//
// Instruction: '...'. Intrinsic: '_mm512_reduce_max_epi32'.
// Requires KNCNI.
func ReduceMaxEpi32(a M512i) int {
	return int(reduceMaxEpi32([64]byte(a)))
}

func reduceMaxEpi32(a [64]byte) int


// MaskReduceMaxEpi64: Reduce the packed 64-bit integers in 'a' by maximum
// using mask 'k'. Returns the maximum of all active elements in 'a'. 
//
//		max[63:0] := MIN_INT
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				max[63:0] := MAXIMUM(max[63:0], a[i+63:i])
//			FI
//		ENDFOR
//		RETURN max[63:0]
//
// Instruction: '...'. Intrinsic: '_mm512_mask_reduce_max_epi64'.
// Requires KNCNI.
func MaskReduceMaxEpi64(k Mmask8, a M512i) int64 {
	return int64(maskReduceMaxEpi64(uint8(k), [64]byte(a)))
}

func maskReduceMaxEpi64(k uint8, a [64]byte) int64


// ReduceMaxEpi64: Reduce the packed 64-bit integers in 'a' by maximum. Returns
// the maximum of all elements in 'a'. 
//
//		max[63:0] := MIN_INT
//		FOR j := 0 to 7
//			i := j*64
//			max[63:0] := MAXIMUM(max[63:0], a[i+63:i])
//		ENDFOR
//		RETURN max[63:0]
//
// Instruction: '...'. Intrinsic: '_mm512_reduce_max_epi64'.
// Requires KNCNI.
func ReduceMaxEpi64(a M512i) int64 {
	return int64(reduceMaxEpi64([64]byte(a)))
}

func reduceMaxEpi64(a [64]byte) int64


// MaskReduceMaxEpu32: Reduce the packed unsigned 32-bit integers in 'a' by
// maximum using mask 'k'. Returns the maximum of all active elements in 'a'. 
//
//		max[31:0] := 0
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				max[31:0] := MAXIMUM(max[31:0], a[i+31:i])
//			FI
//		ENDFOR
//		RETURN max[31:0]
//
// Instruction: '...'. Intrinsic: '_mm512_mask_reduce_max_epu32'.
// Requires KNCNI.
func MaskReduceMaxEpu32(k Mmask16, a M512i) uint32 {
	return uint32(maskReduceMaxEpu32(uint16(k), [64]byte(a)))
}

func maskReduceMaxEpu32(k uint16, a [64]byte) uint32


// ReduceMaxEpu32: Reduce the packed unsigned 32-bit integers in 'a' by
// maximum. Returns the maximum of all elements in 'a'. 
//
//		max[31:0] := 0
//		FOR j := 0 to 15
//			i := j*32
//			max[31:0] := MAXIMUM(max[31:0], a[i+31:i])
//		ENDFOR
//		RETURN max[31:0]
//
// Instruction: '...'. Intrinsic: '_mm512_reduce_max_epu32'.
// Requires KNCNI.
func ReduceMaxEpu32(a M512i) uint32 {
	return uint32(reduceMaxEpu32([64]byte(a)))
}

func reduceMaxEpu32(a [64]byte) uint32


// MaskReduceMaxEpu64: Reduce the packed unsigned 64-bit integers in 'a' by
// maximum using mask 'k'. Returns the maximum of all active elements in 'a'. 
//
//		max[63:0] := 0
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				max[63:0] := MAXIMUM(max[63:0], a[i+63:i])
//			FI
//		ENDFOR
//		RETURN max[63:0]
//
// Instruction: '...'. Intrinsic: '_mm512_mask_reduce_max_epu64'.
// Requires KNCNI.
func MaskReduceMaxEpu64(k Mmask8, a M512i) uint64 {
	return uint64(maskReduceMaxEpu64(uint8(k), [64]byte(a)))
}

func maskReduceMaxEpu64(k uint8, a [64]byte) uint64


// ReduceMaxEpu64: Reduce the packed unsigned 64-bit integers in 'a' by
// maximum. Returns the maximum of all elements in 'a'. 
//
//		max[63:0] := 0
//		FOR j := 0 to 7
//			i := j*64
//			max[63:0] := MAXIMUM(max[63:0], a[i+63:i])
//		ENDFOR
//		RETURN max[63:0]
//
// Instruction: '...'. Intrinsic: '_mm512_reduce_max_epu64'.
// Requires KNCNI.
func ReduceMaxEpu64(a M512i) uint64 {
	return uint64(reduceMaxEpu64([64]byte(a)))
}

func reduceMaxEpu64(a [64]byte) uint64


// MaskReduceMaxPd: Reduce the packed double-precision (64-bit) floating-point
// elements in 'a' by maximum using mask 'k'. Returns the maximum of all active
// elements in 'a'. 
//
//		max[63:0] := MIN_DOUBLE
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				max[63:0] := MAXIMUM(max[63:0], a[i+63:i])
//			FI
//		ENDFOR
//		RETURN max[63:0]
//
// Instruction: '...'. Intrinsic: '_mm512_mask_reduce_max_pd'.
// Requires KNCNI.
func MaskReduceMaxPd(k Mmask8, a M512d) float64 {
	return float64(maskReduceMaxPd(uint8(k), [8]float64(a)))
}

func maskReduceMaxPd(k uint8, a [8]float64) float64


// ReduceMaxPd: Reduce the packed double-precision (64-bit) floating-point
// elements in 'a' by maximum. Returns the maximum of all elements in 'a'. 
//
//		max[63:0] := MIN_DOUBLE
//		FOR j := 0 to 7
//			i := j*64
//			max[63:0] := MAXIMUM(max[63:0], a[i+63:i])
//		ENDFOR
//		RETURN max[63:0]
//
// Instruction: '...'. Intrinsic: '_mm512_reduce_max_pd'.
// Requires KNCNI.
func ReduceMaxPd(a M512d) float64 {
	return float64(reduceMaxPd([8]float64(a)))
}

func reduceMaxPd(a [8]float64) float64


// MaskReduceMaxPs: Reduce the packed single-precision (32-bit) floating-point
// elements in 'a' by maximum using mask 'k'. Returns the maximum of all active
// elements in 'a'. 
//
//		max[31:0] := MIN_FLOAT
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				max[31:0] := MAXIMUM(max[31:0], a[i+31:i])
//			FI
//		ENDFOR
//		RETURN max[31:0]
//
// Instruction: '...'. Intrinsic: '_mm512_mask_reduce_max_ps'.
// Requires KNCNI.
func MaskReduceMaxPs(k Mmask16, a M512) float32 {
	return float32(maskReduceMaxPs(uint16(k), [16]float32(a)))
}

func maskReduceMaxPs(k uint16, a [16]float32) float32


// ReduceMaxPs: Reduce the packed single-precision (32-bit) floating-point
// elements in 'a' by maximum. Returns the maximum of all elements in 'a'. 
//
//		max[31:0] := MIN_FLOAT
//		FOR j := 0 to 15
//			i := j*32
//			max[31:0] := MAXIMUM(max[31:0], a[i+31:i])
//		ENDFOR
//		RETURN max[31:0]
//
// Instruction: '...'. Intrinsic: '_mm512_reduce_max_ps'.
// Requires KNCNI.
func ReduceMaxPs(a M512) float32 {
	return float32(reduceMaxPs([16]float32(a)))
}

func reduceMaxPs(a [16]float32) float32


// MaskReduceMinEpi32: Reduce the packed 32-bit integers in 'a' by maximum
// using mask 'k'. Returns the minimum of all active elements in 'a'. 
//
//		min[31:0] := MAX_INT
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				min[31:0] := MINIMUM(min[31:0], a[i+31:i])
//			FI
//		ENDFOR
//		RETURN min[31:0]
//
// Instruction: '...'. Intrinsic: '_mm512_mask_reduce_min_epi32'.
// Requires KNCNI.
func MaskReduceMinEpi32(k Mmask16, a M512i) int {
	return int(maskReduceMinEpi32(uint16(k), [64]byte(a)))
}

func maskReduceMinEpi32(k uint16, a [64]byte) int


// ReduceMinEpi32: Reduce the packed 32-bit integers in 'a' by minimum. Returns
// the minimum of all elements in 'a'. 
//
//		min[31:0] := MAX_INT
//		FOR j := 0 to 15
//			i := j*32
//			min[31:0] := MINIMUM(min[31:0], a[i+31:i])
//		ENDFOR
//		RETURN min[31:0]
//
// Instruction: '...'. Intrinsic: '_mm512_reduce_min_epi32'.
// Requires KNCNI.
func ReduceMinEpi32(a M512i) int {
	return int(reduceMinEpi32([64]byte(a)))
}

func reduceMinEpi32(a [64]byte) int


// MaskReduceMinEpi64: Reduce the packed 64-bit integers in 'a' by maximum
// using mask 'k'. Returns the minimum of all active elements in 'a'. 
//
//		min[63:0] := MAX_INT
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				min[63:0] := MINIMUM(min[63:0], a[i+63:i])
//			FI
//		ENDFOR
//		RETURN min[63:0]
//
// Instruction: '...'. Intrinsic: '_mm512_mask_reduce_min_epi64'.
// Requires KNCNI.
func MaskReduceMinEpi64(k Mmask8, a M512i) int64 {
	return int64(maskReduceMinEpi64(uint8(k), [64]byte(a)))
}

func maskReduceMinEpi64(k uint8, a [64]byte) int64


// ReduceMinEpi64: Reduce the packed 64-bit integers in 'a' by minimum. Returns
// the minimum of all elements in 'a'. 
//
//		min[63:0] := MAX_INT
//		FOR j := 0 to 7
//			i := j*64
//			min[63:0] := MINIMUM(min[63:0], a[i+63:i])
//		ENDFOR
//		RETURN min[63:0]
//
// Instruction: '...'. Intrinsic: '_mm512_reduce_min_epi64'.
// Requires KNCNI.
func ReduceMinEpi64(a M512i) int64 {
	return int64(reduceMinEpi64([64]byte(a)))
}

func reduceMinEpi64(a [64]byte) int64


// MaskReduceMinEpu32: Reduce the packed unsigned 32-bit integers in 'a' by
// maximum using mask 'k'. Returns the minimum of all active elements in 'a'. 
//
//		min[31:0] := MAX_UINT
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				min[31:0] := MINIMUM(min[31:0], a[i+31:i])
//			FI
//		ENDFOR
//		RETURN min[31:0]
//
// Instruction: '...'. Intrinsic: '_mm512_mask_reduce_min_epu32'.
// Requires KNCNI.
func MaskReduceMinEpu32(k Mmask16, a M512i) uint32 {
	return uint32(maskReduceMinEpu32(uint16(k), [64]byte(a)))
}

func maskReduceMinEpu32(k uint16, a [64]byte) uint32


// ReduceMinEpu32: Reduce the packed unsigned 32-bit integers in 'a' by
// minimum. Returns the minimum of all elements in 'a'. 
//
//		min[31:0] := MAX_UINT
//		FOR j := 0 to 15
//			i := j*32
//			min[31:0] := MINIMUM(min[31:0], a[i+31:i])
//		ENDFOR
//		RETURN min[31:0]
//
// Instruction: '...'. Intrinsic: '_mm512_reduce_min_epu32'.
// Requires KNCNI.
func ReduceMinEpu32(a M512i) uint32 {
	return uint32(reduceMinEpu32([64]byte(a)))
}

func reduceMinEpu32(a [64]byte) uint32


// MaskReduceMinEpu64: Reduce the packed unsigned 64-bit integers in 'a' by
// minimum using mask 'k'. Returns the minimum of all active elements in 'a'. 
//
//		min[63:0] := MAX_UINT
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				min[63:0] := MINIMUM(min[63:0], a[i+63:i])
//			FI
//		ENDFOR
//		RETURN min[63:0]
//
// Instruction: '...'. Intrinsic: '_mm512_mask_reduce_min_epu64'.
// Requires KNCNI.
func MaskReduceMinEpu64(k Mmask8, a M512i) uint64 {
	return uint64(maskReduceMinEpu64(uint8(k), [64]byte(a)))
}

func maskReduceMinEpu64(k uint8, a [64]byte) uint64


// ReduceMinEpu64: Reduce the packed unsigned 64-bit integers in 'a' by
// minimum. Returns the minimum of all elements in 'a'. 
//
//		min[63:0] := MAX_UINT
//		FOR j := 0 to 7
//			i := j*64
//			min[63:0] := MINIMUM(min[63:0], a[i+63:i])
//		ENDFOR
//		RETURN min[63:0]
//
// Instruction: '...'. Intrinsic: '_mm512_reduce_min_epu64'.
// Requires KNCNI.
func ReduceMinEpu64(a M512i) uint64 {
	return uint64(reduceMinEpu64([64]byte(a)))
}

func reduceMinEpu64(a [64]byte) uint64


// MaskReduceMinPd: Reduce the packed double-precision (64-bit) floating-point
// elements in 'a' by maximum using mask 'k'. Returns the minimum of all active
// elements in 'a'. 
//
//		min[63:0] := MAX_DOUBLE
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				min[63:0] := MINIMUM(min[63:0], a[i+63:i])
//			FI
//		ENDFOR
//		RETURN min[63:0]
//
// Instruction: '...'. Intrinsic: '_mm512_mask_reduce_min_pd'.
// Requires KNCNI.
func MaskReduceMinPd(k Mmask8, a M512d) float64 {
	return float64(maskReduceMinPd(uint8(k), [8]float64(a)))
}

func maskReduceMinPd(k uint8, a [8]float64) float64


// ReduceMinPd: Reduce the packed double-precision (64-bit) floating-point
// elements in 'a' by minimum. Returns the minimum of all elements in 'a'. 
//
//		min[63:0] := MAX_DOUBLE
//		FOR j := 0 to 7
//			i := j*64
//			min[63:0] := MINIMUM(min[63:0], a[i+63:i])
//		ENDFOR
//		RETURN min[63:0]
//
// Instruction: '...'. Intrinsic: '_mm512_reduce_min_pd'.
// Requires KNCNI.
func ReduceMinPd(a M512d) float64 {
	return float64(reduceMinPd([8]float64(a)))
}

func reduceMinPd(a [8]float64) float64


// MaskReduceMinPs: Reduce the packed single-precision (32-bit) floating-point
// elements in 'a' by maximum using mask 'k'. Returns the minimum of all active
// elements in 'a'. 
//
//		min[31:0] := MAX_FLOAT
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				min[31:0] := MINIMUM(min[31:0], a[i+31:i])
//			FI
//		ENDFOR
//		RETURN min[31:0]
//
// Instruction: '...'. Intrinsic: '_mm512_mask_reduce_min_ps'.
// Requires KNCNI.
func MaskReduceMinPs(k Mmask16, a M512) float32 {
	return float32(maskReduceMinPs(uint16(k), [16]float32(a)))
}

func maskReduceMinPs(k uint16, a [16]float32) float32


// ReduceMinPs: Reduce the packed single-precision (32-bit) floating-point
// elements in 'a' by minimum. Returns the minimum of all elements in 'a'. 
//
//		min[31:0] := MAX_INT
//		FOR j := 0 to 15
//			i := j*32
//			min[31:0] := MINIMUM(min[31:0], a[i+31:i])
//		ENDFOR
//		RETURN min[31:0]
//
// Instruction: '...'. Intrinsic: '_mm512_reduce_min_ps'.
// Requires KNCNI.
func ReduceMinPs(a M512) float32 {
	return float32(reduceMinPs([16]float32(a)))
}

func reduceMinPs(a [16]float32) float32


// MaskReduceMulEpi32: Reduce the packed 32-bit integers in 'a' by
// multiplication using mask 'k'. Returns the product of all active elements in
// 'a'. 
//
//		prod[31:0] := 1
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				prod[31:0] := prod[31:0] * a[i+31:i]
//			FI
//		ENDFOR
//		RETURN prod[31:0]
//
// Instruction: '...'. Intrinsic: '_mm512_mask_reduce_mul_epi32'.
// Requires KNCNI.
func MaskReduceMulEpi32(k Mmask16, a M512i) int {
	return int(maskReduceMulEpi32(uint16(k), [64]byte(a)))
}

func maskReduceMulEpi32(k uint16, a [64]byte) int


// ReduceMulEpi32: Reduce the packed 32-bit integers in 'a' by multiplication.
// Returns the product of all elements in 'a'. 
//
//		prod[31:0] := 1
//		FOR j := 0 to 15
//			i := j*32
//			prod[31:0] := prod[31:0] * a[i+31:i]
//		ENDFOR
//		RETURN prod[31:0]
//
// Instruction: '...'. Intrinsic: '_mm512_reduce_mul_epi32'.
// Requires KNCNI.
func ReduceMulEpi32(a M512i) int {
	return int(reduceMulEpi32([64]byte(a)))
}

func reduceMulEpi32(a [64]byte) int


// MaskReduceMulEpi64: Reduce the packed 64-bit integers in 'a' by
// multiplication using mask 'k'. Returns the product of all active elements in
// 'a'. 
//
//		prod[63:0] := 1
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				prod[63:0] := prod[63:0] * a[i+63:i]
//			FI
//		ENDFOR
//		RETURN prod[63:0]
//
// Instruction: '...'. Intrinsic: '_mm512_mask_reduce_mul_epi64'.
// Requires KNCNI.
func MaskReduceMulEpi64(k Mmask8, a M512i) int64 {
	return int64(maskReduceMulEpi64(uint8(k), [64]byte(a)))
}

func maskReduceMulEpi64(k uint8, a [64]byte) int64


// ReduceMulEpi64: Reduce the packed 64-bit integers in 'a' by multiplication.
// Returns the product of all elements in 'a'. 
//
//		prod[63:0] := 1
//		FOR j := 0 to 7
//			i := j*64
//			prod[63:0] := prod[63:0] * a[i+63:i]
//		ENDFOR
//		RETURN prod[63:0]
//
// Instruction: '...'. Intrinsic: '_mm512_reduce_mul_epi64'.
// Requires KNCNI.
func ReduceMulEpi64(a M512i) int64 {
	return int64(reduceMulEpi64([64]byte(a)))
}

func reduceMulEpi64(a [64]byte) int64


// MaskReduceMulPd: Reduce the packed double-precision (64-bit) floating-point
// elements in 'a' by multiplication using mask 'k'. Returns the product of all
// active elements in 'a'. 
//
//		prod[63:0] := 1
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				prod[63:0] := prod[63:0] * a[i+63:i]
//			FI
//		ENDFOR
//		RETURN prod[63:0]
//
// Instruction: '...'. Intrinsic: '_mm512_mask_reduce_mul_pd'.
// Requires KNCNI.
func MaskReduceMulPd(k Mmask8, a M512d) float64 {
	return float64(maskReduceMulPd(uint8(k), [8]float64(a)))
}

func maskReduceMulPd(k uint8, a [8]float64) float64


// ReduceMulPd: Reduce the packed double-precision (64-bit) floating-point
// elements in 'a' by multiplication. Returns the product of all elements in
// 'a'. 
//
//		prod[63:0] := 1
//		FOR j := 0 to 7
//			i := j*64
//			prod[63:0] := prod[63:0] * a[i+63:i]
//		ENDFOR
//		RETURN prod[63:0]
//
// Instruction: '...'. Intrinsic: '_mm512_reduce_mul_pd'.
// Requires KNCNI.
func ReduceMulPd(a M512d) float64 {
	return float64(reduceMulPd([8]float64(a)))
}

func reduceMulPd(a [8]float64) float64


// MaskReduceMulPs: Reduce the packed single-precision (32-bit) floating-point
// elements in 'a' by multiplication using mask 'k'. Returns the product of all
// active elements in 'a'. 
//
//		prod[31:0] := 1
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				prod[31:0] := prod[31:0] * a[i+31:i]
//			FI
//		ENDFOR
//		RETURN prod[31:0]
//
// Instruction: '...'. Intrinsic: '_mm512_mask_reduce_mul_ps'.
// Requires KNCNI.
func MaskReduceMulPs(k Mmask16, a M512) float32 {
	return float32(maskReduceMulPs(uint16(k), [16]float32(a)))
}

func maskReduceMulPs(k uint16, a [16]float32) float32


// ReduceMulPs: Reduce the packed single-precision (32-bit) floating-point
// elements in 'a' by multiplication. Returns the product of all elements in
// 'a'. 
//
//		prod[31:0] := 1
//		FOR j := 0 to 15
//			i := j*32
//			prod[31:0] := prod[31:0] * a[i+31:i]
//		ENDFOR
//		RETURN prod[31:0]
//
// Instruction: '...'. Intrinsic: '_mm512_reduce_mul_ps'.
// Requires KNCNI.
func ReduceMulPs(a M512) float32 {
	return float32(reduceMulPs([16]float32(a)))
}

func reduceMulPs(a [16]float32) float32


// MaskReduceOrEpi32: Reduce the packed 32-bit integers in 'a' by bitwise OR
// using mask 'k'. Returns the bitwise OR of all active elements in 'a'. 
//
//		reduced[31:0] := 0
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				reduced[31:0] := reduced[31:0] OR a[i+31:i]
//			FI
//		ENDFOR
//		RETURN reduced[31:0]
//
// Instruction: '...'. Intrinsic: '_mm512_mask_reduce_or_epi32'.
// Requires KNCNI.
func MaskReduceOrEpi32(k Mmask16, a M512i) int {
	return int(maskReduceOrEpi32(uint16(k), [64]byte(a)))
}

func maskReduceOrEpi32(k uint16, a [64]byte) int


// ReduceOrEpi32: Reduce the packed 32-bit integers in 'a' by bitwise OR.
// Returns the bitwise OR of all elements in 'a'. 
//
//		reduced[31:0] := 0
//		FOR j := 0 to 15
//			i := j*32
//			reduced[31:0] := reduced[31:0] OR a[i+31:i]
//		ENDFOR
//		RETURN reduced[31:0]
//
// Instruction: '...'. Intrinsic: '_mm512_reduce_or_epi32'.
// Requires KNCNI.
func ReduceOrEpi32(a M512i) int {
	return int(reduceOrEpi32([64]byte(a)))
}

func reduceOrEpi32(a [64]byte) int


// MaskReduceOrEpi64: Reduce the packed 64-bit integers in 'a' by bitwise OR
// using mask 'k'. Returns the bitwise OR of all active elements in 'a'. 
//
//		reduced[63:0] := 0
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				reduced[63:0] := reduced[63:0] OR a[i+63:i]
//			FI
//		ENDFOR
//		RETURN reduced[63:0]
//
// Instruction: '...'. Intrinsic: '_mm512_mask_reduce_or_epi64'.
// Requires KNCNI.
func MaskReduceOrEpi64(k Mmask8, a M512i) int64 {
	return int64(maskReduceOrEpi64(uint8(k), [64]byte(a)))
}

func maskReduceOrEpi64(k uint8, a [64]byte) int64


// ReduceOrEpi64: Reduce the packed 64-bit integers in 'a' by bitwise OR.
// Returns the bitwise OR of all elements in 'a'. 
//
//		reduced[63:0] := 0
//		FOR j := 0 to 7
//			i := j*64
//			reduced[63:0] := reduced[63:0] OR a[i+63:i]
//		ENDFOR
//		RETURN reduced[63:0]
//
// Instruction: '...'. Intrinsic: '_mm512_reduce_or_epi64'.
// Requires KNCNI.
func ReduceOrEpi64(a M512i) int64 {
	return int64(reduceOrEpi64([64]byte(a)))
}

func reduceOrEpi64(a [64]byte) int64


// MaskReducePd: Extract the reduced argument of packed double-precision
// (64-bit) floating-point elements in 'a' by the number of bits specified by
// 'imm8', and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		ReduceArgumentPD(src1[63:0], imm8[7:0])
//		{
//			m := imm8[7:4] // number of fraction bits after the binary point to be preserved
//			rc := imm8[1:0] // round control
//			rc_src := imm8[2] // round ccontrol source
//			spe := 0
//			tmp[63:0] := pow(2, -m) * ROUND(pow(2, m) * src1[63:0], spe, rc_source, rc)
//			tmp[63:0] := src1[63:0] - tmp[63:0]
//			RETURN tmp[63:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ReduceArgumentPD(src[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VREDUCEPD'. Intrinsic: '_mm512_mask_reduce_pd'.
// Requires AVX512DQ.
func MaskReducePd(src M512d, k Mmask8, a M512d, imm8 int) M512d {
	return M512d(maskReducePd([8]float64(src), uint8(k), [8]float64(a), imm8))
}

func maskReducePd(src [8]float64, k uint8, a [8]float64, imm8 int) [8]float64


// MaskzReducePd: Extract the reduced argument of packed double-precision
// (64-bit) floating-point elements in 'a' by the number of bits specified by
// 'imm8', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		ReduceArgumentPD(src1[63:0], imm8[7:0])
//		{
//			m := imm8[7:4] // number of fraction bits after the binary point to be preserved
//			rc := imm8[1:0] // round control
//			rc_src := imm8[2] // round ccontrol source
//			spe := 0
//			tmp[63:0] := pow(2, -m) * ROUND(pow(2, m) * src1[63:0], spe, rc_source, rc)
//			tmp[63:0] := src1[63:0] - tmp[63:0]
//			RETURN tmp[63:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ReduceArgumentPD(src[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VREDUCEPD'. Intrinsic: '_mm512_maskz_reduce_pd'.
// Requires AVX512DQ.
func MaskzReducePd(k Mmask8, a M512d, imm8 int) M512d {
	return M512d(maskzReducePd(uint8(k), [8]float64(a), imm8))
}

func maskzReducePd(k uint8, a [8]float64, imm8 int) [8]float64


// ReducePd: Extract the reduced argument of packed double-precision (64-bit)
// floating-point elements in 'a' by the number of bits specified by 'imm8',
// and store the results in 'dst'. 
//
//		ReduceArgumentPD(src1[63:0], imm8[7:0])
//		{
//			m := imm8[7:4] // number of fraction bits after the binary point to be preserved
//			rc := imm8[1:0] // round control
//			rc_src := imm8[2] // round ccontrol source
//			spe := 0
//			tmp[63:0] := pow(2, -m) * ROUND(pow(2, m) * src1[63:0], spe, rc_source, rc)
//			tmp[63:0] := src1[63:0] - tmp[63:0]
//			RETURN tmp[63:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ReduceArgumentPD(src[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VREDUCEPD'. Intrinsic: '_mm512_reduce_pd'.
// Requires AVX512DQ.
func ReducePd(a M512d, imm8 int) M512d {
	return M512d(reducePd([8]float64(a), imm8))
}

func reducePd(a [8]float64, imm8 int) [8]float64


// MaskReducePs: Extract the reduced argument of packed single-precision
// (32-bit) floating-point elements in 'a' by the number of bits specified by
// 'imm8', and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		ReduceArgumentPS(src1[31:0], imm8[7:0])
//		{
//			IF src1[31:0] == NAN
//				RETURN (convert src1[31:0] to QNaN)
//			FI
//			
//			m := imm8[7:4] // number of fraction bits after the binary point to be preserved
//			rc := imm8[1:0] // round control
//			rc_src := imm8[2] // round ccontrol source
//			spe := 0
//			tmp[31:0] := pow(2, -m)*ROUND(pow(2, m)*src1[31:0], spe, rc_source, rc)
//			tmp[31:0] := src1[31:0] - tmp[31:0]
//			RETURN tmp[31:0]
//		}
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ReduceArgumentPS(src[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VREDUCEPS'. Intrinsic: '_mm512_mask_reduce_ps'.
// Requires AVX512DQ.
func MaskReducePs(src M512, k Mmask16, a M512, imm8 int) M512 {
	return M512(maskReducePs([16]float32(src), uint16(k), [16]float32(a), imm8))
}

func maskReducePs(src [16]float32, k uint16, a [16]float32, imm8 int) [16]float32


// MaskzReducePs: Extract the reduced argument of packed single-precision
// (32-bit) floating-point elements in 'a' by the number of bits specified by
// 'imm8', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		ReduceArgumentPS(src1[31:0], imm8[7:0])
//		{
//			IF src1[31:0] == NAN
//				RETURN (convert src1[31:0] to QNaN)
//			FI
//			
//			m := imm8[7:4] // number of fraction bits after the binary point to be preserved
//			rc := imm8[1:0] // round control
//			rc_src := imm8[2] // round ccontrol source
//			spe := 0
//			tmp[31:0] := pow(2, -m)*ROUND(pow(2, m)*src1[31:0], spe, rc_source, rc)
//			tmp[31:0] := src1[31:0] - tmp[31:0]
//			RETURN tmp[31:0]
//		}
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ReduceArgumentPS(src[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VREDUCEPS'. Intrinsic: '_mm512_maskz_reduce_ps'.
// Requires AVX512DQ.
func MaskzReducePs(k Mmask16, a M512, imm8 int) M512 {
	return M512(maskzReducePs(uint16(k), [16]float32(a), imm8))
}

func maskzReducePs(k uint16, a [16]float32, imm8 int) [16]float32


// ReducePs: Extract the reduced argument of packed single-precision (32-bit)
// floating-point elements in 'a' by the number of bits specified by 'imm8',
// and store the results in 'dst'. 
//
//		ReduceArgumentPS(src1[31:0], imm8[7:0])
//		{
//			IF src1[31:0] == NAN
//				RETURN (convert src1[31:0] to QNaN)
//			FI
//			
//			m := imm8[7:4] // number of fraction bits after the binary point to be preserved
//			rc := imm8[1:0] // round control
//			rc_src := imm8[2] // round ccontrol source
//			spe := 0
//			tmp[31:0] := pow(2, -m)*ROUND(pow(2, m)*src1[31:0], spe, rc_source, rc)
//			tmp[31:0] := src1[31:0] - tmp[31:0]
//			RETURN tmp[31:0]
//		}
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ReduceArgumentPS(src[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VREDUCEPS'. Intrinsic: '_mm512_reduce_ps'.
// Requires AVX512DQ.
func ReducePs(a M512, imm8 int) M512 {
	return M512(reducePs([16]float32(a), imm8))
}

func reducePs(a [16]float32, imm8 int) [16]float32


// MaskReduceRoundPd: Extract the reduced argument of packed double-precision
// (64-bit) floating-point elements in 'a' by the number of bits specified by
// 'imm8', and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		ReduceArgumentPD(src1[63:0], imm8[7:0])
//		{
//			m := imm8[7:4] // number of fraction bits after the binary point to be preserved
//			rc := imm8[1:0] // round control
//			rc_src := imm8[2] // round ccontrol source
//			spe := 0
//			tmp[63:0] := pow(2, -m) * ROUND(pow(2, m) * src1[63:0], spe, rc_source, rc)
//			tmp[63:0] := src1[63:0] - tmp[63:0]
//			RETURN tmp[63:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ReduceArgumentPD(src[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VREDUCEPD'. Intrinsic: '_mm512_mask_reduce_round_pd'.
// Requires AVX512DQ.
func MaskReduceRoundPd(src M512d, k Mmask8, a M512d, imm8 int, rounding int) M512d {
	return M512d(maskReduceRoundPd([8]float64(src), uint8(k), [8]float64(a), imm8, rounding))
}

func maskReduceRoundPd(src [8]float64, k uint8, a [8]float64, imm8 int, rounding int) [8]float64


// MaskzReduceRoundPd: Extract the reduced argument of packed double-precision
// (64-bit) floating-point elements in 'a' by the number of bits specified by
// 'imm8', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		ReduceArgumentPD(src1[63:0], imm8[7:0])
//		{
//			m := imm8[7:4] // number of fraction bits after the binary point to be preserved
//			rc := imm8[1:0] // round control
//			rc_src := imm8[2] // round ccontrol source
//			spe := 0
//			tmp[63:0] := pow(2, -m) * ROUND(pow(2, m) * src1[63:0], spe, rc_source, rc)
//			tmp[63:0] := src1[63:0] - tmp[63:0]
//			RETURN tmp[63:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ReduceArgumentPD(src[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VREDUCEPD'. Intrinsic: '_mm512_maskz_reduce_round_pd'.
// Requires AVX512DQ.
func MaskzReduceRoundPd(k Mmask8, a M512d, imm8 int, rounding int) M512d {
	return M512d(maskzReduceRoundPd(uint8(k), [8]float64(a), imm8, rounding))
}

func maskzReduceRoundPd(k uint8, a [8]float64, imm8 int, rounding int) [8]float64


// ReduceRoundPd: Extract the reduced argument of packed double-precision
// (64-bit) floating-point elements in 'a' by the number of bits specified by
// 'imm8', and store the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		ReduceArgumentPD(src1[63:0], imm8[7:0])
//		{
//			m := imm8[7:4] // number of fraction bits after the binary point to be preserved
//			rc := imm8[1:0] // round control
//			rc_src := imm8[2] // round ccontrol source
//			spe := 0
//			tmp[63:0] := pow(2, -m) * ROUND(pow(2, m) * src1[63:0], spe, rc_source, rc)
//			tmp[63:0] := src1[63:0] - tmp[63:0]
//			RETURN tmp[63:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ReduceArgumentPD(src[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VREDUCEPD'. Intrinsic: '_mm512_reduce_round_pd'.
// Requires AVX512DQ.
func ReduceRoundPd(a M512d, imm8 int, rounding int) M512d {
	return M512d(reduceRoundPd([8]float64(a), imm8, rounding))
}

func reduceRoundPd(a [8]float64, imm8 int, rounding int) [8]float64


// MaskReduceRoundPs: Extract the reduced argument of packed single-precision
// (32-bit) floating-point elements in 'a' by the number of bits specified by
// 'imm8', and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		ReduceArgumentPS(src1[31:0], imm8[7:0])
//		{
//			IF src1[31:0] == NAN
//				RETURN (convert src1[31:0] to QNaN)
//			FI
//			
//			m := imm8[7:4] // number of fraction bits after the binary point to be preserved
//			rc := imm8[1:0] // round control
//			rc_src := imm8[2] // round ccontrol source
//			spe := 0
//			tmp[31:0] := pow(2, -m)*ROUND(pow(2, m)*src1[31:0], spe, rc_source, rc)
//			tmp[31:0] := src1[31:0] - tmp[31:0]
//			RETURN tmp[31:0]
//		}
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ReduceArgumentPS(src[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VREDUCEPS'. Intrinsic: '_mm512_mask_reduce_round_ps'.
// Requires AVX512DQ.
func MaskReduceRoundPs(src M512, k Mmask16, a M512, imm8 int, rounding int) M512 {
	return M512(maskReduceRoundPs([16]float32(src), uint16(k), [16]float32(a), imm8, rounding))
}

func maskReduceRoundPs(src [16]float32, k uint16, a [16]float32, imm8 int, rounding int) [16]float32


// MaskzReduceRoundPs: Extract the reduced argument of packed single-precision
// (32-bit) floating-point elements in 'a' by the number of bits specified by
// 'imm8', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		ReduceArgumentPS(src1[31:0], imm8[7:0])
//		{
//			IF src1[31:0] == NAN
//				RETURN (convert src1[31:0] to QNaN)
//			FI
//			
//			m := imm8[7:4] // number of fraction bits after the binary point to be preserved
//			rc := imm8[1:0] // round control
//			rc_src := imm8[2] // round ccontrol source
//			spe := 0
//			tmp[31:0] := pow(2, -m)*ROUND(pow(2, m)*src1[31:0], spe, rc_source, rc)
//			tmp[31:0] := src1[31:0] - tmp[31:0]
//			RETURN tmp[31:0]
//		}
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ReduceArgumentPS(src[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VREDUCEPS'. Intrinsic: '_mm512_maskz_reduce_round_ps'.
// Requires AVX512DQ.
func MaskzReduceRoundPs(k Mmask16, a M512, imm8 int, rounding int) M512 {
	return M512(maskzReduceRoundPs(uint16(k), [16]float32(a), imm8, rounding))
}

func maskzReduceRoundPs(k uint16, a [16]float32, imm8 int, rounding int) [16]float32


// ReduceRoundPs: Extract the reduced argument of packed single-precision
// (32-bit) floating-point elements in 'a' by the number of bits specified by
// 'imm8', and store the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		ReduceArgumentPS(src1[31:0], imm8[7:0])
//		{
//			IF src1[31:0] == NAN
//				RETURN (convert src1[31:0] to QNaN)
//			FI
//			
//			m := imm8[7:4] // number of fraction bits after the binary point to be preserved
//			rc := imm8[1:0] // round control
//			rc_src := imm8[2] // round ccontrol source
//			spe := 0
//			tmp[31:0] := pow(2, -m)*ROUND(pow(2, m)*src1[31:0], spe, rc_source, rc)
//			tmp[31:0] := src1[31:0] - tmp[31:0]
//			RETURN tmp[31:0]
//		}
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ReduceArgumentPS(src[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VREDUCEPS'. Intrinsic: '_mm512_reduce_round_ps'.
// Requires AVX512DQ.
func ReduceRoundPs(a M512, imm8 int, rounding int) M512 {
	return M512(reduceRoundPs([16]float32(a), imm8, rounding))
}

func reduceRoundPs(a [16]float32, imm8 int, rounding int) [16]float32


// RemEpi16: Divide packed 16-bit integers in 'a' by packed elements in 'b',
// and store the remainders as packed 32-bit integers in 'dst'. 
//
//		FOR j := 0 to 31
//			i := 16*j
//			dst[i+15:i] := REMAINDER(a[i+15:i] / b[i+15:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_rem_epi16'.
// Requires AVX512F.
func RemEpi16(a M512i, b M512i) M512i {
	return M512i(remEpi16([64]byte(a), [64]byte(b)))
}

func remEpi16(a [64]byte, b [64]byte) [64]byte


// MaskRemEpi32: Divide packed 32-bit integers in 'a' by packed elements in
// 'b', and store the remainders as packed 32-bit integers in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := REMAINDER(a[i+31:i] / b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_rem_epi32'.
// Requires AVX512F.
func MaskRemEpi32(src M512i, k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskRemEpi32([64]byte(src), uint16(k), [64]byte(a), [64]byte(b)))
}

func maskRemEpi32(src [64]byte, k uint16, a [64]byte, b [64]byte) [64]byte


// RemEpi32: Divide packed 32-bit integers in 'a' by packed elements in 'b',
// and store the remainders as packed 32-bit integers in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := REMAINDER(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_rem_epi32'.
// Requires AVX512F.
func RemEpi32(a M512i, b M512i) M512i {
	return M512i(remEpi32([64]byte(a), [64]byte(b)))
}

func remEpi32(a [64]byte, b [64]byte) [64]byte


// RemEpi64: Divide packed 64-bit integers in 'a' by packed elements in 'b',
// and store the remainders as packed 32-bit integers in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			dst[i+63:i] := REMAINDER(a[i+63:i] / b[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_rem_epi64'.
// Requires AVX512F.
func RemEpi64(a M512i, b M512i) M512i {
	return M512i(remEpi64([64]byte(a), [64]byte(b)))
}

func remEpi64(a [64]byte, b [64]byte) [64]byte


// RemEpi8: Divide packed 8-bit integers in 'a' by packed elements in 'b', and
// store the remainders as packed 32-bit integers in 'dst'. 
//
//		FOR j := 0 to 63
//			i := 8*j
//			dst[i+7:i] := REMAINDER(a[i+7:i] / b[i+7:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_rem_epi8'.
// Requires AVX512F.
func RemEpi8(a M512i, b M512i) M512i {
	return M512i(remEpi8([64]byte(a), [64]byte(b)))
}

func remEpi8(a [64]byte, b [64]byte) [64]byte


// RemEpu16: Divide packed unsigned 16-bit integers in 'a' by packed elements
// in 'b', and store the remainders as packed unsigned 32-bit integers in
// 'dst'. 
//
//		FOR j := 0 to 31
//			i := 16*j
//			dst[i+15:i] := REMAINDER(a[i+15:i] / b[i+15:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_rem_epu16'.
// Requires AVX512F.
func RemEpu16(a M512i, b M512i) M512i {
	return M512i(remEpu16([64]byte(a), [64]byte(b)))
}

func remEpu16(a [64]byte, b [64]byte) [64]byte


// MaskRemEpu32: Divide packed unsigned 32-bit integers in 'a' by packed
// elements in 'b', and store the remainders as packed unsigned 32-bit integers
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := REMAINDER(a[i+31:i] / b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_rem_epu32'.
// Requires AVX512F.
func MaskRemEpu32(src M512i, k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskRemEpu32([64]byte(src), uint16(k), [64]byte(a), [64]byte(b)))
}

func maskRemEpu32(src [64]byte, k uint16, a [64]byte, b [64]byte) [64]byte


// RemEpu32: Divide packed unsigned 32-bit integers in 'a' by packed elements
// in 'b', and store the remainders as packed unsigned 32-bit integers in
// 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := REMAINDER(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_rem_epu32'.
// Requires AVX512F.
func RemEpu32(a M512i, b M512i) M512i {
	return M512i(remEpu32([64]byte(a), [64]byte(b)))
}

func remEpu32(a [64]byte, b [64]byte) [64]byte


// RemEpu64: Divide packed unsigned 64-bit integers in 'a' by packed elements
// in 'b', and store the remainders as packed unsigned 32-bit integers in
// 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			dst[i+63:i] := REMAINDER(a[i+63:i] / b[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_rem_epu64'.
// Requires AVX512F.
func RemEpu64(a M512i, b M512i) M512i {
	return M512i(remEpu64([64]byte(a), [64]byte(b)))
}

func remEpu64(a [64]byte, b [64]byte) [64]byte


// RemEpu8: Divide packed unsigned 8-bit integers in 'a' by packed elements in
// 'b', and store the remainders as packed unsigned 32-bit integers in 'dst'. 
//
//		FOR j := 0 to 63
//			i := 8*j
//			dst[i+7:i] := REMAINDER(a[i+7:i] / b[i+7:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_rem_epu8'.
// Requires AVX512F.
func RemEpu8(a M512i, b M512i) M512i {
	return M512i(remEpu8([64]byte(a), [64]byte(b)))
}

func remEpu8(a [64]byte, b [64]byte) [64]byte


// MaskRintPd: Rounds the packed double-precision (64-bit) floating-point
// elements in 'a' to the nearest even integer value and stores the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RoundToNearestEven(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_rint_pd'.
// Requires AVX512F.
func MaskRintPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskRintPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskRintPd(src [8]float64, k uint8, a [8]float64) [8]float64


// RintPd: Rounds the packed double-precision (64-bit) floating-point elements
// in 'a' to the nearest even integer value and stores the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := RoundToNearestEven(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_rint_pd'.
// Requires AVX512F.
func RintPd(a M512d) M512d {
	return M512d(rintPd([8]float64(a)))
}

func rintPd(a [8]float64) [8]float64


// MaskRintPs: Rounds the packed single-precision (32-bit) floating-point
// elements in 'a' to the nearest even integer value and stores the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RoundToNearestEven(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_rint_ps'.
// Requires AVX512F.
func MaskRintPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskRintPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskRintPs(src [16]float32, k uint16, a [16]float32) [16]float32


// RintPs: Rounds the packed single-precision (32-bit) floating-point elements
// in 'a' to the nearest even integer value and stores the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := RoundToNearestEven(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_rint_ps'.
// Requires AVX512F.
func RintPs(a M512) M512 {
	return M512(rintPs([16]float32(a)))
}

func rintPs(a [16]float32) [16]float32


// MaskRolEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the
// left by the number of bits specified in 'imm8', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPROLD'. Intrinsic: '_mm512_mask_rol_epi32'.
// Requires AVX512F.
func MaskRolEpi32(src M512i, k Mmask16, a M512i, imm8 int) M512i {
	return M512i(maskRolEpi32([64]byte(src), uint16(k), [64]byte(a), imm8))
}

func maskRolEpi32(src [64]byte, k uint16, a [64]byte, imm8 int) [64]byte


// MaskzRolEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the
// left by the number of bits specified in 'imm8', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPROLD'. Intrinsic: '_mm512_maskz_rol_epi32'.
// Requires AVX512F.
func MaskzRolEpi32(k Mmask16, a M512i, imm8 int) M512i {
	return M512i(maskzRolEpi32(uint16(k), [64]byte(a), imm8))
}

func maskzRolEpi32(k uint16, a [64]byte, imm8 int) [64]byte


// RolEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the left
// by the number of bits specified in 'imm8', and store the results in 'dst'. 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPROLD'. Intrinsic: '_mm512_rol_epi32'.
// Requires AVX512F.
func RolEpi32(a M512i, imm8 int) M512i {
	return M512i(rolEpi32([64]byte(a), imm8))
}

func rolEpi32(a [64]byte, imm8 int) [64]byte


// MaskRolEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the
// left by the number of bits specified in 'imm8', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPROLQ'. Intrinsic: '_mm512_mask_rol_epi64'.
// Requires AVX512F.
func MaskRolEpi64(src M512i, k Mmask8, a M512i, imm8 int) M512i {
	return M512i(maskRolEpi64([64]byte(src), uint8(k), [64]byte(a), imm8))
}

func maskRolEpi64(src [64]byte, k uint8, a [64]byte, imm8 int) [64]byte


// MaskzRolEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the
// left by the number of bits specified in 'imm8', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPROLQ'. Intrinsic: '_mm512_maskz_rol_epi64'.
// Requires AVX512F.
func MaskzRolEpi64(k Mmask8, a M512i, imm8 int) M512i {
	return M512i(maskzRolEpi64(uint8(k), [64]byte(a), imm8))
}

func maskzRolEpi64(k uint8, a [64]byte, imm8 int) [64]byte


// RolEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the left
// by the number of bits specified in 'imm8', and store the results in 'dst'. 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPROLQ'. Intrinsic: '_mm512_rol_epi64'.
// Requires AVX512F.
func RolEpi64(a M512i, imm8 int) M512i {
	return M512i(rolEpi64([64]byte(a), imm8))
}

func rolEpi64(a [64]byte, imm8 int) [64]byte


// MaskRolvEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the
// left by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPROLVD'. Intrinsic: '_mm512_mask_rolv_epi32'.
// Requires AVX512F.
func MaskRolvEpi32(src M512i, k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskRolvEpi32([64]byte(src), uint16(k), [64]byte(a), [64]byte(b)))
}

func maskRolvEpi32(src [64]byte, k uint16, a [64]byte, b [64]byte) [64]byte


// MaskzRolvEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the
// left by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPROLVD'. Intrinsic: '_mm512_maskz_rolv_epi32'.
// Requires AVX512F.
func MaskzRolvEpi32(k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskzRolvEpi32(uint16(k), [64]byte(a), [64]byte(b)))
}

func maskzRolvEpi32(k uint16, a [64]byte, b [64]byte) [64]byte


// RolvEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the left
// by the number of bits specified in the corresponding element of 'b', and
// store the results in 'dst'. 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPROLVD'. Intrinsic: '_mm512_rolv_epi32'.
// Requires AVX512F.
func RolvEpi32(a M512i, b M512i) M512i {
	return M512i(rolvEpi32([64]byte(a), [64]byte(b)))
}

func rolvEpi32(a [64]byte, b [64]byte) [64]byte


// MaskRolvEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the
// left by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPROLVQ'. Intrinsic: '_mm512_mask_rolv_epi64'.
// Requires AVX512F.
func MaskRolvEpi64(src M512i, k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskRolvEpi64([64]byte(src), uint8(k), [64]byte(a), [64]byte(b)))
}

func maskRolvEpi64(src [64]byte, k uint8, a [64]byte, b [64]byte) [64]byte


// MaskzRolvEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the
// left by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPROLVQ'. Intrinsic: '_mm512_maskz_rolv_epi64'.
// Requires AVX512F.
func MaskzRolvEpi64(k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskzRolvEpi64(uint8(k), [64]byte(a), [64]byte(b)))
}

func maskzRolvEpi64(k uint8, a [64]byte, b [64]byte) [64]byte


// RolvEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the left
// by the number of bits specified in the corresponding element of 'b', and
// store the results in 'dst'. 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPROLVQ'. Intrinsic: '_mm512_rolv_epi64'.
// Requires AVX512F.
func RolvEpi64(a M512i, b M512i) M512i {
	return M512i(rolvEpi64([64]byte(a), [64]byte(b)))
}

func rolvEpi64(a [64]byte, b [64]byte) [64]byte


// MaskRorEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the
// right by the number of bits specified in 'imm8', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPRORD'. Intrinsic: '_mm512_mask_ror_epi32'.
// Requires AVX512F.
func MaskRorEpi32(src M512i, k Mmask16, a M512i, imm8 int) M512i {
	return M512i(maskRorEpi32([64]byte(src), uint16(k), [64]byte(a), imm8))
}

func maskRorEpi32(src [64]byte, k uint16, a [64]byte, imm8 int) [64]byte


// MaskzRorEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the
// right by the number of bits specified in 'imm8', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPRORD'. Intrinsic: '_mm512_maskz_ror_epi32'.
// Requires AVX512F.
func MaskzRorEpi32(k Mmask16, a M512i, imm8 int) M512i {
	return M512i(maskzRorEpi32(uint16(k), [64]byte(a), imm8))
}

func maskzRorEpi32(k uint16, a [64]byte, imm8 int) [64]byte


// RorEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the right
// by the number of bits specified in 'imm8', and store the results in 'dst'. 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPRORD'. Intrinsic: '_mm512_ror_epi32'.
// Requires AVX512F.
func RorEpi32(a M512i, imm8 int) M512i {
	return M512i(rorEpi32([64]byte(a), imm8))
}

func rorEpi32(a [64]byte, imm8 int) [64]byte


// MaskRorEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the
// right by the number of bits specified in 'imm8', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPRORQ'. Intrinsic: '_mm512_mask_ror_epi64'.
// Requires AVX512F.
func MaskRorEpi64(src M512i, k Mmask8, a M512i, imm8 int) M512i {
	return M512i(maskRorEpi64([64]byte(src), uint8(k), [64]byte(a), imm8))
}

func maskRorEpi64(src [64]byte, k uint8, a [64]byte, imm8 int) [64]byte


// MaskzRorEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the
// right by the number of bits specified in 'imm8', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPRORQ'. Intrinsic: '_mm512_maskz_ror_epi64'.
// Requires AVX512F.
func MaskzRorEpi64(k Mmask8, a M512i, imm8 int) M512i {
	return M512i(maskzRorEpi64(uint8(k), [64]byte(a), imm8))
}

func maskzRorEpi64(k uint8, a [64]byte, imm8 int) [64]byte


// RorEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the right
// by the number of bits specified in 'imm8', and store the results in 'dst'. 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPRORQ'. Intrinsic: '_mm512_ror_epi64'.
// Requires AVX512F.
func RorEpi64(a M512i, imm8 int) M512i {
	return M512i(rorEpi64([64]byte(a), imm8))
}

func rorEpi64(a [64]byte, imm8 int) [64]byte


// MaskRorvEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the
// right by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPRORVD'. Intrinsic: '_mm512_mask_rorv_epi32'.
// Requires AVX512F.
func MaskRorvEpi32(src M512i, k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskRorvEpi32([64]byte(src), uint16(k), [64]byte(a), [64]byte(b)))
}

func maskRorvEpi32(src [64]byte, k uint16, a [64]byte, b [64]byte) [64]byte


// MaskzRorvEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the
// right by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPRORVD'. Intrinsic: '_mm512_maskz_rorv_epi32'.
// Requires AVX512F.
func MaskzRorvEpi32(k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskzRorvEpi32(uint16(k), [64]byte(a), [64]byte(b)))
}

func maskzRorvEpi32(k uint16, a [64]byte, b [64]byte) [64]byte


// RorvEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the right
// by the number of bits specified in the corresponding element of 'b', and
// store the results in 'dst'. 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPRORVD'. Intrinsic: '_mm512_rorv_epi32'.
// Requires AVX512F.
func RorvEpi32(a M512i, b M512i) M512i {
	return M512i(rorvEpi32([64]byte(a), [64]byte(b)))
}

func rorvEpi32(a [64]byte, b [64]byte) [64]byte


// MaskRorvEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the
// right by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPRORVQ'. Intrinsic: '_mm512_mask_rorv_epi64'.
// Requires AVX512F.
func MaskRorvEpi64(src M512i, k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskRorvEpi64([64]byte(src), uint8(k), [64]byte(a), [64]byte(b)))
}

func maskRorvEpi64(src [64]byte, k uint8, a [64]byte, b [64]byte) [64]byte


// MaskzRorvEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the
// right by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPRORVQ'. Intrinsic: '_mm512_maskz_rorv_epi64'.
// Requires AVX512F.
func MaskzRorvEpi64(k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskzRorvEpi64(uint8(k), [64]byte(a), [64]byte(b)))
}

func maskzRorvEpi64(k uint8, a [64]byte, b [64]byte) [64]byte


// RorvEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the right
// by the number of bits specified in the corresponding element of 'b', and
// store the results in 'dst'. 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPRORVQ'. Intrinsic: '_mm512_rorv_epi64'.
// Requires AVX512F.
func RorvEpi64(a M512i, b M512i) M512i {
	return M512i(rorvEpi64([64]byte(a), [64]byte(b)))
}

func rorvEpi64(a [64]byte, b [64]byte) [64]byte


// MaskRoundPs: Round the packed single-precision (32-bit) floating-point
// elements in 'a' to the nearest integer value using 'expadj' and in the
// direction of 'rounding', and store the results as packed single-precision
// floating-point elements in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ROUND(a[i+31:i])
//				CASE expadj OF
//				_MM_EXPADJ_NONE: dst[i+31:i] = dst[i+31:i] * 2**0
//				_MM_EXPADJ_4:	dst[i+31:i] = dst[i+31:i] * 2**4
//				_MM_EXPADJ_5:	dst[i+31:i] = dst[i+31:i] * 2**5
//				_MM_EXPADJ_8:	dst[i+31:i] = dst[i+31:i] * 2**8
//				_MM_EXPADJ_16:   dst[i+31:i] = dst[i+31:i] * 2**16
//				_MM_EXPADJ_24:   dst[i+31:i] = dst[i+31:i] * 2**24
//				_MM_EXPADJ_31:   dst[i+31:i] = dst[i+31:i] * 2**31
//				_MM_EXPADJ_32:   dst[i+31:i] = dst[i+31:i] * 2**32
//				ESAC
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VROUNDPS'. Intrinsic: '_mm512_mask_round_ps'.
// Requires KNCNI.
func MaskRoundPs(src M512, k Mmask16, a M512, rounding int, expadj MMEXPADJENUM) M512 {
	return M512(maskRoundPs([16]float32(src), uint16(k), [16]float32(a), rounding, expadj))
}

func maskRoundPs(src [16]float32, k uint16, a [16]float32, rounding int, expadj MMEXPADJENUM) [16]float32


// RoundPs: Round the packed single-precision (32-bit) floating-point elements
// in 'a' to the nearest integer value using 'expadj' and in the direction of
// 'rounding', and store the results as packed single-precision floating-point
// elements in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ROUND(a[i+31:i])
//			CASE expadj OF
//			_MM_EXPADJ_NONE: dst[i+31:i] = dst[i+31:i] * 2**0
//			_MM_EXPADJ_4:	dst[i+31:i] = dst[i+31:i] * 2**4
//			_MM_EXPADJ_5:	dst[i+31:i] = dst[i+31:i] * 2**5
//			_MM_EXPADJ_8:	dst[i+31:i] = dst[i+31:i] * 2**8
//			_MM_EXPADJ_16:   dst[i+31:i] = dst[i+31:i] * 2**16
//			_MM_EXPADJ_24:   dst[i+31:i] = dst[i+31:i] * 2**24
//			_MM_EXPADJ_31:   dst[i+31:i] = dst[i+31:i] * 2**31
//			_MM_EXPADJ_32:   dst[i+31:i] = dst[i+31:i] * 2**32
//			ESAC
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VROUNDPS'. Intrinsic: '_mm512_round_ps'.
// Requires KNCNI.
func RoundPs(a M512, rounding int, expadj MMEXPADJENUM) M512 {
	return M512(roundPs([16]float32(a), rounding, expadj))
}

func roundPs(a [16]float32, rounding int, expadj MMEXPADJENUM) [16]float32


// MaskRoundfxpntAdjustPd: Performs element-by-element rounding of packed
// double-precision (64-bit) floating-point elements in 'a' using 'expadj' and
// in the direction of 'rounding' and stores results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ROUND(a[i+63:i])
//				CASE expadj OF
//				_MM_EXPADJ_NONE: dst[i+31:i] = dst[i+31:i] * 2**0
//				_MM_EXPADJ_4:	dst[i+31:i] = dst[i+31:i] * 2**4
//				_MM_EXPADJ_5:	dst[i+31:i] = dst[i+31:i] * 2**5
//				_MM_EXPADJ_8:	dst[i+31:i] = dst[i+31:i] * 2**8
//				_MM_EXPADJ_16:   dst[i+31:i] = dst[i+31:i] * 2**16
//				_MM_EXPADJ_24:   dst[i+31:i] = dst[i+31:i] * 2**24
//				_MM_EXPADJ_31:   dst[i+31:i] = dst[i+31:i] * 2**31
//				_MM_EXPADJ_32:   dst[i+31:i] = dst[i+31:i] * 2**32
//				ESAC
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRNDFXPNTPD'. Intrinsic: '_mm512_mask_roundfxpnt_adjust_pd'.
// Requires KNCNI.
func MaskRoundfxpntAdjustPd(src M512d, k Mmask8, a M512d, rounding int, expadj MMEXPADJENUM) M512d {
	return M512d(maskRoundfxpntAdjustPd([8]float64(src), uint8(k), [8]float64(a), rounding, expadj))
}

func maskRoundfxpntAdjustPd(src [8]float64, k uint8, a [8]float64, rounding int, expadj MMEXPADJENUM) [8]float64


// RoundfxpntAdjustPd: Performs element-by-element rounding of packed
// double-precision (64-bit) floating-point elements in 'a' using 'expadj' and
// in the direction of 'rounding' and stores results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ROUND(a[i+63:i])
//			CASE expadj OF
//			_MM_EXPADJ_NONE: dst[i+31:i] = dst[i+31:i] * 2**0
//			_MM_EXPADJ_4:	dst[i+31:i] = dst[i+31:i] * 2**4
//			_MM_EXPADJ_5:	dst[i+31:i] = dst[i+31:i] * 2**5
//			_MM_EXPADJ_8:	dst[i+31:i] = dst[i+31:i] * 2**8
//			_MM_EXPADJ_16:   dst[i+31:i] = dst[i+31:i] * 2**16
//			_MM_EXPADJ_24:   dst[i+31:i] = dst[i+31:i] * 2**24
//			_MM_EXPADJ_31:   dst[i+31:i] = dst[i+31:i] * 2**31
//			_MM_EXPADJ_32:   dst[i+31:i] = dst[i+31:i] * 2**32
//			ESAC
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRNDFXPNTPD'. Intrinsic: '_mm512_roundfxpnt_adjust_pd'.
// Requires KNCNI.
func RoundfxpntAdjustPd(a M512d, rounding int, expadj MMEXPADJENUM) M512d {
	return M512d(roundfxpntAdjustPd([8]float64(a), rounding, expadj))
}

func roundfxpntAdjustPd(a [8]float64, rounding int, expadj MMEXPADJENUM) [8]float64


// MaskRoundfxpntAdjustPs: Performs element-by-element rounding of packed
// single-precision (32-bit) floating-point elements in 'a' using 'expadj' and
// in the direction of 'rounding' and stores results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ROUND(a[i+31:i])
//				CASE expadj OF
//				_MM_EXPADJ_NONE: dst[i+31:i] = dst[i+31:i] * 2**0
//				_MM_EXPADJ_4:	dst[i+31:i] = dst[i+31:i] * 2**4
//				_MM_EXPADJ_5:	dst[i+31:i] = dst[i+31:i] * 2**5
//				_MM_EXPADJ_8:	dst[i+31:i] = dst[i+31:i] * 2**8
//				_MM_EXPADJ_16:   dst[i+31:i] = dst[i+31:i] * 2**16
//				_MM_EXPADJ_24:   dst[i+31:i] = dst[i+31:i] * 2**24
//				_MM_EXPADJ_31:   dst[i+31:i] = dst[i+31:i] * 2**31
//				_MM_EXPADJ_32:   dst[i+31:i] = dst[i+31:i] * 2**32
//				ESAC
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRNDFXPNTPS'. Intrinsic: '_mm512_mask_roundfxpnt_adjust_ps'.
// Requires KNCNI.
func MaskRoundfxpntAdjustPs(src M512, k Mmask16, a M512, rounding int, expadj MMEXPADJENUM) M512 {
	return M512(maskRoundfxpntAdjustPs([16]float32(src), uint16(k), [16]float32(a), rounding, expadj))
}

func maskRoundfxpntAdjustPs(src [16]float32, k uint16, a [16]float32, rounding int, expadj MMEXPADJENUM) [16]float32


// RoundfxpntAdjustPs: Performs element-by-element rounding of packed
// single-precision (32-bit) floating-point elements in 'a' using 'expadj' and
// in the direction of 'rounding' and stores results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ROUND(a[i+31:i])
//			CASE expadj OF
//			_MM_EXPADJ_NONE: dst[i+31:i] = dst[i+31:i] * 2**0
//			_MM_EXPADJ_4:	dst[i+31:i] = dst[i+31:i] * 2**4
//			_MM_EXPADJ_5:	dst[i+31:i] = dst[i+31:i] * 2**5
//			_MM_EXPADJ_8:	dst[i+31:i] = dst[i+31:i] * 2**8
//			_MM_EXPADJ_16:   dst[i+31:i] = dst[i+31:i] * 2**16
//			_MM_EXPADJ_24:   dst[i+31:i] = dst[i+31:i] * 2**24
//			_MM_EXPADJ_31:   dst[i+31:i] = dst[i+31:i] * 2**31
//			_MM_EXPADJ_32:   dst[i+31:i] = dst[i+31:i] * 2**32
//			ESAC
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRNDFXPNTPS'. Intrinsic: '_mm512_roundfxpnt_adjust_ps'.
// Requires KNCNI.
func RoundfxpntAdjustPs(a M512, rounding int, expadj MMEXPADJENUM) M512 {
	return M512(roundfxpntAdjustPs([16]float32(a), rounding, expadj))
}

func roundfxpntAdjustPs(a [16]float32, rounding int, expadj MMEXPADJENUM) [16]float32


// MaskRoundscalePd: Round packed double-precision (64-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}	
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRNDSCALEPD'. Intrinsic: '_mm512_mask_roundscale_pd'.
// Requires AVX512F.
func MaskRoundscalePd(src M512d, k Mmask8, a M512d, imm8 int) M512d {
	return M512d(maskRoundscalePd([8]float64(src), uint8(k), [8]float64(a), imm8))
}

func maskRoundscalePd(src [8]float64, k uint8, a [8]float64, imm8 int) [8]float64


// MaskzRoundscalePd: Round packed double-precision (64-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}	
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRNDSCALEPD'. Intrinsic: '_mm512_maskz_roundscale_pd'.
// Requires AVX512F.
func MaskzRoundscalePd(k Mmask8, a M512d, imm8 int) M512d {
	return M512d(maskzRoundscalePd(uint8(k), [8]float64(a), imm8))
}

func maskzRoundscalePd(k uint8, a [8]float64, imm8 int) [8]float64


// RoundscalePd: Round packed double-precision (64-bit) floating-point elements
// in 'a' to the number of fraction bits specified by 'imm8', and store the
// results in 'dst'. 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}	
//		
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRNDSCALEPD'. Intrinsic: '_mm512_roundscale_pd'.
// Requires AVX512F.
func RoundscalePd(a M512d, imm8 int) M512d {
	return M512d(roundscalePd([8]float64(a), imm8))
}

func roundscalePd(a [8]float64, imm8 int) [8]float64


// MaskRoundscalePs: Round packed single-precision (32-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}	
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRNDSCALEPS'. Intrinsic: '_mm512_mask_roundscale_ps'.
// Requires AVX512F.
func MaskRoundscalePs(src M512, k Mmask16, a M512, imm8 int) M512 {
	return M512(maskRoundscalePs([16]float32(src), uint16(k), [16]float32(a), imm8))
}

func maskRoundscalePs(src [16]float32, k uint16, a [16]float32, imm8 int) [16]float32


// MaskzRoundscalePs: Round packed single-precision (32-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}	
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRNDSCALEPS'. Intrinsic: '_mm512_maskz_roundscale_ps'.
// Requires AVX512F.
func MaskzRoundscalePs(k Mmask16, a M512, imm8 int) M512 {
	return M512(maskzRoundscalePs(uint16(k), [16]float32(a), imm8))
}

func maskzRoundscalePs(k uint16, a [16]float32, imm8 int) [16]float32


// RoundscalePs: Round packed single-precision (32-bit) floating-point elements
// in 'a' to the number of fraction bits specified by 'imm8', and store the
// results in 'dst'. 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}	
//		
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRNDSCALEPS'. Intrinsic: '_mm512_roundscale_ps'.
// Requires AVX512F.
func RoundscalePs(a M512, imm8 int) M512 {
	return M512(roundscalePs([16]float32(a), imm8))
}

func roundscalePs(a [16]float32, imm8 int) [16]float32


// MaskRoundscaleRoundPd: Round packed double-precision (64-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}	
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRNDSCALEPD'. Intrinsic: '_mm512_mask_roundscale_round_pd'.
// Requires AVX512F.
func MaskRoundscaleRoundPd(src M512d, k Mmask8, a M512d, imm8 int, rounding int) M512d {
	return M512d(maskRoundscaleRoundPd([8]float64(src), uint8(k), [8]float64(a), imm8, rounding))
}

func maskRoundscaleRoundPd(src [8]float64, k uint8, a [8]float64, imm8 int, rounding int) [8]float64


// MaskzRoundscaleRoundPd: Round packed double-precision (64-bit)
// floating-point elements in 'a' to the number of fraction bits specified by
// 'imm8', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}	
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRNDSCALEPD'. Intrinsic: '_mm512_maskz_roundscale_round_pd'.
// Requires AVX512F.
func MaskzRoundscaleRoundPd(k Mmask8, a M512d, imm8 int, rounding int) M512d {
	return M512d(maskzRoundscaleRoundPd(uint8(k), [8]float64(a), imm8, rounding))
}

func maskzRoundscaleRoundPd(k uint8, a [8]float64, imm8 int, rounding int) [8]float64


// RoundscaleRoundPd: Round packed double-precision (64-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}	
//		
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRNDSCALEPD'. Intrinsic: '_mm512_roundscale_round_pd'.
// Requires AVX512F.
func RoundscaleRoundPd(a M512d, imm8 int, rounding int) M512d {
	return M512d(roundscaleRoundPd([8]float64(a), imm8, rounding))
}

func roundscaleRoundPd(a [8]float64, imm8 int, rounding int) [8]float64


// MaskRoundscaleRoundPs: Round packed single-precision (32-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}	
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRNDSCALEPS'. Intrinsic: '_mm512_mask_roundscale_round_ps'.
// Requires AVX512F.
func MaskRoundscaleRoundPs(src M512, k Mmask16, a M512, imm8 int, rounding int) M512 {
	return M512(maskRoundscaleRoundPs([16]float32(src), uint16(k), [16]float32(a), imm8, rounding))
}

func maskRoundscaleRoundPs(src [16]float32, k uint16, a [16]float32, imm8 int, rounding int) [16]float32


// MaskzRoundscaleRoundPs: Round packed single-precision (32-bit)
// floating-point elements in 'a' to the number of fraction bits specified by
// 'imm8', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}	
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRNDSCALEPS'. Intrinsic: '_mm512_maskz_roundscale_round_ps'.
// Requires AVX512F.
func MaskzRoundscaleRoundPs(k Mmask16, a M512, imm8 int, rounding int) M512 {
	return M512(maskzRoundscaleRoundPs(uint16(k), [16]float32(a), imm8, rounding))
}

func maskzRoundscaleRoundPs(k uint16, a [16]float32, imm8 int, rounding int) [16]float32


// RoundscaleRoundPs: Round packed single-precision (32-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}	
//		
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRNDSCALEPS'. Intrinsic: '_mm512_roundscale_round_ps'.
// Requires AVX512F.
func RoundscaleRoundPs(a M512, imm8 int, rounding int) M512 {
	return M512(roundscaleRoundPs([16]float32(a), imm8, rounding))
}

func roundscaleRoundPs(a [16]float32, imm8 int, rounding int) [16]float32


// MaskRsqrt14Pd: Compute the approximate reciprocal square root of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := APPROXIMATE(1.0 / SQRT(a[i+63:i]))
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRSQRT14PD'. Intrinsic: '_mm512_mask_rsqrt14_pd'.
// Requires AVX512F.
func MaskRsqrt14Pd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskRsqrt14Pd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskRsqrt14Pd(src [8]float64, k uint8, a [8]float64) [8]float64


// MaskzRsqrt14Pd: Compute the approximate reciprocal square root of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := APPROXIMATE(1.0 / SQRT(a[i+63:i]))
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRSQRT14PD'. Intrinsic: '_mm512_maskz_rsqrt14_pd'.
// Requires AVX512F.
func MaskzRsqrt14Pd(k Mmask8, a M512d) M512d {
	return M512d(maskzRsqrt14Pd(uint8(k), [8]float64(a)))
}

func maskzRsqrt14Pd(k uint8, a [8]float64) [8]float64


// Rsqrt14Pd: Compute the approximate reciprocal square root of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst'. The maximum relative error for this approximation is less
// than 2^-14. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := APPROXIMATE(1.0 / SQRT(a[i+63:i]))
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRSQRT14PD'. Intrinsic: '_mm512_rsqrt14_pd'.
// Requires AVX512F.
func Rsqrt14Pd(a M512d) M512d {
	return M512d(rsqrt14Pd([8]float64(a)))
}

func rsqrt14Pd(a [8]float64) [8]float64


// MaskRsqrt14Ps: Compute the approximate reciprocal square root of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := APPROXIMATE(1.0 / SQRT(a[i+31:i]))
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRSQRT14PS'. Intrinsic: '_mm512_mask_rsqrt14_ps'.
// Requires AVX512F.
func MaskRsqrt14Ps(src M512, k Mmask16, a M512) M512 {
	return M512(maskRsqrt14Ps([16]float32(src), uint16(k), [16]float32(a)))
}

func maskRsqrt14Ps(src [16]float32, k uint16, a [16]float32) [16]float32


// MaskzRsqrt14Ps: Compute the approximate reciprocal square root of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := APPROXIMATE(1.0 / SQRT(a[i+31:i]))
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRSQRT14PS'. Intrinsic: '_mm512_maskz_rsqrt14_ps'.
// Requires AVX512F.
func MaskzRsqrt14Ps(k Mmask16, a M512) M512 {
	return M512(maskzRsqrt14Ps(uint16(k), [16]float32(a)))
}

func maskzRsqrt14Ps(k uint16, a [16]float32) [16]float32


// Rsqrt14Ps: Compute the approximate reciprocal square root of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst'. The maximum relative error for this approximation is less
// than 2^-14. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := APPROXIMATE(1.0 / SQRT(a[i+31:i]))
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRSQRT14PS'. Intrinsic: '_mm512_rsqrt14_ps'.
// Requires AVX512F.
func Rsqrt14Ps(a M512) M512 {
	return M512(rsqrt14Ps([16]float32(a)))
}

func rsqrt14Ps(a [16]float32) [16]float32


// MaskRsqrt23Ps: Calculates the reciprocal square root of packed
// single-precision (32-bit) floating-point elements in 'a' to 23 bits of
// accuracy and stores the result in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := Sqrt(1.0 / a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRSQRT23PS'. Intrinsic: '_mm512_mask_rsqrt23_ps'.
// Requires KNCNI.
func MaskRsqrt23Ps(src M512, k Mmask16, a M512) M512 {
	return M512(maskRsqrt23Ps([16]float32(src), uint16(k), [16]float32(a)))
}

func maskRsqrt23Ps(src [16]float32, k uint16, a [16]float32) [16]float32


// Rsqrt23Ps: Calculates the reciprocal square root of packed single-precision
// (32-bit) floating-point elements in 'a' to 23 bits of accuracy and stores
// the result in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := Sqrt(1.0 / a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRSQRT23PS'. Intrinsic: '_mm512_rsqrt23_ps'.
// Requires KNCNI.
func Rsqrt23Ps(a M512) M512 {
	return M512(rsqrt23Ps([16]float32(a)))
}

func rsqrt23Ps(a [16]float32) [16]float32


// MaskRsqrt28Pd: Compute the approximate reciprocal square root of packed
// double-precision (64-bit) floating-point elements in 'a', store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-28. 
//
//		FOR j := 0 to 7
//			i := j*64;
//			IF k[j] THEN
//				dst[i+63:i] := (1.0/SQRT(a[i+63:i]));
//			ELSE
//				dst[i+63:i] := src[i+63:i];
//			FI
//		ENDFOR;
//
// Instruction: 'VRSQRT28PD'. Intrinsic: '_mm512_mask_rsqrt28_pd'.
// Requires AVX512ER.
func MaskRsqrt28Pd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskRsqrt28Pd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskRsqrt28Pd(src [8]float64, k uint8, a [8]float64) [8]float64


// MaskzRsqrt28Pd: Compute the approximate reciprocal square root of packed
// double-precision (64-bit) floating-point elements in 'a', store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). The maximum relative error for this approximation is
// less than 2^-28. 
//
//		FOR j := 0 to 7
//			i := j*64;
//			IF k[j] THEN
//				dst[i+63:i] := (1.0/SQRT(a[i+63:i]));
//			ELSE
//				dst[i+63:i] := 0;
//			FI
//		ENDFOR;
//
// Instruction: 'VRSQRT28PD'. Intrinsic: '_mm512_maskz_rsqrt28_pd'.
// Requires AVX512ER.
func MaskzRsqrt28Pd(k Mmask8, a M512d) M512d {
	return M512d(maskzRsqrt28Pd(uint8(k), [8]float64(a)))
}

func maskzRsqrt28Pd(k uint8, a [8]float64) [8]float64


// Rsqrt28Pd: Compute the approximate reciprocal square root of packed
// double-precision (64-bit) floating-point elements in 'a', store the results
// in 'dst'. The maximum relative error for this approximation is less than
// 2^-28. 
//
//		FOR j := 0 to 7
//			i := j*64;
//			dst[i+63:i] := (1.0/SQRT(a[i+63:i]));
//		ENDFOR;
//
// Instruction: 'VRSQRT28PD'. Intrinsic: '_mm512_rsqrt28_pd'.
// Requires AVX512ER.
func Rsqrt28Pd(a M512d) M512d {
	return M512d(rsqrt28Pd([8]float64(a)))
}

func rsqrt28Pd(a [8]float64) [8]float64


// MaskRsqrt28Ps: Compute the approximate reciprocal square root of packed
// single-precision (32-bit) floating-point elements in 'a', store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-28. 
//
//		FOR j := 0 to 15
//			i := j*32;
//			IF k[j] THEN
//				dst[i+31:i] := (1.0/SQRT(a[i+31:i]));
//			ELSE
//				dst[i+31:i] := src[i+31:i];
//			FI
//		ENDFOR;
//
// Instruction: 'VRSQRT28PS'. Intrinsic: '_mm512_mask_rsqrt28_ps'.
// Requires AVX512ER.
func MaskRsqrt28Ps(src M512, k Mmask16, a M512) M512 {
	return M512(maskRsqrt28Ps([16]float32(src), uint16(k), [16]float32(a)))
}

func maskRsqrt28Ps(src [16]float32, k uint16, a [16]float32) [16]float32


// MaskzRsqrt28Ps: Compute the approximate reciprocal square root of packed
// single-precision (32-bit) floating-point elements in 'a', store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). The maximum relative error for this approximation is
// less than 2^-28. 
//
//		FOR j := 0 to 15
//			i := j*32;
//			IF k[j] THEN
//				dst[i+31:i] := (1.0/SQRT(a[i+31:i]));
//			ELSE
//				dst[i+31:i] := 0;
//			FI
//		ENDFOR;
//
// Instruction: 'VRSQRT28PS'. Intrinsic: '_mm512_maskz_rsqrt28_ps'.
// Requires AVX512ER.
func MaskzRsqrt28Ps(k Mmask16, a M512) M512 {
	return M512(maskzRsqrt28Ps(uint16(k), [16]float32(a)))
}

func maskzRsqrt28Ps(k uint16, a [16]float32) [16]float32


// Rsqrt28Ps: Compute the approximate reciprocal square root of packed
// single-precision (32-bit) floating-point elements in 'a', store the results
// in 'dst'. The maximum relative error for this approximation is less than
// 2^-28. 
//
//		FOR j := 0 to 15
//			i := j*32;
//			dst[i+31:i] := (1.0/SQRT(a[i+31:i]));
//		ENDFOR;
//
// Instruction: 'VRSQRT28PS'. Intrinsic: '_mm512_rsqrt28_ps'.
// Requires AVX512ER.
func Rsqrt28Ps(a M512) M512 {
	return M512(rsqrt28Ps([16]float32(a)))
}

func rsqrt28Ps(a [16]float32) [16]float32


// MaskRsqrt28RoundPd: Compute the approximate reciprocal square root of packed
// double-precision (64-bit) floating-point elements in 'a', store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-28. Rounding is done according to the
// 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64;
//			IF k[j] THEN
//				dst[i+63:i] := (1.0/SQRT(a[i+63:i]));
//			ELSE
//				dst[i+63:i] := src[i+63:i];
//			FI
//		ENDFOR;
//
// Instruction: 'VRSQRT28PD'. Intrinsic: '_mm512_mask_rsqrt28_round_pd'.
// Requires AVX512ER.
func MaskRsqrt28RoundPd(src M512d, k Mmask8, a M512d, rounding int) M512d {
	return M512d(maskRsqrt28RoundPd([8]float64(src), uint8(k), [8]float64(a), rounding))
}

func maskRsqrt28RoundPd(src [8]float64, k uint8, a [8]float64, rounding int) [8]float64


// MaskzRsqrt28RoundPd: Compute the approximate reciprocal square root of
// packed double-precision (64-bit) floating-point elements in 'a', store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-28. Rounding is done according to the
// 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64;
//			IF k[j] THEN
//				dst[i+63:i] := (1.0/SQRT(a[i+63:i]));
//			ELSE
//				dst[i+63:i] := 0;
//			FI
//		ENDFOR;
//
// Instruction: 'VRSQRT28PD'. Intrinsic: '_mm512_maskz_rsqrt28_round_pd'.
// Requires AVX512ER.
func MaskzRsqrt28RoundPd(k Mmask8, a M512d, rounding int) M512d {
	return M512d(maskzRsqrt28RoundPd(uint8(k), [8]float64(a), rounding))
}

func maskzRsqrt28RoundPd(k uint8, a [8]float64, rounding int) [8]float64


// Rsqrt28RoundPd: Compute the approximate reciprocal square root of packed
// double-precision (64-bit) floating-point elements in 'a', store the results
// in 'dst'. The maximum relative error for this approximation is less than
// 2^-28. Rounding is done according to the 'rounding' parameter, which can be
// one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64;
//			dst[i+63:i] := (1.0/SQRT(a[i+63:i]));
//		ENDFOR;
//
// Instruction: 'VRSQRT28PD'. Intrinsic: '_mm512_rsqrt28_round_pd'.
// Requires AVX512ER.
func Rsqrt28RoundPd(a M512d, rounding int) M512d {
	return M512d(rsqrt28RoundPd([8]float64(a), rounding))
}

func rsqrt28RoundPd(a [8]float64, rounding int) [8]float64


// MaskRsqrt28RoundPs: Compute the approximate reciprocal square root of packed
// single-precision (32-bit) floating-point elements in 'a', store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-28. Rounding is done according to the
// 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32;
//			IF k[j] THEN
//				dst[i+31:i] := (1.0/SQRT(a[i+31:i]));
//			ELSE
//				dst[i+31:i] := src[i+31:i];
//			FI
//		ENDFOR;
//
// Instruction: 'VRSQRT28PS'. Intrinsic: '_mm512_mask_rsqrt28_round_ps'.
// Requires AVX512ER.
func MaskRsqrt28RoundPs(src M512, k Mmask16, a M512, rounding int) M512 {
	return M512(maskRsqrt28RoundPs([16]float32(src), uint16(k), [16]float32(a), rounding))
}

func maskRsqrt28RoundPs(src [16]float32, k uint16, a [16]float32, rounding int) [16]float32


// MaskzRsqrt28RoundPs: Compute the approximate reciprocal square root of
// packed single-precision (32-bit) floating-point elements in 'a', store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-28. Rounding is done according to the
// 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32;
//			IF k[j] THEN
//				dst[i+31:i] := (1.0/SQRT(a[i+31:i]));
//			ELSE
//				dst[i+31:i] := 0;
//			FI
//		ENDFOR;
//
// Instruction: 'VRSQRT28PS'. Intrinsic: '_mm512_maskz_rsqrt28_round_ps'.
// Requires AVX512ER.
func MaskzRsqrt28RoundPs(k Mmask16, a M512, rounding int) M512 {
	return M512(maskzRsqrt28RoundPs(uint16(k), [16]float32(a), rounding))
}

func maskzRsqrt28RoundPs(k uint16, a [16]float32, rounding int) [16]float32


// Rsqrt28RoundPs: Compute the approximate reciprocal square root of packed
// single-precision (32-bit) floating-point elements in 'a', store the results
// in 'dst'. The maximum relative error for this approximation is less than
// 2^-28. Rounding is done according to the 'rounding' parameter, which can be
// one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32;
//			dst[i+31:i] := (1.0/SQRT(a[i+31:i]));
//		ENDFOR;
//
// Instruction: 'VRSQRT28PS'. Intrinsic: '_mm512_rsqrt28_round_ps'.
// Requires AVX512ER.
func Rsqrt28RoundPs(a M512, rounding int) M512 {
	return M512(rsqrt28RoundPs([16]float32(a), rounding))
}

func rsqrt28RoundPs(a [16]float32, rounding int) [16]float32


// SadEpu8: Compute the absolute differences of packed unsigned 8-bit integers
// in 'a' and 'b', then horizontally sum each consecutive 8 differences to
// produce four unsigned 16-bit integers, and pack these unsigned 16-bit
// integers in the low 16 bits of 64-bit elements in 'dst'. 
//
//		FOR j := 0 to 63
//			i := j*8
//			tmp[i+7:i] := ABS(a[i+7:i] - b[i+7:i])
//		ENDFOR
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+15:i] := tmp[i+7:i] + tmp[i+15:i+8] + tmp[i+23:i+16] + tmp[i+31:i+24] + tmp[i+39:i+32] + tmp[i+47:i+40] + tmp[i+55:i+48] + tmp[i+63:i+56]
//			dst[i+63:i+16] := 0
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSADBW'. Intrinsic: '_mm512_sad_epu8'.
// Requires AVX512BW.
func SadEpu8(a M512i, b M512i) M512i {
	return M512i(sadEpu8([64]byte(a), [64]byte(b)))
}

func sadEpu8(a [64]byte, b [64]byte) [64]byte


// MaskSbbEpi32: Performs element-by-element three-input subtraction of packed
// 32-bit integer elements of 'v3' as well as the corresponding bit from 'k2'
// from 'v2'. The borrowed value from the subtraction difference for the nth
// element is written to the nth bit of 'borrow' (borrow flag). Results are
// stored in 'dst' using writemask 'k1' (elements are copied from 'v2' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k1[j]
//				dst[i+31:i] := v2[i+31:i] - v3[i+31:i] - k2[j]
//				borrow[j] := Borrow(v2[i+31:i] - v3[i+31:i] - k2[j])
//			ELSE
//				dst[i+31:i] := v2[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSBBD'. Intrinsic: '_mm512_mask_sbb_epi32'.
// Requires KNCNI.
func MaskSbbEpi32(v2 M512i, k1 Mmask16, k2 Mmask16, v3 M512i, borrow Mmask16) M512i {
	return M512i(maskSbbEpi32([64]byte(v2), uint16(k1), uint16(k2), [64]byte(v3), uint16(borrow)))
}

func maskSbbEpi32(v2 [64]byte, k1 uint16, k2 uint16, v3 [64]byte, borrow uint16) [64]byte


// SbbEpi32: Performs element-by-element three-input subtraction of packed
// 32-bit integer elements of 'v3' as well as the corresponding bit from 'k'
// from 'v2'. The borrowed value from the subtraction difference for the nth
// element is written to the nth bit of 'borrow' (borrow flag). Results are
// stored in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := v2[i+31:i] - v3[i+31:i] - k[j]
//			borrow[j] := Borrow(v2[i+31:i] - v3[i+31:i] - k[j])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSBBD'. Intrinsic: '_mm512_sbb_epi32'.
// Requires KNCNI.
func SbbEpi32(v2 M512i, k Mmask16, v3 M512i, borrow Mmask16) M512i {
	return M512i(sbbEpi32([64]byte(v2), uint16(k), [64]byte(v3), uint16(borrow)))
}

func sbbEpi32(v2 [64]byte, k uint16, v3 [64]byte, borrow uint16) [64]byte


// MaskSbbrEpi32: Performs element-by-element three-input subtraction of packed
// 32-bit integer elements of 'v2' as well as the corresponding bit from 'k2'
// from 'v3'. The borrowed value from the subtraction difference for the nth
// element is written to the nth bit of 'borrow' (borrow flag). Results are
// stored in 'dst' using writemask 'k1' (elements are copied from 'v2' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k1[j]
//				dst[i+31:i] := v3[i+31:i] - v2[i+31:i] - k2[j]
//				borrow[j] := Borrow(v2[i+31:i] - v3[i+31:i] - k[j])
//			ELSE
//				dst[i+31:i] := v2[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSBBRD'. Intrinsic: '_mm512_mask_sbbr_epi32'.
// Requires KNCNI.
func MaskSbbrEpi32(v2 M512i, k1 Mmask16, k2 Mmask16, v3 M512i, borrow Mmask16) M512i {
	return M512i(maskSbbrEpi32([64]byte(v2), uint16(k1), uint16(k2), [64]byte(v3), uint16(borrow)))
}

func maskSbbrEpi32(v2 [64]byte, k1 uint16, k2 uint16, v3 [64]byte, borrow uint16) [64]byte


// SbbrEpi32: Performs element-by-element three-input subtraction of packed
// 32-bit integer elements of 'v2' as well as the corresponding bit from 'k'
// from 'v3'. The borrowed value from the subtraction difference for the nth
// element is written to the nth bit of 'borrow' (borrow flag). Results are
// stored in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := v3[i+31:i] - v2[i+31:i] - k[j]
//			borrow[j] := Borrow(v2[i+31:i] - v3[i+31:i] - k[j])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSBBRD'. Intrinsic: '_mm512_sbbr_epi32'.
// Requires KNCNI.
func SbbrEpi32(v2 M512i, k Mmask16, v3 M512i, borrow Mmask16) M512i {
	return M512i(sbbrEpi32([64]byte(v2), uint16(k), [64]byte(v3), uint16(borrow)))
}

func sbbrEpi32(v2 [64]byte, k uint16, v3 [64]byte, borrow uint16) [64]byte


// MaskScalePs: Scales each single-precision (32-bit) floating-point element in
// 'a' by multiplying it by 2**exponent, where the exponenet is the
// corresponding 32-bit integer element in 'b', storing results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] * Pow(2, b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSCALEPS'. Intrinsic: '_mm512_mask_scale_ps'.
// Requires KNCNI.
func MaskScalePs(src M512, k Mmask16, a M512, b M512i) M512 {
	return M512(maskScalePs([16]float32(src), uint16(k), [16]float32(a), [64]byte(b)))
}

func maskScalePs(src [16]float32, k uint16, a [16]float32, b [64]byte) [16]float32


// ScalePs: Scales each single-precision (32-bit) floating-point element in 'a'
// by multiplying it by 2**exponent, where the exponent is the corresponding
// 32-bit integer element in 'b', storing results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := a[i+31:i] * Pow(2, b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSCALEPS'. Intrinsic: '_mm512_scale_ps'.
// Requires KNCNI.
func ScalePs(a M512, b M512i) M512 {
	return M512(scalePs([16]float32(a), [64]byte(b)))
}

func scalePs(a [16]float32, b [64]byte) [16]float32


// MaskScaleRoundPs: Scales each single-precision (32-bit) floating-point
// element in 'a' by multiplying it by 2**exp, where the exp is the
// corresponding 32-bit integer element in 'b', storing results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). Results are rounded using constant 'rounding'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] * Pow(2, b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSCALEPS'. Intrinsic: '_mm512_mask_scale_round_ps'.
// Requires KNCNI.
func MaskScaleRoundPs(src M512, k Mmask16, a M512, b M512i, rounding int) M512 {
	return M512(maskScaleRoundPs([16]float32(src), uint16(k), [16]float32(a), [64]byte(b), rounding))
}

func maskScaleRoundPs(src [16]float32, k uint16, a [16]float32, b [64]byte, rounding int) [16]float32


// ScaleRoundPs: Scales each single-precision (32-bit) floating-point element
// in 'a' by multiplying it by 2**exponent, where the exponenet is the
// corresponding 32-bit integer element in 'b', storing results in 'dst'.
// Intermediate elements are rounded using 'rounding'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := a[i+31:i] * Pow(2, b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_scale_round_ps'.
// Requires KNCNI.
func ScaleRoundPs(a M512, b M512i, rounding int) M512 {
	return M512(scaleRoundPs([16]float32(a), [64]byte(b), rounding))
}

func scaleRoundPs(a [16]float32, b [64]byte, rounding int) [16]float32


// MaskScalefPd: Scale the packed double-precision (64-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSCALEFPD'. Intrinsic: '_mm512_mask_scalef_pd'.
// Requires AVX512F.
func MaskScalefPd(src M512d, k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskScalefPd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b)))
}

func maskScalefPd(src [8]float64, k uint8, a [8]float64, b [8]float64) [8]float64


// MaskzScalefPd: Scale the packed double-precision (64-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSCALEFPD'. Intrinsic: '_mm512_maskz_scalef_pd'.
// Requires AVX512F.
func MaskzScalefPd(k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskzScalefPd(uint8(k), [8]float64(a), [8]float64(b)))
}

func maskzScalefPd(k uint8, a [8]float64, b [8]float64) [8]float64


// ScalefPd: Scale the packed double-precision (64-bit) floating-point elements
// in 'a' using values from 'b', and store the results in 'dst'. 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSCALEFPD'. Intrinsic: '_mm512_scalef_pd'.
// Requires AVX512F.
func ScalefPd(a M512d, b M512d) M512d {
	return M512d(scalefPd([8]float64(a), [8]float64(b)))
}

func scalefPd(a [8]float64, b [8]float64) [8]float64


// MaskScalefPs: Scale the packed single-precision (32-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[31:0]
//		}
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSCALEFPS'. Intrinsic: '_mm512_mask_scalef_ps'.
// Requires AVX512F.
func MaskScalefPs(src M512, k Mmask16, a M512, b M512) M512 {
	return M512(maskScalefPs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b)))
}

func maskScalefPs(src [16]float32, k uint16, a [16]float32, b [16]float32) [16]float32


// MaskzScalefPs: Scale the packed single-precision (32-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[31:0]
//		}
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSCALEFPS'. Intrinsic: '_mm512_maskz_scalef_ps'.
// Requires AVX512F.
func MaskzScalefPs(k Mmask16, a M512, b M512) M512 {
	return M512(maskzScalefPs(uint16(k), [16]float32(a), [16]float32(b)))
}

func maskzScalefPs(k uint16, a [16]float32, b [16]float32) [16]float32


// ScalefPs: Scale the packed single-precision (32-bit) floating-point elements
// in 'a' using values from 'b', and store the results in 'dst'. 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[31:0]
//		}
//		
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSCALEFPS'. Intrinsic: '_mm512_scalef_ps'.
// Requires AVX512F.
func ScalefPs(a M512, b M512) M512 {
	return M512(scalefPs([16]float32(a), [16]float32(b)))
}

func scalefPs(a [16]float32, b [16]float32) [16]float32


// MaskScalefRoundPd: Scale the packed double-precision (64-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSCALEFPD'. Intrinsic: '_mm512_mask_scalef_round_pd'.
// Requires AVX512F.
func MaskScalefRoundPd(src M512d, k Mmask8, a M512d, b M512d, rounding int) M512d {
	return M512d(maskScalefRoundPd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b), rounding))
}

func maskScalefRoundPd(src [8]float64, k uint8, a [8]float64, b [8]float64, rounding int) [8]float64


// MaskzScalefRoundPd: Scale the packed double-precision (64-bit)
// floating-point elements in 'a' using values from 'b', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSCALEFPD'. Intrinsic: '_mm512_maskz_scalef_round_pd'.
// Requires AVX512F.
func MaskzScalefRoundPd(k Mmask8, a M512d, b M512d, rounding int) M512d {
	return M512d(maskzScalefRoundPd(uint8(k), [8]float64(a), [8]float64(b), rounding))
}

func maskzScalefRoundPd(k uint8, a [8]float64, b [8]float64, rounding int) [8]float64


// ScalefRoundPd: Scale the packed double-precision (64-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSCALEFPD'. Intrinsic: '_mm512_scalef_round_pd'.
// Requires AVX512F.
func ScalefRoundPd(a M512d, b M512d, rounding int) M512d {
	return M512d(scalefRoundPd([8]float64(a), [8]float64(b), rounding))
}

func scalefRoundPd(a [8]float64, b [8]float64, rounding int) [8]float64


// MaskScalefRoundPs: Scale the packed single-precision (32-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[31:0]
//		}
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSCALEFPS'. Intrinsic: '_mm512_mask_scalef_round_ps'.
// Requires AVX512F.
func MaskScalefRoundPs(src M512, k Mmask16, a M512, b M512, rounding int) M512 {
	return M512(maskScalefRoundPs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b), rounding))
}

func maskScalefRoundPs(src [16]float32, k uint16, a [16]float32, b [16]float32, rounding int) [16]float32


// MaskzScalefRoundPs: Scale the packed single-precision (32-bit)
// floating-point elements in 'a' using values from 'b', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[31:0]
//		}
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSCALEFPS'. Intrinsic: '_mm512_maskz_scalef_round_ps'.
// Requires AVX512F.
func MaskzScalefRoundPs(k Mmask16, a M512, b M512, rounding int) M512 {
	return M512(maskzScalefRoundPs(uint16(k), [16]float32(a), [16]float32(b), rounding))
}

func maskzScalefRoundPs(k uint16, a [16]float32, b [16]float32, rounding int) [16]float32


// ScalefRoundPs: Scale the packed single-precision (32-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[31:0]
//		}
//		
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSCALEFPS'. Intrinsic: '_mm512_scalef_round_ps'.
// Requires AVX512F.
func ScalefRoundPs(a M512, b M512, rounding int) M512 {
	return M512(scalefRoundPs([16]float32(a), [16]float32(b), rounding))
}

func scalefRoundPs(a [16]float32, b [16]float32, rounding int) [16]float32


// SetEpi32: Set packed 32-bit integers in 'dst' with the supplied values. 
//
//		dst[31:0] := e0
//		dst[63:32] := e1
//		dst[95:64] := e2
//		dst[127:96] := e3
//		dst[159:128] := e4
//		dst[191:160] := e5
//		dst[223:192] := e6
//		dst[255:224] := e7
//		dst[287:256] := e8
//		dst[319:288] := e9
//		dst[351:320] := e10
//		dst[383:352] := e11
//		dst[415:384] := e12
//		dst[447:416] := e13
//		dst[479:448] := e14
//		dst[511:480] := e15
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_set_epi32'.
// Requires AVX512F.
func SetEpi32(e15 int, e14 int, e13 int, e12 int, e11 int, e10 int, e9 int, e8 int, e7 int, e6 int, e5 int, e4 int, e3 int, e2 int, e1 int, e0 int) M512i {
	return M512i(setEpi32(e15, e14, e13, e12, e11, e10, e9, e8, e7, e6, e5, e4, e3, e2, e1, e0))
}

func setEpi32(e15 int, e14 int, e13 int, e12 int, e11 int, e10 int, e9 int, e8 int, e7 int, e6 int, e5 int, e4 int, e3 int, e2 int, e1 int, e0 int) [64]byte


// SetEpi64: Set packed 64-bit integers in 'dst' with the supplied values. 
//
//		dst[63:0] := e0
//		dst[127:64] := e1
//		dst[191:128] := e2
//		dst[255:192] := e3
//		dst[319:256] := e4
//		dst[383:320] := e5
//		dst[447:384] := e6
//		dst[511:448] := e7
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_set_epi64'.
// Requires AVX512F.
func SetEpi64(e7 int64, e6 int64, e5 int64, e4 int64, e3 int64, e2 int64, e1 int64, e0 int64) M512i {
	return M512i(setEpi64(e7, e6, e5, e4, e3, e2, e1, e0))
}

func setEpi64(e7 int64, e6 int64, e5 int64, e4 int64, e3 int64, e2 int64, e1 int64, e0 int64) [64]byte


// SetPd: Set packed double-precision (64-bit) floating-point elements in 'dst'
// with the supplied values. 
//
//		dst[63:0] := e0
//		dst[127:64] := e1
//		dst[191:128] := e2
//		dst[255:192] := e3
//		dst[319:256] := e4
//		dst[383:320] := e5
//		dst[447:384] := e6
//		dst[511:448] := e7
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_set_pd'.
// Requires AVX512F.
func SetPd(e7 float64, e6 float64, e5 float64, e4 float64, e3 float64, e2 float64, e1 float64, e0 float64) M512d {
	return M512d(setPd(e7, e6, e5, e4, e3, e2, e1, e0))
}

func setPd(e7 float64, e6 float64, e5 float64, e4 float64, e3 float64, e2 float64, e1 float64, e0 float64) [8]float64


// SetPs: Set packed single-precision (32-bit) floating-point elements in 'dst'
// with the supplied values. 
//
//		dst[31:0] := e0
//		dst[63:32] := e1
//		dst[95:64] := e2
//		dst[127:96] := e3
//		dst[159:128] := e4
//		dst[191:160] := e5
//		dst[223:192] := e6
//		dst[255:224] := e7
//		dst[287:256] := e8
//		dst[319:288] := e9
//		dst[351:320] := e10
//		dst[383:352] := e11
//		dst[415:384] := e12
//		dst[447:416] := e13
//		dst[479:448] := e14
//		dst[511:480] := e15
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_set_ps'.
// Requires AVX512F.
func SetPs(e15 float32, e14 float32, e13 float32, e12 float32, e11 float32, e10 float32, e9 float32, e8 float32, e7 float32, e6 float32, e5 float32, e4 float32, e3 float32, e2 float32, e1 float32, e0 float32) M512 {
	return M512(setPs(e15, e14, e13, e12, e11, e10, e9, e8, e7, e6, e5, e4, e3, e2, e1, e0))
}

func setPs(e15 float32, e14 float32, e13 float32, e12 float32, e11 float32, e10 float32, e9 float32, e8 float32, e7 float32, e6 float32, e5 float32, e4 float32, e3 float32, e2 float32, e1 float32, e0 float32) [16]float32


// MaskSet1Epi16: Broadcast 16-bit integer 'a' to all elements of 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := a[15:0]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTW'. Intrinsic: '_mm512_mask_set1_epi16'.
// Requires AVX512BW.
func MaskSet1Epi16(src M512i, k Mmask32, a int16) M512i {
	return M512i(maskSet1Epi16([64]byte(src), uint32(k), a))
}

func maskSet1Epi16(src [64]byte, k uint32, a int16) [64]byte


// MaskzSet1Epi16: Broadcast the low packed 16-bit integer from 'a' to all
// elements of 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := a[15:0]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTW'. Intrinsic: '_mm512_maskz_set1_epi16'.
// Requires AVX512BW.
func MaskzSet1Epi16(k Mmask32, a int16) M512i {
	return M512i(maskzSet1Epi16(uint32(k), a))
}

func maskzSet1Epi16(k uint32, a int16) [64]byte


// Set1Epi16: Broadcast the low packed 16-bit integer from 'a' to all all
// elements of 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			dst[i+15:i] := a[15:0]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_set1_epi16'.
// Requires AVX512F.
func Set1Epi16(a int16) M512i {
	return M512i(set1Epi16(a))
}

func set1Epi16(a int16) [64]byte


// MaskSet1Epi32: Broadcast 32-bit integer 'a' to all elements of 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm512_mask_set1_epi32'.
// Requires AVX512F.
func MaskSet1Epi32(src M512i, k Mmask16, a int) M512i {
	return M512i(maskSet1Epi32([64]byte(src), uint16(k), a))
}

func maskSet1Epi32(src [64]byte, k uint16, a int) [64]byte


// MaskzSet1Epi32: Broadcast 32-bit integer 'a' to all elements of 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm512_maskz_set1_epi32'.
// Requires AVX512F.
func MaskzSet1Epi32(k Mmask16, a int) M512i {
	return M512i(maskzSet1Epi32(uint16(k), a))
}

func maskzSet1Epi32(k uint16, a int) [64]byte


// Set1Epi32: Broadcast 32-bit integer 'a' to all elements of 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := a[31:0]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm512_set1_epi32'.
// Requires AVX512F.
func Set1Epi32(a int) M512i {
	return M512i(set1Epi32(a))
}

func set1Epi32(a int) [64]byte


// MaskSet1Epi64: Broadcast 64-bit integer 'a' to all elements of 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm512_mask_set1_epi64'.
// Requires AVX512F.
func MaskSet1Epi64(src M512i, k Mmask8, a int64) M512i {
	return M512i(maskSet1Epi64([64]byte(src), uint8(k), a))
}

func maskSet1Epi64(src [64]byte, k uint8, a int64) [64]byte


// MaskzSet1Epi64: Broadcast 64-bit integer 'a' to all elements of 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm512_maskz_set1_epi64'.
// Requires AVX512F.
func MaskzSet1Epi64(k Mmask8, a int64) M512i {
	return M512i(maskzSet1Epi64(uint8(k), a))
}

func maskzSet1Epi64(k uint8, a int64) [64]byte


// Set1Epi64: Broadcast 64-bit integer 'a' to all elements of 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := a[63:0]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm512_set1_epi64'.
// Requires AVX512F.
func Set1Epi64(a int64) M512i {
	return M512i(set1Epi64(a))
}

func set1Epi64(a int64) [64]byte


// MaskSet1Epi8: Broadcast 8-bit integer 'a' to all elements of 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := a[7:0]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTB'. Intrinsic: '_mm512_mask_set1_epi8'.
// Requires AVX512BW.
func MaskSet1Epi8(src M512i, k Mmask64, a byte) M512i {
	return M512i(maskSet1Epi8([64]byte(src), uint64(k), a))
}

func maskSet1Epi8(src [64]byte, k uint64, a byte) [64]byte


// MaskzSet1Epi8: Broadcast 8-bit integer 'a' to all elements of 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := a[7:0]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTB'. Intrinsic: '_mm512_maskz_set1_epi8'.
// Requires AVX512BW.
func MaskzSet1Epi8(k Mmask64, a byte) M512i {
	return M512i(maskzSet1Epi8(uint64(k), a))
}

func maskzSet1Epi8(k uint64, a byte) [64]byte


// Set1Epi8: Broadcast 8-bit integer 'a' to all elements of 'dst'. 
//
//		FOR j := 0 to 63
//			i := j*8
//			dst[i+7:i] := a[7:0]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_set1_epi8'.
// Requires AVX512F.
func Set1Epi8(a byte) M512i {
	return M512i(set1Epi8(a))
}

func set1Epi8(a byte) [64]byte


// Set1Pd: Broadcast double-precision (64-bit) floating-point value 'a' to all
// elements of 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := a[63:0]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_set1_pd'.
// Requires AVX512F.
func Set1Pd(a float64) M512d {
	return M512d(set1Pd(a))
}

func set1Pd(a float64) [8]float64


// Set1Ps: Broadcast single-precision (32-bit) floating-point value 'a' to all
// elements of 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := a[31:0]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_set1_ps'.
// Requires AVX512F.
func Set1Ps(a float32) M512 {
	return M512(set1Ps(a))
}

func set1Ps(a float32) [16]float32


// Set4Epi32: Set packed 32-bit integers in 'dst' with the repeated 4 element
// sequence. 
//
//		dst[31:0] := d
//		dst[63:32] := c
//		dst[95:64] := b
//		dst[127:96] := a
//		dst[159:128] := d
//		dst[191:160] := c
//		dst[223:192] := b
//		dst[255:224] := a
//		dst[287:256] := d
//		dst[319:288] := c
//		dst[351:320] := b
//		dst[383:352] := a
//		dst[415:384] := d
//		dst[447:416] := c
//		dst[479:448] := b
//		dst[511:480] := a
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_set4_epi32'.
// Requires AVX512F.
func Set4Epi32(d int, c int, b int, a int) M512i {
	return M512i(set4Epi32(d, c, b, a))
}

func set4Epi32(d int, c int, b int, a int) [64]byte


// Set4Epi64: Set packed 64-bit integers in 'dst' with the repeated 4 element
// sequence. 
//
//		dst[63:0] := d
//		dst[127:64] := c
//		dst[191:128] := b
//		dst[255:192] := a
//		dst[319:256] := d
//		dst[383:320] := c
//		dst[447:384] := b
//		dst[511:448] := a
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_set4_epi64'.
// Requires AVX512F.
func Set4Epi64(d int64, c int64, b int64, a int64) M512i {
	return M512i(set4Epi64(d, c, b, a))
}

func set4Epi64(d int64, c int64, b int64, a int64) [64]byte


// Set4Pd: Set packed double-precision (64-bit) floating-point elements in
// 'dst' with the repeated 4 element sequence. 
//
//		dst[63:0] := d
//		dst[127:64] := c
//		dst[191:128] := b
//		dst[255:192] := a
//		dst[319:256] := d
//		dst[383:320] := c
//		dst[447:384] := b
//		dst[511:448] := a
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_set4_pd'.
// Requires AVX512F.
func Set4Pd(d float64, c float64, b float64, a float64) M512d {
	return M512d(set4Pd(d, c, b, a))
}

func set4Pd(d float64, c float64, b float64, a float64) [8]float64


// Set4Ps: Set packed single-precision (32-bit) floating-point elements in
// 'dst' with the repeated 4 element sequence. 
//
//		dst[31:0] := d
//		dst[63:32] := c
//		dst[95:64] := b
//		dst[127:96] := a
//		dst[159:128] := d
//		dst[191:160] := c
//		dst[223:192] := b
//		dst[255:224] := a
//		dst[287:256] := d
//		dst[319:288] := c
//		dst[351:320] := b
//		dst[383:352] := a
//		dst[415:384] := d
//		dst[447:416] := c
//		dst[479:448] := b
//		dst[511:480] := a
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_set4_ps'.
// Requires AVX512F.
func Set4Ps(d float32, c float32, b float32, a float32) M512 {
	return M512(set4Ps(d, c, b, a))
}

func set4Ps(d float32, c float32, b float32, a float32) [16]float32


// SetrEpi32: Set packed 32-bit integers in 'dst' with the supplied values in
// reverse order. 
//
//		dst[31:0] := e15
//		dst[63:32] := e14
//		dst[95:64] := e13
//		dst[127:96] := e12
//		dst[159:128] := e11
//		dst[191:160] := e10
//		dst[223:192] := e9
//		dst[255:224] := e8
//		dst[287:256] := e7
//		dst[319:288] := e6
//		dst[351:320] := e5
//		dst[383:352] := e4
//		dst[415:384] := e3
//		dst[447:416] := e2
//		dst[479:448] := e1
//		dst[511:480] := e0
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_setr_epi32'.
// Requires AVX512F.
func SetrEpi32(e15 int, e14 int, e13 int, e12 int, e11 int, e10 int, e9 int, e8 int, e7 int, e6 int, e5 int, e4 int, e3 int, e2 int, e1 int, e0 int) M512i {
	return M512i(setrEpi32(e15, e14, e13, e12, e11, e10, e9, e8, e7, e6, e5, e4, e3, e2, e1, e0))
}

func setrEpi32(e15 int, e14 int, e13 int, e12 int, e11 int, e10 int, e9 int, e8 int, e7 int, e6 int, e5 int, e4 int, e3 int, e2 int, e1 int, e0 int) [64]byte


// SetrEpi64: Set packed 64-bit integers in 'dst' with the supplied values in
// reverse order. 
//
//		dst[63:0] := e7
//		dst[127:64] := e6
//		dst[191:128] := e5
//		dst[255:192] := e4
//		dst[319:256] := e3
//		dst[383:320] := e2
//		dst[447:384] := e1
//		dst[511:448] := e0
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_setr_epi64'.
// Requires AVX512F.
func SetrEpi64(e7 int64, e6 int64, e5 int64, e4 int64, e3 int64, e2 int64, e1 int64, e0 int64) M512i {
	return M512i(setrEpi64(e7, e6, e5, e4, e3, e2, e1, e0))
}

func setrEpi64(e7 int64, e6 int64, e5 int64, e4 int64, e3 int64, e2 int64, e1 int64, e0 int64) [64]byte


// SetrPd: Set packed double-precision (64-bit) floating-point elements in
// 'dst' with the supplied values in reverse order. 
//
//		dst[63:0] := e7
//		dst[127:64] := e6
//		dst[191:128] := e5
//		dst[255:192] := e4
//		dst[319:256] := e3
//		dst[383:320] := e2
//		dst[447:384] := e1
//		dst[511:448] := e0
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_setr_pd'.
// Requires AVX512F.
func SetrPd(e7 float64, e6 float64, e5 float64, e4 float64, e3 float64, e2 float64, e1 float64, e0 float64) M512d {
	return M512d(setrPd(e7, e6, e5, e4, e3, e2, e1, e0))
}

func setrPd(e7 float64, e6 float64, e5 float64, e4 float64, e3 float64, e2 float64, e1 float64, e0 float64) [8]float64


// SetrPs: Set packed single-precision (32-bit) floating-point elements in
// 'dst' with the supplied values in reverse order. 
//
//		dst[31:0] := e15
//		dst[63:32] := e14
//		dst[95:64] := e13
//		dst[127:96] := e12
//		dst[159:128] := e11
//		dst[191:160] := e10
//		dst[223:192] := e9
//		dst[255:224] := e8
//		dst[287:256] := e7
//		dst[319:288] := e6
//		dst[351:320] := e5
//		dst[383:352] := e4
//		dst[415:384] := e3
//		dst[447:416] := e2
//		dst[479:448] := e1
//		dst[511:480] := e0
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_setr_ps'.
// Requires AVX512F.
func SetrPs(e15 float32, e14 float32, e13 float32, e12 float32, e11 float32, e10 float32, e9 float32, e8 float32, e7 float32, e6 float32, e5 float32, e4 float32, e3 float32, e2 float32, e1 float32, e0 float32) M512 {
	return M512(setrPs(e15, e14, e13, e12, e11, e10, e9, e8, e7, e6, e5, e4, e3, e2, e1, e0))
}

func setrPs(e15 float32, e14 float32, e13 float32, e12 float32, e11 float32, e10 float32, e9 float32, e8 float32, e7 float32, e6 float32, e5 float32, e4 float32, e3 float32, e2 float32, e1 float32, e0 float32) [16]float32


// Setr4Epi32: Set packed 32-bit integers in 'dst' with the repeated 4 element
// sequence in reverse order. 
//
//		dst[31:0] := a
//		dst[63:32] := b
//		dst[95:64] := c
//		dst[127:96] := d
//		dst[159:128] := a
//		dst[191:160] := b
//		dst[223:192] := c
//		dst[255:224] := d
//		dst[287:256] := a
//		dst[319:288] := b
//		dst[351:320] := c
//		dst[383:352] := d
//		dst[415:384] := a
//		dst[447:416] := b
//		dst[479:448] := c
//		dst[511:480] := d
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_setr4_epi32'.
// Requires AVX512F.
func Setr4Epi32(d int, c int, b int, a int) M512i {
	return M512i(setr4Epi32(d, c, b, a))
}

func setr4Epi32(d int, c int, b int, a int) [64]byte


// Setr4Epi64: Set packed 64-bit integers in 'dst' with the repeated 4 element
// sequence in reverse order. 
//
//		dst[63:0] := a
//		dst[127:64] := b
//		dst[191:128] := c
//		dst[255:192] := d
//		dst[319:256] := a
//		dst[383:320] := b
//		dst[447:384] := c
//		dst[511:448] := d
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_setr4_epi64'.
// Requires AVX512F.
func Setr4Epi64(d int64, c int64, b int64, a int64) M512i {
	return M512i(setr4Epi64(d, c, b, a))
}

func setr4Epi64(d int64, c int64, b int64, a int64) [64]byte


// Setr4Pd: Set packed double-precision (64-bit) floating-point elements in
// 'dst' with the repeated 4 element sequence in reverse order. 
//
//		dst[63:0] := a
//		dst[127:64] := b
//		dst[191:128] := c
//		dst[255:192] := d
//		dst[319:256] := a
//		dst[383:320] := b
//		dst[447:384] := c
//		dst[511:448] := d
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_setr4_pd'.
// Requires AVX512F.
func Setr4Pd(d float64, c float64, b float64, a float64) M512d {
	return M512d(setr4Pd(d, c, b, a))
}

func setr4Pd(d float64, c float64, b float64, a float64) [8]float64


// Setr4Ps: Set packed single-precision (32-bit) floating-point elements in
// 'dst' with the repeated 4 element sequence in reverse order. 
//
//		dst[31:0] := a
//		dst[63:32] := b
//		dst[95:64] := c
//		dst[127:96] := d
//		dst[159:128] := a
//		dst[191:160] := b
//		dst[223:192] := c
//		dst[255:224] := d
//		dst[287:256] := a
//		dst[319:288] := b
//		dst[351:320] := c
//		dst[383:352] := d
//		dst[415:384] := a
//		dst[447:416] := b
//		dst[479:448] := c
//		dst[511:480] := d
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_setr4_ps'.
// Requires AVX512F.
func Setr4Ps(d float32, c float32, b float32, a float32) M512 {
	return M512(setr4Ps(d, c, b, a))
}

func setr4Ps(d float32, c float32, b float32, a float32) [16]float32


// Setzero: Return vector of type __m512 with all elements set to zero. 
//
//		dst[MAX:0] := 0
//
// Instruction: 'VPXORQ'. Intrinsic: '_mm512_setzero'.
// Requires AVX512F.
func Setzero() M512 {
	return M512(setzero())
}

func setzero() [16]float32


// SetzeroEpi32: Return vector of type __m512i with all elements set to zero. 
//
//		dst[MAX:0] := 0
//
// Instruction: 'VPXORQ'. Intrinsic: '_mm512_setzero_epi32'.
// Requires AVX512F.
func SetzeroEpi32() M512i {
	return M512i(setzeroEpi32())
}

func setzeroEpi32() [64]byte


// SetzeroPd: Return vector of type __m512d with all elements set to zero. 
//
//		dst[MAX:0] := 0
//
// Instruction: 'VPXORQ'. Intrinsic: '_mm512_setzero_pd'.
// Requires AVX512F.
func SetzeroPd() M512d {
	return M512d(setzeroPd())
}

func setzeroPd() [8]float64


// SetzeroPs: Return vector of type __m512 with all elements set to zero. 
//
//		dst[MAX:0] := 0
//
// Instruction: 'VPXORQ'. Intrinsic: '_mm512_setzero_ps'.
// Requires AVX512F.
func SetzeroPs() M512 {
	return M512(setzeroPs())
}

func setzeroPs() [16]float32


// SetzeroSi512: Return vector of type __m512i with all elements set to zero. 
//
//		dst[MAX:0] := 0
//
// Instruction: 'VPXORQ'. Intrinsic: '_mm512_setzero_si512'.
// Requires AVX512F.
func SetzeroSi512() M512i {
	return M512i(setzeroSi512())
}

func setzeroSi512() [64]byte


// MaskShuffleEpi32: Shuffle 32-bit integers in 'a' within 128-bit lanes using
// the control in 'imm8', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		tmp_dst[223:192] := SELECT4(a[255:128], imm8[5:4])
//		tmp_dst[255:224] := SELECT4(a[255:128], imm8[7:6])
//		tmp_dst[287:256] := SELECT4(a[383:256], imm8[1:0])
//		tmp_dst[319:288] := SELECT4(a[383:256], imm8[3:2])
//		tmp_dst[351:320] := SELECT4(a[383:256], imm8[5:4])
//		tmp_dst[383:352] := SELECT4(a[383:256], imm8[7:6])
//		tmp_dst[415:384] := SELECT4(a[511:384], imm8[1:0])
//		tmp_dst[447:416] := SELECT4(a[511:384], imm8[3:2])
//		tmp_dst[479:448] := SELECT4(a[511:384], imm8[5:4])
//		tmp_dst[511:480] := SELECT4(a[511:384], imm8[7:6])
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSHUFD'. Intrinsic: '_mm512_mask_shuffle_epi32'.
// Requires KNCNI.
func MaskShuffleEpi32(src M512i, k Mmask16, a M512i, imm8 MMPERMENUM) M512i {
	return M512i(maskShuffleEpi32([64]byte(src), uint16(k), [64]byte(a), imm8))
}

func maskShuffleEpi32(src [64]byte, k uint16, a [64]byte, imm8 MMPERMENUM) [64]byte


// MaskzShuffleEpi32: Shuffle 32-bit integers in 'a' within 128-bit lanes using
// the control in 'imm8', and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		tmp_dst[223:192] := SELECT4(a[255:128], imm8[5:4])
//		tmp_dst[255:224] := SELECT4(a[255:128], imm8[7:6])
//		tmp_dst[287:256] := SELECT4(a[383:256], imm8[1:0])
//		tmp_dst[319:288] := SELECT4(a[383:256], imm8[3:2])
//		tmp_dst[351:320] := SELECT4(a[383:256], imm8[5:4])
//		tmp_dst[383:352] := SELECT4(a[383:256], imm8[7:6])
//		tmp_dst[415:384] := SELECT4(a[511:384], imm8[1:0])
//		tmp_dst[447:416] := SELECT4(a[511:384], imm8[3:2])
//		tmp_dst[479:448] := SELECT4(a[511:384], imm8[5:4])
//		tmp_dst[511:480] := SELECT4(a[511:384], imm8[7:6])
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSHUFD'. Intrinsic: '_mm512_maskz_shuffle_epi32'.
// Requires AVX512F.
func MaskzShuffleEpi32(k Mmask16, a M512i, imm8 MMPERMENUM) M512i {
	return M512i(maskzShuffleEpi32(uint16(k), [64]byte(a), imm8))
}

func maskzShuffleEpi32(k uint16, a [64]byte, imm8 MMPERMENUM) [64]byte


// ShuffleEpi32: Shuffle 32-bit integers in 'a' within 128-bit lanes using the
// control in 'imm8', and store the results in 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		dst[223:192] := SELECT4(a[255:128], imm8[5:4])
//		dst[255:224] := SELECT4(a[255:128], imm8[7:6])
//		dst[287:256] := SELECT4(a[383:256], imm8[1:0])
//		dst[319:288] := SELECT4(a[383:256], imm8[3:2])
//		dst[351:320] := SELECT4(a[383:256], imm8[5:4])
//		dst[383:352] := SELECT4(a[383:256], imm8[7:6])
//		dst[415:384] := SELECT4(a[511:384], imm8[1:0])
//		dst[447:416] := SELECT4(a[511:384], imm8[3:2])
//		dst[479:448] := SELECT4(a[511:384], imm8[5:4])
//		dst[511:480] := SELECT4(a[511:384], imm8[7:6])
//		dst[MAX:512] := 0
//
// Instruction: 'VPSHUFD'. Intrinsic: '_mm512_shuffle_epi32'.
// Requires KNCNI.
func ShuffleEpi32(a M512i, imm8 MMPERMENUM) M512i {
	return M512i(shuffleEpi32([64]byte(a), imm8))
}

func shuffleEpi32(a [64]byte, imm8 MMPERMENUM) [64]byte


// MaskShuffleEpi8: Shuffle 8-bit integers in 'a' within 128-bit lanes using
// the control in the corresponding 8-bit element of 'b', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				IF b[i+7] == 1
//					dst[i+7:i] := 0
//				ELSE
//					index[3:0] := b[i+3:i]
//					dst[i+7:i] := a[index*8+7:index*8]
//				FI
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSHUFB'. Intrinsic: '_mm512_mask_shuffle_epi8'.
// Requires AVX512BW.
func MaskShuffleEpi8(src M512i, k Mmask64, a M512i, b M512i) M512i {
	return M512i(maskShuffleEpi8([64]byte(src), uint64(k), [64]byte(a), [64]byte(b)))
}

func maskShuffleEpi8(src [64]byte, k uint64, a [64]byte, b [64]byte) [64]byte


// MaskzShuffleEpi8: Shuffle packed 8-bit integers in 'a' according to shuffle
// control mask in the corresponding 8-bit element of 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				IF b[i+7] == 1
//					dst[i+7:i] := 0
//				ELSE
//					index[3:0] := b[i+3:i]
//					dst[i+7:i] := a[index*8+7:index*8]
//				FI
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSHUFB'. Intrinsic: '_mm512_maskz_shuffle_epi8'.
// Requires AVX512BW.
func MaskzShuffleEpi8(k Mmask64, a M512i, b M512i) M512i {
	return M512i(maskzShuffleEpi8(uint64(k), [64]byte(a), [64]byte(b)))
}

func maskzShuffleEpi8(k uint64, a [64]byte, b [64]byte) [64]byte


// ShuffleEpi8: Shuffle packed 8-bit integers in 'a' according to shuffle
// control mask in the corresponding 8-bit element of 'b', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF b[i+7] == 1
//				dst[i+7:i] := 0
//			ELSE
//				index[3:0] := b[i+3:i]
//				dst[i+7:i] := a[index*8+7:index*8]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSHUFB'. Intrinsic: '_mm512_shuffle_epi8'.
// Requires AVX512BW.
func ShuffleEpi8(a M512i, b M512i) M512i {
	return M512i(shuffleEpi8([64]byte(a), [64]byte(b)))
}

func shuffleEpi8(a [64]byte, b [64]byte) [64]byte


// MaskShuffleF32x4: Shuffle 128-bits (composed of 4 single-precision (32-bit)
// floating-point elements) selected by 'imm8' from 'a' and 'b', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			2:	tmp[127:0] := src[383:256]
//			3:	tmp[127:0] := src[511:384]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT4(a[511:0], imm8[1:0])
//		tmp_dst[255:128] := SELECT4(a[511:0], imm8[3:2])
//		tmp_dst[383:256] := SELECT4(b[511:0], imm8[5:4])
//		tmp_dst[511:384] := SELECT4(b[511:0], imm8[7:6])
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFF32X4'. Intrinsic: '_mm512_mask_shuffle_f32x4'.
// Requires AVX512F.
func MaskShuffleF32x4(src M512, k Mmask16, a M512, b M512, imm8 int) M512 {
	return M512(maskShuffleF32x4([16]float32(src), uint16(k), [16]float32(a), [16]float32(b), imm8))
}

func maskShuffleF32x4(src [16]float32, k uint16, a [16]float32, b [16]float32, imm8 int) [16]float32


// MaskzShuffleF32x4: Shuffle 128-bits (composed of 4 single-precision (32-bit)
// floating-point elements) selected by 'imm8' from 'a' and 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			2:	tmp[127:0] := src[383:256]
//			3:	tmp[127:0] := src[511:384]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT4(a[511:0], imm8[1:0])
//		tmp_dst[255:128] := SELECT4(a[511:0], imm8[3:2])
//		tmp_dst[383:256] := SELECT4(b[511:0], imm8[5:4])
//		tmp_dst[511:384] := SELECT4(b[511:0], imm8[7:6])
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFF32X4'. Intrinsic: '_mm512_maskz_shuffle_f32x4'.
// Requires AVX512F.
func MaskzShuffleF32x4(k Mmask16, a M512, b M512, imm8 int) M512 {
	return M512(maskzShuffleF32x4(uint16(k), [16]float32(a), [16]float32(b), imm8))
}

func maskzShuffleF32x4(k uint16, a [16]float32, b [16]float32, imm8 int) [16]float32


// ShuffleF32x4: Shuffle 128-bits (composed of 4 single-precision (32-bit)
// floating-point elements) selected by 'imm8' from 'a' and 'b', and store the
// results in 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			2:	tmp[127:0] := src[383:256]
//			3:	tmp[127:0] := src[511:384]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		dst[127:0] := SELECT4(a[511:0], imm8[1:0])
//		dst[255:128] := SELECT4(a[511:0], imm8[3:2])
//		dst[383:256] := SELECT4(b[511:0], imm8[5:4])
//		dst[511:384] := SELECT4(b[511:0], imm8[7:6])
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFF32X4'. Intrinsic: '_mm512_shuffle_f32x4'.
// Requires AVX512F.
func ShuffleF32x4(a M512, b M512, imm8 int) M512 {
	return M512(shuffleF32x4([16]float32(a), [16]float32(b), imm8))
}

func shuffleF32x4(a [16]float32, b [16]float32, imm8 int) [16]float32


// MaskShuffleF64x2: Shuffle 128-bits (composed of 2 double-precision (64-bit)
// floating-point elements) selected by 'imm8' from 'a' and 'b', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			2:	tmp[127:0] := src[383:256]
//			3:	tmp[127:0] := src[511:384]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT4(a[511:0], imm8[1:0])
//		tmp_dst[255:128] := SELECT4(a[511:0], imm8[3:2])
//		tmp_dst[383:256] := SELECT4(b[511:0], imm8[5:4])
//		tmp_dst[511:384] := SELECT4(b[511:0], imm8[7:6])
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFF64X2'. Intrinsic: '_mm512_mask_shuffle_f64x2'.
// Requires AVX512F.
func MaskShuffleF64x2(src M512d, k Mmask8, a M512d, b M512d, imm8 int) M512d {
	return M512d(maskShuffleF64x2([8]float64(src), uint8(k), [8]float64(a), [8]float64(b), imm8))
}

func maskShuffleF64x2(src [8]float64, k uint8, a [8]float64, b [8]float64, imm8 int) [8]float64


// MaskzShuffleF64x2: Shuffle 128-bits (composed of 2 double-precision (64-bit)
// floating-point elements) selected by 'imm8' from 'a' and 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			2:	tmp[127:0] := src[383:256]
//			3:	tmp[127:0] := src[511:384]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT4(a[511:0], imm8[1:0])
//		tmp_dst[255:128] := SELECT4(a[511:0], imm8[3:2])
//		tmp_dst[383:256] := SELECT4(b[511:0], imm8[5:4])
//		tmp_dst[511:384] := SELECT4(b[511:0], imm8[7:6])
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFF64X2'. Intrinsic: '_mm512_maskz_shuffle_f64x2'.
// Requires AVX512F.
func MaskzShuffleF64x2(k Mmask8, a M512d, b M512d, imm8 int) M512d {
	return M512d(maskzShuffleF64x2(uint8(k), [8]float64(a), [8]float64(b), imm8))
}

func maskzShuffleF64x2(k uint8, a [8]float64, b [8]float64, imm8 int) [8]float64


// ShuffleF64x2: Shuffle 128-bits (composed of 2 double-precision (64-bit)
// floating-point elements) selected by 'imm8' from 'a' and 'b', and store the
// results in 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			2:	tmp[127:0] := src[383:256]
//			3:	tmp[127:0] := src[511:384]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		dst[127:0] := SELECT4(a[511:0], imm8[1:0])
//		dst[255:128] := SELECT4(a[511:0], imm8[3:2])
//		dst[383:256] := SELECT4(b[511:0], imm8[5:4])
//		dst[511:384] := SELECT4(b[511:0], imm8[7:6])
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFF64X2'. Intrinsic: '_mm512_shuffle_f64x2'.
// Requires AVX512F.
func ShuffleF64x2(a M512d, b M512d, imm8 int) M512d {
	return M512d(shuffleF64x2([8]float64(a), [8]float64(b), imm8))
}

func shuffleF64x2(a [8]float64, b [8]float64, imm8 int) [8]float64


// MaskShuffleI32x4: Shuffle 128-bits (composed of 4 32-bit integers) selected
// by 'imm8' from 'a' and 'b', and store the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			2:	tmp[127:0] := src[383:256]
//			3:	tmp[127:0] := src[511:384]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT4(a[511:0], imm8[1:0])
//		tmp_dst[255:128] := SELECT4(a[511:0], imm8[3:2])
//		tmp_dst[383:256] := SELECT4(b[511:0], imm8[5:4])
//		tmp_dst[511:384] := SELECT4(b[511:0], imm8[7:6])
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFI32X4'. Intrinsic: '_mm512_mask_shuffle_i32x4'.
// Requires AVX512F.
func MaskShuffleI32x4(src M512i, k Mmask16, a M512i, b M512i, imm8 int) M512i {
	return M512i(maskShuffleI32x4([64]byte(src), uint16(k), [64]byte(a), [64]byte(b), imm8))
}

func maskShuffleI32x4(src [64]byte, k uint16, a [64]byte, b [64]byte, imm8 int) [64]byte


// MaskzShuffleI32x4: Shuffle 128-bits (composed of 4 32-bit integers) selected
// by 'imm8' from 'a' and 'b', and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			2:	tmp[127:0] := src[383:256]
//			3:	tmp[127:0] := src[511:384]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT4(a[511:0], imm8[1:0])
//		tmp_dst[255:128] := SELECT4(a[511:0], imm8[3:2])
//		tmp_dst[383:256] := SELECT4(b[511:0], imm8[5:4])
//		tmp_dst[511:384] := SELECT4(b[511:0], imm8[7:6])
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFI32X4'. Intrinsic: '_mm512_maskz_shuffle_i32x4'.
// Requires AVX512F.
func MaskzShuffleI32x4(k Mmask16, a M512i, b M512i, imm8 int) M512i {
	return M512i(maskzShuffleI32x4(uint16(k), [64]byte(a), [64]byte(b), imm8))
}

func maskzShuffleI32x4(k uint16, a [64]byte, b [64]byte, imm8 int) [64]byte


// ShuffleI32x4: Shuffle 128-bits (composed of 4 32-bit integers) selected by
// 'imm8' from 'a' and 'b', and store the results in 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			2:	tmp[127:0] := src[383:256]
//			3:	tmp[127:0] := src[511:384]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		dst[127:0] := SELECT4(a[511:0], imm8[1:0])
//		dst[255:128] := SELECT4(a[511:0], imm8[3:2])
//		dst[383:256] := SELECT4(b[511:0], imm8[5:4])
//		dst[511:384] := SELECT4(b[511:0], imm8[7:6])
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFI32X4'. Intrinsic: '_mm512_shuffle_i32x4'.
// Requires AVX512F.
func ShuffleI32x4(a M512i, b M512i, imm8 int) M512i {
	return M512i(shuffleI32x4([64]byte(a), [64]byte(b), imm8))
}

func shuffleI32x4(a [64]byte, b [64]byte, imm8 int) [64]byte


// MaskShuffleI64x2: Shuffle 128-bits (composed of 2 64-bit integers) selected
// by 'imm8' from 'a' and 'b', and store the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			2:	tmp[127:0] := src[383:256]
//			3:	tmp[127:0] := src[511:384]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT4(a[511:0], imm8[1:0])
//		tmp_dst[255:128] := SELECT4(a[511:0], imm8[3:2])
//		tmp_dst[383:256] := SELECT4(b[511:0], imm8[5:4])
//		tmp_dst[511:384] := SELECT4(b[511:0], imm8[7:6])
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFI64X2'. Intrinsic: '_mm512_mask_shuffle_i64x2'.
// Requires AVX512F.
func MaskShuffleI64x2(src M512i, k Mmask8, a M512i, b M512i, imm8 int) M512i {
	return M512i(maskShuffleI64x2([64]byte(src), uint8(k), [64]byte(a), [64]byte(b), imm8))
}

func maskShuffleI64x2(src [64]byte, k uint8, a [64]byte, b [64]byte, imm8 int) [64]byte


// MaskzShuffleI64x2: Shuffle 128-bits (composed of 2 64-bit integers) selected
// by 'imm8' from 'a' and 'b', and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			2:	tmp[127:0] := src[383:256]
//			3:	tmp[127:0] := src[511:384]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT4(a[511:0], imm8[1:0])
//		tmp_dst[255:128] := SELECT4(a[511:0], imm8[3:2])
//		tmp_dst[383:256] := SELECT4(b[511:0], imm8[5:4])
//		tmp_dst[511:384] := SELECT4(b[511:0], imm8[7:6])
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFI64X2'. Intrinsic: '_mm512_maskz_shuffle_i64x2'.
// Requires AVX512F.
func MaskzShuffleI64x2(k Mmask8, a M512i, b M512i, imm8 int) M512i {
	return M512i(maskzShuffleI64x2(uint8(k), [64]byte(a), [64]byte(b), imm8))
}

func maskzShuffleI64x2(k uint8, a [64]byte, b [64]byte, imm8 int) [64]byte


// ShuffleI64x2: Shuffle 128-bits (composed of 2 64-bit integers) selected by
// 'imm8' from 'a' and 'b', and store the results in 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			2:	tmp[127:0] := src[383:256]
//			3:	tmp[127:0] := src[511:384]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		dst[127:0] := SELECT4(a[511:0], imm8[1:0])
//		dst[255:128] := SELECT4(a[511:0], imm8[3:2])
//		dst[383:256] := SELECT4(b[511:0], imm8[5:4])
//		dst[511:384] := SELECT4(b[511:0], imm8[7:6])
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFI64X2'. Intrinsic: '_mm512_shuffle_i64x2'.
// Requires AVX512F.
func ShuffleI64x2(a M512i, b M512i, imm8 int) M512i {
	return M512i(shuffleI64x2([64]byte(a), [64]byte(b), imm8))
}

func shuffleI64x2(a [64]byte, b [64]byte, imm8 int) [64]byte


// MaskShufflePd: Shuffle double-precision (64-bit) floating-point elements
// within 128-bit lanes using the control in 'imm8', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		tmp_dst[63:0] := (imm8[0] == 0) ? a[63:0] : a[127:64]
//		tmp_dst[127:64] := (imm8[1] == 0) ? b[63:0] : b[127:64]
//		tmp_dst[191:128] := (imm8[2] == 0) ? a[191:128] : a[255:192]
//		tmp_dst[255:192] := (imm8[3] == 0) ? b[191:128] : b[255:192]
//		tmp_dst[319:256] := (imm8[4] == 0) ? a[319:256] : a[383:320]
//		tmp_dst[383:320] := (imm8[5] == 0) ? b[319:256] : b[383:320]
//		tmp_dst[447:384] := (imm8[6] == 0) ? a[447:384] : a[511:448]
//		tmp_dst[511:448] := (imm8[7] == 0) ? b[447:384] : b[511:448]
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFPD'. Intrinsic: '_mm512_mask_shuffle_pd'.
// Requires AVX512F.
func MaskShufflePd(src M512d, k Mmask8, a M512d, b M512d, imm8 int) M512d {
	return M512d(maskShufflePd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b), imm8))
}

func maskShufflePd(src [8]float64, k uint8, a [8]float64, b [8]float64, imm8 int) [8]float64


// MaskzShufflePd: Shuffle double-precision (64-bit) floating-point elements
// within 128-bit lanes using the control in 'imm8', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		tmp_dst[63:0] := (imm8[0] == 0) ? a[63:0] : a[127:64]
//		tmp_dst[127:64] := (imm8[1] == 0) ? b[63:0] : b[127:64]
//		tmp_dst[191:128] := (imm8[2] == 0) ? a[191:128] : a[255:192]
//		tmp_dst[255:192] := (imm8[3] == 0) ? b[191:128] : b[255:192]
//		tmp_dst[319:256] := (imm8[4] == 0) ? a[319:256] : a[383:320]
//		tmp_dst[383:320] := (imm8[5] == 0) ? b[319:256] : b[383:320]
//		tmp_dst[447:384] := (imm8[6] == 0) ? a[447:384] : a[511:448]
//		tmp_dst[511:448] := (imm8[7] == 0) ? b[447:384] : b[511:448]
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFPD'. Intrinsic: '_mm512_maskz_shuffle_pd'.
// Requires AVX512F.
func MaskzShufflePd(k Mmask8, a M512d, b M512d, imm8 int) M512d {
	return M512d(maskzShufflePd(uint8(k), [8]float64(a), [8]float64(b), imm8))
}

func maskzShufflePd(k uint8, a [8]float64, b [8]float64, imm8 int) [8]float64


// ShufflePd: Shuffle double-precision (64-bit) floating-point elements within
// 128-bit lanes using the control in 'imm8', and store the results in 'dst'. 
//
//		dst[63:0] := (imm8[0] == 0) ? a[63:0] : a[127:64]
//		dst[127:64] := (imm8[1] == 0) ? b[63:0] : b[127:64]
//		dst[191:128] := (imm8[2] == 0) ? a[191:128] : a[255:192]
//		dst[255:192] := (imm8[3] == 0) ? b[191:128] : b[255:192]
//		dst[319:256] := (imm8[4] == 0) ? a[319:256] : a[383:320]
//		dst[383:320] := (imm8[5] == 0) ? b[319:256] : b[383:320]
//		dst[447:384] := (imm8[6] == 0) ? a[447:384] : a[511:448]
//		dst[511:448] := (imm8[7] == 0) ? b[447:384] : b[511:448]
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFPD'. Intrinsic: '_mm512_shuffle_pd'.
// Requires AVX512F.
func ShufflePd(a M512d, b M512d, imm8 int) M512d {
	return M512d(shufflePd([8]float64(a), [8]float64(b), imm8))
}

func shufflePd(a [8]float64, b [8]float64, imm8 int) [8]float64


// MaskShufflePs: Shuffle single-precision (32-bit) floating-point elements in
// 'a' within 128-bit lanes using the control in 'imm8', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(b[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(b[127:0], imm8[7:6])
//		tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		tmp_dst[223:192] := SELECT4(b[255:128], imm8[5:4])
//		tmp_dst[255:224] := SELECT4(b[255:128], imm8[7:6])
//		tmp_dst[287:256] := SELECT4(a[383:256], imm8[1:0])
//		tmp_dst[319:288] := SELECT4(a[383:256], imm8[3:2])
//		tmp_dst[351:320] := SELECT4(b[383:256], imm8[5:4])
//		tmp_dst[383:352] := SELECT4(b[383:256], imm8[7:6])
//		tmp_dst[415:384] := SELECT4(a[511:384], imm8[1:0])
//		tmp_dst[447:416] := SELECT4(a[511:384], imm8[3:2])
//		tmp_dst[479:448] := SELECT4(b[511:384], imm8[5:4])
//		tmp_dst[511:480] := SELECT4(b[511:384], imm8[7:6])
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFPS'. Intrinsic: '_mm512_mask_shuffle_ps'.
// Requires AVX512F.
func MaskShufflePs(src M512, k Mmask16, a M512, b M512, imm8 int) M512 {
	return M512(maskShufflePs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b), imm8))
}

func maskShufflePs(src [16]float32, k uint16, a [16]float32, b [16]float32, imm8 int) [16]float32


// MaskzShufflePs: Shuffle single-precision (32-bit) floating-point elements in
// 'a' within 128-bit lanes using the control in 'imm8', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(b[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(b[127:0], imm8[7:6])
//		tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		tmp_dst[223:192] := SELECT4(b[255:128], imm8[5:4])
//		tmp_dst[255:224] := SELECT4(b[255:128], imm8[7:6])
//		tmp_dst[287:256] := SELECT4(a[383:256], imm8[1:0])
//		tmp_dst[319:288] := SELECT4(a[383:256], imm8[3:2])
//		tmp_dst[351:320] := SELECT4(b[383:256], imm8[5:4])
//		tmp_dst[383:352] := SELECT4(b[383:256], imm8[7:6])
//		tmp_dst[415:384] := SELECT4(a[511:384], imm8[1:0])
//		tmp_dst[447:416] := SELECT4(a[511:384], imm8[3:2])
//		tmp_dst[479:448] := SELECT4(b[511:384], imm8[5:4])
//		tmp_dst[511:480] := SELECT4(b[511:384], imm8[7:6])
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFPS'. Intrinsic: '_mm512_maskz_shuffle_ps'.
// Requires AVX512F.
func MaskzShufflePs(k Mmask16, a M512, b M512, imm8 int) M512 {
	return M512(maskzShufflePs(uint16(k), [16]float32(a), [16]float32(b), imm8))
}

func maskzShufflePs(k uint16, a [16]float32, b [16]float32, imm8 int) [16]float32


// ShufflePs: Shuffle single-precision (32-bit) floating-point elements in 'a'
// within 128-bit lanes using the control in 'imm8', and store the results in
// 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		dst[95:64] := SELECT4(b[127:0], imm8[5:4])
//		dst[127:96] := SELECT4(b[127:0], imm8[7:6])
//		dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		dst[223:192] := SELECT4(b[255:128], imm8[5:4])
//		dst[255:224] := SELECT4(b[255:128], imm8[7:6])
//		dst[287:256] := SELECT4(a[383:256], imm8[1:0])
//		dst[319:288] := SELECT4(a[383:256], imm8[3:2])
//		dst[351:320] := SELECT4(b[383:256], imm8[5:4])
//		dst[383:352] := SELECT4(b[383:256], imm8[7:6])
//		dst[415:384] := SELECT4(a[511:384], imm8[1:0])
//		dst[447:416] := SELECT4(a[511:384], imm8[3:2])
//		dst[479:448] := SELECT4(b[511:384], imm8[5:4])
//		dst[511:480] := SELECT4(b[511:384], imm8[7:6])
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFPS'. Intrinsic: '_mm512_shuffle_ps'.
// Requires AVX512F.
func ShufflePs(a M512, b M512, imm8 int) M512 {
	return M512(shufflePs([16]float32(a), [16]float32(b), imm8))
}

func shufflePs(a [16]float32, b [16]float32, imm8 int) [16]float32


// MaskShufflehiEpi16: Shuffle 16-bit integers in the high 64 bits of 128-bit
// lanes of 'a' using the control in 'imm8'. Store the results in the high 64
// bits of 128-bit lanes of 'dst', with the low 64 bits of 128-bit lanes being
// copied from from 'a' to 'dst', using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		tmp_dst[63:0] := a[63:0]
//		tmp_dst[79:64] := (a >> (imm8[1:0] * 16))[79:64]
//		tmp_dst[95:80] := (a >> (imm8[3:2] * 16))[79:64]
//		tmp_dst[111:96] := (a >> (imm8[5:4] * 16))[79:64]
//		tmp_dst[127:112] := (a >> (imm8[7:6] * 16))[79:64]
//		tmp_dst[191:128] := a[191:128]
//		tmp_dst[207:192] := (a >> (imm8[1:0] * 16))[207:192]
//		tmp_dst[223:208] := (a >> (imm8[3:2] * 16))[207:192]
//		tmp_dst[239:224] := (a >> (imm8[5:4] * 16))[207:192]
//		tmp_dst[255:240] := (a >> (imm8[7:6] * 16))[207:192]
//		tmp_dst[319:256] := a[319:256]
//		tmp_dst[335:320] := (a >> (imm8[1:0] * 16))[335:320]
//		tmp_dst[351:336] := (a >> (imm8[3:2] * 16))[335:320]
//		tmp_dst[367:352] := (a >> (imm8[5:4] * 16))[335:320]
//		tmp_dst[383:368] := (a >> (imm8[7:6] * 16))[335:320]
//		tmp_dst[447:384] := a[447:384]
//		tmp_dst[463:448] := (a >> (imm8[1:0] * 16))[463:448]
//		tmp_dst[479:464] := (a >> (imm8[3:2] * 16))[463:448]
//		tmp_dst[495:480] := (a >> (imm8[5:4] * 16))[463:448]
//		tmp_dst[511:496] := (a >> (imm8[7:6] * 16))[463:448]
//		
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSHUFHW'. Intrinsic: '_mm512_mask_shufflehi_epi16'.
// Requires AVX512BW.
func MaskShufflehiEpi16(src M512i, k Mmask32, a M512i, imm8 int) M512i {
	return M512i(maskShufflehiEpi16([64]byte(src), uint32(k), [64]byte(a), imm8))
}

func maskShufflehiEpi16(src [64]byte, k uint32, a [64]byte, imm8 int) [64]byte


// MaskzShufflehiEpi16: Shuffle 16-bit integers in the high 64 bits of 128-bit
// lanes of 'a' using the control in 'imm8'. Store the results in the high 64
// bits of 128-bit lanes of 'dst', with the low 64 bits of 128-bit lanes being
// copied from from 'a' to 'dst', using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		tmp_dst[63:0] := a[63:0]
//		tmp_dst[79:64] := (a >> (imm8[1:0] * 16))[79:64]
//		tmp_dst[95:80] := (a >> (imm8[3:2] * 16))[79:64]
//		tmp_dst[111:96] := (a >> (imm8[5:4] * 16))[79:64]
//		tmp_dst[127:112] := (a >> (imm8[7:6] * 16))[79:64]
//		tmp_dst[191:128] := a[191:128]
//		tmp_dst[207:192] := (a >> (imm8[1:0] * 16))[207:192]
//		tmp_dst[223:208] := (a >> (imm8[3:2] * 16))[207:192]
//		tmp_dst[239:224] := (a >> (imm8[5:4] * 16))[207:192]
//		tmp_dst[255:240] := (a >> (imm8[7:6] * 16))[207:192]
//		tmp_dst[319:256] := a[319:256]
//		tmp_dst[335:320] := (a >> (imm8[1:0] * 16))[335:320]
//		tmp_dst[351:336] := (a >> (imm8[3:2] * 16))[335:320]
//		tmp_dst[367:352] := (a >> (imm8[5:4] * 16))[335:320]
//		tmp_dst[383:368] := (a >> (imm8[7:6] * 16))[335:320]
//		tmp_dst[447:384] := a[447:384]
//		tmp_dst[463:448] := (a >> (imm8[1:0] * 16))[463:448]
//		tmp_dst[479:464] := (a >> (imm8[3:2] * 16))[463:448]
//		tmp_dst[495:480] := (a >> (imm8[5:4] * 16))[463:448]
//		tmp_dst[511:496] := (a >> (imm8[7:6] * 16))[463:448]
//		
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSHUFHW'. Intrinsic: '_mm512_maskz_shufflehi_epi16'.
// Requires AVX512BW.
func MaskzShufflehiEpi16(k Mmask32, a M512i, imm8 int) M512i {
	return M512i(maskzShufflehiEpi16(uint32(k), [64]byte(a), imm8))
}

func maskzShufflehiEpi16(k uint32, a [64]byte, imm8 int) [64]byte


// ShufflehiEpi16: Shuffle 16-bit integers in the high 64 bits of 128-bit lanes
// of 'a' using the control in 'imm8'. Store the results in the high 64 bits of
// 128-bit lanes of 'dst', with the low 64 bits of 128-bit lanes being copied
// from from 'a' to 'dst'. 
//
//		dst[63:0] := a[63:0]
//		dst[79:64] := (a >> (imm8[1:0] * 16))[79:64]
//		dst[95:80] := (a >> (imm8[3:2] * 16))[79:64]
//		dst[111:96] := (a >> (imm8[5:4] * 16))[79:64]
//		dst[127:112] := (a >> (imm8[7:6] * 16))[79:64]
//		dst[191:128] := a[191:128]
//		dst[207:192] := (a >> (imm8[1:0] * 16))[207:192]
//		dst[223:208] := (a >> (imm8[3:2] * 16))[207:192]
//		dst[239:224] := (a >> (imm8[5:4] * 16))[207:192]
//		dst[255:240] := (a >> (imm8[7:6] * 16))[207:192]
//		dst[319:256] := a[319:256]
//		dst[335:320] := (a >> (imm8[1:0] * 16))[335:320]
//		dst[351:336] := (a >> (imm8[3:2] * 16))[335:320]
//		dst[367:352] := (a >> (imm8[5:4] * 16))[335:320]
//		dst[383:368] := (a >> (imm8[7:6] * 16))[335:320]
//		dst[447:384] := a[447:384]
//		dst[463:448] := (a >> (imm8[1:0] * 16))[463:448]
//		dst[479:464] := (a >> (imm8[3:2] * 16))[463:448]
//		dst[495:480] := (a >> (imm8[5:4] * 16))[463:448]
//		dst[511:496] := (a >> (imm8[7:6] * 16))[463:448]
//		dst[MAX:512] := 0
//
// Instruction: 'VPSHUFHW'. Intrinsic: '_mm512_shufflehi_epi16'.
// Requires AVX512BW.
func ShufflehiEpi16(a M512i, imm8 int) M512i {
	return M512i(shufflehiEpi16([64]byte(a), imm8))
}

func shufflehiEpi16(a [64]byte, imm8 int) [64]byte


// MaskShuffleloEpi16: Shuffle 16-bit integers in the low 64 bits of 128-bit
// lanes of 'a' using the control in 'imm8'. Store the results in the low 64
// bits of 128-bit lanes of 'dst', with the high 64 bits of 128-bit lanes being
// copied from from 'a' to 'dst', using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		tmp_dst[15:0] := (a >> (imm8[1:0] * 16))[15:0]
//		tmp_dst[31:16] := (a >> (imm8[3:2] * 16))[15:0]
//		tmp_dst[47:32] := (a >> (imm8[5:4] * 16))[15:0]
//		tmp_dst[63:48] := (a >> (imm8[7:6] * 16))[15:0]
//		tmp_dst[127:64] := a[127:64]
//		tmp_dst[143:128] := (a >> (imm8[1:0] * 16))[143:128]
//		tmp_dst[159:144] := (a >> (imm8[3:2] * 16))[143:128]
//		tmp_dst[175:160] := (a >> (imm8[5:4] * 16))[143:128]
//		tmp_dst[191:176] := (a >> (imm8[7:6] * 16))[143:128]
//		tmp_dst[255:192] := a[255:192]
//		tmp_dst[271:256] := (a >> (imm8[1:0] * 16))[271:256]
//		tmp_dst[287:272] := (a >> (imm8[3:2] * 16))[271:256]
//		tmp_dst[303:288] := (a >> (imm8[5:4] * 16))[271:256]
//		tmp_dst[319:304] := (a >> (imm8[7:6] * 16))[271:256]
//		tmp_dst[383:320] := a[383:320]
//		tmp_dst[399:384] := (a >> (imm8[1:0] * 16))[399:384]
//		tmp_dst[415:400] := (a >> (imm8[3:2] * 16))[399:384]
//		tmp_dst[431:416] := (a >> (imm8[5:4] * 16))[399:384]
//		tmp_dst[447:432] := (a >> (imm8[7:6] * 16))[399:384]
//		tmp_dst[511:448] := a[511:448]
//		
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSHUFLW'. Intrinsic: '_mm512_mask_shufflelo_epi16'.
// Requires AVX512BW.
func MaskShuffleloEpi16(src M512i, k Mmask32, a M512i, imm8 int) M512i {
	return M512i(maskShuffleloEpi16([64]byte(src), uint32(k), [64]byte(a), imm8))
}

func maskShuffleloEpi16(src [64]byte, k uint32, a [64]byte, imm8 int) [64]byte


// MaskzShuffleloEpi16: Shuffle 16-bit integers in the low 64 bits of 128-bit
// lanes of 'a' using the control in 'imm8'. Store the results in the low 64
// bits of 128-bit lanes of 'dst', with the high 64 bits of 128-bit lanes being
// copied from from 'a' to 'dst', using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		tmp_dst[15:0] := (a >> (imm8[1:0] * 16))[15:0]
//		tmp_dst[31:16] := (a >> (imm8[3:2] * 16))[15:0]
//		tmp_dst[47:32] := (a >> (imm8[5:4] * 16))[15:0]
//		tmp_dst[63:48] := (a >> (imm8[7:6] * 16))[15:0]
//		tmp_dst[127:64] := a[127:64]
//		tmp_dst[143:128] := (a >> (imm8[1:0] * 16))[143:128]
//		tmp_dst[159:144] := (a >> (imm8[3:2] * 16))[143:128]
//		tmp_dst[175:160] := (a >> (imm8[5:4] * 16))[143:128]
//		tmp_dst[191:176] := (a >> (imm8[7:6] * 16))[143:128]
//		tmp_dst[255:192] := a[255:192]
//		tmp_dst[271:256] := (a >> (imm8[1:0] * 16))[271:256]
//		tmp_dst[287:272] := (a >> (imm8[3:2] * 16))[271:256]
//		tmp_dst[303:288] := (a >> (imm8[5:4] * 16))[271:256]
//		tmp_dst[319:304] := (a >> (imm8[7:6] * 16))[271:256]
//		tmp_dst[383:320] := a[383:320]
//		tmp_dst[399:384] := (a >> (imm8[1:0] * 16))[399:384]
//		tmp_dst[415:400] := (a >> (imm8[3:2] * 16))[399:384]
//		tmp_dst[431:416] := (a >> (imm8[5:4] * 16))[399:384]
//		tmp_dst[447:432] := (a >> (imm8[7:6] * 16))[399:384]
//		tmp_dst[511:448] := a[511:448]
//		
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSHUFLW'. Intrinsic: '_mm512_maskz_shufflelo_epi16'.
// Requires AVX512BW.
func MaskzShuffleloEpi16(k Mmask32, a M512i, imm8 int) M512i {
	return M512i(maskzShuffleloEpi16(uint32(k), [64]byte(a), imm8))
}

func maskzShuffleloEpi16(k uint32, a [64]byte, imm8 int) [64]byte


// ShuffleloEpi16: Shuffle 16-bit integers in the low 64 bits of 128-bit lanes
// of 'a' using the control in 'imm8'. Store the results in the low 64 bits of
// 128-bit lanes of 'dst', with the high 64 bits of 128-bit lanes being copied
// from from 'a' to 'dst'. 
//
//		dst[15:0] := (a >> (imm8[1:0] * 16))[15:0]
//		dst[31:16] := (a >> (imm8[3:2] * 16))[15:0]
//		dst[47:32] := (a >> (imm8[5:4] * 16))[15:0]
//		dst[63:48] := (a >> (imm8[7:6] * 16))[15:0]
//		dst[127:64] := a[127:64]
//		dst[143:128] := (a >> (imm8[1:0] * 16))[143:128]
//		dst[159:144] := (a >> (imm8[3:2] * 16))[143:128]
//		dst[175:160] := (a >> (imm8[5:4] * 16))[143:128]
//		dst[191:176] := (a >> (imm8[7:6] * 16))[143:128]
//		dst[255:192] := a[255:192]
//		dst[271:256] := (a >> (imm8[1:0] * 16))[271:256]
//		dst[287:272] := (a >> (imm8[3:2] * 16))[271:256]
//		dst[303:288] := (a >> (imm8[5:4] * 16))[271:256]
//		dst[319:304] := (a >> (imm8[7:6] * 16))[271:256]
//		dst[383:320] := a[383:320]
//		dst[399:384] := (a >> (imm8[1:0] * 16))[399:384]
//		dst[415:400] := (a >> (imm8[3:2] * 16))[399:384]
//		dst[431:416] := (a >> (imm8[5:4] * 16))[399:384]
//		dst[447:432] := (a >> (imm8[7:6] * 16))[399:384]
//		dst[511:448] := a[511:448]
//		dst[MAX:512] := 0
//
// Instruction: 'VPSHUFLW'. Intrinsic: '_mm512_shufflelo_epi16'.
// Requires AVX512BW.
func ShuffleloEpi16(a M512i, imm8 int) M512i {
	return M512i(shuffleloEpi16([64]byte(a), imm8))
}

func shuffleloEpi16(a [64]byte, imm8 int) [64]byte


// MaskSinPd: Compute the sine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SIN(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_sin_pd'.
// Requires AVX512F.
func MaskSinPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskSinPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskSinPd(src [8]float64, k uint8, a [8]float64) [8]float64


// SinPd: Compute the sine of packed double-precision (64-bit) floating-point
// elements in 'a' expressed in radians, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := SIN(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_sin_pd'.
// Requires AVX512F.
func SinPd(a M512d) M512d {
	return M512d(sinPd([8]float64(a)))
}

func sinPd(a [8]float64) [8]float64


// MaskSinPs: Compute the sine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SIN(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_sin_ps'.
// Requires AVX512F.
func MaskSinPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskSinPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskSinPs(src [16]float32, k uint16, a [16]float32) [16]float32


// SinPs: Compute the sine of packed single-precision (32-bit) floating-point
// elements in 'a' expressed in radians, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := SIN(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_sin_ps'.
// Requires AVX512F.
func SinPs(a M512) M512 {
	return M512(sinPs([16]float32(a)))
}

func sinPs(a [16]float32) [16]float32


// MaskSincosPd: Computes the sine and cosine of the packed double-precision
// (64-bit) floating-point elements in 'a' and stores the results of the sine
// computation in 'dst' and the results of the cosine computation in 'cos_res'.
// Elements are written to their respective locations using writemask 'k'
// (elements are copied from 'sin_src' or 'cos_src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SIN(a[i+63:i])
//				cos_res[i+63:i] := COS(a[i+63:i])
//			ELSE
//				dst[i+63:i] := sin_src[i+63:i]
//				cos_res[i+63:i] := cos_src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//		cos_res[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_sincos_pd'.
// Requires AVX512F.
func MaskSincosPd(cos_res M512d, sin_src M512d, cos_src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskSincosPd([8]float64(cos_res), [8]float64(sin_src), [8]float64(cos_src), uint8(k), [8]float64(a)))
}

func maskSincosPd(cos_res [8]float64, sin_src [8]float64, cos_src [8]float64, k uint8, a [8]float64) [8]float64


// SincosPd: Computes the sine and cosine of the packed double-precision
// (64-bit) floating-point elements in 'a' and stores the results of the sine
// computation in 'dst' and the results of the cosine computation in 'cos_res'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := SIN(a[i+63:i])
//			cos_res[i+63:i] := COS(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//		cos_res[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_sincos_pd'.
// Requires AVX512F.
func SincosPd(cos_res M512d, a M512d) M512d {
	return M512d(sincosPd([8]float64(cos_res), [8]float64(a)))
}

func sincosPd(cos_res [8]float64, a [8]float64) [8]float64


// MaskSincosPs: Computes the sine and cosine of the packed single-precision
// (32-bit) floating-point elements in 'a' and stores the results of the sine
// computation in 'dst' and the results of the cosine computation in 'cos_res'.
// Elements are written to their respective locations using writemask 'k'
// (elements are copied from 'sin_src' or 'cos_src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SIN(a[i+31:i])
//				cos_res[i+31:i] := COS(a[i+31:i])
//			ELSE
//				dst[i+31:i] := sin_src[i+31:i]
//				cos_res[i+31:i] := cos_src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//		cos_res[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_sincos_ps'.
// Requires AVX512F.
func MaskSincosPs(cos_res M512, sin_src M512, cos_src M512, k Mmask16, a M512) M512 {
	return M512(maskSincosPs([16]float32(cos_res), [16]float32(sin_src), [16]float32(cos_src), uint16(k), [16]float32(a)))
}

func maskSincosPs(cos_res [16]float32, sin_src [16]float32, cos_src [16]float32, k uint16, a [16]float32) [16]float32


// SincosPs: Computes the sine and cosine of the packed single-precision
// (32-bit) floating-point elements in 'a' and stores the results of the sine
// computation in 'dst' and the results of the cosine computation in 'cos_res'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := SIN(a[i+31:i])
//			cos_res[i+31:i] := COS(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//		cos_res[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_sincos_ps'.
// Requires AVX512F.
func SincosPs(cos_res M512, a M512) M512 {
	return M512(sincosPs([16]float32(cos_res), [16]float32(a)))
}

func sincosPs(cos_res [16]float32, a [16]float32) [16]float32


// MaskSindPd: Compute the sine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in degrees, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SIND(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_sind_pd'.
// Requires AVX512F.
func MaskSindPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskSindPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskSindPd(src [8]float64, k uint8, a [8]float64) [8]float64


// SindPd: Compute the sine of packed double-precision (64-bit) floating-point
// elements in 'a' expressed in degrees, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := SIND(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_sind_pd'.
// Requires AVX512F.
func SindPd(a M512d) M512d {
	return M512d(sindPd([8]float64(a)))
}

func sindPd(a [8]float64) [8]float64


// MaskSindPs: Compute the sine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in degrees, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SIND(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_sind_ps'.
// Requires AVX512F.
func MaskSindPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskSindPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskSindPs(src [16]float32, k uint16, a [16]float32) [16]float32


// SindPs: Compute the sine of packed single-precision (32-bit) floating-point
// elements in 'a' expressed in degrees, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := SIND(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_sind_ps'.
// Requires AVX512F.
func SindPs(a M512) M512 {
	return M512(sindPs([16]float32(a)))
}

func sindPs(a [16]float32) [16]float32


// MaskSinhPd: Compute the hyperbolic sine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SINH(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_sinh_pd'.
// Requires AVX512F.
func MaskSinhPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskSinhPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskSinhPd(src [8]float64, k uint8, a [8]float64) [8]float64


// SinhPd: Compute the hyperbolic sine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := SINH(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_sinh_pd'.
// Requires AVX512F.
func SinhPd(a M512d) M512d {
	return M512d(sinhPd([8]float64(a)))
}

func sinhPd(a [8]float64) [8]float64


// MaskSinhPs: Compute the hyperbolic sine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SINH(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_sinh_ps'.
// Requires AVX512F.
func MaskSinhPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskSinhPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskSinhPs(src [16]float32, k uint16, a [16]float32) [16]float32


// SinhPs: Compute the hyperbolic sine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := SINH(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_sinh_ps'.
// Requires AVX512F.
func SinhPs(a M512) M512 {
	return M512(sinhPs([16]float32(a)))
}

func sinhPs(a [16]float32) [16]float32


// MaskSllEpi16: Shift packed 16-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				IF count[63:0] > 15
//					dst[i+15:i] := 0
//				ELSE
//					dst[i+15:i] := ZeroExtend(a[i+15:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLW'. Intrinsic: '_mm512_mask_sll_epi16'.
// Requires AVX512BW.
func MaskSllEpi16(src M512i, k Mmask32, a M512i, count M128i) M512i {
	return M512i(maskSllEpi16([64]byte(src), uint32(k), [64]byte(a), [16]byte(count)))
}

func maskSllEpi16(src [64]byte, k uint32, a [64]byte, count [16]byte) [64]byte


// MaskzSllEpi16: Shift packed 16-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				IF count[63:0] > 15
//					dst[i+15:i] := 0
//				ELSE
//					dst[i+15:i] := ZeroExtend(a[i+15:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLW'. Intrinsic: '_mm512_maskz_sll_epi16'.
// Requires AVX512BW.
func MaskzSllEpi16(k Mmask32, a M512i, count M128i) M512i {
	return M512i(maskzSllEpi16(uint32(k), [64]byte(a), [16]byte(count)))
}

func maskzSllEpi16(k uint32, a [64]byte, count [16]byte) [64]byte


// SllEpi16: Shift packed 16-bit integers in 'a' left by 'count' while shifting
// in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF count[63:0] > 15
//				dst[i+15:i] := 0
//			ELSE
//				dst[i+15:i] := ZeroExtend(a[i+15:i] << count[63:0])
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLW'. Intrinsic: '_mm512_sll_epi16'.
// Requires AVX512BW.
func SllEpi16(a M512i, count M128i) M512i {
	return M512i(sllEpi16([64]byte(a), [16]byte(count)))
}

func sllEpi16(a [64]byte, count [16]byte) [64]byte


// MaskSllEpi32: Shift packed 32-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLD'. Intrinsic: '_mm512_mask_sll_epi32'.
// Requires AVX512F.
func MaskSllEpi32(src M512i, k Mmask16, a M512i, count M128i) M512i {
	return M512i(maskSllEpi32([64]byte(src), uint16(k), [64]byte(a), [16]byte(count)))
}

func maskSllEpi32(src [64]byte, k uint16, a [64]byte, count [16]byte) [64]byte


// MaskzSllEpi32: Shift packed 32-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLD'. Intrinsic: '_mm512_maskz_sll_epi32'.
// Requires AVX512F.
func MaskzSllEpi32(k Mmask16, a M512i, count M128i) M512i {
	return M512i(maskzSllEpi32(uint16(k), [64]byte(a), [16]byte(count)))
}

func maskzSllEpi32(k uint16, a [64]byte, count [16]byte) [64]byte


// SllEpi32: Shift packed 32-bit integers in 'a' left by 'count' while shifting
// in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF count[63:0] > 31
//				dst[i+31:i] := 0
//			ELSE
//				dst[i+31:i] := ZeroExtend(a[i+31:i] << count[63:0])
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLD'. Intrinsic: '_mm512_sll_epi32'.
// Requires AVX512F.
func SllEpi32(a M512i, count M128i) M512i {
	return M512i(sllEpi32([64]byte(a), [16]byte(count)))
}

func sllEpi32(a [64]byte, count [16]byte) [64]byte


// MaskSllEpi64: Shift packed 64-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm512_mask_sll_epi64'.
// Requires AVX512F.
func MaskSllEpi64(src M512i, k Mmask8, a M512i, count M128i) M512i {
	return M512i(maskSllEpi64([64]byte(src), uint8(k), [64]byte(a), [16]byte(count)))
}

func maskSllEpi64(src [64]byte, k uint8, a [64]byte, count [16]byte) [64]byte


// MaskzSllEpi64: Shift packed 64-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm512_maskz_sll_epi64'.
// Requires AVX512F.
func MaskzSllEpi64(k Mmask8, a M512i, count M128i) M512i {
	return M512i(maskzSllEpi64(uint8(k), [64]byte(a), [16]byte(count)))
}

func maskzSllEpi64(k uint8, a [64]byte, count [16]byte) [64]byte


// SllEpi64: Shift packed 64-bit integers in 'a' left by 'count' while shifting
// in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF count[63:0] > 63
//				dst[i+63:i] := 0
//			ELSE
//				dst[i+63:i] := ZeroExtend(a[i+63:i] << count[63:0])
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm512_sll_epi64'.
// Requires AVX512F.
func SllEpi64(a M512i, count M128i) M512i {
	return M512i(sllEpi64([64]byte(a), [16]byte(count)))
}

func sllEpi64(a [64]byte, count [16]byte) [64]byte


// MaskSlliEpi16: Shift packed 16-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				IF imm8[7:0] > 15
//					dst[i+15:i] := 0
//				ELSE
//					dst[i+15:i] := ZeroExtend(a[i+15:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLW'. Intrinsic: '_mm512_mask_slli_epi16'.
// Requires AVX512BW.
func MaskSlliEpi16(src M512i, k Mmask32, a M512i, imm8 uint32) M512i {
	return M512i(maskSlliEpi16([64]byte(src), uint32(k), [64]byte(a), imm8))
}

func maskSlliEpi16(src [64]byte, k uint32, a [64]byte, imm8 uint32) [64]byte


// MaskzSlliEpi16: Shift packed 16-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				IF imm8[7:0] > 15
//					dst[i+15:i] := 0
//				ELSE
//					dst[i+15:i] := ZeroExtend(a[i+15:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLW'. Intrinsic: '_mm512_maskz_slli_epi16'.
// Requires AVX512BW.
func MaskzSlliEpi16(k Mmask32, a M512i, imm8 uint32) M512i {
	return M512i(maskzSlliEpi16(uint32(k), [64]byte(a), imm8))
}

func maskzSlliEpi16(k uint32, a [64]byte, imm8 uint32) [64]byte


// SlliEpi16: Shift packed 16-bit integers in 'a' left by 'imm8' while shifting
// in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF imm8[7:0] > 15
//				dst[i+15:i] := 0
//			ELSE
//				dst[i+15:i] := ZeroExtend(a[i+15:i] << imm8[7:0])
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLW'. Intrinsic: '_mm512_slli_epi16'.
// Requires AVX512BW.
func SlliEpi16(a M512i, imm8 uint32) M512i {
	return M512i(slliEpi16([64]byte(a), imm8))
}

func slliEpi16(a [64]byte, imm8 uint32) [64]byte


// MaskSlliEpi32: Shift packed 32-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLD'. Intrinsic: '_mm512_mask_slli_epi32'.
// Requires KNCNI.
func MaskSlliEpi32(src M512i, k Mmask16, a M512i, imm8 uint32) M512i {
	return M512i(maskSlliEpi32([64]byte(src), uint16(k), [64]byte(a), imm8))
}

func maskSlliEpi32(src [64]byte, k uint16, a [64]byte, imm8 uint32) [64]byte


// MaskzSlliEpi32: Shift packed 32-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLD'. Intrinsic: '_mm512_maskz_slli_epi32'.
// Requires AVX512F.
func MaskzSlliEpi32(k Mmask16, a M512i, imm8 uint32) M512i {
	return M512i(maskzSlliEpi32(uint16(k), [64]byte(a), imm8))
}

func maskzSlliEpi32(k uint16, a [64]byte, imm8 uint32) [64]byte


// SlliEpi32: Shift packed 32-bit integers in 'a' left by 'imm8' while shifting
// in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF imm8[7:0] > 31
//				dst[i+31:i] := 0
//			ELSE
//				dst[i+31:i] := ZeroExtend(a[i+31:i] << imm8[7:0])
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLD'. Intrinsic: '_mm512_slli_epi32'.
// Requires KNCNI.
func SlliEpi32(a M512i, imm8 uint32) M512i {
	return M512i(slliEpi32([64]byte(a), imm8))
}

func slliEpi32(a [64]byte, imm8 uint32) [64]byte


// MaskSlliEpi64: Shift packed 64-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm512_mask_slli_epi64'.
// Requires AVX512F.
func MaskSlliEpi64(src M512i, k Mmask8, a M512i, imm8 uint32) M512i {
	return M512i(maskSlliEpi64([64]byte(src), uint8(k), [64]byte(a), imm8))
}

func maskSlliEpi64(src [64]byte, k uint8, a [64]byte, imm8 uint32) [64]byte


// MaskzSlliEpi64: Shift packed 64-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm512_maskz_slli_epi64'.
// Requires AVX512F.
func MaskzSlliEpi64(k Mmask8, a M512i, imm8 uint32) M512i {
	return M512i(maskzSlliEpi64(uint8(k), [64]byte(a), imm8))
}

func maskzSlliEpi64(k uint8, a [64]byte, imm8 uint32) [64]byte


// SlliEpi64: Shift packed 64-bit integers in 'a' left by 'imm8' while shifting
// in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF imm8[7:0] > 63
//				dst[i+63:i] := 0
//			ELSE
//				dst[i+63:i] := ZeroExtend(a[i+63:i] << imm8[7:0])
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm512_slli_epi64'.
// Requires AVX512F.
func SlliEpi64(a M512i, imm8 uint32) M512i {
	return M512i(slliEpi64([64]byte(a), imm8))
}

func slliEpi64(a [64]byte, imm8 uint32) [64]byte


// MaskSllvEpi16: Shift packed 16-bit integers in 'a' left by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := ZeroExtend(a[i+15:i] << count[i+15:i])
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLVW'. Intrinsic: '_mm512_mask_sllv_epi16'.
// Requires AVX512BW.
func MaskSllvEpi16(src M512i, k Mmask32, a M512i, count M512i) M512i {
	return M512i(maskSllvEpi16([64]byte(src), uint32(k), [64]byte(a), [64]byte(count)))
}

func maskSllvEpi16(src [64]byte, k uint32, a [64]byte, count [64]byte) [64]byte


// MaskzSllvEpi16: Shift packed 16-bit integers in 'a' left by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := ZeroExtend(a[i+15:i] << count[i+15:i])
//			ELSE
//				dst[i+15:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLVW'. Intrinsic: '_mm512_maskz_sllv_epi16'.
// Requires AVX512BW.
func MaskzSllvEpi16(k Mmask32, a M512i, count M512i) M512i {
	return M512i(maskzSllvEpi16(uint32(k), [64]byte(a), [64]byte(count)))
}

func maskzSllvEpi16(k uint32, a [64]byte, count [64]byte) [64]byte


// SllvEpi16: Shift packed 16-bit integers in 'a' left by the amount specified
// by the corresponding element in 'count' while shifting in zeros, and store
// the results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := ZeroExtend(a[i+15:i] << count[i+15:i])
//			ELSE
//				dst[i+15:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLVW'. Intrinsic: '_mm512_sllv_epi16'.
// Requires AVX512BW.
func SllvEpi16(a M512i, count M512i) M512i {
	return M512i(sllvEpi16([64]byte(a), [64]byte(count)))
}

func sllvEpi16(a [64]byte, count [64]byte) [64]byte


// MaskSllvEpi32: Shift packed 32-bit integers in 'a' left by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[i+31:i] << count[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLVD'. Intrinsic: '_mm512_mask_sllv_epi32'.
// Requires KNCNI.
func MaskSllvEpi32(src M512i, k Mmask16, a M512i, count M512i) M512i {
	return M512i(maskSllvEpi32([64]byte(src), uint16(k), [64]byte(a), [64]byte(count)))
}

func maskSllvEpi32(src [64]byte, k uint16, a [64]byte, count [64]byte) [64]byte


// MaskzSllvEpi32: Shift packed 32-bit integers in 'a' left by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[i+31:i] << count[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLVD'. Intrinsic: '_mm512_maskz_sllv_epi32'.
// Requires AVX512F.
func MaskzSllvEpi32(k Mmask16, a M512i, count M512i) M512i {
	return M512i(maskzSllvEpi32(uint16(k), [64]byte(a), [64]byte(count)))
}

func maskzSllvEpi32(k uint16, a [64]byte, count [64]byte) [64]byte


// SllvEpi32: Shift packed 32-bit integers in 'a' left by the amount specified
// by the corresponding element in 'count' while shifting in zeros, and store
// the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ZeroExtend(a[i+31:i] << count[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLVD'. Intrinsic: '_mm512_sllv_epi32'.
// Requires KNCNI.
func SllvEpi32(a M512i, count M512i) M512i {
	return M512i(sllvEpi32([64]byte(a), [64]byte(count)))
}

func sllvEpi32(a [64]byte, count [64]byte) [64]byte


// MaskSllvEpi64: Shift packed 64-bit integers in 'a' left by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[i+63:i] << count[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLVQ'. Intrinsic: '_mm512_mask_sllv_epi64'.
// Requires AVX512F.
func MaskSllvEpi64(src M512i, k Mmask8, a M512i, count M512i) M512i {
	return M512i(maskSllvEpi64([64]byte(src), uint8(k), [64]byte(a), [64]byte(count)))
}

func maskSllvEpi64(src [64]byte, k uint8, a [64]byte, count [64]byte) [64]byte


// MaskzSllvEpi64: Shift packed 64-bit integers in 'a' left by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[i+63:i] << count[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLVQ'. Intrinsic: '_mm512_maskz_sllv_epi64'.
// Requires AVX512F.
func MaskzSllvEpi64(k Mmask8, a M512i, count M512i) M512i {
	return M512i(maskzSllvEpi64(uint8(k), [64]byte(a), [64]byte(count)))
}

func maskzSllvEpi64(k uint8, a [64]byte, count [64]byte) [64]byte


// SllvEpi64: Shift packed 64-bit integers in 'a' left by the amount specified
// by the corresponding element in 'count' while shifting in zeros, and store
// the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ZeroExtend(a[i+63:i] << count[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLVQ'. Intrinsic: '_mm512_sllv_epi64'.
// Requires AVX512F.
func SllvEpi64(a M512i, count M512i) M512i {
	return M512i(sllvEpi64([64]byte(a), [64]byte(count)))
}

func sllvEpi64(a [64]byte, count [64]byte) [64]byte


// MaskSqrtPd: Compute the square root of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SQRT(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSQRTPD'. Intrinsic: '_mm512_mask_sqrt_pd'.
// Requires AVX512F.
func MaskSqrtPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskSqrtPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskSqrtPd(src [8]float64, k uint8, a [8]float64) [8]float64


// MaskzSqrtPd: Compute the square root of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SQRT(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSQRTPD'. Intrinsic: '_mm512_maskz_sqrt_pd'.
// Requires AVX512F.
func MaskzSqrtPd(k Mmask8, a M512d) M512d {
	return M512d(maskzSqrtPd(uint8(k), [8]float64(a)))
}

func maskzSqrtPd(k uint8, a [8]float64) [8]float64


// SqrtPd: Compute the square root of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := SQRT(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSQRTPD'. Intrinsic: '_mm512_sqrt_pd'.
// Requires AVX512F.
func SqrtPd(a M512d) M512d {
	return M512d(sqrtPd([8]float64(a)))
}

func sqrtPd(a [8]float64) [8]float64


// MaskSqrtPs: Compute the square root of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SQRT(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSQRTPS'. Intrinsic: '_mm512_mask_sqrt_ps'.
// Requires AVX512F.
func MaskSqrtPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskSqrtPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskSqrtPs(src [16]float32, k uint16, a [16]float32) [16]float32


// MaskzSqrtPs: Compute the square root of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SQRT(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSQRTPS'. Intrinsic: '_mm512_maskz_sqrt_ps'.
// Requires AVX512F.
func MaskzSqrtPs(k Mmask16, a M512) M512 {
	return M512(maskzSqrtPs(uint16(k), [16]float32(a)))
}

func maskzSqrtPs(k uint16, a [16]float32) [16]float32


// SqrtPs: Compute the square root of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := SQRT(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSQRTPS'. Intrinsic: '_mm512_sqrt_ps'.
// Requires AVX512F.
func SqrtPs(a M512) M512 {
	return M512(sqrtPs([16]float32(a)))
}

func sqrtPs(a [16]float32) [16]float32


// MaskSqrtRoundPd: Compute the square root of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SQRT(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSQRTPD'. Intrinsic: '_mm512_mask_sqrt_round_pd'.
// Requires AVX512F.
func MaskSqrtRoundPd(src M512d, k Mmask8, a M512d, rounding int) M512d {
	return M512d(maskSqrtRoundPd([8]float64(src), uint8(k), [8]float64(a), rounding))
}

func maskSqrtRoundPd(src [8]float64, k uint8, a [8]float64, rounding int) [8]float64


// MaskzSqrtRoundPd: Compute the square root of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SQRT(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSQRTPD'. Intrinsic: '_mm512_maskz_sqrt_round_pd'.
// Requires AVX512F.
func MaskzSqrtRoundPd(k Mmask8, a M512d, rounding int) M512d {
	return M512d(maskzSqrtRoundPd(uint8(k), [8]float64(a), rounding))
}

func maskzSqrtRoundPd(k uint8, a [8]float64, rounding int) [8]float64


// SqrtRoundPd: Compute the square root of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := SQRT(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSQRTPD'. Intrinsic: '_mm512_sqrt_round_pd'.
// Requires AVX512F.
func SqrtRoundPd(a M512d, rounding int) M512d {
	return M512d(sqrtRoundPd([8]float64(a), rounding))
}

func sqrtRoundPd(a [8]float64, rounding int) [8]float64


// MaskSqrtRoundPs: Compute the square root of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SQRT(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSQRTPS'. Intrinsic: '_mm512_mask_sqrt_round_ps'.
// Requires AVX512F.
func MaskSqrtRoundPs(src M512, k Mmask16, a M512, rounding int) M512 {
	return M512(maskSqrtRoundPs([16]float32(src), uint16(k), [16]float32(a), rounding))
}

func maskSqrtRoundPs(src [16]float32, k uint16, a [16]float32, rounding int) [16]float32


// MaskzSqrtRoundPs: Compute the square root of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SQRT(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSQRTPS'. Intrinsic: '_mm512_maskz_sqrt_round_ps'.
// Requires AVX512F.
func MaskzSqrtRoundPs(k Mmask16, a M512, rounding int) M512 {
	return M512(maskzSqrtRoundPs(uint16(k), [16]float32(a), rounding))
}

func maskzSqrtRoundPs(k uint16, a [16]float32, rounding int) [16]float32


// SqrtRoundPs: Compute the square root of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := SQRT(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSQRTPS'. Intrinsic: '_mm512_sqrt_round_ps'.
// Requires AVX512F.
func SqrtRoundPs(a M512, rounding int) M512 {
	return M512(sqrtRoundPs([16]float32(a), rounding))
}

func sqrtRoundPs(a [16]float32, rounding int) [16]float32


// MaskSraEpi16: Shift packed 16-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				IF count[63:0] > 15
//					dst[i+15:i] := SignBit
//				ELSE
//					dst[i+15:i] := SignExtend(a[i+15:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAW'. Intrinsic: '_mm512_mask_sra_epi16'.
// Requires AVX512BW.
func MaskSraEpi16(src M512i, k Mmask32, a M512i, count M128i) M512i {
	return M512i(maskSraEpi16([64]byte(src), uint32(k), [64]byte(a), [16]byte(count)))
}

func maskSraEpi16(src [64]byte, k uint32, a [64]byte, count [16]byte) [64]byte


// MaskzSraEpi16: Shift packed 16-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				IF count[63:0] > 15
//					dst[i+15:i] := SignBit
//				ELSE
//					dst[i+15:i] := SignExtend(a[i+15:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAW'. Intrinsic: '_mm512_maskz_sra_epi16'.
// Requires AVX512BW.
func MaskzSraEpi16(k Mmask32, a M512i, count M128i) M512i {
	return M512i(maskzSraEpi16(uint32(k), [64]byte(a), [16]byte(count)))
}

func maskzSraEpi16(k uint32, a [64]byte, count [16]byte) [64]byte


// SraEpi16: Shift packed 16-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF count[63:0] > 15
//				dst[i+15:i] := SignBit
//			ELSE
//				dst[i+15:i] := SignExtend(a[i+15:i] >> count[63:0])
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAW'. Intrinsic: '_mm512_sra_epi16'.
// Requires AVX512BW.
func SraEpi16(a M512i, count M128i) M512i {
	return M512i(sraEpi16([64]byte(a), [16]byte(count)))
}

func sraEpi16(a [64]byte, count [16]byte) [64]byte


// MaskSraEpi32: Shift packed 32-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := SignBit
//				ELSE
//					dst[i+31:i] := SignExtend(a[i+31:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAD'. Intrinsic: '_mm512_mask_sra_epi32'.
// Requires AVX512F.
func MaskSraEpi32(src M512i, k Mmask16, a M512i, count M128i) M512i {
	return M512i(maskSraEpi32([64]byte(src), uint16(k), [64]byte(a), [16]byte(count)))
}

func maskSraEpi32(src [64]byte, k uint16, a [64]byte, count [16]byte) [64]byte


// MaskzSraEpi32: Shift packed 32-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := SignBit
//				ELSE
//					dst[i+31:i] := SignExtend(a[i+31:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAD'. Intrinsic: '_mm512_maskz_sra_epi32'.
// Requires AVX512F.
func MaskzSraEpi32(k Mmask16, a M512i, count M128i) M512i {
	return M512i(maskzSraEpi32(uint16(k), [64]byte(a), [16]byte(count)))
}

func maskzSraEpi32(k uint16, a [64]byte, count [16]byte) [64]byte


// SraEpi32: Shift packed 32-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF count[63:0] > 31
//				dst[i+31:i] := SignBit
//			ELSE
//				dst[i+31:i] := SignExtend(a[i+31:i] >> count[63:0])
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAD'. Intrinsic: '_mm512_sra_epi32'.
// Requires AVX512F.
func SraEpi32(a M512i, count M128i) M512i {
	return M512i(sraEpi32([64]byte(a), [16]byte(count)))
}

func sraEpi32(a [64]byte, count [16]byte) [64]byte


// MaskSraEpi64: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := SignBit
//				ELSE
//					dst[i+63:i] := SignExtend(a[i+63:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm512_mask_sra_epi64'.
// Requires AVX512F.
func MaskSraEpi64(src M512i, k Mmask8, a M512i, count M128i) M512i {
	return M512i(maskSraEpi64([64]byte(src), uint8(k), [64]byte(a), [16]byte(count)))
}

func maskSraEpi64(src [64]byte, k uint8, a [64]byte, count [16]byte) [64]byte


// MaskzSraEpi64: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := SignBit
//				ELSE
//					dst[i+63:i] := SignExtend(a[i+63:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm512_maskz_sra_epi64'.
// Requires AVX512F.
func MaskzSraEpi64(k Mmask8, a M512i, count M128i) M512i {
	return M512i(maskzSraEpi64(uint8(k), [64]byte(a), [16]byte(count)))
}

func maskzSraEpi64(k uint8, a [64]byte, count [16]byte) [64]byte


// SraEpi64: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF count[63:0] > 63
//				dst[i+63:i] := SignBit
//			ELSE
//				dst[i+63:i] := SignExtend(a[i+63:i] >> count[63:0])
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm512_sra_epi64'.
// Requires AVX512F.
func SraEpi64(a M512i, count M128i) M512i {
	return M512i(sraEpi64([64]byte(a), [16]byte(count)))
}

func sraEpi64(a [64]byte, count [16]byte) [64]byte


// MaskSraiEpi16: Shift packed 16-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				IF imm8[7:0] > 15
//					dst[i+15:i] := SignBit
//				ELSE
//					dst[i+15:i] := SignExtend(a[i+15:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAW'. Intrinsic: '_mm512_mask_srai_epi16'.
// Requires AVX512BW.
func MaskSraiEpi16(src M512i, k Mmask32, a M512i, imm8 uint32) M512i {
	return M512i(maskSraiEpi16([64]byte(src), uint32(k), [64]byte(a), imm8))
}

func maskSraiEpi16(src [64]byte, k uint32, a [64]byte, imm8 uint32) [64]byte


// MaskzSraiEpi16: Shift packed 16-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				IF imm8[7:0] > 15
//					dst[i+15:i] := SignBit
//				ELSE
//					dst[i+15:i] := SignExtend(a[i+15:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAW'. Intrinsic: '_mm512_maskz_srai_epi16'.
// Requires AVX512BW.
func MaskzSraiEpi16(k Mmask32, a M512i, imm8 uint32) M512i {
	return M512i(maskzSraiEpi16(uint32(k), [64]byte(a), imm8))
}

func maskzSraiEpi16(k uint32, a [64]byte, imm8 uint32) [64]byte


// SraiEpi16: Shift packed 16-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF imm8[7:0] > 15
//				dst[i+15:i] := SignBit
//			ELSE
//				dst[i+15:i] := SignExtend(a[i+15:i] >> imm8[7:0])
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAW'. Intrinsic: '_mm512_srai_epi16'.
// Requires AVX512BW.
func SraiEpi16(a M512i, imm8 uint32) M512i {
	return M512i(sraiEpi16([64]byte(a), imm8))
}

func sraiEpi16(a [64]byte, imm8 uint32) [64]byte


// MaskSraiEpi32: Shift packed 32-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := SignBit
//				ELSE
//					dst[i+31:i] := SignExtend(a[i+31:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAD'. Intrinsic: '_mm512_mask_srai_epi32'.
// Requires KNCNI.
func MaskSraiEpi32(src M512i, k Mmask16, a M512i, imm8 uint32) M512i {
	return M512i(maskSraiEpi32([64]byte(src), uint16(k), [64]byte(a), imm8))
}

func maskSraiEpi32(src [64]byte, k uint16, a [64]byte, imm8 uint32) [64]byte


// MaskzSraiEpi32: Shift packed 32-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := SignBit
//				ELSE
//					dst[i+31:i] := SignExtend(a[i+31:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAD'. Intrinsic: '_mm512_maskz_srai_epi32'.
// Requires AVX512F.
func MaskzSraiEpi32(k Mmask16, a M512i, imm8 uint32) M512i {
	return M512i(maskzSraiEpi32(uint16(k), [64]byte(a), imm8))
}

func maskzSraiEpi32(k uint16, a [64]byte, imm8 uint32) [64]byte


// SraiEpi32: Shift packed 32-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF imm8[7:0] > 31
//				dst[i+31:i] := SignBit
//			ELSE
//				dst[i+31:i] := SignExtend(a[i+31:i] >> imm8[7:0])
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAD'. Intrinsic: '_mm512_srai_epi32'.
// Requires KNCNI.
func SraiEpi32(a M512i, imm8 uint32) M512i {
	return M512i(sraiEpi32([64]byte(a), imm8))
}

func sraiEpi32(a [64]byte, imm8 uint32) [64]byte


// MaskSraiEpi64: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := SignBit
//				ELSE
//					dst[i+63:i] := SignExtend(a[i+63:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm512_mask_srai_epi64'.
// Requires AVX512F.
func MaskSraiEpi64(src M512i, k Mmask8, a M512i, imm8 uint32) M512i {
	return M512i(maskSraiEpi64([64]byte(src), uint8(k), [64]byte(a), imm8))
}

func maskSraiEpi64(src [64]byte, k uint8, a [64]byte, imm8 uint32) [64]byte


// MaskzSraiEpi64: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := SignBit
//				ELSE
//					dst[i+63:i] := SignExtend(a[i+63:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm512_maskz_srai_epi64'.
// Requires AVX512F.
func MaskzSraiEpi64(k Mmask8, a M512i, imm8 uint32) M512i {
	return M512i(maskzSraiEpi64(uint8(k), [64]byte(a), imm8))
}

func maskzSraiEpi64(k uint8, a [64]byte, imm8 uint32) [64]byte


// SraiEpi64: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF imm8[7:0] > 63
//				dst[i+63:i] := SignBit
//			ELSE
//				dst[i+63:i] := SignExtend(a[i+63:i] << imm8[7:0])
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm512_srai_epi64'.
// Requires AVX512F.
func SraiEpi64(a M512i, imm8 uint32) M512i {
	return M512i(sraiEpi64([64]byte(a), imm8))
}

func sraiEpi64(a [64]byte, imm8 uint32) [64]byte


// MaskSravEpi16: Shift packed 16-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := SignExtend(a[i+15:i] >> count[i+15:i])
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAVW'. Intrinsic: '_mm512_mask_srav_epi16'.
// Requires AVX512BW.
func MaskSravEpi16(src M512i, k Mmask32, a M512i, count M512i) M512i {
	return M512i(maskSravEpi16([64]byte(src), uint32(k), [64]byte(a), [64]byte(count)))
}

func maskSravEpi16(src [64]byte, k uint32, a [64]byte, count [64]byte) [64]byte


// MaskzSravEpi16: Shift packed 16-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := SignExtend(a[i+15:i] >> count[i+15:i])
//			ELSE
//				dst[i+15:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAVW'. Intrinsic: '_mm512_maskz_srav_epi16'.
// Requires AVX512BW.
func MaskzSravEpi16(k Mmask32, a M512i, count M512i) M512i {
	return M512i(maskzSravEpi16(uint32(k), [64]byte(a), [64]byte(count)))
}

func maskzSravEpi16(k uint32, a [64]byte, count [64]byte) [64]byte


// SravEpi16: Shift packed 16-bit integers in 'a' right by the amount specified
// by the corresponding element in 'count' while shifting in sign bits, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			dst[i+15:i] := SignExtend(a[i+15:i] >> count[i+15:i])	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAVW'. Intrinsic: '_mm512_srav_epi16'.
// Requires AVX512BW.
func SravEpi16(a M512i, count M512i) M512i {
	return M512i(sravEpi16([64]byte(a), [64]byte(count)))
}

func sravEpi16(a [64]byte, count [64]byte) [64]byte


// MaskSravEpi32: Shift packed 32-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[i+31:i] >> count[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAVD'. Intrinsic: '_mm512_mask_srav_epi32'.
// Requires KNCNI.
func MaskSravEpi32(src M512i, k Mmask16, a M512i, count M512i) M512i {
	return M512i(maskSravEpi32([64]byte(src), uint16(k), [64]byte(a), [64]byte(count)))
}

func maskSravEpi32(src [64]byte, k uint16, a [64]byte, count [64]byte) [64]byte


// MaskzSravEpi32: Shift packed 32-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[i+31:i] >> count[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAVD'. Intrinsic: '_mm512_maskz_srav_epi32'.
// Requires AVX512F.
func MaskzSravEpi32(k Mmask16, a M512i, count M512i) M512i {
	return M512i(maskzSravEpi32(uint16(k), [64]byte(a), [64]byte(count)))
}

func maskzSravEpi32(k uint16, a [64]byte, count [64]byte) [64]byte


// SravEpi32: Shift packed 32-bit integers in 'a' right by the amount specified
// by the corresponding element in 'count' while shifting in sign bits, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := SignExtend(a[i+31:i] >> count[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAVD'. Intrinsic: '_mm512_srav_epi32'.
// Requires KNCNI.
func SravEpi32(a M512i, count M512i) M512i {
	return M512i(sravEpi32([64]byte(a), [64]byte(count)))
}

func sravEpi32(a [64]byte, count [64]byte) [64]byte


// MaskSravEpi64: Shift packed 64-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[i+63:i] >> count[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAVQ'. Intrinsic: '_mm512_mask_srav_epi64'.
// Requires AVX512F.
func MaskSravEpi64(src M512i, k Mmask8, a M512i, count M512i) M512i {
	return M512i(maskSravEpi64([64]byte(src), uint8(k), [64]byte(a), [64]byte(count)))
}

func maskSravEpi64(src [64]byte, k uint8, a [64]byte, count [64]byte) [64]byte


// MaskzSravEpi64: Shift packed 64-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[i+63:i] >> count[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAVQ'. Intrinsic: '_mm512_maskz_srav_epi64'.
// Requires AVX512F.
func MaskzSravEpi64(k Mmask8, a M512i, count M512i) M512i {
	return M512i(maskzSravEpi64(uint8(k), [64]byte(a), [64]byte(count)))
}

func maskzSravEpi64(k uint8, a [64]byte, count [64]byte) [64]byte


// SravEpi64: Shift packed 64-bit integers in 'a' right by the amount specified
// by the corresponding element in 'count' while shifting in sign bits, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := SignExtend(a[i+63:i] >> count[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAVQ'. Intrinsic: '_mm512_srav_epi64'.
// Requires AVX512F.
func SravEpi64(a M512i, count M512i) M512i {
	return M512i(sravEpi64([64]byte(a), [64]byte(count)))
}

func sravEpi64(a [64]byte, count [64]byte) [64]byte


// MaskSrlEpi16: Shift packed 16-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				IF count[63:0] > 15
//					dst[i+15:i] := 0
//				ELSE
//					dst[i+15:i] := ZeroExtend(a[i+15:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLW'. Intrinsic: '_mm512_mask_srl_epi16'.
// Requires AVX512BW.
func MaskSrlEpi16(src M512i, k Mmask32, a M512i, count M128i) M512i {
	return M512i(maskSrlEpi16([64]byte(src), uint32(k), [64]byte(a), [16]byte(count)))
}

func maskSrlEpi16(src [64]byte, k uint32, a [64]byte, count [16]byte) [64]byte


// MaskzSrlEpi16: Shift packed 16-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				IF count[63:0] > 15
//					dst[i+15:i] := 0
//				ELSE
//					dst[i+15:i] := ZeroExtend(a[i+15:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLW'. Intrinsic: '_mm512_maskz_srl_epi16'.
// Requires AVX512BW.
func MaskzSrlEpi16(k Mmask32, a M512i, count M128i) M512i {
	return M512i(maskzSrlEpi16(uint32(k), [64]byte(a), [16]byte(count)))
}

func maskzSrlEpi16(k uint32, a [64]byte, count [16]byte) [64]byte


// SrlEpi16: Shift packed 16-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF count[63:0] > 15
//				dst[i+15:i] := 0
//			ELSE
//				dst[i+15:i] := ZeroExtend(a[i+15:i] >> count[63:0])
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLW'. Intrinsic: '_mm512_srl_epi16'.
// Requires AVX512BW.
func SrlEpi16(a M512i, count M128i) M512i {
	return M512i(srlEpi16([64]byte(a), [16]byte(count)))
}

func srlEpi16(a [64]byte, count [16]byte) [64]byte


// MaskSrlEpi32: Shift packed 32-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLD'. Intrinsic: '_mm512_mask_srl_epi32'.
// Requires AVX512F.
func MaskSrlEpi32(src M512i, k Mmask16, a M512i, count M128i) M512i {
	return M512i(maskSrlEpi32([64]byte(src), uint16(k), [64]byte(a), [16]byte(count)))
}

func maskSrlEpi32(src [64]byte, k uint16, a [64]byte, count [16]byte) [64]byte


// MaskzSrlEpi32: Shift packed 32-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLD'. Intrinsic: '_mm512_maskz_srl_epi32'.
// Requires AVX512F.
func MaskzSrlEpi32(k Mmask16, a M512i, count M128i) M512i {
	return M512i(maskzSrlEpi32(uint16(k), [64]byte(a), [16]byte(count)))
}

func maskzSrlEpi32(k uint16, a [64]byte, count [16]byte) [64]byte


// SrlEpi32: Shift packed 32-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF count[63:0] > 31
//				dst[i+31:i] := 0
//			ELSE
//				dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[63:0])
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLD'. Intrinsic: '_mm512_srl_epi32'.
// Requires AVX512F.
func SrlEpi32(a M512i, count M128i) M512i {
	return M512i(srlEpi32([64]byte(a), [16]byte(count)))
}

func srlEpi32(a [64]byte, count [16]byte) [64]byte


// MaskSrlEpi64: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm512_mask_srl_epi64'.
// Requires AVX512F.
func MaskSrlEpi64(src M512i, k Mmask8, a M512i, count M128i) M512i {
	return M512i(maskSrlEpi64([64]byte(src), uint8(k), [64]byte(a), [16]byte(count)))
}

func maskSrlEpi64(src [64]byte, k uint8, a [64]byte, count [16]byte) [64]byte


// MaskzSrlEpi64: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm512_maskz_srl_epi64'.
// Requires AVX512F.
func MaskzSrlEpi64(k Mmask8, a M512i, count M128i) M512i {
	return M512i(maskzSrlEpi64(uint8(k), [64]byte(a), [16]byte(count)))
}

func maskzSrlEpi64(k uint8, a [64]byte, count [16]byte) [64]byte


// SrlEpi64: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF count[63:0] > 63
//				dst[i+63:i] := 0
//			ELSE
//				dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[63:0])
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm512_srl_epi64'.
// Requires AVX512F.
func SrlEpi64(a M512i, count M128i) M512i {
	return M512i(srlEpi64([64]byte(a), [16]byte(count)))
}

func srlEpi64(a [64]byte, count [16]byte) [64]byte


// MaskSrliEpi16: Shift packed 16-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				IF imm8[7:0] > 15
//					dst[i+15:i] := 0
//				ELSE
//					dst[i+15:i] := ZeroExtend(a[i+15:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLW'. Intrinsic: '_mm512_mask_srli_epi16'.
// Requires AVX512BW.
func MaskSrliEpi16(src M512i, k Mmask32, a M512i, imm8 uint32) M512i {
	return M512i(maskSrliEpi16([64]byte(src), uint32(k), [64]byte(a), imm8))
}

func maskSrliEpi16(src [64]byte, k uint32, a [64]byte, imm8 uint32) [64]byte


// MaskzSrliEpi16: Shift packed 16-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				IF imm8[7:0] > 15
//					dst[i+15:i] := 0
//				ELSE
//					dst[i+15:i] := ZeroExtend(a[i+15:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLW'. Intrinsic: '_mm512_maskz_srli_epi16'.
// Requires AVX512BW.
func MaskzSrliEpi16(k Mmask32, a M512i, imm8 int) M512i {
	return M512i(maskzSrliEpi16(uint32(k), [64]byte(a), imm8))
}

func maskzSrliEpi16(k uint32, a [64]byte, imm8 int) [64]byte


// SrliEpi16: Shift packed 16-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF imm8[7:0] > 15
//				dst[i+15:i] := 0
//			ELSE
//				dst[i+15:i] := ZeroExtend(a[i+15:i] >> imm8[7:0])
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLW'. Intrinsic: '_mm512_srli_epi16'.
// Requires AVX512BW.
func SrliEpi16(a M512i, imm8 uint32) M512i {
	return M512i(srliEpi16([64]byte(a), imm8))
}

func srliEpi16(a [64]byte, imm8 uint32) [64]byte


// MaskSrliEpi32: Shift packed 32-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLD'. Intrinsic: '_mm512_mask_srli_epi32'.
// Requires KNCNI.
func MaskSrliEpi32(src M512i, k Mmask16, a M512i, imm8 uint32) M512i {
	return M512i(maskSrliEpi32([64]byte(src), uint16(k), [64]byte(a), imm8))
}

func maskSrliEpi32(src [64]byte, k uint16, a [64]byte, imm8 uint32) [64]byte


// MaskzSrliEpi32: Shift packed 32-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLD'. Intrinsic: '_mm512_maskz_srli_epi32'.
// Requires AVX512F.
func MaskzSrliEpi32(k Mmask16, a M512i, imm8 uint32) M512i {
	return M512i(maskzSrliEpi32(uint16(k), [64]byte(a), imm8))
}

func maskzSrliEpi32(k uint16, a [64]byte, imm8 uint32) [64]byte


// SrliEpi32: Shift packed 32-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF imm8[7:0] > 31
//				dst[i+31:i] := 0
//			ELSE
//				dst[i+31:i] := ZeroExtend(a[i+31:i] >> imm8[7:0])
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLD'. Intrinsic: '_mm512_srli_epi32'.
// Requires KNCNI.
func SrliEpi32(a M512i, imm8 uint32) M512i {
	return M512i(srliEpi32([64]byte(a), imm8))
}

func srliEpi32(a [64]byte, imm8 uint32) [64]byte


// MaskSrliEpi64: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm512_mask_srli_epi64'.
// Requires AVX512F.
func MaskSrliEpi64(src M512i, k Mmask8, a M512i, imm8 uint32) M512i {
	return M512i(maskSrliEpi64([64]byte(src), uint8(k), [64]byte(a), imm8))
}

func maskSrliEpi64(src [64]byte, k uint8, a [64]byte, imm8 uint32) [64]byte


// MaskzSrliEpi64: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm512_maskz_srli_epi64'.
// Requires AVX512F.
func MaskzSrliEpi64(k Mmask8, a M512i, imm8 uint32) M512i {
	return M512i(maskzSrliEpi64(uint8(k), [64]byte(a), imm8))
}

func maskzSrliEpi64(k uint8, a [64]byte, imm8 uint32) [64]byte


// SrliEpi64: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF imm8[7:0] > 63
//				dst[i+63:i] := 0
//			ELSE
//				dst[i+63:i] := ZeroExtend(a[i+63:i] >> imm8[7:0])
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm512_srli_epi64'.
// Requires AVX512F.
func SrliEpi64(a M512i, imm8 uint32) M512i {
	return M512i(srliEpi64([64]byte(a), imm8))
}

func srliEpi64(a [64]byte, imm8 uint32) [64]byte


// MaskSrlvEpi16: Shift packed 16-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := ZeroExtend(a[i+15:i] >> count[i+15:i])
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLVW'. Intrinsic: '_mm512_mask_srlv_epi16'.
// Requires AVX512BW.
func MaskSrlvEpi16(src M512i, k Mmask32, a M512i, count M512i) M512i {
	return M512i(maskSrlvEpi16([64]byte(src), uint32(k), [64]byte(a), [64]byte(count)))
}

func maskSrlvEpi16(src [64]byte, k uint32, a [64]byte, count [64]byte) [64]byte


// MaskzSrlvEpi16: Shift packed 16-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := ZeroExtend(a[i+15:i] >> count[i+15:i])
//			ELSE
//				dst[i+15:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLVW'. Intrinsic: '_mm512_maskz_srlv_epi16'.
// Requires AVX512BW.
func MaskzSrlvEpi16(k Mmask32, a M512i, count M512i) M512i {
	return M512i(maskzSrlvEpi16(uint32(k), [64]byte(a), [64]byte(count)))
}

func maskzSrlvEpi16(k uint32, a [64]byte, count [64]byte) [64]byte


// SrlvEpi16: Shift packed 16-bit integers in 'a' right by the amount specified
// by the corresponding element in 'count' while shifting in zeros, and store
// the results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := ZeroExtend(a[i+15:i] >> count[i+15:i])
//			ELSE
//				dst[i+15:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLVW'. Intrinsic: '_mm512_srlv_epi16'.
// Requires AVX512BW.
func SrlvEpi16(a M512i, count M512i) M512i {
	return M512i(srlvEpi16([64]byte(a), [64]byte(count)))
}

func srlvEpi16(a [64]byte, count [64]byte) [64]byte


// MaskSrlvEpi32: Shift packed 32-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLVD'. Intrinsic: '_mm512_mask_srlv_epi32'.
// Requires KNCNI.
func MaskSrlvEpi32(src M512i, k Mmask16, a M512i, count M512i) M512i {
	return M512i(maskSrlvEpi32([64]byte(src), uint16(k), [64]byte(a), [64]byte(count)))
}

func maskSrlvEpi32(src [64]byte, k uint16, a [64]byte, count [64]byte) [64]byte


// MaskzSrlvEpi32: Shift packed 32-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLVD'. Intrinsic: '_mm512_maskz_srlv_epi32'.
// Requires AVX512F.
func MaskzSrlvEpi32(k Mmask16, a M512i, count M512i) M512i {
	return M512i(maskzSrlvEpi32(uint16(k), [64]byte(a), [64]byte(count)))
}

func maskzSrlvEpi32(k uint16, a [64]byte, count [64]byte) [64]byte


// SrlvEpi32: Shift packed 32-bit integers in 'a' right by the amount specified
// by the corresponding element in 'count' while shifting in zeros, and store
// the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLVD'. Intrinsic: '_mm512_srlv_epi32'.
// Requires KNCNI.
func SrlvEpi32(a M512i, count M512i) M512i {
	return M512i(srlvEpi32([64]byte(a), [64]byte(count)))
}

func srlvEpi32(a [64]byte, count [64]byte) [64]byte


// MaskSrlvEpi64: Shift packed 64-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLVQ'. Intrinsic: '_mm512_mask_srlv_epi64'.
// Requires AVX512F.
func MaskSrlvEpi64(src M512i, k Mmask8, a M512i, count M512i) M512i {
	return M512i(maskSrlvEpi64([64]byte(src), uint8(k), [64]byte(a), [64]byte(count)))
}

func maskSrlvEpi64(src [64]byte, k uint8, a [64]byte, count [64]byte) [64]byte


// MaskzSrlvEpi64: Shift packed 64-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLVQ'. Intrinsic: '_mm512_maskz_srlv_epi64'.
// Requires AVX512F.
func MaskzSrlvEpi64(k Mmask8, a M512i, count M512i) M512i {
	return M512i(maskzSrlvEpi64(uint8(k), [64]byte(a), [64]byte(count)))
}

func maskzSrlvEpi64(k uint8, a [64]byte, count [64]byte) [64]byte


// SrlvEpi64: Shift packed 64-bit integers in 'a' right by the amount specified
// by the corresponding element in 'count' while shifting in zeros, and store
// the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLVQ'. Intrinsic: '_mm512_srlv_epi64'.
// Requires AVX512F.
func SrlvEpi64(a M512i, count M512i) M512i {
	return M512i(srlvEpi64([64]byte(a), [64]byte(count)))
}

func srlvEpi64(a [64]byte, count [64]byte) [64]byte


// MaskStoreEpi32: Store packed 32-bit integers from 'a' into memory using
// writemask 'k'.
// 	'mem_addr' must be aligned on a 64-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVDQA32'. Intrinsic: '_mm512_mask_store_epi32'.
// Requires KNCNI.
func MaskStoreEpi32(mem_addr uintptr, k Mmask16, a M512i)  {
	maskStoreEpi32(uintptr(mem_addr), uint16(k), [64]byte(a))
}

func maskStoreEpi32(mem_addr uintptr, k uint16, a [64]byte) 


// StoreEpi32: Store 512-bits (composed of 16 packed 32-bit integers) from 'a'
// into memory. 
// 	'mem_addr' must be aligned on a 64-byte boundary or a general-protection
// exception may be generated. 
//
//		MEM[mem_addr+511:mem_addr] := a[511:0]
//
// Instruction: 'VMOVDQA32'. Intrinsic: '_mm512_store_epi32'.
// Requires KNCNI.
func StoreEpi32(mem_addr uintptr, a M512i)  {
	storeEpi32(uintptr(mem_addr), [64]byte(a))
}

func storeEpi32(mem_addr uintptr, a [64]byte) 


// MaskStoreEpi64: Store packed 64-bit integers from 'a' into memory using
// writemask 'k'.
// 	'mem_addr' must be aligned on a 64-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVDQA64'. Intrinsic: '_mm512_mask_store_epi64'.
// Requires KNCNI.
func MaskStoreEpi64(mem_addr uintptr, k Mmask8, a M512i)  {
	maskStoreEpi64(uintptr(mem_addr), uint8(k), [64]byte(a))
}

func maskStoreEpi64(mem_addr uintptr, k uint8, a [64]byte) 


// StoreEpi64: Store 512-bits (composed of 8 packed 64-bit integers) from 'a'
// into memory. 
// 	'mem_addr' must be aligned on a 64-byte boundary or a general-protection
// exception may be generated. 
//
//		MEM[mem_addr+511:mem_addr] := a[511:0]
//
// Instruction: 'VMOVDQA64'. Intrinsic: '_mm512_store_epi64'.
// Requires KNCNI.
func StoreEpi64(mem_addr uintptr, a M512i)  {
	storeEpi64(uintptr(mem_addr), [64]byte(a))
}

func storeEpi64(mem_addr uintptr, a [64]byte) 


// MaskStorePd: Store packed double-precision (64-bit) floating-point elements
// from 'a' into memory using writemask 'k'.
// 	'mem_addr' must be aligned on a 64-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVAPD'. Intrinsic: '_mm512_mask_store_pd'.
// Requires KNCNI.
func MaskStorePd(mem_addr uintptr, k Mmask8, a M512d)  {
	maskStorePd(uintptr(mem_addr), uint8(k), [8]float64(a))
}

func maskStorePd(mem_addr uintptr, k uint8, a [8]float64) 


// StorePd: Store 512-bits (composed of 8 packed double-precision (64-bit)
// floating-point elements) from 'a' into memory.
// 	'mem_addr' must be aligned on a 64-byte boundary or a general-protection
// exception may be generated. 
//
//		MEM[mem_addr+511:mem_addr] := a[511:0]
//
// Instruction: 'VMOVAPD'. Intrinsic: '_mm512_store_pd'.
// Requires KNCNI.
func StorePd(mem_addr uintptr, a M512d)  {
	storePd(uintptr(mem_addr), [8]float64(a))
}

func storePd(mem_addr uintptr, a [8]float64) 


// MaskStorePs: Store packed single-precision (32-bit) floating-point elements
// from 'a' into memory using writemask 'k'.
// 	'mem_addr' must be aligned on a 64-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVAPS'. Intrinsic: '_mm512_mask_store_ps'.
// Requires KNCNI.
func MaskStorePs(mem_addr uintptr, k Mmask16, a M512)  {
	maskStorePs(uintptr(mem_addr), uint16(k), [16]float32(a))
}

func maskStorePs(mem_addr uintptr, k uint16, a [16]float32) 


// StorePs: Store 512-bits (composed of 16 packed single-precision (32-bit)
// floating-point elements) from 'a' into memory. 
// 	'mem_addr' must be aligned on a 64-byte boundary or a general-protection
// exception may be generated. 
//
//		MEM[mem_addr+511:mem_addr] := a[511:0]
//
// Instruction: 'VMOVAPS'. Intrinsic: '_mm512_store_ps'.
// Requires KNCNI.
func StorePs(mem_addr uintptr, a M512)  {
	storePs(uintptr(mem_addr), [16]float32(a))
}

func storePs(mem_addr uintptr, a [16]float32) 


// StoreSi512: Store 512-bits of integer data from 'a' into memory. 
// 	'mem_addr' must be aligned on a 64-byte boundary or a general-protection
// exception may be generated. 
//
//		MEM[mem_addr+511:mem_addr] := a[511:0]
//
// Instruction: 'VMOVDQA32'. Intrinsic: '_mm512_store_si512'.
// Requires KNCNI.
func StoreSi512(mem_addr uintptr, a M512i)  {
	storeSi512(uintptr(mem_addr), [64]byte(a))
}

func storeSi512(mem_addr uintptr, a [64]byte) 


// StorenrPd: Stores packed double-precision (64-bit) floating-point elements
// from 'v' to memory address 'mt' with a no-read hint to the processor. 
//
//		addr := MEM[mt]
//		FOR j := 0 to 7
//			i := j*64
//			addr[i+63:i] := v[i+63:i]
//		ENDFOR
//
// Instruction: 'VMOVNRAPD'. Intrinsic: '_mm512_storenr_pd'.
// Requires KNCNI.
func StorenrPd(mt uintptr, v M512d)  {
	storenrPd(uintptr(mt), [8]float64(v))
}

func storenrPd(mt uintptr, v [8]float64) 


// StorenrPs: Stores packed single-precision (32-bit) floating-point elements
// from 'v' to memory address 'mt' with a no-read hint to the processor. 
//
//		addr := MEM[mt]
//		FOR j := 0 to 15
//			i := j*32
//			addr[i+31:i] := v[i+31:i]
//		ENDFOR
//
// Instruction: 'VMOVNRAPS'. Intrinsic: '_mm512_storenr_ps'.
// Requires KNCNI.
func StorenrPs(mt uintptr, v M512)  {
	storenrPs(uintptr(mt), [16]float32(v))
}

func storenrPs(mt uintptr, v [16]float32) 


// StorenrngoPd: Stores packed double-precision (64-bit) floating-point
// elements from 'v' to memory address 'mt' with a no-read hint and using a
// weakly-ordered memory consistency model (stores performed with this function
// are not globally ordered, and subsequent stores from the same thread can be
// observed before them). 
//
//		addr := MEM[mt]
//		FOR j := 0 to 7
//			i := j*64
//			addr[i+63:i] := v[i+63:i]
//		ENDFOR
//
// Instruction: 'VMOVNRNGOAPD'. Intrinsic: '_mm512_storenrngo_pd'.
// Requires KNCNI.
func StorenrngoPd(mt uintptr, v M512d)  {
	storenrngoPd(uintptr(mt), [8]float64(v))
}

func storenrngoPd(mt uintptr, v [8]float64) 


// StorenrngoPs: Stores packed single-precision (32-bit) floating-point
// elements from 'v' to memory address 'mt' with a no-read hint and using a
// weakly-ordered memory consistency model (stores performed with this function
// are not globally ordered, and subsequent stores from the same thread can be
// observed before them). 
//
//		addr := MEM[mt]
//		FOR j := 0 to 15
//			i := j*32
//			addr[i+31:i] := v[i+31:i]
//		ENDFOR
//
// Instruction: 'VMOVNRNGOAPS'. Intrinsic: '_mm512_storenrngo_ps'.
// Requires KNCNI.
func StorenrngoPs(mt uintptr, v M512)  {
	storenrngoPs(uintptr(mt), [16]float32(v))
}

func storenrngoPs(mt uintptr, v [16]float32) 


// MaskStoreuEpi16: Store packed 16-bit integers from 'a' into memory using
// writemask 'k'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				MEM[mem_addr+i+15:mem_addr+i] := a[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDQU16'. Intrinsic: '_mm512_mask_storeu_epi16'.
// Requires AVX512BW.
func MaskStoreuEpi16(mem_addr uintptr, k Mmask32, a M512i)  {
	maskStoreuEpi16(uintptr(mem_addr), uint32(k), [64]byte(a))
}

func maskStoreuEpi16(mem_addr uintptr, k uint32, a [64]byte) 


// MaskStoreuEpi32: Store packed 32-bit integers from 'a' into memory using
// writemask 'k'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVDQU32'. Intrinsic: '_mm512_mask_storeu_epi32'.
// Requires AVX512F.
func MaskStoreuEpi32(mem_addr uintptr, k Mmask16, a M512i)  {
	maskStoreuEpi32(uintptr(mem_addr), uint16(k), [64]byte(a))
}

func maskStoreuEpi32(mem_addr uintptr, k uint16, a [64]byte) 


// MaskStoreuEpi64: Store packed 64-bit integers from 'a' into memory using
// writemask 'k'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVDQU64'. Intrinsic: '_mm512_mask_storeu_epi64'.
// Requires AVX512F.
func MaskStoreuEpi64(mem_addr uintptr, k Mmask8, a M512i)  {
	maskStoreuEpi64(uintptr(mem_addr), uint8(k), [64]byte(a))
}

func maskStoreuEpi64(mem_addr uintptr, k uint8, a [64]byte) 


// MaskStoreuEpi8: Store packed 8-bit integers from 'a' into memory using
// writemask 'k'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				MEM[mem_addr+i+7:mem_addr+i] := a[i+7:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVDQU8'. Intrinsic: '_mm512_mask_storeu_epi8'.
// Requires AVX512BW.
func MaskStoreuEpi8(mem_addr uintptr, k Mmask64, a M512i)  {
	maskStoreuEpi8(uintptr(mem_addr), uint64(k), [64]byte(a))
}

func maskStoreuEpi8(mem_addr uintptr, k uint64, a [64]byte) 


// MaskStoreuPd: Store packed double-precision (64-bit) floating-point elements
// from 'a' into memory using writemask 'k'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVUPD'. Intrinsic: '_mm512_mask_storeu_pd'.
// Requires AVX512F.
func MaskStoreuPd(mem_addr uintptr, k Mmask8, a M512d)  {
	maskStoreuPd(uintptr(mem_addr), uint8(k), [8]float64(a))
}

func maskStoreuPd(mem_addr uintptr, k uint8, a [8]float64) 


// StoreuPd: Store 512-bits (composed of 8 packed double-precision (64-bit)
// floating-point elements) from 'a' into memory. 
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		MEM[mem_addr+511:mem_addr] := a[511:0]
//
// Instruction: 'VMOVUPD'. Intrinsic: '_mm512_storeu_pd'.
// Requires AVX512F.
func StoreuPd(mem_addr uintptr, a M512d)  {
	storeuPd(uintptr(mem_addr), [8]float64(a))
}

func storeuPd(mem_addr uintptr, a [8]float64) 


// MaskStoreuPs: Store packed single-precision (32-bit) floating-point elements
// from 'a' into memory using writemask 'k'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVUPS'. Intrinsic: '_mm512_mask_storeu_ps'.
// Requires AVX512F.
func MaskStoreuPs(mem_addr uintptr, k Mmask16, a M512)  {
	maskStoreuPs(uintptr(mem_addr), uint16(k), [16]float32(a))
}

func maskStoreuPs(mem_addr uintptr, k uint16, a [16]float32) 


// StoreuPs: Store 512-bits (composed of 16 packed single-precision (32-bit)
// floating-point elements) from 'a' into memory. 
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		MEM[mem_addr+511:mem_addr] := a[511:0]
//
// Instruction: 'VMOVUPS'. Intrinsic: '_mm512_storeu_ps'.
// Requires AVX512F.
func StoreuPs(mem_addr uintptr, a M512)  {
	storeuPs(uintptr(mem_addr), [16]float32(a))
}

func storeuPs(mem_addr uintptr, a [16]float32) 


// StoreuSi512: Store 512-bits of integer data from 'a' into memory.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		MEM[mem_addr+511:mem_addr] := a[511:0]
//
// Instruction: 'VMOVDQU32'. Intrinsic: '_mm512_storeu_si512'.
// Requires AVX512F.
func StoreuSi512(mem_addr uintptr, a M512i)  {
	storeuSi512(uintptr(mem_addr), [64]byte(a))
}

func storeuSi512(mem_addr uintptr, a [64]byte) 


// StreamLoadSi512: Load 512-bits of integer data from memory into 'dst' using
// a non-temporal memory hint. 
// 	'mem_addr' must be aligned on a 64-byte boundary or a general-protection
// exception may be generated. 
//
//		dst[511:0] := MEM[mem_addr+511:mem_addr]
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVNTDQA'. Intrinsic: '_mm512_stream_load_si512'.
// Requires AVX512F.
func StreamLoadSi512(mem_addr uintptr) M512i {
	return M512i(streamLoadSi512(uintptr(mem_addr)))
}

func streamLoadSi512(mem_addr uintptr) [64]byte


// StreamPd: Store 512-bits (composed of 8 packed double-precision (64-bit)
// floating-point elements) from 'a' into memory using a non-temporal memory
// hint. 
// 	'mem_addr' must be aligned on a 64-byte boundary or a general-protection
// exception may be generated. 
//
//		MEM[mem_addr+511:mem_addr] := a[511:0]
//
// Instruction: 'VMOVNTPD'. Intrinsic: '_mm512_stream_pd'.
// Requires AVX512F.
func StreamPd(mem_addr uintptr, a M512d)  {
	streamPd(uintptr(mem_addr), [8]float64(a))
}

func streamPd(mem_addr uintptr, a [8]float64) 


// StreamPs: Store 512-bits (composed of 16 packed single-precision (32-bit)
// floating-point elements) from 'a' into memory using a non-temporal memory
// hint. 
// 	'mem_addr' must be aligned on a 64-byte boundary or a general-protection
// exception may be generated. 
//
//		MEM[mem_addr+511:mem_addr] := a[511:0]
//
// Instruction: 'VMOVNTPS'. Intrinsic: '_mm512_stream_ps'.
// Requires AVX512F.
func StreamPs(mem_addr uintptr, a M512)  {
	streamPs(uintptr(mem_addr), [16]float32(a))
}

func streamPs(mem_addr uintptr, a [16]float32) 


// StreamSi512: Store 512-bits of integer data from 'a' into memory using a
// non-temporal memory hint. 
// 	'mem_addr' must be aligned on a 64-byte boundary or a general-protection
// exception may be generated. 
//
//		MEM[mem_addr+511:mem_addr] := a[511:0]
//
// Instruction: 'VMOVNTDQA'. Intrinsic: '_mm512_stream_si512'.
// Requires AVX512F.
func StreamSi512(mem_addr uintptr, a M512i)  {
	streamSi512(uintptr(mem_addr), [64]byte(a))
}

func streamSi512(mem_addr uintptr, a [64]byte) 


// MaskSubEpi16: Subtract packed 16-bit integers in 'b' from packed 16-bit
// integers in 'a', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := a[i+15:i] - b[i+15:i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSUBW'. Intrinsic: '_mm512_mask_sub_epi16'.
// Requires AVX512BW.
func MaskSubEpi16(src M512i, k Mmask32, a M512i, b M512i) M512i {
	return M512i(maskSubEpi16([64]byte(src), uint32(k), [64]byte(a), [64]byte(b)))
}

func maskSubEpi16(src [64]byte, k uint32, a [64]byte, b [64]byte) [64]byte


// MaskzSubEpi16: Subtract packed 16-bit integers in 'b' from packed 16-bit
// integers in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := a[i+15:i] - b[i+15:i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSUBW'. Intrinsic: '_mm512_maskz_sub_epi16'.
// Requires AVX512BW.
func MaskzSubEpi16(k Mmask32, a M512i, b M512i) M512i {
	return M512i(maskzSubEpi16(uint32(k), [64]byte(a), [64]byte(b)))
}

func maskzSubEpi16(k uint32, a [64]byte, b [64]byte) [64]byte


// SubEpi16: Subtract packed 16-bit integers in 'b' from packed 16-bit integers
// in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			dst[i+15:i] := a[i+15:i] - b[i+15:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSUBW'. Intrinsic: '_mm512_sub_epi16'.
// Requires AVX512BW.
func SubEpi16(a M512i, b M512i) M512i {
	return M512i(subEpi16([64]byte(a), [64]byte(b)))
}

func subEpi16(a [64]byte, b [64]byte) [64]byte


// MaskSubEpi32: Subtract packed 32-bit integers in 'b' from packed 32-bit
// integers in 'a', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] - b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSUBD'. Intrinsic: '_mm512_mask_sub_epi32'.
// Requires KNCNI.
func MaskSubEpi32(src M512i, k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskSubEpi32([64]byte(src), uint16(k), [64]byte(a), [64]byte(b)))
}

func maskSubEpi32(src [64]byte, k uint16, a [64]byte, b [64]byte) [64]byte


// MaskzSubEpi32: Subtract packed 32-bit integers in 'b' from packed 32-bit
// integers in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] - b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSUBD'. Intrinsic: '_mm512_maskz_sub_epi32'.
// Requires AVX512F.
func MaskzSubEpi32(k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskzSubEpi32(uint16(k), [64]byte(a), [64]byte(b)))
}

func maskzSubEpi32(k uint16, a [64]byte, b [64]byte) [64]byte


// SubEpi32: Subtract packed 32-bit integers in 'b' from packed 32-bit integers
// in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := a[i+31:i] - b[i+31:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSUBD'. Intrinsic: '_mm512_sub_epi32'.
// Requires KNCNI.
func SubEpi32(a M512i, b M512i) M512i {
	return M512i(subEpi32([64]byte(a), [64]byte(b)))
}

func subEpi32(a [64]byte, b [64]byte) [64]byte


// MaskSubEpi64: Subtract packed 64-bit integers in 'b' from packed 64-bit
// integers in 'a', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] - b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSUBQ'. Intrinsic: '_mm512_mask_sub_epi64'.
// Requires AVX512F.
func MaskSubEpi64(src M512i, k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskSubEpi64([64]byte(src), uint8(k), [64]byte(a), [64]byte(b)))
}

func maskSubEpi64(src [64]byte, k uint8, a [64]byte, b [64]byte) [64]byte


// MaskzSubEpi64: Subtract packed 64-bit integers in 'b' from packed 64-bit
// integers in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] - b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSUBQ'. Intrinsic: '_mm512_maskz_sub_epi64'.
// Requires AVX512F.
func MaskzSubEpi64(k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskzSubEpi64(uint8(k), [64]byte(a), [64]byte(b)))
}

func maskzSubEpi64(k uint8, a [64]byte, b [64]byte) [64]byte


// SubEpi64: Subtract packed 64-bit integers in 'b' from packed 64-bit integers
// in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := a[i+63:i] - b[i+63:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSUBQ'. Intrinsic: '_mm512_sub_epi64'.
// Requires AVX512F.
func SubEpi64(a M512i, b M512i) M512i {
	return M512i(subEpi64([64]byte(a), [64]byte(b)))
}

func subEpi64(a [64]byte, b [64]byte) [64]byte


// MaskSubEpi8: Subtract packed 8-bit integers in 'b' from packed 8-bit
// integers in 'a', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := a[i+7:i] - b[i+7:i]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSUBB'. Intrinsic: '_mm512_mask_sub_epi8'.
// Requires AVX512BW.
func MaskSubEpi8(src M512i, k Mmask64, a M512i, b M512i) M512i {
	return M512i(maskSubEpi8([64]byte(src), uint64(k), [64]byte(a), [64]byte(b)))
}

func maskSubEpi8(src [64]byte, k uint64, a [64]byte, b [64]byte) [64]byte


// MaskzSubEpi8: Subtract packed 8-bit integers in 'b' from packed 8-bit
// integers in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := a[i+7:i] - b[i+7:i]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSUBB'. Intrinsic: '_mm512_maskz_sub_epi8'.
// Requires AVX512BW.
func MaskzSubEpi8(k Mmask64, a M512i, b M512i) M512i {
	return M512i(maskzSubEpi8(uint64(k), [64]byte(a), [64]byte(b)))
}

func maskzSubEpi8(k uint64, a [64]byte, b [64]byte) [64]byte


// SubEpi8: Subtract packed 8-bit integers in 'b' from packed 8-bit integers in
// 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 63
//			i := j*8
//			dst[i+7:i] := a[i+7:i] - b[i+7:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSUBB'. Intrinsic: '_mm512_sub_epi8'.
// Requires AVX512BW.
func SubEpi8(a M512i, b M512i) M512i {
	return M512i(subEpi8([64]byte(a), [64]byte(b)))
}

func subEpi8(a [64]byte, b [64]byte) [64]byte


// MaskSubPd: Subtract packed double-precision (64-bit) floating-point elements
// in 'b' from packed double-precision (64-bit) floating-point elements in 'a',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] - b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSUBPD'. Intrinsic: '_mm512_mask_sub_pd'.
// Requires KNCNI.
func MaskSubPd(src M512d, k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskSubPd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b)))
}

func maskSubPd(src [8]float64, k uint8, a [8]float64, b [8]float64) [8]float64


// MaskzSubPd: Subtract packed double-precision (64-bit) floating-point
// elements in 'b' from packed double-precision (64-bit) floating-point
// elements in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] - b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSUBPD'. Intrinsic: '_mm512_maskz_sub_pd'.
// Requires AVX512F.
func MaskzSubPd(k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskzSubPd(uint8(k), [8]float64(a), [8]float64(b)))
}

func maskzSubPd(k uint8, a [8]float64, b [8]float64) [8]float64


// SubPd: Subtract packed double-precision (64-bit) floating-point elements in
// 'b' from packed double-precision (64-bit) floating-point elements in 'a',
// and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := a[i+63:i] - b[i+63:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSUBPD'. Intrinsic: '_mm512_sub_pd'.
// Requires KNCNI.
func SubPd(a M512d, b M512d) M512d {
	return M512d(subPd([8]float64(a), [8]float64(b)))
}

func subPd(a [8]float64, b [8]float64) [8]float64


// MaskSubPs: Subtract packed single-precision (32-bit) floating-point elements
// in 'b' from packed single-precision (32-bit) floating-point elements in 'a',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] - b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSUBPS'. Intrinsic: '_mm512_mask_sub_ps'.
// Requires KNCNI.
func MaskSubPs(src M512, k Mmask16, a M512, b M512) M512 {
	return M512(maskSubPs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b)))
}

func maskSubPs(src [16]float32, k uint16, a [16]float32, b [16]float32) [16]float32


// MaskzSubPs: Subtract packed single-precision (32-bit) floating-point
// elements in 'b' from packed single-precision (32-bit) floating-point
// elements in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] - b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSUBPS'. Intrinsic: '_mm512_maskz_sub_ps'.
// Requires AVX512F.
func MaskzSubPs(k Mmask16, a M512, b M512) M512 {
	return M512(maskzSubPs(uint16(k), [16]float32(a), [16]float32(b)))
}

func maskzSubPs(k uint16, a [16]float32, b [16]float32) [16]float32


// SubPs: Subtract packed single-precision (32-bit) floating-point elements in
// 'b' from packed single-precision (32-bit) floating-point elements in 'a',
// and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := a[i+31:i] - b[i+31:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSUBPS'. Intrinsic: '_mm512_sub_ps'.
// Requires KNCNI.
func SubPs(a M512, b M512) M512 {
	return M512(subPs([16]float32(a), [16]float32(b)))
}

func subPs(a [16]float32, b [16]float32) [16]float32


// MaskSubRoundPd: Subtract packed double-precision (64-bit) floating-point
// elements in 'b' from packed double-precision (64-bit) floating-point
// elements in 'a', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] - b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSUBPD'. Intrinsic: '_mm512_mask_sub_round_pd'.
// Requires KNCNI.
func MaskSubRoundPd(src M512d, k Mmask8, a M512d, b M512d, rounding int) M512d {
	return M512d(maskSubRoundPd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b), rounding))
}

func maskSubRoundPd(src [8]float64, k uint8, a [8]float64, b [8]float64, rounding int) [8]float64


// MaskzSubRoundPd: Subtract packed double-precision (64-bit) floating-point
// elements in 'b' from packed double-precision (64-bit) floating-point
// elements in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] - b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSUBPD'. Intrinsic: '_mm512_maskz_sub_round_pd'.
// Requires AVX512F.
func MaskzSubRoundPd(k Mmask8, a M512d, b M512d, rounding int) M512d {
	return M512d(maskzSubRoundPd(uint8(k), [8]float64(a), [8]float64(b), rounding))
}

func maskzSubRoundPd(k uint8, a [8]float64, b [8]float64, rounding int) [8]float64


// SubRoundPd: Subtract packed double-precision (64-bit) floating-point
// elements in 'b' from packed double-precision (64-bit) floating-point
// elements in 'a', and store the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := a[i+63:i] - b[i+63:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSUBPD'. Intrinsic: '_mm512_sub_round_pd'.
// Requires KNCNI.
func SubRoundPd(a M512d, b M512d, rounding int) M512d {
	return M512d(subRoundPd([8]float64(a), [8]float64(b), rounding))
}

func subRoundPd(a [8]float64, b [8]float64, rounding int) [8]float64


// MaskSubRoundPs: Subtract packed single-precision (32-bit) floating-point
// elements in 'b' from packed single-precision (32-bit) floating-point
// elements in 'a', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] - b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSUBPS'. Intrinsic: '_mm512_mask_sub_round_ps'.
// Requires KNCNI.
func MaskSubRoundPs(src M512, k Mmask16, a M512, b M512, rounding int) M512 {
	return M512(maskSubRoundPs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b), rounding))
}

func maskSubRoundPs(src [16]float32, k uint16, a [16]float32, b [16]float32, rounding int) [16]float32


// MaskzSubRoundPs: Subtract packed single-precision (32-bit) floating-point
// elements in 'b' from packed single-precision (32-bit) floating-point
// elements in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] - b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSUBPS'. Intrinsic: '_mm512_maskz_sub_round_ps'.
// Requires AVX512F.
func MaskzSubRoundPs(k Mmask16, a M512, b M512, rounding int) M512 {
	return M512(maskzSubRoundPs(uint16(k), [16]float32(a), [16]float32(b), rounding))
}

func maskzSubRoundPs(k uint16, a [16]float32, b [16]float32, rounding int) [16]float32


// SubRoundPs: Subtract packed single-precision (32-bit) floating-point
// elements in 'b' from packed single-precision (32-bit) floating-point
// elements in 'a', and store the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := a[i+31:i] - b[i+31:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSUBPS'. Intrinsic: '_mm512_sub_round_ps'.
// Requires KNCNI.
func SubRoundPs(a M512, b M512, rounding int) M512 {
	return M512(subRoundPs([16]float32(a), [16]float32(b), rounding))
}

func subRoundPs(a [16]float32, b [16]float32, rounding int) [16]float32


// MaskSubrEpi32: Performs element-by-element subtraction of packed 32-bit
// integer elements in 'v2' from 'v3' storing the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set) 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := v3[i+31:i] - v2[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSUBRD'. Intrinsic: '_mm512_mask_subr_epi32'.
// Requires KNCNI.
func MaskSubrEpi32(src M512i, k Mmask16, v2 M512i, v3 M512i) M512i {
	return M512i(maskSubrEpi32([64]byte(src), uint16(k), [64]byte(v2), [64]byte(v3)))
}

func maskSubrEpi32(src [64]byte, k uint16, v2 [64]byte, v3 [64]byte) [64]byte


// SubrEpi32: Performs element-by-element subtraction of packed 32-bit integer
// elements in 'v2' from 'v3' storing the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := v3[i+31:i] - v2[i+31:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSUBRD'. Intrinsic: '_mm512_subr_epi32'.
// Requires KNCNI.
func SubrEpi32(v2 M512i, v3 M512i) M512i {
	return M512i(subrEpi32([64]byte(v2), [64]byte(v3)))
}

func subrEpi32(v2 [64]byte, v3 [64]byte) [64]byte


// MaskSubrPd: Performs element-by-element subtraction of packed
// double-precision (64-bit) floating-point elements in 'v2' from 'v3' storing
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := v3[i+63:i] - v2[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSUBRPD'. Intrinsic: '_mm512_mask_subr_pd'.
// Requires KNCNI.
func MaskSubrPd(src M512d, k Mmask8, v2 M512d, v3 M512d) M512d {
	return M512d(maskSubrPd([8]float64(src), uint8(k), [8]float64(v2), [8]float64(v3)))
}

func maskSubrPd(src [8]float64, k uint8, v2 [8]float64, v3 [8]float64) [8]float64


// SubrPd: Performs element-by-element subtraction of packed double-precision
// (64-bit) floating-point elements in 'v2' from 'v3' storing the results in
// 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := v3[i+63:i] - v2[i+63:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSUBRPD'. Intrinsic: '_mm512_subr_pd'.
// Requires KNCNI.
func SubrPd(v2 M512d, v3 M512d) M512d {
	return M512d(subrPd([8]float64(v2), [8]float64(v3)))
}

func subrPd(v2 [8]float64, v3 [8]float64) [8]float64


// MaskSubrPs: Performs element-by-element subtraction of packed
// single-precision (32-bit) floating-point elements in 'v2' from 'v3' storing
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := v3[i+31:i] - v2[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSUBRPS'. Intrinsic: '_mm512_mask_subr_ps'.
// Requires KNCNI.
func MaskSubrPs(src M512, k Mmask16, v2 M512, v3 M512) M512 {
	return M512(maskSubrPs([16]float32(src), uint16(k), [16]float32(v2), [16]float32(v3)))
}

func maskSubrPs(src [16]float32, k uint16, v2 [16]float32, v3 [16]float32) [16]float32


// SubrPs: Performs element-by-element subtraction of packed single-precision
// (32-bit) floating-point elements in 'v2' from 'v3' storing the results in
// 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := v3[i+31:i] - v2[i+31:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSUBRPS'. Intrinsic: '_mm512_subr_ps'.
// Requires KNCNI.
func SubrPs(v2 M512, v3 M512) M512 {
	return M512(subrPs([16]float32(v2), [16]float32(v3)))
}

func subrPs(v2 [16]float32, v3 [16]float32) [16]float32


// MaskSubrRoundPd: Performs element-by-element subtraction of packed
// double-precision (64-bit) floating-point elements in 'v2' from 'v3' storing
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := v3[i+63:i] - v2[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSUBRPD'. Intrinsic: '_mm512_mask_subr_round_pd'.
// Requires KNCNI.
func MaskSubrRoundPd(src M512d, k Mmask8, v2 M512d, v3 M512d, rounding int) M512d {
	return M512d(maskSubrRoundPd([8]float64(src), uint8(k), [8]float64(v2), [8]float64(v3), rounding))
}

func maskSubrRoundPd(src [8]float64, k uint8, v2 [8]float64, v3 [8]float64, rounding int) [8]float64


// SubrRoundPd: Performs element-by-element subtraction of packed
// double-precision (64-bit) floating-point elements in 'v2' from 'v3' storing
// the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := v3[i+63:i] - v2[i+63:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSUBRPD'. Intrinsic: '_mm512_subr_round_pd'.
// Requires KNCNI.
func SubrRoundPd(v2 M512d, v3 M512d, rounding int) M512d {
	return M512d(subrRoundPd([8]float64(v2), [8]float64(v3), rounding))
}

func subrRoundPd(v2 [8]float64, v3 [8]float64, rounding int) [8]float64


// MaskSubrRoundPs: Performs element-by-element subtraction of packed
// single-precision (32-bit) floating-point elements in 'v2' from 'v3' storing
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := v3[i+31:i] - v2[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSUBRPS'. Intrinsic: '_mm512_mask_subr_round_ps'.
// Requires KNCNI.
func MaskSubrRoundPs(src M512, k Mmask16, v2 M512, v3 M512, rounding int) M512 {
	return M512(maskSubrRoundPs([16]float32(src), uint16(k), [16]float32(v2), [16]float32(v3), rounding))
}

func maskSubrRoundPs(src [16]float32, k uint16, v2 [16]float32, v3 [16]float32, rounding int) [16]float32


// SubrRoundPs: Performs element-by-element subtraction of packed
// single-precision (32-bit) floating-point elements in 'v2' from 'v3' storing
// the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := v3[i+31:i] - v2[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSUBRPS'. Intrinsic: '_mm512_subr_round_ps'.
// Requires KNCNI.
func SubrRoundPs(v2 M512, v3 M512, rounding int) M512 {
	return M512(subrRoundPs([16]float32(v2), [16]float32(v3), rounding))
}

func subrRoundPs(v2 [16]float32, v3 [16]float32, rounding int) [16]float32


// MaskSubrsetbEpi32: Performs element-by-element subtraction of packed 32-bit
// integer elements in 'v2' from 'v3', storing the results in 'dst' and 'v2'.
// The borrowed value from the subtraction difference for the nth element is
// written to the nth bit of 'borrow' (borrow flag). Results are written using
// writemask 'k' (elements are copied from 'k' to 'k_old' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				diff := v3[i+31:i] - v2[i+31:i]
//				borrow[j] := Borrow(v3[i+31:i] - v2[i+31:i])
//				dst[i+31:i] := diff
//				v2[i+31:i] := diff
//			ELSE
//				borrow[j] := k_old[j]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSUBRSETBD'. Intrinsic: '_mm512_mask_subrsetb_epi32'.
// Requires KNCNI.
func MaskSubrsetbEpi32(v2 M512i, k Mmask16, k_old Mmask16, v3 M512i, borrow Mmask16) M512i {
	return M512i(maskSubrsetbEpi32([64]byte(v2), uint16(k), uint16(k_old), [64]byte(v3), uint16(borrow)))
}

func maskSubrsetbEpi32(v2 [64]byte, k uint16, k_old uint16, v3 [64]byte, borrow uint16) [64]byte


// SubrsetbEpi32: Performs element-by-element subtraction of packed 32-bit
// integer elements in 'v2' from 'v3', storing the results in 'dst' and 'v2'.
// The borrowed value from the subtraction difference for the nth element is
// written to the nth bit of 'borrow' (borrow flag). 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := v3[i+31:i] - v2[i+31:i]
//			borrow[j] := Borrow(v3[i+31:i] - v2[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSUBRSETBD'. Intrinsic: '_mm512_subrsetb_epi32'.
// Requires KNCNI.
func SubrsetbEpi32(v2 M512i, v3 M512i, borrow Mmask16) M512i {
	return M512i(subrsetbEpi32([64]byte(v2), [64]byte(v3), uint16(borrow)))
}

func subrsetbEpi32(v2 [64]byte, v3 [64]byte, borrow uint16) [64]byte


// MaskSubsEpi16: Subtract packed 16-bit integers in 'b' from packed 16-bit
// integers in 'a' using saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := Saturate_To_Int16(a[i+15:i] - b[i+15:i])
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSUBSW'. Intrinsic: '_mm512_mask_subs_epi16'.
// Requires AVX512BW.
func MaskSubsEpi16(src M512i, k Mmask32, a M512i, b M512i) M512i {
	return M512i(maskSubsEpi16([64]byte(src), uint32(k), [64]byte(a), [64]byte(b)))
}

func maskSubsEpi16(src [64]byte, k uint32, a [64]byte, b [64]byte) [64]byte


// MaskzSubsEpi16: Subtract packed 16-bit integers in 'b' from packed 16-bit
// integers in 'a' using saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := Saturate_To_Int16(a[i+15:i] - b[i+15:i])
//			ELSE
//				dst[i+15:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSUBSW'. Intrinsic: '_mm512_maskz_subs_epi16'.
// Requires AVX512BW.
func MaskzSubsEpi16(k Mmask32, a M512i, b M512i) M512i {
	return M512i(maskzSubsEpi16(uint32(k), [64]byte(a), [64]byte(b)))
}

func maskzSubsEpi16(k uint32, a [64]byte, b [64]byte) [64]byte


// SubsEpi16: Subtract packed 16-bit integers in 'b' from packed 16-bit
// integers in 'a' using saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			dst[i+15:i] := Saturate_To_Int16(a[i+15:i] - b[i+15:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSUBSW'. Intrinsic: '_mm512_subs_epi16'.
// Requires AVX512BW.
func SubsEpi16(a M512i, b M512i) M512i {
	return M512i(subsEpi16([64]byte(a), [64]byte(b)))
}

func subsEpi16(a [64]byte, b [64]byte) [64]byte


// MaskSubsEpi8: Subtract packed 8-bit integers in 'b' from packed 8-bit
// integers in 'a' using saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := Saturate_To_Int8(a[i+7:i] - b[i+7:i])
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSUBSB'. Intrinsic: '_mm512_mask_subs_epi8'.
// Requires AVX512BW.
func MaskSubsEpi8(src M512i, k Mmask64, a M512i, b M512i) M512i {
	return M512i(maskSubsEpi8([64]byte(src), uint64(k), [64]byte(a), [64]byte(b)))
}

func maskSubsEpi8(src [64]byte, k uint64, a [64]byte, b [64]byte) [64]byte


// MaskzSubsEpi8: Subtract packed 8-bit integers in 'b' from packed 8-bit
// integers in 'a' using saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := Saturate_To_Int8(a[i+7:i] - b[i+7:i])
//			ELSE
//				dst[i+7:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSUBSB'. Intrinsic: '_mm512_maskz_subs_epi8'.
// Requires AVX512BW.
func MaskzSubsEpi8(k Mmask64, a M512i, b M512i) M512i {
	return M512i(maskzSubsEpi8(uint64(k), [64]byte(a), [64]byte(b)))
}

func maskzSubsEpi8(k uint64, a [64]byte, b [64]byte) [64]byte


// SubsEpi8: Subtract packed 8-bit integers in 'b' from packed 8-bit integers
// in 'a' using saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 63
//			i := j*8
//			dst[i+7:i] := Saturate_To_Int8(a[i+7:i] - b[i+7:i])	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSUBSB'. Intrinsic: '_mm512_subs_epi8'.
// Requires AVX512BW.
func SubsEpi8(a M512i, b M512i) M512i {
	return M512i(subsEpi8([64]byte(a), [64]byte(b)))
}

func subsEpi8(a [64]byte, b [64]byte) [64]byte


// MaskSubsEpu16: Subtract packed unsigned 16-bit integers in 'b' from packed
// unsigned 16-bit integers in 'a' using saturation, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := Saturate_To_UnsignedInt16(a[i+15:i] - b[i+15:i])
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSUBUSW'. Intrinsic: '_mm512_mask_subs_epu16'.
// Requires AVX512BW.
func MaskSubsEpu16(src M512i, k Mmask32, a M512i, b M512i) M512i {
	return M512i(maskSubsEpu16([64]byte(src), uint32(k), [64]byte(a), [64]byte(b)))
}

func maskSubsEpu16(src [64]byte, k uint32, a [64]byte, b [64]byte) [64]byte


// MaskzSubsEpu16: Subtract packed unsigned 16-bit integers in 'b' from packed
// unsigned 16-bit integers in 'a' using saturation, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := Saturate_To_UnsignedInt16(a[i+15:i] - b[i+15:i])
//			ELSE
//				dst[i+15:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSUBUSW'. Intrinsic: '_mm512_maskz_subs_epu16'.
// Requires AVX512BW.
func MaskzSubsEpu16(k Mmask32, a M512i, b M512i) M512i {
	return M512i(maskzSubsEpu16(uint32(k), [64]byte(a), [64]byte(b)))
}

func maskzSubsEpu16(k uint32, a [64]byte, b [64]byte) [64]byte


// SubsEpu16: Subtract packed unsigned 16-bit integers in 'b' from packed
// unsigned 16-bit integers in 'a' using saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			dst[i+15:i] := Saturate_To_UnsignedInt16(a[i+15:i] - b[i+15:i])	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSUBUSW'. Intrinsic: '_mm512_subs_epu16'.
// Requires AVX512BW.
func SubsEpu16(a M512i, b M512i) M512i {
	return M512i(subsEpu16([64]byte(a), [64]byte(b)))
}

func subsEpu16(a [64]byte, b [64]byte) [64]byte


// MaskSubsEpu8: Subtract packed unsigned 8-bit integers in 'b' from packed
// unsigned 8-bit integers in 'a' using saturation, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := Saturate_To_UnsignedInt8(a[i+7:i] - b[i+7:i])
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSUBUSB'. Intrinsic: '_mm512_mask_subs_epu8'.
// Requires AVX512BW.
func MaskSubsEpu8(src M512i, k Mmask64, a M512i, b M512i) M512i {
	return M512i(maskSubsEpu8([64]byte(src), uint64(k), [64]byte(a), [64]byte(b)))
}

func maskSubsEpu8(src [64]byte, k uint64, a [64]byte, b [64]byte) [64]byte


// MaskzSubsEpu8: Subtract packed unsigned 8-bit integers in 'b' from packed
// unsigned 8-bit integers in 'a' using saturation, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := Saturate_To_UnsignedInt8(a[i+7:i] - b[i+7:i])
//			ELSE
//				dst[i+7:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSUBUSB'. Intrinsic: '_mm512_maskz_subs_epu8'.
// Requires AVX512BW.
func MaskzSubsEpu8(k Mmask64, a M512i, b M512i) M512i {
	return M512i(maskzSubsEpu8(uint64(k), [64]byte(a), [64]byte(b)))
}

func maskzSubsEpu8(k uint64, a [64]byte, b [64]byte) [64]byte


// SubsEpu8: Subtract packed unsigned 8-bit integers in 'b' from packed
// unsigned 8-bit integers in 'a' using saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 63
//			i := j*8
//			dst[i+7:i] := Saturate_To_UnsignedInt8(a[i+7:i] - b[i+7:i])	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSUBUSB'. Intrinsic: '_mm512_subs_epu8'.
// Requires AVX512BW.
func SubsEpu8(a M512i, b M512i) M512i {
	return M512i(subsEpu8([64]byte(a), [64]byte(b)))
}

func subsEpu8(a [64]byte, b [64]byte) [64]byte


// MaskSubsetbEpi32: Performs element-by-element subtraction of packed 32-bit
// integer elements in 'v3' from 'v2', storing the results in 'dst' and the nth
// borrow bit in the nth position of 'borrow' (borrow flag). Results are stored
// using writemask 'k' (elements are copied from 'v2' and 'k_old' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := v2[i+31:i] - v3[i+31:i]
//				borrow[j] := Borrow(v2[i+31:i] - v3[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//				borrow[j] := k_old[j]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSUBSETBD'. Intrinsic: '_mm512_mask_subsetb_epi32'.
// Requires KNCNI.
func MaskSubsetbEpi32(v2 M512i, k Mmask16, k_old Mmask16, v3 M512i, borrow Mmask16) M512i {
	return M512i(maskSubsetbEpi32([64]byte(v2), uint16(k), uint16(k_old), [64]byte(v3), uint16(borrow)))
}

func maskSubsetbEpi32(v2 [64]byte, k uint16, k_old uint16, v3 [64]byte, borrow uint16) [64]byte


// SubsetbEpi32: Performs element-by-element subtraction of packed 32-bit
// integer elements in 'v3' from 'v2', storing the results in 'dst' and the nth
// borrow bit in the nth position of 'borrow' (borrow flag). 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := v2[i+31:i] - v3[i+31:i]
//			borrow[j] := Borrow(v2[i+31:i] - v3[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSUBSETBD'. Intrinsic: '_mm512_subsetb_epi32'.
// Requires KNCNI.
func SubsetbEpi32(v2 M512i, v3 M512i, borrow Mmask16) M512i {
	return M512i(subsetbEpi32([64]byte(v2), [64]byte(v3), uint16(borrow)))
}

func subsetbEpi32(v2 [64]byte, v3 [64]byte, borrow uint16) [64]byte


// MaskSvmlRoundPd: Round the packed double-precision (64-bit) floating-point
// elements in 'a' to the nearest integer value, and store the results as
// packed double-precision floating-point elements in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ROUND(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i] 
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_svml_round_pd'.
// Requires AVX512F.
func MaskSvmlRoundPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskSvmlRoundPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskSvmlRoundPd(src [8]float64, k uint8, a [8]float64) [8]float64


// SvmlRoundPd: Round the packed double-precision (64-bit) floating-point
// elements in 'a' to the nearest integer value, and store the results as
// packed double-precision floating-point elements in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ROUND(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_svml_round_pd'.
// Requires AVX512F.
func SvmlRoundPd(a M512d) M512d {
	return M512d(svmlRoundPd([8]float64(a)))
}

func svmlRoundPd(a [8]float64) [8]float64


// MaskSwizzleEpi32: Performs a swizzle transformation of each of the four
// groups of packed 4x32-bit integer elements in 'v' using swizzle parameter
// 's', storing the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		CASE s OF
//		_MM_SWIZ_REG_NONE:
//			dst[511:0] := v[511:0]
//		_MM_SWIZ_REG_DCBA:
//			dst[511:0] := v[511:0]
//		_MM_SWIZ_REG_CDAB:
//			FOR j := 0 to 7
//				i := j*64
//				IF k[j*2]
//					dst[i+31:i]	:= v[i+63:i+32]
//				ELSE
//					dst[i+31:i]	:= src[i+31:i]
//				FI
//				IF k[j*2+1]
//					dst[i+63:i+32] := v[i+31:i]
//				ELSE
//					dst[i+63:i+32] := src[i+63:i+32]
//				FI
//			ENDFOR
//		_MM_SWIZ_REG_BADC:
//			FOR j := 0 to 3
//				i := j*128
//				IF k[j*4]
//					dst[i+31:i]	 := v[i+95:i+64]
//				ELSE
//					dst[i+31:i]	 := src[i+31:i]
//				FI
//				IF k[j*4+1]
//					dst[i+63:i+32]  := v[i+127:i+96]
//				ELSE
//					dst[i+63:i+32]  := src[i+63:i+32]
//				FI
//				IF k[j*4+2]
//					dst[i+95:i+64]  := v[i+31:i]
//				ELSE
//					dst[i+95:i+64]  := src[i+95:i+64]
//				FI
//				IF k[j*4+3]
//					dst[i+127:i+96] := v[i+63:i+32]
//				ELSE
//					dst[i+127:i+96] := src[i+127:i+96]
//				FI
//			ENDFOR
//		_MM_SWIZ_REG_AAAA:
//			FOR j := 0 to 3
//				i := j*128
//				IF k[j*4]
//					dst[i+31:i]	 := v[i+31:i]
//				ELSE
//					dst[i+31:i]	 := src[i+31:i]
//				FI
//				IF k[j*4+1]
//					dst[i+63:i+32]  := v[i+31:i]
//				ELSE
//					dst[i+63:i+32]  := src[i+63:i+32]
//				FI
//				IF k[j*4+2]
//					dst[i+95:i+64]  := v[i+31:i]
//				ELSE
//					dst[i+95:i+64]  := src[i+95:i+64]
//				FI
//				IF k[j*4+3]
//					dst[i+127:i+96] := v[i+31:i]
//				ELSE
//					dst[i+127:i+96] := src[i+127:i+96]
//				FI
//			ENDFOR
//		_MM_SWIZ_REG_BBBB:
//			FOR j := 0 to 3
//				i := j*128
//				IF k[j*4]
//					dst[i+31:i]	 := v[i+63:i+32]
//				ELSE
//					dst[i+31:i]	 := src[i+31:i]
//				FI
//				IF k[j*4+1]
//					dst[i+63:i+32]  := v[i+63:i+32]
//				ELSE
//					dst[i+63:i+32]  := src[i+63:i+32]
//				FI
//				IF k[j*4+2]
//					dst[i+95:i+64]  := v[i+63:i+32]
//				ELSE
//					dst[i+95:i+64]  := src[i+95:i+64]
//				FI
//				IF k[j*4+3]
//					dst[i+127:i+96] := v[i+63:i+32]
//				ELSE
//					dst[i+127:i+96] := src[i+127:i+96]
//				FI
//			ENDFOR
//		_MM_SWIZ_REG_CCCC:
//			FOR j := 0 to 3
//				i := j*128
//				IF k[j*4]
//					dst[i+31:i]	 := v[i+95:i+64]
//				ELSE
//					dst[i+31:i]	 := src[i+31:i]
//				FI
//				IF k[j*4+1]
//					dst[i+63:i+32]  := v[i+95:i+64]
//				ELSE
//					dst[i+63:i+32]  := src[i+63:i+32]
//				FI
//				IF k[j*4+2]
//					dst[i+95:i+64]  := v[i+95:i+64]
//				ELSE
//					dst[i+95:i+64]  := src[i+95:i+64]
//				FI
//				IF k[j*4+3]
//					dst[i+127:i+96] := v[i+95:i+64]
//				ELSE
//					dst[i+127:i+96] := src[i+127:i+96]
//				FI
//			ENDFOR
//		_MM_SWIZ_REG_DDDD:
//			FOR j := 0 to 3
//				i := j*128
//				IF k[j*4]
//					dst[i+31:i]	 := v[i+127:i+96]
//				ELSE
//					dst[i+31:i]	 := src[i+31:i]
//				FI
//				IF k[j*4+1]
//					dst[i+63:i+32]  := v[i+127:i+96]
//				ELSE
//					dst[i+63:i+32]  := src[i+63:i+32]
//				FI
//				IF k[j*4+2]
//					dst[i+95:i+64]  := v[i+127:i+96]
//				ELSE
//					dst[i+95:i+64]  := src[i+95:i+64]
//				FI
//				IF k[j*4+3]
//					dst[i+127:i+96] := v[i+127:i+96]
//				ELSE
//					dst[i+127:i+96] := src[i+127:i+96]
//				FI
//			ENDFOR
//		_MM_SWIZ_REG_DACB:
//			FOR j := 0 to 3
//				i := j*128
//				IF k[j*4]
//					dst[i+31:i]	 := v[i+63:i+32]
//				ELSE
//					dst[i+31:i]	 := src[i+31:i]
//				FI
//				IF k[j*4+1]
//					dst[i+63:i+32]  := v[i+95:i+64]
//				ELSE
//					dst[i+63:i+32]  := src[i+63:i+32]
//				FI
//				IF k[j*4+2]
//					dst[i+95:i+64]  := v[i+31:i]
//				ELSE
//					dst[i+95:i+64]  := src[i+95:i+64]
//				FI
//				IF k[j*4+3]
//					dst[i+127:i+96] := v[i+127:i+96]
//				ELSE
//					dst[i+127:i+96] := src[i+127:i+96]
//				FI
//			ENDFOR
//		ESAC
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_swizzle_epi32'.
// Requires KNCNI.
func MaskSwizzleEpi32(src M512i, k Mmask16, v M512i, s MMSWIZZLEENUM) M512i {
	return M512i(maskSwizzleEpi32([64]byte(src), uint16(k), [64]byte(v), s))
}

func maskSwizzleEpi32(src [64]byte, k uint16, v [64]byte, s MMSWIZZLEENUM) [64]byte


// SwizzleEpi32: Performs a swizzle transformation of each of the four groups
// of packed 4x 32-bit integer elements in 'v' using swizzle parameter 's',
// storing the results in 'dst'. 
//
//		CASE s OF
//		_MM_SWIZ_REG_NONE:
//			dst[511:0] := v[511:0]
//		_MM_SWIZ_REG_DCBA:
//			dst[511:0] := v[511:0]
//		_MM_SWIZ_REG_CDAB:
//			FOR j := 0 to 7
//				i := j*64
//				dst[i+31:i]    := v[i+63:i+32]
//				dst[i+63:i+32] := v[i+31:i]
//			ENDFOR
//		_MM_SWIZ_REG_BADC:
//			FOR j := 0 to 3
//				i := j*128
//				dst[i+31:i]	    := v[i+95:i+64]
//				dst[i+63:i+32]  := v[i+127:i+96]
//				dst[i+95:i+64]  := v[i+31:i]
//				dst[i+127:i+96] := v[i+63:i+32]
//			ENDFOR
//		_MM_SWIZ_REG_AAAA:
//			FOR j := 0 to 3
//				i := j*128
//				dst[i+31:i]	    := v[i+31:i]
//				dst[i+63:i+32]  := v[i+31:i]
//				dst[i+95:i+64]  := v[i+31:i]
//				dst[i+127:i+96] := v[i+31:i]
//			ENDFOR
//		_MM_SWIZ_REG_BBBB:
//			FOR j := 0 to 3
//				i := j*128
//				dst[i+31:i]	    := v[i+63:i+32]
//				dst[i+63:i+32]  := v[i+63:i+32]
//				dst[i+95:i+64]  := v[i+63:i+32]
//				dst[i+127:i+96] := v[i+63:i+32]
//			ENDFOR
//		_MM_SWIZ_REG_CCCC:
//			FOR j := 0 to 3
//				i := j*128
//				dst[i+31:i]	    := v[i+95:i+64]
//				dst[i+63:i+32]  := v[i+95:i+64]
//				dst[i+95:i+64]  := v[i+95:i+64]
//				dst[i+127:i+96] := v[i+95:i+64]
//			ENDFOR
//		_MM_SWIZ_REG_DDDD:
//			FOR j := 0 to 3
//				i := j*128
//				dst[i+31:i]	    := v[i+127:i+96]
//				dst[i+63:i+32]  := v[i+127:i+96]
//				dst[i+95:i+64]  := v[i+127:i+96]
//				dst[i+127:i+96] := v[i+127:i+96]
//			ENDFOR
//		_MM_SWIZ_REG_DACB:
//			FOR j := 0 to 3
//				i := j*128
//				dst[i+31:i]	    := v[i+63:i+32]
//				dst[i+63:i+32]  := v[i+95:i+64]
//				dst[i+95:i+64]  := v[i+31:i]
//				dst[i+127:i+96] := v[i+127:i+96]
//			ENDFOR
//		ESAC
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_swizzle_epi32'.
// Requires KNCNI.
func SwizzleEpi32(v M512i, s MMSWIZZLEENUM) M512i {
	return M512i(swizzleEpi32([64]byte(v), s))
}

func swizzleEpi32(v [64]byte, s MMSWIZZLEENUM) [64]byte


// MaskSwizzleEpi64: Performs a swizzle transformation of each of the four
// groups of packed 4x64-bit integer elements in 'v' using swizzle parameter
// 's', storing the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		CASE s OF
//		_MM_SWIZ_REG_NONE:
//			dst[511:0] := v[511:0]
//		_MM_SWIZ_REG_DCBA:
//			dst[511:0] := v[511:0]
//		_MM_SWIZ_REG_CDAB:
//			FOR j := 0 to 3
//				i := j*64
//				IF k[j*2]
//					dst[i+63:i]	 := v[i+127:i+64]
//				ELSE
//					dst[i+63:i]	 := src[i+63:i]
//				FI
//				IF k[j*2+1]
//					dst[i+127:i+64] := v[i+63:i]
//				ELSE
//					dst[i+127:i+64] := src[i+127:i+64]
//				FI
//			ENDFOR
//		_MM_SWIZ_REG_BADC:
//			FOR j := 0 to 1
//				i := j*256
//				IF k[j*4]
//					dst[i+63:i]	  := v[i+191:i+128]
//				ELSE
//					dst[i+63:i]	  := src[i+63:i]
//				FI
//				IF k[j*4+1]
//					dst[i+127:i+64]  := v[i+255:i+192]
//				ELSE
//					dst[i+127:i+64]  := src[i+127:i+64]
//				FI
//				IF k[j*4+2]
//					dst[i+191:i+128] := v[i+63:i]
//				ELSE
//					dst[i+191:i+128] := src[i+191:i+128]
//				FI
//				IF k[j*4+3]
//					dst[i+255:i+192] := v[i+127:i+64]
//				ELSE
//					dst[i+255:i+192] := src[i+255:i+192]
//				FI
//			ENDFOR
//		_MM_SWIZ_REG_AAAA:
//			FOR j := 0 to 1
//				i := j*256
//				IF k[j*4]
//					dst[i+63:i]	  := v[i+63:i]
//				ELSE
//					dst[i+63:i]	  := src[i+63:i]
//				FI
//				IF k[j*4+1]
//					dst[i+127:i+64]  := v[i+63:i]
//				ELSE
//					dst[i+127:i+64]  := src[i+127:i+64]
//				FI
//				IF k[j*4+2]
//					dst[i+191:i+128] := v[i+63:i]
//				ELSE
//					dst[i+191:i+128] := src[i+191:i+128]
//				FI
//				IF k[j*4+3]
//					dst[i+255:i+192] := v[i+63:i]
//				ELSE
//					dst[i+255:i+192] := src[i+255:i+192]
//				FI
//			ENDFOR
//		_MM_SWIZ_REG_BBBB:
//			FOR j := 0 to 1
//				i := j*256
//				IF k[j*4]
//					dst[i+63:i]	  := v[i+127:i+63]
//				ELSE
//					dst[i+63:i]	  := src[i+63:i]
//				FI
//				IF k[j*4+1]
//					dst[i+127:i+64]  := v[i+127:i+63]
//				ELSE
//					dst[i+127:i+64]  := src[i+127:i+64]
//				FI
//				IF k[j*4+2]
//					dst[i+191:i+128] := v[i+127:i+63]
//				ELSE
//					dst[i+191:i+128] := src[i+191:i+128]
//				FI
//				IF k[j*4+3]
//					dst[i+255:i+192] := v[i+127:i+63]
//				ELSE
//					dst[i+255:i+192] := src[i+255:i+192]
//				FI
//			ENDFOR
//		_MM_SWIZ_REG_CCCC:
//			FOR j := 0 to 1
//				i := j*256
//				IF k[j*4]
//					dst[i+63:i]	  := v[i+191:i+128]
//				ELSE
//					dst[i+63:i]	  := src[i+63:i]
//				FI
//				IF k[j*4+1]
//					dst[i+127:i+64]  := v[i+191:i+128]
//				ELSE
//					dst[i+127:i+64]  := src[i+127:i+64]
//				FI
//				IF k[j*4+2]
//					dst[i+191:i+128] := v[i+191:i+128]
//				ELSE
//					dst[i+191:i+128] := src[i+191:i+128]
//				FI
//				IF k[j*4+3]
//					dst[i+255:i+192] := v[i+191:i+128]
//				ELSE
//					dst[i+255:i+192] := src[i+255:i+192]
//				FI
//			ENDFOR
//		_MM_SWIZ_REG_DDDD:
//			FOR j := 0 to 1
//				i := j*256
//				IF k[j*4]
//					dst[i+63:i]	  := v[i+255:i+192]
//				ELSE
//					dst[i+63:i]	  := src[i+63:i]
//				FI
//				IF k[j*4+1]
//					dst[i+127:i+64]  := v[i+255:i+192]
//				ELSE
//					dst[i+127:i+64]  := src[i+127:i+64]
//				FI
//				IF k[j*4+2]
//					dst[i+191:i+128] := v[i+255:i+192]
//				ELSE
//					dst[i+191:i+128] := src[i+191:i+128]
//				FI
//				IF k[j*4+3]
//					dst[i+255:i+192] := v[i+255:i+192]
//				ELSE
//					dst[i+255:i+192] := src[i+255:i+192]
//				FI
//			ENDFOR
//		_MM_SWIZ_REG_DACB:
//			FOR j := 0 to 1
//				i := j*256
//				IF k[j*4]
//					dst[i+63:i]	  := v[i+127:i+64]
//				ELSE
//					dst[i+63:i]	  := src[i+63:i]
//				FI
//				IF k[j*4+1]
//					dst[i+127:i+64]  := v[i+191:i+128]
//				ELSE
//					dst[i+127:i+64]  := src[i+127:i+64]
//				FI
//				IF k[j*4+2]
//					dst[i+191:i+128] := v[i+63:i]
//				ELSE
//					dst[i+191:i+128] := src[i+191:i+128]
//				FI
//				IF k[j*4+3]
//					dst[i+255:i+192] := v[i+255:i+192]
//				ELSE
//					dst[i+255:i+192] := src[i+255:i+192]
//				FI
//			ENDFOR
//		ESAC
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_swizzle_epi64'.
// Requires KNCNI.
func MaskSwizzleEpi64(src M512i, k Mmask8, v M512i, s MMSWIZZLEENUM) M512i {
	return M512i(maskSwizzleEpi64([64]byte(src), uint8(k), [64]byte(v), s))
}

func maskSwizzleEpi64(src [64]byte, k uint8, v [64]byte, s MMSWIZZLEENUM) [64]byte


// SwizzleEpi64: Performs a swizzle transformation of each of the two groups of
// packed 4x64-bit integer elements in 'v' using swizzle parameter 's', storing
// the results in 'dst'. 
//
//		CASE s OF
//		_MM_SWIZ_REG_NONE:
//			dst[511:0] := v[511:0]
//		_MM_SWIZ_REG_DCBA:
//			dst[511:0] := v[511:0]
//		_MM_SWIZ_REG_CDAB:
//			FOR j := 0 to 3
//				i := j*64
//				dst[i+63:i]	    := v[i+127:i+64]
//				dst[i+127:i+64] := v[i+63:i]
//			ENDFOR
//		_MM_SWIZ_REG_BADC:
//			FOR j := 0 to 1
//				i := j*256
//				dst[i+63:i]	     := v[i+191:i+128]
//				dst[i+127:i+64]  := v[i+255:i+192]
//				dst[i+191:i+128] := v[i+63:i]
//				dst[i+255:i+192] := v[i+127:i+64]
//			ENDFOR
//		_MM_SWIZ_REG_AAAA:
//			FOR j := 0 to 1
//				i := j*256
//				dst[i+63:i]	     := v[i+63:i]
//				dst[i+127:i+64]  := v[i+63:i]
//				dst[i+191:i+128] := v[i+63:i]
//				dst[i+255:i+192] := v[i+63:i]
//			ENDFOR
//		_MM_SWIZ_REG_BBBB:
//			FOR j := 0 to 1
//				i := j*256
//				dst[i+63:i]	     := v[i+127:i+63]
//				dst[i+127:i+64]  := v[i+127:i+63]
//				dst[i+191:i+128] := v[i+127:i+63]
//				dst[i+255:i+192] := v[i+127:i+63]
//			ENDFOR
//		_MM_SWIZ_REG_CCCC:
//			FOR j := 0 to 1
//				i := j*256
//				dst[i+63:i]	     := v[i+191:i+128]
//				dst[i+127:i+64]  := v[i+191:i+128]
//				dst[i+191:i+128] := v[i+191:i+128]
//				dst[i+255:i+192] := v[i+191:i+128]
//			ENDFOR
//		_MM_SWIZ_REG_DDDD:
//			FOR j := 0 to 1
//				i := j*256
//				dst[i+63:i]	     := v[i+255:i+192]
//				dst[i+127:i+64]  := v[i+255:i+192]
//				dst[i+191:i+128] := v[i+255:i+192]
//				dst[i+255:i+192] := v[i+255:i+192]
//			ENDFOR
//		_MM_SWIZ_REG_DACB:
//			FOR j := 0 to 1
//				i := j*256
//				dst[i+63:i]	     := v[i+127:i+64]
//				dst[i+127:i+64]  := v[i+191:i+128]
//				dst[i+191:i+128] := v[i+63:i]
//				dst[i+255:i+192] := v[i+255:i+192]
//			ENDFOR
//		ESAC
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_swizzle_epi64'.
// Requires KNCNI.
func SwizzleEpi64(v M512i, s MMSWIZZLEENUM) M512i {
	return M512i(swizzleEpi64([64]byte(v), s))
}

func swizzleEpi64(v [64]byte, s MMSWIZZLEENUM) [64]byte


// MaskSwizzlePd: Performs a swizzle transformation of each of the two groups
// of packed 4x double-precision (64-bit) floating-point elements in 'v' using
// swizzle parameter 's', storing the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		CASE s OF
//		_MM_SWIZ_REG_NONE:
//			dst[511:0] := v[511:0]
//		_MM_SWIZ_REG_DCBA:
//			dst[511:0] := v[511:0]
//		_MM_SWIZ_REG_CDAB:
//			FOR j := 0 to 3
//				i := j*64
//				IF k[j*2]
//					dst[i+63:i]	 := v[i+127:i+64]
//				ELSE
//					dst[i+63:i]	 := src[i+63:i]
//				FI
//				IF k[j*2+1]
//					dst[i+127:i+64] := v[i+63:i]
//				ELSE
//					dst[i+127:i+64] := src[i+127:i+64]
//				FI
//			ENDFOR
//		_MM_SWIZ_REG_BADC:
//			FOR j := 0 to 1
//				i := j*256
//				IF k[j*4]
//					dst[i+63:i]	  := v[i+191:i+128]
//				ELSE
//					dst[i+63:i]	  := src[i+63:i]
//				FI
//				IF k[j*4+1]
//					dst[i+127:i+64]  := v[i+255:i+192]
//				ELSE
//					dst[i+127:i+64]  := src[i+127:i+64]
//				FI
//				IF k[j*4+2]
//					dst[i+191:i+128] := v[i+63:i]
//				ELSE
//					dst[i+191:i+128] := src[i+191:i+128]
//				FI
//				IF k[j*4+3]
//					dst[i+255:i+192] := v[i+127:i+64]
//				ELSE
//					dst[i+255:i+192] := src[i+255:i+192]
//				FI
//			ENDFOR
//		_MM_SWIZ_REG_AAAA:
//			FOR j := 0 to 1
//				i := j*256
//				IF k[j*4]
//					dst[i+63:i]	  := v[i+63:i]
//				ELSE
//					dst[i+63:i]	  := src[i+63:i]
//				FI
//				IF k[j*4+1]
//					dst[i+127:i+64]  := v[i+63:i]
//				ELSE
//					dst[i+127:i+64]  := src[i+127:i+64]
//				FI
//				IF k[j*4+2]
//					dst[i+191:i+128] := v[i+63:i]
//				ELSE
//					dst[i+191:i+128] := src[i+191:i+128]
//				FI
//				IF k[j*4+3]
//					dst[i+255:i+192] := v[i+63:i]
//				ELSE
//					dst[i+255:i+192] := src[i+255:i+192]
//				FI
//			ENDFOR
//		_MM_SWIZ_REG_BBBB:
//			FOR j := 0 to 1
//				i := j*256
//				IF k[j*4]
//					dst[i+63:i]	  := v[i+127:i+63]
//				ELSE
//					dst[i+63:i]	  := src[i+63:i]
//				FI
//				IF k[j*4+1]
//					dst[i+127:i+64]  := v[i+127:i+63]
//				ELSE
//					dst[i+127:i+64]  := src[i+127:i+64]
//				FI
//				IF k[j*4+2]
//					dst[i+191:i+128] := v[i+127:i+63]
//				ELSE
//					dst[i+191:i+128] := src[i+191:i+128]
//				FI
//				IF k[j*4+3]
//					dst[i+255:i+192] := v[i+127:i+63]
//				ELSE
//					dst[i+255:i+192] := src[i+255:i+192]
//				FI
//			ENDFOR
//		_MM_SWIZ_REG_CCCC:
//			FOR j := 0 to 1
//				i := j*256
//				IF k[j*4]
//					dst[i+63:i]	  := v[i+191:i+128]
//				ELSE
//					dst[i+63:i]	  := src[i+63:i]
//				FI
//				IF k[j*4+1]
//					dst[i+127:i+64]  := v[i+191:i+128]
//				ELSE
//					dst[i+127:i+64]  := src[i+127:i+64]
//				FI
//				IF k[j*4+2]
//					dst[i+191:i+128] := v[i+191:i+128]
//				ELSE
//					dst[i+191:i+128] := src[i+191:i+128]
//				FI
//				IF k[j*4+3]
//					dst[i+255:i+192] := v[i+191:i+128]
//				ELSE
//					dst[i+255:i+192] := src[i+255:i+192]
//				FI
//			ENDFOR
//		_MM_SWIZ_REG_DDDD:
//			FOR j := 0 to 1
//				i := j*256
//				IF k[j*4]
//					dst[i+63:i]	  := v[i+255:i+192]
//				ELSE
//					dst[i+63:i]	  := src[i+63:i]
//				FI
//				IF k[j*4+1]
//					dst[i+127:i+64]  := v[i+255:i+192]
//				ELSE
//					dst[i+127:i+64]  := src[i+127:i+64]
//				FI
//				IF k[j*4+2]
//					dst[i+191:i+128] := v[i+255:i+192]
//				ELSE
//					dst[i+191:i+128] := src[i+191:i+128]
//				FI
//				IF k[j*4+3]
//					dst[i+255:i+192] := v[i+255:i+192]
//				ELSE
//					dst[i+255:i+192] := src[i+255:i+192]
//				FI
//			ENDFOR
//		_MM_SWIZ_REG_DACB:
//			FOR j := 0 to 1
//				i := j*256
//				IF k[j*4]
//					dst[i+63:i]	  := v[i+127:i+64]
//				ELSE
//					dst[i+63:i]	  := src[i+63:i]
//				FI
//				IF k[j*4+1]
//					dst[i+127:i+64]  := v[i+191:i+128]
//				ELSE
//					dst[i+127:i+64]  := src[i+127:i+64]
//				FI
//				IF k[j*4+2]
//					dst[i+191:i+128] := v[i+63:i]
//				ELSE
//					dst[i+191:i+128] := src[i+191:i+128]
//				FI
//				IF k[j*4+3]
//					dst[i+255:i+192] := v[i+255:i+192]
//				ELSE
//					dst[i+255:i+192] := src[i+255:i+192]
//				FI
//			ENDFOR
//		ESAC
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_swizzle_pd'.
// Requires KNCNI.
func MaskSwizzlePd(src M512d, k Mmask8, v M512d, s MMSWIZZLEENUM) M512d {
	return M512d(maskSwizzlePd([8]float64(src), uint8(k), [8]float64(v), s))
}

func maskSwizzlePd(src [8]float64, k uint8, v [8]float64, s MMSWIZZLEENUM) [8]float64


// SwizzlePd: Performs a swizzle transformation of each of the two groups of
// packed 4x double-precision (64-bit) floating-point elements in 'v' using
// swizzle parameter 's', storing the results in 'dst'. 
//
//		CASE s OF
//		_MM_SWIZ_REG_NONE:
//			dst[511:0] := v[511:0]
//		_MM_SWIZ_REG_DCBA:
//			dst[511:0] := v[511:0]
//		_MM_SWIZ_REG_CDAB:
//			FOR j := 0 to 3
//				i := j*64
//				dst[i+63:i]     := v[i+127:i+64]
//				dst[i+127:i+64] := v[i+63:i]
//			ENDFOR
//		_MM_SWIZ_REG_BADC:
//			FOR j := 0 to 1
//				i := j*256
//				dst[i+63:i]      := v[i+191:i+128]
//				dst[i+127:i+64]  := v[i+255:i+192]
//				dst[i+191:i+128] := v[i+63:i]
//				dst[i+255:i+192] := v[i+127:i+64]
//			ENDFOR
//		_MM_SWIZ_REG_AAAA:
//			FOR j := 0 to 1
//				i := j*256
//				dst[i+63:i]      := v[i+63:i]
//				dst[i+127:i+64]  := v[i+63:i]
//				dst[i+191:i+128] := v[i+63:i]
//				dst[i+255:i+192] := v[i+63:i]
//			ENDFOR
//		_MM_SWIZ_REG_BBBB:
//			FOR j := 0 to 1
//				i := j*256
//				dst[i+63:i]      := v[i+127:i+63]
//				dst[i+127:i+64]  := v[i+127:i+63]
//				dst[i+191:i+128] := v[i+127:i+63]
//				dst[i+255:i+192] := v[i+127:i+63]
//			ENDFOR
//		_MM_SWIZ_REG_CCCC:
//			FOR j := 0 to 1
//				i := j*256
//				dst[i+63:i]      := v[i+191:i+128]
//				dst[i+127:i+64]  := v[i+191:i+128]
//				dst[i+191:i+128] := v[i+191:i+128]
//				dst[i+255:i+192] := v[i+191:i+128]
//			ENDFOR
//		_MM_SWIZ_REG_DDDD:
//			FOR j := 0 to 1
//				i := j*256
//				dst[i+63:i]	     := v[i+255:i+192]
//				dst[i+127:i+64]  := v[i+255:i+192]
//				dst[i+191:i+128] := v[i+255:i+192]
//				dst[i+255:i+192] := v[i+255:i+192]
//			ENDFOR
//		_MM_SWIZ_REG_DACB:
//			FOR j := 0 to 1
//				i := j*256
//				dst[i+63:i]	     := v[i+127:i+64]
//				dst[i+127:i+64]  := v[i+191:i+128]
//				dst[i+191:i+128] := v[i+63:i]
//				dst[i+255:i+192] := v[i+255:i+192]
//			ENDFOR
//		ESAC
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_swizzle_pd'.
// Requires KNCNI.
func SwizzlePd(v M512d, s MMSWIZZLEENUM) M512d {
	return M512d(swizzlePd([8]float64(v), s))
}

func swizzlePd(v [8]float64, s MMSWIZZLEENUM) [8]float64


// MaskSwizzlePs: Performs a swizzle transformation of each of the four groups
// of packed 4x single-precision (32-bit) floating-point elements in 'v' using
// swizzle parameter 's', storing the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		CASE s OF
//		_MM_SWIZ_REG_NONE:
//			dst[511:0] := v[511:0]
//		_MM_SWIZ_REG_DCBA:
//			dst[511:0] := v[511:0]
//		_MM_SWIZ_REG_CDAB:
//			FOR j := 0 to 7
//				i := j*64
//				IF k[j*2]
//					dst[i+31:i]	:= v[i+63:i+32]
//				ELSE
//					dst[i+31:i]	:= src[i+31:i]
//				FI
//				IF k[j*2+1]
//					dst[i+63:i+32] := v[i+31:i]
//				ELSE
//					dst[i+63:i+32] := src[i+63:i+32]
//				FI
//			ENDFOR
//		_MM_SWIZ_REG_BADC:
//			FOR j := 0 to 3
//				i := j*128
//				IF k[j*4]
//					dst[i+31:i]	 := v[i+95:i+64]
//				ELSE
//					dst[i+31:i]	 := src[i+31:i]
//				FI
//				IF k[j*4+1]
//					dst[i+63:i+32]  := v[i+127:i+96]
//				ELSE
//					dst[i+63:i+32]  := src[i+63:i+32]
//				FI
//				IF k[j*4+2]
//					dst[i+95:i+64]  := v[i+31:i]
//				ELSE
//					dst[i+95:i+64]  := src[i+95:i+64]
//				FI
//				IF k[j*4+3]
//					dst[i+127:i+96] := v[i+63:i+32]
//				ELSE
//					dst[i+127:i+96] := src[i+127:i+96]
//				FI
//			ENDFOR
//		_MM_SWIZ_REG_AAAA:
//			FOR j := 0 to 3
//				i := j*128
//				IF k[j*4]
//					dst[i+31:i]	 := v[i+31:i]
//				ELSE
//					dst[i+31:i]	 := src[i+31:i]
//				FI
//				IF k[j*4+1]
//					dst[i+63:i+32]  := v[i+31:i]
//				ELSE
//					dst[i+63:i+32]  := src[i+63:i+32]
//				FI
//				IF k[j*4+2]
//					dst[i+95:i+64]  := v[i+31:i]
//				ELSE
//					dst[i+95:i+64]  := src[i+95:i+64]
//				FI
//				IF k[j*4+3]
//					dst[i+127:i+96] := v[i+31:i]
//				ELSE
//					dst[i+127:i+96] := src[i+127:i+96]
//				FI
//			ENDFOR
//		_MM_SWIZ_REG_BBBB:
//			FOR j := 0 to 3
//				i := j*128
//				IF k[j*4]
//					dst[i+31:i]	 := v[i+63:i+32]
//				ELSE
//					dst[i+31:i]	 := src[i+31:i]
//				FI
//				IF k[j*4+1]
//					dst[i+63:i+32]  := v[i+63:i+32]
//				ELSE
//					dst[i+63:i+32]  := src[i+63:i+32]
//				FI
//				IF k[j*4+2]
//					dst[i+95:i+64]  := v[i+63:i+32]
//				ELSE
//					dst[i+95:i+64]  := src[i+95:i+64]
//				FI
//				IF k[j*4+3]
//					dst[i+127:i+96] := v[i+63:i+32]
//				ELSE
//					dst[i+127:i+96] := src[i+127:i+96]
//				FI
//			ENDFOR
//		_MM_SWIZ_REG_CCCC:
//			FOR j := 0 to 3
//				i := j*128
//				IF k[j*4]
//					dst[i+31:i]	 := v[i+95:i+64]
//				ELSE
//					dst[i+31:i]	 := src[i+31:i]
//				FI
//				IF k[j*4+1]
//					dst[i+63:i+32]  := v[i+95:i+64]
//				ELSE
//					dst[i+63:i+32]  := src[i+63:i+32]
//				FI
//				IF k[j*4+2]
//					dst[i+95:i+64]  := v[i+95:i+64]
//				ELSE
//					dst[i+95:i+64]  := src[i+95:i+64]
//				FI
//				IF k[j*4+3]
//					dst[i+127:i+96] := v[i+95:i+64]
//				ELSE
//					dst[i+127:i+96] := src[i+127:i+96]
//				FI
//			ENDFOR
//		_MM_SWIZ_REG_DDDD:
//			FOR j := 0 to 3
//				i := j*128
//				IF k[j*4]
//					dst[i+31:i]	 := v[i+127:i+96]
//				ELSE
//					dst[i+31:i]	 := src[i+31:i]
//				FI
//				IF k[j*4+1]
//					dst[i+63:i+32]  := v[i+127:i+96]
//				ELSE
//					dst[i+63:i+32]  := src[i+63:i+32]
//				FI
//				IF k[j*4+2]
//					dst[i+95:i+64]  := v[i+127:i+96]
//				ELSE
//					dst[i+95:i+64]  := src[i+95:i+64]
//				FI
//				IF k[j*4+3]
//					dst[i+127:i+96] := v[i+127:i+96]
//				ELSE
//					dst[i+127:i+96] := src[i+127:i+96]
//				FI
//			ENDFOR
//		_MM_SWIZ_REG_DACB:
//			FOR j := 0 to 3
//				i := j*128
//				IF k[j*4]
//					dst[i+31:i]	 := v[i+63:i+32]
//				ELSE
//					dst[i+31:i]	 := src[i+31:i]
//				FI
//				IF k[j*4+1]
//					dst[i+63:i+32]  := v[i+95:i+64]
//				ELSE
//					dst[i+63:i+32]  := src[i+63:i+32]
//				FI
//				IF k[j*4+2]
//					dst[i+95:i+64]  := v[i+31:i]
//				ELSE
//					dst[i+95:i+64]  := src[i+95:i+64]
//				FI
//				IF k[j*4+3]
//					dst[i+127:i+96] := v[i+127:i+96]
//				ELSE
//					dst[i+127:i+96] := src[i+127:i+96]
//				FI
//			ENDFOR
//		ESAC
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_swizzle_ps'.
// Requires KNCNI.
func MaskSwizzlePs(src M512, k Mmask16, v M512, s MMSWIZZLEENUM) M512 {
	return M512(maskSwizzlePs([16]float32(src), uint16(k), [16]float32(v), s))
}

func maskSwizzlePs(src [16]float32, k uint16, v [16]float32, s MMSWIZZLEENUM) [16]float32


// SwizzlePs: Performs a swizzle transformation of each of the four groups of
// packed 4xsingle-precision (32-bit) floating-point elements in 'v' using
// swizzle parameter 's', storing the results in 'dst'. 
//
//		CASE s OF
//		_MM_SWIZ_REG_NONE:
//			dst[511:0] := v[511:0]
//		_MM_SWIZ_REG_DCBA:
//			dst[511:0] := v[511:0]
//		_MM_SWIZ_REG_CDAB:
//			FOR j := 0 to 7
//				i := j*64
//				dst[i+31:i]    := v[i+63:i+32]
//				dst[i+63:i+32] := v[i+31:i]
//			ENDFOR
//		_MM_SWIZ_REG_BADC:
//			FOR j := 0 to 3
//				i := j*128
//				dst[i+31:i]     := v[i+95:i+64]
//				dst[i+63:i+32]  := v[i+127:i+96]
//				dst[i+95:i+64]  := v[i+31:i]
//				dst[i+127:i+96] := v[i+63:i+32]
//			ENDFOR
//		_MM_SWIZ_REG_AAAA:
//			FOR j := 0 to 3
//				i := j*128
//				dst[i+31:i]     := v[i+31:i]
//				dst[i+63:i+32]  := v[i+31:i]
//				dst[i+95:i+64]  := v[i+31:i]
//				dst[i+127:i+96] := v[i+31:i]
//			ENDFOR
//		_MM_SWIZ_REG_BBBB:
//			FOR j := 0 to 3
//				i := j*128
//				dst[i+31:i]     := v[i+63:i+32]
//				dst[i+63:i+32]  := v[i+63:i+32]
//				dst[i+95:i+64]  := v[i+63:i+32]
//				dst[i+127:i+96] := v[i+63:i+32]
//			ENDFOR
//		_MM_SWIZ_REG_CCCC:
//			FOR j := 0 to 3
//				i := j*128
//				dst[i+31:i]     := v[i+95:i+64]
//				dst[i+63:i+32]  := v[i+95:i+64]
//				dst[i+95:i+64]  := v[i+95:i+64]
//				dst[i+127:i+96] := v[i+95:i+64]
//			ENDFOR
//		_MM_SWIZ_REG_DDDD:
//			FOR j := 0 to 3
//				i := j*128
//				dst[i+31:i]     := v[i+127:i+96]
//				dst[i+63:i+32]  := v[i+127:i+96]
//				dst[i+95:i+64]  := v[i+127:i+96]
//				dst[i+127:i+96] := v[i+127:i+96]
//			ENDFOR
//		_MM_SWIZ_REG_DACB:
//			FOR j := 0 to 3
//				i := j*128
//				dst[i+31:i]     := v[i+63:i+32]
//				dst[i+63:i+32]  := v[i+95:i+64]
//				dst[i+95:i+64]  := v[i+31:i]
//				dst[i+127:i+96] := v[i+127:i+96]
//			ENDFOR
//		ESAC
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_swizzle_ps'.
// Requires KNCNI.
func SwizzlePs(v M512, s MMSWIZZLEENUM) M512 {
	return M512(swizzlePs([16]float32(v), s))
}

func swizzlePs(v [16]float32, s MMSWIZZLEENUM) [16]float32


// MaskTanPd: Compute the tangent of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := TAN(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_tan_pd'.
// Requires AVX512F.
func MaskTanPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskTanPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskTanPd(src [8]float64, k uint8, a [8]float64) [8]float64


// TanPd: Compute the tangent of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := TAN(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_tan_pd'.
// Requires AVX512F.
func TanPd(a M512d) M512d {
	return M512d(tanPd([8]float64(a)))
}

func tanPd(a [8]float64) [8]float64


// MaskTanPs: Compute the tangent of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := TAN(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_tan_ps'.
// Requires AVX512F.
func MaskTanPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskTanPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskTanPs(src [16]float32, k uint16, a [16]float32) [16]float32


// TanPs: Compute the tangent of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := TAN(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_tan_ps'.
// Requires AVX512F.
func TanPs(a M512) M512 {
	return M512(tanPs([16]float32(a)))
}

func tanPs(a [16]float32) [16]float32


// MaskTandPd: Compute the tangent of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in degrees, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := TAND(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_tand_pd'.
// Requires AVX512F.
func MaskTandPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskTandPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskTandPd(src [8]float64, k uint8, a [8]float64) [8]float64


// TandPd: Compute the tangent of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in degrees, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := TAND(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_tand_pd'.
// Requires AVX512F.
func TandPd(a M512d) M512d {
	return M512d(tandPd([8]float64(a)))
}

func tandPd(a [8]float64) [8]float64


// MaskTandPs: Compute the tangent of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in degrees, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := TAND(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_tand_ps'.
// Requires AVX512F.
func MaskTandPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskTandPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskTandPs(src [16]float32, k uint16, a [16]float32) [16]float32


// TandPs: Compute the tangent of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in degrees, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := TAND(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_tand_ps'.
// Requires AVX512F.
func TandPs(a M512) M512 {
	return M512(tandPs([16]float32(a)))
}

func tandPs(a [16]float32) [16]float32


// MaskTanhPd: Compute the hyperbolic tangent of packed double-precision
// (64-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := TANH(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_tanh_pd'.
// Requires AVX512F.
func MaskTanhPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskTanhPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskTanhPd(src [8]float64, k uint8, a [8]float64) [8]float64


// TanhPd: Compute the hyperbolic tangent of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := TANH(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_tanh_pd'.
// Requires AVX512F.
func TanhPd(a M512d) M512d {
	return M512d(tanhPd([8]float64(a)))
}

func tanhPd(a [8]float64) [8]float64


// MaskTanhPs: Compute the hyperbolic tangent of packed single-precision
// (32-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := TANH(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_tanh_ps'.
// Requires AVX512F.
func MaskTanhPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskTanhPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskTanhPs(src [16]float32, k uint16, a [16]float32) [16]float32


// TanhPs: Compute the hyperbolic tangent of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := TANH(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_tanh_ps'.
// Requires AVX512F.
func TanhPs(a M512) M512 {
	return M512(tanhPs([16]float32(a)))
}

func tanhPs(a [16]float32) [16]float32


// MaskTernarylogicEpi32: Bitwise ternary logic that provides the capability to
// implement any three-operand binary function; the specific binary function is
// specified by value in 'imm8'. For each bit in each packed 32-bit integer,
// the corresponding bit from 'src', 'a', and 'b' are used to form a 3 bit
// index into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst' using writemask 'k' at 32-bit granularity (32-bit
// elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				FOR h := 0 to 31
//					index[2:0] := (src[i+h] << 2) OR (a[i+h] << 1) OR b[i+h]
//					dst[i+h] := imm8[index[2:0]]
//				ENDFOR
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPTERNLOGD'. Intrinsic: '_mm512_mask_ternarylogic_epi32'.
// Requires AVX512F.
func MaskTernarylogicEpi32(src M512i, k Mmask16, a M512i, b M512i, imm8 int) M512i {
	return M512i(maskTernarylogicEpi32([64]byte(src), uint16(k), [64]byte(a), [64]byte(b), imm8))
}

func maskTernarylogicEpi32(src [64]byte, k uint16, a [64]byte, b [64]byte, imm8 int) [64]byte


// MaskzTernarylogicEpi32: Bitwise ternary logic that provides the capability
// to implement any three-operand binary function; the specific binary function
// is specified by value in 'imm8'. For each bit in each packed 32-bit integer,
// the corresponding bit from 'a', 'b', and 'c' are used to form a 3 bit index
// into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst' using zeromask 'k' at 32-bit granularity (32-bit
// elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				FOR h := 0 to 31
//					index[2:0] := (a[i+h] << 2) OR (b[i+h] << 1) OR c[i+h]
//					dst[i+h] := imm8[index[2:0]]
//				ENDFOR
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPTERNLOGD'. Intrinsic: '_mm512_maskz_ternarylogic_epi32'.
// Requires AVX512F.
func MaskzTernarylogicEpi32(k Mmask16, a M512i, b M512i, c M512i, imm8 int) M512i {
	return M512i(maskzTernarylogicEpi32(uint16(k), [64]byte(a), [64]byte(b), [64]byte(c), imm8))
}

func maskzTernarylogicEpi32(k uint16, a [64]byte, b [64]byte, c [64]byte, imm8 int) [64]byte


// TernarylogicEpi32: Bitwise ternary logic that provides the capability to
// implement any three-operand binary function; the specific binary function is
// specified by value in 'imm8'. For each bit in each packed 32-bit integer,
// the corresponding bit from 'a', 'b', and 'c' are used to form a 3 bit index
// into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			FOR h := 0 to 31
//				index[2:0] := (a[i+h] << 2) OR (b[i+h] << 1) OR c[i+h]
//				dst[i+h] := imm8[index[2:0]]
//			ENDFOR
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPTERNLOGD'. Intrinsic: '_mm512_ternarylogic_epi32'.
// Requires AVX512F.
func TernarylogicEpi32(a M512i, b M512i, c M512i, imm8 int) M512i {
	return M512i(ternarylogicEpi32([64]byte(a), [64]byte(b), [64]byte(c), imm8))
}

func ternarylogicEpi32(a [64]byte, b [64]byte, c [64]byte, imm8 int) [64]byte


// MaskTernarylogicEpi64: Bitwise ternary logic that provides the capability to
// implement any three-operand binary function; the specific binary function is
// specified by value in 'imm8'. For each bit in each packed 64-bit integer,
// the corresponding bit from 'src', 'a', and 'b' are used to form a 3 bit
// index into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst' using writemask 'k' at 64-bit granularity (64-bit
// elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				FOR h := 0 to 63
//					index[2:0] := (src[i+h] << 2) OR (a[i+h] << 1) OR b[i+h]
//					dst[i+h] := imm8[index[2:0]]
//				ENDFOR
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPTERNLOGQ'. Intrinsic: '_mm512_mask_ternarylogic_epi64'.
// Requires AVX512F.
func MaskTernarylogicEpi64(src M512i, k Mmask8, a M512i, b M512i, imm8 int) M512i {
	return M512i(maskTernarylogicEpi64([64]byte(src), uint8(k), [64]byte(a), [64]byte(b), imm8))
}

func maskTernarylogicEpi64(src [64]byte, k uint8, a [64]byte, b [64]byte, imm8 int) [64]byte


// MaskzTernarylogicEpi64: Bitwise ternary logic that provides the capability
// to implement any three-operand binary function; the specific binary function
// is specified by value in 'imm8'. For each bit in each packed 64-bit integer,
// the corresponding bit from 'a', 'b', and 'c' are used to form a 3 bit index
// into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst' using zeromask 'k' at 64-bit granularity (64-bit
// elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				FOR h := 0 to 63
//					index[2:0] := (a[i+h] << 2) OR (b[i+h] << 1) OR c[i+h]
//					dst[i+h] := imm8[index[2:0]]
//				ENDFOR
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPTERNLOGQ'. Intrinsic: '_mm512_maskz_ternarylogic_epi64'.
// Requires AVX512F.
func MaskzTernarylogicEpi64(k Mmask8, a M512i, b M512i, c M512i, imm8 int) M512i {
	return M512i(maskzTernarylogicEpi64(uint8(k), [64]byte(a), [64]byte(b), [64]byte(c), imm8))
}

func maskzTernarylogicEpi64(k uint8, a [64]byte, b [64]byte, c [64]byte, imm8 int) [64]byte


// TernarylogicEpi64: Bitwise ternary logic that provides the capability to
// implement any three-operand binary function; the specific binary function is
// specified by value in 'imm8'. For each bit in each packed 64-bit integer,
// the corresponding bit from 'a', 'b', and 'c' are used to form a 3 bit index
// into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			FOR h := 0 to 63
//				index[2:0] := (a[i+h] << 2) OR (b[i+h] << 1) OR c[i+h]
//				dst[i+h] := imm8[index[2:0]]
//			ENDFOR
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPTERNLOGQ'. Intrinsic: '_mm512_ternarylogic_epi64'.
// Requires AVX512F.
func TernarylogicEpi64(a M512i, b M512i, c M512i, imm8 int) M512i {
	return M512i(ternarylogicEpi64([64]byte(a), [64]byte(b), [64]byte(c), imm8))
}

func ternarylogicEpi64(a [64]byte, b [64]byte, c [64]byte, imm8 int) [64]byte


// MaskTestEpi16Mask: Compute the bitwise AND of packed 16-bit integers in 'a'
// and 'b', producing intermediate 16-bit values, and set the corresponding bit
// in result mask 'k' (subject to writemask 'k') if the intermediate value is
// non-zero. 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k1[j]
//				k[j] := ((a[i+15:i] AND b[i+15:i]) != 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPTESTMW'. Intrinsic: '_mm512_mask_test_epi16_mask'.
// Requires AVX512BW.
func MaskTestEpi16Mask(k1 Mmask32, a M512i, b M512i) Mmask32 {
	return Mmask32(maskTestEpi16Mask(uint32(k1), [64]byte(a), [64]byte(b)))
}

func maskTestEpi16Mask(k1 uint32, a [64]byte, b [64]byte) uint32


// TestEpi16Mask: Compute the bitwise AND of packed 16-bit integers in 'a' and
// 'b', producing intermediate 16-bit values, and set the corresponding bit in
// result mask 'k' if the intermediate value is non-zero. 
//
//		FOR j := 0 to 31
//			i := j*16
//			k[j] := ((a[i+15:i] AND b[i+15:i]) != 0) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPTESTMW'. Intrinsic: '_mm512_test_epi16_mask'.
// Requires AVX512BW.
func TestEpi16Mask(a M512i, b M512i) Mmask32 {
	return Mmask32(testEpi16Mask([64]byte(a), [64]byte(b)))
}

func testEpi16Mask(a [64]byte, b [64]byte) uint32


// MaskTestEpi32Mask: Compute the bitwise AND of packed 32-bit integers in 'a'
// and 'b', producing intermediate 32-bit values, and set the corresponding bit
// in result mask 'k' (subject to writemask 'k') if the intermediate value is
// non-zero. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k1[j]
//				k[j] := ((a[i+31:i] AND b[i+31:i]) != 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPTESTMD'. Intrinsic: '_mm512_mask_test_epi32_mask'.
// Requires KNCNI.
func MaskTestEpi32Mask(k1 Mmask16, a M512i, b M512i) Mmask16 {
	return Mmask16(maskTestEpi32Mask(uint16(k1), [64]byte(a), [64]byte(b)))
}

func maskTestEpi32Mask(k1 uint16, a [64]byte, b [64]byte) uint16


// TestEpi32Mask: Compute the bitwise AND of packed 32-bit integers in 'a' and
// 'b', producing intermediate 32-bit values, and set the corresponding bit in
// result mask 'k' if the intermediate value is non-zero. 
//
//		FOR j := 0 to 15
//			i := j*32
//			k[j] := ((a[i+31:i] AND b[i+31:i]) != 0) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPTESTMD'. Intrinsic: '_mm512_test_epi32_mask'.
// Requires KNCNI.
func TestEpi32Mask(a M512i, b M512i) Mmask16 {
	return Mmask16(testEpi32Mask([64]byte(a), [64]byte(b)))
}

func testEpi32Mask(a [64]byte, b [64]byte) uint16


// MaskTestEpi64Mask: Compute the bitwise AND of packed 64-bit integers in 'a'
// and 'b', producing intermediate 64-bit values, and set the corresponding bit
// in result mask 'k' (subject to writemask 'k') if the intermediate value is
// non-zero. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ((a[i+63:i] AND b[i+63:i]) != 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPTESTMQ'. Intrinsic: '_mm512_mask_test_epi64_mask'.
// Requires AVX512F.
func MaskTestEpi64Mask(k1 Mmask8, a M512i, b M512i) Mmask8 {
	return Mmask8(maskTestEpi64Mask(uint8(k1), [64]byte(a), [64]byte(b)))
}

func maskTestEpi64Mask(k1 uint8, a [64]byte, b [64]byte) uint8


// TestEpi64Mask: Compute the bitwise AND of packed 64-bit integers in 'a' and
// 'b', producing intermediate 64-bit values, and set the corresponding bit in
// result mask 'k' if the intermediate value is non-zero. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ((a[i+63:i] AND b[i+63:i]) != 0) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPTESTMQ'. Intrinsic: '_mm512_test_epi64_mask'.
// Requires AVX512F.
func TestEpi64Mask(a M512i, b M512i) Mmask8 {
	return Mmask8(testEpi64Mask([64]byte(a), [64]byte(b)))
}

func testEpi64Mask(a [64]byte, b [64]byte) uint8


// MaskTestEpi8Mask: Compute the bitwise AND of packed 8-bit integers in 'a'
// and 'b', producing intermediate 8-bit values, and set the corresponding bit
// in result mask 'k' (subject to writemask 'k') if the intermediate value is
// non-zero. 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k1[j]
//				k[j] := ((a[i+7:i] AND b[i+7:i]) != 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:64] := 0
//
// Instruction: 'VPTESTMB'. Intrinsic: '_mm512_mask_test_epi8_mask'.
// Requires AVX512BW.
func MaskTestEpi8Mask(k1 Mmask64, a M512i, b M512i) Mmask64 {
	return Mmask64(maskTestEpi8Mask(uint64(k1), [64]byte(a), [64]byte(b)))
}

func maskTestEpi8Mask(k1 uint64, a [64]byte, b [64]byte) uint64


// TestEpi8Mask: Compute the bitwise AND of packed 8-bit integers in 'a' and
// 'b', producing intermediate 8-bit values, and set the corresponding bit in
// result mask 'k' if the intermediate value is non-zero. 
//
//		FOR j := 0 to 63
//			i := j*8
//			k[j] := ((a[i+7:i] AND b[i+7:i]) != 0) ? 1 : 0
//		ENDFOR
//		k[MAX:64] := 0
//
// Instruction: 'VPTESTMB'. Intrinsic: '_mm512_test_epi8_mask'.
// Requires AVX512BW.
func TestEpi8Mask(a M512i, b M512i) Mmask64 {
	return Mmask64(testEpi8Mask([64]byte(a), [64]byte(b)))
}

func testEpi8Mask(a [64]byte, b [64]byte) uint64


// MaskTestnEpi16Mask: Compute the bitwise NAND of packed 16-bit integers in
// 'a' and 'b', producing intermediate 16-bit values, and set the corresponding
// bit in result mask 'k' (subject to writemask 'k') if the intermediate value
// is zero. 
//
//		FOR j := 0 to 31
//			i := j*16
//			IF k1[j]
//				k[j] := ((a[i+15:i] AND b[i+15:i]) == 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPTESTNMW'. Intrinsic: '_mm512_mask_testn_epi16_mask'.
// Requires AVX512BW.
func MaskTestnEpi16Mask(k1 Mmask32, a M512i, b M512i) Mmask32 {
	return Mmask32(maskTestnEpi16Mask(uint32(k1), [64]byte(a), [64]byte(b)))
}

func maskTestnEpi16Mask(k1 uint32, a [64]byte, b [64]byte) uint32


// TestnEpi16Mask: Compute the bitwise NAND of packed 16-bit integers in 'a'
// and 'b', producing intermediate 16-bit values, and set the corresponding bit
// in result mask 'k' if the intermediate value is zero. 
//
//		FOR j := 0 to 31
//			i := j*16
//			k[j] := ((a[i+15:i] AND b[i+15:i]) == 0) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPTESTNMW'. Intrinsic: '_mm512_testn_epi16_mask'.
// Requires AVX512BW.
func TestnEpi16Mask(a M512i, b M512i) Mmask32 {
	return Mmask32(testnEpi16Mask([64]byte(a), [64]byte(b)))
}

func testnEpi16Mask(a [64]byte, b [64]byte) uint32


// MaskTestnEpi32Mask: Compute the bitwise NAND of packed 32-bit integers in
// 'a' and 'b', producing intermediate 32-bit values, and set the corresponding
// bit in result mask 'k' (subject to writemask 'k') if the intermediate value
// is zero. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k1[j]
//				k[j] := ((a[i+31:i] AND b[i+31:i]) == 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPTESTNMD'. Intrinsic: '_mm512_mask_testn_epi32_mask'.
// Requires AVX512F.
func MaskTestnEpi32Mask(k1 Mmask16, a M512i, b M512i) Mmask16 {
	return Mmask16(maskTestnEpi32Mask(uint16(k1), [64]byte(a), [64]byte(b)))
}

func maskTestnEpi32Mask(k1 uint16, a [64]byte, b [64]byte) uint16


// TestnEpi32Mask: Compute the bitwise NAND of packed 32-bit integers in 'a'
// and 'b', producing intermediate 32-bit values, and set the corresponding bit
// in result mask 'k' if the intermediate value is zero. 
//
//		FOR j := 0 to 15
//			i := j*32
//			k[j] := ((a[i+31:i] AND b[i+31:i]) == 0) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPTESTNMD'. Intrinsic: '_mm512_testn_epi32_mask'.
// Requires AVX512F.
func TestnEpi32Mask(a M512i, b M512i) Mmask16 {
	return Mmask16(testnEpi32Mask([64]byte(a), [64]byte(b)))
}

func testnEpi32Mask(a [64]byte, b [64]byte) uint16


// MaskTestnEpi64Mask: Compute the bitwise NAND of packed 64-bit integers in
// 'a' and 'b', producing intermediate 64-bit values, and set the corresponding
// bit in result mask 'k' (subject to writemask 'k') if the intermediate value
// is zero. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ((a[i+63:i] AND b[i+63:i]) == 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPTESTNMQ'. Intrinsic: '_mm512_mask_testn_epi64_mask'.
// Requires AVX512F.
func MaskTestnEpi64Mask(k1 Mmask8, a M512i, b M512i) Mmask8 {
	return Mmask8(maskTestnEpi64Mask(uint8(k1), [64]byte(a), [64]byte(b)))
}

func maskTestnEpi64Mask(k1 uint8, a [64]byte, b [64]byte) uint8


// TestnEpi64Mask: Compute the bitwise NAND of packed 64-bit integers in 'a'
// and 'b', producing intermediate 64-bit values, and set the corresponding bit
// in result mask 'k' if the intermediate value is zero. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ((a[i+63:i] AND b[i+63:i]) == 0) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPTESTNMQ'. Intrinsic: '_mm512_testn_epi64_mask'.
// Requires AVX512F.
func TestnEpi64Mask(a M512i, b M512i) Mmask8 {
	return Mmask8(testnEpi64Mask([64]byte(a), [64]byte(b)))
}

func testnEpi64Mask(a [64]byte, b [64]byte) uint8


// MaskTestnEpi8Mask: Compute the bitwise NAND of packed 8-bit integers in 'a'
// and 'b', producing intermediate 8-bit values, and set the corresponding bit
// in result mask 'k' (subject to writemask 'k') if the intermediate value is
// zero. 
//
//		FOR j := 0 to 63
//			i := j*8
//			IF k1[j]
//				k[j] := ((a[i+7:i] AND b[i+7:i]) == 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:64] := 0
//
// Instruction: 'VPTESTNMB'. Intrinsic: '_mm512_mask_testn_epi8_mask'.
// Requires AVX512BW.
func MaskTestnEpi8Mask(k1 Mmask64, a M512i, b M512i) Mmask64 {
	return Mmask64(maskTestnEpi8Mask(uint64(k1), [64]byte(a), [64]byte(b)))
}

func maskTestnEpi8Mask(k1 uint64, a [64]byte, b [64]byte) uint64


// TestnEpi8Mask: Compute the bitwise NAND of packed 8-bit integers in 'a' and
// 'b', producing intermediate 8-bit values, and set the corresponding bit in
// result mask 'k' if the intermediate value is zero. 
//
//		FOR j := 0 to 63
//			i := j*8
//			k[j] := ((a[i+7:i] AND b[i+7:i]) == 0) ? 1 : 0
//		ENDFOR
//		k[MAX:64] := 0
//
// Instruction: 'VPTESTNMB'. Intrinsic: '_mm512_testn_epi8_mask'.
// Requires AVX512BW.
func TestnEpi8Mask(a M512i, b M512i) Mmask64 {
	return Mmask64(testnEpi8Mask([64]byte(a), [64]byte(b)))
}

func testnEpi8Mask(a [64]byte, b [64]byte) uint64


// MaskTruncPd: Truncate the packed double-precision (64-bit) floating-point
// elements in 'a', and store the results as packed double-precision
// floating-point elements in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := TRUNCATE(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_trunc_pd'.
// Requires AVX512F.
func MaskTruncPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskTruncPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskTruncPd(src [8]float64, k uint8, a [8]float64) [8]float64


// TruncPd: Truncate the packed double-precision (64-bit) floating-point
// elements in 'a', and store the results as packed double-precision
// floating-point elements in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := TRUNCATE(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_trunc_pd'.
// Requires AVX512F.
func TruncPd(a M512d) M512d {
	return M512d(truncPd([8]float64(a)))
}

func truncPd(a [8]float64) [8]float64


// MaskTruncPs: Truncate the packed single-precision (32-bit) floating-point
// elements in 'a', and store the results as packed single-precision
// floating-point elements in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := TRUNCATE(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_trunc_ps'.
// Requires AVX512F.
func MaskTruncPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskTruncPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskTruncPs(src [16]float32, k uint16, a [16]float32) [16]float32


// TruncPs: Truncate the packed single-precision (32-bit) floating-point
// elements in 'a', and store the results as packed single-precision
// floating-point elements in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := TRUNCATE(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_trunc_ps'.
// Requires AVX512F.
func TruncPs(a M512) M512 {
	return M512(truncPs([16]float32(a)))
}

func truncPs(a [16]float32) [16]float32


// Undefined: Return vector of type __m512 with undefined elements. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_undefined'.
// Requires AVX512F.
func Undefined() M512 {
	return M512(undefined())
}

func undefined() [16]float32


// UndefinedEpi32: Return vector of type __m512i with undefined elements. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_undefined_epi32'.
// Requires AVX512F.
func UndefinedEpi32() M512i {
	return M512i(undefinedEpi32())
}

func undefinedEpi32() [64]byte


// UndefinedPd: Return vector of type __m512d with undefined elements. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_undefined_pd'.
// Requires AVX512F.
func UndefinedPd() M512d {
	return M512d(undefinedPd())
}

func undefinedPd() [8]float64


// UndefinedPs: Return vector of type __m512 with undefined elements. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_undefined_ps'.
// Requires AVX512F.
func UndefinedPs() M512 {
	return M512(undefinedPs())
}

func undefinedPs() [16]float32


// MaskUnpackhiEpi16: Unpack and interleave 16-bit integers from the high half
// of each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		INTERLEAVE_HIGH_WORDS(src1[127:0], src2[127:0]){
//			dst[15:0] := src1[79:64]
//			dst[31:16] := src2[79:64] 
//			dst[47:32] := src1[95:80] 
//			dst[63:48] := src2[95:80] 
//			dst[79:64] := src1[111:96] 
//			dst[95:80] := src2[111:96] 
//			dst[111:96] := src1[127:112] 
//			dst[127:112] := src2[127:112] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_WORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_WORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_HIGH_WORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_HIGH_WORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKHWD'. Intrinsic: '_mm512_mask_unpackhi_epi16'.
// Requires AVX512BW.
func MaskUnpackhiEpi16(src M512i, k Mmask32, a M512i, b M512i) M512i {
	return M512i(maskUnpackhiEpi16([64]byte(src), uint32(k), [64]byte(a), [64]byte(b)))
}

func maskUnpackhiEpi16(src [64]byte, k uint32, a [64]byte, b [64]byte) [64]byte


// MaskzUnpackhiEpi16: Unpack and interleave 16-bit integers from the high half
// of each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		INTERLEAVE_HIGH_WORDS(src1[127:0], src2[127:0]){
//			dst[15:0] := src1[79:64]
//			dst[31:16] := src2[79:64] 
//			dst[47:32] := src1[95:80] 
//			dst[63:48] := src2[95:80] 
//			dst[79:64] := src1[111:96] 
//			dst[95:80] := src2[111:96] 
//			dst[111:96] := src1[127:112] 
//			dst[127:112] := src2[127:112] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_WORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_WORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_HIGH_WORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_HIGH_WORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKHWD'. Intrinsic: '_mm512_maskz_unpackhi_epi16'.
// Requires AVX512BW.
func MaskzUnpackhiEpi16(k Mmask32, a M512i, b M512i) M512i {
	return M512i(maskzUnpackhiEpi16(uint32(k), [64]byte(a), [64]byte(b)))
}

func maskzUnpackhiEpi16(k uint32, a [64]byte, b [64]byte) [64]byte


// UnpackhiEpi16: Unpack and interleave 16-bit integers from the high half of
// each 128-bit lane in 'a' and 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_HIGH_WORDS(src1[127:0], src2[127:0]){
//			dst[15:0] := src1[79:64]
//			dst[31:16] := src2[79:64] 
//			dst[47:32] := src1[95:80] 
//			dst[63:48] := src2[95:80] 
//			dst[79:64] := src1[111:96] 
//			dst[95:80] := src2[111:96] 
//			dst[111:96] := src1[127:112] 
//			dst[127:112] := src2[127:112] 
//			RETURN dst[127:0]
//		}
//		
//		dst[127:0] := INTERLEAVE_HIGH_WORDS(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_HIGH_WORDS(a[255:128], b[255:128])
//		dst[383:256] := INTERLEAVE_HIGH_WORDS(a[383:256], b[383:256])
//		dst[511:384] := INTERLEAVE_HIGH_WORDS(a[511:384], b[511:384])
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKHWD'. Intrinsic: '_mm512_unpackhi_epi16'.
// Requires AVX512BW.
func UnpackhiEpi16(a M512i, b M512i) M512i {
	return M512i(unpackhiEpi16([64]byte(a), [64]byte(b)))
}

func unpackhiEpi16(a [64]byte, b [64]byte) [64]byte


// MaskUnpackhiEpi32: Unpack and interleave 32-bit integers from the high half
// of each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_HIGH_DWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_HIGH_DWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKHDQ'. Intrinsic: '_mm512_mask_unpackhi_epi32'.
// Requires AVX512F.
func MaskUnpackhiEpi32(src M512i, k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskUnpackhiEpi32([64]byte(src), uint16(k), [64]byte(a), [64]byte(b)))
}

func maskUnpackhiEpi32(src [64]byte, k uint16, a [64]byte, b [64]byte) [64]byte


// MaskzUnpackhiEpi32: Unpack and interleave 32-bit integers from the high half
// of each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_HIGH_DWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_HIGH_DWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKHDQ'. Intrinsic: '_mm512_maskz_unpackhi_epi32'.
// Requires AVX512F.
func MaskzUnpackhiEpi32(k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskzUnpackhiEpi32(uint16(k), [64]byte(a), [64]byte(b)))
}

func maskzUnpackhiEpi32(k uint16, a [64]byte, b [64]byte) [64]byte


// UnpackhiEpi32: Unpack and interleave 32-bit integers from the high half of
// each 128-bit lane in 'a' and 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}	
//		
//		dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
//		dst[383:256] := INTERLEAVE_HIGH_DWORDS(a[383:256], b[383:256])
//		dst[511:384] := INTERLEAVE_HIGH_DWORDS(a[511:384], b[511:384])
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKHDQ'. Intrinsic: '_mm512_unpackhi_epi32'.
// Requires AVX512F.
func UnpackhiEpi32(a M512i, b M512i) M512i {
	return M512i(unpackhiEpi32([64]byte(a), [64]byte(b)))
}

func unpackhiEpi32(a [64]byte, b [64]byte) [64]byte


// MaskUnpackhiEpi64: Unpack and interleave 64-bit integers from the high half
// of each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_HIGH_QWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_HIGH_QWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKHQDQ'. Intrinsic: '_mm512_mask_unpackhi_epi64'.
// Requires AVX512F.
func MaskUnpackhiEpi64(src M512i, k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskUnpackhiEpi64([64]byte(src), uint8(k), [64]byte(a), [64]byte(b)))
}

func maskUnpackhiEpi64(src [64]byte, k uint8, a [64]byte, b [64]byte) [64]byte


// MaskzUnpackhiEpi64: Unpack and interleave 64-bit integers from the high half
// of each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_HIGH_QWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_HIGH_QWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKHQDQ'. Intrinsic: '_mm512_maskz_unpackhi_epi64'.
// Requires AVX512F.
func MaskzUnpackhiEpi64(k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskzUnpackhiEpi64(uint8(k), [64]byte(a), [64]byte(b)))
}

func maskzUnpackhiEpi64(k uint8, a [64]byte, b [64]byte) [64]byte


// UnpackhiEpi64: Unpack and interleave 64-bit integers from the high half of
// each 128-bit lane in 'a' and 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
//		dst[383:256] := INTERLEAVE_HIGH_QWORDS(a[383:256], b[383:256])
//		dst[511:384] := INTERLEAVE_HIGH_QWORDS(a[511:384], b[511:384])
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKHQDQ'. Intrinsic: '_mm512_unpackhi_epi64'.
// Requires AVX512F.
func UnpackhiEpi64(a M512i, b M512i) M512i {
	return M512i(unpackhiEpi64([64]byte(a), [64]byte(b)))
}

func unpackhiEpi64(a [64]byte, b [64]byte) [64]byte


// MaskUnpackhiEpi8: Unpack and interleave 8-bit integers from the high half of
// each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		INTERLEAVE_HIGH_BYTES(src1[127:0], src2[127:0]){
//			dst[7:0] := src1[71:64] 
//			dst[15:8] := src2[71:64] 
//			dst[23:16] := src1[79:72] 
//			dst[31:24] := src2[79:72] 
//			dst[39:32] := src1[87:80] 
//			dst[47:40] := src2[87:80] 
//			dst[55:48] := src1[95:88] 
//			dst[63:56] := src2[95:88] 
//			dst[71:64] := src1[103:96] 
//			dst[79:72] := src2[103:96] 
//			dst[87:80] := src1[111:104] 
//			dst[95:88] := src2[111:104] 
//			dst[103:96] := src1[119:112] 
//			dst[111:104] := src2[119:112] 
//			dst[119:112] := src1[127:120] 
//			dst[127:120] := src2[127:120] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_BYTES(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_BYTES(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_HIGH_BYTES(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_HIGH_BYTES(a[511:384], b[511:384])
//		
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := tmp_dst[i+7:i]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKHBW'. Intrinsic: '_mm512_mask_unpackhi_epi8'.
// Requires AVX512BW.
func MaskUnpackhiEpi8(src M512i, k Mmask64, a M512i, b M512i) M512i {
	return M512i(maskUnpackhiEpi8([64]byte(src), uint64(k), [64]byte(a), [64]byte(b)))
}

func maskUnpackhiEpi8(src [64]byte, k uint64, a [64]byte, b [64]byte) [64]byte


// MaskzUnpackhiEpi8: Unpack and interleave 8-bit integers from the high half
// of each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		INTERLEAVE_HIGH_BYTES(src1[127:0], src2[127:0]){
//			dst[7:0] := src1[71:64] 
//			dst[15:8] := src2[71:64] 
//			dst[23:16] := src1[79:72] 
//			dst[31:24] := src2[79:72] 
//			dst[39:32] := src1[87:80] 
//			dst[47:40] := src2[87:80] 
//			dst[55:48] := src1[95:88] 
//			dst[63:56] := src2[95:88] 
//			dst[71:64] := src1[103:96] 
//			dst[79:72] := src2[103:96] 
//			dst[87:80] := src1[111:104] 
//			dst[95:88] := src2[111:104] 
//			dst[103:96] := src1[119:112] 
//			dst[111:104] := src2[119:112] 
//			dst[119:112] := src1[127:120] 
//			dst[127:120] := src2[127:120] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_BYTES(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_BYTES(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_HIGH_BYTES(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_HIGH_BYTES(a[511:384], b[511:384])
//		
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := tmp_dst[i+7:i]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKHBW'. Intrinsic: '_mm512_maskz_unpackhi_epi8'.
// Requires AVX512BW.
func MaskzUnpackhiEpi8(k Mmask64, a M512i, b M512i) M512i {
	return M512i(maskzUnpackhiEpi8(uint64(k), [64]byte(a), [64]byte(b)))
}

func maskzUnpackhiEpi8(k uint64, a [64]byte, b [64]byte) [64]byte


// UnpackhiEpi8: Unpack and interleave 8-bit integers from the high half of
// each 128-bit lane in 'a' and 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_HIGH_BYTES(src1[127:0], src2[127:0]){
//			dst[7:0] := src1[71:64] 
//			dst[15:8] := src2[71:64] 
//			dst[23:16] := src1[79:72] 
//			dst[31:24] := src2[79:72] 
//			dst[39:32] := src1[87:80] 
//			dst[47:40] := src2[87:80] 
//			dst[55:48] := src1[95:88] 
//			dst[63:56] := src2[95:88] 
//			dst[71:64] := src1[103:96] 
//			dst[79:72] := src2[103:96] 
//			dst[87:80] := src1[111:104] 
//			dst[95:88] := src2[111:104] 
//			dst[103:96] := src1[119:112] 
//			dst[111:104] := src2[119:112] 
//			dst[119:112] := src1[127:120] 
//			dst[127:120] := src2[127:120] 
//			RETURN dst[127:0]
//		}	
//		
//		dst[127:0] := INTERLEAVE_HIGH_BYTES(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_HIGH_BYTES(a[255:128], b[255:128])
//		dst[383:256] := INTERLEAVE_HIGH_BYTES(a[383:256], b[383:256])
//		dst[511:384] := INTERLEAVE_HIGH_BYTES(a[511:384], b[511:384])
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKHBW'. Intrinsic: '_mm512_unpackhi_epi8'.
// Requires AVX512BW.
func UnpackhiEpi8(a M512i, b M512i) M512i {
	return M512i(unpackhiEpi8([64]byte(a), [64]byte(b)))
}

func unpackhiEpi8(a [64]byte, b [64]byte) [64]byte


// MaskUnpackhiPd: Unpack and interleave double-precision (64-bit)
// floating-point elements from the high half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_HIGH_QWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_HIGH_QWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VUNPCKHPD'. Intrinsic: '_mm512_mask_unpackhi_pd'.
// Requires AVX512F.
func MaskUnpackhiPd(src M512d, k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskUnpackhiPd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b)))
}

func maskUnpackhiPd(src [8]float64, k uint8, a [8]float64, b [8]float64) [8]float64


// MaskzUnpackhiPd: Unpack and interleave double-precision (64-bit)
// floating-point elements from the high half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_HIGH_QWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_HIGH_QWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VUNPCKHPD'. Intrinsic: '_mm512_maskz_unpackhi_pd'.
// Requires AVX512F.
func MaskzUnpackhiPd(k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskzUnpackhiPd(uint8(k), [8]float64(a), [8]float64(b)))
}

func maskzUnpackhiPd(k uint8, a [8]float64, b [8]float64) [8]float64


// UnpackhiPd: Unpack and interleave double-precision (64-bit) floating-point
// elements from the high half of each 128-bit lane in 'a' and 'b', and store
// the results in 'dst'. 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
//		dst[383:256] := INTERLEAVE_HIGH_QWORDS(a[383:256], b[383:256])
//		dst[511:384] := INTERLEAVE_HIGH_QWORDS(a[511:384], b[511:384])
//		dst[MAX:512] := 0
//
// Instruction: 'VUNPCKHPD'. Intrinsic: '_mm512_unpackhi_pd'.
// Requires AVX512F.
func UnpackhiPd(a M512d, b M512d) M512d {
	return M512d(unpackhiPd([8]float64(a), [8]float64(b)))
}

func unpackhiPd(a [8]float64, b [8]float64) [8]float64


// MaskUnpackhiPs: Unpack and interleave single-precision (32-bit)
// floating-point elements from the high half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_HIGH_DWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_HIGH_DWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VUNPCKHPS'. Intrinsic: '_mm512_mask_unpackhi_ps'.
// Requires AVX512F.
func MaskUnpackhiPs(src M512, k Mmask16, a M512, b M512) M512 {
	return M512(maskUnpackhiPs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b)))
}

func maskUnpackhiPs(src [16]float32, k uint16, a [16]float32, b [16]float32) [16]float32


// MaskzUnpackhiPs: Unpack and interleave single-precision (32-bit)
// floating-point elements from the high half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_HIGH_DWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_HIGH_DWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VUNPCKHPS'. Intrinsic: '_mm512_maskz_unpackhi_ps'.
// Requires AVX512F.
func MaskzUnpackhiPs(k Mmask16, a M512, b M512) M512 {
	return M512(maskzUnpackhiPs(uint16(k), [16]float32(a), [16]float32(b)))
}

func maskzUnpackhiPs(k uint16, a [16]float32, b [16]float32) [16]float32


// UnpackhiPs: Unpack and interleave single-precision (32-bit) floating-point
// elements from the high half of each 128-bit lane in 'a' and 'b', and store
// the results in 'dst'. 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}	
//		
//		dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
//		dst[383:256] := INTERLEAVE_HIGH_DWORDS(a[383:256], b[383:256])
//		dst[511:384] := INTERLEAVE_HIGH_DWORDS(a[511:384], b[511:384])
//		dst[MAX:512] := 0
//
// Instruction: 'VUNPCKHPS'. Intrinsic: '_mm512_unpackhi_ps'.
// Requires AVX512F.
func UnpackhiPs(a M512, b M512) M512 {
	return M512(unpackhiPs([16]float32(a), [16]float32(b)))
}

func unpackhiPs(a [16]float32, b [16]float32) [16]float32


// MaskUnpackloEpi16: Unpack and interleave 16-bit integers from the low half
// of each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		INTERLEAVE_WORDS(src1[127:0], src2[127:0]){
//			dst[15:0] := src1[15:0] 
//			dst[31:16] := src2[15:0] 
//			dst[47:32] := src1[31:16] 
//			dst[63:48] := src2[31:16] 
//			dst[79:64] := src1[47:32] 
//			dst[95:80] := src2[47:32] 
//			dst[111:96] := src1[63:48] 
//			dst[127:112] := src2[63:48] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_WORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_WORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_WORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_WORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKLWD'. Intrinsic: '_mm512_mask_unpacklo_epi16'.
// Requires AVX512BW.
func MaskUnpackloEpi16(src M512i, k Mmask32, a M512i, b M512i) M512i {
	return M512i(maskUnpackloEpi16([64]byte(src), uint32(k), [64]byte(a), [64]byte(b)))
}

func maskUnpackloEpi16(src [64]byte, k uint32, a [64]byte, b [64]byte) [64]byte


// MaskzUnpackloEpi16: Unpack and interleave 16-bit integers from the low half
// of each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		INTERLEAVE_WORDS(src1[127:0], src2[127:0]){
//			dst[15:0] := src1[15:0] 
//			dst[31:16] := src2[15:0] 
//			dst[47:32] := src1[31:16] 
//			dst[63:48] := src2[31:16] 
//			dst[79:64] := src1[47:32] 
//			dst[95:80] := src2[47:32] 
//			dst[111:96] := src1[63:48] 
//			dst[127:112] := src2[63:48] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_WORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_WORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_WORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_WORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 31
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKLWD'. Intrinsic: '_mm512_maskz_unpacklo_epi16'.
// Requires AVX512BW.
func MaskzUnpackloEpi16(k Mmask32, a M512i, b M512i) M512i {
	return M512i(maskzUnpackloEpi16(uint32(k), [64]byte(a), [64]byte(b)))
}

func maskzUnpackloEpi16(k uint32, a [64]byte, b [64]byte) [64]byte


// UnpackloEpi16: Unpack and interleave 16-bit integers from the low half of
// each 128-bit lane in 'a' and 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_WORDS(src1[127:0], src2[127:0]){
//			dst[15:0] := src1[15:0] 
//			dst[31:16] := src2[15:0] 
//			dst[47:32] := src1[31:16] 
//			dst[63:48] := src2[31:16] 
//			dst[79:64] := src1[47:32] 
//			dst[95:80] := src2[47:32] 
//			dst[111:96] := src1[63:48] 
//			dst[127:112] := src2[63:48] 
//			RETURN dst[127:0]
//		}	
//		
//		dst[127:0] := INTERLEAVE_WORDS(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_WORDS(a[255:128], b[255:128])
//		dst[383:256] := INTERLEAVE_WORDS(a[383:256], b[383:256])
//		dst[511:384] := INTERLEAVE_WORDS(a[511:384], b[511:384])
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKLWD'. Intrinsic: '_mm512_unpacklo_epi16'.
// Requires AVX512BW.
func UnpackloEpi16(a M512i, b M512i) M512i {
	return M512i(unpackloEpi16([64]byte(a), [64]byte(b)))
}

func unpackloEpi16(a [64]byte, b [64]byte) [64]byte


// MaskUnpackloEpi32: Unpack and interleave 32-bit integers from the low half
// of each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_DWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_DWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKLDQ'. Intrinsic: '_mm512_mask_unpacklo_epi32'.
// Requires AVX512F.
func MaskUnpackloEpi32(src M512i, k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskUnpackloEpi32([64]byte(src), uint16(k), [64]byte(a), [64]byte(b)))
}

func maskUnpackloEpi32(src [64]byte, k uint16, a [64]byte, b [64]byte) [64]byte


// MaskzUnpackloEpi32: Unpack and interleave 32-bit integers from the low half
// of each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_DWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_DWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKLDQ'. Intrinsic: '_mm512_maskz_unpacklo_epi32'.
// Requires AVX512F.
func MaskzUnpackloEpi32(k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskzUnpackloEpi32(uint16(k), [64]byte(a), [64]byte(b)))
}

func maskzUnpackloEpi32(k uint16, a [64]byte, b [64]byte) [64]byte


// UnpackloEpi32: Unpack and interleave 32-bit integers from the low half of
// each 128-bit lane in 'a' and 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
//		dst[383:256] := INTERLEAVE_DWORDS(a[383:256], b[383:256])
//		dst[511:384] := INTERLEAVE_DWORDS(a[511:384], b[511:384])
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKLDQ'. Intrinsic: '_mm512_unpacklo_epi32'.
// Requires AVX512F.
func UnpackloEpi32(a M512i, b M512i) M512i {
	return M512i(unpackloEpi32([64]byte(a), [64]byte(b)))
}

func unpackloEpi32(a [64]byte, b [64]byte) [64]byte


// MaskUnpackloEpi64: Unpack and interleave 64-bit integers from the low half
// of each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_QWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_QWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKLQDQ'. Intrinsic: '_mm512_mask_unpacklo_epi64'.
// Requires AVX512F.
func MaskUnpackloEpi64(src M512i, k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskUnpackloEpi64([64]byte(src), uint8(k), [64]byte(a), [64]byte(b)))
}

func maskUnpackloEpi64(src [64]byte, k uint8, a [64]byte, b [64]byte) [64]byte


// MaskzUnpackloEpi64: Unpack and interleave 64-bit integers from the low half
// of each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_QWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_QWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKLQDQ'. Intrinsic: '_mm512_maskz_unpacklo_epi64'.
// Requires AVX512F.
func MaskzUnpackloEpi64(k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskzUnpackloEpi64(uint8(k), [64]byte(a), [64]byte(b)))
}

func maskzUnpackloEpi64(k uint8, a [64]byte, b [64]byte) [64]byte


// UnpackloEpi64: Unpack and interleave 64-bit integers from the low half of
// each 128-bit lane in 'a' and 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
//		dst[383:256] := INTERLEAVE_QWORDS(a[383:256], b[383:256])
//		dst[511:384] := INTERLEAVE_QWORDS(a[511:384], b[511:384])
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKLQDQ'. Intrinsic: '_mm512_unpacklo_epi64'.
// Requires AVX512F.
func UnpackloEpi64(a M512i, b M512i) M512i {
	return M512i(unpackloEpi64([64]byte(a), [64]byte(b)))
}

func unpackloEpi64(a [64]byte, b [64]byte) [64]byte


// MaskUnpackloEpi8: Unpack and interleave 8-bit integers from the low half of
// each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		INTERLEAVE_BYTES(src1[127:0], src2[127:0]){
//			dst[7:0] := src1[7:0] 
//			dst[15:8] := src2[7:0] 
//			dst[23:16] := src1[15:8] 
//			dst[31:24] := src2[15:8] 
//			dst[39:32] := src1[23:16] 
//			dst[47:40] := src2[23:16] 
//			dst[55:48] := src1[31:24] 
//			dst[63:56] := src2[31:24] 
//			dst[71:64] := src1[39:32]
//			dst[79:72] := src2[39:32] 
//			dst[87:80] := src1[47:40] 
//			dst[95:88] := src2[47:40] 
//			dst[103:96] := src1[55:48] 
//			dst[111:104] := src2[55:48] 
//			dst[119:112] := src1[63:56] 
//			dst[127:120] := src2[63:56] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_BYTES(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_BYTES(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_BYTES(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_BYTES(a[511:384], b[511:384])
//		
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := tmp_dst[i+7:i]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKLBW'. Intrinsic: '_mm512_mask_unpacklo_epi8'.
// Requires AVX512BW.
func MaskUnpackloEpi8(src M512i, k Mmask64, a M512i, b M512i) M512i {
	return M512i(maskUnpackloEpi8([64]byte(src), uint64(k), [64]byte(a), [64]byte(b)))
}

func maskUnpackloEpi8(src [64]byte, k uint64, a [64]byte, b [64]byte) [64]byte


// MaskzUnpackloEpi8: Unpack and interleave 8-bit integers from the low half of
// each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		INTERLEAVE_BYTES(src1[127:0], src2[127:0]){
//			dst[7:0] := src1[7:0] 
//			dst[15:8] := src2[7:0] 
//			dst[23:16] := src1[15:8] 
//			dst[31:24] := src2[15:8] 
//			dst[39:32] := src1[23:16] 
//			dst[47:40] := src2[23:16] 
//			dst[55:48] := src1[31:24] 
//			dst[63:56] := src2[31:24] 
//			dst[71:64] := src1[39:32]
//			dst[79:72] := src2[39:32] 
//			dst[87:80] := src1[47:40] 
//			dst[95:88] := src2[47:40] 
//			dst[103:96] := src1[55:48] 
//			dst[111:104] := src2[55:48] 
//			dst[119:112] := src1[63:56] 
//			dst[127:120] := src2[63:56] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_BYTES(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_BYTES(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_BYTES(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_BYTES(a[511:384], b[511:384])
//		
//		FOR j := 0 to 63
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := tmp_dst[i+7:i]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKLBW'. Intrinsic: '_mm512_maskz_unpacklo_epi8'.
// Requires AVX512BW.
func MaskzUnpackloEpi8(k Mmask64, a M512i, b M512i) M512i {
	return M512i(maskzUnpackloEpi8(uint64(k), [64]byte(a), [64]byte(b)))
}

func maskzUnpackloEpi8(k uint64, a [64]byte, b [64]byte) [64]byte


// UnpackloEpi8: Unpack and interleave 8-bit integers from the low half of each
// 128-bit lane in 'a' and 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_BYTES(src1[127:0], src2[127:0]){
//			dst[7:0] := src1[7:0] 
//			dst[15:8] := src2[7:0] 
//			dst[23:16] := src1[15:8] 
//			dst[31:24] := src2[15:8] 
//			dst[39:32] := src1[23:16] 
//			dst[47:40] := src2[23:16] 
//			dst[55:48] := src1[31:24] 
//			dst[63:56] := src2[31:24] 
//			dst[71:64] := src1[39:32]
//			dst[79:72] := src2[39:32] 
//			dst[87:80] := src1[47:40] 
//			dst[95:88] := src2[47:40] 
//			dst[103:96] := src1[55:48] 
//			dst[111:104] := src2[55:48] 
//			dst[119:112] := src1[63:56] 
//			dst[127:120] := src2[63:56] 
//			RETURN dst[127:0]
//		}	
//		
//		dst[127:0] := INTERLEAVE_BYTES(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_BYTES(a[255:128], b[255:128])
//		dst[383:256] := INTERLEAVE_BYTES(a[383:256], b[383:256])
//		dst[511:384] := INTERLEAVE_BYTES(a[511:384], b[511:384])
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKLBW'. Intrinsic: '_mm512_unpacklo_epi8'.
// Requires AVX512BW.
func UnpackloEpi8(a M512i, b M512i) M512i {
	return M512i(unpackloEpi8([64]byte(a), [64]byte(b)))
}

func unpackloEpi8(a [64]byte, b [64]byte) [64]byte


// MaskUnpackloPd: Unpack and interleave double-precision (64-bit)
// floating-point elements from the low half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_QWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_QWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VUNPCKLPD'. Intrinsic: '_mm512_mask_unpacklo_pd'.
// Requires AVX512F.
func MaskUnpackloPd(src M512d, k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskUnpackloPd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b)))
}

func maskUnpackloPd(src [8]float64, k uint8, a [8]float64, b [8]float64) [8]float64


// MaskzUnpackloPd: Unpack and interleave double-precision (64-bit)
// floating-point elements from the low half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_QWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_QWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VUNPCKLPD'. Intrinsic: '_mm512_maskz_unpacklo_pd'.
// Requires AVX512F.
func MaskzUnpackloPd(k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskzUnpackloPd(uint8(k), [8]float64(a), [8]float64(b)))
}

func maskzUnpackloPd(k uint8, a [8]float64, b [8]float64) [8]float64


// UnpackloPd: Unpack and interleave double-precision (64-bit) floating-point
// elements from the low half of each 128-bit lane in 'a' and 'b', and store
// the results in 'dst'. 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
//		dst[383:256] := INTERLEAVE_QWORDS(a[383:256], b[383:256])
//		dst[511:384] := INTERLEAVE_QWORDS(a[511:384], b[511:384])
//		dst[MAX:512] := 0
//
// Instruction: 'VUNPCKLPD'. Intrinsic: '_mm512_unpacklo_pd'.
// Requires AVX512F.
func UnpackloPd(a M512d, b M512d) M512d {
	return M512d(unpackloPd([8]float64(a), [8]float64(b)))
}

func unpackloPd(a [8]float64, b [8]float64) [8]float64


// MaskUnpackloPs: Unpack and interleave single-precision (32-bit)
// floating-point elements from the low half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_DWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_DWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VUNPCKLPS'. Intrinsic: '_mm512_mask_unpacklo_ps'.
// Requires AVX512F.
func MaskUnpackloPs(src M512, k Mmask16, a M512, b M512) M512 {
	return M512(maskUnpackloPs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b)))
}

func maskUnpackloPs(src [16]float32, k uint16, a [16]float32, b [16]float32) [16]float32


// MaskzUnpackloPs: Unpack and interleave single-precision (32-bit)
// floating-point elements from the low half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_DWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_DWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VUNPCKLPS'. Intrinsic: '_mm512_maskz_unpacklo_ps'.
// Requires AVX512F.
func MaskzUnpackloPs(k Mmask16, a M512, b M512) M512 {
	return M512(maskzUnpackloPs(uint16(k), [16]float32(a), [16]float32(b)))
}

func maskzUnpackloPs(k uint16, a [16]float32, b [16]float32) [16]float32


// UnpackloPs: Unpack and interleave single-precision (32-bit) floating-point
// elements from the low half of each 128-bit lane in 'a' and 'b', and store
// the results in 'dst'. 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
//		dst[383:256] := INTERLEAVE_DWORDS(a[383:256], b[383:256])
//		dst[511:384] := INTERLEAVE_DWORDS(a[511:384], b[511:384])
//		dst[MAX:512] := 0
//
// Instruction: 'VUNPCKLPS'. Intrinsic: '_mm512_unpacklo_ps'.
// Requires AVX512F.
func UnpackloPs(a M512, b M512) M512 {
	return M512(unpackloPs([16]float32(a), [16]float32(b)))
}

func unpackloPs(a [16]float32, b [16]float32) [16]float32


// MaskXorEpi32: Compute the bitwise XOR of packed 32-bit integers in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPXORD'. Intrinsic: '_mm512_mask_xor_epi32'.
// Requires KNCNI.
func MaskXorEpi32(src M512i, k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskXorEpi32([64]byte(src), uint16(k), [64]byte(a), [64]byte(b)))
}

func maskXorEpi32(src [64]byte, k uint16, a [64]byte, b [64]byte) [64]byte


// MaskzXorEpi32: Compute the bitwise XOR of packed 32-bit integers in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPXORD'. Intrinsic: '_mm512_maskz_xor_epi32'.
// Requires AVX512F.
func MaskzXorEpi32(k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskzXorEpi32(uint16(k), [64]byte(a), [64]byte(b)))
}

func maskzXorEpi32(k uint16, a [64]byte, b [64]byte) [64]byte


// XorEpi32: Compute the bitwise XOR of packed 32-bit integers in 'a' and 'b',
// and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPXORD'. Intrinsic: '_mm512_xor_epi32'.
// Requires KNCNI.
func XorEpi32(a M512i, b M512i) M512i {
	return M512i(xorEpi32([64]byte(a), [64]byte(b)))
}

func xorEpi32(a [64]byte, b [64]byte) [64]byte


// MaskXorEpi64: Compute the bitwise XOR of packed 64-bit integers in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPXORQ'. Intrinsic: '_mm512_mask_xor_epi64'.
// Requires KNCNI.
func MaskXorEpi64(src M512i, k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskXorEpi64([64]byte(src), uint8(k), [64]byte(a), [64]byte(b)))
}

func maskXorEpi64(src [64]byte, k uint8, a [64]byte, b [64]byte) [64]byte


// MaskzXorEpi64: Compute the bitwise XOR of packed 64-bit integers in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPXORQ'. Intrinsic: '_mm512_maskz_xor_epi64'.
// Requires AVX512F.
func MaskzXorEpi64(k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskzXorEpi64(uint8(k), [64]byte(a), [64]byte(b)))
}

func maskzXorEpi64(k uint8, a [64]byte, b [64]byte) [64]byte


// XorEpi64: Compute the bitwise XOR of packed 64-bit integers in 'a' and 'b',
// and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPXORQ'. Intrinsic: '_mm512_xor_epi64'.
// Requires KNCNI.
func XorEpi64(a M512i, b M512i) M512i {
	return M512i(xorEpi64([64]byte(a), [64]byte(b)))
}

func xorEpi64(a [64]byte, b [64]byte) [64]byte


// MaskXorPd: Compute the bitwise XOR of packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VXORPD'. Intrinsic: '_mm512_mask_xor_pd'.
// Requires AVX512DQ.
func MaskXorPd(src M512d, k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskXorPd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b)))
}

func maskXorPd(src [8]float64, k uint8, a [8]float64, b [8]float64) [8]float64


// MaskzXorPd: Compute the bitwise XOR of packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VXORPD'. Intrinsic: '_mm512_maskz_xor_pd'.
// Requires AVX512DQ.
func MaskzXorPd(k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskzXorPd(uint8(k), [8]float64(a), [8]float64(b)))
}

func maskzXorPd(k uint8, a [8]float64, b [8]float64) [8]float64


// XorPd: Compute the bitwise XOR of packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VXORPD'. Intrinsic: '_mm512_xor_pd'.
// Requires AVX512DQ.
func XorPd(a M512d, b M512d) M512d {
	return M512d(xorPd([8]float64(a), [8]float64(b)))
}

func xorPd(a [8]float64, b [8]float64) [8]float64


// MaskXorPs: Compute the bitwise XOR of packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VXORPS'. Intrinsic: '_mm512_mask_xor_ps'.
// Requires AVX512DQ.
func MaskXorPs(src M512, k Mmask16, a M512, b M512) M512 {
	return M512(maskXorPs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b)))
}

func maskXorPs(src [16]float32, k uint16, a [16]float32, b [16]float32) [16]float32


// MaskzXorPs: Compute the bitwise XOR of packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VXORPS'. Intrinsic: '_mm512_maskz_xor_ps'.
// Requires AVX512DQ.
func MaskzXorPs(k Mmask16, a M512, b M512) M512 {
	return M512(maskzXorPs(uint16(k), [16]float32(a), [16]float32(b)))
}

func maskzXorPs(k uint16, a [16]float32, b [16]float32) [16]float32


// XorPs: Compute the bitwise XOR of packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VXORPS'. Intrinsic: '_mm512_xor_ps'.
// Requires AVX512DQ.
func XorPs(a M512, b M512) M512 {
	return M512(xorPs([16]float32(a), [16]float32(b)))
}

func xorPs(a [16]float32, b [16]float32) [16]float32


// XorSi512: Compute the bitwise XOR of 512 bits (representing integer data) in
// 'a' and 'b', and store the result in 'dst'. 
//
//		dst[511:0] := (a[511:0] XOR b[511:0])
//		dst[MAX:512] := 0
//
// Instruction: 'VPXORD'. Intrinsic: '_mm512_xor_si512'.
// Requires KNCNI.
func XorSi512(a M512i, b M512i) M512i {
	return M512i(xorSi512([64]byte(a), [64]byte(b)))
}

func xorSi512(a [64]byte, b [64]byte) [64]byte

