package avx512f

import "github.com/klauspost/intrinsics/x86"

var _ = x86.M64{}  // Make sure we use x86 package


// MaskAbsEpi32: Compute the absolute value of packed 32-bit integers in 'a',
// and store the unsigned results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ABS(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPABSD'. Intrinsic: '_mm_mask_abs_epi32'.
// Requires AVX512F.
func MaskAbsEpi32(src x86.M128i, k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskAbsEpi32([16]byte(src), uint8(k), [16]byte(a)))
}

func maskAbsEpi32(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzAbsEpi32: Compute the absolute value of packed 32-bit integers in 'a',
// and store the unsigned results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ABS(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPABSD'. Intrinsic: '_mm_maskz_abs_epi32'.
// Requires AVX512F.
func MaskzAbsEpi32(k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskzAbsEpi32(uint8(k), [16]byte(a)))
}

func maskzAbsEpi32(k uint8, a [16]byte) [16]byte


// M256MaskAbsEpi32: Compute the absolute value of packed 32-bit integers in
// 'a', and store the unsigned results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ABS(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPABSD'. Intrinsic: '_mm256_mask_abs_epi32'.
// Requires AVX512F.
func M256MaskAbsEpi32(src x86.M256i, k x86.Mmask8, a x86.M256i) x86.M256i {
	return x86.M256i(m256MaskAbsEpi32([32]byte(src), uint8(k), [32]byte(a)))
}

func m256MaskAbsEpi32(src [32]byte, k uint8, a [32]byte) [32]byte


// M256MaskzAbsEpi32: Compute the absolute value of packed 32-bit integers in
// 'a', and store the unsigned results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ABS(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPABSD'. Intrinsic: '_mm256_maskz_abs_epi32'.
// Requires AVX512F.
func M256MaskzAbsEpi32(k x86.Mmask8, a x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzAbsEpi32(uint8(k), [32]byte(a)))
}

func m256MaskzAbsEpi32(k uint8, a [32]byte) [32]byte


// M512AbsEpi32: Compute the absolute value of packed 32-bit integers in 'a',
// and store the unsigned results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ABS(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPABSD'. Intrinsic: '_mm512_abs_epi32'.
// Requires AVX512F.
func M512AbsEpi32(a x86.M512i) x86.M512i {
	return x86.M512i(m512AbsEpi32([64]byte(a)))
}

func m512AbsEpi32(a [64]byte) [64]byte


// M512MaskAbsEpi32: Compute the absolute value of packed 32-bit integers in
// 'a', and store the unsigned results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ABS(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPABSD'. Intrinsic: '_mm512_mask_abs_epi32'.
// Requires AVX512F.
func M512MaskAbsEpi32(src x86.M512i, k x86.Mmask16, a x86.M512i) x86.M512i {
	return x86.M512i(m512MaskAbsEpi32([64]byte(src), uint16(k), [64]byte(a)))
}

func m512MaskAbsEpi32(src [64]byte, k uint16, a [64]byte) [64]byte


// M512MaskzAbsEpi32: Compute the absolute value of packed 32-bit integers in
// 'a', and store the unsigned results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ABS(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPABSD'. Intrinsic: '_mm512_maskz_abs_epi32'.
// Requires AVX512F.
func M512MaskzAbsEpi32(k x86.Mmask16, a x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzAbsEpi32(uint16(k), [64]byte(a)))
}

func m512MaskzAbsEpi32(k uint16, a [64]byte) [64]byte


// AbsEpi64: Compute the absolute value of packed 64-bit integers in 'a', and
// store the unsigned results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := ABS(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPABSQ'. Intrinsic: '_mm_abs_epi64'.
// Requires AVX512F.
func AbsEpi64(a x86.M128i) x86.M128i {
	return x86.M128i(absEpi64([16]byte(a)))
}

func absEpi64(a [16]byte) [16]byte


// MaskAbsEpi64: Compute the absolute value of packed 64-bit integers in 'a',
// and store the unsigned results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ABS(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPABSQ'. Intrinsic: '_mm_mask_abs_epi64'.
// Requires AVX512F.
func MaskAbsEpi64(src x86.M128i, k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskAbsEpi64([16]byte(src), uint8(k), [16]byte(a)))
}

func maskAbsEpi64(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzAbsEpi64: Compute the absolute value of packed 64-bit integers in 'a',
// and store the unsigned results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ABS(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPABSQ'. Intrinsic: '_mm_maskz_abs_epi64'.
// Requires AVX512F.
func MaskzAbsEpi64(k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskzAbsEpi64(uint8(k), [16]byte(a)))
}

func maskzAbsEpi64(k uint8, a [16]byte) [16]byte


// M256AbsEpi64: Compute the absolute value of packed 64-bit integers in 'a',
// and store the unsigned results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ABS(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPABSQ'. Intrinsic: '_mm256_abs_epi64'.
// Requires AVX512F.
func M256AbsEpi64(a x86.M256i) x86.M256i {
	return x86.M256i(m256AbsEpi64([32]byte(a)))
}

func m256AbsEpi64(a [32]byte) [32]byte


// M256MaskAbsEpi64: Compute the absolute value of packed 64-bit integers in
// 'a', and store the unsigned results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ABS(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPABSQ'. Intrinsic: '_mm256_mask_abs_epi64'.
// Requires AVX512F.
func M256MaskAbsEpi64(src x86.M256i, k x86.Mmask8, a x86.M256i) x86.M256i {
	return x86.M256i(m256MaskAbsEpi64([32]byte(src), uint8(k), [32]byte(a)))
}

func m256MaskAbsEpi64(src [32]byte, k uint8, a [32]byte) [32]byte


// M256MaskzAbsEpi64: Compute the absolute value of packed 64-bit integers in
// 'a', and store the unsigned results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ABS(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPABSQ'. Intrinsic: '_mm256_maskz_abs_epi64'.
// Requires AVX512F.
func M256MaskzAbsEpi64(k x86.Mmask8, a x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzAbsEpi64(uint8(k), [32]byte(a)))
}

func m256MaskzAbsEpi64(k uint8, a [32]byte) [32]byte


// M512AbsEpi64: Compute the absolute value of packed 64-bit integers in 'a',
// and store the unsigned results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ABS(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPABSQ'. Intrinsic: '_mm512_abs_epi64'.
// Requires AVX512F.
func M512AbsEpi64(a x86.M512i) x86.M512i {
	return x86.M512i(m512AbsEpi64([64]byte(a)))
}

func m512AbsEpi64(a [64]byte) [64]byte


// M512MaskAbsEpi64: Compute the absolute value of packed 64-bit integers in
// 'a', and store the unsigned results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ABS(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPABSQ'. Intrinsic: '_mm512_mask_abs_epi64'.
// Requires AVX512F.
func M512MaskAbsEpi64(src x86.M512i, k x86.Mmask8, a x86.M512i) x86.M512i {
	return x86.M512i(m512MaskAbsEpi64([64]byte(src), uint8(k), [64]byte(a)))
}

func m512MaskAbsEpi64(src [64]byte, k uint8, a [64]byte) [64]byte


// M512MaskzAbsEpi64: Compute the absolute value of packed 64-bit integers in
// 'a', and store the unsigned results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ABS(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPABSQ'. Intrinsic: '_mm512_maskz_abs_epi64'.
// Requires AVX512F.
func M512MaskzAbsEpi64(k x86.Mmask8, a x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzAbsEpi64(uint8(k), [64]byte(a)))
}

func m512MaskzAbsEpi64(k uint8, a [64]byte) [64]byte


// M512AcosPd: Compute the inverse cosine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ACOS(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_acos_pd'.
// Requires AVX512F.
func M512AcosPd(a x86.M512d) x86.M512d {
	return x86.M512d(m512AcosPd([8]float64(a)))
}

func m512AcosPd(a [8]float64) [8]float64


// M512MaskAcosPd: Compute the inverse cosine of packed double-precision
// (64-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ACOS(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_acos_pd'.
// Requires AVX512F.
func M512MaskAcosPd(src x86.M512d, k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskAcosPd([8]float64(src), uint8(k), [8]float64(a)))
}

func m512MaskAcosPd(src [8]float64, k uint8, a [8]float64) [8]float64


// M512AcosPs: Compute the inverse cosine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ACOS(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_acos_ps'.
// Requires AVX512F.
func M512AcosPs(a x86.M512) x86.M512 {
	return x86.M512(m512AcosPs([16]float32(a)))
}

func m512AcosPs(a [16]float32) [16]float32


// M512MaskAcosPs: Compute the inverse cosine of packed single-precision
// (32-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ACOS(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_acos_ps'.
// Requires AVX512F.
func M512MaskAcosPs(src x86.M512, k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskAcosPs([16]float32(src), uint16(k), [16]float32(a)))
}

func m512MaskAcosPs(src [16]float32, k uint16, a [16]float32) [16]float32


// M512AcoshPd: Compute the inverse hyperbolic cosine of packed
// double-precision (64-bit) floating-point elements in 'a' expressed in
// radians, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ACOSH(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_acosh_pd'.
// Requires AVX512F.
func M512AcoshPd(a x86.M512d) x86.M512d {
	return x86.M512d(m512AcoshPd([8]float64(a)))
}

func m512AcoshPd(a [8]float64) [8]float64


// M512MaskAcoshPd: Compute the inverse hyperbolic cosine of packed
// double-precision (64-bit) floating-point elements in 'a' expressed in
// radians, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ACOSH(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_acosh_pd'.
// Requires AVX512F.
func M512MaskAcoshPd(src x86.M512d, k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskAcoshPd([8]float64(src), uint8(k), [8]float64(a)))
}

func m512MaskAcoshPd(src [8]float64, k uint8, a [8]float64) [8]float64


// M512AcoshPs: Compute the inverse hyperbolic cosine of packed
// single-precision (32-bit) floating-point elements in 'a' expressed in
// radians, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ACOSH(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_acosh_ps'.
// Requires AVX512F.
func M512AcoshPs(a x86.M512) x86.M512 {
	return x86.M512(m512AcoshPs([16]float32(a)))
}

func m512AcoshPs(a [16]float32) [16]float32


// M512MaskAcoshPs: Compute the inverse hyperbolic cosine of packed
// single-precision (32-bit) floating-point elements in 'a' expressed in
// radians, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ACOSH(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_acosh_ps'.
// Requires AVX512F.
func M512MaskAcoshPs(src x86.M512, k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskAcoshPs([16]float32(src), uint16(k), [16]float32(a)))
}

func m512MaskAcoshPs(src [16]float32, k uint16, a [16]float32) [16]float32


// MaskAddEpi32: Add packed 32-bit integers in 'a' and 'b', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] + b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPADDD'. Intrinsic: '_mm_mask_add_epi32'.
// Requires AVX512F.
func MaskAddEpi32(src x86.M128i, k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskAddEpi32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskAddEpi32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzAddEpi32: Add packed 32-bit integers in 'a' and 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] + b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPADDD'. Intrinsic: '_mm_maskz_add_epi32'.
// Requires AVX512F.
func MaskzAddEpi32(k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskzAddEpi32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzAddEpi32(k uint8, a [16]byte, b [16]byte) [16]byte


// M256MaskAddEpi32: Add packed 32-bit integers in 'a' and 'b', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] + b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDD'. Intrinsic: '_mm256_mask_add_epi32'.
// Requires AVX512F.
func M256MaskAddEpi32(src x86.M256i, k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskAddEpi32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskAddEpi32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// M256MaskzAddEpi32: Add packed 32-bit integers in 'a' and 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] + b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDD'. Intrinsic: '_mm256_maskz_add_epi32'.
// Requires AVX512F.
func M256MaskzAddEpi32(k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzAddEpi32(uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskzAddEpi32(k uint8, a [32]byte, b [32]byte) [32]byte


// M512MaskzAddEpi32: Add packed 32-bit integers in 'a' and 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] + b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPADDD'. Intrinsic: '_mm512_maskz_add_epi32'.
// Requires AVX512F.
func M512MaskzAddEpi32(k x86.Mmask16, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzAddEpi32(uint16(k), [64]byte(a), [64]byte(b)))
}

func m512MaskzAddEpi32(k uint16, a [64]byte, b [64]byte) [64]byte


// MaskAddEpi64: Add packed 64-bit integers in 'a' and 'b', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] + b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPADDQ'. Intrinsic: '_mm_mask_add_epi64'.
// Requires AVX512F.
func MaskAddEpi64(src x86.M128i, k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskAddEpi64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskAddEpi64(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzAddEpi64: Add packed 64-bit integers in 'a' and 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] + b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPADDQ'. Intrinsic: '_mm_maskz_add_epi64'.
// Requires AVX512F.
func MaskzAddEpi64(k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskzAddEpi64(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzAddEpi64(k uint8, a [16]byte, b [16]byte) [16]byte


// M256MaskAddEpi64: Add packed 64-bit integers in 'a' and 'b', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] + b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDQ'. Intrinsic: '_mm256_mask_add_epi64'.
// Requires AVX512F.
func M256MaskAddEpi64(src x86.M256i, k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskAddEpi64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskAddEpi64(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// M256MaskzAddEpi64: Add packed 64-bit integers in 'a' and 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] + b[i+63:i]
//			ELSE
//				dst[i+63:i] :=0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDQ'. Intrinsic: '_mm256_maskz_add_epi64'.
// Requires AVX512F.
func M256MaskzAddEpi64(k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzAddEpi64(uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskzAddEpi64(k uint8, a [32]byte, b [32]byte) [32]byte


// M512AddEpi64: Add packed 64-bit integers in 'a' and 'b', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := a[i+63:i] + b[i+63:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPADDQ'. Intrinsic: '_mm512_add_epi64'.
// Requires AVX512F.
func M512AddEpi64(a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512AddEpi64([64]byte(a), [64]byte(b)))
}

func m512AddEpi64(a [64]byte, b [64]byte) [64]byte


// M512MaskAddEpi64: Add packed 64-bit integers in 'a' and 'b', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] + b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPADDQ'. Intrinsic: '_mm512_mask_add_epi64'.
// Requires AVX512F.
func M512MaskAddEpi64(src x86.M512i, k x86.Mmask8, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskAddEpi64([64]byte(src), uint8(k), [64]byte(a), [64]byte(b)))
}

func m512MaskAddEpi64(src [64]byte, k uint8, a [64]byte, b [64]byte) [64]byte


// M512MaskzAddEpi64: Add packed 64-bit integers in 'a' and 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] + b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPADDQ'. Intrinsic: '_mm512_maskz_add_epi64'.
// Requires AVX512F.
func M512MaskzAddEpi64(k x86.Mmask8, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzAddEpi64(uint8(k), [64]byte(a), [64]byte(b)))
}

func m512MaskzAddEpi64(k uint8, a [64]byte, b [64]byte) [64]byte


// M512MaskzAddPd: Add packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] + b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VADDPD'. Intrinsic: '_mm512_maskz_add_pd'.
// Requires AVX512F.
func M512MaskzAddPd(k x86.Mmask8, a x86.M512d, b x86.M512d) x86.M512d {
	return x86.M512d(m512MaskzAddPd(uint8(k), [8]float64(a), [8]float64(b)))
}

func m512MaskzAddPd(k uint8, a [8]float64, b [8]float64) [8]float64


// M512MaskzAddPs: Add packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] + b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VADDPS'. Intrinsic: '_mm512_maskz_add_ps'.
// Requires AVX512F.
func M512MaskzAddPs(k x86.Mmask16, a x86.M512, b x86.M512) x86.M512 {
	return x86.M512(m512MaskzAddPs(uint16(k), [16]float32(a), [16]float32(b)))
}

func m512MaskzAddPs(k uint16, a [16]float32, b [16]float32) [16]float32


// M512MaskzAddRoundPd: Add packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] + b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VADDPD'. Intrinsic: '_mm512_maskz_add_round_pd'.
// Requires AVX512F.
func M512MaskzAddRoundPd(k x86.Mmask8, a x86.M512d, b x86.M512d, rounding int) x86.M512d {
	return x86.M512d(m512MaskzAddRoundPd(uint8(k), [8]float64(a), [8]float64(b), rounding))
}

func m512MaskzAddRoundPd(k uint8, a [8]float64, b [8]float64, rounding int) [8]float64


// M512MaskzAddRoundPs: Add packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] + b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VADDPS'. Intrinsic: '_mm512_maskz_add_round_ps'.
// Requires AVX512F.
func M512MaskzAddRoundPs(k x86.Mmask16, a x86.M512, b x86.M512, rounding int) x86.M512 {
	return x86.M512(m512MaskzAddRoundPs(uint16(k), [16]float32(a), [16]float32(b), rounding))
}

func m512MaskzAddRoundPs(k uint16, a [16]float32, b [16]float32, rounding int) [16]float32


// AddRoundSd: Add the lower double-precision (64-bit) floating-point element
// in 'a' and 'b', store the result in the lower element of 'dst', and copy the
// upper element from 'a' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := a[63:0] + b[63:0]
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VADDSD'. Intrinsic: '_mm_add_round_sd'.
// Requires AVX512F.
func AddRoundSd(a x86.M128d, b x86.M128d, rounding int) x86.M128d {
	return x86.M128d(addRoundSd([2]float64(a), [2]float64(b), rounding))
}

func addRoundSd(a [2]float64, b [2]float64, rounding int) [2]float64


// MaskAddRoundSd: Add the lower double-precision (64-bit) floating-point
// element in 'a' and 'b', store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'src' when mask bit 0 is not set),
// and copy the upper element from 'a' to the upper element of 'dst'.
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := a[63:0] + b[63:0]
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VADDSD'. Intrinsic: '_mm_mask_add_round_sd'.
// Requires AVX512F.
func MaskAddRoundSd(src x86.M128d, k x86.Mmask8, a x86.M128d, b x86.M128d, rounding int) x86.M128d {
	return x86.M128d(maskAddRoundSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), rounding))
}

func maskAddRoundSd(src [2]float64, k uint8, a [2]float64, b [2]float64, rounding int) [2]float64


// MaskzAddRoundSd: Add the lower double-precision (64-bit) floating-point
// element in 'a' and 'b', store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper element from 'a' to the upper element of 'dst'.
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := a[63:0] + b[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VADDSD'. Intrinsic: '_mm_maskz_add_round_sd'.
// Requires AVX512F.
func MaskzAddRoundSd(k x86.Mmask8, a x86.M128d, b x86.M128d, rounding int) x86.M128d {
	return x86.M128d(maskzAddRoundSd(uint8(k), [2]float64(a), [2]float64(b), rounding))
}

func maskzAddRoundSd(k uint8, a [2]float64, b [2]float64, rounding int) [2]float64


// AddRoundSs: Add the lower single-precision (32-bit) floating-point element
// in 'a' and 'b', store the result in the lower element of 'dst', and copy the
// upper 3 packed elements from 'a' to the upper elements of 'dst'.
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := a[31:0] + b[31:0]
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VADDSS'. Intrinsic: '_mm_add_round_ss'.
// Requires AVX512F.
func AddRoundSs(a x86.M128, b x86.M128, rounding int) x86.M128 {
	return x86.M128(addRoundSs([4]float32(a), [4]float32(b), rounding))
}

func addRoundSs(a [4]float32, b [4]float32, rounding int) [4]float32


// MaskAddRoundSs: Add the lower single-precision (32-bit) floating-point
// element in 'a' and 'b', store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'src' when mask bit 0 is not set),
// and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'. 
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := a[31:0] + b[31:0]
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VADDSS'. Intrinsic: '_mm_mask_add_round_ss'.
// Requires AVX512F.
func MaskAddRoundSs(src x86.M128, k x86.Mmask8, a x86.M128, b x86.M128, rounding int) x86.M128 {
	return x86.M128(maskAddRoundSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), rounding))
}

func maskAddRoundSs(src [4]float32, k uint8, a [4]float32, b [4]float32, rounding int) [4]float32


// MaskzAddRoundSs: Add the lower single-precision (32-bit) floating-point
// element in 'a' and 'b', store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := a[31:0] + b[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VADDSS'. Intrinsic: '_mm_maskz_add_round_ss'.
// Requires AVX512F.
func MaskzAddRoundSs(k x86.Mmask8, a x86.M128, b x86.M128, rounding int) x86.M128 {
	return x86.M128(maskzAddRoundSs(uint8(k), [4]float32(a), [4]float32(b), rounding))
}

func maskzAddRoundSs(k uint8, a [4]float32, b [4]float32, rounding int) [4]float32


// MaskAddSd: Add the lower double-precision (64-bit) floating-point element in
// 'a' and 'b', store the result in the lower element of 'dst' using writemask
// 'k' (the element is copied from 'src' when mask bit 0 is not set), and copy
// the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := a[63:0] + b[63:0]
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VADDSD'. Intrinsic: '_mm_mask_add_sd'.
// Requires AVX512F.
func MaskAddSd(src x86.M128d, k x86.Mmask8, a x86.M128d, b x86.M128d) x86.M128d {
	return x86.M128d(maskAddSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskAddSd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzAddSd: Add the lower double-precision (64-bit) floating-point element
// in 'a' and 'b', store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := a[63:0] + b[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VADDSD'. Intrinsic: '_mm_maskz_add_sd'.
// Requires AVX512F.
func MaskzAddSd(k x86.Mmask8, a x86.M128d, b x86.M128d) x86.M128d {
	return x86.M128d(maskzAddSd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzAddSd(k uint8, a [2]float64, b [2]float64) [2]float64


// MaskAddSs: Add the lower single-precision (32-bit) floating-point element in
// 'a' and 'b', store the result in the lower element of 'dst' using writemask
// 'k' (the element is copied from 'src' when mask bit 0 is not set), and copy
// the upper 3 packed elements from 'a' to the upper elements of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := a[31:0] + b[31:0]
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VADDSS'. Intrinsic: '_mm_mask_add_ss'.
// Requires AVX512F.
func MaskAddSs(src x86.M128, k x86.Mmask8, a x86.M128, b x86.M128) x86.M128 {
	return x86.M128(maskAddSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskAddSs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzAddSs: Add the lower single-precision (32-bit) floating-point element
// in 'a' and 'b', store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := a[31:0] + b[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VADDSS'. Intrinsic: '_mm_maskz_add_ss'.
// Requires AVX512F.
func MaskzAddSs(k x86.Mmask8, a x86.M128, b x86.M128) x86.M128 {
	return x86.M128(maskzAddSs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzAddSs(k uint8, a [4]float32, b [4]float32) [4]float32


// M512MaskzAlignrEpi32: Concatenate 'a' and 'b' into a 128-byte immediate
// result, shift the result right by 'count' 32-bit elements, and stores the
// low 64 bytes (16 elements) in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		temp[1023:512] := a[511:0]
//		temp[511:0] := b[511:0]
//		temp[1023:0] := temp[1023:0] >> (32*count)
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := temp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VALIGND'. Intrinsic: '_mm512_maskz_alignr_epi32'.
// Requires AVX512F.
func M512MaskzAlignrEpi32(k x86.Mmask16, a x86.M512i, b x86.M512i, count int) x86.M512i {
	return x86.M512i(m512MaskzAlignrEpi32(uint16(k), [64]byte(a), [64]byte(b), count))
}

func m512MaskzAlignrEpi32(k uint16, a [64]byte, b [64]byte, count int) [64]byte


// M512AlignrEpi64: Concatenate 'a' and 'b' into a 128-byte immediate result,
// shift the result right by 'count' 64-bit elements, and store the low 64
// bytes (8 elements) in 'dst'. 
//
//		temp[1023:512] := a[511:0]
//		temp[511:0] := b[511:0]
//		temp[1023:0] := temp[1023:0] >> (64*count)
//		dst[511:0] := temp[511:0]
//		dst[MAX:512] := 0
//
// Instruction: 'VALIGNQ'. Intrinsic: '_mm512_alignr_epi64'.
// Requires AVX512F.
func M512AlignrEpi64(a x86.M512i, b x86.M512i, count int) x86.M512i {
	return x86.M512i(m512AlignrEpi64([64]byte(a), [64]byte(b), count))
}

func m512AlignrEpi64(a [64]byte, b [64]byte, count int) [64]byte


// M512MaskAlignrEpi64: Concatenate 'a' and 'b' into a 128-byte immediate
// result, shift the result right by 'count' 64-bit elements, and store the low
// 64 bytes (8 elements) in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		temp[1023:512] := a[511:0]
//		temp[511:0] := b[511:0]
//		temp[1023:0] := temp[1023:0] >> (64*count)
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := temp[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VALIGNQ'. Intrinsic: '_mm512_mask_alignr_epi64'.
// Requires AVX512F.
func M512MaskAlignrEpi64(src x86.M512i, k x86.Mmask8, a x86.M512i, b x86.M512i, count int) x86.M512i {
	return x86.M512i(m512MaskAlignrEpi64([64]byte(src), uint8(k), [64]byte(a), [64]byte(b), count))
}

func m512MaskAlignrEpi64(src [64]byte, k uint8, a [64]byte, b [64]byte, count int) [64]byte


// M512MaskzAlignrEpi64: Concatenate 'a' and 'b' into a 128-byte immediate
// result, shift the result right by 'count' 64-bit elements, and stores the
// low 64 bytes (8 elements) in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		temp[1023:512] := a[511:0]
//		temp[511:0] := b[511:0]
//		temp[1023:0] := temp[1023:0] >> (64*count)
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := temp[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VALIGNQ'. Intrinsic: '_mm512_maskz_alignr_epi64'.
// Requires AVX512F.
func M512MaskzAlignrEpi64(k x86.Mmask8, a x86.M512i, b x86.M512i, count int) x86.M512i {
	return x86.M512i(m512MaskzAlignrEpi64(uint8(k), [64]byte(a), [64]byte(b), count))
}

func m512MaskzAlignrEpi64(k uint8, a [64]byte, b [64]byte, count int) [64]byte


// MaskAndEpi32: Compute the bitwise AND of packed 32-bit integers in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] AND b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPANDD'. Intrinsic: '_mm_mask_and_epi32'.
// Requires AVX512F.
func MaskAndEpi32(src x86.M128i, k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskAndEpi32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskAndEpi32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzAndEpi32: Compute the bitwise AND of packed 32-bit integers in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] AND b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPANDD'. Intrinsic: '_mm_maskz_and_epi32'.
// Requires AVX512F.
func MaskzAndEpi32(k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskzAndEpi32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzAndEpi32(k uint8, a [16]byte, b [16]byte) [16]byte


// M256MaskAndEpi32: Compute the bitwise AND of packed 32-bit integers in 'a'
// and 'b', and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] BITWISE AND b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPANDD'. Intrinsic: '_mm256_mask_and_epi32'.
// Requires AVX512F.
func M256MaskAndEpi32(src x86.M256i, k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskAndEpi32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskAndEpi32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// M256MaskzAndEpi32: Compute the bitwise AND of packed 32-bit integers in 'a'
// and 'b', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] BITWISE AND b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPANDD'. Intrinsic: '_mm256_maskz_and_epi32'.
// Requires AVX512F.
func M256MaskzAndEpi32(k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzAndEpi32(uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskzAndEpi32(k uint8, a [32]byte, b [32]byte) [32]byte


// M512MaskzAndEpi32: Compute the bitwise AND of packed 32-bit integers in 'a'
// and 'b', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] AND b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPANDD'. Intrinsic: '_mm512_maskz_and_epi32'.
// Requires AVX512F.
func M512MaskzAndEpi32(k x86.Mmask16, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzAndEpi32(uint16(k), [64]byte(a), [64]byte(b)))
}

func m512MaskzAndEpi32(k uint16, a [64]byte, b [64]byte) [64]byte


// MaskAndEpi64: Compute the bitwise AND of packed 64-bit integers in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] AND b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPANDQ'. Intrinsic: '_mm_mask_and_epi64'.
// Requires AVX512F.
func MaskAndEpi64(src x86.M128i, k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskAndEpi64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskAndEpi64(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzAndEpi64: Compute the bitwise AND of packed 64-bit integers in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] AND b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPANDQ'. Intrinsic: '_mm_maskz_and_epi64'.
// Requires AVX512F.
func MaskzAndEpi64(k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskzAndEpi64(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzAndEpi64(k uint8, a [16]byte, b [16]byte) [16]byte


// M256MaskAndEpi64: Compute the bitwise AND of packed 64-bit integers in 'a'
// and 'b', and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] AND b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPANDQ'. Intrinsic: '_mm256_mask_and_epi64'.
// Requires AVX512F.
func M256MaskAndEpi64(src x86.M256i, k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskAndEpi64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskAndEpi64(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// M256MaskzAndEpi64: Compute the bitwise AND of packed 64-bit integers in 'a'
// and 'b', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] AND b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPANDQ'. Intrinsic: '_mm256_maskz_and_epi64'.
// Requires AVX512F.
func M256MaskzAndEpi64(k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzAndEpi64(uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskzAndEpi64(k uint8, a [32]byte, b [32]byte) [32]byte


// M512MaskzAndEpi64: Compute the bitwise AND of packed 64-bit integers in 'a'
// and 'b', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] AND b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPANDQ'. Intrinsic: '_mm512_maskz_and_epi64'.
// Requires AVX512F.
func M512MaskzAndEpi64(k x86.Mmask8, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzAndEpi64(uint8(k), [64]byte(a), [64]byte(b)))
}

func m512MaskzAndEpi64(k uint8, a [64]byte, b [64]byte) [64]byte


// MaskAndnotEpi32: Compute the bitwise AND NOT of packed 32-bit integers in
// 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ((NOT a[i+31:i]) AND b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPANDND'. Intrinsic: '_mm_mask_andnot_epi32'.
// Requires AVX512F.
func MaskAndnotEpi32(src x86.M128i, k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskAndnotEpi32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskAndnotEpi32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzAndnotEpi32: Compute the bitwise AND NOT of packed 32-bit integers in
// 'a' and 'b', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (NOT a[i+31:i]) AND b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPANDND'. Intrinsic: '_mm_maskz_andnot_epi32'.
// Requires AVX512F.
func MaskzAndnotEpi32(k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskzAndnotEpi32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzAndnotEpi32(k uint8, a [16]byte, b [16]byte) [16]byte


// M256MaskAndnotEpi32: Compute the bitwise AND NOT of packed 32-bit integers
// in 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ((NOT a[i+31:i]) AND b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPANDND'. Intrinsic: '_mm256_mask_andnot_epi32'.
// Requires AVX512F.
func M256MaskAndnotEpi32(src x86.M256i, k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskAndnotEpi32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskAndnotEpi32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// M256MaskzAndnotEpi32: Compute the bitwise AND NOT of packed 32-bit integers
// in 'a' and 'b', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (NOT a[i+31:i]) AND b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPANDND'. Intrinsic: '_mm256_maskz_andnot_epi32'.
// Requires AVX512F.
func M256MaskzAndnotEpi32(k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzAndnotEpi32(uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskzAndnotEpi32(k uint8, a [32]byte, b [32]byte) [32]byte


// M512MaskzAndnotEpi32: Compute the bitwise AND NOT of packed 32-bit integers
// in 'a' and 'b', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (NOT a[i+31:i]) AND b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPANDND'. Intrinsic: '_mm512_maskz_andnot_epi32'.
// Requires AVX512F.
func M512MaskzAndnotEpi32(k x86.Mmask16, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzAndnotEpi32(uint16(k), [64]byte(a), [64]byte(b)))
}

func m512MaskzAndnotEpi32(k uint16, a [64]byte, b [64]byte) [64]byte


// MaskAndnotEpi64: Compute the bitwise AND NOT of packed 64-bit integers in
// 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ((NOT a[i+63:i]) AND b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPANDNQ'. Intrinsic: '_mm_mask_andnot_epi64'.
// Requires AVX512F.
func MaskAndnotEpi64(src x86.M128i, k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskAndnotEpi64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskAndnotEpi64(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzAndnotEpi64: Compute the bitwise AND NOT of packed 64-bit integers in
// 'a' and 'b', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (NOT a[i+63:i]) AND b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPANDNQ'. Intrinsic: '_mm_maskz_andnot_epi64'.
// Requires AVX512F.
func MaskzAndnotEpi64(k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskzAndnotEpi64(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzAndnotEpi64(k uint8, a [16]byte, b [16]byte) [16]byte


// M256MaskAndnotEpi64: Compute the bitwise AND NOT of packed 64-bit integers
// in 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ((NOT a[i+63:i]) AND b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPANDNQ'. Intrinsic: '_mm256_mask_andnot_epi64'.
// Requires AVX512F.
func M256MaskAndnotEpi64(src x86.M256i, k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskAndnotEpi64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskAndnotEpi64(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// M256MaskzAndnotEpi64: Compute the bitwise AND NOT of packed 64-bit integers
// in 'a' and 'b', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (NOT a[i+63:i]) AND b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPANDNQ'. Intrinsic: '_mm256_maskz_andnot_epi64'.
// Requires AVX512F.
func M256MaskzAndnotEpi64(k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzAndnotEpi64(uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskzAndnotEpi64(k uint8, a [32]byte, b [32]byte) [32]byte


// M512MaskzAndnotEpi64: Compute the bitwise AND NOT of packed 64-bit integers
// in 'a' and 'b', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (NOT a[i+63:i]) AND b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPANDNQ'. Intrinsic: '_mm512_maskz_andnot_epi64'.
// Requires AVX512F.
func M512MaskzAndnotEpi64(k x86.Mmask8, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzAndnotEpi64(uint8(k), [64]byte(a), [64]byte(b)))
}

func m512MaskzAndnotEpi64(k uint8, a [64]byte, b [64]byte) [64]byte


// M512AsinPd: Compute the inverse sine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ASIN(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_asin_pd'.
// Requires AVX512F.
func M512AsinPd(a x86.M512d) x86.M512d {
	return x86.M512d(m512AsinPd([8]float64(a)))
}

func m512AsinPd(a [8]float64) [8]float64


// M512MaskAsinPd: Compute the inverse sine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ASIN(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_asin_pd'.
// Requires AVX512F.
func M512MaskAsinPd(src x86.M512d, k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskAsinPd([8]float64(src), uint8(k), [8]float64(a)))
}

func m512MaskAsinPd(src [8]float64, k uint8, a [8]float64) [8]float64


// M512AsinPs: Compute the inverse sine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ASIN(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_asin_ps'.
// Requires AVX512F.
func M512AsinPs(a x86.M512) x86.M512 {
	return x86.M512(m512AsinPs([16]float32(a)))
}

func m512AsinPs(a [16]float32) [16]float32


// M512MaskAsinPs: Compute the inverse sine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ASIN(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_asin_ps'.
// Requires AVX512F.
func M512MaskAsinPs(src x86.M512, k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskAsinPs([16]float32(src), uint16(k), [16]float32(a)))
}

func m512MaskAsinPs(src [16]float32, k uint16, a [16]float32) [16]float32


// M512AsinhPd: Compute the inverse hyperbolic sine of packed double-precision
// (64-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ASINH(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_asinh_pd'.
// Requires AVX512F.
func M512AsinhPd(a x86.M512d) x86.M512d {
	return x86.M512d(m512AsinhPd([8]float64(a)))
}

func m512AsinhPd(a [8]float64) [8]float64


// M512MaskAsinhPd: Compute the inverse hyperbolic sine of packed
// double-precision (64-bit) floating-point elements in 'a' expressed in
// radians, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ASINH(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_asinh_pd'.
// Requires AVX512F.
func M512MaskAsinhPd(src x86.M512d, k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskAsinhPd([8]float64(src), uint8(k), [8]float64(a)))
}

func m512MaskAsinhPd(src [8]float64, k uint8, a [8]float64) [8]float64


// M512AsinhPs: Compute the inverse hyperbolic sine of packed single-precision
// (32-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ASINH(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_asinh_ps'.
// Requires AVX512F.
func M512AsinhPs(a x86.M512) x86.M512 {
	return x86.M512(m512AsinhPs([16]float32(a)))
}

func m512AsinhPs(a [16]float32) [16]float32


// M512MaskAsinhPs: Compute the inverse hyperbolic sine of packed
// single-precision (32-bit) floating-point elements in 'a' expressed in
// radians, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ASINH(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_asinh_ps'.
// Requires AVX512F.
func M512MaskAsinhPs(src x86.M512, k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskAsinhPs([16]float32(src), uint16(k), [16]float32(a)))
}

func m512MaskAsinhPs(src [16]float32, k uint16, a [16]float32) [16]float32


// M512AtanPd: Compute the inverse tangent of packed double-precision (64-bit)
// floating-point elements in 'a' and store the results in 'dst' expressed in
// radians. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ATAN(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_atan_pd'.
// Requires AVX512F.
func M512AtanPd(a x86.M512d) x86.M512d {
	return x86.M512d(m512AtanPd([8]float64(a)))
}

func m512AtanPd(a [8]float64) [8]float64


// M512MaskAtanPd: Compute the inverse tangent of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'
// expressed in radians using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ATAN(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_atan_pd'.
// Requires AVX512F.
func M512MaskAtanPd(src x86.M512d, k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskAtanPd([8]float64(src), uint8(k), [8]float64(a)))
}

func m512MaskAtanPd(src [8]float64, k uint8, a [8]float64) [8]float64


// M512AtanPs: Compute the inverse tangent of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' expressed in
// radians. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ATAN(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_atan_ps'.
// Requires AVX512F.
func M512AtanPs(a x86.M512) x86.M512 {
	return x86.M512(m512AtanPs([16]float32(a)))
}

func m512AtanPs(a [16]float32) [16]float32


// M512MaskAtanPs: Compute the inverse tangent of packed single-precision
// (32-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ATAN(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_atan_ps'.
// Requires AVX512F.
func M512MaskAtanPs(src x86.M512, k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskAtanPs([16]float32(src), uint16(k), [16]float32(a)))
}

func m512MaskAtanPs(src [16]float32, k uint16, a [16]float32) [16]float32


// M512Atan2Pd: Compute the inverse tangent of packed double-precision (64-bit)
// floating-point elements in 'a' divided by packed elements in 'b', and store
// the results in 'dst' expressed in radians. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ATAN(a[i+63:i] / b[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_atan2_pd'.
// Requires AVX512F.
func M512Atan2Pd(a x86.M512d, b x86.M512d) x86.M512d {
	return x86.M512d(m512Atan2Pd([8]float64(a), [8]float64(b)))
}

func m512Atan2Pd(a [8]float64, b [8]float64) [8]float64


// M512MaskAtan2Pd: Compute the inverse tangent of packed double-precision
// (64-bit) floating-point elements in 'a' divided by packed elements in 'b',
// and store the results in 'dst' expressed in radians using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ATAN(a[i+63:i] / b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_atan2_pd'.
// Requires AVX512F.
func M512MaskAtan2Pd(src x86.M512d, k x86.Mmask8, a x86.M512d, b x86.M512d) x86.M512d {
	return x86.M512d(m512MaskAtan2Pd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b)))
}

func m512MaskAtan2Pd(src [8]float64, k uint8, a [8]float64, b [8]float64) [8]float64


// M512Atan2Ps: Compute the inverse tangent of packed single-precision (32-bit)
// floating-point elements in 'a' divided by packed elements in 'b', and store
// the results in 'dst' expressed in radians. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ATAN(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_atan2_ps'.
// Requires AVX512F.
func M512Atan2Ps(a x86.M512, b x86.M512) x86.M512 {
	return x86.M512(m512Atan2Ps([16]float32(a), [16]float32(b)))
}

func m512Atan2Ps(a [16]float32, b [16]float32) [16]float32


// M512MaskAtan2Ps: Compute the inverse tangent of packed single-precision
// (32-bit) floating-point elements in 'a' divided by packed elements in 'b',
// and store the results in 'dst' expressed in radians using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ATAN(a[i+31:i] / b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_atan2_ps'.
// Requires AVX512F.
func M512MaskAtan2Ps(src x86.M512, k x86.Mmask16, a x86.M512, b x86.M512) x86.M512 {
	return x86.M512(m512MaskAtan2Ps([16]float32(src), uint16(k), [16]float32(a), [16]float32(b)))
}

func m512MaskAtan2Ps(src [16]float32, k uint16, a [16]float32, b [16]float32) [16]float32


// M512AtanhPd: Compute the inverse hyperbolic tangent of packed
// double-precision (64-bit) floating-point elements in 'a' and store the
// results in 'dst' expressed in radians. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ATANH(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_atanh_pd'.
// Requires AVX512F.
func M512AtanhPd(a x86.M512d) x86.M512d {
	return x86.M512d(m512AtanhPd([8]float64(a)))
}

func m512AtanhPd(a [8]float64) [8]float64


// M512MaskAtanhPd: Compute the inverse hyperbolic tangent of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst' expressed in radians using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ATANH(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_atanh_pd'.
// Requires AVX512F.
func M512MaskAtanhPd(src x86.M512d, k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskAtanhPd([8]float64(src), uint8(k), [8]float64(a)))
}

func m512MaskAtanhPd(src [8]float64, k uint8, a [8]float64) [8]float64


// M512AtanhPs: Compute the inverse hyperblic tangent of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst' expressed in radians. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ATANH(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_atanh_ps'.
// Requires AVX512F.
func M512AtanhPs(a x86.M512) x86.M512 {
	return x86.M512(m512AtanhPs([16]float32(a)))
}

func m512AtanhPs(a [16]float32) [16]float32


// M512MaskAtanhPs: Compute the inverse hyperbolic tangent of packed
// single-precision (32-bit) floating-point elements in 'a' expressed in
// radians, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ATANH(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_atanh_ps'.
// Requires AVX512F.
func M512MaskAtanhPs(src x86.M512, k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskAtanhPs([16]float32(src), uint16(k), [16]float32(a)))
}

func m512MaskAtanhPs(src [16]float32, k uint16, a [16]float32) [16]float32


// MaskBlendEpi32: Blend packed 32-bit integers from 'a' and 'b' using control
// mask 'k', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := b[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBLENDMD'. Intrinsic: '_mm_mask_blend_epi32'.
// Requires AVX512F.
func MaskBlendEpi32(k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskBlendEpi32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskBlendEpi32(k uint8, a [16]byte, b [16]byte) [16]byte


// M256MaskBlendEpi32: Blend packed 32-bit integers from 'a' and 'b' using
// control mask 'k', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := b[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBLENDMD'. Intrinsic: '_mm256_mask_blend_epi32'.
// Requires AVX512F.
func M256MaskBlendEpi32(k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskBlendEpi32(uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskBlendEpi32(k uint8, a [32]byte, b [32]byte) [32]byte


// MaskBlendEpi64: Blend packed 64-bit integers from 'a' and 'b' using control
// mask 'k', and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := b[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBLENDMQ'. Intrinsic: '_mm_mask_blend_epi64'.
// Requires AVX512F.
func MaskBlendEpi64(k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskBlendEpi64(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskBlendEpi64(k uint8, a [16]byte, b [16]byte) [16]byte


// M256MaskBlendEpi64: Blend packed 64-bit integers from 'a' and 'b' using
// control mask 'k', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := b[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBLENDMQ'. Intrinsic: '_mm256_mask_blend_epi64'.
// Requires AVX512F.
func M256MaskBlendEpi64(k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskBlendEpi64(uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskBlendEpi64(k uint8, a [32]byte, b [32]byte) [32]byte


// MaskBlendPd: Blend packed double-precision (64-bit) floating-point elements
// from 'a' and 'b' using control mask 'k', and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := b[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VBLENDMPD'. Intrinsic: '_mm_mask_blend_pd'.
// Requires AVX512F.
func MaskBlendPd(k x86.Mmask8, a x86.M128d, b x86.M128d) x86.M128d {
	return x86.M128d(maskBlendPd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskBlendPd(k uint8, a [2]float64, b [2]float64) [2]float64


// M256MaskBlendPd: Blend packed double-precision (64-bit) floating-point
// elements from 'a' and 'b' using control mask 'k', and store the results in
// 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := b[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBLENDMPD'. Intrinsic: '_mm256_mask_blend_pd'.
// Requires AVX512F.
func M256MaskBlendPd(k x86.Mmask8, a x86.M256d, b x86.M256d) x86.M256d {
	return x86.M256d(m256MaskBlendPd(uint8(k), [4]float64(a), [4]float64(b)))
}

func m256MaskBlendPd(k uint8, a [4]float64, b [4]float64) [4]float64


// MaskBlendPs: Blend packed single-precision (32-bit) floating-point elements
// from 'a' and 'b' using control mask 'k', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := b[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VBLENDMPS'. Intrinsic: '_mm_mask_blend_ps'.
// Requires AVX512F.
func MaskBlendPs(k x86.Mmask8, a x86.M128, b x86.M128) x86.M128 {
	return x86.M128(maskBlendPs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskBlendPs(k uint8, a [4]float32, b [4]float32) [4]float32


// M256MaskBlendPs: Blend packed single-precision (32-bit) floating-point
// elements from 'a' and 'b' using control mask 'k', and store the results in
// 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := b[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBLENDMPS'. Intrinsic: '_mm256_mask_blend_ps'.
// Requires AVX512F.
func M256MaskBlendPs(k x86.Mmask8, a x86.M256, b x86.M256) x86.M256 {
	return x86.M256(m256MaskBlendPs(uint8(k), [8]float32(a), [8]float32(b)))
}

func m256MaskBlendPs(k uint8, a [8]float32, b [8]float32) [8]float32


// M256BroadcastF32x4: Broadcast the 4 packed single-precision (32-bit)
// floating-point elements from 'a' to all elements of 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			n := (j mod 4)*32
//			dst[i+31:i] := a[n+31:n]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTF32X4'. Intrinsic: '_mm256_broadcast_f32x4'.
// Requires AVX512F.
func M256BroadcastF32x4(a x86.M128) x86.M256 {
	return x86.M256(m256BroadcastF32x4([4]float32(a)))
}

func m256BroadcastF32x4(a [4]float32) [8]float32


// M256MaskBroadcastF32x4: Broadcast the 4 packed single-precision (32-bit)
// floating-point elements from 'a' to all elements of 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			n := (j mod 4)*32
//			IF k[j]
//				dst[i+31:i] := a[n+31:n]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTF32X4'. Intrinsic: '_mm256_mask_broadcast_f32x4'.
// Requires AVX512F.
func M256MaskBroadcastF32x4(src x86.M256, k x86.Mmask8, a x86.M128) x86.M256 {
	return x86.M256(m256MaskBroadcastF32x4([8]float32(src), uint8(k), [4]float32(a)))
}

func m256MaskBroadcastF32x4(src [8]float32, k uint8, a [4]float32) [8]float32


// M256MaskzBroadcastF32x4: Broadcast the 4 packed single-precision (32-bit)
// floating-point elements from 'a' to all elements of 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			n := (j mod 4)*32
//			IF k[j]
//				dst[i+31:i] := a[n+31:n]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTF32X4'. Intrinsic: '_mm256_maskz_broadcast_f32x4'.
// Requires AVX512F.
func M256MaskzBroadcastF32x4(k x86.Mmask8, a x86.M128) x86.M256 {
	return x86.M256(m256MaskzBroadcastF32x4(uint8(k), [4]float32(a)))
}

func m256MaskzBroadcastF32x4(k uint8, a [4]float32) [8]float32


// M512BroadcastF32x4: Broadcast the 4 packed single-precision (32-bit)
// floating-point elements from 'a' to all elements of 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			n := (j mod 4)*32
//			dst[i+31:i] := a[n+31:n]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTF32X4'. Intrinsic: '_mm512_broadcast_f32x4'.
// Requires AVX512F.
func M512BroadcastF32x4(a x86.M128) x86.M512 {
	return x86.M512(m512BroadcastF32x4([4]float32(a)))
}

func m512BroadcastF32x4(a [4]float32) [16]float32


// M512MaskBroadcastF32x4: Broadcast the 4 packed single-precision (32-bit)
// floating-point elements from 'a' to all elements of 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			n := (j mod 4)*32
//			IF k[j]
//				dst[i+31:i] := a[n+31:n]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTF32X4'. Intrinsic: '_mm512_mask_broadcast_f32x4'.
// Requires AVX512F.
func M512MaskBroadcastF32x4(src x86.M512, k x86.Mmask16, a x86.M128) x86.M512 {
	return x86.M512(m512MaskBroadcastF32x4([16]float32(src), uint16(k), [4]float32(a)))
}

func m512MaskBroadcastF32x4(src [16]float32, k uint16, a [4]float32) [16]float32


// M512MaskzBroadcastF32x4: Broadcast the 4 packed single-precision (32-bit)
// floating-point elements from 'a' to all elements of 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			n := (j mod 4)*32
//			IF k[j]
//				dst[i+31:i] := a[n+31:n]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTF32X4'. Intrinsic: '_mm512_maskz_broadcast_f32x4'.
// Requires AVX512F.
func M512MaskzBroadcastF32x4(k x86.Mmask16, a x86.M128) x86.M512 {
	return x86.M512(m512MaskzBroadcastF32x4(uint16(k), [4]float32(a)))
}

func m512MaskzBroadcastF32x4(k uint16, a [4]float32) [16]float32


// M512BroadcastF64x4: Broadcast the 4 packed double-precision (64-bit)
// floating-point elements from 'a' to all elements of 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			n := (j mod 4)*64
//			dst[i+63:i] := a[n+63:n]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTF64X4'. Intrinsic: '_mm512_broadcast_f64x4'.
// Requires AVX512F.
func M512BroadcastF64x4(a x86.M256d) x86.M512d {
	return x86.M512d(m512BroadcastF64x4([4]float64(a)))
}

func m512BroadcastF64x4(a [4]float64) [8]float64


// M512MaskBroadcastF64x4: Broadcast the 4 packed double-precision (64-bit)
// floating-point elements from 'a' to all elements of 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			n := (j mod 4)*64
//			IF k[j]
//				dst[i+63:i] := a[n+63:n]
//			ELSE
//				dst[i+63:i] := src[n+63:n]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTF64X4'. Intrinsic: '_mm512_mask_broadcast_f64x4'.
// Requires AVX512F.
func M512MaskBroadcastF64x4(src x86.M512d, k x86.Mmask8, a x86.M256d) x86.M512d {
	return x86.M512d(m512MaskBroadcastF64x4([8]float64(src), uint8(k), [4]float64(a)))
}

func m512MaskBroadcastF64x4(src [8]float64, k uint8, a [4]float64) [8]float64


// M512MaskzBroadcastF64x4: Broadcast the 4 packed double-precision (64-bit)
// floating-point elements from 'a' to all elements of 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			n := (j mod 4)*64
//			IF k[j]
//				dst[i+63:i] := a[n+63:n]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTF64X4'. Intrinsic: '_mm512_maskz_broadcast_f64x4'.
// Requires AVX512F.
func M512MaskzBroadcastF64x4(k x86.Mmask8, a x86.M256d) x86.M512d {
	return x86.M512d(m512MaskzBroadcastF64x4(uint8(k), [4]float64(a)))
}

func m512MaskzBroadcastF64x4(k uint8, a [4]float64) [8]float64


// M256BroadcastI32x4: Broadcast the 4 packed 32-bit integers from 'a' to all
// elements of 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			n := (j mod 4)*32
//			dst[i+31:i] := a[n+31:n]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTI32X4'. Intrinsic: '_mm256_broadcast_i32x4'.
// Requires AVX512F.
func M256BroadcastI32x4(a x86.M128i) x86.M256i {
	return x86.M256i(m256BroadcastI32x4([16]byte(a)))
}

func m256BroadcastI32x4(a [16]byte) [32]byte


// M256MaskBroadcastI32x4: Broadcast the 4 packed 32-bit integers from 'a' to
// all elements of 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			n := (j mod 4)*32
//			IF k[j]
//				dst[i+31:i] := a[n+31:n]
//			ELSE
//				dst[i+31:i] := src[n+31:n]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTI32X4'. Intrinsic: '_mm256_mask_broadcast_i32x4'.
// Requires AVX512F.
func M256MaskBroadcastI32x4(src x86.M256i, k x86.Mmask8, a x86.M128i) x86.M256i {
	return x86.M256i(m256MaskBroadcastI32x4([32]byte(src), uint8(k), [16]byte(a)))
}

func m256MaskBroadcastI32x4(src [32]byte, k uint8, a [16]byte) [32]byte


// M256MaskzBroadcastI32x4: Broadcast the 4 packed 32-bit integers from 'a' to
// all elements of 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			n := (j mod 4)*32
//			IF k[j]
//				dst[i+31:i] := a[n+31:n]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTI32X4'. Intrinsic: '_mm256_maskz_broadcast_i32x4'.
// Requires AVX512F.
func M256MaskzBroadcastI32x4(k x86.Mmask8, a x86.M128i) x86.M256i {
	return x86.M256i(m256MaskzBroadcastI32x4(uint8(k), [16]byte(a)))
}

func m256MaskzBroadcastI32x4(k uint8, a [16]byte) [32]byte


// M512BroadcastI32x4: Broadcast the 4 packed 32-bit integers from 'a' to all
// elements of 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			n := (j mod 4)*32
//			dst[i+31:i] := a[n+31:n]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTI32X4'. Intrinsic: '_mm512_broadcast_i32x4'.
// Requires AVX512F.
func M512BroadcastI32x4(a x86.M128i) x86.M512i {
	return x86.M512i(m512BroadcastI32x4([16]byte(a)))
}

func m512BroadcastI32x4(a [16]byte) [64]byte


// M512MaskBroadcastI32x4: Broadcast the 4 packed 32-bit integers from 'a' to
// all elements of 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			n := (j mod 4)*32
//			IF k[j]
//				dst[i+31:i] := a[n+31:n]
//			ELSE
//				dst[i+31:i] := src[n+31:n]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTI32X4'. Intrinsic: '_mm512_mask_broadcast_i32x4'.
// Requires AVX512F.
func M512MaskBroadcastI32x4(src x86.M512i, k x86.Mmask16, a x86.M128i) x86.M512i {
	return x86.M512i(m512MaskBroadcastI32x4([64]byte(src), uint16(k), [16]byte(a)))
}

func m512MaskBroadcastI32x4(src [64]byte, k uint16, a [16]byte) [64]byte


// M512MaskzBroadcastI32x4: Broadcast the 4 packed 32-bit integers from 'a' to
// all elements of 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			n := (j mod 4)*32
//			IF k[j]
//				dst[i+31:i] := a[n+31:n]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTI32X4'. Intrinsic: '_mm512_maskz_broadcast_i32x4'.
// Requires AVX512F.
func M512MaskzBroadcastI32x4(k x86.Mmask16, a x86.M128i) x86.M512i {
	return x86.M512i(m512MaskzBroadcastI32x4(uint16(k), [16]byte(a)))
}

func m512MaskzBroadcastI32x4(k uint16, a [16]byte) [64]byte


// M512BroadcastI64x4: Broadcast the 4 packed 64-bit integers from 'a' to all
// elements of 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			n := (j mod 4)*64
//			dst[i+63:i] := a[n+63:n]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTI64X4'. Intrinsic: '_mm512_broadcast_i64x4'.
// Requires AVX512F.
func M512BroadcastI64x4(a x86.M256i) x86.M512i {
	return x86.M512i(m512BroadcastI64x4([32]byte(a)))
}

func m512BroadcastI64x4(a [32]byte) [64]byte


// M512MaskBroadcastI64x4: Broadcast the 4 packed 64-bit integers from 'a' to
// all elements of 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			n := (j mod 4)*64
//			IF k[j]
//				dst[i+63:i] := a[n+63:n]
//			ELSE
//				dst[i+63:i] := src[n+63:n]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTI64X4'. Intrinsic: '_mm512_mask_broadcast_i64x4'.
// Requires AVX512F.
func M512MaskBroadcastI64x4(src x86.M512i, k x86.Mmask8, a x86.M256i) x86.M512i {
	return x86.M512i(m512MaskBroadcastI64x4([64]byte(src), uint8(k), [32]byte(a)))
}

func m512MaskBroadcastI64x4(src [64]byte, k uint8, a [32]byte) [64]byte


// M512MaskzBroadcastI64x4: Broadcast the 4 packed 64-bit integers from 'a' to
// all elements of 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			n := (j mod 4)*64
//			IF k[j]
//				dst[i+63:i] := a[n+63:n]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTI64X4'. Intrinsic: '_mm512_maskz_broadcast_i64x4'.
// Requires AVX512F.
func M512MaskzBroadcastI64x4(k x86.Mmask8, a x86.M256i) x86.M512i {
	return x86.M512i(m512MaskzBroadcastI64x4(uint8(k), [32]byte(a)))
}

func m512MaskzBroadcastI64x4(k uint8, a [32]byte) [64]byte


// MaskBroadcastdEpi32: Broadcast the low packed 32-bit integer from 'a' to all
// elements of 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm_mask_broadcastd_epi32'.
// Requires AVX512F.
func MaskBroadcastdEpi32(src x86.M128i, k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskBroadcastdEpi32([16]byte(src), uint8(k), [16]byte(a)))
}

func maskBroadcastdEpi32(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzBroadcastdEpi32: Broadcast the low packed 32-bit integer from 'a' to
// all elements of 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm_maskz_broadcastd_epi32'.
// Requires AVX512F.
func MaskzBroadcastdEpi32(k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskzBroadcastdEpi32(uint8(k), [16]byte(a)))
}

func maskzBroadcastdEpi32(k uint8, a [16]byte) [16]byte


// M256MaskBroadcastdEpi32: Broadcast the low packed 32-bit integer from 'a' to
// all elements of 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm256_mask_broadcastd_epi32'.
// Requires AVX512F.
func M256MaskBroadcastdEpi32(src x86.M256i, k x86.Mmask8, a x86.M128i) x86.M256i {
	return x86.M256i(m256MaskBroadcastdEpi32([32]byte(src), uint8(k), [16]byte(a)))
}

func m256MaskBroadcastdEpi32(src [32]byte, k uint8, a [16]byte) [32]byte


// M256MaskzBroadcastdEpi32: Broadcast the low packed 32-bit integer from 'a'
// to all elements of 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm256_maskz_broadcastd_epi32'.
// Requires AVX512F.
func M256MaskzBroadcastdEpi32(k x86.Mmask8, a x86.M128i) x86.M256i {
	return x86.M256i(m256MaskzBroadcastdEpi32(uint8(k), [16]byte(a)))
}

func m256MaskzBroadcastdEpi32(k uint8, a [16]byte) [32]byte


// M512BroadcastdEpi32: Broadcast the low packed 32-bit integer from 'a' to all
// elements of 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := a[31:0]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm512_broadcastd_epi32'.
// Requires AVX512F.
func M512BroadcastdEpi32(a x86.M128i) x86.M512i {
	return x86.M512i(m512BroadcastdEpi32([16]byte(a)))
}

func m512BroadcastdEpi32(a [16]byte) [64]byte


// M512MaskBroadcastdEpi32: Broadcast the low packed 32-bit integer from 'a' to
// all elements of 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm512_mask_broadcastd_epi32'.
// Requires AVX512F.
func M512MaskBroadcastdEpi32(src x86.M512i, k x86.Mmask16, a x86.M128i) x86.M512i {
	return x86.M512i(m512MaskBroadcastdEpi32([64]byte(src), uint16(k), [16]byte(a)))
}

func m512MaskBroadcastdEpi32(src [64]byte, k uint16, a [16]byte) [64]byte


// M512MaskzBroadcastdEpi32: Broadcast the low packed 32-bit integer from 'a'
// to all elements of 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm512_maskz_broadcastd_epi32'.
// Requires AVX512F.
func M512MaskzBroadcastdEpi32(k x86.Mmask16, a x86.M128i) x86.M512i {
	return x86.M512i(m512MaskzBroadcastdEpi32(uint16(k), [16]byte(a)))
}

func m512MaskzBroadcastdEpi32(k uint16, a [16]byte) [64]byte


// MaskBroadcastqEpi64: Broadcast the low packed 64-bit integer from 'a' to all
// elements of 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm_mask_broadcastq_epi64'.
// Requires AVX512F.
func MaskBroadcastqEpi64(src x86.M128i, k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskBroadcastqEpi64([16]byte(src), uint8(k), [16]byte(a)))
}

func maskBroadcastqEpi64(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzBroadcastqEpi64: Broadcast the low packed 64-bit integer from 'a' to
// all elements of 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm_maskz_broadcastq_epi64'.
// Requires AVX512F.
func MaskzBroadcastqEpi64(k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskzBroadcastqEpi64(uint8(k), [16]byte(a)))
}

func maskzBroadcastqEpi64(k uint8, a [16]byte) [16]byte


// M256MaskBroadcastqEpi64: Broadcast the low packed 64-bit integer from 'a' to
// all elements of 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm256_mask_broadcastq_epi64'.
// Requires AVX512F.
func M256MaskBroadcastqEpi64(src x86.M256i, k x86.Mmask8, a x86.M128i) x86.M256i {
	return x86.M256i(m256MaskBroadcastqEpi64([32]byte(src), uint8(k), [16]byte(a)))
}

func m256MaskBroadcastqEpi64(src [32]byte, k uint8, a [16]byte) [32]byte


// M256MaskzBroadcastqEpi64: Broadcast the low packed 64-bit integer from 'a'
// to all elements of 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm256_maskz_broadcastq_epi64'.
// Requires AVX512F.
func M256MaskzBroadcastqEpi64(k x86.Mmask8, a x86.M128i) x86.M256i {
	return x86.M256i(m256MaskzBroadcastqEpi64(uint8(k), [16]byte(a)))
}

func m256MaskzBroadcastqEpi64(k uint8, a [16]byte) [32]byte


// M512BroadcastqEpi64: Broadcast the low packed 64-bit integer from 'a' to all
// elements of 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := a[63:0]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm512_broadcastq_epi64'.
// Requires AVX512F.
func M512BroadcastqEpi64(a x86.M128i) x86.M512i {
	return x86.M512i(m512BroadcastqEpi64([16]byte(a)))
}

func m512BroadcastqEpi64(a [16]byte) [64]byte


// M512MaskBroadcastqEpi64: Broadcast the low packed 64-bit integer from 'a' to
// all elements of 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm512_mask_broadcastq_epi64'.
// Requires AVX512F.
func M512MaskBroadcastqEpi64(src x86.M512i, k x86.Mmask8, a x86.M128i) x86.M512i {
	return x86.M512i(m512MaskBroadcastqEpi64([64]byte(src), uint8(k), [16]byte(a)))
}

func m512MaskBroadcastqEpi64(src [64]byte, k uint8, a [16]byte) [64]byte


// M512MaskzBroadcastqEpi64: Broadcast the low packed 64-bit integer from 'a'
// to all elements of 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm512_maskz_broadcastq_epi64'.
// Requires AVX512F.
func M512MaskzBroadcastqEpi64(k x86.Mmask8, a x86.M128i) x86.M512i {
	return x86.M512i(m512MaskzBroadcastqEpi64(uint8(k), [16]byte(a)))
}

func m512MaskzBroadcastqEpi64(k uint8, a [16]byte) [64]byte


// M256MaskBroadcastsdPd: Broadcast the low double-precision (64-bit)
// floating-point element from 'a' to all elements of 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTSD'. Intrinsic: '_mm256_mask_broadcastsd_pd'.
// Requires AVX512F.
func M256MaskBroadcastsdPd(src x86.M256d, k x86.Mmask8, a x86.M128d) x86.M256d {
	return x86.M256d(m256MaskBroadcastsdPd([4]float64(src), uint8(k), [2]float64(a)))
}

func m256MaskBroadcastsdPd(src [4]float64, k uint8, a [2]float64) [4]float64


// M256MaskzBroadcastsdPd: Broadcast the low double-precision (64-bit)
// floating-point element from 'a' to all elements of 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTSD'. Intrinsic: '_mm256_maskz_broadcastsd_pd'.
// Requires AVX512F.
func M256MaskzBroadcastsdPd(k x86.Mmask8, a x86.M128d) x86.M256d {
	return x86.M256d(m256MaskzBroadcastsdPd(uint8(k), [2]float64(a)))
}

func m256MaskzBroadcastsdPd(k uint8, a [2]float64) [4]float64


// M512BroadcastsdPd: Broadcast the low double-precision (64-bit)
// floating-point element from 'a' to all elements of 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := a[63:0]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTSD'. Intrinsic: '_mm512_broadcastsd_pd'.
// Requires AVX512F.
func M512BroadcastsdPd(a x86.M128d) x86.M512d {
	return x86.M512d(m512BroadcastsdPd([2]float64(a)))
}

func m512BroadcastsdPd(a [2]float64) [8]float64


// M512MaskBroadcastsdPd: Broadcast the low double-precision (64-bit)
// floating-point element from 'a' to all elements of 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTSD'. Intrinsic: '_mm512_mask_broadcastsd_pd'.
// Requires AVX512F.
func M512MaskBroadcastsdPd(src x86.M512d, k x86.Mmask8, a x86.M128d) x86.M512d {
	return x86.M512d(m512MaskBroadcastsdPd([8]float64(src), uint8(k), [2]float64(a)))
}

func m512MaskBroadcastsdPd(src [8]float64, k uint8, a [2]float64) [8]float64


// M512MaskzBroadcastsdPd: Broadcast the low double-precision (64-bit)
// floating-point element from 'a' to all elements of 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTSD'. Intrinsic: '_mm512_maskz_broadcastsd_pd'.
// Requires AVX512F.
func M512MaskzBroadcastsdPd(k x86.Mmask8, a x86.M128d) x86.M512d {
	return x86.M512d(m512MaskzBroadcastsdPd(uint8(k), [2]float64(a)))
}

func m512MaskzBroadcastsdPd(k uint8, a [2]float64) [8]float64


// MaskBroadcastssPs: Broadcast the low single-precision (32-bit)
// floating-point element from 'a' to all elements of 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VBROADCASTSS'. Intrinsic: '_mm_mask_broadcastss_ps'.
// Requires AVX512F.
func MaskBroadcastssPs(src x86.M128, k x86.Mmask8, a x86.M128) x86.M128 {
	return x86.M128(maskBroadcastssPs([4]float32(src), uint8(k), [4]float32(a)))
}

func maskBroadcastssPs(src [4]float32, k uint8, a [4]float32) [4]float32


// MaskzBroadcastssPs: Broadcast the low single-precision (32-bit)
// floating-point element from 'a' to all elements of 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VBROADCASTSS'. Intrinsic: '_mm_maskz_broadcastss_ps'.
// Requires AVX512F.
func MaskzBroadcastssPs(k x86.Mmask8, a x86.M128) x86.M128 {
	return x86.M128(maskzBroadcastssPs(uint8(k), [4]float32(a)))
}

func maskzBroadcastssPs(k uint8, a [4]float32) [4]float32


// M256MaskBroadcastssPs: Broadcast the low single-precision (32-bit)
// floating-point element from 'a' to all elements of 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTSS'. Intrinsic: '_mm256_mask_broadcastss_ps'.
// Requires AVX512F.
func M256MaskBroadcastssPs(src x86.M256, k x86.Mmask8, a x86.M128) x86.M256 {
	return x86.M256(m256MaskBroadcastssPs([8]float32(src), uint8(k), [4]float32(a)))
}

func m256MaskBroadcastssPs(src [8]float32, k uint8, a [4]float32) [8]float32


// M256MaskzBroadcastssPs: Broadcast the low single-precision (32-bit)
// floating-point element from 'a' to all elements of 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTSS'. Intrinsic: '_mm256_maskz_broadcastss_ps'.
// Requires AVX512F.
func M256MaskzBroadcastssPs(k x86.Mmask8, a x86.M128) x86.M256 {
	return x86.M256(m256MaskzBroadcastssPs(uint8(k), [4]float32(a)))
}

func m256MaskzBroadcastssPs(k uint8, a [4]float32) [8]float32


// M512BroadcastssPs: Broadcast the low single-precision (32-bit)
// floating-point element from 'a' to all elements of 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := a[31:0]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTSS'. Intrinsic: '_mm512_broadcastss_ps'.
// Requires AVX512F.
func M512BroadcastssPs(a x86.M128) x86.M512 {
	return x86.M512(m512BroadcastssPs([4]float32(a)))
}

func m512BroadcastssPs(a [4]float32) [16]float32


// M512MaskBroadcastssPs: Broadcast the low single-precision (32-bit)
// floating-point element from 'a' to all elements of 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTSS'. Intrinsic: '_mm512_mask_broadcastss_ps'.
// Requires AVX512F.
func M512MaskBroadcastssPs(src x86.M512, k x86.Mmask16, a x86.M128) x86.M512 {
	return x86.M512(m512MaskBroadcastssPs([16]float32(src), uint16(k), [4]float32(a)))
}

func m512MaskBroadcastssPs(src [16]float32, k uint16, a [4]float32) [16]float32


// M512MaskzBroadcastssPs: Broadcast the low single-precision (32-bit)
// floating-point element from 'a' to all elements of 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTSS'. Intrinsic: '_mm512_maskz_broadcastss_ps'.
// Requires AVX512F.
func M512MaskzBroadcastssPs(k x86.Mmask16, a x86.M128) x86.M512 {
	return x86.M512(m512MaskzBroadcastssPs(uint16(k), [4]float32(a)))
}

func m512MaskzBroadcastssPs(k uint16, a [4]float32) [16]float32


// M512Castpd128Pd512: Cast vector of type __m128d to type __m512d; the upper
// 384 bits of the result are undefined. 
// 	This intrinsic is only used for compilation and does not generate any
// instructions, thus it has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_castpd128_pd512'.
// Requires AVX512F.
func M512Castpd128Pd512(a x86.M128d) x86.M512d {
	return x86.M512d(m512Castpd128Pd512([2]float64(a)))
}

func m512Castpd128Pd512(a [2]float64) [8]float64


// M512Castpd256Pd512: Cast vector of type __m256d to type __m512d; the upper
// 256 bits of the result are undefined. 
// 	This intrinsic is only used for compilation and does not generate any
// instructions, thus it has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_castpd256_pd512'.
// Requires AVX512F.
func M512Castpd256Pd512(a x86.M256d) x86.M512d {
	return x86.M512d(m512Castpd256Pd512([4]float64(a)))
}

func m512Castpd256Pd512(a [4]float64) [8]float64


// M512Castpd512Pd128: Cast vector of type __m512d to type __m128d. 
// 	This intrinsic is only used for compilation and does not generate any
// instructions, thus it has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_castpd512_pd128'.
// Requires AVX512F.
func M512Castpd512Pd128(a x86.M512d) x86.M128d {
	return x86.M128d(m512Castpd512Pd128([8]float64(a)))
}

func m512Castpd512Pd128(a [8]float64) [2]float64


// M512Castpd512Pd256: Cast vector of type __m512d to type __m256d. 
// 	This intrinsic is only used for compilation and does not generate any
// instructions, thus it has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_castpd512_pd256'.
// Requires AVX512F.
func M512Castpd512Pd256(a x86.M512d) x86.M256d {
	return x86.M256d(m512Castpd512Pd256([8]float64(a)))
}

func m512Castpd512Pd256(a [8]float64) [4]float64


// M512Castps128Ps512: Cast vector of type __m128 to type __m512; the upper 384
// bits of the result are undefined. 
// 	This intrinsic is only used for compilation and does not generate any
// instructions, thus it has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_castps128_ps512'.
// Requires AVX512F.
func M512Castps128Ps512(a x86.M128) x86.M512 {
	return x86.M512(m512Castps128Ps512([4]float32(a)))
}

func m512Castps128Ps512(a [4]float32) [16]float32


// M512Castps256Ps512: Cast vector of type __m256 to type __m512; the upper 256
// bits of the result are undefined. 
// 	This intrinsic is only used for compilation and does not generate any
// instructions, thus it has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_castps256_ps512'.
// Requires AVX512F.
func M512Castps256Ps512(a x86.M256) x86.M512 {
	return x86.M512(m512Castps256Ps512([8]float32(a)))
}

func m512Castps256Ps512(a [8]float32) [16]float32


// M512Castps512Ps128: Cast vector of type __m512 to type __m128. 
// 	This intrinsic is only used for compilation and does not generate any
// instructions, thus it has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_castps512_ps128'.
// Requires AVX512F.
func M512Castps512Ps128(a x86.M512) x86.M128 {
	return x86.M128(m512Castps512Ps128([16]float32(a)))
}

func m512Castps512Ps128(a [16]float32) [4]float32


// M512Castps512Ps256: Cast vector of type __m512 to type __m256. 
// 	This intrinsic is only used for compilation and does not generate any
// instructions, thus it has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_castps512_ps256'.
// Requires AVX512F.
func M512Castps512Ps256(a x86.M512) x86.M256 {
	return x86.M256(m512Castps512Ps256([16]float32(a)))
}

func m512Castps512Ps256(a [16]float32) [8]float32


// M512Castsi128Si512: Cast vector of type __m128i to type __m512i; the upper
// 384 bits of the result are undefined. 
// 	This intrinsic is only used for compilation and does not generate any
// instructions, thus it has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_castsi128_si512'.
// Requires AVX512F.
func M512Castsi128Si512(a x86.M128i) x86.M512i {
	return x86.M512i(m512Castsi128Si512([16]byte(a)))
}

func m512Castsi128Si512(a [16]byte) [64]byte


// M512Castsi256Si512: Cast vector of type __m256i to type __m512i; the upper
// 256 bits of the result are undefined.
// 	 This intrinsic is only used for compilation and does not generate any
// instructions, thus it has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_castsi256_si512'.
// Requires AVX512F.
func M512Castsi256Si512(a x86.M256i) x86.M512i {
	return x86.M512i(m512Castsi256Si512([32]byte(a)))
}

func m512Castsi256Si512(a [32]byte) [64]byte


// M512Castsi512Si128: Cast vector of type __m512i to type __m128i.
// 	 This intrinsic is only used for compilation and does not generate any
// instructions, thus it has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_castsi512_si128'.
// Requires AVX512F.
func M512Castsi512Si128(a x86.M512i) x86.M128i {
	return x86.M128i(m512Castsi512Si128([64]byte(a)))
}

func m512Castsi512Si128(a [64]byte) [16]byte


// M512Castsi512Si256: Cast vector of type __m512i to type __m256i.
// 	 This intrinsic is only used for compilation and does not generate any
// instructions, thus it has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_castsi512_si256'.
// Requires AVX512F.
func M512Castsi512Si256(a x86.M512i) x86.M256i {
	return x86.M256i(m512Castsi512Si256([64]byte(a)))
}

func m512Castsi512Si256(a [64]byte) [32]byte


// M512CbrtPd: Compute the cube root of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := CubeRoot(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_cbrt_pd'.
// Requires AVX512F.
func M512CbrtPd(a x86.M512d) x86.M512d {
	return x86.M512d(m512CbrtPd([8]float64(a)))
}

func m512CbrtPd(a [8]float64) [8]float64


// M512MaskCbrtPd: Compute the cube root of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := CubeRoot(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_cbrt_pd'.
// Requires AVX512F.
func M512MaskCbrtPd(src x86.M512d, k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskCbrtPd([8]float64(src), uint8(k), [8]float64(a)))
}

func m512MaskCbrtPd(src [8]float64, k uint8, a [8]float64) [8]float64


// M512CbrtPs: Compute the cube root of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := CubeRoot(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_cbrt_ps'.
// Requires AVX512F.
func M512CbrtPs(a x86.M512) x86.M512 {
	return x86.M512(m512CbrtPs([16]float32(a)))
}

func m512CbrtPs(a [16]float32) [16]float32


// M512MaskCbrtPs: Compute the cube root of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := CubeRoot(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_cbrt_ps'.
// Requires AVX512F.
func M512MaskCbrtPs(src x86.M512, k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskCbrtPs([16]float32(src), uint16(k), [16]float32(a)))
}

func m512MaskCbrtPs(src [16]float32, k uint16, a [16]float32) [16]float32


// M512CdfnormPd: Compute the cumulative distribution function of packed
// double-precision (64-bit) floating-point elements in 'a' using the normal
// distribution, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := CDFNormal(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_cdfnorm_pd'.
// Requires AVX512F.
func M512CdfnormPd(a x86.M512d) x86.M512d {
	return x86.M512d(m512CdfnormPd([8]float64(a)))
}

func m512CdfnormPd(a [8]float64) [8]float64


// M512MaskCdfnormPd: Compute the cumulative distribution function of packed
// double-precision (64-bit) floating-point elements in 'a' using the normal
// distribution, and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := CDFNormal(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_cdfnorm_pd'.
// Requires AVX512F.
func M512MaskCdfnormPd(src x86.M512d, k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskCdfnormPd([8]float64(src), uint8(k), [8]float64(a)))
}

func m512MaskCdfnormPd(src [8]float64, k uint8, a [8]float64) [8]float64


// M512CdfnormPs: Compute the cumulative distribution function of packed
// single-precision (32-bit) floating-point elements in 'a' using the normal
// distribution, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := CDFNormal(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_cdfnorm_ps'.
// Requires AVX512F.
func M512CdfnormPs(a x86.M512) x86.M512 {
	return x86.M512(m512CdfnormPs([16]float32(a)))
}

func m512CdfnormPs(a [16]float32) [16]float32


// M512MaskCdfnormPs: Compute the cumulative distribution function of packed
// single-precision (32-bit) floating-point elements in 'a' using the normal
// distribution, and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := CDFNormal(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_cdfnorm_ps'.
// Requires AVX512F.
func M512MaskCdfnormPs(src x86.M512, k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskCdfnormPs([16]float32(src), uint16(k), [16]float32(a)))
}

func m512MaskCdfnormPs(src [16]float32, k uint16, a [16]float32) [16]float32


// M512CdfnorminvPd: Compute the inverse cumulative distribution function of
// packed double-precision (64-bit) floating-point elements in 'a' using the
// normal distribution, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := InverseCDFNormal(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_cdfnorminv_pd'.
// Requires AVX512F.
func M512CdfnorminvPd(a x86.M512d) x86.M512d {
	return x86.M512d(m512CdfnorminvPd([8]float64(a)))
}

func m512CdfnorminvPd(a [8]float64) [8]float64


// M512MaskCdfnorminvPd: Compute the inverse cumulative distribution function
// of packed double-precision (64-bit) floating-point elements in 'a' using the
// normal distribution, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := InverseCDFNormal(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_cdfnorminv_pd'.
// Requires AVX512F.
func M512MaskCdfnorminvPd(src x86.M512d, k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskCdfnorminvPd([8]float64(src), uint8(k), [8]float64(a)))
}

func m512MaskCdfnorminvPd(src [8]float64, k uint8, a [8]float64) [8]float64


// M512CdfnorminvPs: Compute the inverse cumulative distribution function of
// packed single-precision (32-bit) floating-point elements in 'a' using the
// normal distribution, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := InverseCDFNormal(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_cdfnorminv_ps'.
// Requires AVX512F.
func M512CdfnorminvPs(a x86.M512) x86.M512 {
	return x86.M512(m512CdfnorminvPs([16]float32(a)))
}

func m512CdfnorminvPs(a [16]float32) [16]float32


// M512MaskCdfnorminvPs: Compute the inverse cumulative distribution function
// of packed single-precision (32-bit) floating-point elements in 'a' using the
// normal distribution, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := InverseCDFNormal(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_cdfnorminv_ps'.
// Requires AVX512F.
func M512MaskCdfnorminvPs(src x86.M512, k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskCdfnorminvPs([16]float32(src), uint16(k), [16]float32(a)))
}

func m512MaskCdfnorminvPs(src [16]float32, k uint16, a [16]float32) [16]float32


// M512CeilPd: Round the packed double-precision (64-bit) floating-point
// elements in 'a' up to an integer value, and store the results as packed
// double-precision floating-point elements in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := CEIL(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_ceil_pd'.
// Requires AVX512F.
func M512CeilPd(a x86.M512d) x86.M512d {
	return x86.M512d(m512CeilPd([8]float64(a)))
}

func m512CeilPd(a [8]float64) [8]float64


// M512MaskCeilPd: Round the packed double-precision (64-bit) floating-point
// elements in 'a' up to an integer value, and store the results as packed
// double-precision floating-point elements in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := CEIL(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_ceil_pd'.
// Requires AVX512F.
func M512MaskCeilPd(src x86.M512d, k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskCeilPd([8]float64(src), uint8(k), [8]float64(a)))
}

func m512MaskCeilPd(src [8]float64, k uint8, a [8]float64) [8]float64


// M512CeilPs: Round the packed single-precision (32-bit) floating-point
// elements in 'a' up to an integer value, and store the results as packed
// single-precision floating-point elements in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := CEIL(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_ceil_ps'.
// Requires AVX512F.
func M512CeilPs(a x86.M512) x86.M512 {
	return x86.M512(m512CeilPs([16]float32(a)))
}

func m512CeilPs(a [16]float32) [16]float32


// M512MaskCeilPs: Round the packed single-precision (32-bit) floating-point
// elements in 'a' up to an integer value, and store the results as packed
// single-precision floating-point elements in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := CEIL(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_ceil_ps'.
// Requires AVX512F.
func M512MaskCeilPs(src x86.M512, k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskCeilPs([16]float32(src), uint16(k), [16]float32(a)))
}

func m512MaskCeilPs(src [16]float32, k uint16, a [16]float32) [16]float32


// CmpEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' based on the
// comparison operand specified by 'imm8', and store the results in mask vector
// 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm_cmp_epi32_mask'.
// Requires AVX512F.
func CmpEpi32Mask(a x86.M128i, b x86.M128i, imm8 uint8) x86.Mmask8 {
	return x86.Mmask8(cmpEpi32Mask([16]byte(a), [16]byte(b), imm8))
}

func cmpEpi32Mask(a [16]byte, b [16]byte, imm8 uint8) uint8


// MaskCmpEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' based on the
// comparison operand specified by 'imm8', and store the results in mask vector
// 'k1' using zeromask 'k' (elements are zeroed out when the corresponding mask
// bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm_mask_cmp_epi32_mask'.
// Requires AVX512F.
func MaskCmpEpi32Mask(k1 x86.Mmask8, a x86.M128i, b x86.M128i, imm8 uint8) x86.Mmask8 {
	return x86.Mmask8(maskCmpEpi32Mask(uint8(k1), [16]byte(a), [16]byte(b), imm8))
}

func maskCmpEpi32Mask(k1 uint8, a [16]byte, b [16]byte, imm8 uint8) uint8


// M256CmpEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' based on the
// comparison operand specified by 'imm8', and store the results in mask vector
// 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_cmp_epi32_mask'.
// Requires AVX512F.
func M256CmpEpi32Mask(a x86.M256i, b x86.M256i, imm8 uint8) x86.Mmask8 {
	return x86.Mmask8(m256CmpEpi32Mask([32]byte(a), [32]byte(b), imm8))
}

func m256CmpEpi32Mask(a [32]byte, b [32]byte, imm8 uint8) uint8


// M256MaskCmpEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' based on
// the comparison operand specified by 'imm8', and store the results in mask
// vector 'k1' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_mask_cmp_epi32_mask'.
// Requires AVX512F.
func M256MaskCmpEpi32Mask(k1 x86.Mmask8, a x86.M256i, b x86.M256i, imm8 uint8) x86.Mmask8 {
	return x86.Mmask8(m256MaskCmpEpi32Mask(uint8(k1), [32]byte(a), [32]byte(b), imm8))
}

func m256MaskCmpEpi32Mask(k1 uint8, a [32]byte, b [32]byte, imm8 uint8) uint8


// CmpEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' based on the
// comparison operand specified by 'imm8', and store the results in mask vector
// 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm_cmp_epi64_mask'.
// Requires AVX512F.
func CmpEpi64Mask(a x86.M128i, b x86.M128i, imm8 uint8) x86.Mmask8 {
	return x86.Mmask8(cmpEpi64Mask([16]byte(a), [16]byte(b), imm8))
}

func cmpEpi64Mask(a [16]byte, b [16]byte, imm8 uint8) uint8


// MaskCmpEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' based on the
// comparison operand specified by 'imm8', and store the results in mask vector
// 'k1' using zeromask 'k' (elements are zeroed out when the corresponding mask
// bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm_mask_cmp_epi64_mask'.
// Requires AVX512F.
func MaskCmpEpi64Mask(k1 x86.Mmask8, a x86.M128i, b x86.M128i, imm8 uint8) x86.Mmask8 {
	return x86.Mmask8(maskCmpEpi64Mask(uint8(k1), [16]byte(a), [16]byte(b), imm8))
}

func maskCmpEpi64Mask(k1 uint8, a [16]byte, b [16]byte, imm8 uint8) uint8


// M256CmpEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' based on the
// comparison operand specified by 'imm8', and store the results in mask vector
// 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_cmp_epi64_mask'.
// Requires AVX512F.
func M256CmpEpi64Mask(a x86.M256i, b x86.M256i, imm8 uint8) x86.Mmask8 {
	return x86.Mmask8(m256CmpEpi64Mask([32]byte(a), [32]byte(b), imm8))
}

func m256CmpEpi64Mask(a [32]byte, b [32]byte, imm8 uint8) uint8


// M256MaskCmpEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' based on
// the comparison operand specified by 'imm8', and store the results in mask
// vector 'k1' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_mask_cmp_epi64_mask'.
// Requires AVX512F.
func M256MaskCmpEpi64Mask(k1 x86.Mmask8, a x86.M256i, b x86.M256i, imm8 uint8) x86.Mmask8 {
	return x86.Mmask8(m256MaskCmpEpi64Mask(uint8(k1), [32]byte(a), [32]byte(b), imm8))
}

func m256MaskCmpEpi64Mask(k1 uint8, a [32]byte, b [32]byte, imm8 uint8) uint8


// M512CmpEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' based on the
// comparison operand specified by 'imm8', and store the results in mask vector
// 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm512_cmp_epi64_mask'.
// Requires AVX512F.
func M512CmpEpi64Mask(a x86.M512i, b x86.M512i, imm8 uint8) x86.Mmask8 {
	return x86.Mmask8(m512CmpEpi64Mask([64]byte(a), [64]byte(b), imm8))
}

func m512CmpEpi64Mask(a [64]byte, b [64]byte, imm8 uint8) uint8


// M512MaskCmpEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' based on
// the comparison operand specified by 'imm8', and store the results in mask
// vector 'k1' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm512_mask_cmp_epi64_mask'.
// Requires AVX512F.
func M512MaskCmpEpi64Mask(k1 x86.Mmask8, a x86.M512i, b x86.M512i, imm8 uint8) x86.Mmask8 {
	return x86.Mmask8(m512MaskCmpEpi64Mask(uint8(k1), [64]byte(a), [64]byte(b), imm8))
}

func m512MaskCmpEpi64Mask(k1 uint8, a [64]byte, b [64]byte, imm8 uint8) uint8


// CmpEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b' based
// on the comparison operand specified by 'imm8', and store the results in mask
// vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm_cmp_epu32_mask'.
// Requires AVX512F.
func CmpEpu32Mask(a x86.M128i, b x86.M128i, imm8 uint8) x86.Mmask8 {
	return x86.Mmask8(cmpEpu32Mask([16]byte(a), [16]byte(b), imm8))
}

func cmpEpu32Mask(a [16]byte, b [16]byte, imm8 uint8) uint8


// MaskCmpEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// based on the comparison operand specified by 'imm8', and store the results
// in mask vector 'k1' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm_mask_cmp_epu32_mask'.
// Requires AVX512F.
func MaskCmpEpu32Mask(k1 x86.Mmask8, a x86.M128i, b x86.M128i, imm8 uint8) x86.Mmask8 {
	return x86.Mmask8(maskCmpEpu32Mask(uint8(k1), [16]byte(a), [16]byte(b), imm8))
}

func maskCmpEpu32Mask(k1 uint8, a [16]byte, b [16]byte, imm8 uint8) uint8


// M256CmpEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// based on the comparison operand specified by 'imm8', and store the results
// in mask vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_cmp_epu32_mask'.
// Requires AVX512F.
func M256CmpEpu32Mask(a x86.M256i, b x86.M256i, imm8 uint8) x86.Mmask8 {
	return x86.Mmask8(m256CmpEpu32Mask([32]byte(a), [32]byte(b), imm8))
}

func m256CmpEpu32Mask(a [32]byte, b [32]byte, imm8 uint8) uint8


// M256MaskCmpEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// based on the comparison operand specified by 'imm8', and store the results
// in mask vector 'k1' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_mask_cmp_epu32_mask'.
// Requires AVX512F.
func M256MaskCmpEpu32Mask(k1 x86.Mmask8, a x86.M256i, b x86.M256i, imm8 uint8) x86.Mmask8 {
	return x86.Mmask8(m256MaskCmpEpu32Mask(uint8(k1), [32]byte(a), [32]byte(b), imm8))
}

func m256MaskCmpEpu32Mask(k1 uint8, a [32]byte, b [32]byte, imm8 uint8) uint8


// CmpEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b' based
// on the comparison operand specified by 'imm8', and store the results in mask
// vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm_cmp_epu64_mask'.
// Requires AVX512F.
func CmpEpu64Mask(a x86.M128i, b x86.M128i, imm8 uint8) x86.Mmask8 {
	return x86.Mmask8(cmpEpu64Mask([16]byte(a), [16]byte(b), imm8))
}

func cmpEpu64Mask(a [16]byte, b [16]byte, imm8 uint8) uint8


// MaskCmpEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// based on the comparison operand specified by 'imm8', and store the results
// in mask vector 'k1' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm_mask_cmp_epu64_mask'.
// Requires AVX512F.
func MaskCmpEpu64Mask(k1 x86.Mmask8, a x86.M128i, b x86.M128i, imm8 uint8) x86.Mmask8 {
	return x86.Mmask8(maskCmpEpu64Mask(uint8(k1), [16]byte(a), [16]byte(b), imm8))
}

func maskCmpEpu64Mask(k1 uint8, a [16]byte, b [16]byte, imm8 uint8) uint8


// M256CmpEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// based on the comparison operand specified by 'imm8', and store the results
// in mask vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_cmp_epu64_mask'.
// Requires AVX512F.
func M256CmpEpu64Mask(a x86.M256i, b x86.M256i, imm8 uint8) x86.Mmask8 {
	return x86.Mmask8(m256CmpEpu64Mask([32]byte(a), [32]byte(b), imm8))
}

func m256CmpEpu64Mask(a [32]byte, b [32]byte, imm8 uint8) uint8


// M256MaskCmpEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// based on the comparison operand specified by 'imm8', and store the results
// in mask vector 'k1' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_mask_cmp_epu64_mask'.
// Requires AVX512F.
func M256MaskCmpEpu64Mask(k1 x86.Mmask8, a x86.M256i, b x86.M256i, imm8 uint8) x86.Mmask8 {
	return x86.Mmask8(m256MaskCmpEpu64Mask(uint8(k1), [32]byte(a), [32]byte(b), imm8))
}

func m256MaskCmpEpu64Mask(k1 uint8, a [32]byte, b [32]byte, imm8 uint8) uint8


// M512CmpEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// based on the comparison operand specified by 'imm8', and store the results
// in mask vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm512_cmp_epu64_mask'.
// Requires AVX512F.
func M512CmpEpu64Mask(a x86.M512i, b x86.M512i, imm8 uint8) x86.Mmask8 {
	return x86.Mmask8(m512CmpEpu64Mask([64]byte(a), [64]byte(b), imm8))
}

func m512CmpEpu64Mask(a [64]byte, b [64]byte, imm8 uint8) uint8


// M512MaskCmpEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// based on the comparison operand specified by 'imm8', and store the results
// in mask vector 'k1' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm512_mask_cmp_epu64_mask'.
// Requires AVX512F.
func M512MaskCmpEpu64Mask(k1 x86.Mmask8, a x86.M512i, b x86.M512i, imm8 uint8) x86.Mmask8 {
	return x86.Mmask8(m512MaskCmpEpu64Mask(uint8(k1), [64]byte(a), [64]byte(b), imm8))
}

func m512MaskCmpEpu64Mask(k1 uint8, a [64]byte, b [64]byte, imm8 uint8) uint8


// CmpPdMask: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b' based on the comparison operand specified by 'imm8', and
// store the results in mask vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VCMPPD'. Intrinsic: '_mm_cmp_pd_mask'.
// Requires AVX512F.
func CmpPdMask(a x86.M128d, b x86.M128d, imm8 int) x86.Mmask8 {
	return x86.Mmask8(cmpPdMask([2]float64(a), [2]float64(b), imm8))
}

func cmpPdMask(a [2]float64, b [2]float64, imm8 int) uint8


// MaskCmpPdMask: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' based on the comparison operand specified by 'imm8',
// and store the results in mask vector 'k' using zeromask 'k1' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VCMPPD'. Intrinsic: '_mm_mask_cmp_pd_mask'.
// Requires AVX512F.
func MaskCmpPdMask(k1 x86.Mmask8, a x86.M128d, b x86.M128d, imm8 int) x86.Mmask8 {
	return x86.Mmask8(maskCmpPdMask(uint8(k1), [2]float64(a), [2]float64(b), imm8))
}

func maskCmpPdMask(k1 uint8, a [2]float64, b [2]float64, imm8 int) uint8


// M256CmpPdMask: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' based on the comparison operand specified by 'imm8',
// and store the results in mask vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := (a[i+63:i] OP b[i+63:i]) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VCMPPD'. Intrinsic: '_mm256_cmp_pd_mask'.
// Requires AVX512F.
func M256CmpPdMask(a x86.M256d, b x86.M256d, imm8 int) x86.Mmask8 {
	return x86.Mmask8(m256CmpPdMask([4]float64(a), [4]float64(b), imm8))
}

func m256CmpPdMask(a [4]float64, b [4]float64, imm8 int) uint8


// M256MaskCmpPdMask: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' based on the comparison operand specified by 'imm8',
// and store the results in mask vector 'k' using zeromask 'k1' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VCMPPD'. Intrinsic: '_mm256_mask_cmp_pd_mask'.
// Requires AVX512F.
func M256MaskCmpPdMask(k1 x86.Mmask8, a x86.M256d, b x86.M256d, imm8 int) x86.Mmask8 {
	return x86.Mmask8(m256MaskCmpPdMask(uint8(k1), [4]float64(a), [4]float64(b), imm8))
}

func m256MaskCmpPdMask(k1 uint8, a [4]float64, b [4]float64, imm8 int) uint8


// CmpPsMask: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b' based on the comparison operand specified by 'imm8', and
// store the results in mask vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VCMPPS'. Intrinsic: '_mm_cmp_ps_mask'.
// Requires AVX512F.
func CmpPsMask(a x86.M128, b x86.M128, imm8 int) x86.Mmask8 {
	return x86.Mmask8(cmpPsMask([4]float32(a), [4]float32(b), imm8))
}

func cmpPsMask(a [4]float32, b [4]float32, imm8 int) uint8


// MaskCmpPsMask: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' based on the comparison operand specified by 'imm8',
// and store the results in mask vector 'k' using zeromask 'k1' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VCMPPS'. Intrinsic: '_mm_mask_cmp_ps_mask'.
// Requires AVX512F.
func MaskCmpPsMask(k1 x86.Mmask8, a x86.M128, b x86.M128, imm8 int) x86.Mmask8 {
	return x86.Mmask8(maskCmpPsMask(uint8(k1), [4]float32(a), [4]float32(b), imm8))
}

func maskCmpPsMask(k1 uint8, a [4]float32, b [4]float32, imm8 int) uint8


// M256CmpPsMask: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' based on the comparison operand specified by 'imm8',
// and store the results in mask vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := (a[i+31:i] OP b[i+31:i]) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VCMPPS'. Intrinsic: '_mm256_cmp_ps_mask'.
// Requires AVX512F.
func M256CmpPsMask(a x86.M256, b x86.M256, imm8 int) x86.Mmask8 {
	return x86.Mmask8(m256CmpPsMask([8]float32(a), [8]float32(b), imm8))
}

func m256CmpPsMask(a [8]float32, b [8]float32, imm8 int) uint8


// M256MaskCmpPsMask: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' based on the comparison operand specified by 'imm8',
// and store the results in mask vector 'k' using zeromask 'k1' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VCMPPS'. Intrinsic: '_mm256_mask_cmp_ps_mask'.
// Requires AVX512F.
func M256MaskCmpPsMask(k1 x86.Mmask8, a x86.M256, b x86.M256, imm8 int) x86.Mmask8 {
	return x86.Mmask8(m256MaskCmpPsMask(uint8(k1), [8]float32(a), [8]float32(b), imm8))
}

func m256MaskCmpPsMask(k1 uint8, a [8]float32, b [8]float32, imm8 int) uint8


// CmpRoundSdMask: Compare the lower double-precision (64-bit) floating-point
// element in 'a' and 'b' based on the comparison operand specified by 'imm8',
// and store the result in mask vector 'k'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		
//		k[0] := ( a[63:0] OP b[63:0] ) ? 1 : 0
//		
//		k[MAX:1] := 0
//
// Instruction: 'VCMPSD'. Intrinsic: '_mm_cmp_round_sd_mask'.
// Requires AVX512F.
func CmpRoundSdMask(a x86.M128d, b x86.M128d, imm8 int, sae int) x86.Mmask8 {
	return x86.Mmask8(cmpRoundSdMask([2]float64(a), [2]float64(b), imm8, sae))
}

func cmpRoundSdMask(a [2]float64, b [2]float64, imm8 int, sae int) uint8


// MaskCmpRoundSdMask: Compare the lower double-precision (64-bit)
// floating-point element in 'a' and 'b' based on the comparison operand
// specified by 'imm8', and store the result in mask vector 'k' using zeromask
// 'k1' (the element is zeroed out when mask bit 0 is not set).
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		
//		IF k1[0]
//			k[0] := ( a[63:0] OP b[63:0] ) ? 1 : 0
//		ELSE
//			k[0] := 0
//		FI
//		k[MAX:1] := 0
//
// Instruction: 'VCMPSD'. Intrinsic: '_mm_mask_cmp_round_sd_mask'.
// Requires AVX512F.
func MaskCmpRoundSdMask(k1 x86.Mmask8, a x86.M128d, b x86.M128d, imm8 int, sae int) x86.Mmask8 {
	return x86.Mmask8(maskCmpRoundSdMask(uint8(k1), [2]float64(a), [2]float64(b), imm8, sae))
}

func maskCmpRoundSdMask(k1 uint8, a [2]float64, b [2]float64, imm8 int, sae int) uint8


// CmpRoundSsMask: Compare the lower single-precision (32-bit) floating-point
// element in 'a' and 'b' based on the comparison operand specified by 'imm8',
// and store the result in mask vector 'k'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		
//		k[0] := ( a[31:0] OP b[31:0] ) ? 1 : 0
//		
//		k[MAX:1] := 0
//
// Instruction: 'VCMPSS'. Intrinsic: '_mm_cmp_round_ss_mask'.
// Requires AVX512F.
func CmpRoundSsMask(a x86.M128, b x86.M128, imm8 int, sae int) x86.Mmask8 {
	return x86.Mmask8(cmpRoundSsMask([4]float32(a), [4]float32(b), imm8, sae))
}

func cmpRoundSsMask(a [4]float32, b [4]float32, imm8 int, sae int) uint8


// MaskCmpRoundSsMask: Compare the lower single-precision (32-bit)
// floating-point element in 'a' and 'b' based on the comparison operand
// specified by 'imm8', and store the result in mask vector 'k' using zeromask
// 'k1' (the element is zeroed out when mask bit 0 is not set). 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		
//		IF k1[0]
//			k[0] := ( a[31:0] OP b[31:0] ) ? 1 : 0
//		ELSE
//			k[0] := 0
//		FI
//		k[MAX:1] := 0
//
// Instruction: 'VCMPSS'. Intrinsic: '_mm_mask_cmp_round_ss_mask'.
// Requires AVX512F.
func MaskCmpRoundSsMask(k1 x86.Mmask8, a x86.M128, b x86.M128, imm8 int, sae int) x86.Mmask8 {
	return x86.Mmask8(maskCmpRoundSsMask(uint8(k1), [4]float32(a), [4]float32(b), imm8, sae))
}

func maskCmpRoundSsMask(k1 uint8, a [4]float32, b [4]float32, imm8 int, sae int) uint8


// CmpSdMask: Compare the lower double-precision (64-bit) floating-point
// element in 'a' and 'b' based on the comparison operand specified by 'imm8',
// and store the result in mask vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		
//		k[0] := ( a[63:0] OP b[63:0] ) ? 1 : 0
//		
//		k[MAX:1] := 0
//
// Instruction: 'VCMPSD'. Intrinsic: '_mm_cmp_sd_mask'.
// Requires AVX512F.
func CmpSdMask(a x86.M128d, b x86.M128d, imm8 int) x86.Mmask8 {
	return x86.Mmask8(cmpSdMask([2]float64(a), [2]float64(b), imm8))
}

func cmpSdMask(a [2]float64, b [2]float64, imm8 int) uint8


// MaskCmpSdMask: Compare the lower double-precision (64-bit) floating-point
// element in 'a' and 'b' based on the comparison operand specified by 'imm8',
// and store the result in mask vector 'k' using zeromask 'k1' (the element is
// zeroed out when mask bit 0 is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		
//		IF k1[0]
//			k[0] := ( a[63:0] OP b[63:0] ) ? 1 : 0
//		ELSE
//			k[0] := 0
//		FI
//		k[MAX:1] := 0
//
// Instruction: 'VCMPSD'. Intrinsic: '_mm_mask_cmp_sd_mask'.
// Requires AVX512F.
func MaskCmpSdMask(k1 x86.Mmask8, a x86.M128d, b x86.M128d, imm8 int) x86.Mmask8 {
	return x86.Mmask8(maskCmpSdMask(uint8(k1), [2]float64(a), [2]float64(b), imm8))
}

func maskCmpSdMask(k1 uint8, a [2]float64, b [2]float64, imm8 int) uint8


// CmpSsMask: Compare the lower single-precision (32-bit) floating-point
// element in 'a' and 'b' based on the comparison operand specified by 'imm8',
// and store the result in mask vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		
//		k[0] := ( a[31:0] OP b[31:0] ) ? 1 : 0
//		
//		k[MAX:1] := 0
//
// Instruction: 'VCMPSS'. Intrinsic: '_mm_cmp_ss_mask'.
// Requires AVX512F.
func CmpSsMask(a x86.M128, b x86.M128, imm8 int) x86.Mmask8 {
	return x86.Mmask8(cmpSsMask([4]float32(a), [4]float32(b), imm8))
}

func cmpSsMask(a [4]float32, b [4]float32, imm8 int) uint8


// MaskCmpSsMask: Compare the lower single-precision (32-bit) floating-point
// element in 'a' and 'b' based on the comparison operand specified by 'imm8',
// and store the result in mask vector 'k' using zeromask 'k1' (the element is
// zeroed out when mask bit 0 is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		
//		IF k1[0]
//			k[0] := ( a[31:0] OP b[31:0] ) ? 1 : 0
//		ELSE
//			k[0] := 0
//		FI
//		k[MAX:1] := 0
//
// Instruction: 'VCMPSS'. Intrinsic: '_mm_mask_cmp_ss_mask'.
// Requires AVX512F.
func MaskCmpSsMask(k1 x86.Mmask8, a x86.M128, b x86.M128, imm8 int) x86.Mmask8 {
	return x86.Mmask8(maskCmpSsMask(uint8(k1), [4]float32(a), [4]float32(b), imm8))
}

func maskCmpSsMask(k1 uint8, a [4]float32, b [4]float32, imm8 int) uint8


// CmpeqEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for equality,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm_cmpeq_epi32_mask'.
// Requires AVX512F.
func CmpeqEpi32Mask(a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(cmpeqEpi32Mask([16]byte(a), [16]byte(b)))
}

func cmpeqEpi32Mask(a [16]byte, b [16]byte) uint8


// MaskCmpeqEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm_mask_cmpeq_epi32_mask'.
// Requires AVX512F.
func MaskCmpeqEpi32Mask(k1 x86.Mmask8, a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(maskCmpeqEpi32Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpeqEpi32Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// M256CmpeqEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_cmpeq_epi32_mask'.
// Requires AVX512F.
func M256CmpeqEpi32Mask(a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256CmpeqEpi32Mask([32]byte(a), [32]byte(b)))
}

func m256CmpeqEpi32Mask(a [32]byte, b [32]byte) uint8


// M256MaskCmpeqEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_mask_cmpeq_epi32_mask'.
// Requires AVX512F.
func M256MaskCmpeqEpi32Mask(k1 x86.Mmask8, a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256MaskCmpeqEpi32Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func m256MaskCmpeqEpi32Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpeqEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for equality,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm_cmpeq_epi64_mask'.
// Requires AVX512F.
func CmpeqEpi64Mask(a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(cmpeqEpi64Mask([16]byte(a), [16]byte(b)))
}

func cmpeqEpi64Mask(a [16]byte, b [16]byte) uint8


// MaskCmpeqEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm_mask_cmpeq_epi64_mask'.
// Requires AVX512F.
func MaskCmpeqEpi64Mask(k1 x86.Mmask8, a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(maskCmpeqEpi64Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpeqEpi64Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// M256CmpeqEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_cmpeq_epi64_mask'.
// Requires AVX512F.
func M256CmpeqEpi64Mask(a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256CmpeqEpi64Mask([32]byte(a), [32]byte(b)))
}

func m256CmpeqEpi64Mask(a [32]byte, b [32]byte) uint8


// M256MaskCmpeqEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_mask_cmpeq_epi64_mask'.
// Requires AVX512F.
func M256MaskCmpeqEpi64Mask(k1 x86.Mmask8, a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256MaskCmpeqEpi64Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func m256MaskCmpeqEpi64Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// M512CmpeqEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPEQQ'. Intrinsic: '_mm512_cmpeq_epi64_mask'.
// Requires AVX512F.
func M512CmpeqEpi64Mask(a x86.M512i, b x86.M512i) x86.Mmask8 {
	return x86.Mmask8(m512CmpeqEpi64Mask([64]byte(a), [64]byte(b)))
}

func m512CmpeqEpi64Mask(a [64]byte, b [64]byte) uint8


// M512MaskCmpeqEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPEQQ'. Intrinsic: '_mm512_mask_cmpeq_epi64_mask'.
// Requires AVX512F.
func M512MaskCmpeqEpi64Mask(k1 x86.Mmask8, a x86.M512i, b x86.M512i) x86.Mmask8 {
	return x86.Mmask8(m512MaskCmpeqEpi64Mask(uint8(k1), [64]byte(a), [64]byte(b)))
}

func m512MaskCmpeqEpi64Mask(k1 uint8, a [64]byte, b [64]byte) uint8


// CmpeqEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm_cmpeq_epu32_mask'.
// Requires AVX512F.
func CmpeqEpu32Mask(a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(cmpeqEpu32Mask([16]byte(a), [16]byte(b)))
}

func cmpeqEpu32Mask(a [16]byte, b [16]byte) uint8


// MaskCmpeqEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for equality, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm_mask_cmpeq_epu32_mask'.
// Requires AVX512F.
func MaskCmpeqEpu32Mask(k1 x86.Mmask8, a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(maskCmpeqEpu32Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpeqEpu32Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// M256CmpeqEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for equality, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_cmpeq_epu32_mask'.
// Requires AVX512F.
func M256CmpeqEpu32Mask(a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256CmpeqEpu32Mask([32]byte(a), [32]byte(b)))
}

func m256CmpeqEpu32Mask(a [32]byte, b [32]byte) uint8


// M256MaskCmpeqEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and
// 'b' for equality, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_mask_cmpeq_epu32_mask'.
// Requires AVX512F.
func M256MaskCmpeqEpu32Mask(k1 x86.Mmask8, a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256MaskCmpeqEpu32Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func m256MaskCmpeqEpu32Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpeqEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm_cmpeq_epu64_mask'.
// Requires AVX512F.
func CmpeqEpu64Mask(a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(cmpeqEpu64Mask([16]byte(a), [16]byte(b)))
}

func cmpeqEpu64Mask(a [16]byte, b [16]byte) uint8


// MaskCmpeqEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for equality, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm_mask_cmpeq_epu64_mask'.
// Requires AVX512F.
func MaskCmpeqEpu64Mask(k1 x86.Mmask8, a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(maskCmpeqEpu64Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpeqEpu64Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// M256CmpeqEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for equality, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_cmpeq_epu64_mask'.
// Requires AVX512F.
func M256CmpeqEpu64Mask(a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256CmpeqEpu64Mask([32]byte(a), [32]byte(b)))
}

func m256CmpeqEpu64Mask(a [32]byte, b [32]byte) uint8


// M256MaskCmpeqEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and
// 'b' for equality, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_mask_cmpeq_epu64_mask'.
// Requires AVX512F.
func M256MaskCmpeqEpu64Mask(k1 x86.Mmask8, a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256MaskCmpeqEpu64Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func m256MaskCmpeqEpu64Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// M512CmpeqEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for equality, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm512_cmpeq_epu64_mask'.
// Requires AVX512F.
func M512CmpeqEpu64Mask(a x86.M512i, b x86.M512i) x86.Mmask8 {
	return x86.Mmask8(m512CmpeqEpu64Mask([64]byte(a), [64]byte(b)))
}

func m512CmpeqEpu64Mask(a [64]byte, b [64]byte) uint8


// M512MaskCmpeqEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and
// 'b' for equality, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm512_mask_cmpeq_epu64_mask'.
// Requires AVX512F.
func M512MaskCmpeqEpu64Mask(k1 x86.Mmask8, a x86.M512i, b x86.M512i) x86.Mmask8 {
	return x86.Mmask8(m512MaskCmpeqEpu64Mask(uint8(k1), [64]byte(a), [64]byte(b)))
}

func m512MaskCmpeqEpu64Mask(k1 uint8, a [64]byte, b [64]byte) uint8


// CmpgeEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ( a[i+31:i] >= b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm_cmpge_epi32_mask'.
// Requires AVX512F.
func CmpgeEpi32Mask(a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(cmpgeEpi32Mask([16]byte(a), [16]byte(b)))
}

func cmpgeEpi32Mask(a [16]byte, b [16]byte) uint8


// MaskCmpgeEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] >= b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm_mask_cmpge_epi32_mask'.
// Requires AVX512F.
func MaskCmpgeEpi32Mask(k1 x86.Mmask8, a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(maskCmpgeEpi32Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpgeEpi32Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// M256CmpgeEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] >= b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_cmpge_epi32_mask'.
// Requires AVX512F.
func M256CmpgeEpi32Mask(a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256CmpgeEpi32Mask([32]byte(a), [32]byte(b)))
}

func m256CmpgeEpi32Mask(a [32]byte, b [32]byte) uint8


// M256MaskCmpgeEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] >= b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_mask_cmpge_epi32_mask'.
// Requires AVX512F.
func M256MaskCmpgeEpi32Mask(k1 x86.Mmask8, a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256MaskCmpgeEpi32Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func m256MaskCmpgeEpi32Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpgeEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ( a[i+63:i] >= b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm_cmpge_epi64_mask'.
// Requires AVX512F.
func CmpgeEpi64Mask(a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(cmpgeEpi64Mask([16]byte(a), [16]byte(b)))
}

func cmpgeEpi64Mask(a [16]byte, b [16]byte) uint8


// MaskCmpgeEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] >= b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm_mask_cmpge_epi64_mask'.
// Requires AVX512F.
func MaskCmpgeEpi64Mask(k1 x86.Mmask8, a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(maskCmpgeEpi64Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpgeEpi64Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// M256CmpgeEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] >= b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_cmpge_epi64_mask'.
// Requires AVX512F.
func M256CmpgeEpi64Mask(a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256CmpgeEpi64Mask([32]byte(a), [32]byte(b)))
}

func m256CmpgeEpi64Mask(a [32]byte, b [32]byte) uint8


// M256MaskCmpgeEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] >= b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_mask_cmpge_epi64_mask'.
// Requires AVX512F.
func M256MaskCmpgeEpi64Mask(k1 x86.Mmask8, a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256MaskCmpgeEpi64Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func m256MaskCmpgeEpi64Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// M512CmpgeEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ( a[i+63:i] >= b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm512_cmpge_epi64_mask'.
// Requires AVX512F.
func M512CmpgeEpi64Mask(a x86.M512i, b x86.M512i) x86.Mmask8 {
	return x86.Mmask8(m512CmpgeEpi64Mask([64]byte(a), [64]byte(b)))
}

func m512CmpgeEpi64Mask(a [64]byte, b [64]byte) uint8


// M512MaskCmpgeEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] >= b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm512_mask_cmpge_epi64_mask'.
// Requires AVX512F.
func M512MaskCmpgeEpi64Mask(k1 x86.Mmask8, a x86.M512i, b x86.M512i) x86.Mmask8 {
	return x86.Mmask8(m512MaskCmpgeEpi64Mask(uint8(k1), [64]byte(a), [64]byte(b)))
}

func m512MaskCmpgeEpi64Mask(k1 uint8, a [64]byte, b [64]byte) uint8


// CmpgeEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ( a[i+31:i] >= b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm_cmpge_epu32_mask'.
// Requires AVX512F.
func CmpgeEpu32Mask(a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(cmpgeEpu32Mask([16]byte(a), [16]byte(b)))
}

func cmpgeEpu32Mask(a [16]byte, b [16]byte) uint8


// MaskCmpgeEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for greater-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] >= b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm_mask_cmpge_epu32_mask'.
// Requires AVX512F.
func MaskCmpgeEpu32Mask(k1 x86.Mmask8, a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(maskCmpgeEpu32Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpgeEpu32Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// M256CmpgeEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] >= b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_cmpge_epu32_mask'.
// Requires AVX512F.
func M256CmpgeEpu32Mask(a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256CmpgeEpu32Mask([32]byte(a), [32]byte(b)))
}

func m256CmpgeEpu32Mask(a [32]byte, b [32]byte) uint8


// M256MaskCmpgeEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and
// 'b' for greater-than-or-equal, and store the results in mask vector 'k1'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] >= b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_mask_cmpge_epu32_mask'.
// Requires AVX512F.
func M256MaskCmpgeEpu32Mask(k1 x86.Mmask8, a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256MaskCmpgeEpu32Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func m256MaskCmpgeEpu32Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpgeEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ( a[i+63:i] >= b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm_cmpge_epu64_mask'.
// Requires AVX512F.
func CmpgeEpu64Mask(a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(cmpgeEpu64Mask([16]byte(a), [16]byte(b)))
}

func cmpgeEpu64Mask(a [16]byte, b [16]byte) uint8


// MaskCmpgeEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for greater-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] >= b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm_mask_cmpge_epu64_mask'.
// Requires AVX512F.
func MaskCmpgeEpu64Mask(k1 x86.Mmask8, a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(maskCmpgeEpu64Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpgeEpu64Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// M256CmpgeEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] >= b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_cmpge_epu64_mask'.
// Requires AVX512F.
func M256CmpgeEpu64Mask(a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256CmpgeEpu64Mask([32]byte(a), [32]byte(b)))
}

func m256CmpgeEpu64Mask(a [32]byte, b [32]byte) uint8


// M256MaskCmpgeEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and
// 'b' for greater-than-or-equal, and store the results in mask vector 'k1'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] >= b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_mask_cmpge_epu64_mask'.
// Requires AVX512F.
func M256MaskCmpgeEpu64Mask(k1 x86.Mmask8, a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256MaskCmpgeEpu64Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func m256MaskCmpgeEpu64Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// M512CmpgeEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ( a[i+63:i] >= b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm512_cmpge_epu64_mask'.
// Requires AVX512F.
func M512CmpgeEpu64Mask(a x86.M512i, b x86.M512i) x86.Mmask8 {
	return x86.Mmask8(m512CmpgeEpu64Mask([64]byte(a), [64]byte(b)))
}

func m512CmpgeEpu64Mask(a [64]byte, b [64]byte) uint8


// M512MaskCmpgeEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and
// 'b' for greater-than-or-equal, and store the results in mask vector 'k1'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] >= b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm512_mask_cmpge_epu64_mask'.
// Requires AVX512F.
func M512MaskCmpgeEpu64Mask(k1 x86.Mmask8, a x86.M512i, b x86.M512i) x86.Mmask8 {
	return x86.Mmask8(m512MaskCmpgeEpu64Mask(uint8(k1), [64]byte(a), [64]byte(b)))
}

func m512MaskCmpgeEpu64Mask(k1 uint8, a [64]byte, b [64]byte) uint8


// CmpgtEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ( a[i+31:i] > b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm_cmpgt_epi32_mask'.
// Requires AVX512F.
func CmpgtEpi32Mask(a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(cmpgtEpi32Mask([16]byte(a), [16]byte(b)))
}

func cmpgtEpi32Mask(a [16]byte, b [16]byte) uint8


// MaskCmpgtEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] > b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm_mask_cmpgt_epi32_mask'.
// Requires AVX512F.
func MaskCmpgtEpi32Mask(k1 x86.Mmask8, a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(maskCmpgtEpi32Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpgtEpi32Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// M256CmpgtEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] > b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_cmpgt_epi32_mask'.
// Requires AVX512F.
func M256CmpgtEpi32Mask(a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256CmpgtEpi32Mask([32]byte(a), [32]byte(b)))
}

func m256CmpgtEpi32Mask(a [32]byte, b [32]byte) uint8


// M256MaskCmpgtEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] > b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_mask_cmpgt_epi32_mask'.
// Requires AVX512F.
func M256MaskCmpgtEpi32Mask(k1 x86.Mmask8, a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256MaskCmpgtEpi32Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func m256MaskCmpgtEpi32Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpgtEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ( a[i+63:i] > b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm_cmpgt_epi64_mask'.
// Requires AVX512F.
func CmpgtEpi64Mask(a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(cmpgtEpi64Mask([16]byte(a), [16]byte(b)))
}

func cmpgtEpi64Mask(a [16]byte, b [16]byte) uint8


// MaskCmpgtEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] > b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm_mask_cmpgt_epi64_mask'.
// Requires AVX512F.
func MaskCmpgtEpi64Mask(k1 x86.Mmask8, a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(maskCmpgtEpi64Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpgtEpi64Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// M256CmpgtEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] > b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_cmpgt_epi64_mask'.
// Requires AVX512F.
func M256CmpgtEpi64Mask(a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256CmpgtEpi64Mask([32]byte(a), [32]byte(b)))
}

func m256CmpgtEpi64Mask(a [32]byte, b [32]byte) uint8


// M256MaskCmpgtEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] > b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_mask_cmpgt_epi64_mask'.
// Requires AVX512F.
func M256MaskCmpgtEpi64Mask(k1 x86.Mmask8, a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256MaskCmpgtEpi64Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func m256MaskCmpgtEpi64Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// M512CmpgtEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ( a[i+63:i] > b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPGTQ'. Intrinsic: '_mm512_cmpgt_epi64_mask'.
// Requires AVX512F.
func M512CmpgtEpi64Mask(a x86.M512i, b x86.M512i) x86.Mmask8 {
	return x86.Mmask8(m512CmpgtEpi64Mask([64]byte(a), [64]byte(b)))
}

func m512CmpgtEpi64Mask(a [64]byte, b [64]byte) uint8


// M512MaskCmpgtEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] > b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPGTQ'. Intrinsic: '_mm512_mask_cmpgt_epi64_mask'.
// Requires AVX512F.
func M512MaskCmpgtEpi64Mask(k1 x86.Mmask8, a x86.M512i, b x86.M512i) x86.Mmask8 {
	return x86.Mmask8(m512MaskCmpgtEpi64Mask(uint8(k1), [64]byte(a), [64]byte(b)))
}

func m512MaskCmpgtEpi64Mask(k1 uint8, a [64]byte, b [64]byte) uint8


// CmpgtEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ( a[i+31:i] > b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm_cmpgt_epu32_mask'.
// Requires AVX512F.
func CmpgtEpu32Mask(a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(cmpgtEpu32Mask([16]byte(a), [16]byte(b)))
}

func cmpgtEpu32Mask(a [16]byte, b [16]byte) uint8


// MaskCmpgtEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for greater-than, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] > b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm_mask_cmpgt_epu32_mask'.
// Requires AVX512F.
func MaskCmpgtEpu32Mask(k1 x86.Mmask8, a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(maskCmpgtEpu32Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpgtEpu32Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// M256CmpgtEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for greater-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] > b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_cmpgt_epu32_mask'.
// Requires AVX512F.
func M256CmpgtEpu32Mask(a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256CmpgtEpu32Mask([32]byte(a), [32]byte(b)))
}

func m256CmpgtEpu32Mask(a [32]byte, b [32]byte) uint8


// M256MaskCmpgtEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and
// 'b' for greater-than, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] > b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_mask_cmpgt_epu32_mask'.
// Requires AVX512F.
func M256MaskCmpgtEpu32Mask(k1 x86.Mmask8, a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256MaskCmpgtEpu32Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func m256MaskCmpgtEpu32Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpgtEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ( a[i+63:i] > b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm_cmpgt_epu64_mask'.
// Requires AVX512F.
func CmpgtEpu64Mask(a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(cmpgtEpu64Mask([16]byte(a), [16]byte(b)))
}

func cmpgtEpu64Mask(a [16]byte, b [16]byte) uint8


// MaskCmpgtEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for greater-than, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] > b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm_mask_cmpgt_epu64_mask'.
// Requires AVX512F.
func MaskCmpgtEpu64Mask(k1 x86.Mmask8, a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(maskCmpgtEpu64Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpgtEpu64Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// M256CmpgtEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for greater-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] > b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_cmpgt_epu64_mask'.
// Requires AVX512F.
func M256CmpgtEpu64Mask(a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256CmpgtEpu64Mask([32]byte(a), [32]byte(b)))
}

func m256CmpgtEpu64Mask(a [32]byte, b [32]byte) uint8


// M256MaskCmpgtEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and
// 'b' for greater-than, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] > b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_mask_cmpgt_epu64_mask'.
// Requires AVX512F.
func M256MaskCmpgtEpu64Mask(k1 x86.Mmask8, a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256MaskCmpgtEpu64Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func m256MaskCmpgtEpu64Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// M512CmpgtEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for greater-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ( a[i+63:i] > b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm512_cmpgt_epu64_mask'.
// Requires AVX512F.
func M512CmpgtEpu64Mask(a x86.M512i, b x86.M512i) x86.Mmask8 {
	return x86.Mmask8(m512CmpgtEpu64Mask([64]byte(a), [64]byte(b)))
}

func m512CmpgtEpu64Mask(a [64]byte, b [64]byte) uint8


// M512MaskCmpgtEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and
// 'b' for greater-than, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] > b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm512_mask_cmpgt_epu64_mask'.
// Requires AVX512F.
func M512MaskCmpgtEpu64Mask(k1 x86.Mmask8, a x86.M512i, b x86.M512i) x86.Mmask8 {
	return x86.Mmask8(m512MaskCmpgtEpu64Mask(uint8(k1), [64]byte(a), [64]byte(b)))
}

func m512MaskCmpgtEpu64Mask(k1 uint8, a [64]byte, b [64]byte) uint8


// CmpleEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ( a[i+31:i] <= b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm_cmple_epi32_mask'.
// Requires AVX512F.
func CmpleEpi32Mask(a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(cmpleEpi32Mask([16]byte(a), [16]byte(b)))
}

func cmpleEpi32Mask(a [16]byte, b [16]byte) uint8


// MaskCmpleEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] <= b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm_mask_cmple_epi32_mask'.
// Requires AVX512F.
func MaskCmpleEpi32Mask(k1 x86.Mmask8, a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(maskCmpleEpi32Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpleEpi32Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// M256CmpleEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] <= b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_cmple_epi32_mask'.
// Requires AVX512F.
func M256CmpleEpi32Mask(a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256CmpleEpi32Mask([32]byte(a), [32]byte(b)))
}

func m256CmpleEpi32Mask(a [32]byte, b [32]byte) uint8


// M256MaskCmpleEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] <= b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_mask_cmple_epi32_mask'.
// Requires AVX512F.
func M256MaskCmpleEpi32Mask(k1 x86.Mmask8, a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256MaskCmpleEpi32Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func m256MaskCmpleEpi32Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpleEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ( a[i+63:i] <= b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm_cmple_epi64_mask'.
// Requires AVX512F.
func CmpleEpi64Mask(a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(cmpleEpi64Mask([16]byte(a), [16]byte(b)))
}

func cmpleEpi64Mask(a [16]byte, b [16]byte) uint8


// MaskCmpleEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] <= b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm_mask_cmple_epi64_mask'.
// Requires AVX512F.
func MaskCmpleEpi64Mask(k1 x86.Mmask8, a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(maskCmpleEpi64Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpleEpi64Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// M256CmpleEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] <= b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_cmple_epi64_mask'.
// Requires AVX512F.
func M256CmpleEpi64Mask(a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256CmpleEpi64Mask([32]byte(a), [32]byte(b)))
}

func m256CmpleEpi64Mask(a [32]byte, b [32]byte) uint8


// M256MaskCmpleEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] <= b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_mask_cmple_epi64_mask'.
// Requires AVX512F.
func M256MaskCmpleEpi64Mask(k1 x86.Mmask8, a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256MaskCmpleEpi64Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func m256MaskCmpleEpi64Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// M512CmpleEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ( a[i+63:i] <= b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm512_cmple_epi64_mask'.
// Requires AVX512F.
func M512CmpleEpi64Mask(a x86.M512i, b x86.M512i) x86.Mmask8 {
	return x86.Mmask8(m512CmpleEpi64Mask([64]byte(a), [64]byte(b)))
}

func m512CmpleEpi64Mask(a [64]byte, b [64]byte) uint8


// M512MaskCmpleEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] <= b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm512_mask_cmple_epi64_mask'.
// Requires AVX512F.
func M512MaskCmpleEpi64Mask(k1 x86.Mmask8, a x86.M512i, b x86.M512i) x86.Mmask8 {
	return x86.Mmask8(m512MaskCmpleEpi64Mask(uint8(k1), [64]byte(a), [64]byte(b)))
}

func m512MaskCmpleEpi64Mask(k1 uint8, a [64]byte, b [64]byte) uint8


// CmpleEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ( a[i+31:i] <= b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm_cmple_epu32_mask'.
// Requires AVX512F.
func CmpleEpu32Mask(a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(cmpleEpu32Mask([16]byte(a), [16]byte(b)))
}

func cmpleEpu32Mask(a [16]byte, b [16]byte) uint8


// MaskCmpleEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for less-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] <= b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm_mask_cmple_epu32_mask'.
// Requires AVX512F.
func MaskCmpleEpu32Mask(k1 x86.Mmask8, a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(maskCmpleEpu32Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpleEpu32Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// M256CmpleEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] <= b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_cmple_epu32_mask'.
// Requires AVX512F.
func M256CmpleEpu32Mask(a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256CmpleEpu32Mask([32]byte(a), [32]byte(b)))
}

func m256CmpleEpu32Mask(a [32]byte, b [32]byte) uint8


// M256MaskCmpleEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and
// 'b' for less-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] <= b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_mask_cmple_epu32_mask'.
// Requires AVX512F.
func M256MaskCmpleEpu32Mask(k1 x86.Mmask8, a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256MaskCmpleEpu32Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func m256MaskCmpleEpu32Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpleEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ( a[i+63:i] <= b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm_cmple_epu64_mask'.
// Requires AVX512F.
func CmpleEpu64Mask(a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(cmpleEpu64Mask([16]byte(a), [16]byte(b)))
}

func cmpleEpu64Mask(a [16]byte, b [16]byte) uint8


// MaskCmpleEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for less-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] <= b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm_mask_cmple_epu64_mask'.
// Requires AVX512F.
func MaskCmpleEpu64Mask(k1 x86.Mmask8, a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(maskCmpleEpu64Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpleEpu64Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// M256CmpleEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] <= b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_cmple_epu64_mask'.
// Requires AVX512F.
func M256CmpleEpu64Mask(a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256CmpleEpu64Mask([32]byte(a), [32]byte(b)))
}

func m256CmpleEpu64Mask(a [32]byte, b [32]byte) uint8


// M256MaskCmpleEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and
// 'b' for less-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] <= b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_mask_cmple_epu64_mask'.
// Requires AVX512F.
func M256MaskCmpleEpu64Mask(k1 x86.Mmask8, a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256MaskCmpleEpu64Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func m256MaskCmpleEpu64Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// M512CmpleEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ( a[i+63:i] <= b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm512_cmple_epu64_mask'.
// Requires AVX512F.
func M512CmpleEpu64Mask(a x86.M512i, b x86.M512i) x86.Mmask8 {
	return x86.Mmask8(m512CmpleEpu64Mask([64]byte(a), [64]byte(b)))
}

func m512CmpleEpu64Mask(a [64]byte, b [64]byte) uint8


// M512MaskCmpleEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and
// 'b' for less-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] <= b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm512_mask_cmple_epu64_mask'.
// Requires AVX512F.
func M512MaskCmpleEpu64Mask(k1 x86.Mmask8, a x86.M512i, b x86.M512i) x86.Mmask8 {
	return x86.Mmask8(m512MaskCmpleEpu64Mask(uint8(k1), [64]byte(a), [64]byte(b)))
}

func m512MaskCmpleEpu64Mask(k1 uint8, a [64]byte, b [64]byte) uint8


// CmpltEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for less-than,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ( a[i+31:i] < b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm_cmplt_epi32_mask'.
// Requires AVX512F.
func CmpltEpi32Mask(a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(cmpltEpi32Mask([16]byte(a), [16]byte(b)))
}

func cmpltEpi32Mask(a [16]byte, b [16]byte) uint8


// MaskCmpltEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] < b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm_mask_cmplt_epi32_mask'.
// Requires AVX512F.
func MaskCmpltEpi32Mask(k1 x86.Mmask8, a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(maskCmpltEpi32Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpltEpi32Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// M256CmpltEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] < b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_cmplt_epi32_mask'.
// Requires AVX512F.
func M256CmpltEpi32Mask(a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256CmpltEpi32Mask([32]byte(a), [32]byte(b)))
}

func m256CmpltEpi32Mask(a [32]byte, b [32]byte) uint8


// M256MaskCmpltEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] < b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_mask_cmplt_epi32_mask'.
// Requires AVX512F.
func M256MaskCmpltEpi32Mask(k1 x86.Mmask8, a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256MaskCmpltEpi32Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func m256MaskCmpltEpi32Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// M512CmpltEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			k[j] := ( a[i+31:i] < b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm512_cmplt_epi32_mask'.
// Requires AVX512F.
func M512CmpltEpi32Mask(a x86.M512i, b x86.M512i) x86.Mmask16 {
	return x86.Mmask16(m512CmpltEpi32Mask([64]byte(a), [64]byte(b)))
}

func m512CmpltEpi32Mask(a [64]byte, b [64]byte) uint16


// M512MaskCmpltEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] <= b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm512_mask_cmplt_epi32_mask'.
// Requires AVX512F.
func M512MaskCmpltEpi32Mask(k1 x86.Mmask16, a x86.M512i, b x86.M512i) x86.Mmask16 {
	return x86.Mmask16(m512MaskCmpltEpi32Mask(uint16(k1), [64]byte(a), [64]byte(b)))
}

func m512MaskCmpltEpi32Mask(k1 uint16, a [64]byte, b [64]byte) uint16


// CmpltEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for less-than,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ( a[i+63:i] < b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm_cmplt_epi64_mask'.
// Requires AVX512F.
func CmpltEpi64Mask(a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(cmpltEpi64Mask([16]byte(a), [16]byte(b)))
}

func cmpltEpi64Mask(a [16]byte, b [16]byte) uint8


// MaskCmpltEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] < b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm_mask_cmplt_epi64_mask'.
// Requires AVX512F.
func MaskCmpltEpi64Mask(k1 x86.Mmask8, a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(maskCmpltEpi64Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpltEpi64Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// M256CmpltEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] < b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_cmplt_epi64_mask'.
// Requires AVX512F.
func M256CmpltEpi64Mask(a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256CmpltEpi64Mask([32]byte(a), [32]byte(b)))
}

func m256CmpltEpi64Mask(a [32]byte, b [32]byte) uint8


// M256MaskCmpltEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] < b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_mask_cmplt_epi64_mask'.
// Requires AVX512F.
func M256MaskCmpltEpi64Mask(k1 x86.Mmask8, a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256MaskCmpltEpi64Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func m256MaskCmpltEpi64Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// M512CmpltEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ( a[i+63:i] < b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm512_cmplt_epi64_mask'.
// Requires AVX512F.
func M512CmpltEpi64Mask(a x86.M512i, b x86.M512i) x86.Mmask8 {
	return x86.Mmask8(m512CmpltEpi64Mask([64]byte(a), [64]byte(b)))
}

func m512CmpltEpi64Mask(a [64]byte, b [64]byte) uint8


// M512MaskCmpltEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] < b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm512_mask_cmplt_epi64_mask'.
// Requires AVX512F.
func M512MaskCmpltEpi64Mask(k1 x86.Mmask8, a x86.M512i, b x86.M512i) x86.Mmask8 {
	return x86.Mmask8(m512MaskCmpltEpi64Mask(uint8(k1), [64]byte(a), [64]byte(b)))
}

func m512MaskCmpltEpi64Mask(k1 uint8, a [64]byte, b [64]byte) uint8


// CmpltEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ( a[i+31:i] < b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm_cmplt_epu32_mask'.
// Requires AVX512F.
func CmpltEpu32Mask(a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(cmpltEpu32Mask([16]byte(a), [16]byte(b)))
}

func cmpltEpu32Mask(a [16]byte, b [16]byte) uint8


// MaskCmpltEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for less-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] < b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm_mask_cmplt_epu32_mask'.
// Requires AVX512F.
func MaskCmpltEpu32Mask(k1 x86.Mmask8, a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(maskCmpltEpu32Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpltEpu32Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// M256CmpltEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for less-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] < b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_cmplt_epu32_mask'.
// Requires AVX512F.
func M256CmpltEpu32Mask(a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256CmpltEpu32Mask([32]byte(a), [32]byte(b)))
}

func m256CmpltEpu32Mask(a [32]byte, b [32]byte) uint8


// M256MaskCmpltEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and
// 'b' for less-than, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] < b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_mask_cmplt_epu32_mask'.
// Requires AVX512F.
func M256MaskCmpltEpu32Mask(k1 x86.Mmask8, a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256MaskCmpltEpu32Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func m256MaskCmpltEpu32Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpltEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ( a[i+63:i] < b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm_cmplt_epu64_mask'.
// Requires AVX512F.
func CmpltEpu64Mask(a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(cmpltEpu64Mask([16]byte(a), [16]byte(b)))
}

func cmpltEpu64Mask(a [16]byte, b [16]byte) uint8


// MaskCmpltEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for less-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] < b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm_mask_cmplt_epu64_mask'.
// Requires AVX512F.
func MaskCmpltEpu64Mask(k1 x86.Mmask8, a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(maskCmpltEpu64Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpltEpu64Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// M256CmpltEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for less-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] < b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_cmplt_epu64_mask'.
// Requires AVX512F.
func M256CmpltEpu64Mask(a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256CmpltEpu64Mask([32]byte(a), [32]byte(b)))
}

func m256CmpltEpu64Mask(a [32]byte, b [32]byte) uint8


// M256MaskCmpltEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and
// 'b' for less-than, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] < b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_mask_cmplt_epu64_mask'.
// Requires AVX512F.
func M256MaskCmpltEpu64Mask(k1 x86.Mmask8, a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256MaskCmpltEpu64Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func m256MaskCmpltEpu64Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// M512CmpltEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for less-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ( a[i+63:i] < b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm512_cmplt_epu64_mask'.
// Requires AVX512F.
func M512CmpltEpu64Mask(a x86.M512i, b x86.M512i) x86.Mmask8 {
	return x86.Mmask8(m512CmpltEpu64Mask([64]byte(a), [64]byte(b)))
}

func m512CmpltEpu64Mask(a [64]byte, b [64]byte) uint8


// M512MaskCmpltEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and
// 'b' for less-than, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] < b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm512_mask_cmplt_epu64_mask'.
// Requires AVX512F.
func M512MaskCmpltEpu64Mask(k1 x86.Mmask8, a x86.M512i, b x86.M512i) x86.Mmask8 {
	return x86.Mmask8(m512MaskCmpltEpu64Mask(uint8(k1), [64]byte(a), [64]byte(b)))
}

func m512MaskCmpltEpu64Mask(k1 uint8, a [64]byte, b [64]byte) uint8


// CmpneqEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm_cmpneq_epi32_mask'.
// Requires AVX512F.
func CmpneqEpi32Mask(a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(cmpneqEpi32Mask([16]byte(a), [16]byte(b)))
}

func cmpneqEpi32Mask(a [16]byte, b [16]byte) uint8


// MaskCmpneqEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm_mask_cmpneq_epi32_mask'.
// Requires AVX512F.
func MaskCmpneqEpi32Mask(k1 x86.Mmask8, a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(maskCmpneqEpi32Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpneqEpi32Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// M256CmpneqEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_cmpneq_epi32_mask'.
// Requires AVX512F.
func M256CmpneqEpi32Mask(a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256CmpneqEpi32Mask([32]byte(a), [32]byte(b)))
}

func m256CmpneqEpi32Mask(a [32]byte, b [32]byte) uint8


// M256MaskCmpneqEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_mask_cmpneq_epi32_mask'.
// Requires AVX512F.
func M256MaskCmpneqEpi32Mask(k1 x86.Mmask8, a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256MaskCmpneqEpi32Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func m256MaskCmpneqEpi32Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpneqEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm_cmpneq_epi64_mask'.
// Requires AVX512F.
func CmpneqEpi64Mask(a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(cmpneqEpi64Mask([16]byte(a), [16]byte(b)))
}

func cmpneqEpi64Mask(a [16]byte, b [16]byte) uint8


// MaskCmpneqEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm_mask_cmpneq_epi64_mask'.
// Requires AVX512F.
func MaskCmpneqEpi64Mask(k1 x86.Mmask8, a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(maskCmpneqEpi64Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpneqEpi64Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// M256CmpneqEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_cmpneq_epi64_mask'.
// Requires AVX512F.
func M256CmpneqEpi64Mask(a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256CmpneqEpi64Mask([32]byte(a), [32]byte(b)))
}

func m256CmpneqEpi64Mask(a [32]byte, b [32]byte) uint8


// M256MaskCmpneqEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_mask_cmpneq_epi64_mask'.
// Requires AVX512F.
func M256MaskCmpneqEpi64Mask(k1 x86.Mmask8, a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256MaskCmpneqEpi64Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func m256MaskCmpneqEpi64Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// M512CmpneqEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm512_cmpneq_epi64_mask'.
// Requires AVX512F.
func M512CmpneqEpi64Mask(a x86.M512i, b x86.M512i) x86.Mmask8 {
	return x86.Mmask8(m512CmpneqEpi64Mask([64]byte(a), [64]byte(b)))
}

func m512CmpneqEpi64Mask(a [64]byte, b [64]byte) uint8


// M512MaskCmpneqEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm512_mask_cmpneq_epi64_mask'.
// Requires AVX512F.
func M512MaskCmpneqEpi64Mask(k1 x86.Mmask8, a x86.M512i, b x86.M512i) x86.Mmask8 {
	return x86.Mmask8(m512MaskCmpneqEpi64Mask(uint8(k1), [64]byte(a), [64]byte(b)))
}

func m512MaskCmpneqEpi64Mask(k1 uint8, a [64]byte, b [64]byte) uint8


// CmpneqEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm_cmpneq_epu32_mask'.
// Requires AVX512F.
func CmpneqEpu32Mask(a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(cmpneqEpu32Mask([16]byte(a), [16]byte(b)))
}

func cmpneqEpu32Mask(a [16]byte, b [16]byte) uint8


// MaskCmpneqEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for not-equal, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm_mask_cmpneq_epu32_mask'.
// Requires AVX512F.
func MaskCmpneqEpu32Mask(k1 x86.Mmask8, a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(maskCmpneqEpu32Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpneqEpu32Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// M256CmpneqEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for not-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_cmpneq_epu32_mask'.
// Requires AVX512F.
func M256CmpneqEpu32Mask(a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256CmpneqEpu32Mask([32]byte(a), [32]byte(b)))
}

func m256CmpneqEpu32Mask(a [32]byte, b [32]byte) uint8


// M256MaskCmpneqEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and
// 'b' for not-equal, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_mask_cmpneq_epu32_mask'.
// Requires AVX512F.
func M256MaskCmpneqEpu32Mask(k1 x86.Mmask8, a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256MaskCmpneqEpu32Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func m256MaskCmpneqEpu32Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpneqEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm_cmpneq_epu64_mask'.
// Requires AVX512F.
func CmpneqEpu64Mask(a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(cmpneqEpu64Mask([16]byte(a), [16]byte(b)))
}

func cmpneqEpu64Mask(a [16]byte, b [16]byte) uint8


// MaskCmpneqEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for not-equal, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm_mask_cmpneq_epu64_mask'.
// Requires AVX512F.
func MaskCmpneqEpu64Mask(k1 x86.Mmask8, a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(maskCmpneqEpu64Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpneqEpu64Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// M256CmpneqEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for not-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_cmpneq_epu64_mask'.
// Requires AVX512F.
func M256CmpneqEpu64Mask(a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256CmpneqEpu64Mask([32]byte(a), [32]byte(b)))
}

func m256CmpneqEpu64Mask(a [32]byte, b [32]byte) uint8


// M256MaskCmpneqEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and
// 'b' for not-equal, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_mask_cmpneq_epu64_mask'.
// Requires AVX512F.
func M256MaskCmpneqEpu64Mask(k1 x86.Mmask8, a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256MaskCmpneqEpu64Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func m256MaskCmpneqEpu64Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// M512CmpneqEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for not-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm512_cmpneq_epu64_mask'.
// Requires AVX512F.
func M512CmpneqEpu64Mask(a x86.M512i, b x86.M512i) x86.Mmask8 {
	return x86.Mmask8(m512CmpneqEpu64Mask([64]byte(a), [64]byte(b)))
}

func m512CmpneqEpu64Mask(a [64]byte, b [64]byte) uint8


// M512MaskCmpneqEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and
// 'b' for not-equal, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm512_mask_cmpneq_epu64_mask'.
// Requires AVX512F.
func M512MaskCmpneqEpu64Mask(k1 x86.Mmask8, a x86.M512i, b x86.M512i) x86.Mmask8 {
	return x86.Mmask8(m512MaskCmpneqEpu64Mask(uint8(k1), [64]byte(a), [64]byte(b)))
}

func m512MaskCmpneqEpu64Mask(k1 uint8, a [64]byte, b [64]byte) uint8


// ComiRoundSd: Compare the lower double-precision (64-bit) floating-point
// element in 'a' and 'b' based on the comparison operand specified by 'imm8',
// and return the boolean result (0 or 1).
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		
//		RETURN ( a[63:0] OP b[63:0] ) ? 1 : 0
//
// Instruction: 'VCOMISD'. Intrinsic: '_mm_comi_round_sd'.
// Requires AVX512F.
func ComiRoundSd(a x86.M128d, b x86.M128d, imm8 int, sae int) int {
	return int(comiRoundSd([2]float64(a), [2]float64(b), imm8, sae))
}

func comiRoundSd(a [2]float64, b [2]float64, imm8 int, sae int) int


// ComiRoundSs: Compare the lower single-precision (32-bit) floating-point
// element in 'a' and 'b' based on the comparison operand specified by 'imm8',
// and return the boolean result (0 or 1).
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		
//		RETURN ( a[31:0] OP b[31:0] ) ? 1 : 0
//
// Instruction: 'VCOMISS'. Intrinsic: '_mm_comi_round_ss'.
// Requires AVX512F.
func ComiRoundSs(a x86.M128, b x86.M128, imm8 int, sae int) int {
	return int(comiRoundSs([4]float32(a), [4]float32(b), imm8, sae))
}

func comiRoundSs(a [4]float32, b [4]float32, imm8 int, sae int) int


// MaskCompressEpi32: Contiguously store the active 32-bit integers in 'a'
// (those with their respective bit set in writemask 'k') to 'dst', and pass
// through the remaining elements from 'src'. 
//
//		size := 32
//		m := 0
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[127:m] := src[127:m]
//		dst[MAX:128] := 0
//
// Instruction: 'VPCOMPRESSD'. Intrinsic: '_mm_mask_compress_epi32'.
// Requires AVX512F.
func MaskCompressEpi32(src x86.M128i, k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskCompressEpi32([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCompressEpi32(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCompressEpi32: Contiguously store the active 32-bit integers in 'a'
// (those with their respective bit set in zeromask 'k') to 'dst', and set the
// remaining elements to zero. 
//
//		size := 32
//		m := 0
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[127:m] := 0
//		dst[MAX:128] := 0
//
// Instruction: 'VPCOMPRESSD'. Intrinsic: '_mm_maskz_compress_epi32'.
// Requires AVX512F.
func MaskzCompressEpi32(k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskzCompressEpi32(uint8(k), [16]byte(a)))
}

func maskzCompressEpi32(k uint8, a [16]byte) [16]byte


// M256MaskCompressEpi32: Contiguously store the active 32-bit integers in 'a'
// (those with their respective bit set in writemask 'k') to 'dst', and pass
// through the remaining elements from 'src'. 
//
//		size := 32
//		m := 0
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[255:m] := src[255:m]
//		dst[MAX:256] := 0
//
// Instruction: 'VPCOMPRESSD'. Intrinsic: '_mm256_mask_compress_epi32'.
// Requires AVX512F.
func M256MaskCompressEpi32(src x86.M256i, k x86.Mmask8, a x86.M256i) x86.M256i {
	return x86.M256i(m256MaskCompressEpi32([32]byte(src), uint8(k), [32]byte(a)))
}

func m256MaskCompressEpi32(src [32]byte, k uint8, a [32]byte) [32]byte


// M256MaskzCompressEpi32: Contiguously store the active 32-bit integers in 'a'
// (those with their respective bit set in zeromask 'k') to 'dst', and set the
// remaining elements to zero. 
//
//		size := 32
//		m := 0
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[255:m] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VPCOMPRESSD'. Intrinsic: '_mm256_maskz_compress_epi32'.
// Requires AVX512F.
func M256MaskzCompressEpi32(k x86.Mmask8, a x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzCompressEpi32(uint8(k), [32]byte(a)))
}

func m256MaskzCompressEpi32(k uint8, a [32]byte) [32]byte


// M512MaskCompressEpi32: Contiguously store the active 32-bit integers in 'a'
// (those with their respective bit set in writemask 'k') to 'dst', and pass
// through the remaining elements from 'src'. 
//
//		size := 32
//		m := 0
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[511:m] := src[511:m]
//		dst[MAX:512] := 0
//
// Instruction: 'VPCOMPRESSD'. Intrinsic: '_mm512_mask_compress_epi32'.
// Requires AVX512F.
func M512MaskCompressEpi32(src x86.M512i, k x86.Mmask16, a x86.M512i) x86.M512i {
	return x86.M512i(m512MaskCompressEpi32([64]byte(src), uint16(k), [64]byte(a)))
}

func m512MaskCompressEpi32(src [64]byte, k uint16, a [64]byte) [64]byte


// M512MaskzCompressEpi32: Contiguously store the active 32-bit integers in 'a'
// (those with their respective bit set in zeromask 'k') to 'dst', and set the
// remaining elements to zero. 
//
//		size := 32
//		m := 0
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[511:m] := 0
//		dst[MAX:512] := 0
//
// Instruction: 'VPCOMPRESSD'. Intrinsic: '_mm512_maskz_compress_epi32'.
// Requires AVX512F.
func M512MaskzCompressEpi32(k x86.Mmask16, a x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzCompressEpi32(uint16(k), [64]byte(a)))
}

func m512MaskzCompressEpi32(k uint16, a [64]byte) [64]byte


// MaskCompressEpi64: Contiguously store the active 64-bit integers in 'a'
// (those with their respective bit set in writemask 'k') to 'dst', and pass
// through the remaining elements from 'src'. 
//
//		size := 64
//		m := 0
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[127:m] := src[127:m]
//		dst[MAX:128] := 0
//
// Instruction: 'VPCOMPRESSQ'. Intrinsic: '_mm_mask_compress_epi64'.
// Requires AVX512F.
func MaskCompressEpi64(src x86.M128i, k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskCompressEpi64([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCompressEpi64(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCompressEpi64: Contiguously store the active 64-bit integers in 'a'
// (those with their respective bit set in zeromask 'k') to 'dst', and set the
// remaining elements to zero. 
//
//		size := 64
//		m := 0
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[127:m] := 0
//		dst[MAX:128] := 0
//
// Instruction: 'VPCOMPRESSQ'. Intrinsic: '_mm_maskz_compress_epi64'.
// Requires AVX512F.
func MaskzCompressEpi64(k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskzCompressEpi64(uint8(k), [16]byte(a)))
}

func maskzCompressEpi64(k uint8, a [16]byte) [16]byte


// M256MaskCompressEpi64: Contiguously store the active 64-bit integers in 'a'
// (those with their respective bit set in writemask 'k') to 'dst', and pass
// through the remaining elements from 'src'. 
//
//		size := 64
//		m := 0
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[255:m] := src[255:m]
//		dst[MAX:256] := 0
//
// Instruction: 'VPCOMPRESSQ'. Intrinsic: '_mm256_mask_compress_epi64'.
// Requires AVX512F.
func M256MaskCompressEpi64(src x86.M256i, k x86.Mmask8, a x86.M256i) x86.M256i {
	return x86.M256i(m256MaskCompressEpi64([32]byte(src), uint8(k), [32]byte(a)))
}

func m256MaskCompressEpi64(src [32]byte, k uint8, a [32]byte) [32]byte


// M256MaskzCompressEpi64: Contiguously store the active 64-bit integers in 'a'
// (those with their respective bit set in zeromask 'k') to 'dst', and set the
// remaining elements to zero. 
//
//		size := 64
//		m := 0
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[255:m] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VPCOMPRESSQ'. Intrinsic: '_mm256_maskz_compress_epi64'.
// Requires AVX512F.
func M256MaskzCompressEpi64(k x86.Mmask8, a x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzCompressEpi64(uint8(k), [32]byte(a)))
}

func m256MaskzCompressEpi64(k uint8, a [32]byte) [32]byte


// M512MaskCompressEpi64: Contiguously store the active 64-bit integers in 'a'
// (those with their respective bit set in writemask 'k') to 'dst', and pass
// through the remaining elements from 'src'. 
//
//		size := 64
//		m := 0
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[511:m] := src[511:m]
//		dst[MAX:512] := 0
//
// Instruction: 'VPCOMPRESSQ'. Intrinsic: '_mm512_mask_compress_epi64'.
// Requires AVX512F.
func M512MaskCompressEpi64(src x86.M512i, k x86.Mmask8, a x86.M512i) x86.M512i {
	return x86.M512i(m512MaskCompressEpi64([64]byte(src), uint8(k), [64]byte(a)))
}

func m512MaskCompressEpi64(src [64]byte, k uint8, a [64]byte) [64]byte


// M512MaskzCompressEpi64: Contiguously store the active 64-bit integers in 'a'
// (those with their respective bit set in zeromask 'k') to 'dst', and set the
// remaining elements to zero. 
//
//		size := 64
//		m := 0
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[511:m] := 0
//		dst[MAX:512] := 0
//
// Instruction: 'VPCOMPRESSQ'. Intrinsic: '_mm512_maskz_compress_epi64'.
// Requires AVX512F.
func M512MaskzCompressEpi64(k x86.Mmask8, a x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzCompressEpi64(uint8(k), [64]byte(a)))
}

func m512MaskzCompressEpi64(k uint8, a [64]byte) [64]byte


// MaskCompressPd: Contiguously store the active double-precision (64-bit)
// floating-point elements in 'a' (those with their respective bit set in
// writemask 'k') to 'dst', and pass through the remaining elements from 'src'. 
//
//		size := 64
//		m := 0
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[127:m] := src[127:m]
//		dst[MAX:128] := 0
//
// Instruction: 'VCOMPRESSPD'. Intrinsic: '_mm_mask_compress_pd'.
// Requires AVX512F.
func MaskCompressPd(src x86.M128d, k x86.Mmask8, a x86.M128d) x86.M128d {
	return x86.M128d(maskCompressPd([2]float64(src), uint8(k), [2]float64(a)))
}

func maskCompressPd(src [2]float64, k uint8, a [2]float64) [2]float64


// MaskzCompressPd: Contiguously store the active double-precision (64-bit)
// floating-point elements in 'a' (those with their respective bit set in
// zeromask 'k') to 'dst', and set the remaining elements to zero. 
//
//		size := 64
//		m := 0
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[127:m] := 0
//		dst[MAX:128] := 0
//
// Instruction: 'VCOMPRESSPD'. Intrinsic: '_mm_maskz_compress_pd'.
// Requires AVX512F.
func MaskzCompressPd(k x86.Mmask8, a x86.M128d) x86.M128d {
	return x86.M128d(maskzCompressPd(uint8(k), [2]float64(a)))
}

func maskzCompressPd(k uint8, a [2]float64) [2]float64


// M256MaskCompressPd: Contiguously store the active double-precision (64-bit)
// floating-point elements in 'a' (those with their respective bit set in
// writemask 'k') to 'dst', and pass through the remaining elements from 'src'. 
//
//		size := 64
//		m := 0
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[255:m] := src[255:m]
//		dst[MAX:256] := 0
//
// Instruction: 'VCOMPRESSPD'. Intrinsic: '_mm256_mask_compress_pd'.
// Requires AVX512F.
func M256MaskCompressPd(src x86.M256d, k x86.Mmask8, a x86.M256d) x86.M256d {
	return x86.M256d(m256MaskCompressPd([4]float64(src), uint8(k), [4]float64(a)))
}

func m256MaskCompressPd(src [4]float64, k uint8, a [4]float64) [4]float64


// M256MaskzCompressPd: Contiguously store the active double-precision (64-bit)
// floating-point elements in 'a' (those with their respective bit set in
// zeromask 'k') to 'dst', and set the remaining elements to zero. 
//
//		size := 64
//		m := 0
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[255:m] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VCOMPRESSPD'. Intrinsic: '_mm256_maskz_compress_pd'.
// Requires AVX512F.
func M256MaskzCompressPd(k x86.Mmask8, a x86.M256d) x86.M256d {
	return x86.M256d(m256MaskzCompressPd(uint8(k), [4]float64(a)))
}

func m256MaskzCompressPd(k uint8, a [4]float64) [4]float64


// M512MaskCompressPd: Contiguously store the active double-precision (64-bit)
// floating-point elements in 'a' (those with their respective bit set in
// writemask 'k') to 'dst', and pass through the remaining elements from 'src'. 
//
//		size := 64
//		m := 0
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[511:m] := src[511:m]
//		dst[MAX:512] := 0
//
// Instruction: 'VCOMPRESSPD'. Intrinsic: '_mm512_mask_compress_pd'.
// Requires AVX512F.
func M512MaskCompressPd(src x86.M512d, k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskCompressPd([8]float64(src), uint8(k), [8]float64(a)))
}

func m512MaskCompressPd(src [8]float64, k uint8, a [8]float64) [8]float64


// M512MaskzCompressPd: Contiguously store the active double-precision (64-bit)
// floating-point elements in 'a' (those with their respective bit set in
// zeromask 'k') to 'dst', and set the remaining elements to zero. 
//
//		size := 64
//		m := 0
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[511:m] := 0
//		dst[MAX:512] := 0
//
// Instruction: 'VCOMPRESSPD'. Intrinsic: '_mm512_maskz_compress_pd'.
// Requires AVX512F.
func M512MaskzCompressPd(k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskzCompressPd(uint8(k), [8]float64(a)))
}

func m512MaskzCompressPd(k uint8, a [8]float64) [8]float64


// MaskCompressPs: Contiguously store the active single-precision (32-bit)
// floating-point elements in 'a' (those with their respective bit set in
// writemask 'k') to 'dst', and pass through the remaining elements from 'src'. 
//
//		size := 32
//		m := 0
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[127:m] := src[127:m]
//		dst[MAX:128] := 0
//
// Instruction: 'VCOMPRESSPS'. Intrinsic: '_mm_mask_compress_ps'.
// Requires AVX512F.
func MaskCompressPs(src x86.M128, k x86.Mmask8, a x86.M128) x86.M128 {
	return x86.M128(maskCompressPs([4]float32(src), uint8(k), [4]float32(a)))
}

func maskCompressPs(src [4]float32, k uint8, a [4]float32) [4]float32


// MaskzCompressPs: Contiguously store the active single-precision (32-bit)
// floating-point elements in 'a' (those with their respective bit set in
// zeromask 'k') to 'dst', and set the remaining elements to zero. 
//
//		size := 32
//		m := 0
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[127:m] := 0
//		dst[MAX:128] := 0
//
// Instruction: 'VCOMPRESSPS'. Intrinsic: '_mm_maskz_compress_ps'.
// Requires AVX512F.
func MaskzCompressPs(k x86.Mmask8, a x86.M128) x86.M128 {
	return x86.M128(maskzCompressPs(uint8(k), [4]float32(a)))
}

func maskzCompressPs(k uint8, a [4]float32) [4]float32


// M256MaskCompressPs: Contiguously store the active single-precision (32-bit)
// floating-point elements in 'a' (those with their respective bit set in
// writemask 'k') to 'dst', and pass through the remaining elements from 'src'. 
//
//		size := 32
//		m := 0
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[255:m] := src[255:m]
//		dst[MAX:256] := 0
//
// Instruction: 'VCOMPRESSPS'. Intrinsic: '_mm256_mask_compress_ps'.
// Requires AVX512F.
func M256MaskCompressPs(src x86.M256, k x86.Mmask8, a x86.M256) x86.M256 {
	return x86.M256(m256MaskCompressPs([8]float32(src), uint8(k), [8]float32(a)))
}

func m256MaskCompressPs(src [8]float32, k uint8, a [8]float32) [8]float32


// M256MaskzCompressPs: Contiguously store the active single-precision (32-bit)
// floating-point elements in 'a' (those with their respective bit set in
// zeromask 'k') to 'dst', and set the remaining elements to zero. 
//
//		size := 32
//		m := 0
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[255:m] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VCOMPRESSPS'. Intrinsic: '_mm256_maskz_compress_ps'.
// Requires AVX512F.
func M256MaskzCompressPs(k x86.Mmask8, a x86.M256) x86.M256 {
	return x86.M256(m256MaskzCompressPs(uint8(k), [8]float32(a)))
}

func m256MaskzCompressPs(k uint8, a [8]float32) [8]float32


// M512MaskCompressPs: Contiguously store the active single-precision (32-bit)
// floating-point elements in 'a' (those with their respective bit set in
// writemask 'k') to 'dst', and pass through the remaining elements from 'src'. 
//
//		size := 32
//		m := 0
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[511:m] := src[511:m]
//		dst[MAX:512] := 0
//
// Instruction: 'VCOMPRESSPS'. Intrinsic: '_mm512_mask_compress_ps'.
// Requires AVX512F.
func M512MaskCompressPs(src x86.M512, k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskCompressPs([16]float32(src), uint16(k), [16]float32(a)))
}

func m512MaskCompressPs(src [16]float32, k uint16, a [16]float32) [16]float32


// M512MaskzCompressPs: Contiguously store the active single-precision (32-bit)
// floating-point elements in 'a' (those with their respective bit set in
// zeromask 'k') to 'dst', and set the remaining elements to zero. 
//
//		size := 32
//		m := 0
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[511:m] := 0
//		dst[MAX:512] := 0
//
// Instruction: 'VCOMPRESSPS'. Intrinsic: '_mm512_maskz_compress_ps'.
// Requires AVX512F.
func M512MaskzCompressPs(k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskzCompressPs(uint16(k), [16]float32(a)))
}

func m512MaskzCompressPs(k uint16, a [16]float32) [16]float32


// Skipped: _mm_mask_compressstoreu_epi32. Contains pointer parameter.


// Skipped: _mm256_mask_compressstoreu_epi32. Contains pointer parameter.


// Skipped: _mm512_mask_compressstoreu_epi32. Contains pointer parameter.


// Skipped: _mm_mask_compressstoreu_epi64. Contains pointer parameter.


// Skipped: _mm256_mask_compressstoreu_epi64. Contains pointer parameter.


// Skipped: _mm512_mask_compressstoreu_epi64. Contains pointer parameter.


// Skipped: _mm_mask_compressstoreu_pd. Contains pointer parameter.


// Skipped: _mm256_mask_compressstoreu_pd. Contains pointer parameter.


// Skipped: _mm512_mask_compressstoreu_pd. Contains pointer parameter.


// Skipped: _mm_mask_compressstoreu_ps. Contains pointer parameter.


// Skipped: _mm256_mask_compressstoreu_ps. Contains pointer parameter.


// Skipped: _mm512_mask_compressstoreu_ps. Contains pointer parameter.


// M512CosPd: Compute the cosine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := COS(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_cos_pd'.
// Requires AVX512F.
func M512CosPd(a x86.M512d) x86.M512d {
	return x86.M512d(m512CosPd([8]float64(a)))
}

func m512CosPd(a [8]float64) [8]float64


// M512MaskCosPd: Compute the cosine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := COS(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_cos_pd'.
// Requires AVX512F.
func M512MaskCosPd(src x86.M512d, k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskCosPd([8]float64(src), uint8(k), [8]float64(a)))
}

func m512MaskCosPd(src [8]float64, k uint8, a [8]float64) [8]float64


// M512CosPs: Compute the cosine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := COS(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_cos_ps'.
// Requires AVX512F.
func M512CosPs(a x86.M512) x86.M512 {
	return x86.M512(m512CosPs([16]float32(a)))
}

func m512CosPs(a [16]float32) [16]float32


// M512MaskCosPs: Compute the cosine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := COS(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_cos_ps'.
// Requires AVX512F.
func M512MaskCosPs(src x86.M512, k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskCosPs([16]float32(src), uint16(k), [16]float32(a)))
}

func m512MaskCosPs(src [16]float32, k uint16, a [16]float32) [16]float32


// M512CosdPd: Compute the cosine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in degrees, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := COSD(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_cosd_pd'.
// Requires AVX512F.
func M512CosdPd(a x86.M512d) x86.M512d {
	return x86.M512d(m512CosdPd([8]float64(a)))
}

func m512CosdPd(a [8]float64) [8]float64


// M512MaskCosdPd: Compute the cosine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in degrees, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := COSD(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_cosd_pd'.
// Requires AVX512F.
func M512MaskCosdPd(src x86.M512d, k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskCosdPd([8]float64(src), uint8(k), [8]float64(a)))
}

func m512MaskCosdPd(src [8]float64, k uint8, a [8]float64) [8]float64


// M512CosdPs: Compute the cosine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in degrees, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := COSD(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_cosd_ps'.
// Requires AVX512F.
func M512CosdPs(a x86.M512) x86.M512 {
	return x86.M512(m512CosdPs([16]float32(a)))
}

func m512CosdPs(a [16]float32) [16]float32


// M512MaskCosdPs: Compute the cosine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in degrees, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := COSD(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_cosd_ps'.
// Requires AVX512F.
func M512MaskCosdPs(src x86.M512, k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskCosdPs([16]float32(src), uint16(k), [16]float32(a)))
}

func m512MaskCosdPs(src [16]float32, k uint16, a [16]float32) [16]float32


// M512CoshPd: Compute the hyperbolic cosine of packed double-precision
// (64-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := COSH(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_cosh_pd'.
// Requires AVX512F.
func M512CoshPd(a x86.M512d) x86.M512d {
	return x86.M512d(m512CoshPd([8]float64(a)))
}

func m512CoshPd(a [8]float64) [8]float64


// M512MaskCoshPd: Compute the hyperbolic cosine of packed double-precision
// (64-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := COSH(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_cosh_pd'.
// Requires AVX512F.
func M512MaskCoshPd(src x86.M512d, k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskCoshPd([8]float64(src), uint8(k), [8]float64(a)))
}

func m512MaskCoshPd(src [8]float64, k uint8, a [8]float64) [8]float64


// M512CoshPs: Compute the hyperbolic cosine of packed single-precision
// (32-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := COSH(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_cosh_ps'.
// Requires AVX512F.
func M512CoshPs(a x86.M512) x86.M512 {
	return x86.M512(m512CoshPs([16]float32(a)))
}

func m512CoshPs(a [16]float32) [16]float32


// M512MaskCoshPs: Compute the hyperbolic cosine of packed single-precision
// (32-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := COSH(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_cosh_ps'.
// Requires AVX512F.
func M512MaskCoshPs(src x86.M512, k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskCoshPs([16]float32(src), uint16(k), [16]float32(a)))
}

func m512MaskCoshPs(src [16]float32, k uint16, a [16]float32) [16]float32


// M512CvtRoundepi32Ps: Convert packed 32-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTDQ2PS'. Intrinsic: '_mm512_cvt_roundepi32_ps'.
// Requires AVX512F.
func M512CvtRoundepi32Ps(a x86.M512i, rounding int) x86.M512 {
	return x86.M512(m512CvtRoundepi32Ps([64]byte(a), rounding))
}

func m512CvtRoundepi32Ps(a [64]byte, rounding int) [16]float32


// M512MaskCvtRoundepi32Ps: Convert packed 32-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTDQ2PS'. Intrinsic: '_mm512_mask_cvt_roundepi32_ps'.
// Requires AVX512F.
func M512MaskCvtRoundepi32Ps(src x86.M512, k x86.Mmask16, a x86.M512i, rounding int) x86.M512 {
	return x86.M512(m512MaskCvtRoundepi32Ps([16]float32(src), uint16(k), [64]byte(a), rounding))
}

func m512MaskCvtRoundepi32Ps(src [16]float32, k uint16, a [64]byte, rounding int) [16]float32


// M512MaskzCvtRoundepi32Ps: Convert packed 32-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTDQ2PS'. Intrinsic: '_mm512_maskz_cvt_roundepi32_ps'.
// Requires AVX512F.
func M512MaskzCvtRoundepi32Ps(k x86.Mmask16, a x86.M512i, rounding int) x86.M512 {
	return x86.M512(m512MaskzCvtRoundepi32Ps(uint16(k), [64]byte(a), rounding))
}

func m512MaskzCvtRoundepi32Ps(k uint16, a [64]byte, rounding int) [16]float32


// M512CvtRoundepu32Ps: Convert packed unsigned 32-bit integers in 'a' to
// packed single-precision (32-bit) floating-point elements, and store the
// results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := ConvertUnsignedInt32_To_FP32(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTUDQ2PS'. Intrinsic: '_mm512_cvt_roundepu32_ps'.
// Requires AVX512F.
func M512CvtRoundepu32Ps(a x86.M512i, rounding int) x86.M512 {
	return x86.M512(m512CvtRoundepu32Ps([64]byte(a), rounding))
}

func m512CvtRoundepu32Ps(a [64]byte, rounding int) [16]float32


// M512MaskCvtRoundepu32Ps: Convert packed unsigned 32-bit integers in 'a' to
// packed single-precision (32-bit) floating-point elements, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ConvertUnsignedInt32_To_FP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTUDQ2PS'. Intrinsic: '_mm512_mask_cvt_roundepu32_ps'.
// Requires AVX512F.
func M512MaskCvtRoundepu32Ps(src x86.M512, k x86.Mmask16, a x86.M512i, rounding int) x86.M512 {
	return x86.M512(m512MaskCvtRoundepu32Ps([16]float32(src), uint16(k), [64]byte(a), rounding))
}

func m512MaskCvtRoundepu32Ps(src [16]float32, k uint16, a [64]byte, rounding int) [16]float32


// M512MaskzCvtRoundepu32Ps: Convert packed unsigned 32-bit integers in 'a' to
// packed single-precision (32-bit) floating-point elements, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := ConvertUnsignedInt32_To_FP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTUDQ2PS'. Intrinsic: '_mm512_maskz_cvt_roundepu32_ps'.
// Requires AVX512F.
func M512MaskzCvtRoundepu32Ps(k x86.Mmask16, a x86.M512i, rounding int) x86.M512 {
	return x86.M512(m512MaskzCvtRoundepu32Ps(uint16(k), [64]byte(a), rounding))
}

func m512MaskzCvtRoundepu32Ps(k uint16, a [64]byte, rounding int) [16]float32


// CvtRoundi32Ss: Convert the 32-bit integer 'b' to a single-precision (32-bit)
// floating-point element, store the result in the lower element of 'dst', and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_Int32_To_FP32(b[31:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTSI2SS'. Intrinsic: '_mm_cvt_roundi32_ss'.
// Requires AVX512F.
func CvtRoundi32Ss(a x86.M128, b int, rounding int) x86.M128 {
	return x86.M128(cvtRoundi32Ss([4]float32(a), b, rounding))
}

func cvtRoundi32Ss(a [4]float32, b int, rounding int) [4]float32


// CvtRoundi64Sd: Convert the 64-bit integer 'b' to a double-precision (64-bit)
// floating-point element, store the result in the lower element of 'dst', and
// copy the upper element from 'a' to the upper element of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_Int64_To_FP64(b[63:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTSI2SD'. Intrinsic: '_mm_cvt_roundi64_sd'.
// Requires AVX512F.
func CvtRoundi64Sd(a x86.M128d, b int64, rounding int) x86.M128d {
	return x86.M128d(cvtRoundi64Sd([2]float64(a), b, rounding))
}

func cvtRoundi64Sd(a [2]float64, b int64, rounding int) [2]float64


// CvtRoundi64Ss: Convert the 64-bit integer 'b' to a single-precision (32-bit)
// floating-point element, store the result in the lower element of 'dst', and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_Int64_To_FP32(b[63:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTSI2SS'. Intrinsic: '_mm_cvt_roundi64_ss'.
// Requires AVX512F.
func CvtRoundi64Ss(a x86.M128, b int64, rounding int) x86.M128 {
	return x86.M128(cvtRoundi64Ss([4]float32(a), b, rounding))
}

func cvtRoundi64Ss(a [4]float32, b int64, rounding int) [4]float32


// M512CvtRoundpdEpi32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_Int32(a[k+63:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2DQ'. Intrinsic: '_mm512_cvt_roundpd_epi32'.
// Requires AVX512F.
func M512CvtRoundpdEpi32(a x86.M512d, rounding int) x86.M256i {
	return x86.M256i(m512CvtRoundpdEpi32([8]float64(a), rounding))
}

func m512CvtRoundpdEpi32(a [8]float64, rounding int) [32]byte


// M512MaskCvtRoundpdEpi32: Convert packed double-precision (64-bit)
// floating-point elements in 'a' to packed 32-bit integers, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*32
//			l := j*64
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_Int32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2DQ'. Intrinsic: '_mm512_mask_cvt_roundpd_epi32'.
// Requires AVX512F.
func M512MaskCvtRoundpdEpi32(src x86.M256i, k x86.Mmask8, a x86.M512d, rounding int) x86.M256i {
	return x86.M256i(m512MaskCvtRoundpdEpi32([32]byte(src), uint8(k), [8]float64(a), rounding))
}

func m512MaskCvtRoundpdEpi32(src [32]byte, k uint8, a [8]float64, rounding int) [32]byte


// M512MaskzCvtRoundpdEpi32: Convert packed double-precision (64-bit)
// floating-point elements in 'a' to packed 32-bit integers, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_Int32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2DQ'. Intrinsic: '_mm512_maskz_cvt_roundpd_epi32'.
// Requires AVX512F.
func M512MaskzCvtRoundpdEpi32(k x86.Mmask8, a x86.M512d, rounding int) x86.M256i {
	return x86.M256i(m512MaskzCvtRoundpdEpi32(uint8(k), [8]float64(a), rounding))
}

func m512MaskzCvtRoundpdEpi32(k uint8, a [8]float64, rounding int) [32]byte


// M512CvtRoundpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[k+63:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2UDQ'. Intrinsic: '_mm512_cvt_roundpd_epu32'.
// Requires AVX512F.
func M512CvtRoundpdEpu32(a x86.M512d, rounding int) x86.M256i {
	return x86.M256i(m512CvtRoundpdEpu32([8]float64(a), rounding))
}

func m512CvtRoundpdEpu32(a [8]float64, rounding int) [32]byte


// M512MaskCvtRoundpdEpu32: Convert packed double-precision (64-bit)
// floating-point elements in 'a' to packed unsigned 32-bit integers, and store
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*32
//			l := j*64
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2UDQ'. Intrinsic: '_mm512_mask_cvt_roundpd_epu32'.
// Requires AVX512F.
func M512MaskCvtRoundpdEpu32(src x86.M256i, k x86.Mmask8, a x86.M512d, rounding int) x86.M256i {
	return x86.M256i(m512MaskCvtRoundpdEpu32([32]byte(src), uint8(k), [8]float64(a), rounding))
}

func m512MaskCvtRoundpdEpu32(src [32]byte, k uint8, a [8]float64, rounding int) [32]byte


// M512MaskzCvtRoundpdEpu32: Convert packed double-precision (64-bit)
// floating-point elements in 'a' to packed unsigned 32-bit integers, and store
// the results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2UDQ'. Intrinsic: '_mm512_maskz_cvt_roundpd_epu32'.
// Requires AVX512F.
func M512MaskzCvtRoundpdEpu32(k x86.Mmask8, a x86.M512d, rounding int) x86.M256i {
	return x86.M256i(m512MaskzCvtRoundpdEpu32(uint8(k), [8]float64(a), rounding))
}

func m512MaskzCvtRoundpdEpu32(k uint8, a [8]float64, rounding int) [32]byte


// M512CvtRoundpdPs: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed single-precision (32-bit) floating-point elements,
// and store the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_FP32(a[k+63:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2PS'. Intrinsic: '_mm512_cvt_roundpd_ps'.
// Requires AVX512F.
func M512CvtRoundpdPs(a x86.M512d, rounding int) x86.M256 {
	return x86.M256(m512CvtRoundpdPs([8]float64(a), rounding))
}

func m512CvtRoundpdPs(a [8]float64, rounding int) [8]float32


// M512MaskCvtRoundpdPs: Convert packed double-precision (64-bit)
// floating-point elements in 'a' to packed single-precision (32-bit)
// floating-point elements, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*32
//			l := j*64
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_FP32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2PS'. Intrinsic: '_mm512_mask_cvt_roundpd_ps'.
// Requires AVX512F.
func M512MaskCvtRoundpdPs(src x86.M256, k x86.Mmask8, a x86.M512d, rounding int) x86.M256 {
	return x86.M256(m512MaskCvtRoundpdPs([8]float32(src), uint8(k), [8]float64(a), rounding))
}

func m512MaskCvtRoundpdPs(src [8]float32, k uint8, a [8]float64, rounding int) [8]float32


// M512MaskzCvtRoundpdPs: Convert packed double-precision (64-bit)
// floating-point elements in 'a' to packed single-precision (32-bit)
// floating-point elements, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*32
//			l := j*64
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_FP32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2PS'. Intrinsic: '_mm512_maskz_cvt_roundpd_ps'.
// Requires AVX512F.
func M512MaskzCvtRoundpdPs(k x86.Mmask8, a x86.M512d, rounding int) x86.M256 {
	return x86.M256(m512MaskzCvtRoundpdPs(uint8(k), [8]float64(a), rounding))
}

func m512MaskzCvtRoundpdPs(k uint8, a [8]float64, rounding int) [8]float32


// M512CvtRoundphPs: Convert packed half-precision (16-bit) floating-point
// elements in 'a' to packed single-precision (32-bit) floating-point elements,
// and store the results in 'dst'. 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 15
//			i := j*32
//			m := j*16
//			dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPH2PS'. Intrinsic: '_mm512_cvt_roundph_ps'.
// Requires AVX512F.
func M512CvtRoundphPs(a x86.M256i, sae int) x86.M512 {
	return x86.M512(m512CvtRoundphPs([32]byte(a), sae))
}

func m512CvtRoundphPs(a [32]byte, sae int) [16]float32


// M512MaskCvtRoundphPs: Convert packed half-precision (16-bit) floating-point
// elements in 'a' to packed single-precision (32-bit) floating-point elements,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 15
//			i := j*32
//			m := j*16
//			IF k[j]
//				dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPH2PS'. Intrinsic: '_mm512_mask_cvt_roundph_ps'.
// Requires AVX512F.
func M512MaskCvtRoundphPs(src x86.M512, k x86.Mmask16, a x86.M256i, sae int) x86.M512 {
	return x86.M512(m512MaskCvtRoundphPs([16]float32(src), uint16(k), [32]byte(a), sae))
}

func m512MaskCvtRoundphPs(src [16]float32, k uint16, a [32]byte, sae int) [16]float32


// M512MaskzCvtRoundphPs: Convert packed half-precision (16-bit) floating-point
// elements in 'a' to packed single-precision (32-bit) floating-point elements,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 15
//			i := j*32
//			m := j*16
//			IF k[j]
//				dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPH2PS'. Intrinsic: '_mm512_maskz_cvt_roundph_ps'.
// Requires AVX512F.
func M512MaskzCvtRoundphPs(k x86.Mmask16, a x86.M256i, sae int) x86.M512 {
	return x86.M512(m512MaskzCvtRoundphPs(uint16(k), [32]byte(a), sae))
}

func m512MaskzCvtRoundphPs(k uint16, a [32]byte, sae int) [16]float32


// M512CvtRoundpsEpi32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2DQ'. Intrinsic: '_mm512_cvt_roundps_epi32'.
// Requires AVX512F.
func M512CvtRoundpsEpi32(a x86.M512, rounding int) x86.M512i {
	return x86.M512i(m512CvtRoundpsEpi32([16]float32(a), rounding))
}

func m512CvtRoundpsEpi32(a [16]float32, rounding int) [64]byte


// M512MaskCvtRoundpsEpi32: Convert packed single-precision (32-bit)
// floating-point elements in 'a' to packed 32-bit integers, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2DQ'. Intrinsic: '_mm512_mask_cvt_roundps_epi32'.
// Requires AVX512F.
func M512MaskCvtRoundpsEpi32(src x86.M512i, k x86.Mmask16, a x86.M512, rounding int) x86.M512i {
	return x86.M512i(m512MaskCvtRoundpsEpi32([64]byte(src), uint16(k), [16]float32(a), rounding))
}

func m512MaskCvtRoundpsEpi32(src [64]byte, k uint16, a [16]float32, rounding int) [64]byte


// M512MaskzCvtRoundpsEpi32: Convert packed single-precision (32-bit)
// floating-point elements in 'a' to packed 32-bit integers, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2DQ'. Intrinsic: '_mm512_maskz_cvt_roundps_epi32'.
// Requires AVX512F.
func M512MaskzCvtRoundpsEpi32(k x86.Mmask16, a x86.M512, rounding int) x86.M512i {
	return x86.M512i(m512MaskzCvtRoundpsEpi32(uint16(k), [16]float32(a), rounding))
}

func m512MaskzCvtRoundpsEpi32(k uint16, a [16]float32, rounding int) [64]byte


// M512CvtRoundpsEpu32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2UDQ'. Intrinsic: '_mm512_cvt_roundps_epu32'.
// Requires AVX512F.
func M512CvtRoundpsEpu32(a x86.M512, rounding int) x86.M512i {
	return x86.M512i(m512CvtRoundpsEpu32([16]float32(a), rounding))
}

func m512CvtRoundpsEpu32(a [16]float32, rounding int) [64]byte


// M512MaskCvtRoundpsEpu32: Convert packed single-precision (32-bit)
// floating-point elements in 'a' to packed unsigned 32-bit integers, and store
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2UDQ'. Intrinsic: '_mm512_mask_cvt_roundps_epu32'.
// Requires AVX512F.
func M512MaskCvtRoundpsEpu32(src x86.M512i, k x86.Mmask16, a x86.M512, rounding int) x86.M512i {
	return x86.M512i(m512MaskCvtRoundpsEpu32([64]byte(src), uint16(k), [16]float32(a), rounding))
}

func m512MaskCvtRoundpsEpu32(src [64]byte, k uint16, a [16]float32, rounding int) [64]byte


// M512MaskzCvtRoundpsEpu32: Convert packed single-precision (32-bit)
// floating-point elements in 'a' to packed unsigned 32-bit integers, and store
// the results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2UDQ'. Intrinsic: '_mm512_maskz_cvt_roundps_epu32'.
// Requires AVX512F.
func M512MaskzCvtRoundpsEpu32(k x86.Mmask16, a x86.M512, rounding int) x86.M512i {
	return x86.M512i(m512MaskzCvtRoundpsEpu32(uint16(k), [16]float32(a), rounding))
}

func m512MaskzCvtRoundpsEpu32(k uint16, a [16]float32, rounding int) [64]byte


// M512CvtRoundpsPd: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed double-precision (64-bit) floating-point elements,
// and store the results in 'dst'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 32*j
//			dst[i+63:i] := Convert_FP32_To_FP64(a[k+31:k])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2PD'. Intrinsic: '_mm512_cvt_roundps_pd'.
// Requires AVX512F.
func M512CvtRoundpsPd(a x86.M256, sae int) x86.M512d {
	return x86.M512d(m512CvtRoundpsPd([8]float32(a), sae))
}

func m512CvtRoundpsPd(a [8]float32, sae int) [8]float64


// M512MaskCvtRoundpsPd: Convert packed single-precision (32-bit)
// floating-point elements in 'a' to packed double-precision (64-bit)
// floating-point elements, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set).
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_FP64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2PD'. Intrinsic: '_mm512_mask_cvt_roundps_pd'.
// Requires AVX512F.
func M512MaskCvtRoundpsPd(src x86.M512d, k x86.Mmask8, a x86.M256, sae int) x86.M512d {
	return x86.M512d(m512MaskCvtRoundpsPd([8]float64(src), uint8(k), [8]float32(a), sae))
}

func m512MaskCvtRoundpsPd(src [8]float64, k uint8, a [8]float32, sae int) [8]float64


// M512MaskzCvtRoundpsPd: Convert packed single-precision (32-bit)
// floating-point elements in 'a' to packed double-precision (64-bit)
// floating-point elements, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set).
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_FP64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2PD'. Intrinsic: '_mm512_maskz_cvt_roundps_pd'.
// Requires AVX512F.
func M512MaskzCvtRoundpsPd(k x86.Mmask8, a x86.M256, sae int) x86.M512d {
	return x86.M512d(m512MaskzCvtRoundpsPd(uint8(k), [8]float32(a), sae))
}

func m512MaskzCvtRoundpsPd(k uint8, a [8]float32, sae int) [8]float64


// MaskCvtRoundpsPh: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed half-precision (16-bit) floating-point elements,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 3
//			i := 16*j
//			l := 32*j
//			IF k[j]
//				dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm_mask_cvt_roundps_ph'.
// Requires AVX512F.
func MaskCvtRoundpsPh(src x86.M128i, k x86.Mmask8, a x86.M128, rounding int) x86.M128i {
	return x86.M128i(maskCvtRoundpsPh([16]byte(src), uint8(k), [4]float32(a), rounding))
}

func maskCvtRoundpsPh(src [16]byte, k uint8, a [4]float32, rounding int) [16]byte


// MaskzCvtRoundpsPh: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed half-precision (16-bit) floating-point elements,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 3
//			i := 16*j
//			l := 32*j
//			IF k[j]
//				dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm_maskz_cvt_roundps_ph'.
// Requires AVX512F.
func MaskzCvtRoundpsPh(k x86.Mmask8, a x86.M128, rounding int) x86.M128i {
	return x86.M128i(maskzCvtRoundpsPh(uint8(k), [4]float32(a), rounding))
}

func maskzCvtRoundpsPh(k uint8, a [4]float32, rounding int) [16]byte


// M256MaskCvtRoundpsPh: Convert packed single-precision (32-bit)
// floating-point elements in 'a' to packed half-precision (16-bit)
// floating-point elements, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := 16*j
//			l := 32*j
//			IF k[j]
//				dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm256_mask_cvt_roundps_ph'.
// Requires AVX512F.
func M256MaskCvtRoundpsPh(src x86.M128i, k x86.Mmask8, a x86.M256, rounding int) x86.M128i {
	return x86.M128i(m256MaskCvtRoundpsPh([16]byte(src), uint8(k), [8]float32(a), rounding))
}

func m256MaskCvtRoundpsPh(src [16]byte, k uint8, a [8]float32, rounding int) [16]byte


// M256MaskzCvtRoundpsPh: Convert packed single-precision (32-bit)
// floating-point elements in 'a' to packed half-precision (16-bit)
// floating-point elements, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := 16*j
//			l := 32*j
//			IF k[j]
//				dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm256_maskz_cvt_roundps_ph'.
// Requires AVX512F.
func M256MaskzCvtRoundpsPh(k x86.Mmask8, a x86.M256, rounding int) x86.M128i {
	return x86.M128i(m256MaskzCvtRoundpsPh(uint8(k), [8]float32(a), rounding))
}

func m256MaskzCvtRoundpsPh(k uint8, a [8]float32, rounding int) [16]byte


// M512CvtRoundpsPh: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed half-precision (16-bit) floating-point elements,
// and store the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 16*j
//			l := 32*j
//			dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm512_cvt_roundps_ph'.
// Requires AVX512F.
func M512CvtRoundpsPh(a x86.M512, rounding int) x86.M256i {
	return x86.M256i(m512CvtRoundpsPh([16]float32(a), rounding))
}

func m512CvtRoundpsPh(a [16]float32, rounding int) [32]byte


// M512MaskCvtRoundpsPh: Convert packed single-precision (32-bit)
// floating-point elements in 'a' to packed half-precision (16-bit)
// floating-point elements, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 16*j
//			l := 32*j
//			IF k[j]
//				dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm512_mask_cvt_roundps_ph'.
// Requires AVX512F.
func M512MaskCvtRoundpsPh(src x86.M256i, k x86.Mmask16, a x86.M512, rounding int) x86.M256i {
	return x86.M256i(m512MaskCvtRoundpsPh([32]byte(src), uint16(k), [16]float32(a), rounding))
}

func m512MaskCvtRoundpsPh(src [32]byte, k uint16, a [16]float32, rounding int) [32]byte


// M512MaskzCvtRoundpsPh: Convert packed single-precision (32-bit)
// floating-point elements in 'a' to packed half-precision (16-bit)
// floating-point elements, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 16*j
//			l := 32*j
//			IF k[j]
//				dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm512_maskz_cvt_roundps_ph'.
// Requires AVX512F.
func M512MaskzCvtRoundpsPh(k x86.Mmask16, a x86.M512, rounding int) x86.M256i {
	return x86.M256i(m512MaskzCvtRoundpsPh(uint16(k), [16]float32(a), rounding))
}

func m512MaskzCvtRoundpsPh(k uint16, a [16]float32, rounding int) [32]byte


// CvtRoundsdI32: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to a 32-bit integer, and store the result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_FP64_To_Int32(a[63:0])
//
// Instruction: 'VCVTSD2SI'. Intrinsic: '_mm_cvt_roundsd_i32'.
// Requires AVX512F.
func CvtRoundsdI32(a x86.M128d, rounding int) int {
	return int(cvtRoundsdI32([2]float64(a), rounding))
}

func cvtRoundsdI32(a [2]float64, rounding int) int


// CvtRoundsdI64: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to a 64-bit integer, and store the result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_FP64_To_Int64(a[63:0])
//
// Instruction: 'VCVTSD2SI'. Intrinsic: '_mm_cvt_roundsd_i64'.
// Requires AVX512F.
func CvtRoundsdI64(a x86.M128d, rounding int) int64 {
	return int64(cvtRoundsdI64([2]float64(a), rounding))
}

func cvtRoundsdI64(a [2]float64, rounding int) int64


// CvtRoundsdSi32: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to a 32-bit integer, and store the result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_FP64_To_Int32(a[63:0])
//
// Instruction: 'VCVTSD2SI'. Intrinsic: '_mm_cvt_roundsd_si32'.
// Requires AVX512F.
func CvtRoundsdSi32(a x86.M128d, rounding int) int {
	return int(cvtRoundsdSi32([2]float64(a), rounding))
}

func cvtRoundsdSi32(a [2]float64, rounding int) int


// CvtRoundsdSi64: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to a 64-bit integer, and store the result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_FP64_To_Int64(a[63:0])
//
// Instruction: 'VCVTSD2SI'. Intrinsic: '_mm_cvt_roundsd_si64'.
// Requires AVX512F.
func CvtRoundsdSi64(a x86.M128d, rounding int) int64 {
	return int64(cvtRoundsdSi64([2]float64(a), rounding))
}

func cvtRoundsdSi64(a [2]float64, rounding int) int64


// CvtRoundsdSs: Convert the lower double-precision (64-bit) floating-point
// element in 'b' to a single-precision (32-bit) floating-point element, store
// the result in the lower element of 'dst', and copy the upper 3 packed
// elements from 'a' to the upper elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_FP64_To_FP32(b[63:0])
//		dst[127:32] := a[127:31]
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTSD2SS'. Intrinsic: '_mm_cvt_roundsd_ss'.
// Requires AVX512F.
func CvtRoundsdSs(a x86.M128, b x86.M128d, rounding int) x86.M128 {
	return x86.M128(cvtRoundsdSs([4]float32(a), [2]float64(b), rounding))
}

func cvtRoundsdSs(a [4]float32, b [2]float64, rounding int) [4]float32


// MaskCvtRoundsdSs: Convert the lower double-precision (64-bit) floating-point
// element in 'b' to a single-precision (32-bit) floating-point element, store
// the result in the lower element of 'dst' using writemask 'k' (the element is
// copied from 'src' when mask bit 0 is not set), and copy the upper element
// from 'a' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := Convert_FP64_To_FP32(b[63:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:31]
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTSD2SS'. Intrinsic: '_mm_mask_cvt_roundsd_ss'.
// Requires AVX512F.
func MaskCvtRoundsdSs(src x86.M128, k x86.Mmask8, a x86.M128, b x86.M128d, rounding int) x86.M128 {
	return x86.M128(maskCvtRoundsdSs([4]float32(src), uint8(k), [4]float32(a), [2]float64(b), rounding))
}

func maskCvtRoundsdSs(src [4]float32, k uint8, a [4]float32, b [2]float64, rounding int) [4]float32


// MaskzCvtRoundsdSs: Convert the lower double-precision (64-bit)
// floating-point element in 'b' to a single-precision (32-bit) floating-point
// element, store the result in the lower element of 'dst' using zeromask 'k'
// (the element is zeroed out when mask bit 0 is not set), and copy the upper 3
// packed elements from 'a' to the upper elements of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := Convert_FP64_To_FP32(b[63:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:31]
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTSD2SS'. Intrinsic: '_mm_maskz_cvt_roundsd_ss'.
// Requires AVX512F.
func MaskzCvtRoundsdSs(k x86.Mmask8, a x86.M128, b x86.M128d, rounding int) x86.M128 {
	return x86.M128(maskzCvtRoundsdSs(uint8(k), [4]float32(a), [2]float64(b), rounding))
}

func maskzCvtRoundsdSs(k uint8, a [4]float32, b [2]float64, rounding int) [4]float32


// CvtRoundsdU32: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to an unsigned 32-bit integer, and store the result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_FP64_To_UnsignedInt32(a[63:0])
//
// Instruction: 'VCVTSD2USI'. Intrinsic: '_mm_cvt_roundsd_u32'.
// Requires AVX512F.
func CvtRoundsdU32(a x86.M128d, rounding int) uint32 {
	return uint32(cvtRoundsdU32([2]float64(a), rounding))
}

func cvtRoundsdU32(a [2]float64, rounding int) uint32


// CvtRoundsdU64: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to an unsigned 64-bit integer, and store the result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_FP64_To_UnsignedInt64(a[63:0])
//
// Instruction: 'VCVTSD2USI'. Intrinsic: '_mm_cvt_roundsd_u64'.
// Requires AVX512F.
func CvtRoundsdU64(a x86.M128d, rounding int) uint64 {
	return uint64(cvtRoundsdU64([2]float64(a), rounding))
}

func cvtRoundsdU64(a [2]float64, rounding int) uint64


// CvtRoundsi32Ss: Convert the 32-bit integer 'b' to a single-precision
// (32-bit) floating-point element, store the result in the lower element of
// 'dst', and copy the upper 3 packed elements from 'a' to the upper elements
// of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_Int32_To_FP32(b[31:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTSI2SS'. Intrinsic: '_mm_cvt_roundsi32_ss'.
// Requires AVX512F.
func CvtRoundsi32Ss(a x86.M128, b int, rounding int) x86.M128 {
	return x86.M128(cvtRoundsi32Ss([4]float32(a), b, rounding))
}

func cvtRoundsi32Ss(a [4]float32, b int, rounding int) [4]float32


// CvtRoundsi64Sd: Convert the 64-bit integer 'b' to a double-precision
// (64-bit) floating-point element, store the result in the lower element of
// 'dst', and copy the upper element from 'a' to the upper element of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_Int64_To_FP64(b[63:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTSI2SD'. Intrinsic: '_mm_cvt_roundsi64_sd'.
// Requires AVX512F.
func CvtRoundsi64Sd(a x86.M128d, b int64, rounding int) x86.M128d {
	return x86.M128d(cvtRoundsi64Sd([2]float64(a), b, rounding))
}

func cvtRoundsi64Sd(a [2]float64, b int64, rounding int) [2]float64


// CvtRoundsi64Ss: Convert the 64-bit integer 'b' to a single-precision
// (32-bit) floating-point element, store the result in the lower element of
// 'dst', and copy the upper 3 packed elements from 'a' to the upper elements
// of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_Int64_To_FP32(b[63:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTSI2SS'. Intrinsic: '_mm_cvt_roundsi64_ss'.
// Requires AVX512F.
func CvtRoundsi64Ss(a x86.M128, b int64, rounding int) x86.M128 {
	return x86.M128(cvtRoundsi64Ss([4]float32(a), b, rounding))
}

func cvtRoundsi64Ss(a [4]float32, b int64, rounding int) [4]float32


// CvtRoundssI32: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to a 32-bit integer, and store the result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_FP32_To_Int32(a[31:0])
//
// Instruction: 'VCVTSS2SI'. Intrinsic: '_mm_cvt_roundss_i32'.
// Requires AVX512F.
func CvtRoundssI32(a x86.M128, rounding int) int {
	return int(cvtRoundssI32([4]float32(a), rounding))
}

func cvtRoundssI32(a [4]float32, rounding int) int


// CvtRoundssI64: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to a 64-bit integer, and store the result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_FP32_To_Int64(a[31:0])
//
// Instruction: 'VCVTSS2SI'. Intrinsic: '_mm_cvt_roundss_i64'.
// Requires AVX512F.
func CvtRoundssI64(a x86.M128, rounding int) int64 {
	return int64(cvtRoundssI64([4]float32(a), rounding))
}

func cvtRoundssI64(a [4]float32, rounding int) int64


// CvtRoundssSd: Convert the lower single-precision (32-bit) floating-point
// element in 'b' to a double-precision (64-bit) floating-point element, store
// the result in the lower element of 'dst', and copy the upper element from
// 'a' to the upper element of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_FP32_To_FP64(b[31:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTSS2SD'. Intrinsic: '_mm_cvt_roundss_sd'.
// Requires AVX512F.
func CvtRoundssSd(a x86.M128d, b x86.M128, rounding int) x86.M128d {
	return x86.M128d(cvtRoundssSd([2]float64(a), [4]float32(b), rounding))
}

func cvtRoundssSd(a [2]float64, b [4]float32, rounding int) [2]float64


// MaskCvtRoundssSd: Convert the lower single-precision (32-bit) floating-point
// element in 'b' to a double-precision (64-bit) floating-point element, store
// the result in the lower element of 'dst' using writemask 'k' (the element is
// copied from 'src' when mask bit 0 is not set), and copy the upper element
// from 'a' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := Convert_FP32_To_FP64(b[31:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTSS2SD'. Intrinsic: '_mm_mask_cvt_roundss_sd'.
// Requires AVX512F.
func MaskCvtRoundssSd(src x86.M128d, k x86.Mmask8, a x86.M128d, b x86.M128, rounding int) x86.M128d {
	return x86.M128d(maskCvtRoundssSd([2]float64(src), uint8(k), [2]float64(a), [4]float32(b), rounding))
}

func maskCvtRoundssSd(src [2]float64, k uint8, a [2]float64, b [4]float32, rounding int) [2]float64


// MaskzCvtRoundssSd: Convert the lower single-precision (32-bit)
// floating-point element in 'b' to a double-precision (64-bit) floating-point
// element, store the result in the lower element of 'dst' using zeromask 'k'
// (the element is zeroed out when mask bit 0 is not set), and copy the upper
// element from 'a' to the upper element of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := Convert_FP32_To_FP64(b[31:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTSS2SD'. Intrinsic: '_mm_maskz_cvt_roundss_sd'.
// Requires AVX512F.
func MaskzCvtRoundssSd(k x86.Mmask8, a x86.M128d, b x86.M128, rounding int) x86.M128d {
	return x86.M128d(maskzCvtRoundssSd(uint8(k), [2]float64(a), [4]float32(b), rounding))
}

func maskzCvtRoundssSd(k uint8, a [2]float64, b [4]float32, rounding int) [2]float64


// CvtRoundssSi32: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to a 32-bit integer, and store the result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_FP32_To_Int32(a[31:0])
//
// Instruction: 'VCVTSS2SI'. Intrinsic: '_mm_cvt_roundss_si32'.
// Requires AVX512F.
func CvtRoundssSi32(a x86.M128, rounding int) int {
	return int(cvtRoundssSi32([4]float32(a), rounding))
}

func cvtRoundssSi32(a [4]float32, rounding int) int


// CvtRoundssSi64: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to a 64-bit integer, and store the result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_FP32_To_Int64(a[31:0])
//
// Instruction: 'VCVTSS2SI'. Intrinsic: '_mm_cvt_roundss_si64'.
// Requires AVX512F.
func CvtRoundssSi64(a x86.M128, rounding int) int64 {
	return int64(cvtRoundssSi64([4]float32(a), rounding))
}

func cvtRoundssSi64(a [4]float32, rounding int) int64


// CvtRoundssU32: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to an unsigned 32-bit integer, and store the result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_FP32_To_UnsignedInt32(a[31:0])
//
// Instruction: 'VCVTSS2USI'. Intrinsic: '_mm_cvt_roundss_u32'.
// Requires AVX512F.
func CvtRoundssU32(a x86.M128, rounding int) uint32 {
	return uint32(cvtRoundssU32([4]float32(a), rounding))
}

func cvtRoundssU32(a [4]float32, rounding int) uint32


// CvtRoundssU64: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to an unsigned 64-bit integer, and store the result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_FP32_To_UnsignedInt64(a[31:0])
//
// Instruction: 'VCVTSS2USI'. Intrinsic: '_mm_cvt_roundss_u64'.
// Requires AVX512F.
func CvtRoundssU64(a x86.M128, rounding int) uint64 {
	return uint64(cvtRoundssU64([4]float32(a), rounding))
}

func cvtRoundssU64(a [4]float32, rounding int) uint64


// CvtRoundu32Ss: Convert the unsigned 32-bit integer 'b' to a single-precision
// (32-bit) floating-point element, store the result in the lower element of
// 'dst', and copy the upper 3 packed elements from 'a' to the upper elements
// of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_UnsignedInt32_To_FP32(b[31:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTUSI2SS'. Intrinsic: '_mm_cvt_roundu32_ss'.
// Requires AVX512F.
func CvtRoundu32Ss(a x86.M128, b uint32, rounding int) x86.M128 {
	return x86.M128(cvtRoundu32Ss([4]float32(a), b, rounding))
}

func cvtRoundu32Ss(a [4]float32, b uint32, rounding int) [4]float32


// CvtRoundu64Sd: Convert the unsigned 64-bit integer 'b' to a double-precision
// (64-bit) floating-point element, store the result in the lower element of
// 'dst', and copy the upper element from 'a' to the upper element of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_UnsignedInt64_To_FP64(b[63:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTUSI2SD'. Intrinsic: '_mm_cvt_roundu64_sd'.
// Requires AVX512F.
func CvtRoundu64Sd(a x86.M128d, b uint64, rounding int) x86.M128d {
	return x86.M128d(cvtRoundu64Sd([2]float64(a), b, rounding))
}

func cvtRoundu64Sd(a [2]float64, b uint64, rounding int) [2]float64


// CvtRoundu64Ss: Convert the unsigned 64-bit integer 'b' to a single-precision
// (32-bit) floating-point element, store the result in the lower element of
// 'dst', and copy the upper 3 packed elements from 'a' to the upper elements
// of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_UnsignedInt64_To_FP32(b[63:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTUSI2SS'. Intrinsic: '_mm_cvt_roundu64_ss'.
// Requires AVX512F.
func CvtRoundu64Ss(a x86.M128, b uint64, rounding int) x86.M128 {
	return x86.M128(cvtRoundu64Ss([4]float32(a), b, rounding))
}

func cvtRoundu64Ss(a [4]float32, b uint64, rounding int) [4]float32


// MaskCvtepi16Epi32: Sign extend packed 16-bit integers in 'a' to packed
// 32-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			l := j*16
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[l+15:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSXWD'. Intrinsic: '_mm_mask_cvtepi16_epi32'.
// Requires AVX512F.
func MaskCvtepi16Epi32(src x86.M128i, k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskCvtepi16Epi32([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi16Epi32(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepi16Epi32: Sign extend packed 16-bit integers in 'a' to packed
// 32-bit integers, and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[l+15:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSXWD'. Intrinsic: '_mm_maskz_cvtepi16_epi32'.
// Requires AVX512F.
func MaskzCvtepi16Epi32(k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskzCvtepi16Epi32(uint8(k), [16]byte(a)))
}

func maskzCvtepi16Epi32(k uint8, a [16]byte) [16]byte


// M256MaskCvtepi16Epi32: Sign extend packed 16-bit integers in 'a' to packed
// 32-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			l := j*16
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[l+15:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXWD'. Intrinsic: '_mm256_mask_cvtepi16_epi32'.
// Requires AVX512F.
func M256MaskCvtepi16Epi32(src x86.M256i, k x86.Mmask8, a x86.M128i) x86.M256i {
	return x86.M256i(m256MaskCvtepi16Epi32([32]byte(src), uint8(k), [16]byte(a)))
}

func m256MaskCvtepi16Epi32(src [32]byte, k uint8, a [16]byte) [32]byte


// M256MaskzCvtepi16Epi32: Sign extend packed 16-bit integers in 'a' to packed
// 32-bit integers, and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[l+15:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXWD'. Intrinsic: '_mm256_maskz_cvtepi16_epi32'.
// Requires AVX512F.
func M256MaskzCvtepi16Epi32(k x86.Mmask8, a x86.M128i) x86.M256i {
	return x86.M256i(m256MaskzCvtepi16Epi32(uint8(k), [16]byte(a)))
}

func m256MaskzCvtepi16Epi32(k uint8, a [16]byte) [32]byte


// M512Cvtepi16Epi32: Sign extend packed 16-bit integers in 'a' to packed
// 32-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			k := 16*j
//			dst[i+31:i] := SignExtend(a[k+15:k])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXWD'. Intrinsic: '_mm512_cvtepi16_epi32'.
// Requires AVX512F.
func M512Cvtepi16Epi32(a x86.M256i) x86.M512i {
	return x86.M512i(m512Cvtepi16Epi32([32]byte(a)))
}

func m512Cvtepi16Epi32(a [32]byte) [64]byte


// M512MaskCvtepi16Epi32: Sign extend packed 16-bit integers in 'a' to packed
// 32-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			l := j*16
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[l+15:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXWD'. Intrinsic: '_mm512_mask_cvtepi16_epi32'.
// Requires AVX512F.
func M512MaskCvtepi16Epi32(src x86.M512i, k x86.Mmask16, a x86.M256i) x86.M512i {
	return x86.M512i(m512MaskCvtepi16Epi32([64]byte(src), uint16(k), [32]byte(a)))
}

func m512MaskCvtepi16Epi32(src [64]byte, k uint16, a [32]byte) [64]byte


// M512MaskzCvtepi16Epi32: Sign extend packed 16-bit integers in 'a' to packed
// 32-bit integers, and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[l+15:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXWD'. Intrinsic: '_mm512_maskz_cvtepi16_epi32'.
// Requires AVX512F.
func M512MaskzCvtepi16Epi32(k x86.Mmask16, a x86.M256i) x86.M512i {
	return x86.M512i(m512MaskzCvtepi16Epi32(uint16(k), [32]byte(a)))
}

func m512MaskzCvtepi16Epi32(k uint16, a [32]byte) [64]byte


// MaskCvtepi16Epi64: Sign extend packed 16-bit integers in the low 4 bytes of
// 'a' to packed 64-bit integers, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+15:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSXWQ'. Intrinsic: '_mm_mask_cvtepi16_epi64'.
// Requires AVX512F.
func MaskCvtepi16Epi64(src x86.M128i, k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskCvtepi16Epi64([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi16Epi64(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepi16Epi64: Sign extend packed 16-bit integers in the low 4 bytes of
// 'a' to packed 64-bit integers, and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+15:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSXWQ'. Intrinsic: '_mm_maskz_cvtepi16_epi64'.
// Requires AVX512F.
func MaskzCvtepi16Epi64(k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskzCvtepi16Epi64(uint8(k), [16]byte(a)))
}

func maskzCvtepi16Epi64(k uint8, a [16]byte) [16]byte


// M256MaskCvtepi16Epi64: Sign extend packed 16-bit integers in the low 8 bytes
// of 'a' to packed 64-bit integers, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+15:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXWQ'. Intrinsic: '_mm256_mask_cvtepi16_epi64'.
// Requires AVX512F.
func M256MaskCvtepi16Epi64(src x86.M256i, k x86.Mmask8, a x86.M128i) x86.M256i {
	return x86.M256i(m256MaskCvtepi16Epi64([32]byte(src), uint8(k), [16]byte(a)))
}

func m256MaskCvtepi16Epi64(src [32]byte, k uint8, a [16]byte) [32]byte


// M256MaskzCvtepi16Epi64: Sign extend packed 16-bit integers in the low 8
// bytes of 'a' to packed 64-bit integers, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+15:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXWQ'. Intrinsic: '_mm256_maskz_cvtepi16_epi64'.
// Requires AVX512F.
func M256MaskzCvtepi16Epi64(k x86.Mmask8, a x86.M128i) x86.M256i {
	return x86.M256i(m256MaskzCvtepi16Epi64(uint8(k), [16]byte(a)))
}

func m256MaskzCvtepi16Epi64(k uint8, a [16]byte) [32]byte


// M512Cvtepi16Epi64: Sign extend packed 16-bit integers in 'a' to packed
// 64-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 16*j
//			dst[i+63:i] := SignExtend(a[k+15:k])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXWQ'. Intrinsic: '_mm512_cvtepi16_epi64'.
// Requires AVX512F.
func M512Cvtepi16Epi64(a x86.M128i) x86.M512i {
	return x86.M512i(m512Cvtepi16Epi64([16]byte(a)))
}

func m512Cvtepi16Epi64(a [16]byte) [64]byte


// M512MaskCvtepi16Epi64: Sign extend packed 16-bit integers in 'a' to packed
// 64-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+15:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXWQ'. Intrinsic: '_mm512_mask_cvtepi16_epi64'.
// Requires AVX512F.
func M512MaskCvtepi16Epi64(src x86.M512i, k x86.Mmask8, a x86.M128i) x86.M512i {
	return x86.M512i(m512MaskCvtepi16Epi64([64]byte(src), uint8(k), [16]byte(a)))
}

func m512MaskCvtepi16Epi64(src [64]byte, k uint8, a [16]byte) [64]byte


// M512MaskzCvtepi16Epi64: Sign extend packed 16-bit integers in 'a' to packed
// 64-bit integers, and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+15:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXWQ'. Intrinsic: '_mm512_maskz_cvtepi16_epi64'.
// Requires AVX512F.
func M512MaskzCvtepi16Epi64(k x86.Mmask8, a x86.M128i) x86.M512i {
	return x86.M512i(m512MaskzCvtepi16Epi64(uint8(k), [16]byte(a)))
}

func m512MaskzCvtepi16Epi64(k uint8, a [16]byte) [64]byte


// Cvtepi32Epi16: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			k := 16*j
//			dst[k+15:k] := Truncate_Int32_To_Int16(a[i+31:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVDW'. Intrinsic: '_mm_cvtepi32_epi16'.
// Requires AVX512F.
func Cvtepi32Epi16(a x86.M128i) x86.M128i {
	return x86.M128i(cvtepi32Epi16([16]byte(a)))
}

func cvtepi32Epi16(a [16]byte) [16]byte


// MaskCvtepi32Epi16: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Truncate_Int32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVDW'. Intrinsic: '_mm_mask_cvtepi32_epi16'.
// Requires AVX512F.
func MaskCvtepi32Epi16(src x86.M128i, k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskCvtepi32Epi16([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi32Epi16(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepi32Epi16: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Truncate_Int32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVDW'. Intrinsic: '_mm_maskz_cvtepi32_epi16'.
// Requires AVX512F.
func MaskzCvtepi32Epi16(k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskzCvtepi32Epi16(uint8(k), [16]byte(a)))
}

func maskzCvtepi32Epi16(k uint8, a [16]byte) [16]byte


// M256Cvtepi32Epi16: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 16*j
//			dst[k+15:k] := Truncate_Int32_To_Int16(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVDW'. Intrinsic: '_mm256_cvtepi32_epi16'.
// Requires AVX512F.
func M256Cvtepi32Epi16(a x86.M256i) x86.M128i {
	return x86.M128i(m256Cvtepi32Epi16([32]byte(a)))
}

func m256Cvtepi32Epi16(a [32]byte) [16]byte


// M256MaskCvtepi32Epi16: Convert packed 32-bit integers in 'a' to packed
// 16-bit integers with truncation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Truncate_Int32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVDW'. Intrinsic: '_mm256_mask_cvtepi32_epi16'.
// Requires AVX512F.
func M256MaskCvtepi32Epi16(src x86.M128i, k x86.Mmask8, a x86.M256i) x86.M128i {
	return x86.M128i(m256MaskCvtepi32Epi16([16]byte(src), uint8(k), [32]byte(a)))
}

func m256MaskCvtepi32Epi16(src [16]byte, k uint8, a [32]byte) [16]byte


// M256MaskzCvtepi32Epi16: Convert packed 32-bit integers in 'a' to packed
// 16-bit integers with truncation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Truncate_Int32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVDW'. Intrinsic: '_mm256_maskz_cvtepi32_epi16'.
// Requires AVX512F.
func M256MaskzCvtepi32Epi16(k x86.Mmask8, a x86.M256i) x86.M128i {
	return x86.M128i(m256MaskzCvtepi32Epi16(uint8(k), [32]byte(a)))
}

func m256MaskzCvtepi32Epi16(k uint8, a [32]byte) [16]byte


// M512Cvtepi32Epi16: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			k := 16*j
//			dst[k+15:k] := Truncate_Int32_To_Int16(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVDW'. Intrinsic: '_mm512_cvtepi32_epi16'.
// Requires AVX512F.
func M512Cvtepi32Epi16(a x86.M512i) x86.M256i {
	return x86.M256i(m512Cvtepi32Epi16([64]byte(a)))
}

func m512Cvtepi32Epi16(a [64]byte) [32]byte


// M512MaskCvtepi32Epi16: Convert packed 32-bit integers in 'a' to packed
// 16-bit integers with truncation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Truncate_Int32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVDW'. Intrinsic: '_mm512_mask_cvtepi32_epi16'.
// Requires AVX512F.
func M512MaskCvtepi32Epi16(src x86.M256i, k x86.Mmask16, a x86.M512i) x86.M256i {
	return x86.M256i(m512MaskCvtepi32Epi16([32]byte(src), uint16(k), [64]byte(a)))
}

func m512MaskCvtepi32Epi16(src [32]byte, k uint16, a [64]byte) [32]byte


// M512MaskzCvtepi32Epi16: Convert packed 32-bit integers in 'a' to packed
// 16-bit integers with truncation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Truncate_Int32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVDW'. Intrinsic: '_mm512_maskz_cvtepi32_epi16'.
// Requires AVX512F.
func M512MaskzCvtepi32Epi16(k x86.Mmask16, a x86.M512i) x86.M256i {
	return x86.M256i(m512MaskzCvtepi32Epi16(uint16(k), [64]byte(a)))
}

func m512MaskzCvtepi32Epi16(k uint16, a [64]byte) [32]byte


// MaskCvtepi32Epi64: Sign extend packed 32-bit integers in 'a' to packed
// 64-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSXDQ'. Intrinsic: '_mm_mask_cvtepi32_epi64'.
// Requires AVX512F.
func MaskCvtepi32Epi64(src x86.M128i, k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskCvtepi32Epi64([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi32Epi64(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepi32Epi64: Sign extend packed 32-bit integers in 'a' to packed
// 64-bit integers, and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+31:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSXDQ'. Intrinsic: '_mm_maskz_cvtepi32_epi64'.
// Requires AVX512F.
func MaskzCvtepi32Epi64(k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskzCvtepi32Epi64(uint8(k), [16]byte(a)))
}

func maskzCvtepi32Epi64(k uint8, a [16]byte) [16]byte


// M256MaskCvtepi32Epi64: Sign extend packed 32-bit integers in 'a' to packed
// 64-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXDQ'. Intrinsic: '_mm256_mask_cvtepi32_epi64'.
// Requires AVX512F.
func M256MaskCvtepi32Epi64(src x86.M256i, k x86.Mmask8, a x86.M128i) x86.M256i {
	return x86.M256i(m256MaskCvtepi32Epi64([32]byte(src), uint8(k), [16]byte(a)))
}

func m256MaskCvtepi32Epi64(src [32]byte, k uint8, a [16]byte) [32]byte


// M256MaskzCvtepi32Epi64: Sign extend packed 32-bit integers in 'a' to packed
// 64-bit integers, and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+31:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXDQ'. Intrinsic: '_mm256_maskz_cvtepi32_epi64'.
// Requires AVX512F.
func M256MaskzCvtepi32Epi64(k x86.Mmask8, a x86.M128i) x86.M256i {
	return x86.M256i(m256MaskzCvtepi32Epi64(uint8(k), [16]byte(a)))
}

func m256MaskzCvtepi32Epi64(k uint8, a [16]byte) [32]byte


// M512Cvtepi32Epi64: Sign extend packed 32-bit integers in 'a' to packed
// 64-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 32*j
//			dst[i+63:i] := SignExtend(a[k+31:k])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXDQ'. Intrinsic: '_mm512_cvtepi32_epi64'.
// Requires AVX512F.
func M512Cvtepi32Epi64(a x86.M256i) x86.M512i {
	return x86.M512i(m512Cvtepi32Epi64([32]byte(a)))
}

func m512Cvtepi32Epi64(a [32]byte) [64]byte


// M512MaskCvtepi32Epi64: Sign extend packed 32-bit integers in 'a' to packed
// 64-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXDQ'. Intrinsic: '_mm512_mask_cvtepi32_epi64'.
// Requires AVX512F.
func M512MaskCvtepi32Epi64(src x86.M512i, k x86.Mmask8, a x86.M256i) x86.M512i {
	return x86.M512i(m512MaskCvtepi32Epi64([64]byte(src), uint8(k), [32]byte(a)))
}

func m512MaskCvtepi32Epi64(src [64]byte, k uint8, a [32]byte) [64]byte


// M512MaskzCvtepi32Epi64: Sign extend packed 32-bit integers in 'a' to packed
// 64-bit integers, and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+31:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXDQ'. Intrinsic: '_mm512_maskz_cvtepi32_epi64'.
// Requires AVX512F.
func M512MaskzCvtepi32Epi64(k x86.Mmask8, a x86.M256i) x86.M512i {
	return x86.M512i(m512MaskzCvtepi32Epi64(uint8(k), [32]byte(a)))
}

func m512MaskzCvtepi32Epi64(k uint8, a [32]byte) [64]byte


// Cvtepi32Epi8: Convert packed 32-bit integers in 'a' to packed 8-bit integers
// with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			k := 8*j
//			dst[k+7:k] := Truncate_Int32_To_Int8(a[i+31:i])
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVDB'. Intrinsic: '_mm_cvtepi32_epi8'.
// Requires AVX512F.
func Cvtepi32Epi8(a x86.M128i) x86.M128i {
	return x86.M128i(cvtepi32Epi8([16]byte(a)))
}

func cvtepi32Epi8(a [16]byte) [16]byte


// MaskCvtepi32Epi8: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVDB'. Intrinsic: '_mm_mask_cvtepi32_epi8'.
// Requires AVX512F.
func MaskCvtepi32Epi8(src x86.M128i, k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskCvtepi32Epi8([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi32Epi8(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepi32Epi8: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVDB'. Intrinsic: '_mm_maskz_cvtepi32_epi8'.
// Requires AVX512F.
func MaskzCvtepi32Epi8(k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskzCvtepi32Epi8(uint8(k), [16]byte(a)))
}

func maskzCvtepi32Epi8(k uint8, a [16]byte) [16]byte


// M256Cvtepi32Epi8: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 8*j
//			dst[k+7:k] := Truncate_Int32_To_Int8(a[i+31:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVDB'. Intrinsic: '_mm256_cvtepi32_epi8'.
// Requires AVX512F.
func M256Cvtepi32Epi8(a x86.M256i) x86.M128i {
	return x86.M128i(m256Cvtepi32Epi8([32]byte(a)))
}

func m256Cvtepi32Epi8(a [32]byte) [16]byte


// M256MaskCvtepi32Epi8: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVDB'. Intrinsic: '_mm256_mask_cvtepi32_epi8'.
// Requires AVX512F.
func M256MaskCvtepi32Epi8(src x86.M128i, k x86.Mmask8, a x86.M256i) x86.M128i {
	return x86.M128i(m256MaskCvtepi32Epi8([16]byte(src), uint8(k), [32]byte(a)))
}

func m256MaskCvtepi32Epi8(src [16]byte, k uint8, a [32]byte) [16]byte


// M256MaskzCvtepi32Epi8: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVDB'. Intrinsic: '_mm256_maskz_cvtepi32_epi8'.
// Requires AVX512F.
func M256MaskzCvtepi32Epi8(k x86.Mmask8, a x86.M256i) x86.M128i {
	return x86.M128i(m256MaskzCvtepi32Epi8(uint8(k), [32]byte(a)))
}

func m256MaskzCvtepi32Epi8(k uint8, a [32]byte) [16]byte


// M512Cvtepi32Epi8: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			k := 8*j
//			dst[k+7:k] := Truncate_Int32_To_Int8(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVDB'. Intrinsic: '_mm512_cvtepi32_epi8'.
// Requires AVX512F.
func M512Cvtepi32Epi8(a x86.M512i) x86.M128i {
	return x86.M128i(m512Cvtepi32Epi8([64]byte(a)))
}

func m512Cvtepi32Epi8(a [64]byte) [16]byte


// M512MaskCvtepi32Epi8: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVDB'. Intrinsic: '_mm512_mask_cvtepi32_epi8'.
// Requires AVX512F.
func M512MaskCvtepi32Epi8(src x86.M128i, k x86.Mmask16, a x86.M512i) x86.M128i {
	return x86.M128i(m512MaskCvtepi32Epi8([16]byte(src), uint16(k), [64]byte(a)))
}

func m512MaskCvtepi32Epi8(src [16]byte, k uint16, a [64]byte) [16]byte


// M512MaskzCvtepi32Epi8: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVDB'. Intrinsic: '_mm512_maskz_cvtepi32_epi8'.
// Requires AVX512F.
func M512MaskzCvtepi32Epi8(k x86.Mmask16, a x86.M512i) x86.M128i {
	return x86.M128i(m512MaskzCvtepi32Epi8(uint16(k), [64]byte(a)))
}

func m512MaskzCvtepi32Epi8(k uint16, a [64]byte) [16]byte


// MaskCvtepi32Pd: Convert packed 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*32
//			m := j*64
//			IF k[j]
//				dst[m+63:m] := Convert_Int32_To_FP64(a[i+31:i])
//			ELSE
//				dst[m+63:m] := src[m+63:m]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTDQ2PD'. Intrinsic: '_mm_mask_cvtepi32_pd'.
// Requires AVX512F.
func MaskCvtepi32Pd(src x86.M128d, k x86.Mmask8, a x86.M128i) x86.M128d {
	return x86.M128d(maskCvtepi32Pd([2]float64(src), uint8(k), [16]byte(a)))
}

func maskCvtepi32Pd(src [2]float64, k uint8, a [16]byte) [2]float64


// MaskzCvtepi32Pd: Convert packed 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*32
//			m := j*64
//			IF k[j]
//				dst[m+63:m] := Convert_Int32_To_FP64(a[i+31:i])
//			ELSE
//				dst[m+63:m] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTDQ2PD'. Intrinsic: '_mm_maskz_cvtepi32_pd'.
// Requires AVX512F.
func MaskzCvtepi32Pd(k x86.Mmask8, a x86.M128i) x86.M128d {
	return x86.M128d(maskzCvtepi32Pd(uint8(k), [16]byte(a)))
}

func maskzCvtepi32Pd(k uint8, a [16]byte) [2]float64


// M256MaskCvtepi32Pd: Convert packed 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			m := j*64
//			IF k[j]
//				dst[m+63:m] := Convert_Int32_To_FP64(a[i+31:i])
//			ELSE
//				dst[m+63:m] := src[m+63:m]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTDQ2PD'. Intrinsic: '_mm256_mask_cvtepi32_pd'.
// Requires AVX512F.
func M256MaskCvtepi32Pd(src x86.M256d, k x86.Mmask8, a x86.M128i) x86.M256d {
	return x86.M256d(m256MaskCvtepi32Pd([4]float64(src), uint8(k), [16]byte(a)))
}

func m256MaskCvtepi32Pd(src [4]float64, k uint8, a [16]byte) [4]float64


// M256MaskzCvtepi32Pd: Convert packed 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			m := j*64
//			IF k[j]
//				dst[m+63:m] := Convert_Int32_To_FP64(a[i+31:i])
//			ELSE
//				dst[m+63:m] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTDQ2PD'. Intrinsic: '_mm256_maskz_cvtepi32_pd'.
// Requires AVX512F.
func M256MaskzCvtepi32Pd(k x86.Mmask8, a x86.M128i) x86.M256d {
	return x86.M256d(m256MaskzCvtepi32Pd(uint8(k), [16]byte(a)))
}

func m256MaskzCvtepi32Pd(k uint8, a [16]byte) [4]float64


// M512Cvtepi32Pd: Convert packed 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			m := j*64
//			dst[m+63:m] := Convert_Int32_To_FP64(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTDQ2PD'. Intrinsic: '_mm512_cvtepi32_pd'.
// Requires AVX512F.
func M512Cvtepi32Pd(a x86.M256i) x86.M512d {
	return x86.M512d(m512Cvtepi32Pd([32]byte(a)))
}

func m512Cvtepi32Pd(a [32]byte) [8]float64


// M512MaskCvtepi32Pd: Convert packed 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			m := j*64
//			IF k[j]
//				dst[m+63:m] := Convert_Int32_To_FP64(a[i+31:i])
//			ELSE
//				dst[m+63:m] := src[m+63:m]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTDQ2PD'. Intrinsic: '_mm512_mask_cvtepi32_pd'.
// Requires AVX512F.
func M512MaskCvtepi32Pd(src x86.M512d, k x86.Mmask8, a x86.M256i) x86.M512d {
	return x86.M512d(m512MaskCvtepi32Pd([8]float64(src), uint8(k), [32]byte(a)))
}

func m512MaskCvtepi32Pd(src [8]float64, k uint8, a [32]byte) [8]float64


// M512MaskzCvtepi32Pd: Convert packed 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			m := j*64
//			IF k[j]
//				dst[m+63:m] := Convert_Int32_To_FP64(a[i+31:i])
//			ELSE
//				dst[m+63:m] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTDQ2PD'. Intrinsic: '_mm512_maskz_cvtepi32_pd'.
// Requires AVX512F.
func M512MaskzCvtepi32Pd(k x86.Mmask8, a x86.M256i) x86.M512d {
	return x86.M512d(m512MaskzCvtepi32Pd(uint8(k), [32]byte(a)))
}

func m512MaskzCvtepi32Pd(k uint8, a [32]byte) [8]float64


// MaskCvtepi32Ps: Convert packed 32-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTDQ2PS'. Intrinsic: '_mm_mask_cvtepi32_ps'.
// Requires AVX512F.
func MaskCvtepi32Ps(src x86.M128, k x86.Mmask8, a x86.M128i) x86.M128 {
	return x86.M128(maskCvtepi32Ps([4]float32(src), uint8(k), [16]byte(a)))
}

func maskCvtepi32Ps(src [4]float32, k uint8, a [16]byte) [4]float32


// MaskzCvtepi32Ps: Convert packed 32-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTDQ2PS'. Intrinsic: '_mm_maskz_cvtepi32_ps'.
// Requires AVX512F.
func MaskzCvtepi32Ps(k x86.Mmask8, a x86.M128i) x86.M128 {
	return x86.M128(maskzCvtepi32Ps(uint8(k), [16]byte(a)))
}

func maskzCvtepi32Ps(k uint8, a [16]byte) [4]float32


// M256MaskCvtepi32Ps: Convert packed 32-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTDQ2PS'. Intrinsic: '_mm256_mask_cvtepi32_ps'.
// Requires AVX512F.
func M256MaskCvtepi32Ps(src x86.M256, k x86.Mmask8, a x86.M256i) x86.M256 {
	return x86.M256(m256MaskCvtepi32Ps([8]float32(src), uint8(k), [32]byte(a)))
}

func m256MaskCvtepi32Ps(src [8]float32, k uint8, a [32]byte) [8]float32


// M256MaskzCvtepi32Ps: Convert packed 32-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTDQ2PS'. Intrinsic: '_mm256_maskz_cvtepi32_ps'.
// Requires AVX512F.
func M256MaskzCvtepi32Ps(k x86.Mmask8, a x86.M256i) x86.M256 {
	return x86.M256(m256MaskzCvtepi32Ps(uint8(k), [32]byte(a)))
}

func m256MaskzCvtepi32Ps(k uint8, a [32]byte) [8]float32


// M512Cvtepi32Ps: Convert packed 32-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTDQ2PS'. Intrinsic: '_mm512_cvtepi32_ps'.
// Requires AVX512F.
func M512Cvtepi32Ps(a x86.M512i) x86.M512 {
	return x86.M512(m512Cvtepi32Ps([64]byte(a)))
}

func m512Cvtepi32Ps(a [64]byte) [16]float32


// M512MaskCvtepi32Ps: Convert packed 32-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTDQ2PS'. Intrinsic: '_mm512_mask_cvtepi32_ps'.
// Requires AVX512F.
func M512MaskCvtepi32Ps(src x86.M512, k x86.Mmask16, a x86.M512i) x86.M512 {
	return x86.M512(m512MaskCvtepi32Ps([16]float32(src), uint16(k), [64]byte(a)))
}

func m512MaskCvtepi32Ps(src [16]float32, k uint16, a [64]byte) [16]float32


// M512MaskzCvtepi32Ps: Convert packed 32-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTDQ2PS'. Intrinsic: '_mm512_maskz_cvtepi32_ps'.
// Requires AVX512F.
func M512MaskzCvtepi32Ps(k x86.Mmask16, a x86.M512i) x86.M512 {
	return x86.M512(m512MaskzCvtepi32Ps(uint16(k), [64]byte(a)))
}

func m512MaskzCvtepi32Ps(k uint16, a [64]byte) [16]float32


// Skipped: _mm_mask_cvtepi32_storeu_epi16. Contains pointer parameter.


// Skipped: _mm256_mask_cvtepi32_storeu_epi16. Contains pointer parameter.


// Skipped: _mm512_mask_cvtepi32_storeu_epi16. Contains pointer parameter.


// Skipped: _mm_mask_cvtepi32_storeu_epi8. Contains pointer parameter.


// Skipped: _mm256_mask_cvtepi32_storeu_epi8. Contains pointer parameter.


// Skipped: _mm512_mask_cvtepi32_storeu_epi8. Contains pointer parameter.


// Cvtepi64Epi16: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			k := 16*j
//			dst[k+15:k] := Truncate_Int64_To_Int16(a[i+63:i])
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVQW'. Intrinsic: '_mm_cvtepi64_epi16'.
// Requires AVX512F.
func Cvtepi64Epi16(a x86.M128i) x86.M128i {
	return x86.M128i(cvtepi64Epi16([16]byte(a)))
}

func cvtepi64Epi16(a [16]byte) [16]byte


// MaskCvtepi64Epi16: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Truncate_Int64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVQW'. Intrinsic: '_mm_mask_cvtepi64_epi16'.
// Requires AVX512F.
func MaskCvtepi64Epi16(src x86.M128i, k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskCvtepi64Epi16([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi64Epi16(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepi64Epi16: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Truncate_Int64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVQW'. Intrinsic: '_mm_maskz_cvtepi64_epi16'.
// Requires AVX512F.
func MaskzCvtepi64Epi16(k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskzCvtepi64Epi16(uint8(k), [16]byte(a)))
}

func maskzCvtepi64Epi16(k uint8, a [16]byte) [16]byte


// M256Cvtepi64Epi16: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			k := 16*j
//			dst[k+15:k] := Truncate_Int64_To_Int16(a[i+63:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVQW'. Intrinsic: '_mm256_cvtepi64_epi16'.
// Requires AVX512F.
func M256Cvtepi64Epi16(a x86.M256i) x86.M128i {
	return x86.M128i(m256Cvtepi64Epi16([32]byte(a)))
}

func m256Cvtepi64Epi16(a [32]byte) [16]byte


// M256MaskCvtepi64Epi16: Convert packed 64-bit integers in 'a' to packed
// 16-bit integers with truncation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Truncate_Int64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVQW'. Intrinsic: '_mm256_mask_cvtepi64_epi16'.
// Requires AVX512F.
func M256MaskCvtepi64Epi16(src x86.M128i, k x86.Mmask8, a x86.M256i) x86.M128i {
	return x86.M128i(m256MaskCvtepi64Epi16([16]byte(src), uint8(k), [32]byte(a)))
}

func m256MaskCvtepi64Epi16(src [16]byte, k uint8, a [32]byte) [16]byte


// M256MaskzCvtepi64Epi16: Convert packed 64-bit integers in 'a' to packed
// 16-bit integers with truncation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Truncate_Int64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVQW'. Intrinsic: '_mm256_maskz_cvtepi64_epi16'.
// Requires AVX512F.
func M256MaskzCvtepi64Epi16(k x86.Mmask8, a x86.M256i) x86.M128i {
	return x86.M128i(m256MaskzCvtepi64Epi16(uint8(k), [32]byte(a)))
}

func m256MaskzCvtepi64Epi16(k uint8, a [32]byte) [16]byte


// M512Cvtepi64Epi16: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 16*j
//			dst[k+15:k] := Truncate_Int64_To_Int16(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQW'. Intrinsic: '_mm512_cvtepi64_epi16'.
// Requires AVX512F.
func M512Cvtepi64Epi16(a x86.M512i) x86.M128i {
	return x86.M128i(m512Cvtepi64Epi16([64]byte(a)))
}

func m512Cvtepi64Epi16(a [64]byte) [16]byte


// M512MaskCvtepi64Epi16: Convert packed 64-bit integers in 'a' to packed
// 16-bit integers with truncation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Truncate_Int64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQW'. Intrinsic: '_mm512_mask_cvtepi64_epi16'.
// Requires AVX512F.
func M512MaskCvtepi64Epi16(src x86.M128i, k x86.Mmask8, a x86.M512i) x86.M128i {
	return x86.M128i(m512MaskCvtepi64Epi16([16]byte(src), uint8(k), [64]byte(a)))
}

func m512MaskCvtepi64Epi16(src [16]byte, k uint8, a [64]byte) [16]byte


// M512MaskzCvtepi64Epi16: Convert packed 64-bit integers in 'a' to packed
// 16-bit integers with truncation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Truncate_Int64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQW'. Intrinsic: '_mm512_maskz_cvtepi64_epi16'.
// Requires AVX512F.
func M512MaskzCvtepi64Epi16(k x86.Mmask8, a x86.M512i) x86.M128i {
	return x86.M128i(m512MaskzCvtepi64Epi16(uint8(k), [64]byte(a)))
}

func m512MaskzCvtepi64Epi16(k uint8, a [64]byte) [16]byte


// Cvtepi64Epi32: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			k := 32*j
//			dst[k+31:k] := Truncate_Int64_To_Int32(a[i+63:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVQD'. Intrinsic: '_mm_cvtepi64_epi32'.
// Requires AVX512F.
func Cvtepi64Epi32(a x86.M128i) x86.M128i {
	return x86.M128i(cvtepi64Epi32([16]byte(a)))
}

func cvtepi64Epi32(a [16]byte) [16]byte


// MaskCvtepi64Epi32: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Truncate_Int64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := src[l+31:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVQD'. Intrinsic: '_mm_mask_cvtepi64_epi32'.
// Requires AVX512F.
func MaskCvtepi64Epi32(src x86.M128i, k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskCvtepi64Epi32([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi64Epi32(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepi64Epi32: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Truncate_Int64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVQD'. Intrinsic: '_mm_maskz_cvtepi64_epi32'.
// Requires AVX512F.
func MaskzCvtepi64Epi32(k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskzCvtepi64Epi32(uint8(k), [16]byte(a)))
}

func maskzCvtepi64Epi32(k uint8, a [16]byte) [16]byte


// M256Cvtepi64Epi32: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			k := 32*j
//			dst[k+31:k] := Truncate_Int64_To_Int32(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQD'. Intrinsic: '_mm256_cvtepi64_epi32'.
// Requires AVX512F.
func M256Cvtepi64Epi32(a x86.M256i) x86.M128i {
	return x86.M128i(m256Cvtepi64Epi32([32]byte(a)))
}

func m256Cvtepi64Epi32(a [32]byte) [16]byte


// M256MaskCvtepi64Epi32: Convert packed 64-bit integers in 'a' to packed
// 32-bit integers with truncation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Truncate_Int64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := src[l+31:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQD'. Intrinsic: '_mm256_mask_cvtepi64_epi32'.
// Requires AVX512F.
func M256MaskCvtepi64Epi32(src x86.M128i, k x86.Mmask8, a x86.M256i) x86.M128i {
	return x86.M128i(m256MaskCvtepi64Epi32([16]byte(src), uint8(k), [32]byte(a)))
}

func m256MaskCvtepi64Epi32(src [16]byte, k uint8, a [32]byte) [16]byte


// M256MaskzCvtepi64Epi32: Convert packed 64-bit integers in 'a' to packed
// 32-bit integers with truncation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Truncate_Int64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQD'. Intrinsic: '_mm256_maskz_cvtepi64_epi32'.
// Requires AVX512F.
func M256MaskzCvtepi64Epi32(k x86.Mmask8, a x86.M256i) x86.M128i {
	return x86.M128i(m256MaskzCvtepi64Epi32(uint8(k), [32]byte(a)))
}

func m256MaskzCvtepi64Epi32(k uint8, a [32]byte) [16]byte


// M512Cvtepi64Epi32: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 32*j
//			dst[k+31:k] := Truncate_Int64_To_Int32(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVQD'. Intrinsic: '_mm512_cvtepi64_epi32'.
// Requires AVX512F.
func M512Cvtepi64Epi32(a x86.M512i) x86.M256i {
	return x86.M256i(m512Cvtepi64Epi32([64]byte(a)))
}

func m512Cvtepi64Epi32(a [64]byte) [32]byte


// M512MaskCvtepi64Epi32: Convert packed 64-bit integers in 'a' to packed
// 32-bit integers with truncation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Truncate_Int64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := src[l+31:l]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVQD'. Intrinsic: '_mm512_mask_cvtepi64_epi32'.
// Requires AVX512F.
func M512MaskCvtepi64Epi32(src x86.M256i, k x86.Mmask8, a x86.M512i) x86.M256i {
	return x86.M256i(m512MaskCvtepi64Epi32([32]byte(src), uint8(k), [64]byte(a)))
}

func m512MaskCvtepi64Epi32(src [32]byte, k uint8, a [64]byte) [32]byte


// M512MaskzCvtepi64Epi32: Convert packed 64-bit integers in 'a' to packed
// 32-bit integers with truncation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Truncate_Int64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVQD'. Intrinsic: '_mm512_maskz_cvtepi64_epi32'.
// Requires AVX512F.
func M512MaskzCvtepi64Epi32(k x86.Mmask8, a x86.M512i) x86.M256i {
	return x86.M256i(m512MaskzCvtepi64Epi32(uint8(k), [64]byte(a)))
}

func m512MaskzCvtepi64Epi32(k uint8, a [64]byte) [32]byte


// Cvtepi64Epi8: Convert packed 64-bit integers in 'a' to packed 8-bit integers
// with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			k := 8*j
//			dst[k+7:k] := Truncate_Int64_To_Int8(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQB'. Intrinsic: '_mm_cvtepi64_epi8'.
// Requires AVX512F.
func Cvtepi64Epi8(a x86.M128i) x86.M128i {
	return x86.M128i(cvtepi64Epi8([16]byte(a)))
}

func cvtepi64Epi8(a [16]byte) [16]byte


// MaskCvtepi64Epi8: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQB'. Intrinsic: '_mm_mask_cvtepi64_epi8'.
// Requires AVX512F.
func MaskCvtepi64Epi8(src x86.M128i, k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskCvtepi64Epi8([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi64Epi8(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepi64Epi8: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQB'. Intrinsic: '_mm_maskz_cvtepi64_epi8'.
// Requires AVX512F.
func MaskzCvtepi64Epi8(k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskzCvtepi64Epi8(uint8(k), [16]byte(a)))
}

func maskzCvtepi64Epi8(k uint8, a [16]byte) [16]byte


// M256Cvtepi64Epi8: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			k := 8*j
//			dst[k+7:k] := Truncate_Int64_To_Int8(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQB'. Intrinsic: '_mm256_cvtepi64_epi8'.
// Requires AVX512F.
func M256Cvtepi64Epi8(a x86.M256i) x86.M128i {
	return x86.M128i(m256Cvtepi64Epi8([32]byte(a)))
}

func m256Cvtepi64Epi8(a [32]byte) [16]byte


// M256MaskCvtepi64Epi8: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQB'. Intrinsic: '_mm256_mask_cvtepi64_epi8'.
// Requires AVX512F.
func M256MaskCvtepi64Epi8(src x86.M128i, k x86.Mmask8, a x86.M256i) x86.M128i {
	return x86.M128i(m256MaskCvtepi64Epi8([16]byte(src), uint8(k), [32]byte(a)))
}

func m256MaskCvtepi64Epi8(src [16]byte, k uint8, a [32]byte) [16]byte


// M256MaskzCvtepi64Epi8: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQB'. Intrinsic: '_mm256_maskz_cvtepi64_epi8'.
// Requires AVX512F.
func M256MaskzCvtepi64Epi8(k x86.Mmask8, a x86.M256i) x86.M128i {
	return x86.M128i(m256MaskzCvtepi64Epi8(uint8(k), [32]byte(a)))
}

func m256MaskzCvtepi64Epi8(k uint8, a [32]byte) [16]byte


// M512Cvtepi64Epi8: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 8*j
//			dst[k+7:k] := Truncate_Int64_To_Int8(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQB'. Intrinsic: '_mm512_cvtepi64_epi8'.
// Requires AVX512F.
func M512Cvtepi64Epi8(a x86.M512i) x86.M128i {
	return x86.M128i(m512Cvtepi64Epi8([64]byte(a)))
}

func m512Cvtepi64Epi8(a [64]byte) [16]byte


// M512MaskCvtepi64Epi8: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQB'. Intrinsic: '_mm512_mask_cvtepi64_epi8'.
// Requires AVX512F.
func M512MaskCvtepi64Epi8(src x86.M128i, k x86.Mmask8, a x86.M512i) x86.M128i {
	return x86.M128i(m512MaskCvtepi64Epi8([16]byte(src), uint8(k), [64]byte(a)))
}

func m512MaskCvtepi64Epi8(src [16]byte, k uint8, a [64]byte) [16]byte


// M512MaskzCvtepi64Epi8: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQB'. Intrinsic: '_mm512_maskz_cvtepi64_epi8'.
// Requires AVX512F.
func M512MaskzCvtepi64Epi8(k x86.Mmask8, a x86.M512i) x86.M128i {
	return x86.M128i(m512MaskzCvtepi64Epi8(uint8(k), [64]byte(a)))
}

func m512MaskzCvtepi64Epi8(k uint8, a [64]byte) [16]byte


// Skipped: _mm_mask_cvtepi64_storeu_epi16. Contains pointer parameter.


// Skipped: _mm256_mask_cvtepi64_storeu_epi16. Contains pointer parameter.


// Skipped: _mm512_mask_cvtepi64_storeu_epi16. Contains pointer parameter.


// Skipped: _mm_mask_cvtepi64_storeu_epi32. Contains pointer parameter.


// Skipped: _mm256_mask_cvtepi64_storeu_epi32. Contains pointer parameter.


// Skipped: _mm512_mask_cvtepi64_storeu_epi32. Contains pointer parameter.


// Skipped: _mm_mask_cvtepi64_storeu_epi8. Contains pointer parameter.


// Skipped: _mm256_mask_cvtepi64_storeu_epi8. Contains pointer parameter.


// Skipped: _mm512_mask_cvtepi64_storeu_epi8. Contains pointer parameter.


// MaskCvtepi8Epi32: Sign extend packed 8-bit integers in the low 4 bytes of
// 'a' to packed 32-bit integers, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[l+7:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSXBD'. Intrinsic: '_mm_mask_cvtepi8_epi32'.
// Requires AVX512F.
func MaskCvtepi8Epi32(src x86.M128i, k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskCvtepi8Epi32([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi8Epi32(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepi8Epi32: Sign extend packed 8-bit integers in the low 4 bytes of
// 'a' to packed 32-bit integers, and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[l+7:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSXBD'. Intrinsic: '_mm_maskz_cvtepi8_epi32'.
// Requires AVX512F.
func MaskzCvtepi8Epi32(k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskzCvtepi8Epi32(uint8(k), [16]byte(a)))
}

func maskzCvtepi8Epi32(k uint8, a [16]byte) [16]byte


// M256MaskCvtepi8Epi32: Sign extend packed 8-bit integers in the low 8 bytes
// of 'a' to packed 32-bit integers, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[l+7:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXBD'. Intrinsic: '_mm256_mask_cvtepi8_epi32'.
// Requires AVX512F.
func M256MaskCvtepi8Epi32(src x86.M256i, k x86.Mmask8, a x86.M128i) x86.M256i {
	return x86.M256i(m256MaskCvtepi8Epi32([32]byte(src), uint8(k), [16]byte(a)))
}

func m256MaskCvtepi8Epi32(src [32]byte, k uint8, a [16]byte) [32]byte


// M256MaskzCvtepi8Epi32: Sign extend packed 8-bit integers in the low 8 bytes
// of 'a' to packed 32-bit integers, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[l+7:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXBD'. Intrinsic: '_mm256_maskz_cvtepi8_epi32'.
// Requires AVX512F.
func M256MaskzCvtepi8Epi32(k x86.Mmask8, a x86.M128i) x86.M256i {
	return x86.M256i(m256MaskzCvtepi8Epi32(uint8(k), [16]byte(a)))
}

func m256MaskzCvtepi8Epi32(k uint8, a [16]byte) [32]byte


// M512Cvtepi8Epi32: Sign extend packed 8-bit integers in 'a' to packed 32-bit
// integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			k := 8*j
//			dst[i+31:i] := SignExtend(a[k+7:k])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXBD'. Intrinsic: '_mm512_cvtepi8_epi32'.
// Requires AVX512F.
func M512Cvtepi8Epi32(a x86.M128i) x86.M512i {
	return x86.M512i(m512Cvtepi8Epi32([16]byte(a)))
}

func m512Cvtepi8Epi32(a [16]byte) [64]byte


// M512MaskCvtepi8Epi32: Sign extend packed 8-bit integers in 'a' to packed
// 32-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[l+7:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXBD'. Intrinsic: '_mm512_mask_cvtepi8_epi32'.
// Requires AVX512F.
func M512MaskCvtepi8Epi32(src x86.M512i, k x86.Mmask16, a x86.M128i) x86.M512i {
	return x86.M512i(m512MaskCvtepi8Epi32([64]byte(src), uint16(k), [16]byte(a)))
}

func m512MaskCvtepi8Epi32(src [64]byte, k uint16, a [16]byte) [64]byte


// M512MaskzCvtepi8Epi32: Sign extend packed 8-bit integers in 'a' to packed
// 32-bit integers, and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[l+7:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXBD'. Intrinsic: '_mm512_maskz_cvtepi8_epi32'.
// Requires AVX512F.
func M512MaskzCvtepi8Epi32(k x86.Mmask16, a x86.M128i) x86.M512i {
	return x86.M512i(m512MaskzCvtepi8Epi32(uint16(k), [16]byte(a)))
}

func m512MaskzCvtepi8Epi32(k uint16, a [16]byte) [64]byte


// MaskCvtepi8Epi64: Sign extend packed 8-bit integers in the low 2 bytes of
// 'a' to packed 64-bit integers, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+7:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSXBQ'. Intrinsic: '_mm_mask_cvtepi8_epi64'.
// Requires AVX512F.
func MaskCvtepi8Epi64(src x86.M128i, k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskCvtepi8Epi64([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi8Epi64(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepi8Epi64: Sign extend packed 8-bit integers in the low 2 bytes of
// 'a' to packed 64-bit integers, and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+7:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSXBQ'. Intrinsic: '_mm_maskz_cvtepi8_epi64'.
// Requires AVX512F.
func MaskzCvtepi8Epi64(k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskzCvtepi8Epi64(uint8(k), [16]byte(a)))
}

func maskzCvtepi8Epi64(k uint8, a [16]byte) [16]byte


// M256MaskCvtepi8Epi64: Sign extend packed 8-bit integers in the low 4 bytes
// of 'a' to packed 64-bit integers, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+7:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXBQ'. Intrinsic: '_mm256_mask_cvtepi8_epi64'.
// Requires AVX512F.
func M256MaskCvtepi8Epi64(src x86.M256i, k x86.Mmask8, a x86.M128i) x86.M256i {
	return x86.M256i(m256MaskCvtepi8Epi64([32]byte(src), uint8(k), [16]byte(a)))
}

func m256MaskCvtepi8Epi64(src [32]byte, k uint8, a [16]byte) [32]byte


// M256MaskzCvtepi8Epi64: Sign extend packed 8-bit integers in the low 4 bytes
// of 'a' to packed 64-bit integers, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+7:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXBQ'. Intrinsic: '_mm256_maskz_cvtepi8_epi64'.
// Requires AVX512F.
func M256MaskzCvtepi8Epi64(k x86.Mmask8, a x86.M128i) x86.M256i {
	return x86.M256i(m256MaskzCvtepi8Epi64(uint8(k), [16]byte(a)))
}

func m256MaskzCvtepi8Epi64(k uint8, a [16]byte) [32]byte


// M512Cvtepi8Epi64: Sign extend packed 8-bit integers in the low 8 bytes of
// 'a' to packed 64-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 8*j
//			dst[i+63:i] := SignExtend(a[k+7:k])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXBQ'. Intrinsic: '_mm512_cvtepi8_epi64'.
// Requires AVX512F.
func M512Cvtepi8Epi64(a x86.M128i) x86.M512i {
	return x86.M512i(m512Cvtepi8Epi64([16]byte(a)))
}

func m512Cvtepi8Epi64(a [16]byte) [64]byte


// M512MaskCvtepi8Epi64: Sign extend packed 8-bit integers in the low 8 bytes
// of 'a' to packed 64-bit integers, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+7:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXBQ'. Intrinsic: '_mm512_mask_cvtepi8_epi64'.
// Requires AVX512F.
func M512MaskCvtepi8Epi64(src x86.M512i, k x86.Mmask8, a x86.M128i) x86.M512i {
	return x86.M512i(m512MaskCvtepi8Epi64([64]byte(src), uint8(k), [16]byte(a)))
}

func m512MaskCvtepi8Epi64(src [64]byte, k uint8, a [16]byte) [64]byte


// M512MaskzCvtepi8Epi64: Sign extend packed 8-bit integers in the low 8 bytes
// of 'a' to packed 64-bit integers, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+7:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXBQ'. Intrinsic: '_mm512_maskz_cvtepi8_epi64'.
// Requires AVX512F.
func M512MaskzCvtepi8Epi64(k x86.Mmask8, a x86.M128i) x86.M512i {
	return x86.M512i(m512MaskzCvtepi8Epi64(uint8(k), [16]byte(a)))
}

func m512MaskzCvtepi8Epi64(k uint8, a [16]byte) [64]byte


// MaskCvtepu16Epi32: Zero extend packed unsigned 16-bit integers in 'a' to
// packed 32-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[l+15:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVZXWD'. Intrinsic: '_mm_mask_cvtepu16_epi32'.
// Requires AVX512F.
func MaskCvtepu16Epi32(src x86.M128i, k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskCvtepu16Epi32([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepu16Epi32(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepu16Epi32: Zero extend packed unsigned 16-bit integers in 'a' to
// packed 32-bit integers, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[l+15:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVZXWD'. Intrinsic: '_mm_maskz_cvtepu16_epi32'.
// Requires AVX512F.
func MaskzCvtepu16Epi32(k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskzCvtepu16Epi32(uint8(k), [16]byte(a)))
}

func maskzCvtepu16Epi32(k uint8, a [16]byte) [16]byte


// M256MaskCvtepu16Epi32: Zero extend packed unsigned 16-bit integers in 'a' to
// packed 32-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[l+15:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXWD'. Intrinsic: '_mm256_mask_cvtepu16_epi32'.
// Requires AVX512F.
func M256MaskCvtepu16Epi32(src x86.M256i, k x86.Mmask8, a x86.M128i) x86.M256i {
	return x86.M256i(m256MaskCvtepu16Epi32([32]byte(src), uint8(k), [16]byte(a)))
}

func m256MaskCvtepu16Epi32(src [32]byte, k uint8, a [16]byte) [32]byte


// M256MaskzCvtepu16Epi32: Zero extend packed unsigned 16-bit integers in 'a'
// to packed 32-bit integers, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[l+15:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXWD'. Intrinsic: '_mm256_maskz_cvtepu16_epi32'.
// Requires AVX512F.
func M256MaskzCvtepu16Epi32(k x86.Mmask8, a x86.M128i) x86.M256i {
	return x86.M256i(m256MaskzCvtepu16Epi32(uint8(k), [16]byte(a)))
}

func m256MaskzCvtepu16Epi32(k uint8, a [16]byte) [32]byte


// M512Cvtepu16Epi32: Zero extend packed unsigned 16-bit integers in 'a' to
// packed 32-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			k := 16*j
//			dst[i+31:i] := ZeroExtend(a[k+15:k])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXWD'. Intrinsic: '_mm512_cvtepu16_epi32'.
// Requires AVX512F.
func M512Cvtepu16Epi32(a x86.M256i) x86.M512i {
	return x86.M512i(m512Cvtepu16Epi32([32]byte(a)))
}

func m512Cvtepu16Epi32(a [32]byte) [64]byte


// M512MaskCvtepu16Epi32: Zero extend packed unsigned 16-bit integers in 'a' to
// packed 32-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[l+15:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXWD'. Intrinsic: '_mm512_mask_cvtepu16_epi32'.
// Requires AVX512F.
func M512MaskCvtepu16Epi32(src x86.M512i, k x86.Mmask16, a x86.M256i) x86.M512i {
	return x86.M512i(m512MaskCvtepu16Epi32([64]byte(src), uint16(k), [32]byte(a)))
}

func m512MaskCvtepu16Epi32(src [64]byte, k uint16, a [32]byte) [64]byte


// M512MaskzCvtepu16Epi32: Zero extend packed unsigned 16-bit integers in 'a'
// to packed 32-bit integers, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[l+15:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXWD'. Intrinsic: '_mm512_maskz_cvtepu16_epi32'.
// Requires AVX512F.
func M512MaskzCvtepu16Epi32(k x86.Mmask16, a x86.M256i) x86.M512i {
	return x86.M512i(m512MaskzCvtepu16Epi32(uint16(k), [32]byte(a)))
}

func m512MaskzCvtepu16Epi32(k uint16, a [32]byte) [64]byte


// MaskCvtepu16Epi64: Zero extend packed unsigned 16-bit integers in the low 4
// bytes of 'a' to packed 64-bit integers, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+15:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVZXWQ'. Intrinsic: '_mm_mask_cvtepu16_epi64'.
// Requires AVX512F.
func MaskCvtepu16Epi64(src x86.M128i, k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskCvtepu16Epi64([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepu16Epi64(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepu16Epi64: Zero extend packed unsigned 16-bit integers in the low 4
// bytes of 'a' to packed 64-bit integers, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+15:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVZXWQ'. Intrinsic: '_mm_maskz_cvtepu16_epi64'.
// Requires AVX512F.
func MaskzCvtepu16Epi64(k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskzCvtepu16Epi64(uint8(k), [16]byte(a)))
}

func maskzCvtepu16Epi64(k uint8, a [16]byte) [16]byte


// M256MaskCvtepu16Epi64: Zero extend packed unsigned 16-bit integers in the
// low 8 bytes of 'a' to packed 64-bit integers, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+15:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXWQ'. Intrinsic: '_mm256_mask_cvtepu16_epi64'.
// Requires AVX512F.
func M256MaskCvtepu16Epi64(src x86.M256i, k x86.Mmask8, a x86.M128i) x86.M256i {
	return x86.M256i(m256MaskCvtepu16Epi64([32]byte(src), uint8(k), [16]byte(a)))
}

func m256MaskCvtepu16Epi64(src [32]byte, k uint8, a [16]byte) [32]byte


// M256MaskzCvtepu16Epi64: Zero extend packed unsigned 16-bit integers in the
// low 8 bytes of 'a' to packed 64-bit integers, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+15:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXWQ'. Intrinsic: '_mm256_maskz_cvtepu16_epi64'.
// Requires AVX512F.
func M256MaskzCvtepu16Epi64(k x86.Mmask8, a x86.M128i) x86.M256i {
	return x86.M256i(m256MaskzCvtepu16Epi64(uint8(k), [16]byte(a)))
}

func m256MaskzCvtepu16Epi64(k uint8, a [16]byte) [32]byte


// M512Cvtepu16Epi64: Zero extend packed unsigned 16-bit integers in 'a' to
// packed 64-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 16*j
//			dst[i+63:i] := ZeroExtend(a[k+15:k])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXWQ'. Intrinsic: '_mm512_cvtepu16_epi64'.
// Requires AVX512F.
func M512Cvtepu16Epi64(a x86.M128i) x86.M512i {
	return x86.M512i(m512Cvtepu16Epi64([16]byte(a)))
}

func m512Cvtepu16Epi64(a [16]byte) [64]byte


// M512MaskCvtepu16Epi64: Zero extend packed unsigned 16-bit integers in 'a' to
// packed 64-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+15:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXWQ'. Intrinsic: '_mm512_mask_cvtepu16_epi64'.
// Requires AVX512F.
func M512MaskCvtepu16Epi64(src x86.M512i, k x86.Mmask8, a x86.M128i) x86.M512i {
	return x86.M512i(m512MaskCvtepu16Epi64([64]byte(src), uint8(k), [16]byte(a)))
}

func m512MaskCvtepu16Epi64(src [64]byte, k uint8, a [16]byte) [64]byte


// M512MaskzCvtepu16Epi64: Zero extend packed unsigned 16-bit integers in 'a'
// to packed 64-bit integers, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+15:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXWQ'. Intrinsic: '_mm512_maskz_cvtepu16_epi64'.
// Requires AVX512F.
func M512MaskzCvtepu16Epi64(k x86.Mmask8, a x86.M128i) x86.M512i {
	return x86.M512i(m512MaskzCvtepu16Epi64(uint8(k), [16]byte(a)))
}

func m512MaskzCvtepu16Epi64(k uint8, a [16]byte) [64]byte


// MaskCvtepu32Epi64: Zero extend packed unsigned 32-bit integers in 'a' to
// packed 64-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVZXDQ'. Intrinsic: '_mm_mask_cvtepu32_epi64'.
// Requires AVX512F.
func MaskCvtepu32Epi64(src x86.M128i, k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskCvtepu32Epi64([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepu32Epi64(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepu32Epi64: Zero extend packed unsigned 32-bit integers in 'a' to
// packed 64-bit integers, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+31:l])
//			ELSE 
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVZXDQ'. Intrinsic: '_mm_maskz_cvtepu32_epi64'.
// Requires AVX512F.
func MaskzCvtepu32Epi64(k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskzCvtepu32Epi64(uint8(k), [16]byte(a)))
}

func maskzCvtepu32Epi64(k uint8, a [16]byte) [16]byte


// M256MaskCvtepu32Epi64: Zero extend packed unsigned 32-bit integers in 'a' to
// packed 64-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXDQ'. Intrinsic: '_mm256_mask_cvtepu32_epi64'.
// Requires AVX512F.
func M256MaskCvtepu32Epi64(src x86.M256i, k x86.Mmask8, a x86.M128i) x86.M256i {
	return x86.M256i(m256MaskCvtepu32Epi64([32]byte(src), uint8(k), [16]byte(a)))
}

func m256MaskCvtepu32Epi64(src [32]byte, k uint8, a [16]byte) [32]byte


// M256MaskzCvtepu32Epi64: Zero extend packed unsigned 32-bit integers in 'a'
// to packed 64-bit integers, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+31:l])
//			ELSE 
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXDQ'. Intrinsic: '_mm256_maskz_cvtepu32_epi64'.
// Requires AVX512F.
func M256MaskzCvtepu32Epi64(k x86.Mmask8, a x86.M128i) x86.M256i {
	return x86.M256i(m256MaskzCvtepu32Epi64(uint8(k), [16]byte(a)))
}

func m256MaskzCvtepu32Epi64(k uint8, a [16]byte) [32]byte


// M512Cvtepu32Epi64: Zero extend packed unsigned 32-bit integers in 'a' to
// packed 64-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 32*j
//			dst[i+63:i] := ZeroExtend(a[k+31:k])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXDQ'. Intrinsic: '_mm512_cvtepu32_epi64'.
// Requires AVX512F.
func M512Cvtepu32Epi64(a x86.M256i) x86.M512i {
	return x86.M512i(m512Cvtepu32Epi64([32]byte(a)))
}

func m512Cvtepu32Epi64(a [32]byte) [64]byte


// M512MaskCvtepu32Epi64: Zero extend packed unsigned 32-bit integers in 'a' to
// packed 64-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXDQ'. Intrinsic: '_mm512_mask_cvtepu32_epi64'.
// Requires AVX512F.
func M512MaskCvtepu32Epi64(src x86.M512i, k x86.Mmask8, a x86.M256i) x86.M512i {
	return x86.M512i(m512MaskCvtepu32Epi64([64]byte(src), uint8(k), [32]byte(a)))
}

func m512MaskCvtepu32Epi64(src [64]byte, k uint8, a [32]byte) [64]byte


// M512MaskzCvtepu32Epi64: Zero extend packed unsigned 32-bit integers in 'a'
// to packed 64-bit integers, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+31:l])
//			ELSE 
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXDQ'. Intrinsic: '_mm512_maskz_cvtepu32_epi64'.
// Requires AVX512F.
func M512MaskzCvtepu32Epi64(k x86.Mmask8, a x86.M256i) x86.M512i {
	return x86.M512i(m512MaskzCvtepu32Epi64(uint8(k), [32]byte(a)))
}

func m512MaskzCvtepu32Epi64(k uint8, a [32]byte) [64]byte


// Cvtepu32Pd: Convert packed unsigned 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			l := j*32
//			dst[i+63:i] := ConvertUnsignedIntegerTo_FP64(a[l+31:l])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTUDQ2PD'. Intrinsic: '_mm_cvtepu32_pd'.
// Requires AVX512F.
func Cvtepu32Pd(a x86.M128i) x86.M128d {
	return x86.M128d(cvtepu32Pd([16]byte(a)))
}

func cvtepu32Pd(a [16]byte) [2]float64


// MaskCvtepu32Pd: Convert packed unsigned 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := ConvertUnsignedIntegerTo_FP64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTUDQ2PD'. Intrinsic: '_mm_mask_cvtepu32_pd'.
// Requires AVX512F.
func MaskCvtepu32Pd(src x86.M128d, k x86.Mmask8, a x86.M128i) x86.M128d {
	return x86.M128d(maskCvtepu32Pd([2]float64(src), uint8(k), [16]byte(a)))
}

func maskCvtepu32Pd(src [2]float64, k uint8, a [16]byte) [2]float64


// MaskzCvtepu32Pd: Convert packed unsigned 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := ConvertUnsignedIntegerTo_FP64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTUDQ2PD'. Intrinsic: '_mm_maskz_cvtepu32_pd'.
// Requires AVX512F.
func MaskzCvtepu32Pd(k x86.Mmask8, a x86.M128i) x86.M128d {
	return x86.M128d(maskzCvtepu32Pd(uint8(k), [16]byte(a)))
}

func maskzCvtepu32Pd(k uint8, a [16]byte) [2]float64


// M256Cvtepu32Pd: Convert packed unsigned 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			dst[i+63:i] := ConvertUnsignedIntegerTo_FP64(a[l+31:l])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTUDQ2PD'. Intrinsic: '_mm256_cvtepu32_pd'.
// Requires AVX512F.
func M256Cvtepu32Pd(a x86.M128i) x86.M256d {
	return x86.M256d(m256Cvtepu32Pd([16]byte(a)))
}

func m256Cvtepu32Pd(a [16]byte) [4]float64


// M256MaskCvtepu32Pd: Convert packed unsigned 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := ConvertUnsignedIntegerTo_FP64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTUDQ2PD'. Intrinsic: '_mm256_mask_cvtepu32_pd'.
// Requires AVX512F.
func M256MaskCvtepu32Pd(src x86.M256d, k x86.Mmask8, a x86.M128i) x86.M256d {
	return x86.M256d(m256MaskCvtepu32Pd([4]float64(src), uint8(k), [16]byte(a)))
}

func m256MaskCvtepu32Pd(src [4]float64, k uint8, a [16]byte) [4]float64


// M256MaskzCvtepu32Pd: Convert packed unsigned 32-bit integers in 'a' to
// packed double-precision (64-bit) floating-point elements, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := ConvertUnsignedIntegerTo_FP64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTUDQ2PD'. Intrinsic: '_mm256_maskz_cvtepu32_pd'.
// Requires AVX512F.
func M256MaskzCvtepu32Pd(k x86.Mmask8, a x86.M128i) x86.M256d {
	return x86.M256d(m256MaskzCvtepu32Pd(uint8(k), [16]byte(a)))
}

func m256MaskzCvtepu32Pd(k uint8, a [16]byte) [4]float64


// M512Cvtepu32Pd: Convert packed unsigned 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			dst[i+63:i] := ConvertUnsignedIntegerTo_FP64(a[l+31:l])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTUDQ2PD'. Intrinsic: '_mm512_cvtepu32_pd'.
// Requires AVX512F.
func M512Cvtepu32Pd(a x86.M256i) x86.M512d {
	return x86.M512d(m512Cvtepu32Pd([32]byte(a)))
}

func m512Cvtepu32Pd(a [32]byte) [8]float64


// M512MaskCvtepu32Pd: Convert packed unsigned 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := ConvertUnsignedIntegerTo_FP64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTUDQ2PD'. Intrinsic: '_mm512_mask_cvtepu32_pd'.
// Requires AVX512F.
func M512MaskCvtepu32Pd(src x86.M512d, k x86.Mmask8, a x86.M256i) x86.M512d {
	return x86.M512d(m512MaskCvtepu32Pd([8]float64(src), uint8(k), [32]byte(a)))
}

func m512MaskCvtepu32Pd(src [8]float64, k uint8, a [32]byte) [8]float64


// M512MaskzCvtepu32Pd: Convert packed unsigned 32-bit integers in 'a' to
// packed double-precision (64-bit) floating-point elements, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := ConvertUnsignedIntegerTo_FP64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTUDQ2PD'. Intrinsic: '_mm512_maskz_cvtepu32_pd'.
// Requires AVX512F.
func M512MaskzCvtepu32Pd(k x86.Mmask8, a x86.M256i) x86.M512d {
	return x86.M512d(m512MaskzCvtepu32Pd(uint8(k), [32]byte(a)))
}

func m512MaskzCvtepu32Pd(k uint8, a [32]byte) [8]float64


// M512Cvtepu32Ps: Convert packed unsigned 32-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := ConvertUnsignedInt32_To_FP32(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTUDQ2PS'. Intrinsic: '_mm512_cvtepu32_ps'.
// Requires AVX512F.
func M512Cvtepu32Ps(a x86.M512i) x86.M512 {
	return x86.M512(m512Cvtepu32Ps([64]byte(a)))
}

func m512Cvtepu32Ps(a [64]byte) [16]float32


// M512MaskCvtepu32Ps: Convert packed unsigned 32-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ConvertUnsignedInt32_To_FP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTUDQ2PS'. Intrinsic: '_mm512_mask_cvtepu32_ps'.
// Requires AVX512F.
func M512MaskCvtepu32Ps(src x86.M512, k x86.Mmask16, a x86.M512i) x86.M512 {
	return x86.M512(m512MaskCvtepu32Ps([16]float32(src), uint16(k), [64]byte(a)))
}

func m512MaskCvtepu32Ps(src [16]float32, k uint16, a [64]byte) [16]float32


// M512MaskzCvtepu32Ps: Convert packed unsigned 32-bit integers in 'a' to
// packed single-precision (32-bit) floating-point elements, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := ConvertUnsignedInt32_To_FP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTUDQ2PS'. Intrinsic: '_mm512_maskz_cvtepu32_ps'.
// Requires AVX512F.
func M512MaskzCvtepu32Ps(k x86.Mmask16, a x86.M512i) x86.M512 {
	return x86.M512(m512MaskzCvtepu32Ps(uint16(k), [64]byte(a)))
}

func m512MaskzCvtepu32Ps(k uint16, a [64]byte) [16]float32


// MaskCvtepu8Epi32: Zero extend packed unsigned 8-bit integers in the low 4
// bytes of 'a' to packed 32-bit integers, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[l+7:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVZXBD'. Intrinsic: '_mm_mask_cvtepu8_epi32'.
// Requires AVX512F.
func MaskCvtepu8Epi32(src x86.M128i, k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskCvtepu8Epi32([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepu8Epi32(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepu8Epi32: Zero extend packed unsigned 8-bit integers in th elow 4
// bytes of 'a' to packed 32-bit integers, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[l+7:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVZXBD'. Intrinsic: '_mm_maskz_cvtepu8_epi32'.
// Requires AVX512F.
func MaskzCvtepu8Epi32(k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskzCvtepu8Epi32(uint8(k), [16]byte(a)))
}

func maskzCvtepu8Epi32(k uint8, a [16]byte) [16]byte


// M256MaskCvtepu8Epi32: Zero extend packed unsigned 8-bit integers in the low
// 8 bytes of 'a' to packed 32-bit integers, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[l+7:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXBD'. Intrinsic: '_mm256_mask_cvtepu8_epi32'.
// Requires AVX512F.
func M256MaskCvtepu8Epi32(src x86.M256i, k x86.Mmask8, a x86.M128i) x86.M256i {
	return x86.M256i(m256MaskCvtepu8Epi32([32]byte(src), uint8(k), [16]byte(a)))
}

func m256MaskCvtepu8Epi32(src [32]byte, k uint8, a [16]byte) [32]byte


// M256MaskzCvtepu8Epi32: Zero extend packed unsigned 8-bit integers in the low
// 8 bytes of 'a' to packed 32-bit integers, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[l+7:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXBD'. Intrinsic: '_mm256_maskz_cvtepu8_epi32'.
// Requires AVX512F.
func M256MaskzCvtepu8Epi32(k x86.Mmask8, a x86.M128i) x86.M256i {
	return x86.M256i(m256MaskzCvtepu8Epi32(uint8(k), [16]byte(a)))
}

func m256MaskzCvtepu8Epi32(k uint8, a [16]byte) [32]byte


// M512Cvtepu8Epi32: Zero extend packed unsigned 8-bit integers in 'a' to
// packed 32-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			k := 8*j
//			dst[i+31:i] := ZeroExtend(a[k+7:k])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXBD'. Intrinsic: '_mm512_cvtepu8_epi32'.
// Requires AVX512F.
func M512Cvtepu8Epi32(a x86.M128i) x86.M512i {
	return x86.M512i(m512Cvtepu8Epi32([16]byte(a)))
}

func m512Cvtepu8Epi32(a [16]byte) [64]byte


// M512MaskCvtepu8Epi32: Zero extend packed unsigned 8-bit integers in 'a' to
// packed 32-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[l+7:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXBD'. Intrinsic: '_mm512_mask_cvtepu8_epi32'.
// Requires AVX512F.
func M512MaskCvtepu8Epi32(src x86.M512i, k x86.Mmask16, a x86.M128i) x86.M512i {
	return x86.M512i(m512MaskCvtepu8Epi32([64]byte(src), uint16(k), [16]byte(a)))
}

func m512MaskCvtepu8Epi32(src [64]byte, k uint16, a [16]byte) [64]byte


// M512MaskzCvtepu8Epi32: Zero extend packed unsigned 8-bit integers in 'a' to
// packed 32-bit integers, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[l+7:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXBD'. Intrinsic: '_mm512_maskz_cvtepu8_epi32'.
// Requires AVX512F.
func M512MaskzCvtepu8Epi32(k x86.Mmask16, a x86.M128i) x86.M512i {
	return x86.M512i(m512MaskzCvtepu8Epi32(uint16(k), [16]byte(a)))
}

func m512MaskzCvtepu8Epi32(k uint16, a [16]byte) [64]byte


// MaskCvtepu8Epi64: Zero extend packed unsigned 8-bit integers in the low 2
// bytes of 'a' to packed 64-bit integers, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+7:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVZXBQ'. Intrinsic: '_mm_mask_cvtepu8_epi64'.
// Requires AVX512F.
func MaskCvtepu8Epi64(src x86.M128i, k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskCvtepu8Epi64([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepu8Epi64(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepu8Epi64: Zero extend packed unsigned 8-bit integers in the low 2
// bytes of 'a' to packed 64-bit integers, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+7:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVZXBQ'. Intrinsic: '_mm_maskz_cvtepu8_epi64'.
// Requires AVX512F.
func MaskzCvtepu8Epi64(k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskzCvtepu8Epi64(uint8(k), [16]byte(a)))
}

func maskzCvtepu8Epi64(k uint8, a [16]byte) [16]byte


// M256MaskCvtepu8Epi64: Zero extend packed unsigned 8-bit integers in the low
// 4 bytes of 'a' to packed 64-bit integers, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+7:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXBQ'. Intrinsic: '_mm256_mask_cvtepu8_epi64'.
// Requires AVX512F.
func M256MaskCvtepu8Epi64(src x86.M256i, k x86.Mmask8, a x86.M128i) x86.M256i {
	return x86.M256i(m256MaskCvtepu8Epi64([32]byte(src), uint8(k), [16]byte(a)))
}

func m256MaskCvtepu8Epi64(src [32]byte, k uint8, a [16]byte) [32]byte


// M256MaskzCvtepu8Epi64: Zero extend packed unsigned 8-bit integers in the low
// 4 bytes of 'a' to packed 64-bit integers, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+7:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXBQ'. Intrinsic: '_mm256_maskz_cvtepu8_epi64'.
// Requires AVX512F.
func M256MaskzCvtepu8Epi64(k x86.Mmask8, a x86.M128i) x86.M256i {
	return x86.M256i(m256MaskzCvtepu8Epi64(uint8(k), [16]byte(a)))
}

func m256MaskzCvtepu8Epi64(k uint8, a [16]byte) [32]byte


// M512Cvtepu8Epi64: Zero extend packed unsigned 8-bit integers in the low 8
// byte sof 'a' to packed 64-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 8*j
//			dst[i+63:i] := ZeroExtend(a[k+7:k])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXBQ'. Intrinsic: '_mm512_cvtepu8_epi64'.
// Requires AVX512F.
func M512Cvtepu8Epi64(a x86.M128i) x86.M512i {
	return x86.M512i(m512Cvtepu8Epi64([16]byte(a)))
}

func m512Cvtepu8Epi64(a [16]byte) [64]byte


// M512MaskCvtepu8Epi64: Zero extend packed unsigned 8-bit integers in the low
// 8 bytes of 'a' to packed 64-bit integers, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+7:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXBQ'. Intrinsic: '_mm512_mask_cvtepu8_epi64'.
// Requires AVX512F.
func M512MaskCvtepu8Epi64(src x86.M512i, k x86.Mmask8, a x86.M128i) x86.M512i {
	return x86.M512i(m512MaskCvtepu8Epi64([64]byte(src), uint8(k), [16]byte(a)))
}

func m512MaskCvtepu8Epi64(src [64]byte, k uint8, a [16]byte) [64]byte


// M512MaskzCvtepu8Epi64: Zero extend packed unsigned 8-bit integers in the low
// 8 bytes of 'a' to packed 64-bit integers, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+7:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXBQ'. Intrinsic: '_mm512_maskz_cvtepu8_epi64'.
// Requires AVX512F.
func M512MaskzCvtepu8Epi64(k x86.Mmask8, a x86.M128i) x86.M512i {
	return x86.M512i(m512MaskzCvtepu8Epi64(uint8(k), [16]byte(a)))
}

func m512MaskzCvtepu8Epi64(k uint8, a [16]byte) [64]byte


// Cvti32Sd: Convert the 32-bit integer 'b' to a double-precision (64-bit)
// floating-point element, store the result in the lower element of 'dst', and
// copy the upper element from 'a' to the upper element of 'dst'. 
//
//		dst[63:0] := Convert_Int32_To_FP64(b[31:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTSI2SD'. Intrinsic: '_mm_cvti32_sd'.
// Requires AVX512F.
func Cvti32Sd(a x86.M128d, b int) x86.M128d {
	return x86.M128d(cvti32Sd([2]float64(a), b))
}

func cvti32Sd(a [2]float64, b int) [2]float64


// Cvti32Ss: Convert the 32-bit integer 'b' to a single-precision (32-bit)
// floating-point element, store the result in the lower element of 'dst', and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'. 
//
//		dst[31:0] := Convert_Int32_To_FP32(b[31:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTSI2SS'. Intrinsic: '_mm_cvti32_ss'.
// Requires AVX512F.
func Cvti32Ss(a x86.M128, b int) x86.M128 {
	return x86.M128(cvti32Ss([4]float32(a), b))
}

func cvti32Ss(a [4]float32, b int) [4]float32


// Cvti64Sd: Convert the 64-bit integer 'b' to a double-precision (64-bit)
// floating-point element, store the result in the lower element of 'dst', and
// copy the upper element from 'a' to the upper element of 'dst'. 
//
//		dst[63:0] := Convert_Int64_To_FP64(b[63:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTSI2SD'. Intrinsic: '_mm_cvti64_sd'.
// Requires AVX512F.
func Cvti64Sd(a x86.M128d, b int64) x86.M128d {
	return x86.M128d(cvti64Sd([2]float64(a), b))
}

func cvti64Sd(a [2]float64, b int64) [2]float64


// Cvti64Ss: Convert the 64-bit integer 'b' to a single-precision (32-bit)
// floating-point element, store the result in the lower element of 'dst', and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'. 
//
//		dst[31:0] := Convert_Int64_To_FP32(b[63:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTSI2SS'. Intrinsic: '_mm_cvti64_ss'.
// Requires AVX512F.
func Cvti64Ss(a x86.M128, b int64) x86.M128 {
	return x86.M128(cvti64Ss([4]float32(a), b))
}

func cvti64Ss(a [4]float32, b int64) [4]float32


// MaskCvtpdEpi32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*32
//			l := j*64
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_Int32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTPD2DQ'. Intrinsic: '_mm_mask_cvtpd_epi32'.
// Requires AVX512F.
func MaskCvtpdEpi32(src x86.M128i, k x86.Mmask8, a x86.M128d) x86.M128i {
	return x86.M128i(maskCvtpdEpi32([16]byte(src), uint8(k), [2]float64(a)))
}

func maskCvtpdEpi32(src [16]byte, k uint8, a [2]float64) [16]byte


// MaskzCvtpdEpi32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 1
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_Int32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTPD2DQ'. Intrinsic: '_mm_maskz_cvtpd_epi32'.
// Requires AVX512F.
func MaskzCvtpdEpi32(k x86.Mmask8, a x86.M128d) x86.M128i {
	return x86.M128i(maskzCvtpdEpi32(uint8(k), [2]float64(a)))
}

func maskzCvtpdEpi32(k uint8, a [2]float64) [16]byte


// M256MaskCvtpdEpi32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			l := j*64
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_Int32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPD2DQ'. Intrinsic: '_mm256_mask_cvtpd_epi32'.
// Requires AVX512F.
func M256MaskCvtpdEpi32(src x86.M128i, k x86.Mmask8, a x86.M256d) x86.M128i {
	return x86.M128i(m256MaskCvtpdEpi32([16]byte(src), uint8(k), [4]float64(a)))
}

func m256MaskCvtpdEpi32(src [16]byte, k uint8, a [4]float64) [16]byte


// M256MaskzCvtpdEpi32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_Int32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPD2DQ'. Intrinsic: '_mm256_maskz_cvtpd_epi32'.
// Requires AVX512F.
func M256MaskzCvtpdEpi32(k x86.Mmask8, a x86.M256d) x86.M128i {
	return x86.M128i(m256MaskzCvtpdEpi32(uint8(k), [4]float64(a)))
}

func m256MaskzCvtpdEpi32(k uint8, a [4]float64) [16]byte


// M512CvtpdEpi32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_Int32(a[k+63:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2DQ'. Intrinsic: '_mm512_cvtpd_epi32'.
// Requires AVX512F.
func M512CvtpdEpi32(a x86.M512d) x86.M256i {
	return x86.M256i(m512CvtpdEpi32([8]float64(a)))
}

func m512CvtpdEpi32(a [8]float64) [32]byte


// M512MaskCvtpdEpi32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			l := j*64
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_Int32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2DQ'. Intrinsic: '_mm512_mask_cvtpd_epi32'.
// Requires AVX512F.
func M512MaskCvtpdEpi32(src x86.M256i, k x86.Mmask8, a x86.M512d) x86.M256i {
	return x86.M256i(m512MaskCvtpdEpi32([32]byte(src), uint8(k), [8]float64(a)))
}

func m512MaskCvtpdEpi32(src [32]byte, k uint8, a [8]float64) [32]byte


// M512MaskzCvtpdEpi32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_Int32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2DQ'. Intrinsic: '_mm512_maskz_cvtpd_epi32'.
// Requires AVX512F.
func M512MaskzCvtpdEpi32(k x86.Mmask8, a x86.M512d) x86.M256i {
	return x86.M256i(m512MaskzCvtpdEpi32(uint8(k), [8]float64(a)))
}

func m512MaskzCvtpdEpi32(k uint8, a [8]float64) [32]byte


// CvtpdEpu32: Convert packed double-precision (64-bit) floating-point elements
// in 'a' to packed unsigned 32-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[k+63:k])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTPD2UDQ'. Intrinsic: '_mm_cvtpd_epu32'.
// Requires AVX512F.
func CvtpdEpu32(a x86.M128d) x86.M128i {
	return x86.M128i(cvtpdEpu32([2]float64(a)))
}

func cvtpdEpu32(a [2]float64) [16]byte


// MaskCvtpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*32
//			l := j*64
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTPD2UDQ'. Intrinsic: '_mm_mask_cvtpd_epu32'.
// Requires AVX512F.
func MaskCvtpdEpu32(src x86.M128i, k x86.Mmask8, a x86.M128d) x86.M128i {
	return x86.M128i(maskCvtpdEpu32([16]byte(src), uint8(k), [2]float64(a)))
}

func maskCvtpdEpu32(src [16]byte, k uint8, a [2]float64) [16]byte


// MaskzCvtpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTPD2UDQ'. Intrinsic: '_mm_maskz_cvtpd_epu32'.
// Requires AVX512F.
func MaskzCvtpdEpu32(k x86.Mmask8, a x86.M128d) x86.M128i {
	return x86.M128i(maskzCvtpdEpu32(uint8(k), [2]float64(a)))
}

func maskzCvtpdEpu32(k uint8, a [2]float64) [16]byte


// M256CvtpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[k+63:k])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPD2UDQ'. Intrinsic: '_mm256_cvtpd_epu32'.
// Requires AVX512F.
func M256CvtpdEpu32(a x86.M256d) x86.M128i {
	return x86.M128i(m256CvtpdEpu32([4]float64(a)))
}

func m256CvtpdEpu32(a [4]float64) [16]byte


// M256MaskCvtpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			l := j*64
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPD2UDQ'. Intrinsic: '_mm256_mask_cvtpd_epu32'.
// Requires AVX512F.
func M256MaskCvtpdEpu32(src x86.M128i, k x86.Mmask8, a x86.M256d) x86.M128i {
	return x86.M128i(m256MaskCvtpdEpu32([16]byte(src), uint8(k), [4]float64(a)))
}

func m256MaskCvtpdEpu32(src [16]byte, k uint8, a [4]float64) [16]byte


// M256MaskzCvtpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPD2UDQ'. Intrinsic: '_mm256_maskz_cvtpd_epu32'.
// Requires AVX512F.
func M256MaskzCvtpdEpu32(k x86.Mmask8, a x86.M256d) x86.M128i {
	return x86.M128i(m256MaskzCvtpdEpu32(uint8(k), [4]float64(a)))
}

func m256MaskzCvtpdEpu32(k uint8, a [4]float64) [16]byte


// M512CvtpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[k+63:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2UDQ'. Intrinsic: '_mm512_cvtpd_epu32'.
// Requires AVX512F.
func M512CvtpdEpu32(a x86.M512d) x86.M256i {
	return x86.M256i(m512CvtpdEpu32([8]float64(a)))
}

func m512CvtpdEpu32(a [8]float64) [32]byte


// M512MaskCvtpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			l := j*64
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2UDQ'. Intrinsic: '_mm512_mask_cvtpd_epu32'.
// Requires AVX512F.
func M512MaskCvtpdEpu32(src x86.M256i, k x86.Mmask8, a x86.M512d) x86.M256i {
	return x86.M256i(m512MaskCvtpdEpu32([32]byte(src), uint8(k), [8]float64(a)))
}

func m512MaskCvtpdEpu32(src [32]byte, k uint8, a [8]float64) [32]byte


// M512MaskzCvtpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2UDQ'. Intrinsic: '_mm512_maskz_cvtpd_epu32'.
// Requires AVX512F.
func M512MaskzCvtpdEpu32(k x86.Mmask8, a x86.M512d) x86.M256i {
	return x86.M256i(m512MaskzCvtpdEpu32(uint8(k), [8]float64(a)))
}

func m512MaskzCvtpdEpu32(k uint8, a [8]float64) [32]byte


// MaskCvtpdPs: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed single-precision (32-bit) floating-point elements,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_FP32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTPD2PS'. Intrinsic: '_mm_mask_cvtpd_ps'.
// Requires AVX512F.
func MaskCvtpdPs(src x86.M128, k x86.Mmask8, a x86.M128d) x86.M128 {
	return x86.M128(maskCvtpdPs([4]float32(src), uint8(k), [2]float64(a)))
}

func maskCvtpdPs(src [4]float32, k uint8, a [2]float64) [4]float32


// MaskzCvtpdPs: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed single-precision (32-bit) floating-point elements,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*32
//			l := j*64
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_FP32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTPD2PS'. Intrinsic: '_mm_maskz_cvtpd_ps'.
// Requires AVX512F.
func MaskzCvtpdPs(k x86.Mmask8, a x86.M128d) x86.M128 {
	return x86.M128(maskzCvtpdPs(uint8(k), [2]float64(a)))
}

func maskzCvtpdPs(k uint8, a [2]float64) [4]float32


// M256MaskCvtpdPs: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed single-precision (32-bit) floating-point elements,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_FP32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPD2PS'. Intrinsic: '_mm256_mask_cvtpd_ps'.
// Requires AVX512F.
func M256MaskCvtpdPs(src x86.M128, k x86.Mmask8, a x86.M256d) x86.M128 {
	return x86.M128(m256MaskCvtpdPs([4]float32(src), uint8(k), [4]float64(a)))
}

func m256MaskCvtpdPs(src [4]float32, k uint8, a [4]float64) [4]float32


// M256MaskzCvtpdPs: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed single-precision (32-bit) floating-point elements,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			l := j*64
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_FP32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPD2PS'. Intrinsic: '_mm256_maskz_cvtpd_ps'.
// Requires AVX512F.
func M256MaskzCvtpdPs(k x86.Mmask8, a x86.M256d) x86.M128 {
	return x86.M128(m256MaskzCvtpdPs(uint8(k), [4]float64(a)))
}

func m256MaskzCvtpdPs(k uint8, a [4]float64) [4]float32


// M512CvtpdPs: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed single-precision (32-bit) floating-point elements,
// and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_FP32(a[k+63:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2PS'. Intrinsic: '_mm512_cvtpd_ps'.
// Requires AVX512F.
func M512CvtpdPs(a x86.M512d) x86.M256 {
	return x86.M256(m512CvtpdPs([8]float64(a)))
}

func m512CvtpdPs(a [8]float64) [8]float32


// M512MaskCvtpdPs: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed single-precision (32-bit) floating-point elements,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_FP32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2PS'. Intrinsic: '_mm512_mask_cvtpd_ps'.
// Requires AVX512F.
func M512MaskCvtpdPs(src x86.M256, k x86.Mmask8, a x86.M512d) x86.M256 {
	return x86.M256(m512MaskCvtpdPs([8]float32(src), uint8(k), [8]float64(a)))
}

func m512MaskCvtpdPs(src [8]float32, k uint8, a [8]float64) [8]float32


// M512MaskzCvtpdPs: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed single-precision (32-bit) floating-point elements,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			l := j*64
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_FP32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2PS'. Intrinsic: '_mm512_maskz_cvtpd_ps'.
// Requires AVX512F.
func M512MaskzCvtpdPs(k x86.Mmask8, a x86.M512d) x86.M256 {
	return x86.M256(m512MaskzCvtpdPs(uint8(k), [8]float64(a)))
}

func m512MaskzCvtpdPs(k uint8, a [8]float64) [8]float32


// MaskCvtphPs: Convert packed half-precision (16-bit) floating-point elements
// in 'a' to packed single-precision (32-bit) floating-point elements, and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			m := j*16
//			IF k[j]
//				dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPH2PS'. Intrinsic: '_mm_mask_cvtph_ps'.
// Requires AVX512F.
func MaskCvtphPs(src x86.M128, k x86.Mmask8, a x86.M128i) x86.M128 {
	return x86.M128(maskCvtphPs([4]float32(src), uint8(k), [16]byte(a)))
}

func maskCvtphPs(src [4]float32, k uint8, a [16]byte) [4]float32


// MaskzCvtphPs: Convert packed half-precision (16-bit) floating-point elements
// in 'a' to packed single-precision (32-bit) floating-point elements, and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			m := j*16
//			IF k[j]
//				dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPH2PS'. Intrinsic: '_mm_maskz_cvtph_ps'.
// Requires AVX512F.
func MaskzCvtphPs(k x86.Mmask8, a x86.M128i) x86.M128 {
	return x86.M128(maskzCvtphPs(uint8(k), [16]byte(a)))
}

func maskzCvtphPs(k uint8, a [16]byte) [4]float32


// M256MaskCvtphPs: Convert packed half-precision (16-bit) floating-point
// elements in 'a' to packed single-precision (32-bit) floating-point elements,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			m := j*16
//			IF k[j]
//				dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPH2PS'. Intrinsic: '_mm256_mask_cvtph_ps'.
// Requires AVX512F.
func M256MaskCvtphPs(src x86.M256, k x86.Mmask8, a x86.M128i) x86.M256 {
	return x86.M256(m256MaskCvtphPs([8]float32(src), uint8(k), [16]byte(a)))
}

func m256MaskCvtphPs(src [8]float32, k uint8, a [16]byte) [8]float32


// M256MaskzCvtphPs: Convert packed half-precision (16-bit) floating-point
// elements in 'a' to packed single-precision (32-bit) floating-point elements,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			m := j*16
//			IF k[j]
//				dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPH2PS'. Intrinsic: '_mm256_maskz_cvtph_ps'.
// Requires AVX512F.
func M256MaskzCvtphPs(k x86.Mmask8, a x86.M128i) x86.M256 {
	return x86.M256(m256MaskzCvtphPs(uint8(k), [16]byte(a)))
}

func m256MaskzCvtphPs(k uint8, a [16]byte) [8]float32


// M512CvtphPs: Convert packed half-precision (16-bit) floating-point elements
// in 'a' to packed single-precision (32-bit) floating-point elements, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			m := j*16
//			dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPH2PS'. Intrinsic: '_mm512_cvtph_ps'.
// Requires AVX512F.
func M512CvtphPs(a x86.M256i) x86.M512 {
	return x86.M512(m512CvtphPs([32]byte(a)))
}

func m512CvtphPs(a [32]byte) [16]float32


// M512MaskCvtphPs: Convert packed half-precision (16-bit) floating-point
// elements in 'a' to packed single-precision (32-bit) floating-point elements,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			m := j*16
//			IF k[j]
//				dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPH2PS'. Intrinsic: '_mm512_mask_cvtph_ps'.
// Requires AVX512F.
func M512MaskCvtphPs(src x86.M512, k x86.Mmask16, a x86.M256i) x86.M512 {
	return x86.M512(m512MaskCvtphPs([16]float32(src), uint16(k), [32]byte(a)))
}

func m512MaskCvtphPs(src [16]float32, k uint16, a [32]byte) [16]float32


// M512MaskzCvtphPs: Convert packed half-precision (16-bit) floating-point
// elements in 'a' to packed single-precision (32-bit) floating-point elements,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			m := j*16
//			IF k[j]
//				dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPH2PS'. Intrinsic: '_mm512_maskz_cvtph_ps'.
// Requires AVX512F.
func M512MaskzCvtphPs(k x86.Mmask16, a x86.M256i) x86.M512 {
	return x86.M512(m512MaskzCvtphPs(uint16(k), [32]byte(a)))
}

func m512MaskzCvtphPs(k uint16, a [32]byte) [16]float32


// MaskCvtpsEpi32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPS2DQ'. Intrinsic: '_mm_mask_cvtps_epi32'.
// Requires AVX512F.
func MaskCvtpsEpi32(src x86.M128i, k x86.Mmask8, a x86.M128) x86.M128i {
	return x86.M128i(maskCvtpsEpi32([16]byte(src), uint8(k), [4]float32(a)))
}

func maskCvtpsEpi32(src [16]byte, k uint8, a [4]float32) [16]byte


// MaskzCvtpsEpi32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPS2DQ'. Intrinsic: '_mm_maskz_cvtps_epi32'.
// Requires AVX512F.
func MaskzCvtpsEpi32(k x86.Mmask8, a x86.M128) x86.M128i {
	return x86.M128i(maskzCvtpsEpi32(uint8(k), [4]float32(a)))
}

func maskzCvtpsEpi32(k uint8, a [4]float32) [16]byte


// M256MaskCvtpsEpi32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2DQ'. Intrinsic: '_mm256_mask_cvtps_epi32'.
// Requires AVX512F.
func M256MaskCvtpsEpi32(src x86.M256i, k x86.Mmask8, a x86.M256) x86.M256i {
	return x86.M256i(m256MaskCvtpsEpi32([32]byte(src), uint8(k), [8]float32(a)))
}

func m256MaskCvtpsEpi32(src [32]byte, k uint8, a [8]float32) [32]byte


// M256MaskzCvtpsEpi32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2DQ'. Intrinsic: '_mm256_maskz_cvtps_epi32'.
// Requires AVX512F.
func M256MaskzCvtpsEpi32(k x86.Mmask8, a x86.M256) x86.M256i {
	return x86.M256i(m256MaskzCvtpsEpi32(uint8(k), [8]float32(a)))
}

func m256MaskzCvtpsEpi32(k uint8, a [8]float32) [32]byte


// M512CvtpsEpi32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2DQ'. Intrinsic: '_mm512_cvtps_epi32'.
// Requires AVX512F.
func M512CvtpsEpi32(a x86.M512) x86.M512i {
	return x86.M512i(m512CvtpsEpi32([16]float32(a)))
}

func m512CvtpsEpi32(a [16]float32) [64]byte


// M512MaskCvtpsEpi32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2DQ'. Intrinsic: '_mm512_mask_cvtps_epi32'.
// Requires AVX512F.
func M512MaskCvtpsEpi32(src x86.M512i, k x86.Mmask16, a x86.M512) x86.M512i {
	return x86.M512i(m512MaskCvtpsEpi32([64]byte(src), uint16(k), [16]float32(a)))
}

func m512MaskCvtpsEpi32(src [64]byte, k uint16, a [16]float32) [64]byte


// M512MaskzCvtpsEpi32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2DQ'. Intrinsic: '_mm512_maskz_cvtps_epi32'.
// Requires AVX512F.
func M512MaskzCvtpsEpi32(k x86.Mmask16, a x86.M512) x86.M512i {
	return x86.M512i(m512MaskzCvtpsEpi32(uint16(k), [16]float32(a)))
}

func m512MaskzCvtpsEpi32(k uint16, a [16]float32) [64]byte


// CvtpsEpu32: Convert packed single-precision (32-bit) floating-point elements
// in 'a' to packed unsigned 32-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPS2UDQ'. Intrinsic: '_mm_cvtps_epu32'.
// Requires AVX512F.
func CvtpsEpu32(a x86.M128) x86.M128i {
	return x86.M128i(cvtpsEpu32([4]float32(a)))
}

func cvtpsEpu32(a [4]float32) [16]byte


// MaskCvtpsEpu32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPS2UDQ'. Intrinsic: '_mm_mask_cvtps_epu32'.
// Requires AVX512F.
func MaskCvtpsEpu32(src x86.M128i, k x86.Mmask8, a x86.M128) x86.M128i {
	return x86.M128i(maskCvtpsEpu32([16]byte(src), uint8(k), [4]float32(a)))
}

func maskCvtpsEpu32(src [16]byte, k uint8, a [4]float32) [16]byte


// MaskzCvtpsEpu32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPS2UDQ'. Intrinsic: '_mm_maskz_cvtps_epu32'.
// Requires AVX512F.
func MaskzCvtpsEpu32(k x86.Mmask8, a x86.M128) x86.M128i {
	return x86.M128i(maskzCvtpsEpu32(uint8(k), [4]float32(a)))
}

func maskzCvtpsEpu32(k uint8, a [4]float32) [16]byte


// M256CvtpsEpu32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2UDQ'. Intrinsic: '_mm256_cvtps_epu32'.
// Requires AVX512F.
func M256CvtpsEpu32(a x86.M256) x86.M256i {
	return x86.M256i(m256CvtpsEpu32([8]float32(a)))
}

func m256CvtpsEpu32(a [8]float32) [32]byte


// M256MaskCvtpsEpu32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2UDQ'. Intrinsic: '_mm256_mask_cvtps_epu32'.
// Requires AVX512F.
func M256MaskCvtpsEpu32(src x86.M256i, k x86.Mmask8, a x86.M256) x86.M256i {
	return x86.M256i(m256MaskCvtpsEpu32([32]byte(src), uint8(k), [8]float32(a)))
}

func m256MaskCvtpsEpu32(src [32]byte, k uint8, a [8]float32) [32]byte


// M256MaskzCvtpsEpu32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2UDQ'. Intrinsic: '_mm256_maskz_cvtps_epu32'.
// Requires AVX512F.
func M256MaskzCvtpsEpu32(k x86.Mmask8, a x86.M256) x86.M256i {
	return x86.M256i(m256MaskzCvtpsEpu32(uint8(k), [8]float32(a)))
}

func m256MaskzCvtpsEpu32(k uint8, a [8]float32) [32]byte


// M512CvtpsEpu32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2UDQ'. Intrinsic: '_mm512_cvtps_epu32'.
// Requires AVX512F.
func M512CvtpsEpu32(a x86.M512) x86.M512i {
	return x86.M512i(m512CvtpsEpu32([16]float32(a)))
}

func m512CvtpsEpu32(a [16]float32) [64]byte


// M512MaskCvtpsEpu32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2UDQ'. Intrinsic: '_mm512_mask_cvtps_epu32'.
// Requires AVX512F.
func M512MaskCvtpsEpu32(src x86.M512i, k x86.Mmask16, a x86.M512) x86.M512i {
	return x86.M512i(m512MaskCvtpsEpu32([64]byte(src), uint16(k), [16]float32(a)))
}

func m512MaskCvtpsEpu32(src [64]byte, k uint16, a [16]float32) [64]byte


// M512MaskzCvtpsEpu32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2UDQ'. Intrinsic: '_mm512_maskz_cvtps_epu32'.
// Requires AVX512F.
func M512MaskzCvtpsEpu32(k x86.Mmask16, a x86.M512) x86.M512i {
	return x86.M512i(m512MaskzCvtpsEpu32(uint16(k), [16]float32(a)))
}

func m512MaskzCvtpsEpu32(k uint16, a [16]float32) [64]byte


// M512CvtpsPd: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed double-precision (64-bit) floating-point elements,
// and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 32*j
//			dst[i+63:i] := Convert_FP32_To_FP64(a[k+31:k])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2PD'. Intrinsic: '_mm512_cvtps_pd'.
// Requires AVX512F.
func M512CvtpsPd(a x86.M256) x86.M512d {
	return x86.M512d(m512CvtpsPd([8]float32(a)))
}

func m512CvtpsPd(a [8]float32) [8]float64


// M512MaskCvtpsPd: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed double-precision (64-bit) floating-point elements,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_FP64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2PD'. Intrinsic: '_mm512_mask_cvtps_pd'.
// Requires AVX512F.
func M512MaskCvtpsPd(src x86.M512d, k x86.Mmask8, a x86.M256) x86.M512d {
	return x86.M512d(m512MaskCvtpsPd([8]float64(src), uint8(k), [8]float32(a)))
}

func m512MaskCvtpsPd(src [8]float64, k uint8, a [8]float32) [8]float64


// M512MaskzCvtpsPd: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed double-precision (64-bit) floating-point elements,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_FP64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2PD'. Intrinsic: '_mm512_maskz_cvtps_pd'.
// Requires AVX512F.
func M512MaskzCvtpsPd(k x86.Mmask8, a x86.M256) x86.M512d {
	return x86.M512d(m512MaskzCvtpsPd(uint8(k), [8]float32(a)))
}

func m512MaskzCvtpsPd(k uint8, a [8]float32) [8]float64


// MaskCvtpsPh: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed half-precision (16-bit) floating-point elements,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 3
//			i := 16*j
//			l := 32*j
//			IF k[j]
//				dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm_mask_cvtps_ph'.
// Requires AVX512F.
func MaskCvtpsPh(src x86.M128i, k x86.Mmask8, a x86.M128, rounding int) x86.M128i {
	return x86.M128i(maskCvtpsPh([16]byte(src), uint8(k), [4]float32(a), rounding))
}

func maskCvtpsPh(src [16]byte, k uint8, a [4]float32, rounding int) [16]byte


// MaskzCvtpsPh: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed half-precision (16-bit) floating-point elements,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 3
//			i := 16*j
//			l := 32*j
//			IF k[j]
//				dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm_maskz_cvtps_ph'.
// Requires AVX512F.
func MaskzCvtpsPh(k x86.Mmask8, a x86.M128, rounding int) x86.M128i {
	return x86.M128i(maskzCvtpsPh(uint8(k), [4]float32(a), rounding))
}

func maskzCvtpsPh(k uint8, a [4]float32, rounding int) [16]byte


// M256MaskCvtpsPh: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed half-precision (16-bit) floating-point elements,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := 16*j
//			l := 32*j
//			IF k[j]
//				dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm256_mask_cvtps_ph'.
// Requires AVX512F.
func M256MaskCvtpsPh(src x86.M128i, k x86.Mmask8, a x86.M256, rounding int) x86.M128i {
	return x86.M128i(m256MaskCvtpsPh([16]byte(src), uint8(k), [8]float32(a), rounding))
}

func m256MaskCvtpsPh(src [16]byte, k uint8, a [8]float32, rounding int) [16]byte


// M256MaskzCvtpsPh: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed half-precision (16-bit) floating-point elements,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := 16*j
//			l := 32*j
//			IF k[j]
//				dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm256_maskz_cvtps_ph'.
// Requires AVX512F.
func M256MaskzCvtpsPh(k x86.Mmask8, a x86.M256, rounding int) x86.M128i {
	return x86.M128i(m256MaskzCvtpsPh(uint8(k), [8]float32(a), rounding))
}

func m256MaskzCvtpsPh(k uint8, a [8]float32, rounding int) [16]byte


// M512CvtpsPh: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed half-precision (16-bit) floating-point elements,
// and store the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 16*j
//			l := 32*j
//			dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm512_cvtps_ph'.
// Requires AVX512F.
func M512CvtpsPh(a x86.M512, rounding int) x86.M256i {
	return x86.M256i(m512CvtpsPh([16]float32(a), rounding))
}

func m512CvtpsPh(a [16]float32, rounding int) [32]byte


// M512MaskCvtpsPh: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed half-precision (16-bit) floating-point elements,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 16*j
//			l := 32*j
//			IF k[j]
//				dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm512_mask_cvtps_ph'.
// Requires AVX512F.
func M512MaskCvtpsPh(src x86.M256i, k x86.Mmask16, a x86.M512, rounding int) x86.M256i {
	return x86.M256i(m512MaskCvtpsPh([32]byte(src), uint16(k), [16]float32(a), rounding))
}

func m512MaskCvtpsPh(src [32]byte, k uint16, a [16]float32, rounding int) [32]byte


// M512MaskzCvtpsPh: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed half-precision (16-bit) floating-point elements,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 16*j
//			l := 32*j
//			IF k[j]
//				dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm512_maskz_cvtps_ph'.
// Requires AVX512F.
func M512MaskzCvtpsPh(k x86.Mmask16, a x86.M512, rounding int) x86.M256i {
	return x86.M256i(m512MaskzCvtpsPh(uint16(k), [16]float32(a), rounding))
}

func m512MaskzCvtpsPh(k uint16, a [16]float32, rounding int) [32]byte


// CvtsdI32: Convert the lower double-precision (64-bit) floating-point element
// in 'a' to a 32-bit integer, and store the result in 'dst'. 
//
//		dst[31:0] := Convert_FP64_To_Int32(a[63:0])
//
// Instruction: 'VCVTSD2SI'. Intrinsic: '_mm_cvtsd_i32'.
// Requires AVX512F.
func CvtsdI32(a x86.M128d) int {
	return int(cvtsdI32([2]float64(a)))
}

func cvtsdI32(a [2]float64) int


// CvtsdI64: Convert the lower double-precision (64-bit) floating-point element
// in 'a' to a 64-bit integer, and store the result in 'dst'. 
//
//		dst[63:0] := Convert_FP64_To_Int64(a[63:0])
//
// Instruction: 'VCVTSD2SI'. Intrinsic: '_mm_cvtsd_i64'.
// Requires AVX512F.
func CvtsdI64(a x86.M128d) int64 {
	return int64(cvtsdI64([2]float64(a)))
}

func cvtsdI64(a [2]float64) int64


// MaskCvtsdSs: Convert the lower double-precision (64-bit) floating-point
// element in 'b' to a single-precision (32-bit) floating-point element, store
// the result in the lower element of 'dst' using writemask 'k' (the element is
// copied from 'src' when mask bit 0 is not set), and copy the upper element
// from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := Convert_FP64_To_FP32(b[63:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:31]
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTSD2SS'. Intrinsic: '_mm_mask_cvtsd_ss'.
// Requires AVX512F.
func MaskCvtsdSs(src x86.M128, k x86.Mmask8, a x86.M128, b x86.M128d) x86.M128 {
	return x86.M128(maskCvtsdSs([4]float32(src), uint8(k), [4]float32(a), [2]float64(b)))
}

func maskCvtsdSs(src [4]float32, k uint8, a [4]float32, b [2]float64) [4]float32


// MaskzCvtsdSs: Convert the lower double-precision (64-bit) floating-point
// element in 'b' to a single-precision (32-bit) floating-point element, store
// the result in the lower element of 'dst' using zeromask 'k' (the element is
// zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements
// from 'a' to the upper elements of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := Convert_FP64_To_FP32(b[63:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:31]
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTSD2SS'. Intrinsic: '_mm_maskz_cvtsd_ss'.
// Requires AVX512F.
func MaskzCvtsdSs(k x86.Mmask8, a x86.M128, b x86.M128d) x86.M128 {
	return x86.M128(maskzCvtsdSs(uint8(k), [4]float32(a), [2]float64(b)))
}

func maskzCvtsdSs(k uint8, a [4]float32, b [2]float64) [4]float32


// CvtsdU32: Convert the lower double-precision (64-bit) floating-point element
// in 'a' to an unsigned 32-bit integer, and store the result in 'dst'. 
//
//		dst[31:0] := Convert_FP64_To_UnsignedInt32(a[63:0])
//
// Instruction: 'VCVTSD2USI'. Intrinsic: '_mm_cvtsd_u32'.
// Requires AVX512F.
func CvtsdU32(a x86.M128d) uint32 {
	return uint32(cvtsdU32([2]float64(a)))
}

func cvtsdU32(a [2]float64) uint32


// CvtsdU64: Convert the lower double-precision (64-bit) floating-point element
// in 'a' to an unsigned 64-bit integer, and store the result in 'dst'. 
//
//		dst[63:0] := Convert_FP64_To_UnsignedInt64(a[63:0])
//
// Instruction: 'VCVTSD2USI'. Intrinsic: '_mm_cvtsd_u64'.
// Requires AVX512F.
func CvtsdU64(a x86.M128d) uint64 {
	return uint64(cvtsdU64([2]float64(a)))
}

func cvtsdU64(a [2]float64) uint64


// Cvtsepi32Epi16: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			k := 16*j
//			dst[k+15:k] := Saturate_Int32_To_Int16(a[i+31:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSDW'. Intrinsic: '_mm_cvtsepi32_epi16'.
// Requires AVX512F.
func Cvtsepi32Epi16(a x86.M128i) x86.M128i {
	return x86.M128i(cvtsepi32Epi16([16]byte(a)))
}

func cvtsepi32Epi16(a [16]byte) [16]byte


// MaskCvtsepi32Epi16: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_Int32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSDW'. Intrinsic: '_mm_mask_cvtsepi32_epi16'.
// Requires AVX512F.
func MaskCvtsepi32Epi16(src x86.M128i, k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskCvtsepi32Epi16([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtsepi32Epi16(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtsepi32Epi16: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_Int32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSDW'. Intrinsic: '_mm_maskz_cvtsepi32_epi16'.
// Requires AVX512F.
func MaskzCvtsepi32Epi16(k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskzCvtsepi32Epi16(uint8(k), [16]byte(a)))
}

func maskzCvtsepi32Epi16(k uint8, a [16]byte) [16]byte


// M256Cvtsepi32Epi16: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 16*j
//			dst[k+15:k] := Saturate_Int32_To_Int16(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSDW'. Intrinsic: '_mm256_cvtsepi32_epi16'.
// Requires AVX512F.
func M256Cvtsepi32Epi16(a x86.M256i) x86.M128i {
	return x86.M128i(m256Cvtsepi32Epi16([32]byte(a)))
}

func m256Cvtsepi32Epi16(a [32]byte) [16]byte


// M256MaskCvtsepi32Epi16: Convert packed 32-bit integers in 'a' to packed
// 16-bit integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_Int32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSDW'. Intrinsic: '_mm256_mask_cvtsepi32_epi16'.
// Requires AVX512F.
func M256MaskCvtsepi32Epi16(src x86.M128i, k x86.Mmask8, a x86.M256i) x86.M128i {
	return x86.M128i(m256MaskCvtsepi32Epi16([16]byte(src), uint8(k), [32]byte(a)))
}

func m256MaskCvtsepi32Epi16(src [16]byte, k uint8, a [32]byte) [16]byte


// M256MaskzCvtsepi32Epi16: Convert packed 32-bit integers in 'a' to packed
// 16-bit integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_Int32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSDW'. Intrinsic: '_mm256_maskz_cvtsepi32_epi16'.
// Requires AVX512F.
func M256MaskzCvtsepi32Epi16(k x86.Mmask8, a x86.M256i) x86.M128i {
	return x86.M128i(m256MaskzCvtsepi32Epi16(uint8(k), [32]byte(a)))
}

func m256MaskzCvtsepi32Epi16(k uint8, a [32]byte) [16]byte


// M512Cvtsepi32Epi16: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			k := 16*j
//			dst[k+15:k] := Saturate_Int32_To_Int16(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSDW'. Intrinsic: '_mm512_cvtsepi32_epi16'.
// Requires AVX512F.
func M512Cvtsepi32Epi16(a x86.M512i) x86.M256i {
	return x86.M256i(m512Cvtsepi32Epi16([64]byte(a)))
}

func m512Cvtsepi32Epi16(a [64]byte) [32]byte


// M512MaskCvtsepi32Epi16: Convert packed 32-bit integers in 'a' to packed
// 16-bit integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_Int32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSDW'. Intrinsic: '_mm512_mask_cvtsepi32_epi16'.
// Requires AVX512F.
func M512MaskCvtsepi32Epi16(src x86.M256i, k x86.Mmask16, a x86.M512i) x86.M256i {
	return x86.M256i(m512MaskCvtsepi32Epi16([32]byte(src), uint16(k), [64]byte(a)))
}

func m512MaskCvtsepi32Epi16(src [32]byte, k uint16, a [64]byte) [32]byte


// M512MaskzCvtsepi32Epi16: Convert packed 32-bit integers in 'a' to packed
// 16-bit integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_Int32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSDW'. Intrinsic: '_mm512_maskz_cvtsepi32_epi16'.
// Requires AVX512F.
func M512MaskzCvtsepi32Epi16(k x86.Mmask16, a x86.M512i) x86.M256i {
	return x86.M256i(m512MaskzCvtsepi32Epi16(uint16(k), [64]byte(a)))
}

func m512MaskzCvtsepi32Epi16(k uint16, a [64]byte) [32]byte


// Cvtsepi32Epi8: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			k := 8*j
//			dst[k+7:k] := Saturate_Int32_To_Int8(a[i+31:i])
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVSDB'. Intrinsic: '_mm_cvtsepi32_epi8'.
// Requires AVX512F.
func Cvtsepi32Epi8(a x86.M128i) x86.M128i {
	return x86.M128i(cvtsepi32Epi8([16]byte(a)))
}

func cvtsepi32Epi8(a [16]byte) [16]byte


// MaskCvtsepi32Epi8: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVSDB'. Intrinsic: '_mm_mask_cvtsepi32_epi8'.
// Requires AVX512F.
func MaskCvtsepi32Epi8(src x86.M128i, k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskCvtsepi32Epi8([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtsepi32Epi8(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtsepi32Epi8: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVSDB'. Intrinsic: '_mm_maskz_cvtsepi32_epi8'.
// Requires AVX512F.
func MaskzCvtsepi32Epi8(k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskzCvtsepi32Epi8(uint8(k), [16]byte(a)))
}

func maskzCvtsepi32Epi8(k uint8, a [16]byte) [16]byte


// M256Cvtsepi32Epi8: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 8*j
//			dst[k+7:k] := Saturate_Int32_To_Int8(a[i+31:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSDB'. Intrinsic: '_mm256_cvtsepi32_epi8'.
// Requires AVX512F.
func M256Cvtsepi32Epi8(a x86.M256i) x86.M128i {
	return x86.M128i(m256Cvtsepi32Epi8([32]byte(a)))
}

func m256Cvtsepi32Epi8(a [32]byte) [16]byte


// M256MaskCvtsepi32Epi8: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSDB'. Intrinsic: '_mm256_mask_cvtsepi32_epi8'.
// Requires AVX512F.
func M256MaskCvtsepi32Epi8(src x86.M128i, k x86.Mmask8, a x86.M256i) x86.M128i {
	return x86.M128i(m256MaskCvtsepi32Epi8([16]byte(src), uint8(k), [32]byte(a)))
}

func m256MaskCvtsepi32Epi8(src [16]byte, k uint8, a [32]byte) [16]byte


// M256MaskzCvtsepi32Epi8: Convert packed 32-bit integers in 'a' to packed
// 8-bit integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSDB'. Intrinsic: '_mm256_maskz_cvtsepi32_epi8'.
// Requires AVX512F.
func M256MaskzCvtsepi32Epi8(k x86.Mmask8, a x86.M256i) x86.M128i {
	return x86.M128i(m256MaskzCvtsepi32Epi8(uint8(k), [32]byte(a)))
}

func m256MaskzCvtsepi32Epi8(k uint8, a [32]byte) [16]byte


// M512Cvtsepi32Epi8: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			k := 8*j
//			dst[k+7:k] := Saturate_Int32_To_Int8(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSDB'. Intrinsic: '_mm512_cvtsepi32_epi8'.
// Requires AVX512F.
func M512Cvtsepi32Epi8(a x86.M512i) x86.M128i {
	return x86.M128i(m512Cvtsepi32Epi8([64]byte(a)))
}

func m512Cvtsepi32Epi8(a [64]byte) [16]byte


// M512MaskCvtsepi32Epi8: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSDB'. Intrinsic: '_mm512_mask_cvtsepi32_epi8'.
// Requires AVX512F.
func M512MaskCvtsepi32Epi8(src x86.M128i, k x86.Mmask16, a x86.M512i) x86.M128i {
	return x86.M128i(m512MaskCvtsepi32Epi8([16]byte(src), uint16(k), [64]byte(a)))
}

func m512MaskCvtsepi32Epi8(src [16]byte, k uint16, a [64]byte) [16]byte


// M512MaskzCvtsepi32Epi8: Convert packed 32-bit integers in 'a' to packed
// 8-bit integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSDB'. Intrinsic: '_mm512_maskz_cvtsepi32_epi8'.
// Requires AVX512F.
func M512MaskzCvtsepi32Epi8(k x86.Mmask16, a x86.M512i) x86.M128i {
	return x86.M128i(m512MaskzCvtsepi32Epi8(uint16(k), [64]byte(a)))
}

func m512MaskzCvtsepi32Epi8(k uint16, a [64]byte) [16]byte


// Skipped: _mm_mask_cvtsepi32_storeu_epi16. Contains pointer parameter.


// Skipped: _mm256_mask_cvtsepi32_storeu_epi16. Contains pointer parameter.


// Skipped: _mm512_mask_cvtsepi32_storeu_epi16. Contains pointer parameter.


// Skipped: _mm_mask_cvtsepi32_storeu_epi8. Contains pointer parameter.


// Skipped: _mm256_mask_cvtsepi32_storeu_epi8. Contains pointer parameter.


// Skipped: _mm512_mask_cvtsepi32_storeu_epi8. Contains pointer parameter.


// Cvtsepi64Epi16: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			k := 16*j
//			dst[k+15:k] := Saturate_Int64_To_Int16(a[i+63:i])
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVSQW'. Intrinsic: '_mm_cvtsepi64_epi16'.
// Requires AVX512F.
func Cvtsepi64Epi16(a x86.M128i) x86.M128i {
	return x86.M128i(cvtsepi64Epi16([16]byte(a)))
}

func cvtsepi64Epi16(a [16]byte) [16]byte


// MaskCvtsepi64Epi16: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_Int64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVSQW'. Intrinsic: '_mm_mask_cvtsepi64_epi16'.
// Requires AVX512F.
func MaskCvtsepi64Epi16(src x86.M128i, k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskCvtsepi64Epi16([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtsepi64Epi16(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtsepi64Epi16: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_Int64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVSQW'. Intrinsic: '_mm_maskz_cvtsepi64_epi16'.
// Requires AVX512F.
func MaskzCvtsepi64Epi16(k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskzCvtsepi64Epi16(uint8(k), [16]byte(a)))
}

func maskzCvtsepi64Epi16(k uint8, a [16]byte) [16]byte


// M256Cvtsepi64Epi16: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			k := 16*j
//			dst[k+15:k] := Saturate_Int64_To_Int16(a[i+63:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSQW'. Intrinsic: '_mm256_cvtsepi64_epi16'.
// Requires AVX512F.
func M256Cvtsepi64Epi16(a x86.M256i) x86.M128i {
	return x86.M128i(m256Cvtsepi64Epi16([32]byte(a)))
}

func m256Cvtsepi64Epi16(a [32]byte) [16]byte


// M256MaskCvtsepi64Epi16: Convert packed 64-bit integers in 'a' to packed
// 16-bit integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_Int64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSQW'. Intrinsic: '_mm256_mask_cvtsepi64_epi16'.
// Requires AVX512F.
func M256MaskCvtsepi64Epi16(src x86.M128i, k x86.Mmask8, a x86.M256i) x86.M128i {
	return x86.M128i(m256MaskCvtsepi64Epi16([16]byte(src), uint8(k), [32]byte(a)))
}

func m256MaskCvtsepi64Epi16(src [16]byte, k uint8, a [32]byte) [16]byte


// M256MaskzCvtsepi64Epi16: Convert packed 64-bit integers in 'a' to packed
// 16-bit integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_Int64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSQW'. Intrinsic: '_mm256_maskz_cvtsepi64_epi16'.
// Requires AVX512F.
func M256MaskzCvtsepi64Epi16(k x86.Mmask8, a x86.M256i) x86.M128i {
	return x86.M128i(m256MaskzCvtsepi64Epi16(uint8(k), [32]byte(a)))
}

func m256MaskzCvtsepi64Epi16(k uint8, a [32]byte) [16]byte


// M512Cvtsepi64Epi16: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 16*j
//			dst[k+15:k] := Saturate_Int64_To_Int16(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSQW'. Intrinsic: '_mm512_cvtsepi64_epi16'.
// Requires AVX512F.
func M512Cvtsepi64Epi16(a x86.M512i) x86.M128i {
	return x86.M128i(m512Cvtsepi64Epi16([64]byte(a)))
}

func m512Cvtsepi64Epi16(a [64]byte) [16]byte


// M512MaskCvtsepi64Epi16: Convert packed 64-bit integers in 'a' to packed
// 16-bit integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_Int64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSQW'. Intrinsic: '_mm512_mask_cvtsepi64_epi16'.
// Requires AVX512F.
func M512MaskCvtsepi64Epi16(src x86.M128i, k x86.Mmask8, a x86.M512i) x86.M128i {
	return x86.M128i(m512MaskCvtsepi64Epi16([16]byte(src), uint8(k), [64]byte(a)))
}

func m512MaskCvtsepi64Epi16(src [16]byte, k uint8, a [64]byte) [16]byte


// M512MaskzCvtsepi64Epi16: Convert packed 64-bit integers in 'a' to packed
// 16-bit integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_Int64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSQW'. Intrinsic: '_mm512_maskz_cvtsepi64_epi16'.
// Requires AVX512F.
func M512MaskzCvtsepi64Epi16(k x86.Mmask8, a x86.M512i) x86.M128i {
	return x86.M128i(m512MaskzCvtsepi64Epi16(uint8(k), [64]byte(a)))
}

func m512MaskzCvtsepi64Epi16(k uint8, a [64]byte) [16]byte


// Cvtsepi64Epi32: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			k := 32*j
//			dst[k+31:k] := Saturate_Int64_To_Int32(a[i+63:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSQD'. Intrinsic: '_mm_cvtsepi64_epi32'.
// Requires AVX512F.
func Cvtsepi64Epi32(a x86.M128i) x86.M128i {
	return x86.M128i(cvtsepi64Epi32([16]byte(a)))
}

func cvtsepi64Epi32(a [16]byte) [16]byte


// MaskCvtsepi64Epi32: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Saturate_Int64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := src[l+31:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSQD'. Intrinsic: '_mm_mask_cvtsepi64_epi32'.
// Requires AVX512F.
func MaskCvtsepi64Epi32(src x86.M128i, k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskCvtsepi64Epi32([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtsepi64Epi32(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtsepi64Epi32: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Saturate_Int64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSQD'. Intrinsic: '_mm_maskz_cvtsepi64_epi32'.
// Requires AVX512F.
func MaskzCvtsepi64Epi32(k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskzCvtsepi64Epi32(uint8(k), [16]byte(a)))
}

func maskzCvtsepi64Epi32(k uint8, a [16]byte) [16]byte


// M256Cvtsepi64Epi32: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			k := 32*j
//			dst[k+31:k] := Saturate_Int64_To_Int32(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSQD'. Intrinsic: '_mm256_cvtsepi64_epi32'.
// Requires AVX512F.
func M256Cvtsepi64Epi32(a x86.M256i) x86.M128i {
	return x86.M128i(m256Cvtsepi64Epi32([32]byte(a)))
}

func m256Cvtsepi64Epi32(a [32]byte) [16]byte


// M256MaskCvtsepi64Epi32: Convert packed 64-bit integers in 'a' to packed
// 32-bit integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Saturate_Int64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := src[l+31:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSQD'. Intrinsic: '_mm256_mask_cvtsepi64_epi32'.
// Requires AVX512F.
func M256MaskCvtsepi64Epi32(src x86.M128i, k x86.Mmask8, a x86.M256i) x86.M128i {
	return x86.M128i(m256MaskCvtsepi64Epi32([16]byte(src), uint8(k), [32]byte(a)))
}

func m256MaskCvtsepi64Epi32(src [16]byte, k uint8, a [32]byte) [16]byte


// M256MaskzCvtsepi64Epi32: Convert packed 64-bit integers in 'a' to packed
// 32-bit integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Saturate_Int64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSQD'. Intrinsic: '_mm256_maskz_cvtsepi64_epi32'.
// Requires AVX512F.
func M256MaskzCvtsepi64Epi32(k x86.Mmask8, a x86.M256i) x86.M128i {
	return x86.M128i(m256MaskzCvtsepi64Epi32(uint8(k), [32]byte(a)))
}

func m256MaskzCvtsepi64Epi32(k uint8, a [32]byte) [16]byte


// M512Cvtsepi64Epi32: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 32*j
//			dst[k+31:k] := Saturate_Int64_To_Int32(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSQD'. Intrinsic: '_mm512_cvtsepi64_epi32'.
// Requires AVX512F.
func M512Cvtsepi64Epi32(a x86.M512i) x86.M256i {
	return x86.M256i(m512Cvtsepi64Epi32([64]byte(a)))
}

func m512Cvtsepi64Epi32(a [64]byte) [32]byte


// M512MaskCvtsepi64Epi32: Convert packed 64-bit integers in 'a' to packed
// 32-bit integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Saturate_Int64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := src[l+31:l]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSQD'. Intrinsic: '_mm512_mask_cvtsepi64_epi32'.
// Requires AVX512F.
func M512MaskCvtsepi64Epi32(src x86.M256i, k x86.Mmask8, a x86.M512i) x86.M256i {
	return x86.M256i(m512MaskCvtsepi64Epi32([32]byte(src), uint8(k), [64]byte(a)))
}

func m512MaskCvtsepi64Epi32(src [32]byte, k uint8, a [64]byte) [32]byte


// M512MaskzCvtsepi64Epi32: Convert packed 64-bit integers in 'a' to packed
// 32-bit integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Saturate_Int64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSQD'. Intrinsic: '_mm512_maskz_cvtsepi64_epi32'.
// Requires AVX512F.
func M512MaskzCvtsepi64Epi32(k x86.Mmask8, a x86.M512i) x86.M256i {
	return x86.M256i(m512MaskzCvtsepi64Epi32(uint8(k), [64]byte(a)))
}

func m512MaskzCvtsepi64Epi32(k uint8, a [64]byte) [32]byte


// Cvtsepi64Epi8: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			k := 8*j
//			dst[k+7:k] := Saturate_Int64_To_Int8(a[i+63:i])
//		ENDFOR
//		dst[MAX:16] := 0
//
// Instruction: 'VPMOVSQB'. Intrinsic: '_mm_cvtsepi64_epi8'.
// Requires AVX512F.
func Cvtsepi64Epi8(a x86.M128i) x86.M128i {
	return x86.M128i(cvtsepi64Epi8([16]byte(a)))
}

func cvtsepi64Epi8(a [16]byte) [16]byte


// MaskCvtsepi64Epi8: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:16] := 0
//
// Instruction: 'VPMOVSQB'. Intrinsic: '_mm_mask_cvtsepi64_epi8'.
// Requires AVX512F.
func MaskCvtsepi64Epi8(src x86.M128i, k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskCvtsepi64Epi8([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtsepi64Epi8(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtsepi64Epi8: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:16] := 0
//
// Instruction: 'VPMOVSQB'. Intrinsic: '_mm_maskz_cvtsepi64_epi8'.
// Requires AVX512F.
func MaskzCvtsepi64Epi8(k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskzCvtsepi64Epi8(uint8(k), [16]byte(a)))
}

func maskzCvtsepi64Epi8(k uint8, a [16]byte) [16]byte


// M256Cvtsepi64Epi8: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			k := 8*j
//			dst[k+7:k] := Saturate_Int64_To_Int8(a[i+63:i])
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVSQB'. Intrinsic: '_mm256_cvtsepi64_epi8'.
// Requires AVX512F.
func M256Cvtsepi64Epi8(a x86.M256i) x86.M128i {
	return x86.M128i(m256Cvtsepi64Epi8([32]byte(a)))
}

func m256Cvtsepi64Epi8(a [32]byte) [16]byte


// M256MaskCvtsepi64Epi8: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVSQB'. Intrinsic: '_mm256_mask_cvtsepi64_epi8'.
// Requires AVX512F.
func M256MaskCvtsepi64Epi8(src x86.M128i, k x86.Mmask8, a x86.M256i) x86.M128i {
	return x86.M128i(m256MaskCvtsepi64Epi8([16]byte(src), uint8(k), [32]byte(a)))
}

func m256MaskCvtsepi64Epi8(src [16]byte, k uint8, a [32]byte) [16]byte


// M256MaskzCvtsepi64Epi8: Convert packed 64-bit integers in 'a' to packed
// 8-bit integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVSQB'. Intrinsic: '_mm256_maskz_cvtsepi64_epi8'.
// Requires AVX512F.
func M256MaskzCvtsepi64Epi8(k x86.Mmask8, a x86.M256i) x86.M128i {
	return x86.M128i(m256MaskzCvtsepi64Epi8(uint8(k), [32]byte(a)))
}

func m256MaskzCvtsepi64Epi8(k uint8, a [32]byte) [16]byte


// M512Cvtsepi64Epi8: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 8*j
//			dst[k+7:k] := Saturate_Int64_To_Int8(a[i+63:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSQB'. Intrinsic: '_mm512_cvtsepi64_epi8'.
// Requires AVX512F.
func M512Cvtsepi64Epi8(a x86.M512i) x86.M128i {
	return x86.M128i(m512Cvtsepi64Epi8([64]byte(a)))
}

func m512Cvtsepi64Epi8(a [64]byte) [16]byte


// M512MaskCvtsepi64Epi8: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSQB'. Intrinsic: '_mm512_mask_cvtsepi64_epi8'.
// Requires AVX512F.
func M512MaskCvtsepi64Epi8(src x86.M128i, k x86.Mmask8, a x86.M512i) x86.M128i {
	return x86.M128i(m512MaskCvtsepi64Epi8([16]byte(src), uint8(k), [64]byte(a)))
}

func m512MaskCvtsepi64Epi8(src [16]byte, k uint8, a [64]byte) [16]byte


// M512MaskzCvtsepi64Epi8: Convert packed 64-bit integers in 'a' to packed
// 8-bit integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSQB'. Intrinsic: '_mm512_maskz_cvtsepi64_epi8'.
// Requires AVX512F.
func M512MaskzCvtsepi64Epi8(k x86.Mmask8, a x86.M512i) x86.M128i {
	return x86.M128i(m512MaskzCvtsepi64Epi8(uint8(k), [64]byte(a)))
}

func m512MaskzCvtsepi64Epi8(k uint8, a [64]byte) [16]byte


// Skipped: _mm_mask_cvtsepi64_storeu_epi16. Contains pointer parameter.


// Skipped: _mm256_mask_cvtsepi64_storeu_epi16. Contains pointer parameter.


// Skipped: _mm512_mask_cvtsepi64_storeu_epi16. Contains pointer parameter.


// Skipped: _mm_mask_cvtsepi64_storeu_epi32. Contains pointer parameter.


// Skipped: _mm256_mask_cvtsepi64_storeu_epi32. Contains pointer parameter.


// Skipped: _mm512_mask_cvtsepi64_storeu_epi32. Contains pointer parameter.


// Skipped: _mm_mask_cvtsepi64_storeu_epi8. Contains pointer parameter.


// Skipped: _mm256_mask_cvtsepi64_storeu_epi8. Contains pointer parameter.


// Skipped: _mm512_mask_cvtsepi64_storeu_epi8. Contains pointer parameter.


// CvtssI32: Convert the lower single-precision (32-bit) floating-point element
// in 'a' to a 32-bit integer, and store the result in 'dst'. 
//
//		dst[31:0] := Convert_FP32_To_Int32(a[31:0])
//
// Instruction: 'VCVTSS2SI'. Intrinsic: '_mm_cvtss_i32'.
// Requires AVX512F.
func CvtssI32(a x86.M128) int {
	return int(cvtssI32([4]float32(a)))
}

func cvtssI32(a [4]float32) int


// CvtssI64: Convert the lower single-precision (32-bit) floating-point element
// in 'a' to a 64-bit integer, and store the result in 'dst'. 
//
//		dst[63:0] := Convert_FP32_To_Int64(a[31:0])
//
// Instruction: 'VCVTSS2SI'. Intrinsic: '_mm_cvtss_i64'.
// Requires AVX512F.
func CvtssI64(a x86.M128) int64 {
	return int64(cvtssI64([4]float32(a)))
}

func cvtssI64(a [4]float32) int64


// MaskCvtssSd: Convert the lower single-precision (32-bit) floating-point
// element in 'b' to a double-precision (64-bit) floating-point element, store
// the result in the lower element of 'dst' using writemask 'k' (the element is
// copied from 'src' when mask bit 0 is not set), and copy the upper element
// from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := Convert_FP32_To_FP64(b[31:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTSS2SD'. Intrinsic: '_mm_mask_cvtss_sd'.
// Requires AVX512F.
func MaskCvtssSd(src x86.M128d, k x86.Mmask8, a x86.M128d, b x86.M128) x86.M128d {
	return x86.M128d(maskCvtssSd([2]float64(src), uint8(k), [2]float64(a), [4]float32(b)))
}

func maskCvtssSd(src [2]float64, k uint8, a [2]float64, b [4]float32) [2]float64


// MaskzCvtssSd: Convert the lower single-precision (32-bit) floating-point
// element in 'b' to a double-precision (64-bit) floating-point element, store
// the result in the lower element of 'dst' using zeromask 'k' (the element is
// zeroed out when mask bit 0 is not set), and copy the upper element from 'a'
// to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := Convert_FP32_To_FP64(b[31:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTSS2SD'. Intrinsic: '_mm_maskz_cvtss_sd'.
// Requires AVX512F.
func MaskzCvtssSd(k x86.Mmask8, a x86.M128d, b x86.M128) x86.M128d {
	return x86.M128d(maskzCvtssSd(uint8(k), [2]float64(a), [4]float32(b)))
}

func maskzCvtssSd(k uint8, a [2]float64, b [4]float32) [2]float64


// CvtssU32: Convert the lower single-precision (32-bit) floating-point element
// in 'a' to an unsigned 32-bit integer, and store the result in 'dst'. 
//
//		dst[31:0] := Convert_FP32_To_UnsignedInt32(a[31:0])
//
// Instruction: 'VCVTSS2USI'. Intrinsic: '_mm_cvtss_u32'.
// Requires AVX512F.
func CvtssU32(a x86.M128) uint32 {
	return uint32(cvtssU32([4]float32(a)))
}

func cvtssU32(a [4]float32) uint32


// CvtssU64: Convert the lower single-precision (32-bit) floating-point element
// in 'a' to an unsigned 64-bit integer, and store the result in 'dst'. 
//
//		dst[63:0] := Convert_FP32_To_UnsignedInt64(a[31:0])
//
// Instruction: 'VCVTSS2USI'. Intrinsic: '_mm_cvtss_u64'.
// Requires AVX512F.
func CvtssU64(a x86.M128) uint64 {
	return uint64(cvtssU64([4]float32(a)))
}

func cvtssU64(a [4]float32) uint64


// M512CvttRoundpdEpi32: Convert packed double-precision (64-bit)
// floating-point elements in 'a' to packed 32-bit integers with truncation,
// and store the results in 'dst'. 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := 32*i
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_IntegerTruncate(a[k+63:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2DQ'. Intrinsic: '_mm512_cvtt_roundpd_epi32'.
// Requires AVX512F.
func M512CvttRoundpdEpi32(a x86.M512d, sae int) x86.M256i {
	return x86.M256i(m512CvttRoundpdEpi32([8]float64(a), sae))
}

func m512CvttRoundpdEpi32(a [8]float64, sae int) [32]byte


// M512MaskCvttRoundpdEpi32: Convert packed double-precision (64-bit)
// floating-point elements in 'a' to packed 32-bit integers with truncation,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set).  Pass __MM_FROUND_NO_EXC
// to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := 32*i
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_IntegerTruncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2DQ'. Intrinsic: '_mm512_mask_cvtt_roundpd_epi32'.
// Requires AVX512F.
func M512MaskCvttRoundpdEpi32(src x86.M256i, k x86.Mmask8, a x86.M512d, sae int) x86.M256i {
	return x86.M256i(m512MaskCvttRoundpdEpi32([32]byte(src), uint8(k), [8]float64(a), sae))
}

func m512MaskCvttRoundpdEpi32(src [32]byte, k uint8, a [8]float64, sae int) [32]byte


// M512MaskzCvttRoundpdEpi32: Convert packed double-precision (64-bit)
// floating-point elements in 'a' to packed 32-bit integers with truncation,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := 32*i
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_IntegerTruncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2DQ'. Intrinsic: '_mm512_maskz_cvtt_roundpd_epi32'.
// Requires AVX512F.
func M512MaskzCvttRoundpdEpi32(k x86.Mmask8, a x86.M512d, sae int) x86.M256i {
	return x86.M256i(m512MaskzCvttRoundpdEpi32(uint8(k), [8]float64(a), sae))
}

func m512MaskzCvttRoundpdEpi32(k uint8, a [8]float64, sae int) [32]byte


// M512CvttRoundpdEpu32: Convert packed double-precision (64-bit)
// floating-point elements in 'a' to packed unsigned 32-bit integers with
// truncation, and store the results in 'dst'. 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := 32*i
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_UnsignedIntegerTruncate(a[k+63:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2UDQ'. Intrinsic: '_mm512_cvtt_roundpd_epu32'.
// Requires AVX512F.
func M512CvttRoundpdEpu32(a x86.M512d, sae int) x86.M256i {
	return x86.M256i(m512CvttRoundpdEpu32([8]float64(a), sae))
}

func m512CvttRoundpdEpu32(a [8]float64, sae int) [32]byte


// M512MaskCvttRoundpdEpu32: Convert packed double-precision (64-bit)
// floating-point elements in 'a' to packed unsigned 32-bit integers with
// truncation, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set).  
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := 32*i
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedIntegerTruncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2UDQ'. Intrinsic: '_mm512_mask_cvtt_roundpd_epu32'.
// Requires AVX512F.
func M512MaskCvttRoundpdEpu32(src x86.M256i, k x86.Mmask8, a x86.M512d, sae int) x86.M256i {
	return x86.M256i(m512MaskCvttRoundpdEpu32([32]byte(src), uint8(k), [8]float64(a), sae))
}

func m512MaskCvttRoundpdEpu32(src [32]byte, k uint8, a [8]float64, sae int) [32]byte


// M512MaskzCvttRoundpdEpu32: Convert packed double-precision (64-bit)
// floating-point elements in 'a' to packed unsigned 32-bit integers with
// truncation, and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := 32*i
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedIntegerTruncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2UDQ'. Intrinsic: '_mm512_maskz_cvtt_roundpd_epu32'.
// Requires AVX512F.
func M512MaskzCvttRoundpdEpu32(k x86.Mmask8, a x86.M512d, sae int) x86.M256i {
	return x86.M256i(m512MaskzCvttRoundpdEpu32(uint8(k), [8]float64(a), sae))
}

func m512MaskzCvttRoundpdEpu32(k uint8, a [8]float64, sae int) [32]byte


// M512CvttRoundpsEpi32: Convert packed single-precision (32-bit)
// floating-point elements in 'a' to packed 32-bit integers with truncation,
// and store the results in 'dst'. 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 15
//			i := 32*i
//			dst[i+31:i] := Convert_FP32_To_IntegerTruncate(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2DQ'. Intrinsic: '_mm512_cvtt_roundps_epi32'.
// Requires AVX512F.
func M512CvttRoundpsEpi32(a x86.M512, sae int) x86.M512i {
	return x86.M512i(m512CvttRoundpsEpi32([16]float32(a), sae))
}

func m512CvttRoundpsEpi32(a [16]float32, sae int) [64]byte


// M512MaskCvttRoundpsEpi32: Convert packed single-precision (32-bit)
// floating-point elements in 'a' to packed 32-bit integers with truncation,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 15
//			i := 32*i
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_IntegerTruncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2DQ'. Intrinsic: '_mm512_mask_cvtt_roundps_epi32'.
// Requires AVX512F.
func M512MaskCvttRoundpsEpi32(src x86.M512i, k x86.Mmask16, a x86.M512, sae int) x86.M512i {
	return x86.M512i(m512MaskCvttRoundpsEpi32([64]byte(src), uint16(k), [16]float32(a), sae))
}

func m512MaskCvttRoundpsEpi32(src [64]byte, k uint16, a [16]float32, sae int) [64]byte


// M512MaskzCvttRoundpsEpi32: Convert packed single-precision (32-bit)
// floating-point elements in 'a' to packed 32-bit integers with truncation,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 15
//			i := 32*i
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_IntegerTruncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2DQ'. Intrinsic: '_mm512_maskz_cvtt_roundps_epi32'.
// Requires AVX512F.
func M512MaskzCvttRoundpsEpi32(k x86.Mmask16, a x86.M512, sae int) x86.M512i {
	return x86.M512i(m512MaskzCvttRoundpsEpi32(uint16(k), [16]float32(a), sae))
}

func m512MaskzCvttRoundpsEpi32(k uint16, a [16]float32, sae int) [64]byte


// M512CvttRoundpsEpu32: Convert packed single-precision (32-bit)
// floating-point elements in 'a' to packed unsigned 32-bit integers with
// truncation, and store the results in 'dst'. 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 15
//			i := 32*i
//			dst[i+31:i] := Convert_FP32_To_UnsignedIntegerTruncate(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2UDQ'. Intrinsic: '_mm512_cvtt_roundps_epu32'.
// Requires AVX512F.
func M512CvttRoundpsEpu32(a x86.M512, sae int) x86.M512i {
	return x86.M512i(m512CvttRoundpsEpu32([16]float32(a), sae))
}

func m512CvttRoundpsEpu32(a [16]float32, sae int) [64]byte


// M512MaskCvttRoundpsEpu32: Convert packed single-precision (32-bit)
// floating-point elements in 'a' to packed unsigned 32-bit integers with
// truncation, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set).  
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 15
//			i := 32*i
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_UnsignedIntegerTruncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2UDQ'. Intrinsic: '_mm512_mask_cvtt_roundps_epu32'.
// Requires AVX512F.
func M512MaskCvttRoundpsEpu32(src x86.M512i, k x86.Mmask16, a x86.M512, sae int) x86.M512i {
	return x86.M512i(m512MaskCvttRoundpsEpu32([64]byte(src), uint16(k), [16]float32(a), sae))
}

func m512MaskCvttRoundpsEpu32(src [64]byte, k uint16, a [16]float32, sae int) [64]byte


// M512MaskzCvttRoundpsEpu32: Convert packed single-precision (32-bit)
// floating-point elements in 'a' to packed unsigned 32-bit integers with
// truncation, and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 15
//			i := 32*i
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_UnsignedIntegerTruncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2UDQ'. Intrinsic: '_mm512_maskz_cvtt_roundps_epu32'.
// Requires AVX512F.
func M512MaskzCvttRoundpsEpu32(k x86.Mmask16, a x86.M512, sae int) x86.M512i {
	return x86.M512i(m512MaskzCvttRoundpsEpu32(uint16(k), [16]float32(a), sae))
}

func m512MaskzCvttRoundpsEpu32(k uint16, a [16]float32, sae int) [64]byte


// CvttRoundsdI32: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to a 32-bit integer with truncation, and store the result in
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_FP64_To_Int32_Truncate(a[63:0])
//
// Instruction: 'VCVTTSD2SI'. Intrinsic: '_mm_cvtt_roundsd_i32'.
// Requires AVX512F.
func CvttRoundsdI32(a x86.M128d, rounding int) int {
	return int(cvttRoundsdI32([2]float64(a), rounding))
}

func cvttRoundsdI32(a [2]float64, rounding int) int


// CvttRoundsdI64: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to a 64-bit integer with truncation, and store the result in
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_FP64_To_Int64_Truncate(a[63:0])
//
// Instruction: 'VCVTTSD2SI'. Intrinsic: '_mm_cvtt_roundsd_i64'.
// Requires AVX512F.
func CvttRoundsdI64(a x86.M128d, rounding int) int64 {
	return int64(cvttRoundsdI64([2]float64(a), rounding))
}

func cvttRoundsdI64(a [2]float64, rounding int) int64


// CvttRoundsdSi32: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to a 32-bit integer with truncation, and store the result in
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_FP64_To_Int32_Truncate(a[63:0])
//
// Instruction: 'VCVTTSD2SI'. Intrinsic: '_mm_cvtt_roundsd_si32'.
// Requires AVX512F.
func CvttRoundsdSi32(a x86.M128d, rounding int) int {
	return int(cvttRoundsdSi32([2]float64(a), rounding))
}

func cvttRoundsdSi32(a [2]float64, rounding int) int


// CvttRoundsdSi64: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to a 64-bit integer with truncation, and store the result in
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_FP64_To_Int64_Truncate(a[63:0])
//
// Instruction: 'VCVTTSD2SI'. Intrinsic: '_mm_cvtt_roundsd_si64'.
// Requires AVX512F.
func CvttRoundsdSi64(a x86.M128d, rounding int) int64 {
	return int64(cvttRoundsdSi64([2]float64(a), rounding))
}

func cvttRoundsdSi64(a [2]float64, rounding int) int64


// CvttRoundsdU32: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to an unsigned 32-bit integer with truncation, and store the
// result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_FP64_To_UnsignedInt32_Truncate(a[63:0])
//
// Instruction: 'VCVTTSD2USI'. Intrinsic: '_mm_cvtt_roundsd_u32'.
// Requires AVX512F.
func CvttRoundsdU32(a x86.M128d, rounding int) uint32 {
	return uint32(cvttRoundsdU32([2]float64(a), rounding))
}

func cvttRoundsdU32(a [2]float64, rounding int) uint32


// CvttRoundsdU64: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to an unsigned 64-bit integer with truncation, and store the
// result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_FP64_To_UnsignedInt64_Truncate(a[63:0])
//
// Instruction: 'VCVTTSD2USI'. Intrinsic: '_mm_cvtt_roundsd_u64'.
// Requires AVX512F.
func CvttRoundsdU64(a x86.M128d, rounding int) uint64 {
	return uint64(cvttRoundsdU64([2]float64(a), rounding))
}

func cvttRoundsdU64(a [2]float64, rounding int) uint64


// CvttRoundssI32: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to a 32-bit integer with truncation, and store the result in
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_FP32_To_Int32_Truncate(a[31:0])
//
// Instruction: 'VCVTTSS2SI'. Intrinsic: '_mm_cvtt_roundss_i32'.
// Requires AVX512F.
func CvttRoundssI32(a x86.M128, rounding int) int {
	return int(cvttRoundssI32([4]float32(a), rounding))
}

func cvttRoundssI32(a [4]float32, rounding int) int


// CvttRoundssI64: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to a 64-bit integer with truncation, and store the result in
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_FP32_To_Int64_Truncate(a[31:0])
//
// Instruction: 'VCVTTSS2SI'. Intrinsic: '_mm_cvtt_roundss_i64'.
// Requires AVX512F.
func CvttRoundssI64(a x86.M128, rounding int) int64 {
	return int64(cvttRoundssI64([4]float32(a), rounding))
}

func cvttRoundssI64(a [4]float32, rounding int) int64


// CvttRoundssSi32: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to a 32-bit integer with truncation, and store the result in
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_FP32_To_Int32_Truncate(a[31:0])
//
// Instruction: 'VCVTTSS2SI'. Intrinsic: '_mm_cvtt_roundss_si32'.
// Requires AVX512F.
func CvttRoundssSi32(a x86.M128, rounding int) int {
	return int(cvttRoundssSi32([4]float32(a), rounding))
}

func cvttRoundssSi32(a [4]float32, rounding int) int


// CvttRoundssSi64: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to a 64-bit integer with truncation, and store the result in
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_FP32_To_Int64_Truncate(a[31:0])
//
// Instruction: 'VCVTTSS2SI'. Intrinsic: '_mm_cvtt_roundss_si64'.
// Requires AVX512F.
func CvttRoundssSi64(a x86.M128, rounding int) int64 {
	return int64(cvttRoundssSi64([4]float32(a), rounding))
}

func cvttRoundssSi64(a [4]float32, rounding int) int64


// CvttRoundssU32: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to an unsigned 32-bit integer with truncation, and store the
// result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_FP32_To_UnsignedInt32_Truncate(a[31:0])
//
// Instruction: 'VCVTTSS2USI'. Intrinsic: '_mm_cvtt_roundss_u32'.
// Requires AVX512F.
func CvttRoundssU32(a x86.M128, rounding int) uint32 {
	return uint32(cvttRoundssU32([4]float32(a), rounding))
}

func cvttRoundssU32(a [4]float32, rounding int) uint32


// CvttRoundssU64: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to an unsigned 64-bit integer with truncation, and store the
// result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_FP32_To_UnsignedInt64_Truncate(a[31:0])
//
// Instruction: 'VCVTTSS2USI'. Intrinsic: '_mm_cvtt_roundss_u64'.
// Requires AVX512F.
func CvttRoundssU64(a x86.M128, rounding int) uint64 {
	return uint64(cvttRoundssU64([4]float32(a), rounding))
}

func cvttRoundssU64(a [4]float32, rounding int) uint64


// MaskCvttpdEpi32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_Int32_Truncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTTPD2DQ'. Intrinsic: '_mm_mask_cvttpd_epi32'.
// Requires AVX512F.
func MaskCvttpdEpi32(src x86.M128i, k x86.Mmask8, a x86.M128d) x86.M128i {
	return x86.M128i(maskCvttpdEpi32([16]byte(src), uint8(k), [2]float64(a)))
}

func maskCvttpdEpi32(src [16]byte, k uint8, a [2]float64) [16]byte


// MaskzCvttpdEpi32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_Int32_Truncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTTPD2DQ'. Intrinsic: '_mm_maskz_cvttpd_epi32'.
// Requires AVX512F.
func MaskzCvttpdEpi32(k x86.Mmask8, a x86.M128d) x86.M128i {
	return x86.M128i(maskzCvttpdEpi32(uint8(k), [2]float64(a)))
}

func maskzCvttpdEpi32(k uint8, a [2]float64) [16]byte


// M256MaskCvttpdEpi32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_Int32_Truncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPD2DQ'. Intrinsic: '_mm256_mask_cvttpd_epi32'.
// Requires AVX512F.
func M256MaskCvttpdEpi32(src x86.M128i, k x86.Mmask8, a x86.M256d) x86.M128i {
	return x86.M128i(m256MaskCvttpdEpi32([16]byte(src), uint8(k), [4]float64(a)))
}

func m256MaskCvttpdEpi32(src [16]byte, k uint8, a [4]float64) [16]byte


// M256MaskzCvttpdEpi32: Convert packed double-precision (64-bit)
// floating-point elements in 'a' to packed 32-bit integers with truncation,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_Int32_Truncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPD2DQ'. Intrinsic: '_mm256_maskz_cvttpd_epi32'.
// Requires AVX512F.
func M256MaskzCvttpdEpi32(k x86.Mmask8, a x86.M256d) x86.M128i {
	return x86.M128i(m256MaskzCvttpdEpi32(uint8(k), [4]float64(a)))
}

func m256MaskzCvttpdEpi32(k uint8, a [4]float64) [16]byte


// M512CvttpdEpi32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_Int32_Truncate(a[k+63:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2DQ'. Intrinsic: '_mm512_cvttpd_epi32'.
// Requires AVX512F.
func M512CvttpdEpi32(a x86.M512d) x86.M256i {
	return x86.M256i(m512CvttpdEpi32([8]float64(a)))
}

func m512CvttpdEpi32(a [8]float64) [32]byte


// M512MaskCvttpdEpi32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_Int32_Truncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2DQ'. Intrinsic: '_mm512_mask_cvttpd_epi32'.
// Requires AVX512F.
func M512MaskCvttpdEpi32(src x86.M256i, k x86.Mmask8, a x86.M512d) x86.M256i {
	return x86.M256i(m512MaskCvttpdEpi32([32]byte(src), uint8(k), [8]float64(a)))
}

func m512MaskCvttpdEpi32(src [32]byte, k uint8, a [8]float64) [32]byte


// M512MaskzCvttpdEpi32: Convert packed double-precision (64-bit)
// floating-point elements in 'a' to packed 32-bit integers with truncation,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_Int32_Truncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2DQ'. Intrinsic: '_mm512_maskz_cvttpd_epi32'.
// Requires AVX512F.
func M512MaskzCvttpdEpi32(k x86.Mmask8, a x86.M512d) x86.M256i {
	return x86.M256i(m512MaskzCvttpdEpi32(uint8(k), [8]float64(a)))
}

func m512MaskzCvttpdEpi32(k uint8, a [8]float64) [32]byte


// CvttpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[k+63:k])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTTPD2UDQ'. Intrinsic: '_mm_cvttpd_epu32'.
// Requires AVX512F.
func CvttpdEpu32(a x86.M128d) x86.M128i {
	return x86.M128i(cvttpdEpu32([2]float64(a)))
}

func cvttpdEpu32(a [2]float64) [16]byte


// MaskCvttpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTTPD2UDQ'. Intrinsic: '_mm_mask_cvttpd_epu32'.
// Requires AVX512F.
func MaskCvttpdEpu32(src x86.M128i, k x86.Mmask8, a x86.M128d) x86.M128i {
	return x86.M128i(maskCvttpdEpu32([16]byte(src), uint8(k), [2]float64(a)))
}

func maskCvttpdEpu32(src [16]byte, k uint8, a [2]float64) [16]byte


// MaskzCvttpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTTPD2UDQ'. Intrinsic: '_mm_maskz_cvttpd_epu32'.
// Requires AVX512F.
func MaskzCvttpdEpu32(k x86.Mmask8, a x86.M128d) x86.M128i {
	return x86.M128i(maskzCvttpdEpu32(uint8(k), [2]float64(a)))
}

func maskzCvttpdEpu32(k uint8, a [2]float64) [16]byte


// M256CvttpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[k+63:k])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPD2UDQ'. Intrinsic: '_mm256_cvttpd_epu32'.
// Requires AVX512F.
func M256CvttpdEpu32(a x86.M256d) x86.M128i {
	return x86.M128i(m256CvttpdEpu32([4]float64(a)))
}

func m256CvttpdEpu32(a [4]float64) [16]byte


// M256MaskCvttpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPD2UDQ'. Intrinsic: '_mm256_mask_cvttpd_epu32'.
// Requires AVX512F.
func M256MaskCvttpdEpu32(src x86.M128i, k x86.Mmask8, a x86.M256d) x86.M128i {
	return x86.M128i(m256MaskCvttpdEpu32([16]byte(src), uint8(k), [4]float64(a)))
}

func m256MaskCvttpdEpu32(src [16]byte, k uint8, a [4]float64) [16]byte


// M256MaskzCvttpdEpu32: Convert packed double-precision (64-bit)
// floating-point elements in 'a' to packed unsigned 32-bit integers with
// truncation, and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPD2UDQ'. Intrinsic: '_mm256_maskz_cvttpd_epu32'.
// Requires AVX512F.
func M256MaskzCvttpdEpu32(k x86.Mmask8, a x86.M256d) x86.M128i {
	return x86.M128i(m256MaskzCvttpdEpu32(uint8(k), [4]float64(a)))
}

func m256MaskzCvttpdEpu32(k uint8, a [4]float64) [16]byte


// M512CvttpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[k+63:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2UDQ'. Intrinsic: '_mm512_cvttpd_epu32'.
// Requires AVX512F.
func M512CvttpdEpu32(a x86.M512d) x86.M256i {
	return x86.M256i(m512CvttpdEpu32([8]float64(a)))
}

func m512CvttpdEpu32(a [8]float64) [32]byte


// M512MaskCvttpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2UDQ'. Intrinsic: '_mm512_mask_cvttpd_epu32'.
// Requires AVX512F.
func M512MaskCvttpdEpu32(src x86.M256i, k x86.Mmask8, a x86.M512d) x86.M256i {
	return x86.M256i(m512MaskCvttpdEpu32([32]byte(src), uint8(k), [8]float64(a)))
}

func m512MaskCvttpdEpu32(src [32]byte, k uint8, a [8]float64) [32]byte


// M512MaskzCvttpdEpu32: Convert packed double-precision (64-bit)
// floating-point elements in 'a' to packed unsigned 32-bit integers with
// truncation, and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2UDQ'. Intrinsic: '_mm512_maskz_cvttpd_epu32'.
// Requires AVX512F.
func M512MaskzCvttpdEpu32(k x86.Mmask8, a x86.M512d) x86.M256i {
	return x86.M256i(m512MaskzCvttpdEpu32(uint8(k), [8]float64(a)))
}

func m512MaskzCvttpdEpu32(k uint8, a [8]float64) [32]byte


// MaskCvttpsEpi32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_Int32_Truncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPS2DQ'. Intrinsic: '_mm_mask_cvttps_epi32'.
// Requires AVX512F.
func MaskCvttpsEpi32(src x86.M128i, k x86.Mmask8, a x86.M128) x86.M128i {
	return x86.M128i(maskCvttpsEpi32([16]byte(src), uint8(k), [4]float32(a)))
}

func maskCvttpsEpi32(src [16]byte, k uint8, a [4]float32) [16]byte


// MaskzCvttpsEpi32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*i
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_IntegerTruncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPS2DQ'. Intrinsic: '_mm_maskz_cvttps_epi32'.
// Requires AVX512F.
func MaskzCvttpsEpi32(k x86.Mmask8, a x86.M128) x86.M128i {
	return x86.M128i(maskzCvttpsEpi32(uint8(k), [4]float32(a)))
}

func maskzCvttpsEpi32(k uint8, a [4]float32) [16]byte


// M256MaskCvttpsEpi32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_Int32_Truncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPS2DQ'. Intrinsic: '_mm256_mask_cvttps_epi32'.
// Requires AVX512F.
func M256MaskCvttpsEpi32(src x86.M256i, k x86.Mmask8, a x86.M256) x86.M256i {
	return x86.M256i(m256MaskCvttpsEpi32([32]byte(src), uint8(k), [8]float32(a)))
}

func m256MaskCvttpsEpi32(src [32]byte, k uint8, a [8]float32) [32]byte


// M256MaskzCvttpsEpi32: Convert packed single-precision (32-bit)
// floating-point elements in 'a' to packed 32-bit integers with truncation,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*i
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_IntegerTruncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPS2DQ'. Intrinsic: '_mm256_maskz_cvttps_epi32'.
// Requires AVX512F.
func M256MaskzCvttpsEpi32(k x86.Mmask8, a x86.M256) x86.M256i {
	return x86.M256i(m256MaskzCvttpsEpi32(uint8(k), [8]float32(a)))
}

func m256MaskzCvttpsEpi32(k uint8, a [8]float32) [32]byte


// M512CvttpsEpi32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := Convert_FP32_To_Int32_Truncate(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2DQ'. Intrinsic: '_mm512_cvttps_epi32'.
// Requires AVX512F.
func M512CvttpsEpi32(a x86.M512) x86.M512i {
	return x86.M512i(m512CvttpsEpi32([16]float32(a)))
}

func m512CvttpsEpi32(a [16]float32) [64]byte


// M512MaskCvttpsEpi32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_Int32_Truncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2DQ'. Intrinsic: '_mm512_mask_cvttps_epi32'.
// Requires AVX512F.
func M512MaskCvttpsEpi32(src x86.M512i, k x86.Mmask16, a x86.M512) x86.M512i {
	return x86.M512i(m512MaskCvttpsEpi32([64]byte(src), uint16(k), [16]float32(a)))
}

func m512MaskCvttpsEpi32(src [64]byte, k uint16, a [16]float32) [64]byte


// M512MaskzCvttpsEpi32: Convert packed single-precision (32-bit)
// floating-point elements in 'a' to packed 32-bit integers with truncation,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_Int32_Truncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2DQ'. Intrinsic: '_mm512_maskz_cvttps_epi32'.
// Requires AVX512F.
func M512MaskzCvttpsEpi32(k x86.Mmask16, a x86.M512) x86.M512i {
	return x86.M512i(m512MaskzCvttpsEpi32(uint16(k), [16]float32(a)))
}

func m512MaskzCvttpsEpi32(k uint16, a [16]float32) [64]byte


// CvttpsEpu32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			dst[i+31:i] := Convert_FP32_To_UnsignedInt32_Truncate(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPS2UDQ'. Intrinsic: '_mm_cvttps_epu32'.
// Requires AVX512F.
func CvttpsEpu32(a x86.M128) x86.M128i {
	return x86.M128i(cvttpsEpu32([4]float32(a)))
}

func cvttpsEpu32(a [4]float32) [16]byte


// MaskCvttpsEpu32: Convert packed double-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPS2UDQ'. Intrinsic: '_mm_mask_cvttps_epu32'.
// Requires AVX512F.
func MaskCvttpsEpu32(src x86.M128i, k x86.Mmask8, a x86.M128) x86.M128i {
	return x86.M128i(maskCvttpsEpu32([16]byte(src), uint8(k), [4]float32(a)))
}

func maskCvttpsEpu32(src [16]byte, k uint8, a [4]float32) [16]byte


// MaskzCvttpsEpu32: Convert packed double-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPS2UDQ'. Intrinsic: '_mm_maskz_cvttps_epu32'.
// Requires AVX512F.
func MaskzCvttpsEpu32(k x86.Mmask8, a x86.M128) x86.M128i {
	return x86.M128i(maskzCvttpsEpu32(uint8(k), [4]float32(a)))
}

func maskzCvttpsEpu32(k uint8, a [4]float32) [16]byte


// M256CvttpsEpu32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			dst[i+31:i] := Convert_FP32_To_UnsignedInt32_Truncate(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPS2UDQ'. Intrinsic: '_mm256_cvttps_epu32'.
// Requires AVX512F.
func M256CvttpsEpu32(a x86.M256) x86.M256i {
	return x86.M256i(m256CvttpsEpu32([8]float32(a)))
}

func m256CvttpsEpu32(a [8]float32) [32]byte


// M256MaskCvttpsEpu32: Convert packed double-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPS2UDQ'. Intrinsic: '_mm256_mask_cvttps_epu32'.
// Requires AVX512F.
func M256MaskCvttpsEpu32(src x86.M256i, k x86.Mmask8, a x86.M256) x86.M256i {
	return x86.M256i(m256MaskCvttpsEpu32([32]byte(src), uint8(k), [8]float32(a)))
}

func m256MaskCvttpsEpu32(src [32]byte, k uint8, a [8]float32) [32]byte


// M256MaskzCvttpsEpu32: Convert packed double-precision (32-bit)
// floating-point elements in 'a' to packed unsigned 32-bit integers with
// truncation, and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPS2UDQ'. Intrinsic: '_mm256_maskz_cvttps_epu32'.
// Requires AVX512F.
func M256MaskzCvttpsEpu32(k x86.Mmask8, a x86.M256) x86.M256i {
	return x86.M256i(m256MaskzCvttpsEpu32(uint8(k), [8]float32(a)))
}

func m256MaskzCvttpsEpu32(k uint8, a [8]float32) [32]byte


// M512CvttpsEpu32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := Convert_FP32_To_UnsignedInt32_Truncate(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2UDQ'. Intrinsic: '_mm512_cvttps_epu32'.
// Requires AVX512F.
func M512CvttpsEpu32(a x86.M512) x86.M512i {
	return x86.M512i(m512CvttpsEpu32([16]float32(a)))
}

func m512CvttpsEpu32(a [16]float32) [64]byte


// M512MaskCvttpsEpu32: Convert packed double-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2UDQ'. Intrinsic: '_mm512_mask_cvttps_epu32'.
// Requires AVX512F.
func M512MaskCvttpsEpu32(src x86.M512i, k x86.Mmask16, a x86.M512) x86.M512i {
	return x86.M512i(m512MaskCvttpsEpu32([64]byte(src), uint16(k), [16]float32(a)))
}

func m512MaskCvttpsEpu32(src [64]byte, k uint16, a [16]float32) [64]byte


// M512MaskzCvttpsEpu32: Convert packed double-precision (32-bit)
// floating-point elements in 'a' to packed unsigned 32-bit integers with
// truncation, and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2UDQ'. Intrinsic: '_mm512_maskz_cvttps_epu32'.
// Requires AVX512F.
func M512MaskzCvttpsEpu32(k x86.Mmask16, a x86.M512) x86.M512i {
	return x86.M512i(m512MaskzCvttpsEpu32(uint16(k), [16]float32(a)))
}

func m512MaskzCvttpsEpu32(k uint16, a [16]float32) [64]byte


// CvttsdI32: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to a 32-bit integer with truncation, and store the result in
// 'dst'. 
//
//		dst[31:0] := Convert_FP64_To_Int32_Truncate(a[63:0])
//
// Instruction: 'VCVTTSD2SI'. Intrinsic: '_mm_cvttsd_i32'.
// Requires AVX512F.
func CvttsdI32(a x86.M128d) int {
	return int(cvttsdI32([2]float64(a)))
}

func cvttsdI32(a [2]float64) int


// CvttsdI64: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to a 64-bit integer with truncation, and store the result in
// 'dst'. 
//
//		dst[63:0] := Convert_FP64_To_Int64_Truncate(a[63:0])
//
// Instruction: 'VCVTTSD2SI'. Intrinsic: '_mm_cvttsd_i64'.
// Requires AVX512F.
func CvttsdI64(a x86.M128d) int64 {
	return int64(cvttsdI64([2]float64(a)))
}

func cvttsdI64(a [2]float64) int64


// CvttsdU32: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to an unsigned 32-bit integer with truncation, and store the
// result in 'dst'. 
//
//		dst[31:0] := Convert_FP64_To_UnsignedInt32_Truncate(a[63:0])
//
// Instruction: 'VCVTTSD2USI'. Intrinsic: '_mm_cvttsd_u32'.
// Requires AVX512F.
func CvttsdU32(a x86.M128d) uint32 {
	return uint32(cvttsdU32([2]float64(a)))
}

func cvttsdU32(a [2]float64) uint32


// CvttsdU64: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to an unsigned 64-bit integer with truncation, and store the
// result in 'dst'. 
//
//		dst[63:0] := Convert_FP64_To_UnsignedInt64_Truncate(a[63:0])
//
// Instruction: 'VCVTTSD2USI'. Intrinsic: '_mm_cvttsd_u64'.
// Requires AVX512F.
func CvttsdU64(a x86.M128d) uint64 {
	return uint64(cvttsdU64([2]float64(a)))
}

func cvttsdU64(a [2]float64) uint64


// CvttssI32: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to a 32-bit integer with truncation, and store the result in
// 'dst'. 
//
//		dst[31:0] := Convert_FP32_To_Int32_Truncate(a[31:0])
//
// Instruction: 'VCVTTSS2SI'. Intrinsic: '_mm_cvttss_i32'.
// Requires AVX512F.
func CvttssI32(a x86.M128) int {
	return int(cvttssI32([4]float32(a)))
}

func cvttssI32(a [4]float32) int


// CvttssI64: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to a 64-bit integer with truncation, and store the result in
// 'dst'. 
//
//		dst[63:0] := Convert_FP32_To_Int64_Truncate(a[31:0])
//
// Instruction: 'VCVTTSS2SI'. Intrinsic: '_mm_cvttss_i64'.
// Requires AVX512F.
func CvttssI64(a x86.M128) int64 {
	return int64(cvttssI64([4]float32(a)))
}

func cvttssI64(a [4]float32) int64


// CvttssU32: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to an unsigned 32-bit integer with truncation, and store the
// result in 'dst'. 
//
//		dst[31:0] := Convert_FP32_To_UnsignedInt32_Truncate(a[31:0])
//
// Instruction: 'VCVTTSS2USI'. Intrinsic: '_mm_cvttss_u32'.
// Requires AVX512F.
func CvttssU32(a x86.M128) uint32 {
	return uint32(cvttssU32([4]float32(a)))
}

func cvttssU32(a [4]float32) uint32


// CvttssU64: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to an unsigned 64-bit integer with truncation, and store the
// result in 'dst'. 
//
//		dst[63:0] := Convert_FP32_To_UnsignedInt64_Truncate(a[31:0])
//
// Instruction: 'VCVTTSS2USI'. Intrinsic: '_mm_cvttss_u64'.
// Requires AVX512F.
func CvttssU64(a x86.M128) uint64 {
	return uint64(cvttssU64([4]float32(a)))
}

func cvttssU64(a [4]float32) uint64


// Cvtu32Sd: Convert the unsigned 32-bit integer 'b' to a double-precision
// (64-bit) floating-point element, store the result in the lower element of
// 'dst', and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		dst[63:0] := Convert_UnsignedInt32_To_FP64(b[31:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTUSI2SD'. Intrinsic: '_mm_cvtu32_sd'.
// Requires AVX512F.
func Cvtu32Sd(a x86.M128d, b uint32) x86.M128d {
	return x86.M128d(cvtu32Sd([2]float64(a), b))
}

func cvtu32Sd(a [2]float64, b uint32) [2]float64


// Cvtu32Ss: Convert the unsigned 32-bit integer 'b' to a single-precision
// (32-bit) floating-point element, store the result in the lower element of
// 'dst', and copy the upper 3 packed elements from 'a' to the upper elements
// of 'dst'. 
//
//		dst[31:0] := Convert_UnsignedInt32_To_FP32(b[31:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTUSI2SS'. Intrinsic: '_mm_cvtu32_ss'.
// Requires AVX512F.
func Cvtu32Ss(a x86.M128, b uint32) x86.M128 {
	return x86.M128(cvtu32Ss([4]float32(a), b))
}

func cvtu32Ss(a [4]float32, b uint32) [4]float32


// Cvtu64Sd: Convert the unsigned 64-bit integer 'b' to a double-precision
// (64-bit) floating-point element, store the result in the lower element of
// 'dst', and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		dst[63:0] := Convert_UnsignedInt64_To_FP64(b[63:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTUSI2SD'. Intrinsic: '_mm_cvtu64_sd'.
// Requires AVX512F.
func Cvtu64Sd(a x86.M128d, b uint64) x86.M128d {
	return x86.M128d(cvtu64Sd([2]float64(a), b))
}

func cvtu64Sd(a [2]float64, b uint64) [2]float64


// Cvtu64Ss: Convert the unsigned 64-bit integer 'b' to a single-precision
// (32-bit) floating-point element, store the result in the lower element of
// 'dst', and copy the upper 3 packed elements from 'a' to the upper elements
// of 'dst'. 
//
//		dst[31:0] := Convert_UnsignedInt64_To_FP32(b[63:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTUSI2SS'. Intrinsic: '_mm_cvtu64_ss'.
// Requires AVX512F.
func Cvtu64Ss(a x86.M128, b uint64) x86.M128 {
	return x86.M128(cvtu64Ss([4]float32(a), b))
}

func cvtu64Ss(a [4]float32, b uint64) [4]float32


// Cvtusepi32Epi16: Convert packed unsigned 32-bit integers in 'a' to packed
// unsigned 16-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			k := 16*j
//			dst[k+15:k] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSDW'. Intrinsic: '_mm_cvtusepi32_epi16'.
// Requires AVX512F.
func Cvtusepi32Epi16(a x86.M128i) x86.M128i {
	return x86.M128i(cvtusepi32Epi16([16]byte(a)))
}

func cvtusepi32Epi16(a [16]byte) [16]byte


// MaskCvtusepi32Epi16: Convert packed unsigned 32-bit integers in 'a' to
// packed unsigned 16-bit integers with unsigned saturation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSDW'. Intrinsic: '_mm_mask_cvtusepi32_epi16'.
// Requires AVX512F.
func MaskCvtusepi32Epi16(src x86.M128i, k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskCvtusepi32Epi16([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtusepi32Epi16(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtusepi32Epi16: Convert packed unsigned 32-bit integers in 'a' to
// packed unsigned 16-bit integers with unsigned saturation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSDW'. Intrinsic: '_mm_maskz_cvtusepi32_epi16'.
// Requires AVX512F.
func MaskzCvtusepi32Epi16(k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskzCvtusepi32Epi16(uint8(k), [16]byte(a)))
}

func maskzCvtusepi32Epi16(k uint8, a [16]byte) [16]byte


// M256Cvtusepi32Epi16: Convert packed unsigned 32-bit integers in 'a' to
// packed unsigned 16-bit integers with unsigned saturation, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 16*j
//			dst[k+15:k] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSDW'. Intrinsic: '_mm256_cvtusepi32_epi16'.
// Requires AVX512F.
func M256Cvtusepi32Epi16(a x86.M256i) x86.M128i {
	return x86.M128i(m256Cvtusepi32Epi16([32]byte(a)))
}

func m256Cvtusepi32Epi16(a [32]byte) [16]byte


// M256MaskCvtusepi32Epi16: Convert packed unsigned 32-bit integers in 'a' to
// packed unsigned 16-bit integers with unsigned saturation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSDW'. Intrinsic: '_mm256_mask_cvtusepi32_epi16'.
// Requires AVX512F.
func M256MaskCvtusepi32Epi16(src x86.M128i, k x86.Mmask8, a x86.M256i) x86.M128i {
	return x86.M128i(m256MaskCvtusepi32Epi16([16]byte(src), uint8(k), [32]byte(a)))
}

func m256MaskCvtusepi32Epi16(src [16]byte, k uint8, a [32]byte) [16]byte


// M256MaskzCvtusepi32Epi16: Convert packed unsigned 32-bit integers in 'a' to
// packed unsigned 16-bit integers with unsigned saturation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSDW'. Intrinsic: '_mm256_maskz_cvtusepi32_epi16'.
// Requires AVX512F.
func M256MaskzCvtusepi32Epi16(k x86.Mmask8, a x86.M256i) x86.M128i {
	return x86.M128i(m256MaskzCvtusepi32Epi16(uint8(k), [32]byte(a)))
}

func m256MaskzCvtusepi32Epi16(k uint8, a [32]byte) [16]byte


// M512Cvtusepi32Epi16: Convert packed unsigned 32-bit integers in 'a' to
// packed unsigned 16-bit integers with unsigned saturation, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			k := 16*j
//			dst[k+15:k] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVUSDW'. Intrinsic: '_mm512_cvtusepi32_epi16'.
// Requires AVX512F.
func M512Cvtusepi32Epi16(a x86.M512i) x86.M256i {
	return x86.M256i(m512Cvtusepi32Epi16([64]byte(a)))
}

func m512Cvtusepi32Epi16(a [64]byte) [32]byte


// M512MaskCvtusepi32Epi16: Convert packed unsigned 32-bit integers in 'a' to
// packed unsigned 16-bit integers with unsigned saturation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVUSDW'. Intrinsic: '_mm512_mask_cvtusepi32_epi16'.
// Requires AVX512F.
func M512MaskCvtusepi32Epi16(src x86.M256i, k x86.Mmask16, a x86.M512i) x86.M256i {
	return x86.M256i(m512MaskCvtusepi32Epi16([32]byte(src), uint16(k), [64]byte(a)))
}

func m512MaskCvtusepi32Epi16(src [32]byte, k uint16, a [64]byte) [32]byte


// M512MaskzCvtusepi32Epi16: Convert packed unsigned 32-bit integers in 'a' to
// packed unsigned 16-bit integers with unsigned saturation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVUSDW'. Intrinsic: '_mm512_maskz_cvtusepi32_epi16'.
// Requires AVX512F.
func M512MaskzCvtusepi32Epi16(k x86.Mmask16, a x86.M512i) x86.M256i {
	return x86.M256i(m512MaskzCvtusepi32Epi16(uint16(k), [64]byte(a)))
}

func m512MaskzCvtusepi32Epi16(k uint16, a [64]byte) [32]byte


// Cvtusepi32Epi8: Convert packed unsigned 32-bit integers in 'a' to packed
// unsigned 8-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			k := 8*j
//			dst[k+7:k] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVUSDB'. Intrinsic: '_mm_cvtusepi32_epi8'.
// Requires AVX512F.
func Cvtusepi32Epi8(a x86.M128i) x86.M128i {
	return x86.M128i(cvtusepi32Epi8([16]byte(a)))
}

func cvtusepi32Epi8(a [16]byte) [16]byte


// MaskCvtusepi32Epi8: Convert packed unsigned 32-bit integers in 'a' to packed
// unsigned 8-bit integers with unsigned saturation, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVUSDB'. Intrinsic: '_mm_mask_cvtusepi32_epi8'.
// Requires AVX512F.
func MaskCvtusepi32Epi8(src x86.M128i, k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskCvtusepi32Epi8([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtusepi32Epi8(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtusepi32Epi8: Convert packed unsigned 32-bit integers in 'a' to
// packed unsigned 8-bit integers with unsigned saturation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVUSDB'. Intrinsic: '_mm_maskz_cvtusepi32_epi8'.
// Requires AVX512F.
func MaskzCvtusepi32Epi8(k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskzCvtusepi32Epi8(uint8(k), [16]byte(a)))
}

func maskzCvtusepi32Epi8(k uint8, a [16]byte) [16]byte


// M256Cvtusepi32Epi8: Convert packed unsigned 32-bit integers in 'a' to packed
// unsigned 8-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 8*j
//			dst[k+7:k] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSDB'. Intrinsic: '_mm256_cvtusepi32_epi8'.
// Requires AVX512F.
func M256Cvtusepi32Epi8(a x86.M256i) x86.M128i {
	return x86.M128i(m256Cvtusepi32Epi8([32]byte(a)))
}

func m256Cvtusepi32Epi8(a [32]byte) [16]byte


// M256MaskCvtusepi32Epi8: Convert packed unsigned 32-bit integers in 'a' to
// packed unsigned 8-bit integers with unsigned saturation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSDB'. Intrinsic: '_mm256_mask_cvtusepi32_epi8'.
// Requires AVX512F.
func M256MaskCvtusepi32Epi8(src x86.M128i, k x86.Mmask8, a x86.M256i) x86.M128i {
	return x86.M128i(m256MaskCvtusepi32Epi8([16]byte(src), uint8(k), [32]byte(a)))
}

func m256MaskCvtusepi32Epi8(src [16]byte, k uint8, a [32]byte) [16]byte


// M256MaskzCvtusepi32Epi8: Convert packed unsigned 32-bit integers in 'a' to
// packed unsigned 8-bit integers with unsigned saturation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSDB'. Intrinsic: '_mm256_maskz_cvtusepi32_epi8'.
// Requires AVX512F.
func M256MaskzCvtusepi32Epi8(k x86.Mmask8, a x86.M256i) x86.M128i {
	return x86.M128i(m256MaskzCvtusepi32Epi8(uint8(k), [32]byte(a)))
}

func m256MaskzCvtusepi32Epi8(k uint8, a [32]byte) [16]byte


// M512Cvtusepi32Epi8: Convert packed unsigned 32-bit integers in 'a' to packed
// unsigned 8-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			k := 8*j
//			dst[k+7:k] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSDB'. Intrinsic: '_mm512_cvtusepi32_epi8'.
// Requires AVX512F.
func M512Cvtusepi32Epi8(a x86.M512i) x86.M128i {
	return x86.M128i(m512Cvtusepi32Epi8([64]byte(a)))
}

func m512Cvtusepi32Epi8(a [64]byte) [16]byte


// M512MaskCvtusepi32Epi8: Convert packed unsigned 32-bit integers in 'a' to
// packed unsigned 8-bit integers with unsigned saturation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSDB'. Intrinsic: '_mm512_mask_cvtusepi32_epi8'.
// Requires AVX512F.
func M512MaskCvtusepi32Epi8(src x86.M128i, k x86.Mmask16, a x86.M512i) x86.M128i {
	return x86.M128i(m512MaskCvtusepi32Epi8([16]byte(src), uint16(k), [64]byte(a)))
}

func m512MaskCvtusepi32Epi8(src [16]byte, k uint16, a [64]byte) [16]byte


// M512MaskzCvtusepi32Epi8: Convert packed unsigned 32-bit integers in 'a' to
// packed unsigned 8-bit integers with unsigned saturation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSDB'. Intrinsic: '_mm512_maskz_cvtusepi32_epi8'.
// Requires AVX512F.
func M512MaskzCvtusepi32Epi8(k x86.Mmask16, a x86.M512i) x86.M128i {
	return x86.M128i(m512MaskzCvtusepi32Epi8(uint16(k), [64]byte(a)))
}

func m512MaskzCvtusepi32Epi8(k uint16, a [64]byte) [16]byte


// Skipped: _mm_mask_cvtusepi32_storeu_epi16. Contains pointer parameter.


// Skipped: _mm256_mask_cvtusepi32_storeu_epi16. Contains pointer parameter.


// Skipped: _mm512_mask_cvtusepi32_storeu_epi16. Contains pointer parameter.


// Skipped: _mm_mask_cvtusepi32_storeu_epi8. Contains pointer parameter.


// Skipped: _mm256_mask_cvtusepi32_storeu_epi8. Contains pointer parameter.


// Skipped: _mm512_mask_cvtusepi32_storeu_epi8. Contains pointer parameter.


// Cvtusepi64Epi16: Convert packed unsigned 64-bit integers in 'a' to packed
// unsigned 16-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			k := 16*j
//			dst[k+15:k] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVUSQW'. Intrinsic: '_mm_cvtusepi64_epi16'.
// Requires AVX512F.
func Cvtusepi64Epi16(a x86.M128i) x86.M128i {
	return x86.M128i(cvtusepi64Epi16([16]byte(a)))
}

func cvtusepi64Epi16(a [16]byte) [16]byte


// MaskCvtusepi64Epi16: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 16-bit integers with unsigned saturation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVUSQW'. Intrinsic: '_mm_mask_cvtusepi64_epi16'.
// Requires AVX512F.
func MaskCvtusepi64Epi16(src x86.M128i, k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskCvtusepi64Epi16([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtusepi64Epi16(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtusepi64Epi16: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 16-bit integers with unsigned saturation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVUSQW'. Intrinsic: '_mm_maskz_cvtusepi64_epi16'.
// Requires AVX512F.
func MaskzCvtusepi64Epi16(k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskzCvtusepi64Epi16(uint8(k), [16]byte(a)))
}

func maskzCvtusepi64Epi16(k uint8, a [16]byte) [16]byte


// M256Cvtusepi64Epi16: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 16-bit integers with unsigned saturation, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			k := 16*j
//			dst[k+15:k] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSQW'. Intrinsic: '_mm256_cvtusepi64_epi16'.
// Requires AVX512F.
func M256Cvtusepi64Epi16(a x86.M256i) x86.M128i {
	return x86.M128i(m256Cvtusepi64Epi16([32]byte(a)))
}

func m256Cvtusepi64Epi16(a [32]byte) [16]byte


// M256MaskCvtusepi64Epi16: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 16-bit integers with unsigned saturation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSQW'. Intrinsic: '_mm256_mask_cvtusepi64_epi16'.
// Requires AVX512F.
func M256MaskCvtusepi64Epi16(src x86.M128i, k x86.Mmask8, a x86.M256i) x86.M128i {
	return x86.M128i(m256MaskCvtusepi64Epi16([16]byte(src), uint8(k), [32]byte(a)))
}

func m256MaskCvtusepi64Epi16(src [16]byte, k uint8, a [32]byte) [16]byte


// M256MaskzCvtusepi64Epi16: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 16-bit integers with unsigned saturation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSQW'. Intrinsic: '_mm256_maskz_cvtusepi64_epi16'.
// Requires AVX512F.
func M256MaskzCvtusepi64Epi16(k x86.Mmask8, a x86.M256i) x86.M128i {
	return x86.M128i(m256MaskzCvtusepi64Epi16(uint8(k), [32]byte(a)))
}

func m256MaskzCvtusepi64Epi16(k uint8, a [32]byte) [16]byte


// M512Cvtusepi64Epi16: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 16-bit integers with unsigned saturation, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 16*j
//			dst[k+15:k] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSQW'. Intrinsic: '_mm512_cvtusepi64_epi16'.
// Requires AVX512F.
func M512Cvtusepi64Epi16(a x86.M512i) x86.M128i {
	return x86.M128i(m512Cvtusepi64Epi16([64]byte(a)))
}

func m512Cvtusepi64Epi16(a [64]byte) [16]byte


// M512MaskCvtusepi64Epi16: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 16-bit integers with unsigned saturation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSQW'. Intrinsic: '_mm512_mask_cvtusepi64_epi16'.
// Requires AVX512F.
func M512MaskCvtusepi64Epi16(src x86.M128i, k x86.Mmask8, a x86.M512i) x86.M128i {
	return x86.M128i(m512MaskCvtusepi64Epi16([16]byte(src), uint8(k), [64]byte(a)))
}

func m512MaskCvtusepi64Epi16(src [16]byte, k uint8, a [64]byte) [16]byte


// M512MaskzCvtusepi64Epi16: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 16-bit integers with unsigned saturation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSQW'. Intrinsic: '_mm512_maskz_cvtusepi64_epi16'.
// Requires AVX512F.
func M512MaskzCvtusepi64Epi16(k x86.Mmask8, a x86.M512i) x86.M128i {
	return x86.M128i(m512MaskzCvtusepi64Epi16(uint8(k), [64]byte(a)))
}

func m512MaskzCvtusepi64Epi16(k uint8, a [64]byte) [16]byte


// Cvtusepi64Epi32: Convert packed unsigned 64-bit integers in 'a' to packed
// unsigned 32-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			k := 32*j
//			dst[k+31:k] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSQD'. Intrinsic: '_mm_cvtusepi64_epi32'.
// Requires AVX512F.
func Cvtusepi64Epi32(a x86.M128i) x86.M128i {
	return x86.M128i(cvtusepi64Epi32([16]byte(a)))
}

func cvtusepi64Epi32(a [16]byte) [16]byte


// MaskCvtusepi64Epi32: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 32-bit integers with unsigned saturation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := src[l+31:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSQD'. Intrinsic: '_mm_mask_cvtusepi64_epi32'.
// Requires AVX512F.
func MaskCvtusepi64Epi32(src x86.M128i, k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskCvtusepi64Epi32([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtusepi64Epi32(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtusepi64Epi32: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 32-bit integers with unsigned saturation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSQD'. Intrinsic: '_mm_maskz_cvtusepi64_epi32'.
// Requires AVX512F.
func MaskzCvtusepi64Epi32(k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskzCvtusepi64Epi32(uint8(k), [16]byte(a)))
}

func maskzCvtusepi64Epi32(k uint8, a [16]byte) [16]byte


// M256Cvtusepi64Epi32: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 32-bit integers with unsigned saturation, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			k := 32*j
//			dst[k+31:k] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSQD'. Intrinsic: '_mm256_cvtusepi64_epi32'.
// Requires AVX512F.
func M256Cvtusepi64Epi32(a x86.M256i) x86.M128i {
	return x86.M128i(m256Cvtusepi64Epi32([32]byte(a)))
}

func m256Cvtusepi64Epi32(a [32]byte) [16]byte


// M256MaskCvtusepi64Epi32: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 32-bit integers with unsigned saturation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := src[l+31:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSQD'. Intrinsic: '_mm256_mask_cvtusepi64_epi32'.
// Requires AVX512F.
func M256MaskCvtusepi64Epi32(src x86.M128i, k x86.Mmask8, a x86.M256i) x86.M128i {
	return x86.M128i(m256MaskCvtusepi64Epi32([16]byte(src), uint8(k), [32]byte(a)))
}

func m256MaskCvtusepi64Epi32(src [16]byte, k uint8, a [32]byte) [16]byte


// M256MaskzCvtusepi64Epi32: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 32-bit integers with unsigned saturation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSQD'. Intrinsic: '_mm256_maskz_cvtusepi64_epi32'.
// Requires AVX512F.
func M256MaskzCvtusepi64Epi32(k x86.Mmask8, a x86.M256i) x86.M128i {
	return x86.M128i(m256MaskzCvtusepi64Epi32(uint8(k), [32]byte(a)))
}

func m256MaskzCvtusepi64Epi32(k uint8, a [32]byte) [16]byte


// M512Cvtusepi64Epi32: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 32-bit integers with unsigned saturation, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 32*j
//			dst[k+31:k] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVUSQD'. Intrinsic: '_mm512_cvtusepi64_epi32'.
// Requires AVX512F.
func M512Cvtusepi64Epi32(a x86.M512i) x86.M256i {
	return x86.M256i(m512Cvtusepi64Epi32([64]byte(a)))
}

func m512Cvtusepi64Epi32(a [64]byte) [32]byte


// M512MaskCvtusepi64Epi32: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 32-bit integers with unsigned saturation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := src[l+31:l]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVUSQD'. Intrinsic: '_mm512_mask_cvtusepi64_epi32'.
// Requires AVX512F.
func M512MaskCvtusepi64Epi32(src x86.M256i, k x86.Mmask8, a x86.M512i) x86.M256i {
	return x86.M256i(m512MaskCvtusepi64Epi32([32]byte(src), uint8(k), [64]byte(a)))
}

func m512MaskCvtusepi64Epi32(src [32]byte, k uint8, a [64]byte) [32]byte


// M512MaskzCvtusepi64Epi32: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 32-bit integers with unsigned saturation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVUSQD'. Intrinsic: '_mm512_maskz_cvtusepi64_epi32'.
// Requires AVX512F.
func M512MaskzCvtusepi64Epi32(k x86.Mmask8, a x86.M512i) x86.M256i {
	return x86.M256i(m512MaskzCvtusepi64Epi32(uint8(k), [64]byte(a)))
}

func m512MaskzCvtusepi64Epi32(k uint8, a [64]byte) [32]byte


// Cvtusepi64Epi8: Convert packed unsigned 64-bit integers in 'a' to packed
// unsigned 8-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			k := 8*j
//			dst[k+7:k] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
//		ENDFOR
//		dst[MAX:16] := 0
//
// Instruction: 'VPMOVUSQB'. Intrinsic: '_mm_cvtusepi64_epi8'.
// Requires AVX512F.
func Cvtusepi64Epi8(a x86.M128i) x86.M128i {
	return x86.M128i(cvtusepi64Epi8([16]byte(a)))
}

func cvtusepi64Epi8(a [16]byte) [16]byte


// MaskCvtusepi64Epi8: Convert packed unsigned 64-bit integers in 'a' to packed
// unsigned 8-bit integers with unsigned saturation, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:16] := 0
//
// Instruction: 'VPMOVUSQB'. Intrinsic: '_mm_mask_cvtusepi64_epi8'.
// Requires AVX512F.
func MaskCvtusepi64Epi8(src x86.M128i, k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskCvtusepi64Epi8([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtusepi64Epi8(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtusepi64Epi8: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 8-bit integers with unsigned saturation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:16] := 0
//
// Instruction: 'VPMOVUSQB'. Intrinsic: '_mm_maskz_cvtusepi64_epi8'.
// Requires AVX512F.
func MaskzCvtusepi64Epi8(k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskzCvtusepi64Epi8(uint8(k), [16]byte(a)))
}

func maskzCvtusepi64Epi8(k uint8, a [16]byte) [16]byte


// M256Cvtusepi64Epi8: Convert packed unsigned 64-bit integers in 'a' to packed
// unsigned 8-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			k := 8*j
//			dst[k+7:k] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVUSQB'. Intrinsic: '_mm256_cvtusepi64_epi8'.
// Requires AVX512F.
func M256Cvtusepi64Epi8(a x86.M256i) x86.M128i {
	return x86.M128i(m256Cvtusepi64Epi8([32]byte(a)))
}

func m256Cvtusepi64Epi8(a [32]byte) [16]byte


// M256MaskCvtusepi64Epi8: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 8-bit integers with unsigned saturation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVUSQB'. Intrinsic: '_mm256_mask_cvtusepi64_epi8'.
// Requires AVX512F.
func M256MaskCvtusepi64Epi8(src x86.M128i, k x86.Mmask8, a x86.M256i) x86.M128i {
	return x86.M128i(m256MaskCvtusepi64Epi8([16]byte(src), uint8(k), [32]byte(a)))
}

func m256MaskCvtusepi64Epi8(src [16]byte, k uint8, a [32]byte) [16]byte


// M256MaskzCvtusepi64Epi8: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 8-bit integers with unsigned saturation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVUSQB'. Intrinsic: '_mm256_maskz_cvtusepi64_epi8'.
// Requires AVX512F.
func M256MaskzCvtusepi64Epi8(k x86.Mmask8, a x86.M256i) x86.M128i {
	return x86.M128i(m256MaskzCvtusepi64Epi8(uint8(k), [32]byte(a)))
}

func m256MaskzCvtusepi64Epi8(k uint8, a [32]byte) [16]byte


// M512Cvtusepi64Epi8: Convert packed unsigned 64-bit integers in 'a' to packed
// unsigned 8-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 8*j
//			dst[k+7:k] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSQB'. Intrinsic: '_mm512_cvtusepi64_epi8'.
// Requires AVX512F.
func M512Cvtusepi64Epi8(a x86.M512i) x86.M128i {
	return x86.M128i(m512Cvtusepi64Epi8([64]byte(a)))
}

func m512Cvtusepi64Epi8(a [64]byte) [16]byte


// M512MaskCvtusepi64Epi8: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 8-bit integers with unsigned saturation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSQB'. Intrinsic: '_mm512_mask_cvtusepi64_epi8'.
// Requires AVX512F.
func M512MaskCvtusepi64Epi8(src x86.M128i, k x86.Mmask8, a x86.M512i) x86.M128i {
	return x86.M128i(m512MaskCvtusepi64Epi8([16]byte(src), uint8(k), [64]byte(a)))
}

func m512MaskCvtusepi64Epi8(src [16]byte, k uint8, a [64]byte) [16]byte


// M512MaskzCvtusepi64Epi8: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 8-bit integers with unsigned saturation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSQB'. Intrinsic: '_mm512_maskz_cvtusepi64_epi8'.
// Requires AVX512F.
func M512MaskzCvtusepi64Epi8(k x86.Mmask8, a x86.M512i) x86.M128i {
	return x86.M128i(m512MaskzCvtusepi64Epi8(uint8(k), [64]byte(a)))
}

func m512MaskzCvtusepi64Epi8(k uint8, a [64]byte) [16]byte


// Skipped: _mm_mask_cvtusepi64_storeu_epi16. Contains pointer parameter.


// Skipped: _mm256_mask_cvtusepi64_storeu_epi16. Contains pointer parameter.


// Skipped: _mm512_mask_cvtusepi64_storeu_epi16. Contains pointer parameter.


// Skipped: _mm_mask_cvtusepi64_storeu_epi32. Contains pointer parameter.


// Skipped: _mm256_mask_cvtusepi64_storeu_epi32. Contains pointer parameter.


// Skipped: _mm512_mask_cvtusepi64_storeu_epi32. Contains pointer parameter.


// Skipped: _mm_mask_cvtusepi64_storeu_epi8. Contains pointer parameter.


// Skipped: _mm256_mask_cvtusepi64_storeu_epi8. Contains pointer parameter.


// Skipped: _mm512_mask_cvtusepi64_storeu_epi8. Contains pointer parameter.


// M512DivEpi16: Divide packed 16-bit integers in 'a' by packed elements in
// 'b', and store the truncated results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := 16*j
//			dst[i+15:i] := TRUNCATE(a[i+15:i] / b[i+15:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_div_epi16'.
// Requires AVX512F.
func M512DivEpi16(a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512DivEpi16([64]byte(a), [64]byte(b)))
}

func m512DivEpi16(a [64]byte, b [64]byte) [64]byte


// M512DivEpi32: Divide packed 32-bit integers in 'a' by packed elements in
// 'b', and store the truncated results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := TRUNCATE(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_div_epi32'.
// Requires AVX512F.
func M512DivEpi32(a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512DivEpi32([64]byte(a), [64]byte(b)))
}

func m512DivEpi32(a [64]byte, b [64]byte) [64]byte


// M512MaskDivEpi32: Divide packed 32-bit integers in 'a' by packed elements in
// 'b', and store the truncated results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := TRUNCATE(a[i+31:i] / b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_div_epi32'.
// Requires AVX512F.
func M512MaskDivEpi32(src x86.M512i, k x86.Mmask16, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskDivEpi32([64]byte(src), uint16(k), [64]byte(a), [64]byte(b)))
}

func m512MaskDivEpi32(src [64]byte, k uint16, a [64]byte, b [64]byte) [64]byte


// M512DivEpi64: Divide packed 64-bit integers in 'a' by packed elements in
// 'b', and store the truncated results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			dst[i+63:i] := TRUNCATE(a[i+63:i] / b[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_div_epi64'.
// Requires AVX512F.
func M512DivEpi64(a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512DivEpi64([64]byte(a), [64]byte(b)))
}

func m512DivEpi64(a [64]byte, b [64]byte) [64]byte


// M512DivEpi8: Divide packed 8-bit integers in 'a' by packed elements in 'b',
// and store the truncated results in 'dst'. 
//
//		FOR j := 0 to 63
//			i := 8*j
//			dst[i+7:i] := TRUNCATE(a[i+7:i] / b[i+7:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_div_epi8'.
// Requires AVX512F.
func M512DivEpi8(a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512DivEpi8([64]byte(a), [64]byte(b)))
}

func m512DivEpi8(a [64]byte, b [64]byte) [64]byte


// M512DivEpu16: Divide packed unsigned 16-bit integers in 'a' by packed
// elements in 'b', and store the truncated results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := 16*j
//			dst[i+15:i] := TRUNCATE(a[i+15:i] / b[i+15:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_div_epu16'.
// Requires AVX512F.
func M512DivEpu16(a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512DivEpu16([64]byte(a), [64]byte(b)))
}

func m512DivEpu16(a [64]byte, b [64]byte) [64]byte


// M512DivEpu32: Divide packed unsigned 32-bit integers in 'a' by packed
// elements in 'b', and store the truncated results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := TRUNCATE(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_div_epu32'.
// Requires AVX512F.
func M512DivEpu32(a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512DivEpu32([64]byte(a), [64]byte(b)))
}

func m512DivEpu32(a [64]byte, b [64]byte) [64]byte


// M512MaskDivEpu32: Divide packed unsigned 32-bit integers in 'a' by packed
// elements in 'b', and store the truncated results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := TRUNCATE(a[i+31:i] / b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_div_epu32'.
// Requires AVX512F.
func M512MaskDivEpu32(src x86.M512i, k x86.Mmask16, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskDivEpu32([64]byte(src), uint16(k), [64]byte(a), [64]byte(b)))
}

func m512MaskDivEpu32(src [64]byte, k uint16, a [64]byte, b [64]byte) [64]byte


// M512DivEpu64: Divide packed unsigned 64-bit integers in 'a' by packed
// elements in 'b', and store the truncated results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			dst[i+63:i] := TRUNCATE(a[i+63:i] / b[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_div_epu64'.
// Requires AVX512F.
func M512DivEpu64(a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512DivEpu64([64]byte(a), [64]byte(b)))
}

func m512DivEpu64(a [64]byte, b [64]byte) [64]byte


// M512DivEpu8: Divide packed unsigned 8-bit integers in 'a' by packed elements
// in 'b', and store the truncated results in 'dst'. 
//
//		FOR j := 0 to 63
//			i := 8*j
//			dst[i+7:i] := TRUNCATE(a[i+7:i] / b[i+7:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_div_epu8'.
// Requires AVX512F.
func M512DivEpu8(a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512DivEpu8([64]byte(a), [64]byte(b)))
}

func m512DivEpu8(a [64]byte, b [64]byte) [64]byte


// MaskDivPd: Divide packed double-precision (64-bit) floating-point elements
// in 'a' by packed elements in 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] / b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VDIVPD'. Intrinsic: '_mm_mask_div_pd'.
// Requires AVX512F.
func MaskDivPd(src x86.M128d, k x86.Mmask8, a x86.M128d, b x86.M128d) x86.M128d {
	return x86.M128d(maskDivPd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskDivPd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzDivPd: Divide packed double-precision (64-bit) floating-point elements
// in 'a' by packed elements in 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] / b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VDIVPD'. Intrinsic: '_mm_maskz_div_pd'.
// Requires AVX512F.
func MaskzDivPd(k x86.Mmask8, a x86.M128d, b x86.M128d) x86.M128d {
	return x86.M128d(maskzDivPd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzDivPd(k uint8, a [2]float64, b [2]float64) [2]float64


// M256MaskDivPd: Divide packed double-precision (64-bit) floating-point
// elements in 'a' by packed elements in 'b', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] / b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VDIVPD'. Intrinsic: '_mm256_mask_div_pd'.
// Requires AVX512F.
func M256MaskDivPd(src x86.M256d, k x86.Mmask8, a x86.M256d, b x86.M256d) x86.M256d {
	return x86.M256d(m256MaskDivPd([4]float64(src), uint8(k), [4]float64(a), [4]float64(b)))
}

func m256MaskDivPd(src [4]float64, k uint8, a [4]float64, b [4]float64) [4]float64


// M256MaskzDivPd: Divide packed double-precision (64-bit) floating-point
// elements in 'a' by packed elements in 'b', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] / b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VDIVPD'. Intrinsic: '_mm256_maskz_div_pd'.
// Requires AVX512F.
func M256MaskzDivPd(k x86.Mmask8, a x86.M256d, b x86.M256d) x86.M256d {
	return x86.M256d(m256MaskzDivPd(uint8(k), [4]float64(a), [4]float64(b)))
}

func m256MaskzDivPd(k uint8, a [4]float64, b [4]float64) [4]float64


// M512DivPd: Divide packed double-precision (64-bit) floating-point elements
// in 'a' by packed elements in 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			dst[i+63:i] := a[i+63:i] / b[i+63:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VDIVPD'. Intrinsic: '_mm512_div_pd'.
// Requires AVX512F.
func M512DivPd(a x86.M512d, b x86.M512d) x86.M512d {
	return x86.M512d(m512DivPd([8]float64(a), [8]float64(b)))
}

func m512DivPd(a [8]float64, b [8]float64) [8]float64


// M512MaskDivPd: Divide packed double-precision (64-bit) floating-point
// elements in 'a' by packed elements in 'b', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] / b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VDIVPD'. Intrinsic: '_mm512_mask_div_pd'.
// Requires AVX512F.
func M512MaskDivPd(src x86.M512d, k x86.Mmask8, a x86.M512d, b x86.M512d) x86.M512d {
	return x86.M512d(m512MaskDivPd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b)))
}

func m512MaskDivPd(src [8]float64, k uint8, a [8]float64, b [8]float64) [8]float64


// M512MaskzDivPd: Divide packed double-precision (64-bit) floating-point
// elements in 'a' by packed elements in 'b', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] / b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VDIVPD'. Intrinsic: '_mm512_maskz_div_pd'.
// Requires AVX512F.
func M512MaskzDivPd(k x86.Mmask8, a x86.M512d, b x86.M512d) x86.M512d {
	return x86.M512d(m512MaskzDivPd(uint8(k), [8]float64(a), [8]float64(b)))
}

func m512MaskzDivPd(k uint8, a [8]float64, b [8]float64) [8]float64


// MaskDivPs: Divide packed single-precision (32-bit) floating-point elements
// in 'a' by packed elements in 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] / b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VDIVPS'. Intrinsic: '_mm_mask_div_ps'.
// Requires AVX512F.
func MaskDivPs(src x86.M128, k x86.Mmask8, a x86.M128, b x86.M128) x86.M128 {
	return x86.M128(maskDivPs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskDivPs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzDivPs: Divide packed single-precision (32-bit) floating-point elements
// in 'a' by packed elements in 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] / b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VDIVPS'. Intrinsic: '_mm_maskz_div_ps'.
// Requires AVX512F.
func MaskzDivPs(k x86.Mmask8, a x86.M128, b x86.M128) x86.M128 {
	return x86.M128(maskzDivPs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzDivPs(k uint8, a [4]float32, b [4]float32) [4]float32


// M256MaskDivPs: Divide packed single-precision (32-bit) floating-point
// elements in 'a' by packed elements in 'b', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] / b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VDIVPS'. Intrinsic: '_mm256_mask_div_ps'.
// Requires AVX512F.
func M256MaskDivPs(src x86.M256, k x86.Mmask8, a x86.M256, b x86.M256) x86.M256 {
	return x86.M256(m256MaskDivPs([8]float32(src), uint8(k), [8]float32(a), [8]float32(b)))
}

func m256MaskDivPs(src [8]float32, k uint8, a [8]float32, b [8]float32) [8]float32


// M256MaskzDivPs: Divide packed single-precision (32-bit) floating-point
// elements in 'a' by packed elements in 'b', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] / b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VDIVPS'. Intrinsic: '_mm256_maskz_div_ps'.
// Requires AVX512F.
func M256MaskzDivPs(k x86.Mmask8, a x86.M256, b x86.M256) x86.M256 {
	return x86.M256(m256MaskzDivPs(uint8(k), [8]float32(a), [8]float32(b)))
}

func m256MaskzDivPs(k uint8, a [8]float32, b [8]float32) [8]float32


// M512DivPs: Divide packed single-precision (32-bit) floating-point elements
// in 'a' by packed elements in 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := a[i+31:i] / b[i+31:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VDIVPS'. Intrinsic: '_mm512_div_ps'.
// Requires AVX512F.
func M512DivPs(a x86.M512, b x86.M512) x86.M512 {
	return x86.M512(m512DivPs([16]float32(a), [16]float32(b)))
}

func m512DivPs(a [16]float32, b [16]float32) [16]float32


// M512MaskDivPs: Divide packed single-precision (32-bit) floating-point
// elements in 'a' by packed elements in 'b', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] / b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VDIVPS'. Intrinsic: '_mm512_mask_div_ps'.
// Requires AVX512F.
func M512MaskDivPs(src x86.M512, k x86.Mmask16, a x86.M512, b x86.M512) x86.M512 {
	return x86.M512(m512MaskDivPs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b)))
}

func m512MaskDivPs(src [16]float32, k uint16, a [16]float32, b [16]float32) [16]float32


// M512MaskzDivPs: Divide packed single-precision (32-bit) floating-point
// elements in 'a' by packed elements in 'b', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] / b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VDIVPS'. Intrinsic: '_mm512_maskz_div_ps'.
// Requires AVX512F.
func M512MaskzDivPs(k x86.Mmask16, a x86.M512, b x86.M512) x86.M512 {
	return x86.M512(m512MaskzDivPs(uint16(k), [16]float32(a), [16]float32(b)))
}

func m512MaskzDivPs(k uint16, a [16]float32, b [16]float32) [16]float32


// M512DivRoundPd: Divide packed double-precision (64-bit) floating-point
// elements in 'a' by packed elements in 'b', =and store the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := 64*j
//			dst[i+63:i] := a[i+63:i] / b[i+63:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VDIVPD'. Intrinsic: '_mm512_div_round_pd'.
// Requires AVX512F.
func M512DivRoundPd(a x86.M512d, b x86.M512d, rounding int) x86.M512d {
	return x86.M512d(m512DivRoundPd([8]float64(a), [8]float64(b), rounding))
}

func m512DivRoundPd(a [8]float64, b [8]float64, rounding int) [8]float64


// M512MaskDivRoundPd: Divide packed double-precision (64-bit) floating-point
// elements in 'a' by packed elements in 'b', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := 64*j
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] / b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VDIVPD'. Intrinsic: '_mm512_mask_div_round_pd'.
// Requires AVX512F.
func M512MaskDivRoundPd(src x86.M512d, k x86.Mmask8, a x86.M512d, b x86.M512d, rounding int) x86.M512d {
	return x86.M512d(m512MaskDivRoundPd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b), rounding))
}

func m512MaskDivRoundPd(src [8]float64, k uint8, a [8]float64, b [8]float64, rounding int) [8]float64


// M512MaskzDivRoundPd: Divide packed double-precision (64-bit) floating-point
// elements in 'a' by packed elements in 'b', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := 64*j
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] / b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VDIVPD'. Intrinsic: '_mm512_maskz_div_round_pd'.
// Requires AVX512F.
func M512MaskzDivRoundPd(k x86.Mmask8, a x86.M512d, b x86.M512d, rounding int) x86.M512d {
	return x86.M512d(m512MaskzDivRoundPd(uint8(k), [8]float64(a), [8]float64(b), rounding))
}

func m512MaskzDivRoundPd(k uint8, a [8]float64, b [8]float64, rounding int) [8]float64


// M512DivRoundPs: Divide packed single-precision (32-bit) floating-point
// elements in 'a' by packed elements in 'b', and store the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := a[i+31:i] / b[i+31:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VDIVPS'. Intrinsic: '_mm512_div_round_ps'.
// Requires AVX512F.
func M512DivRoundPs(a x86.M512, b x86.M512, rounding int) x86.M512 {
	return x86.M512(m512DivRoundPs([16]float32(a), [16]float32(b), rounding))
}

func m512DivRoundPs(a [16]float32, b [16]float32, rounding int) [16]float32


// M512MaskDivRoundPs: Divide packed single-precision (32-bit) floating-point
// elements in 'a' by packed elements in 'b', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] / b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VDIVPS'. Intrinsic: '_mm512_mask_div_round_ps'.
// Requires AVX512F.
func M512MaskDivRoundPs(src x86.M512, k x86.Mmask16, a x86.M512, b x86.M512, rounding int) x86.M512 {
	return x86.M512(m512MaskDivRoundPs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b), rounding))
}

func m512MaskDivRoundPs(src [16]float32, k uint16, a [16]float32, b [16]float32, rounding int) [16]float32


// M512MaskzDivRoundPs: Divide packed single-precision (32-bit) floating-point
// elements in 'a' by packed elements in 'b', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] / b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VDIVPS'. Intrinsic: '_mm512_maskz_div_round_ps'.
// Requires AVX512F.
func M512MaskzDivRoundPs(k x86.Mmask16, a x86.M512, b x86.M512, rounding int) x86.M512 {
	return x86.M512(m512MaskzDivRoundPs(uint16(k), [16]float32(a), [16]float32(b), rounding))
}

func m512MaskzDivRoundPs(k uint16, a [16]float32, b [16]float32, rounding int) [16]float32


// DivRoundSd: Divide the lower double-precision (64-bit) floating-point
// element in 'a' by the lower double-precision (64-bit) floating-point element
// in 'b', store the result in the lower element of 'dst', and copy the upper
// element from 'a' to the upper element of 'dst'.
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := a[63:0] / b[63:0]
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VDIVSD'. Intrinsic: '_mm_div_round_sd'.
// Requires AVX512F.
func DivRoundSd(a x86.M128d, b x86.M128d, rounding int) x86.M128d {
	return x86.M128d(divRoundSd([2]float64(a), [2]float64(b), rounding))
}

func divRoundSd(a [2]float64, b [2]float64, rounding int) [2]float64


// MaskDivRoundSd: Divide the lower double-precision (64-bit) floating-point
// element in 'a' by the lower double-precision (64-bit) floating-point element
// in 'b', store the result in the lower element of 'dst' using writemask 'k'
// (the element is copied from 'src' when mask bit 0 is not set), and copy the
// upper element from 'a' to the upper element of 'dst'. 
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := a[63:0] / b[63:0]
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VDIVSD'. Intrinsic: '_mm_mask_div_round_sd'.
// Requires AVX512F.
func MaskDivRoundSd(src x86.M128d, k x86.Mmask8, a x86.M128d, b x86.M128d, rounding int) x86.M128d {
	return x86.M128d(maskDivRoundSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), rounding))
}

func maskDivRoundSd(src [2]float64, k uint8, a [2]float64, b [2]float64, rounding int) [2]float64


// MaskzDivRoundSd: Divide the lower double-precision (64-bit) floating-point
// element in 'a' by the lower double-precision (64-bit) floating-point element
// in 'b', store the result in the lower element of 'dst' using zeromask 'k'
// (the element is zeroed out when mask bit 0 is not set), and copy the upper
// element from 'a' to the upper element of 'dst'.
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := a[63:0] / b[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VDIVSD'. Intrinsic: '_mm_maskz_div_round_sd'.
// Requires AVX512F.
func MaskzDivRoundSd(k x86.Mmask8, a x86.M128d, b x86.M128d, rounding int) x86.M128d {
	return x86.M128d(maskzDivRoundSd(uint8(k), [2]float64(a), [2]float64(b), rounding))
}

func maskzDivRoundSd(k uint8, a [2]float64, b [2]float64, rounding int) [2]float64


// DivRoundSs: Divide the lower single-precision (32-bit) floating-point
// element in 'a' by the lower single-precision (32-bit) floating-point element
// in 'b', store the result in the lower element of 'dst', and copy the upper 3
// packed elements from 'a' to the upper elements of 'dst'.
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := a[31:0] / b[31:0]
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VDIVSS'. Intrinsic: '_mm_div_round_ss'.
// Requires AVX512F.
func DivRoundSs(a x86.M128, b x86.M128, rounding int) x86.M128 {
	return x86.M128(divRoundSs([4]float32(a), [4]float32(b), rounding))
}

func divRoundSs(a [4]float32, b [4]float32, rounding int) [4]float32


// MaskDivRoundSs: Divide the lower single-precision (32-bit) floating-point
// element in 'a' by the lower single-precision (32-bit) floating-point element
// in 'b', store the result in the lower element of 'dst' using writemask 'k'
// (the element is copied from 'src' when mask bit 0 is not set), and copy the
// upper 3 packed elements from 'a' to the upper elements of 'dst'. 
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := a[31:0] / b[31:0]
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VDIVSS'. Intrinsic: '_mm_mask_div_round_ss'.
// Requires AVX512F.
func MaskDivRoundSs(src x86.M128, k x86.Mmask8, a x86.M128, b x86.M128, rounding int) x86.M128 {
	return x86.M128(maskDivRoundSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), rounding))
}

func maskDivRoundSs(src [4]float32, k uint8, a [4]float32, b [4]float32, rounding int) [4]float32


// MaskzDivRoundSs: Divide the lower single-precision (32-bit) floating-point
// element in 'a' by the lower single-precision (32-bit) floating-point element
// in 'b', store the result in the lower element of 'dst' using zeromask 'k'
// (the element is zeroed out when mask bit 0 is not set), and copy the upper 3
// packed elements from 'a' to the upper elements of 'dst'.
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := a[31:0] / b[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VDIVSS'. Intrinsic: '_mm_maskz_div_round_ss'.
// Requires AVX512F.
func MaskzDivRoundSs(k x86.Mmask8, a x86.M128, b x86.M128, rounding int) x86.M128 {
	return x86.M128(maskzDivRoundSs(uint8(k), [4]float32(a), [4]float32(b), rounding))
}

func maskzDivRoundSs(k uint8, a [4]float32, b [4]float32, rounding int) [4]float32


// MaskDivSd: Divide the lower double-precision (64-bit) floating-point element
// in 'a' by the lower double-precision (64-bit) floating-point element in 'b',
// store the result in the lower element of 'dst' using writemask 'k' (the
// element is copied from 'src' when mask bit 0 is not set), and copy the upper
// element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := a[63:0] / b[63:0]
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VDIVSD'. Intrinsic: '_mm_mask_div_sd'.
// Requires AVX512F.
func MaskDivSd(src x86.M128d, k x86.Mmask8, a x86.M128d, b x86.M128d) x86.M128d {
	return x86.M128d(maskDivSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskDivSd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzDivSd: Divide the lower double-precision (64-bit) floating-point
// element in 'a' by the lower double-precision (64-bit) floating-point element
// in 'b', store the result in the lower element of 'dst' using zeromask 'k'
// (the element is zeroed out when mask bit 0 is not set), and copy the upper
// element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := a[63:0] / b[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VDIVSD'. Intrinsic: '_mm_maskz_div_sd'.
// Requires AVX512F.
func MaskzDivSd(k x86.Mmask8, a x86.M128d, b x86.M128d) x86.M128d {
	return x86.M128d(maskzDivSd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzDivSd(k uint8, a [2]float64, b [2]float64) [2]float64


// MaskDivSs: Divide the lower single-precision (32-bit) floating-point element
// in 'a' by the lower single-precision (32-bit) floating-point element in 'b',
// store the result in the lower element of 'dst' using writemask 'k' (the
// element is copied from 'src' when mask bit 0 is not set), and copy the upper
// 3 packed elements from 'a' to the upper elements of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := a[31:0] / b[31:0]
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VDIVSS'. Intrinsic: '_mm_mask_div_ss'.
// Requires AVX512F.
func MaskDivSs(src x86.M128, k x86.Mmask8, a x86.M128, b x86.M128) x86.M128 {
	return x86.M128(maskDivSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskDivSs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzDivSs: Divide the lower single-precision (32-bit) floating-point
// element in 'a' by the lower single-precision (32-bit) floating-point element
// in 'b', store the result in the lower element of 'dst' using zeromask 'k'
// (the element is zeroed out when mask bit 0 is not set), and copy the upper 3
// packed elements from 'a' to the upper elements of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := a[31:0] / b[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VDIVSS'. Intrinsic: '_mm_maskz_div_ss'.
// Requires AVX512F.
func MaskzDivSs(k x86.Mmask8, a x86.M128, b x86.M128) x86.M128 {
	return x86.M128(maskzDivSs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzDivSs(k uint8, a [4]float32, b [4]float32) [4]float32


// M512ErfPd: Compute the error function of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ERF(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_erf_pd'.
// Requires AVX512F.
func M512ErfPd(a x86.M512d) x86.M512d {
	return x86.M512d(m512ErfPd([8]float64(a)))
}

func m512ErfPd(a [8]float64) [8]float64


// M512MaskErfPd: Compute the error function of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ERF(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_erf_pd'.
// Requires AVX512F.
func M512MaskErfPd(src x86.M512d, k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskErfPd([8]float64(src), uint8(k), [8]float64(a)))
}

func m512MaskErfPd(src [8]float64, k uint8, a [8]float64) [8]float64


// M512ErfPs: Compute the error function of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ERF(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_erf_ps'.
// Requires AVX512F.
func M512ErfPs(a x86.M512) x86.M512 {
	return x86.M512(m512ErfPs([16]float32(a)))
}

func m512ErfPs(a [16]float32) [16]float32


// M512MaskErfPs: Compute the error function of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ERF(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_erf_ps'.
// Requires AVX512F.
func M512MaskErfPs(src x86.M512, k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskErfPs([16]float32(src), uint16(k), [16]float32(a)))
}

func m512MaskErfPs(src [16]float32, k uint16, a [16]float32) [16]float32


// M512ErfcPd: Compute the complementary error function of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := 1.0 - ERF(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_erfc_pd'.
// Requires AVX512F.
func M512ErfcPd(a x86.M512d) x86.M512d {
	return x86.M512d(m512ErfcPd([8]float64(a)))
}

func m512ErfcPd(a [8]float64) [8]float64


// M512MaskErfcPd: Compute the complementary error function of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := 1.0 - ERF(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_erfc_pd'.
// Requires AVX512F.
func M512MaskErfcPd(src x86.M512d, k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskErfcPd([8]float64(src), uint8(k), [8]float64(a)))
}

func m512MaskErfcPd(src [8]float64, k uint8, a [8]float64) [8]float64


// M512ErfcPs: Compute the complementary error function of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := 1.0 - ERF(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_erfc_ps'.
// Requires AVX512F.
func M512ErfcPs(a x86.M512) x86.M512 {
	return x86.M512(m512ErfcPs([16]float32(a)))
}

func m512ErfcPs(a [16]float32) [16]float32


// M512MaskErfcPs: Compute the complementary error function of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := 1.0 - ERF(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_erfc_ps'.
// Requires AVX512F.
func M512MaskErfcPs(src x86.M512, k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskErfcPs([16]float32(src), uint16(k), [16]float32(a)))
}

func m512MaskErfcPs(src [16]float32, k uint16, a [16]float32) [16]float32


// M512ErfcinvPd: Compute the inverse complementary error function of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := 1.0 / (1.0 - ERF(a[i+63:i]))
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_erfcinv_pd'.
// Requires AVX512F.
func M512ErfcinvPd(a x86.M512d) x86.M512d {
	return x86.M512d(m512ErfcinvPd([8]float64(a)))
}

func m512ErfcinvPd(a [8]float64) [8]float64


// M512MaskErfcinvPd: Compute the inverse complementary error function of
// packed double-precision (64-bit) floating-point elements in 'a', and store
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := 1.0 / (1.0 - ERF(a[i+63:i]))
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_erfcinv_pd'.
// Requires AVX512F.
func M512MaskErfcinvPd(src x86.M512d, k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskErfcinvPd([8]float64(src), uint8(k), [8]float64(a)))
}

func m512MaskErfcinvPd(src [8]float64, k uint8, a [8]float64) [8]float64


// M512ErfcinvPs: Compute the inverse complementary error function of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := 1.0 / (1.0 - ERF(a[i+31:i]))
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_erfcinv_ps'.
// Requires AVX512F.
func M512ErfcinvPs(a x86.M512) x86.M512 {
	return x86.M512(m512ErfcinvPs([16]float32(a)))
}

func m512ErfcinvPs(a [16]float32) [16]float32


// M512MaskErfcinvPs: Compute the inverse complementary error function of
// packed single-precision (32-bit) floating-point elements in 'a', and store
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := 1.0 / (1.0 - ERF(a[i+31:i]))
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_erfcinv_ps'.
// Requires AVX512F.
func M512MaskErfcinvPs(src x86.M512, k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskErfcinvPs([16]float32(src), uint16(k), [16]float32(a)))
}

func m512MaskErfcinvPs(src [16]float32, k uint16, a [16]float32) [16]float32


// M512ErfinvPd: Compute the inverse error function of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := 1.0 / ERF(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_erfinv_pd'.
// Requires AVX512F.
func M512ErfinvPd(a x86.M512d) x86.M512d {
	return x86.M512d(m512ErfinvPd([8]float64(a)))
}

func m512ErfinvPd(a [8]float64) [8]float64


// M512MaskErfinvPd: Compute the inverse error function of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := 1.0 / ERF(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_erfinv_pd'.
// Requires AVX512F.
func M512MaskErfinvPd(src x86.M512d, k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskErfinvPd([8]float64(src), uint8(k), [8]float64(a)))
}

func m512MaskErfinvPd(src [8]float64, k uint8, a [8]float64) [8]float64


// M512ErfinvPs: Compute the inverse error function of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := 1.0 / ERF(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_erfinv_ps'.
// Requires AVX512F.
func M512ErfinvPs(a x86.M512) x86.M512 {
	return x86.M512(m512ErfinvPs([16]float32(a)))
}

func m512ErfinvPs(a [16]float32) [16]float32


// M512MaskErfinvPs: Compute the inverse error function of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := 1.0 / ERF(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_erfinv_ps'.
// Requires AVX512F.
func M512MaskErfinvPs(src x86.M512, k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskErfinvPs([16]float32(src), uint16(k), [16]float32(a)))
}

func m512MaskErfinvPs(src [16]float32, k uint16, a [16]float32) [16]float32


// M512ExpPd: Compute the exponential value of 'e' raised to the power of
// packed double-precision (64-bit) floating-point elements in 'a', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := e^(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_exp_pd'.
// Requires AVX512F.
func M512ExpPd(a x86.M512d) x86.M512d {
	return x86.M512d(m512ExpPd([8]float64(a)))
}

func m512ExpPd(a [8]float64) [8]float64


// M512MaskExpPd: Compute the exponential value of 'e' raised to the power of
// packed double-precision (64-bit) floating-point elements in 'a', and store
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := e^(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_exp_pd'.
// Requires AVX512F.
func M512MaskExpPd(src x86.M512d, k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskExpPd([8]float64(src), uint8(k), [8]float64(a)))
}

func m512MaskExpPd(src [8]float64, k uint8, a [8]float64) [8]float64


// M512ExpPs: Compute the exponential value of 'e' raised to the power of
// packed single-precision (32-bit) floating-point elements in 'a', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := e^(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_exp_ps'.
// Requires AVX512F.
func M512ExpPs(a x86.M512) x86.M512 {
	return x86.M512(m512ExpPs([16]float32(a)))
}

func m512ExpPs(a [16]float32) [16]float32


// M512MaskExpPs: Compute the exponential value of 'e' raised to the power of
// packed single-precision (32-bit) floating-point elements in 'a', and store
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := e^(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_exp_ps'.
// Requires AVX512F.
func M512MaskExpPs(src x86.M512, k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskExpPs([16]float32(src), uint16(k), [16]float32(a)))
}

func m512MaskExpPs(src [16]float32, k uint16, a [16]float32) [16]float32


// M512Exp10Pd: Compute the exponential value of 10 raised to the power of
// packed double-precision (64-bit) floating-point elements in 'a', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := 10^(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_exp10_pd'.
// Requires AVX512F.
func M512Exp10Pd(a x86.M512d) x86.M512d {
	return x86.M512d(m512Exp10Pd([8]float64(a)))
}

func m512Exp10Pd(a [8]float64) [8]float64


// M512MaskExp10Pd: Compute the exponential value of 10 raised to the power of
// packed double-precision (64-bit) floating-point elements in 'a', and store
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := 10^(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_exp10_pd'.
// Requires AVX512F.
func M512MaskExp10Pd(src x86.M512d, k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskExp10Pd([8]float64(src), uint8(k), [8]float64(a)))
}

func m512MaskExp10Pd(src [8]float64, k uint8, a [8]float64) [8]float64


// M512Exp10Ps: Compute the exponential value of 10 raised to the power of
// packed single-precision (32-bit) floating-point elements in 'a', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := 10^(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_exp10_ps'.
// Requires AVX512F.
func M512Exp10Ps(a x86.M512) x86.M512 {
	return x86.M512(m512Exp10Ps([16]float32(a)))
}

func m512Exp10Ps(a [16]float32) [16]float32


// M512MaskExp10Ps: Compute the exponential value of 10 raised to the power of
// packed single-precision (32-bit) floating-point elements in 'a', and store
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := 10^(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_exp10_ps'.
// Requires AVX512F.
func M512MaskExp10Ps(src x86.M512, k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskExp10Ps([16]float32(src), uint16(k), [16]float32(a)))
}

func m512MaskExp10Ps(src [16]float32, k uint16, a [16]float32) [16]float32


// M512Exp2Pd: Compute the exponential value of 2 raised to the power of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := 2^(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_exp2_pd'.
// Requires AVX512F.
func M512Exp2Pd(a x86.M512d) x86.M512d {
	return x86.M512d(m512Exp2Pd([8]float64(a)))
}

func m512Exp2Pd(a [8]float64) [8]float64


// M512MaskExp2Pd: Compute the exponential value of 2 raised to the power of
// packed double-precision (64-bit) floating-point elements in 'a', and store
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := 2^(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_exp2_pd'.
// Requires AVX512F.
func M512MaskExp2Pd(src x86.M512d, k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskExp2Pd([8]float64(src), uint8(k), [8]float64(a)))
}

func m512MaskExp2Pd(src [8]float64, k uint8, a [8]float64) [8]float64


// M512Exp2Ps: Compute the exponential value of 2 raised to the power of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := 2^(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_exp2_ps'.
// Requires AVX512F.
func M512Exp2Ps(a x86.M512) x86.M512 {
	return x86.M512(m512Exp2Ps([16]float32(a)))
}

func m512Exp2Ps(a [16]float32) [16]float32


// M512MaskExp2Ps: Compute the exponential value of 2 raised to the power of
// packed single-precision (32-bit) floating-point elements in 'a', and store
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := 2^(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_exp2_ps'.
// Requires AVX512F.
func M512MaskExp2Ps(src x86.M512, k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskExp2Ps([16]float32(src), uint16(k), [16]float32(a)))
}

func m512MaskExp2Ps(src [16]float32, k uint16, a [16]float32) [16]float32


// MaskExpandEpi32: Load contiguous active 32-bit integers from 'a' (those with
// their respective bit set in mask 'k'), and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[m+31:m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPEXPANDD'. Intrinsic: '_mm_mask_expand_epi32'.
// Requires AVX512F.
func MaskExpandEpi32(src x86.M128i, k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskExpandEpi32([16]byte(src), uint8(k), [16]byte(a)))
}

func maskExpandEpi32(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzExpandEpi32: Load contiguous active 32-bit integers from 'a' (those
// with their respective bit set in mask 'k'), and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[m+31:m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPEXPANDD'. Intrinsic: '_mm_maskz_expand_epi32'.
// Requires AVX512F.
func MaskzExpandEpi32(k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskzExpandEpi32(uint8(k), [16]byte(a)))
}

func maskzExpandEpi32(k uint8, a [16]byte) [16]byte


// M256MaskExpandEpi32: Load contiguous active 32-bit integers from 'a' (those
// with their respective bit set in mask 'k'), and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[m+31:m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPEXPANDD'. Intrinsic: '_mm256_mask_expand_epi32'.
// Requires AVX512F.
func M256MaskExpandEpi32(src x86.M256i, k x86.Mmask8, a x86.M256i) x86.M256i {
	return x86.M256i(m256MaskExpandEpi32([32]byte(src), uint8(k), [32]byte(a)))
}

func m256MaskExpandEpi32(src [32]byte, k uint8, a [32]byte) [32]byte


// M256MaskzExpandEpi32: Load contiguous active 32-bit integers from 'a' (those
// with their respective bit set in mask 'k'), and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[m+31:m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPEXPANDD'. Intrinsic: '_mm256_maskz_expand_epi32'.
// Requires AVX512F.
func M256MaskzExpandEpi32(k x86.Mmask8, a x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzExpandEpi32(uint8(k), [32]byte(a)))
}

func m256MaskzExpandEpi32(k uint8, a [32]byte) [32]byte


// M512MaskExpandEpi32: Load contiguous active 32-bit integers from 'a' (those
// with their respective bit set in mask 'k'), and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[m+31:m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPEXPANDD'. Intrinsic: '_mm512_mask_expand_epi32'.
// Requires AVX512F.
func M512MaskExpandEpi32(src x86.M512i, k x86.Mmask16, a x86.M512i) x86.M512i {
	return x86.M512i(m512MaskExpandEpi32([64]byte(src), uint16(k), [64]byte(a)))
}

func m512MaskExpandEpi32(src [64]byte, k uint16, a [64]byte) [64]byte


// M512MaskzExpandEpi32: Load contiguous active 32-bit integers from 'a' (those
// with their respective bit set in mask 'k'), and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		m := 0
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[m+31:m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPEXPANDD'. Intrinsic: '_mm512_maskz_expand_epi32'.
// Requires AVX512F.
func M512MaskzExpandEpi32(k x86.Mmask16, a x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzExpandEpi32(uint16(k), [64]byte(a)))
}

func m512MaskzExpandEpi32(k uint16, a [64]byte) [64]byte


// MaskExpandEpi64: Load contiguous active 64-bit integers from 'a' (those with
// their respective bit set in mask 'k'), and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		m := 0
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[m+63:m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPEXPANDQ'. Intrinsic: '_mm_mask_expand_epi64'.
// Requires AVX512F.
func MaskExpandEpi64(src x86.M128i, k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskExpandEpi64([16]byte(src), uint8(k), [16]byte(a)))
}

func maskExpandEpi64(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzExpandEpi64: Load contiguous active 64-bit integers from 'a' (those
// with their respective bit set in mask 'k'), and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		m := 0
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[m+63:m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPEXPANDQ'. Intrinsic: '_mm_maskz_expand_epi64'.
// Requires AVX512F.
func MaskzExpandEpi64(k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskzExpandEpi64(uint8(k), [16]byte(a)))
}

func maskzExpandEpi64(k uint8, a [16]byte) [16]byte


// M256MaskExpandEpi64: Load contiguous active 64-bit integers from 'a' (those
// with their respective bit set in mask 'k'), and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[m+63:m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPEXPANDQ'. Intrinsic: '_mm256_mask_expand_epi64'.
// Requires AVX512F.
func M256MaskExpandEpi64(src x86.M256i, k x86.Mmask8, a x86.M256i) x86.M256i {
	return x86.M256i(m256MaskExpandEpi64([32]byte(src), uint8(k), [32]byte(a)))
}

func m256MaskExpandEpi64(src [32]byte, k uint8, a [32]byte) [32]byte


// M256MaskzExpandEpi64: Load contiguous active 64-bit integers from 'a' (those
// with their respective bit set in mask 'k'), and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[m+63:m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPEXPANDQ'. Intrinsic: '_mm256_maskz_expand_epi64'.
// Requires AVX512F.
func M256MaskzExpandEpi64(k x86.Mmask8, a x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzExpandEpi64(uint8(k), [32]byte(a)))
}

func m256MaskzExpandEpi64(k uint8, a [32]byte) [32]byte


// M512MaskExpandEpi64: Load contiguous active 64-bit integers from 'a' (those
// with their respective bit set in mask 'k'), and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[m+63:m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPEXPANDQ'. Intrinsic: '_mm512_mask_expand_epi64'.
// Requires AVX512F.
func M512MaskExpandEpi64(src x86.M512i, k x86.Mmask8, a x86.M512i) x86.M512i {
	return x86.M512i(m512MaskExpandEpi64([64]byte(src), uint8(k), [64]byte(a)))
}

func m512MaskExpandEpi64(src [64]byte, k uint8, a [64]byte) [64]byte


// M512MaskzExpandEpi64: Load contiguous active 64-bit integers from 'a' (those
// with their respective bit set in mask 'k'), and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[m+63:m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPEXPANDQ'. Intrinsic: '_mm512_maskz_expand_epi64'.
// Requires AVX512F.
func M512MaskzExpandEpi64(k x86.Mmask8, a x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzExpandEpi64(uint8(k), [64]byte(a)))
}

func m512MaskzExpandEpi64(k uint8, a [64]byte) [64]byte


// MaskExpandPd: Load contiguous active double-precision (64-bit)
// floating-point elements from 'a' (those with their respective bit set in
// mask 'k'), and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[m+63:m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXPANDPD'. Intrinsic: '_mm_mask_expand_pd'.
// Requires AVX512F.
func MaskExpandPd(src x86.M128d, k x86.Mmask8, a x86.M128d) x86.M128d {
	return x86.M128d(maskExpandPd([2]float64(src), uint8(k), [2]float64(a)))
}

func maskExpandPd(src [2]float64, k uint8, a [2]float64) [2]float64


// MaskzExpandPd: Load contiguous active double-precision (64-bit)
// floating-point elements from 'a' (those with their respective bit set in
// mask 'k'), and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[m+63:m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXPANDPD'. Intrinsic: '_mm_maskz_expand_pd'.
// Requires AVX512F.
func MaskzExpandPd(k x86.Mmask8, a x86.M128d) x86.M128d {
	return x86.M128d(maskzExpandPd(uint8(k), [2]float64(a)))
}

func maskzExpandPd(k uint8, a [2]float64) [2]float64


// M256MaskExpandPd: Load contiguous active double-precision (64-bit)
// floating-point elements from 'a' (those with their respective bit set in
// mask 'k'), and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[m+63:m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VEXPANDPD'. Intrinsic: '_mm256_mask_expand_pd'.
// Requires AVX512F.
func M256MaskExpandPd(src x86.M256d, k x86.Mmask8, a x86.M256d) x86.M256d {
	return x86.M256d(m256MaskExpandPd([4]float64(src), uint8(k), [4]float64(a)))
}

func m256MaskExpandPd(src [4]float64, k uint8, a [4]float64) [4]float64


// M256MaskzExpandPd: Load contiguous active double-precision (64-bit)
// floating-point elements from 'a' (those with their respective bit set in
// mask 'k'), and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[m+63:m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VEXPANDPD'. Intrinsic: '_mm256_maskz_expand_pd'.
// Requires AVX512F.
func M256MaskzExpandPd(k x86.Mmask8, a x86.M256d) x86.M256d {
	return x86.M256d(m256MaskzExpandPd(uint8(k), [4]float64(a)))
}

func m256MaskzExpandPd(k uint8, a [4]float64) [4]float64


// M512MaskExpandPd: Load contiguous active double-precision (64-bit)
// floating-point elements from 'a' (those with their respective bit set in
// mask 'k'), and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[m+63:m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VEXPANDPD'. Intrinsic: '_mm512_mask_expand_pd'.
// Requires AVX512F.
func M512MaskExpandPd(src x86.M512d, k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskExpandPd([8]float64(src), uint8(k), [8]float64(a)))
}

func m512MaskExpandPd(src [8]float64, k uint8, a [8]float64) [8]float64


// M512MaskzExpandPd: Load contiguous active double-precision (64-bit)
// floating-point elements from 'a' (those with their respective bit set in
// mask 'k'), and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[m+63:m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VEXPANDPD'. Intrinsic: '_mm512_maskz_expand_pd'.
// Requires AVX512F.
func M512MaskzExpandPd(k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskzExpandPd(uint8(k), [8]float64(a)))
}

func m512MaskzExpandPd(k uint8, a [8]float64) [8]float64


// MaskExpandPs: Load contiguous active single-precision (32-bit)
// floating-point elements from 'a' (those with their respective bit set in
// mask 'k'), and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[m+31:m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXPANDPS'. Intrinsic: '_mm_mask_expand_ps'.
// Requires AVX512F.
func MaskExpandPs(src x86.M128, k x86.Mmask8, a x86.M128) x86.M128 {
	return x86.M128(maskExpandPs([4]float32(src), uint8(k), [4]float32(a)))
}

func maskExpandPs(src [4]float32, k uint8, a [4]float32) [4]float32


// MaskzExpandPs: Load contiguous active single-precision (32-bit)
// floating-point elements from 'a' (those with their respective bit set in
// mask 'k'), and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[m+31:m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXPANDPS'. Intrinsic: '_mm_maskz_expand_ps'.
// Requires AVX512F.
func MaskzExpandPs(k x86.Mmask8, a x86.M128) x86.M128 {
	return x86.M128(maskzExpandPs(uint8(k), [4]float32(a)))
}

func maskzExpandPs(k uint8, a [4]float32) [4]float32


// M256MaskExpandPs: Load contiguous active single-precision (32-bit)
// floating-point elements from 'a' (those with their respective bit set in
// mask 'k'), and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[m+31:m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VEXPANDPS'. Intrinsic: '_mm256_mask_expand_ps'.
// Requires AVX512F.
func M256MaskExpandPs(src x86.M256, k x86.Mmask8, a x86.M256) x86.M256 {
	return x86.M256(m256MaskExpandPs([8]float32(src), uint8(k), [8]float32(a)))
}

func m256MaskExpandPs(src [8]float32, k uint8, a [8]float32) [8]float32


// M256MaskzExpandPs: Load contiguous active single-precision (32-bit)
// floating-point elements from 'a' (those with their respective bit set in
// mask 'k'), and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[m+31:m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VEXPANDPS'. Intrinsic: '_mm256_maskz_expand_ps'.
// Requires AVX512F.
func M256MaskzExpandPs(k x86.Mmask8, a x86.M256) x86.M256 {
	return x86.M256(m256MaskzExpandPs(uint8(k), [8]float32(a)))
}

func m256MaskzExpandPs(k uint8, a [8]float32) [8]float32


// M512MaskExpandPs: Load contiguous active single-precision (32-bit)
// floating-point elements from 'a' (those with their respective bit set in
// mask 'k'), and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[m+31:m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VEXPANDPS'. Intrinsic: '_mm512_mask_expand_ps'.
// Requires AVX512F.
func M512MaskExpandPs(src x86.M512, k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskExpandPs([16]float32(src), uint16(k), [16]float32(a)))
}

func m512MaskExpandPs(src [16]float32, k uint16, a [16]float32) [16]float32


// M512MaskzExpandPs: Load contiguous active single-precision (32-bit)
// floating-point elements from 'a' (those with their respective bit set in
// mask 'k'), and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[m+31:m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VEXPANDPS'. Intrinsic: '_mm512_maskz_expand_ps'.
// Requires AVX512F.
func M512MaskzExpandPs(k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskzExpandPs(uint16(k), [16]float32(a)))
}

func m512MaskzExpandPs(k uint16, a [16]float32) [16]float32


// Skipped: _mm_mask_expandloadu_epi32. Contains pointer parameter.


// Skipped: _mm_maskz_expandloadu_epi32. Contains pointer parameter.


// Skipped: _mm256_mask_expandloadu_epi32. Contains pointer parameter.


// Skipped: _mm256_maskz_expandloadu_epi32. Contains pointer parameter.


// Skipped: _mm512_mask_expandloadu_epi32. Contains pointer parameter.


// Skipped: _mm512_maskz_expandloadu_epi32. Contains pointer parameter.


// Skipped: _mm_mask_expandloadu_epi64. Contains pointer parameter.


// Skipped: _mm_maskz_expandloadu_epi64. Contains pointer parameter.


// Skipped: _mm256_mask_expandloadu_epi64. Contains pointer parameter.


// Skipped: _mm256_maskz_expandloadu_epi64. Contains pointer parameter.


// Skipped: _mm512_mask_expandloadu_epi64. Contains pointer parameter.


// Skipped: _mm512_maskz_expandloadu_epi64. Contains pointer parameter.


// Skipped: _mm_mask_expandloadu_pd. Contains pointer parameter.


// Skipped: _mm_maskz_expandloadu_pd. Contains pointer parameter.


// Skipped: _mm256_mask_expandloadu_pd. Contains pointer parameter.


// Skipped: _mm256_maskz_expandloadu_pd. Contains pointer parameter.


// Skipped: _mm512_mask_expandloadu_pd. Contains pointer parameter.


// Skipped: _mm512_maskz_expandloadu_pd. Contains pointer parameter.


// Skipped: _mm_mask_expandloadu_ps. Contains pointer parameter.


// Skipped: _mm_maskz_expandloadu_ps. Contains pointer parameter.


// Skipped: _mm256_mask_expandloadu_ps. Contains pointer parameter.


// Skipped: _mm256_maskz_expandloadu_ps. Contains pointer parameter.


// Skipped: _mm512_mask_expandloadu_ps. Contains pointer parameter.


// Skipped: _mm512_maskz_expandloadu_ps. Contains pointer parameter.


// M512Expm1Pd: Compute the exponential value of 'e' raised to the power of
// packed double-precision (64-bit) floating-point elements in 'a', subtract
// one from each element, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := e^(a[i+63:i]) - 1.0
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_expm1_pd'.
// Requires AVX512F.
func M512Expm1Pd(a x86.M512d) x86.M512d {
	return x86.M512d(m512Expm1Pd([8]float64(a)))
}

func m512Expm1Pd(a [8]float64) [8]float64


// M512MaskExpm1Pd: Compute the exponential value of 'e' raised to the power of
// packed double-precision (64-bit) floating-point elements in 'a', subtract
// one from each element, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := e^(a[i+63:i]) - 1.0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_expm1_pd'.
// Requires AVX512F.
func M512MaskExpm1Pd(src x86.M512d, k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskExpm1Pd([8]float64(src), uint8(k), [8]float64(a)))
}

func m512MaskExpm1Pd(src [8]float64, k uint8, a [8]float64) [8]float64


// M512Expm1Ps: Compute the exponential value of 'e' raised to the power of
// packed single-precision (32-bit) floating-point elements in 'a', subtract
// one from each element, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := e^(a[i+31:i]) - 1.0
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_expm1_ps'.
// Requires AVX512F.
func M512Expm1Ps(a x86.M512) x86.M512 {
	return x86.M512(m512Expm1Ps([16]float32(a)))
}

func m512Expm1Ps(a [16]float32) [16]float32


// M512MaskExpm1Ps: Compute the exponential value of 'e' raised to the power of
// packed single-precision (32-bit) floating-point elements in 'a', subtract
// one from each element, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := e^(a[i+31:i]) - 1.0
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_expm1_ps'.
// Requires AVX512F.
func M512MaskExpm1Ps(src x86.M512, k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskExpm1Ps([16]float32(src), uint16(k), [16]float32(a)))
}

func m512MaskExpm1Ps(src [16]float32, k uint16, a [16]float32) [16]float32


// M256Extractf32x4Ps: Extract 128 bits (composed of 4 packed single-precision
// (32-bit) floating-point elements) from 'a', selected with 'imm8', and store
// the result in 'dst'. 
//
//		CASE imm8[7:0] of
//		0: dst[127:0] := a[127:0]
//		1: dst[127:0] := a[255:128]
//		ESAC
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTF32X4'. Intrinsic: '_mm256_extractf32x4_ps'.
// Requires AVX512F.
func M256Extractf32x4Ps(a x86.M256, imm8 int) x86.M128 {
	return x86.M128(m256Extractf32x4Ps([8]float32(a), imm8))
}

func m256Extractf32x4Ps(a [8]float32, imm8 int) [4]float32


// M256MaskExtractf32x4Ps: Extract 128 bits (composed of 4 packed
// single-precision (32-bit) floating-point elements) from 'a', selected with
// 'imm8', and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		CASE imm8[7:0] of
//		0: tmp[127:0] := a[127:0]
//		1: tmp[127:0] := a[255:128]
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTF32X4'. Intrinsic: '_mm256_mask_extractf32x4_ps'.
// Requires AVX512F.
func M256MaskExtractf32x4Ps(src x86.M128, k x86.Mmask8, a x86.M256, imm8 int) x86.M128 {
	return x86.M128(m256MaskExtractf32x4Ps([4]float32(src), uint8(k), [8]float32(a), imm8))
}

func m256MaskExtractf32x4Ps(src [4]float32, k uint8, a [8]float32, imm8 int) [4]float32


// M256MaskzExtractf32x4Ps: Extract 128 bits (composed of 4 packed
// single-precision (32-bit) floating-point elements) from 'a', selected with
// 'imm8', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		CASE imm8[7:0] of
//		0: tmp[127:0] := a[127:0]
//		1: tmp[127:0] := a[255:128]
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTF32X4'. Intrinsic: '_mm256_maskz_extractf32x4_ps'.
// Requires AVX512F.
func M256MaskzExtractf32x4Ps(k x86.Mmask8, a x86.M256, imm8 int) x86.M128 {
	return x86.M128(m256MaskzExtractf32x4Ps(uint8(k), [8]float32(a), imm8))
}

func m256MaskzExtractf32x4Ps(k uint8, a [8]float32, imm8 int) [4]float32


// M512Extractf32x4Ps: Extract 128 bits (composed of 4 packed single-precision
// (32-bit) floating-point elements) from 'a', selected with 'imm8', and store
// the result in 'dst'. 
//
//		CASE imm8[7:0] of
//		0: dst[127:0] := a[127:0]
//		1: dst[127:0] := a[255:128]
//		2: dst[127:0] := a[383:256]
//		3: dst[127:0] := a[511:384]
//		ESAC
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTF32X4'. Intrinsic: '_mm512_extractf32x4_ps'.
// Requires AVX512F.
func M512Extractf32x4Ps(a x86.M512, imm8 int) x86.M128 {
	return x86.M128(m512Extractf32x4Ps([16]float32(a), imm8))
}

func m512Extractf32x4Ps(a [16]float32, imm8 int) [4]float32


// M512MaskExtractf32x4Ps: Extract 128 bits (composed of 4 packed
// single-precision (32-bit) floating-point elements) from 'a', selected with
// 'imm8', and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		CASE imm8[7:0] of
//		0: tmp[127:0] := a[127:0]
//		1: tmp[127:0] := a[255:128]
//		2: tmp[127:0] := a[383:256]
//		3: tmp[127:0] := a[511:384]
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTF32X4'. Intrinsic: '_mm512_mask_extractf32x4_ps'.
// Requires AVX512F.
func M512MaskExtractf32x4Ps(src x86.M128, k x86.Mmask8, a x86.M512, imm8 int) x86.M128 {
	return x86.M128(m512MaskExtractf32x4Ps([4]float32(src), uint8(k), [16]float32(a), imm8))
}

func m512MaskExtractf32x4Ps(src [4]float32, k uint8, a [16]float32, imm8 int) [4]float32


// M512MaskzExtractf32x4Ps: Extract 128 bits (composed of 4 packed
// single-precision (32-bit) floating-point elements) from 'a', selected with
// 'imm8', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		CASE imm8[7:0] of
//		0: tmp[127:0] := a[127:0]
//		1: tmp[127:0] := a[255:128]
//		2: tmp[127:0] := a[383:256]
//		3: tmp[127:0] := a[511:384]
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTF32X4'. Intrinsic: '_mm512_maskz_extractf32x4_ps'.
// Requires AVX512F.
func M512MaskzExtractf32x4Ps(k x86.Mmask8, a x86.M512, imm8 int) x86.M128 {
	return x86.M128(m512MaskzExtractf32x4Ps(uint8(k), [16]float32(a), imm8))
}

func m512MaskzExtractf32x4Ps(k uint8, a [16]float32, imm8 int) [4]float32


// M512Extractf64x4Pd: Extract 256 bits (composed of 4 packed double-precision
// (64-bit) floating-point elements) from 'a', selected with 'imm8', and store
// the result in 'dst'. 
//
//		CASE imm8[7:0] of
//		0: dst[255:0] := a[255:0]
//		1: dst[255:0] := a[511:256]
//		ESAC
//		dst[MAX:256] := 0
//
// Instruction: 'VEXTRACTF64X4'. Intrinsic: '_mm512_extractf64x4_pd'.
// Requires AVX512F.
func M512Extractf64x4Pd(a x86.M512d, imm8 int) x86.M256d {
	return x86.M256d(m512Extractf64x4Pd([8]float64(a), imm8))
}

func m512Extractf64x4Pd(a [8]float64, imm8 int) [4]float64


// M512MaskExtractf64x4Pd: Extract 256 bits (composed of 4 packed
// double-precision (64-bit) floating-point elements) from 'a', selected with
// 'imm8', and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		CASE imm8[7:0] of
//		0: tmp[255:0] := a[255:0]
//		1: tmp[255:0] := a[511:256]
//		ESAC
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VEXTRACTF64X4'. Intrinsic: '_mm512_mask_extractf64x4_pd'.
// Requires AVX512F.
func M512MaskExtractf64x4Pd(src x86.M256d, k x86.Mmask8, a x86.M512d, imm8 int) x86.M256d {
	return x86.M256d(m512MaskExtractf64x4Pd([4]float64(src), uint8(k), [8]float64(a), imm8))
}

func m512MaskExtractf64x4Pd(src [4]float64, k uint8, a [8]float64, imm8 int) [4]float64


// M512MaskzExtractf64x4Pd: Extract 256 bits (composed of 4 packed
// double-precision (64-bit) floating-point elements) from 'a', selected with
// 'imm8', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		CASE imm8[7:0] of
//		0: tmp[255:0] := a[255:0]
//		1: tmp[255:0] := a[511:256]
//		ESAC
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VEXTRACTF64X4'. Intrinsic: '_mm512_maskz_extractf64x4_pd'.
// Requires AVX512F.
func M512MaskzExtractf64x4Pd(k x86.Mmask8, a x86.M512d, imm8 int) x86.M256d {
	return x86.M256d(m512MaskzExtractf64x4Pd(uint8(k), [8]float64(a), imm8))
}

func m512MaskzExtractf64x4Pd(k uint8, a [8]float64, imm8 int) [4]float64


// M256Extracti32x4Epi32: Extract 128 bits (composed of 4 packed 32-bit
// integers) from 'a', selected with 'imm8', and store the result in 'dst'. 
//
//		CASE imm8[7:0] of
//		0: dst[127:0] := a[127:0]
//		1: dst[127:0] := a[255:128]
//		ESAC
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTI32X4'. Intrinsic: '_mm256_extracti32x4_epi32'.
// Requires AVX512F.
func M256Extracti32x4Epi32(a x86.M256i, imm8 int) x86.M128i {
	return x86.M128i(m256Extracti32x4Epi32([32]byte(a), imm8))
}

func m256Extracti32x4Epi32(a [32]byte, imm8 int) [16]byte


// M256MaskExtracti32x4Epi32: Extract 128 bits (composed of 4 packed 32-bit
// integers) from 'a', selected with 'imm8', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		CASE imm8[7:0] of
//		0: dst[127:0] := a[127:0]
//		1: dst[127:0] := a[255:128]
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTI32X4'. Intrinsic: '_mm256_mask_extracti32x4_epi32'.
// Requires AVX512F.
func M256MaskExtracti32x4Epi32(src x86.M128i, k x86.Mmask8, a x86.M256i, imm8 int) x86.M128i {
	return x86.M128i(m256MaskExtracti32x4Epi32([16]byte(src), uint8(k), [32]byte(a), imm8))
}

func m256MaskExtracti32x4Epi32(src [16]byte, k uint8, a [32]byte, imm8 int) [16]byte


// M256MaskzExtracti32x4Epi32: Extract 128 bits (composed of 4 packed 32-bit
// integers) from 'a', selected with 'imm8', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		CASE imm8[7:0] of
//		0: dst[127:0] := a[127:0]
//		1: dst[127:0] := a[255:128]
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTI32X4'. Intrinsic: '_mm256_maskz_extracti32x4_epi32'.
// Requires AVX512F.
func M256MaskzExtracti32x4Epi32(k x86.Mmask8, a x86.M256i, imm8 int) x86.M128i {
	return x86.M128i(m256MaskzExtracti32x4Epi32(uint8(k), [32]byte(a), imm8))
}

func m256MaskzExtracti32x4Epi32(k uint8, a [32]byte, imm8 int) [16]byte


// M512Extracti32x4Epi32: Extract 128 bits (composed of 4 packed 32-bit
// integers) from 'a', selected with 'imm8', and store the result in 'dst'. 
//
//		CASE imm8[7:0] of
//		0: dst[127:0] := a[127:0]
//		1: dst[127:0] := a[255:128]
//		2: dst[127:0] := a[383:256]
//		3: dst[127:0] := a[511:384]
//		ESAC
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTI32X4'. Intrinsic: '_mm512_extracti32x4_epi32'.
// Requires AVX512F.
func M512Extracti32x4Epi32(a x86.M512i, imm8 int) x86.M128i {
	return x86.M128i(m512Extracti32x4Epi32([64]byte(a), imm8))
}

func m512Extracti32x4Epi32(a [64]byte, imm8 int) [16]byte


// M512MaskExtracti32x4Epi32: Extract 128 bits (composed of 4 packed 32-bit
// integers) from 'a', selected with 'imm8', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		CASE imm8[7:0] of
//		0: dst[127:0] := a[127:0]
//		1: dst[127:0] := a[255:128]
//		2: dst[127:0] := a[383:256]
//		3: dst[127:0] := a[511:384]
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTI32X4'. Intrinsic: '_mm512_mask_extracti32x4_epi32'.
// Requires AVX512F.
func M512MaskExtracti32x4Epi32(src x86.M128i, k x86.Mmask8, a x86.M512i, imm8 int) x86.M128i {
	return x86.M128i(m512MaskExtracti32x4Epi32([16]byte(src), uint8(k), [64]byte(a), imm8))
}

func m512MaskExtracti32x4Epi32(src [16]byte, k uint8, a [64]byte, imm8 int) [16]byte


// M512MaskzExtracti32x4Epi32: Extract 128 bits (composed of 4 packed 32-bit
// integers) from 'a', selected with 'imm8', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		CASE imm8[7:0] of
//		0: dst[127:0] := a[127:0]
//		1: dst[127:0] := a[255:128]
//		2: dst[127:0] := a[383:256]
//		3: dst[127:0] := a[511:384]
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTI32X4'. Intrinsic: '_mm512_maskz_extracti32x4_epi32'.
// Requires AVX512F.
func M512MaskzExtracti32x4Epi32(k x86.Mmask8, a x86.M512i, imm8 int) x86.M128i {
	return x86.M128i(m512MaskzExtracti32x4Epi32(uint8(k), [64]byte(a), imm8))
}

func m512MaskzExtracti32x4Epi32(k uint8, a [64]byte, imm8 int) [16]byte


// M512Extracti64x4Epi64: Extract 256 bits (composed of 4 packed 64-bit
// integers) from 'a', selected with 'imm8', and store the result in 'dst'. 
//
//		CASE imm8[7:0] of
//		0: dst[255:0] := a[255:0]
//		1: dst[255:0] := a[511:256]
//		ESAC
//		dst[MAX:256] := 0
//
// Instruction: 'VEXTRACTI64X4'. Intrinsic: '_mm512_extracti64x4_epi64'.
// Requires AVX512F.
func M512Extracti64x4Epi64(a x86.M512i, imm8 int) x86.M256i {
	return x86.M256i(m512Extracti64x4Epi64([64]byte(a), imm8))
}

func m512Extracti64x4Epi64(a [64]byte, imm8 int) [32]byte


// M512MaskExtracti64x4Epi64: Extract 256 bits (composed of 4 packed 64-bit
// integers) from 'a', selected with 'imm8', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		CASE imm8[7:0] of
//		0: dst[255:0] := a[255:0]
//		1: dst[255:0] := a[511:256]
//		ESAC
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VEXTRACTI64X4'. Intrinsic: '_mm512_mask_extracti64x4_epi64'.
// Requires AVX512F.
func M512MaskExtracti64x4Epi64(src x86.M256i, k x86.Mmask8, a x86.M512i, imm8 int) x86.M256i {
	return x86.M256i(m512MaskExtracti64x4Epi64([32]byte(src), uint8(k), [64]byte(a), imm8))
}

func m512MaskExtracti64x4Epi64(src [32]byte, k uint8, a [64]byte, imm8 int) [32]byte


// M512MaskzExtracti64x4Epi64: Extract 256 bits (composed of 4 packed 64-bit
// integers) from 'a', selected with 'imm8', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		CASE imm8[7:0] of
//		0: dst[255:0] := a[255:0]
//		1: dst[255:0] := a[511:256]
//		ESAC
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VEXTRACTI64X4'. Intrinsic: '_mm512_maskz_extracti64x4_epi64'.
// Requires AVX512F.
func M512MaskzExtracti64x4Epi64(k x86.Mmask8, a x86.M512i, imm8 int) x86.M256i {
	return x86.M256i(m512MaskzExtracti64x4Epi64(uint8(k), [64]byte(a), imm8))
}

func m512MaskzExtracti64x4Epi64(k uint8, a [64]byte, imm8 int) [32]byte


// FixupimmPd: Fix up packed double-precision (64-bit) floating-point elements
// in 'a' and 'b' using packed 64-bit integers in 'c', and store the results in
// 'dst'. 'imm8' is used to set the required flags reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMPD'. Intrinsic: '_mm_fixupimm_pd'.
// Requires AVX512F.
func FixupimmPd(a x86.M128d, b x86.M128d, c x86.M128i, imm8 int) x86.M128d {
	return x86.M128d(fixupimmPd([2]float64(a), [2]float64(b), [16]byte(c), imm8))
}

func fixupimmPd(a [2]float64, b [2]float64, c [16]byte, imm8 int) [2]float64


// MaskFixupimmPd: Fix up packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' using packed 64-bit integers in 'c', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMPD'. Intrinsic: '_mm_mask_fixupimm_pd'.
// Requires AVX512F.
func MaskFixupimmPd(a x86.M128d, k x86.Mmask8, b x86.M128d, c x86.M128i, imm8 int) x86.M128d {
	return x86.M128d(maskFixupimmPd([2]float64(a), uint8(k), [2]float64(b), [16]byte(c), imm8))
}

func maskFixupimmPd(a [2]float64, k uint8, b [2]float64, c [16]byte, imm8 int) [2]float64


// MaskzFixupimmPd: Fix up packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' using packed 64-bit integers in 'c', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMPD'. Intrinsic: '_mm_maskz_fixupimm_pd'.
// Requires AVX512F.
func MaskzFixupimmPd(k x86.Mmask8, a x86.M128d, b x86.M128d, c x86.M128i, imm8 int) x86.M128d {
	return x86.M128d(maskzFixupimmPd(uint8(k), [2]float64(a), [2]float64(b), [16]byte(c), imm8))
}

func maskzFixupimmPd(k uint8, a [2]float64, b [2]float64, c [16]byte, imm8 int) [2]float64


// M256FixupimmPd: Fix up packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' using packed 64-bit integers in 'c', and store the
// results in 'dst'. 'imm8' is used to set the required flags reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFIXUPIMMPD'. Intrinsic: '_mm256_fixupimm_pd'.
// Requires AVX512F.
func M256FixupimmPd(a x86.M256d, b x86.M256d, c x86.M256i, imm8 int) x86.M256d {
	return x86.M256d(m256FixupimmPd([4]float64(a), [4]float64(b), [32]byte(c), imm8))
}

func m256FixupimmPd(a [4]float64, b [4]float64, c [32]byte, imm8 int) [4]float64


// M256MaskFixupimmPd: Fix up packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' using packed 64-bit integers in 'c', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFIXUPIMMPD'. Intrinsic: '_mm256_mask_fixupimm_pd'.
// Requires AVX512F.
func M256MaskFixupimmPd(a x86.M256d, k x86.Mmask8, b x86.M256d, c x86.M256i, imm8 int) x86.M256d {
	return x86.M256d(m256MaskFixupimmPd([4]float64(a), uint8(k), [4]float64(b), [32]byte(c), imm8))
}

func m256MaskFixupimmPd(a [4]float64, k uint8, b [4]float64, c [32]byte, imm8 int) [4]float64


// M256MaskzFixupimmPd: Fix up packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' using packed 64-bit integers in 'c', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFIXUPIMMPD'. Intrinsic: '_mm256_maskz_fixupimm_pd'.
// Requires AVX512F.
func M256MaskzFixupimmPd(k x86.Mmask8, a x86.M256d, b x86.M256d, c x86.M256i, imm8 int) x86.M256d {
	return x86.M256d(m256MaskzFixupimmPd(uint8(k), [4]float64(a), [4]float64(b), [32]byte(c), imm8))
}

func m256MaskzFixupimmPd(k uint8, a [4]float64, b [4]float64, c [32]byte, imm8 int) [4]float64


// M512FixupimmPd: Fix up packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' using packed 64-bit integers in 'c', and store the
// results in 'dst'. 'imm8' is used to set the required flags reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFIXUPIMMPD'. Intrinsic: '_mm512_fixupimm_pd'.
// Requires AVX512F.
func M512FixupimmPd(a x86.M512d, b x86.M512d, c x86.M512i, imm8 int) x86.M512d {
	return x86.M512d(m512FixupimmPd([8]float64(a), [8]float64(b), [64]byte(c), imm8))
}

func m512FixupimmPd(a [8]float64, b [8]float64, c [64]byte, imm8 int) [8]float64


// M512MaskFixupimmPd: Fix up packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' using packed 64-bit integers in 'c', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFIXUPIMMPD'. Intrinsic: '_mm512_mask_fixupimm_pd'.
// Requires AVX512F.
func M512MaskFixupimmPd(a x86.M512d, k x86.Mmask8, b x86.M512d, c x86.M512i, imm8 int) x86.M512d {
	return x86.M512d(m512MaskFixupimmPd([8]float64(a), uint8(k), [8]float64(b), [64]byte(c), imm8))
}

func m512MaskFixupimmPd(a [8]float64, k uint8, b [8]float64, c [64]byte, imm8 int) [8]float64


// M512MaskzFixupimmPd: Fix up packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' using packed 64-bit integers in 'c', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFIXUPIMMPD'. Intrinsic: '_mm512_maskz_fixupimm_pd'.
// Requires AVX512F.
func M512MaskzFixupimmPd(k x86.Mmask8, a x86.M512d, b x86.M512d, c x86.M512i, imm8 int) x86.M512d {
	return x86.M512d(m512MaskzFixupimmPd(uint8(k), [8]float64(a), [8]float64(b), [64]byte(c), imm8))
}

func m512MaskzFixupimmPd(k uint8, a [8]float64, b [8]float64, c [64]byte, imm8 int) [8]float64


// FixupimmPs: Fix up packed single-precision (32-bit) floating-point elements
// in 'a' and 'b' using packed 32-bit integers in 'c', and store the results in
// 'dst'. 'imm8' is used to set the required flags reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMPS'. Intrinsic: '_mm_fixupimm_ps'.
// Requires AVX512F.
func FixupimmPs(a x86.M128, b x86.M128, c x86.M128i, imm8 int) x86.M128 {
	return x86.M128(fixupimmPs([4]float32(a), [4]float32(b), [16]byte(c), imm8))
}

func fixupimmPs(a [4]float32, b [4]float32, c [16]byte, imm8 int) [4]float32


// MaskFixupimmPs: Fix up packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' using packed 32-bit integers in 'c', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMPS'. Intrinsic: '_mm_mask_fixupimm_ps'.
// Requires AVX512F.
func MaskFixupimmPs(a x86.M128, k x86.Mmask8, b x86.M128, c x86.M128i, imm8 int) x86.M128 {
	return x86.M128(maskFixupimmPs([4]float32(a), uint8(k), [4]float32(b), [16]byte(c), imm8))
}

func maskFixupimmPs(a [4]float32, k uint8, b [4]float32, c [16]byte, imm8 int) [4]float32


// MaskzFixupimmPs: Fix up packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' using packed 32-bit integers in 'c', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMPS'. Intrinsic: '_mm_maskz_fixupimm_ps'.
// Requires AVX512F.
func MaskzFixupimmPs(k x86.Mmask8, a x86.M128, b x86.M128, c x86.M128i, imm8 int) x86.M128 {
	return x86.M128(maskzFixupimmPs(uint8(k), [4]float32(a), [4]float32(b), [16]byte(c), imm8))
}

func maskzFixupimmPs(k uint8, a [4]float32, b [4]float32, c [16]byte, imm8 int) [4]float32


// M256FixupimmPs: Fix up packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' using packed 32-bit integers in 'c', and store the
// results in 'dst'. 'imm8' is used to set the required flags reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFIXUPIMMPS'. Intrinsic: '_mm256_fixupimm_ps'.
// Requires AVX512F.
func M256FixupimmPs(a x86.M256, b x86.M256, c x86.M256i, imm8 int) x86.M256 {
	return x86.M256(m256FixupimmPs([8]float32(a), [8]float32(b), [32]byte(c), imm8))
}

func m256FixupimmPs(a [8]float32, b [8]float32, c [32]byte, imm8 int) [8]float32


// M256MaskFixupimmPs: Fix up packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' using packed 32-bit integers in 'c', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFIXUPIMMPS'. Intrinsic: '_mm256_mask_fixupimm_ps'.
// Requires AVX512F.
func M256MaskFixupimmPs(a x86.M256, k x86.Mmask8, b x86.M256, c x86.M256i, imm8 int) x86.M256 {
	return x86.M256(m256MaskFixupimmPs([8]float32(a), uint8(k), [8]float32(b), [32]byte(c), imm8))
}

func m256MaskFixupimmPs(a [8]float32, k uint8, b [8]float32, c [32]byte, imm8 int) [8]float32


// M256MaskzFixupimmPs: Fix up packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' using packed 32-bit integers in 'c', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFIXUPIMMPS'. Intrinsic: '_mm256_maskz_fixupimm_ps'.
// Requires AVX512F.
func M256MaskzFixupimmPs(k x86.Mmask8, a x86.M256, b x86.M256, c x86.M256i, imm8 int) x86.M256 {
	return x86.M256(m256MaskzFixupimmPs(uint8(k), [8]float32(a), [8]float32(b), [32]byte(c), imm8))
}

func m256MaskzFixupimmPs(k uint8, a [8]float32, b [8]float32, c [32]byte, imm8 int) [8]float32


// M512FixupimmPs: Fix up packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' using packed 32-bit integers in 'c', and store the
// results in 'dst'. 'imm8' is used to set the required flags reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFIXUPIMMPS'. Intrinsic: '_mm512_fixupimm_ps'.
// Requires AVX512F.
func M512FixupimmPs(a x86.M512, b x86.M512, c x86.M512i, imm8 int) x86.M512 {
	return x86.M512(m512FixupimmPs([16]float32(a), [16]float32(b), [64]byte(c), imm8))
}

func m512FixupimmPs(a [16]float32, b [16]float32, c [64]byte, imm8 int) [16]float32


// M512MaskFixupimmPs: Fix up packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' using packed 32-bit integers in 'c', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFIXUPIMMPS'. Intrinsic: '_mm512_mask_fixupimm_ps'.
// Requires AVX512F.
func M512MaskFixupimmPs(a x86.M512, k x86.Mmask16, b x86.M512, c x86.M512i, imm8 int) x86.M512 {
	return x86.M512(m512MaskFixupimmPs([16]float32(a), uint16(k), [16]float32(b), [64]byte(c), imm8))
}

func m512MaskFixupimmPs(a [16]float32, k uint16, b [16]float32, c [64]byte, imm8 int) [16]float32


// M512MaskzFixupimmPs: Fix up packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' using packed 32-bit integers in 'c', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFIXUPIMMPS'. Intrinsic: '_mm512_maskz_fixupimm_ps'.
// Requires AVX512F.
func M512MaskzFixupimmPs(k x86.Mmask16, a x86.M512, b x86.M512, c x86.M512i, imm8 int) x86.M512 {
	return x86.M512(m512MaskzFixupimmPs(uint16(k), [16]float32(a), [16]float32(b), [64]byte(c), imm8))
}

func m512MaskzFixupimmPs(k uint16, a [16]float32, b [16]float32, c [64]byte, imm8 int) [16]float32


// M512FixupimmRoundPd: Fix up packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' using packed 64-bit integers in 'c', and store the
// results in 'dst'. 'imm8' is used to set the required flags reporting.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFIXUPIMMPD'. Intrinsic: '_mm512_fixupimm_round_pd'.
// Requires AVX512F.
func M512FixupimmRoundPd(a x86.M512d, b x86.M512d, c x86.M512i, imm8 int, rounding int) x86.M512d {
	return x86.M512d(m512FixupimmRoundPd([8]float64(a), [8]float64(b), [64]byte(c), imm8, rounding))
}

func m512FixupimmRoundPd(a [8]float64, b [8]float64, c [64]byte, imm8 int, rounding int) [8]float64


// M512MaskFixupimmRoundPd: Fix up packed double-precision (64-bit)
// floating-point elements in 'a' and 'b' using packed 64-bit integers in 'c',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'a' when the corresponding mask bit is not set). 'imm8' is used to set the
// required flags reporting.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFIXUPIMMPD'. Intrinsic: '_mm512_mask_fixupimm_round_pd'.
// Requires AVX512F.
func M512MaskFixupimmRoundPd(a x86.M512d, k x86.Mmask8, b x86.M512d, c x86.M512i, imm8 int, rounding int) x86.M512d {
	return x86.M512d(m512MaskFixupimmRoundPd([8]float64(a), uint8(k), [8]float64(b), [64]byte(c), imm8, rounding))
}

func m512MaskFixupimmRoundPd(a [8]float64, k uint8, b [8]float64, c [64]byte, imm8 int, rounding int) [8]float64


// M512MaskzFixupimmRoundPd: Fix up packed double-precision (64-bit)
// floating-point elements in 'a' and 'b' using packed 64-bit integers in 'c',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 'imm8' is used to set the
// required flags reporting.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFIXUPIMMPD'. Intrinsic: '_mm512_maskz_fixupimm_round_pd'.
// Requires AVX512F.
func M512MaskzFixupimmRoundPd(k x86.Mmask8, a x86.M512d, b x86.M512d, c x86.M512i, imm8 int, rounding int) x86.M512d {
	return x86.M512d(m512MaskzFixupimmRoundPd(uint8(k), [8]float64(a), [8]float64(b), [64]byte(c), imm8, rounding))
}

func m512MaskzFixupimmRoundPd(k uint8, a [8]float64, b [8]float64, c [64]byte, imm8 int, rounding int) [8]float64


// M512FixupimmRoundPs: Fix up packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' using packed 32-bit integers in 'c', and store the
// results in 'dst'. 'imm8' is used to set the required flags reporting.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFIXUPIMMPS'. Intrinsic: '_mm512_fixupimm_round_ps'.
// Requires AVX512F.
func M512FixupimmRoundPs(a x86.M512, b x86.M512, c x86.M512i, imm8 int, rounding int) x86.M512 {
	return x86.M512(m512FixupimmRoundPs([16]float32(a), [16]float32(b), [64]byte(c), imm8, rounding))
}

func m512FixupimmRoundPs(a [16]float32, b [16]float32, c [64]byte, imm8 int, rounding int) [16]float32


// M512MaskFixupimmRoundPs: Fix up packed single-precision (32-bit)
// floating-point elements in 'a' and 'b' using packed 32-bit integers in 'c',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'a' when the corresponding mask bit is not set). 'imm8' is used to set the
// required flags reporting.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFIXUPIMMPS'. Intrinsic: '_mm512_mask_fixupimm_round_ps'.
// Requires AVX512F.
func M512MaskFixupimmRoundPs(a x86.M512, k x86.Mmask16, b x86.M512, c x86.M512i, imm8 int, rounding int) x86.M512 {
	return x86.M512(m512MaskFixupimmRoundPs([16]float32(a), uint16(k), [16]float32(b), [64]byte(c), imm8, rounding))
}

func m512MaskFixupimmRoundPs(a [16]float32, k uint16, b [16]float32, c [64]byte, imm8 int, rounding int) [16]float32


// M512MaskzFixupimmRoundPs: Fix up packed single-precision (32-bit)
// floating-point elements in 'a' and 'b' using packed 32-bit integers in 'c',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 'imm8' is used to set the
// required flags reporting.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFIXUPIMMPS'. Intrinsic: '_mm512_maskz_fixupimm_round_ps'.
// Requires AVX512F.
func M512MaskzFixupimmRoundPs(k x86.Mmask16, a x86.M512, b x86.M512, c x86.M512i, imm8 int, rounding int) x86.M512 {
	return x86.M512(m512MaskzFixupimmRoundPs(uint16(k), [16]float32(a), [16]float32(b), [64]byte(c), imm8, rounding))
}

func m512MaskzFixupimmRoundPs(k uint16, a [16]float32, b [16]float32, c [64]byte, imm8 int, rounding int) [16]float32


// FixupimmRoundSd: Fix up the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b' using the lower 64-bit integer in 'c', store the
// result in the lower element of 'dst', and copy the upper element from 'a' to
// the upper element of 'dst'. 'imm8' is used to set the required flags
// reporting.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		dst[63:0] := FIXUPIMMPD(a[63:0], b[63:0], c[63:0], imm8[7:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMSD'. Intrinsic: '_mm_fixupimm_round_sd'.
// Requires AVX512F.
func FixupimmRoundSd(a x86.M128d, b x86.M128d, c x86.M128i, imm8 int, rounding int) x86.M128d {
	return x86.M128d(fixupimmRoundSd([2]float64(a), [2]float64(b), [16]byte(c), imm8, rounding))
}

func fixupimmRoundSd(a [2]float64, b [2]float64, c [16]byte, imm8 int, rounding int) [2]float64


// MaskFixupimmRoundSd: Fix up the lower double-precision (64-bit)
// floating-point elements in 'a' and 'b' using the lower 64-bit integer in
// 'c', store the result in the lower element of 'dst' using writemask 'k' (the
// element is copied from 'a' when mask bit 0 is not set), and copy the upper
// element from 'a' to the upper element of 'dst'. 'imm8' is used to set the
// required flags reporting.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		IF k[0]
//			dst[63:0] := FIXUPIMMPD(a[63:0], b[63:0], c[63:0], imm8[7:0])
//		ELSE
//			dst[63:0] := a[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMSD'. Intrinsic: '_mm_mask_fixupimm_round_sd'.
// Requires AVX512F.
func MaskFixupimmRoundSd(a x86.M128d, k x86.Mmask8, b x86.M128d, c x86.M128i, imm8 int, rounding int) x86.M128d {
	return x86.M128d(maskFixupimmRoundSd([2]float64(a), uint8(k), [2]float64(b), [16]byte(c), imm8, rounding))
}

func maskFixupimmRoundSd(a [2]float64, k uint8, b [2]float64, c [16]byte, imm8 int, rounding int) [2]float64


// MaskzFixupimmRoundSd: Fix up the lower double-precision (64-bit)
// floating-point elements in 'a' and 'b' using the lower 64-bit integer in
// 'c', store the result in the lower element of 'dst' using zeromask 'k' (the
// element is zeroed out when mask bit 0 is not set), and copy the upper
// element from 'a' to the upper element of 'dst'. 'imm8' is used to set the
// required flags reporting.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		IF k[0]
//			dst[63:0] := FIXUPIMMPD(a[63:0], b[63:0], c[63:0], imm8[7:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMSD'. Intrinsic: '_mm_maskz_fixupimm_round_sd'.
// Requires AVX512F.
func MaskzFixupimmRoundSd(k x86.Mmask8, a x86.M128d, b x86.M128d, c x86.M128i, imm8 int, rounding int) x86.M128d {
	return x86.M128d(maskzFixupimmRoundSd(uint8(k), [2]float64(a), [2]float64(b), [16]byte(c), imm8, rounding))
}

func maskzFixupimmRoundSd(k uint8, a [2]float64, b [2]float64, c [16]byte, imm8 int, rounding int) [2]float64


// FixupimmRoundSs: Fix up the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b' using the lower 32-bit integer in 'c', store the
// result in the lower element of 'dst', and copy the upper 3 packed elements
// from 'a' to the upper elements of 'dst'. 'imm8' is used to set the required
// flags reporting.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		dst[31:0] := FIXUPIMMPD(a[31:0], b[31:0], c[31:0], imm8[7:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMSS'. Intrinsic: '_mm_fixupimm_round_ss'.
// Requires AVX512F.
func FixupimmRoundSs(a x86.M128, b x86.M128, c x86.M128i, imm8 int, rounding int) x86.M128 {
	return x86.M128(fixupimmRoundSs([4]float32(a), [4]float32(b), [16]byte(c), imm8, rounding))
}

func fixupimmRoundSs(a [4]float32, b [4]float32, c [16]byte, imm8 int, rounding int) [4]float32


// MaskFixupimmRoundSs: Fix up the lower single-precision (32-bit)
// floating-point elements in 'a' and 'b' using the lower 32-bit integer in
// 'c', store the result in the lower element of 'dst' using writemask 'k' (the
// element is copied from 'a' when mask bit 0 is not set), and copy the upper 3
// packed elements from 'a' to the upper elements of 'dst'. 'imm8' is used to
// set the required flags reporting.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		IF k[0]
//			dst[31:0] := FIXUPIMMPD(a[31:0], b[31:0], c[31:0], imm8[7:0])
//		ELSE
//			dst[31:0] := a[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMSS'. Intrinsic: '_mm_mask_fixupimm_round_ss'.
// Requires AVX512F.
func MaskFixupimmRoundSs(a x86.M128, k x86.Mmask8, b x86.M128, c x86.M128i, imm8 int, rounding int) x86.M128 {
	return x86.M128(maskFixupimmRoundSs([4]float32(a), uint8(k), [4]float32(b), [16]byte(c), imm8, rounding))
}

func maskFixupimmRoundSs(a [4]float32, k uint8, b [4]float32, c [16]byte, imm8 int, rounding int) [4]float32


// MaskzFixupimmRoundSs: Fix up the lower single-precision (32-bit)
// floating-point elements in 'a' and 'b' using the lower 32-bit integer in
// 'c', store the result in the lower element of 'dst' using zeromask 'k' (the
// element is zeroed out when mask bit 0 is not set), and copy the upper 3
// packed elements from 'a' to the upper elements of 'dst'. 'imm8' is used to
// set the required flags reporting.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		IF k[0]
//			dst[31:0] := FIXUPIMMPD(a[31:0], b[31:0], c[31:0], imm8[7:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMSS'. Intrinsic: '_mm_maskz_fixupimm_round_ss'.
// Requires AVX512F.
func MaskzFixupimmRoundSs(k x86.Mmask8, a x86.M128, b x86.M128, c x86.M128i, imm8 int, rounding int) x86.M128 {
	return x86.M128(maskzFixupimmRoundSs(uint8(k), [4]float32(a), [4]float32(b), [16]byte(c), imm8, rounding))
}

func maskzFixupimmRoundSs(k uint8, a [4]float32, b [4]float32, c [16]byte, imm8 int, rounding int) [4]float32


// FixupimmSd: Fix up the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b' using the lower 64-bit integer in 'c', store the
// result in the lower element of 'dst', and copy the upper element from 'a' to
// the upper element of 'dst'. 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		dst[63:0] := FIXUPIMMPD(a[63:0], b[63:0], c[63:0], imm8[7:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMSD'. Intrinsic: '_mm_fixupimm_sd'.
// Requires AVX512F.
func FixupimmSd(a x86.M128d, b x86.M128d, c x86.M128i, imm8 int) x86.M128d {
	return x86.M128d(fixupimmSd([2]float64(a), [2]float64(b), [16]byte(c), imm8))
}

func fixupimmSd(a [2]float64, b [2]float64, c [16]byte, imm8 int) [2]float64


// MaskFixupimmSd: Fix up the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b' using the lower 64-bit integer in 'c', store the
// result in the lower element of 'dst' using writemask 'k' (the element is
// copied from 'a' when mask bit 0 is not set), and copy the upper element from
// 'a' to the upper element of 'dst'. 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		IF k[0]
//			dst[63:0] := FIXUPIMMPD(a[63:0], b[63:0], c[63:0], imm8[7:0])
//		ELSE
//			dst[63:0] := a[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMSD'. Intrinsic: '_mm_mask_fixupimm_sd'.
// Requires AVX512F.
func MaskFixupimmSd(a x86.M128d, k x86.Mmask8, b x86.M128d, c x86.M128i, imm8 int) x86.M128d {
	return x86.M128d(maskFixupimmSd([2]float64(a), uint8(k), [2]float64(b), [16]byte(c), imm8))
}

func maskFixupimmSd(a [2]float64, k uint8, b [2]float64, c [16]byte, imm8 int) [2]float64


// MaskzFixupimmSd: Fix up the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b' using the lower 64-bit integer in 'c', store the
// result in the lower element of 'dst' using zeromask 'k' (the element is
// zeroed out when mask bit 0 is not set), and copy the upper element from 'a'
// to the upper element of 'dst'. 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		IF k[0]
//			dst[63:0] := FIXUPIMMPD(a[63:0], b[63:0], c[63:0], imm8[7:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMSD'. Intrinsic: '_mm_maskz_fixupimm_sd'.
// Requires AVX512F.
func MaskzFixupimmSd(k x86.Mmask8, a x86.M128d, b x86.M128d, c x86.M128i, imm8 int) x86.M128d {
	return x86.M128d(maskzFixupimmSd(uint8(k), [2]float64(a), [2]float64(b), [16]byte(c), imm8))
}

func maskzFixupimmSd(k uint8, a [2]float64, b [2]float64, c [16]byte, imm8 int) [2]float64


// FixupimmSs: Fix up the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b' using the lower 32-bit integer in 'c', store the
// result in the lower element of 'dst', and copy the upper 3 packed elements
// from 'a' to the upper elements of 'dst'. 'imm8' is used to set the required
// flags reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		dst[31:0] := FIXUPIMMPD(a[31:0], b[31:0], c[31:0], imm8[7:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMSS'. Intrinsic: '_mm_fixupimm_ss'.
// Requires AVX512F.
func FixupimmSs(a x86.M128, b x86.M128, c x86.M128i, imm8 int) x86.M128 {
	return x86.M128(fixupimmSs([4]float32(a), [4]float32(b), [16]byte(c), imm8))
}

func fixupimmSs(a [4]float32, b [4]float32, c [16]byte, imm8 int) [4]float32


// MaskFixupimmSs: Fix up the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b' using the lower 32-bit integer in 'c', store the
// result in the lower element of 'dst' using writemask 'k' (the element is
// copied from 'a' when mask bit 0 is not set), and copy the upper 3 packed
// elements from 'a' to the upper elements of 'dst'. 'imm8' is used to set the
// required flags reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		IF k[0]
//			dst[31:0] := FIXUPIMMPD(a[31:0], b[31:0], c[31:0], imm8[7:0])
//		ELSE
//			dst[31:0] := a[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMSS'. Intrinsic: '_mm_mask_fixupimm_ss'.
// Requires AVX512F.
func MaskFixupimmSs(a x86.M128, k x86.Mmask8, b x86.M128, c x86.M128i, imm8 int) x86.M128 {
	return x86.M128(maskFixupimmSs([4]float32(a), uint8(k), [4]float32(b), [16]byte(c), imm8))
}

func maskFixupimmSs(a [4]float32, k uint8, b [4]float32, c [16]byte, imm8 int) [4]float32


// MaskzFixupimmSs: Fix up the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b' using the lower 32-bit integer in 'c', store the
// result in the lower element of 'dst' using zeromask 'k' (the element is
// zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements
// from 'a' to the upper elements of 'dst'. 'imm8' is used to set the required
// flags reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		IF k[0]
//			dst[31:0] := FIXUPIMMPD(a[31:0], b[31:0], c[31:0], imm8[7:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMSS'. Intrinsic: '_mm_maskz_fixupimm_ss'.
// Requires AVX512F.
func MaskzFixupimmSs(k x86.Mmask8, a x86.M128, b x86.M128, c x86.M128i, imm8 int) x86.M128 {
	return x86.M128(maskzFixupimmSs(uint8(k), [4]float32(a), [4]float32(b), [16]byte(c), imm8))
}

func maskzFixupimmSs(k uint8, a [4]float32, b [4]float32, c [16]byte, imm8 int) [4]float32


// M512FloorPd: Round the packed double-precision (64-bit) floating-point
// elements in 'a' down to an integer value, and store the results as packed
// double-precision floating-point elements in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := FLOOR(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_floor_pd'.
// Requires AVX512F.
func M512FloorPd(a x86.M512d) x86.M512d {
	return x86.M512d(m512FloorPd([8]float64(a)))
}

func m512FloorPd(a [8]float64) [8]float64


// M512MaskFloorPd: Round the packed double-precision (64-bit) floating-point
// elements in 'a' down to an integer value, and store the results as packed
// double-precision floating-point elements in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := FLOOR(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_floor_pd'.
// Requires AVX512F.
func M512MaskFloorPd(src x86.M512d, k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskFloorPd([8]float64(src), uint8(k), [8]float64(a)))
}

func m512MaskFloorPd(src [8]float64, k uint8, a [8]float64) [8]float64


// M512FloorPs: Round the packed single-precision (32-bit) floating-point
// elements in 'a' down to an integer value, and store the results as packed
// single-precision floating-point elements in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := FLOOR(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_floor_ps'.
// Requires AVX512F.
func M512FloorPs(a x86.M512) x86.M512 {
	return x86.M512(m512FloorPs([16]float32(a)))
}

func m512FloorPs(a [16]float32) [16]float32


// M512MaskFloorPs: Round the packed single-precision (32-bit) floating-point
// elements in 'a' down to an integer value, and store the results as packed
// single-precision floating-point elements in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := FLOOR(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_floor_ps'.
// Requires AVX512F.
func M512MaskFloorPs(src x86.M512, k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskFloorPs([16]float32(src), uint16(k), [16]float32(a)))
}

func m512MaskFloorPs(src [16]float32, k uint16, a [16]float32) [16]float32


// MaskFmaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132PD, VFMADD213PD, VFMADD231PD'. Intrinsic: '_mm_mask_fmadd_pd'.
// Requires AVX512F.
func MaskFmaddPd(a x86.M128d, k x86.Mmask8, b x86.M128d, c x86.M128d) x86.M128d {
	return x86.M128d(maskFmaddPd([2]float64(a), uint8(k), [2]float64(b), [2]float64(c)))
}

func maskFmaddPd(a [2]float64, k uint8, b [2]float64, c [2]float64) [2]float64


// Mask3FmaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132PD, VFMADD213PD, VFMADD231PD'. Intrinsic: '_mm_mask3_fmadd_pd'.
// Requires AVX512F.
func Mask3FmaddPd(a x86.M128d, b x86.M128d, c x86.M128d, k x86.Mmask8) x86.M128d {
	return x86.M128d(mask3FmaddPd([2]float64(a), [2]float64(b), [2]float64(c), uint8(k)))
}

func mask3FmaddPd(a [2]float64, b [2]float64, c [2]float64, k uint8) [2]float64


// MaskzFmaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132PD, VFMADD213PD, VFMADD231PD'. Intrinsic: '_mm_maskz_fmadd_pd'.
// Requires AVX512F.
func MaskzFmaddPd(k x86.Mmask8, a x86.M128d, b x86.M128d, c x86.M128d) x86.M128d {
	return x86.M128d(maskzFmaddPd(uint8(k), [2]float64(a), [2]float64(b), [2]float64(c)))
}

func maskzFmaddPd(k uint8, a [2]float64, b [2]float64, c [2]float64) [2]float64


// M256MaskFmaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADD132PD, VFMADD213PD, VFMADD231PD'. Intrinsic: '_mm256_mask_fmadd_pd'.
// Requires AVX512F.
func M256MaskFmaddPd(a x86.M256d, k x86.Mmask8, b x86.M256d, c x86.M256d) x86.M256d {
	return x86.M256d(m256MaskFmaddPd([4]float64(a), uint8(k), [4]float64(b), [4]float64(c)))
}

func m256MaskFmaddPd(a [4]float64, k uint8, b [4]float64, c [4]float64) [4]float64


// M256Mask3FmaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADD132PD, VFMADD213PD, VFMADD231PD'. Intrinsic: '_mm256_mask3_fmadd_pd'.
// Requires AVX512F.
func M256Mask3FmaddPd(a x86.M256d, b x86.M256d, c x86.M256d, k x86.Mmask8) x86.M256d {
	return x86.M256d(m256Mask3FmaddPd([4]float64(a), [4]float64(b), [4]float64(c), uint8(k)))
}

func m256Mask3FmaddPd(a [4]float64, b [4]float64, c [4]float64, k uint8) [4]float64


// M256MaskzFmaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADD132PD, VFMADD213PD, VFMADD231PD'. Intrinsic: '_mm256_maskz_fmadd_pd'.
// Requires AVX512F.
func M256MaskzFmaddPd(k x86.Mmask8, a x86.M256d, b x86.M256d, c x86.M256d) x86.M256d {
	return x86.M256d(m256MaskzFmaddPd(uint8(k), [4]float64(a), [4]float64(b), [4]float64(c)))
}

func m256MaskzFmaddPd(k uint8, a [4]float64, b [4]float64, c [4]float64) [4]float64


// M512MaskzFmaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADD132PD, VFMADD213PD, VFMADD231PD'. Intrinsic: '_mm512_maskz_fmadd_pd'.
// Requires AVX512F.
func M512MaskzFmaddPd(k x86.Mmask8, a x86.M512d, b x86.M512d, c x86.M512d) x86.M512d {
	return x86.M512d(m512MaskzFmaddPd(uint8(k), [8]float64(a), [8]float64(b), [8]float64(c)))
}

func m512MaskzFmaddPd(k uint8, a [8]float64, b [8]float64, c [8]float64) [8]float64


// MaskFmaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132PS, VFMADD213PS, VFMADD231PS'. Intrinsic: '_mm_mask_fmadd_ps'.
// Requires AVX512F.
func MaskFmaddPs(a x86.M128, k x86.Mmask8, b x86.M128, c x86.M128) x86.M128 {
	return x86.M128(maskFmaddPs([4]float32(a), uint8(k), [4]float32(b), [4]float32(c)))
}

func maskFmaddPs(a [4]float32, k uint8, b [4]float32, c [4]float32) [4]float32


// Mask3FmaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132PS, VFMADD213PS, VFMADD231PS'. Intrinsic: '_mm_mask3_fmadd_ps'.
// Requires AVX512F.
func Mask3FmaddPs(a x86.M128, b x86.M128, c x86.M128, k x86.Mmask8) x86.M128 {
	return x86.M128(mask3FmaddPs([4]float32(a), [4]float32(b), [4]float32(c), uint8(k)))
}

func mask3FmaddPs(a [4]float32, b [4]float32, c [4]float32, k uint8) [4]float32


// MaskzFmaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132PS, VFMADD213PS, VFMADD231PS'. Intrinsic: '_mm_maskz_fmadd_ps'.
// Requires AVX512F.
func MaskzFmaddPs(k x86.Mmask8, a x86.M128, b x86.M128, c x86.M128) x86.M128 {
	return x86.M128(maskzFmaddPs(uint8(k), [4]float32(a), [4]float32(b), [4]float32(c)))
}

func maskzFmaddPs(k uint8, a [4]float32, b [4]float32, c [4]float32) [4]float32


// M256MaskFmaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADD132PS, VFMADD213PS, VFMADD231PS'. Intrinsic: '_mm256_mask_fmadd_ps'.
// Requires AVX512F.
func M256MaskFmaddPs(a x86.M256, k x86.Mmask8, b x86.M256, c x86.M256) x86.M256 {
	return x86.M256(m256MaskFmaddPs([8]float32(a), uint8(k), [8]float32(b), [8]float32(c)))
}

func m256MaskFmaddPs(a [8]float32, k uint8, b [8]float32, c [8]float32) [8]float32


// M256Mask3FmaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADD132PS, VFMADD213PS, VFMADD231PS'. Intrinsic: '_mm256_mask3_fmadd_ps'.
// Requires AVX512F.
func M256Mask3FmaddPs(a x86.M256, b x86.M256, c x86.M256, k x86.Mmask8) x86.M256 {
	return x86.M256(m256Mask3FmaddPs([8]float32(a), [8]float32(b), [8]float32(c), uint8(k)))
}

func m256Mask3FmaddPs(a [8]float32, b [8]float32, c [8]float32, k uint8) [8]float32


// M256MaskzFmaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADD132PS, VFMADD213PS, VFMADD231PS'. Intrinsic: '_mm256_maskz_fmadd_ps'.
// Requires AVX512F.
func M256MaskzFmaddPs(k x86.Mmask8, a x86.M256, b x86.M256, c x86.M256) x86.M256 {
	return x86.M256(m256MaskzFmaddPs(uint8(k), [8]float32(a), [8]float32(b), [8]float32(c)))
}

func m256MaskzFmaddPs(k uint8, a [8]float32, b [8]float32, c [8]float32) [8]float32


// M512MaskzFmaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADD132PS, VFMADD213PS, VFMADD231PS'. Intrinsic: '_mm512_maskz_fmadd_ps'.
// Requires AVX512F.
func M512MaskzFmaddPs(k x86.Mmask16, a x86.M512, b x86.M512, c x86.M512) x86.M512 {
	return x86.M512(m512MaskzFmaddPs(uint16(k), [16]float32(a), [16]float32(b), [16]float32(c)))
}

func m512MaskzFmaddPs(k uint16, a [16]float32, b [16]float32, c [16]float32) [16]float32


// M512MaskzFmaddRoundPd: Multiply packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', add the intermediate result to
// packed elements in 'c', and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADD132PD, VFMADD213PD, VFMADD231PD'. Intrinsic: '_mm512_maskz_fmadd_round_pd'.
// Requires AVX512F.
func M512MaskzFmaddRoundPd(k x86.Mmask8, a x86.M512d, b x86.M512d, c x86.M512d, rounding int) x86.M512d {
	return x86.M512d(m512MaskzFmaddRoundPd(uint8(k), [8]float64(a), [8]float64(b), [8]float64(c), rounding))
}

func m512MaskzFmaddRoundPd(k uint8, a [8]float64, b [8]float64, c [8]float64, rounding int) [8]float64


// M512MaskzFmaddRoundPs: Multiply packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', add the intermediate result to
// packed elements in 'c', and store the results in 'a' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				a[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADD132PS, VFMADD213PS, VFMADD231PS'. Intrinsic: '_mm512_maskz_fmadd_round_ps'.
// Requires AVX512F.
func M512MaskzFmaddRoundPs(k x86.Mmask16, a x86.M512, b x86.M512, c x86.M512, rounding int) x86.M512 {
	return x86.M512(m512MaskzFmaddRoundPs(uint16(k), [16]float32(a), [16]float32(b), [16]float32(c), rounding))
}

func m512MaskzFmaddRoundPs(k uint16, a [16]float32, b [16]float32, c [16]float32, rounding int) [16]float32


// MaskFmaddRoundSd: Multiply the lower double-precision (64-bit)
// floating-point elements in 'a' and 'b', and add the intermediate result to
// the lower element in 'c'. Store the result in the lower element of 'dst'
// using writemask 'k' (the element is copied from 'a' when mask bit 0 is not
// set), and copy the upper element from 'a' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := (a[63:0] * b[63:0]) + c[63:0]
//		ELSE
//			dst[63:0] := a[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132SD, VFMADD213SD, VFMADD231SD'. Intrinsic: '_mm_mask_fmadd_round_sd'.
// Requires AVX512F.
func MaskFmaddRoundSd(a x86.M128d, k x86.Mmask8, b x86.M128d, c x86.M128d, rounding int) x86.M128d {
	return x86.M128d(maskFmaddRoundSd([2]float64(a), uint8(k), [2]float64(b), [2]float64(c), rounding))
}

func maskFmaddRoundSd(a [2]float64, k uint8, b [2]float64, c [2]float64, rounding int) [2]float64


// Mask3FmaddRoundSd: Multiply the lower double-precision (64-bit)
// floating-point elements in 'a' and 'b', and add the intermediate result to
// the lower element in 'c'. Store the result in the lower element of 'dst'
// using writemask 'k' (the element is copied from 'c' when mask bit 0 is not
// set), and copy the upper element from 'a' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := (a[63:0] * b[63:0]) + c[63:0]
//		ELSE
//			dst[63:0] := c[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132SD, VFMADD213SD, VFMADD231SD'. Intrinsic: '_mm_mask3_fmadd_round_sd'.
// Requires AVX512F.
func Mask3FmaddRoundSd(a x86.M128d, b x86.M128d, c x86.M128d, k x86.Mmask8, rounding int) x86.M128d {
	return x86.M128d(mask3FmaddRoundSd([2]float64(a), [2]float64(b), [2]float64(c), uint8(k), rounding))
}

func mask3FmaddRoundSd(a [2]float64, b [2]float64, c [2]float64, k uint8, rounding int) [2]float64


// MaskzFmaddRoundSd: Multiply the lower double-precision (64-bit)
// floating-point elements in 'a' and 'b', and add the intermediate result to
// the lower element in 'c'. Store the result in the lower element of 'dst'
// using zeromask 'k' (the element is zeroed out when mask bit 0 is not set),
// and copy the upper element from 'a' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := (a[63:0] * b[63:0]) + c[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132SD, VFMADD213SD, VFMADD231SD'. Intrinsic: '_mm_maskz_fmadd_round_sd'.
// Requires AVX512F.
func MaskzFmaddRoundSd(k x86.Mmask8, a x86.M128d, b x86.M128d, c x86.M128d, rounding int) x86.M128d {
	return x86.M128d(maskzFmaddRoundSd(uint8(k), [2]float64(a), [2]float64(b), [2]float64(c), rounding))
}

func maskzFmaddRoundSd(k uint8, a [2]float64, b [2]float64, c [2]float64, rounding int) [2]float64


// MaskFmaddRoundSs: Multiply the lower single-precision (32-bit)
// floating-point elements in 'a' and 'b', and add the intermediate result to
// the lower element in 'c'. Store the result in the lower element of 'dst'
// using writemask 'k' (the element is copied from 'a' when mask bit 0 is not
// set), and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := (a[31:0] * b[31:0]) + c[31:0]
//		ELSE
//			dst[31:0] := a[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132SS, VFMADD213SS, VFMADD231SS'. Intrinsic: '_mm_mask_fmadd_round_ss'.
// Requires AVX512F.
func MaskFmaddRoundSs(a x86.M128, k x86.Mmask8, b x86.M128, c x86.M128, rounding int) x86.M128 {
	return x86.M128(maskFmaddRoundSs([4]float32(a), uint8(k), [4]float32(b), [4]float32(c), rounding))
}

func maskFmaddRoundSs(a [4]float32, k uint8, b [4]float32, c [4]float32, rounding int) [4]float32


// Mask3FmaddRoundSs: Multiply the lower single-precision (32-bit)
// floating-point elements in 'a' and 'b', and add the intermediate result to
// the lower element in 'c'. Store the result in the lower element of 'dst'
// using writemask 'k' (the element is copied from 'c' when mask bit 0 is not
// set), and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := (a[31:0] * b[31:0]) + c[31:0]
//		ELSE
//			dst[31:0] := c[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132SS, VFMADD213SS, VFMADD231SS'. Intrinsic: '_mm_mask3_fmadd_round_ss'.
// Requires AVX512F.
func Mask3FmaddRoundSs(a x86.M128, b x86.M128, c x86.M128, k x86.Mmask8, rounding int) x86.M128 {
	return x86.M128(mask3FmaddRoundSs([4]float32(a), [4]float32(b), [4]float32(c), uint8(k), rounding))
}

func mask3FmaddRoundSs(a [4]float32, b [4]float32, c [4]float32, k uint8, rounding int) [4]float32


// MaskzFmaddRoundSs: Multiply the lower single-precision (32-bit)
// floating-point elements in 'a' and 'b', and add the intermediate result to
// the lower element in 'c'. Store the result in the lower element of 'dst'
// using zeromask 'k' (the element is zeroed out when mask bit 0 is not set),
// and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := (a[31:0] * b[31:0]) + c[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132SS, VFMADD213SS, VFMADD231SS'. Intrinsic: '_mm_maskz_fmadd_round_ss'.
// Requires AVX512F.
func MaskzFmaddRoundSs(k x86.Mmask8, a x86.M128, b x86.M128, c x86.M128, rounding int) x86.M128 {
	return x86.M128(maskzFmaddRoundSs(uint8(k), [4]float32(a), [4]float32(b), [4]float32(c), rounding))
}

func maskzFmaddRoundSs(k uint8, a [4]float32, b [4]float32, c [4]float32, rounding int) [4]float32


// MaskFmaddSd: Multiply the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', and add the intermediate result to the lower
// element in 'c'. Store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'a' when mask bit 0 is not set),
// and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := (a[63:0] * b[63:0]) + c[63:0]
//		ELSE
//			dst[63:0] := a[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132SD, VFMADD213SD, VFMADD231SD'. Intrinsic: '_mm_mask_fmadd_sd'.
// Requires AVX512F.
func MaskFmaddSd(a x86.M128d, k x86.Mmask8, b x86.M128d, c x86.M128d) x86.M128d {
	return x86.M128d(maskFmaddSd([2]float64(a), uint8(k), [2]float64(b), [2]float64(c)))
}

func maskFmaddSd(a [2]float64, k uint8, b [2]float64, c [2]float64) [2]float64


// Mask3FmaddSd: Multiply the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', and add the intermediate result to the lower
// element in 'c'. Store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'c' when mask bit 0 is not set),
// and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := (a[63:0] * b[63:0]) + c[63:0]
//		ELSE
//			dst[63:0] := c[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132SD, VFMADD213SD, VFMADD231SD'. Intrinsic: '_mm_mask3_fmadd_sd'.
// Requires AVX512F.
func Mask3FmaddSd(a x86.M128d, b x86.M128d, c x86.M128d, k x86.Mmask8) x86.M128d {
	return x86.M128d(mask3FmaddSd([2]float64(a), [2]float64(b), [2]float64(c), uint8(k)))
}

func mask3FmaddSd(a [2]float64, b [2]float64, c [2]float64, k uint8) [2]float64


// MaskzFmaddSd: Multiply the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', and add the intermediate result to the lower
// element in 'c'. Store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := (a[63:0] * b[63:0]) + c[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132SD, VFMADD213SD, VFMADD231SD'. Intrinsic: '_mm_maskz_fmadd_sd'.
// Requires AVX512F.
func MaskzFmaddSd(k x86.Mmask8, a x86.M128d, b x86.M128d, c x86.M128d) x86.M128d {
	return x86.M128d(maskzFmaddSd(uint8(k), [2]float64(a), [2]float64(b), [2]float64(c)))
}

func maskzFmaddSd(k uint8, a [2]float64, b [2]float64, c [2]float64) [2]float64


// MaskFmaddSs: Multiply the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', and add the intermediate result to the lower
// element in 'c'. Store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'a' when mask bit 0 is not set),
// and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'. 
//
//		IF k[0]
//			dst[31:0] := (a[31:0] * b[31:0]) + c[31:0]
//		ELSE
//			dst[31:0] := a[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132SS, VFMADD213SS, VFMADD231SS'. Intrinsic: '_mm_mask_fmadd_ss'.
// Requires AVX512F.
func MaskFmaddSs(a x86.M128, k x86.Mmask8, b x86.M128, c x86.M128) x86.M128 {
	return x86.M128(maskFmaddSs([4]float32(a), uint8(k), [4]float32(b), [4]float32(c)))
}

func maskFmaddSs(a [4]float32, k uint8, b [4]float32, c [4]float32) [4]float32


// Mask3FmaddSs: Multiply the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', and add the intermediate result to the lower
// element in 'c'. Store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'c' when mask bit 0 is not set),
// and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'. 
//
//		IF k[0]
//			dst[31:0] := (a[31:0] * b[31:0]) + c[31:0]
//		ELSE
//			dst[31:0] := c[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132SS, VFMADD213SS, VFMADD231SS'. Intrinsic: '_mm_mask3_fmadd_ss'.
// Requires AVX512F.
func Mask3FmaddSs(a x86.M128, b x86.M128, c x86.M128, k x86.Mmask8) x86.M128 {
	return x86.M128(mask3FmaddSs([4]float32(a), [4]float32(b), [4]float32(c), uint8(k)))
}

func mask3FmaddSs(a [4]float32, b [4]float32, c [4]float32, k uint8) [4]float32


// MaskzFmaddSs: Multiply the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', and add the intermediate result to the lower
// element in 'c'. Store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := (a[31:0] * b[31:0]) + c[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132SS, VFMADD213SS, VFMADD231SS'. Intrinsic: '_mm_maskz_fmadd_ss'.
// Requires AVX512F.
func MaskzFmaddSs(k x86.Mmask8, a x86.M128, b x86.M128, c x86.M128) x86.M128 {
	return x86.M128(maskzFmaddSs(uint8(k), [4]float32(a), [4]float32(b), [4]float32(c)))
}

func maskzFmaddSs(k uint8, a [4]float32, b [4]float32, c [4]float32) [4]float32


// MaskFmaddsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm_mask_fmaddsub_pd'.
// Requires AVX512F.
func MaskFmaddsubPd(a x86.M128d, k x86.Mmask8, b x86.M128d, c x86.M128d) x86.M128d {
	return x86.M128d(maskFmaddsubPd([2]float64(a), uint8(k), [2]float64(b), [2]float64(c)))
}

func maskFmaddsubPd(a [2]float64, k uint8, b [2]float64, c [2]float64) [2]float64


// Mask3FmaddsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'c' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm_mask3_fmaddsub_pd'.
// Requires AVX512F.
func Mask3FmaddsubPd(a x86.M128d, b x86.M128d, c x86.M128d, k x86.Mmask8) x86.M128d {
	return x86.M128d(mask3FmaddsubPd([2]float64(a), [2]float64(b), [2]float64(c), uint8(k)))
}

func mask3FmaddsubPd(a [2]float64, b [2]float64, c [2]float64, k uint8) [2]float64


// MaskzFmaddsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm_maskz_fmaddsub_pd'.
// Requires AVX512F.
func MaskzFmaddsubPd(k x86.Mmask8, a x86.M128d, b x86.M128d, c x86.M128d) x86.M128d {
	return x86.M128d(maskzFmaddsubPd(uint8(k), [2]float64(a), [2]float64(b), [2]float64(c)))
}

func maskzFmaddsubPd(k uint8, a [2]float64, b [2]float64, c [2]float64) [2]float64


// M256MaskFmaddsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm256_mask_fmaddsub_pd'.
// Requires AVX512F.
func M256MaskFmaddsubPd(a x86.M256d, k x86.Mmask8, b x86.M256d, c x86.M256d) x86.M256d {
	return x86.M256d(m256MaskFmaddsubPd([4]float64(a), uint8(k), [4]float64(b), [4]float64(c)))
}

func m256MaskFmaddsubPd(a [4]float64, k uint8, b [4]float64, c [4]float64) [4]float64


// M256Mask3FmaddsubPd: Multiply packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', alternatively add and subtract
// packed elements in 'c' to/from the intermediate result, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'c' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm256_mask3_fmaddsub_pd'.
// Requires AVX512F.
func M256Mask3FmaddsubPd(a x86.M256d, b x86.M256d, c x86.M256d, k x86.Mmask8) x86.M256d {
	return x86.M256d(m256Mask3FmaddsubPd([4]float64(a), [4]float64(b), [4]float64(c), uint8(k)))
}

func m256Mask3FmaddsubPd(a [4]float64, b [4]float64, c [4]float64, k uint8) [4]float64


// M256MaskzFmaddsubPd: Multiply packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', alternatively add and subtract
// packed elements in 'c' to/from the intermediate result, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm256_maskz_fmaddsub_pd'.
// Requires AVX512F.
func M256MaskzFmaddsubPd(k x86.Mmask8, a x86.M256d, b x86.M256d, c x86.M256d) x86.M256d {
	return x86.M256d(m256MaskzFmaddsubPd(uint8(k), [4]float64(a), [4]float64(b), [4]float64(c)))
}

func m256MaskzFmaddsubPd(k uint8, a [4]float64, b [4]float64, c [4]float64) [4]float64


// M512FmaddsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF (j is even) 
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm512_fmaddsub_pd'.
// Requires AVX512F.
func M512FmaddsubPd(a x86.M512d, b x86.M512d, c x86.M512d) x86.M512d {
	return x86.M512d(m512FmaddsubPd([8]float64(a), [8]float64(b), [8]float64(c)))
}

func m512FmaddsubPd(a [8]float64, b [8]float64, c [8]float64) [8]float64


// M512MaskFmaddsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm512_mask_fmaddsub_pd'.
// Requires AVX512F.
func M512MaskFmaddsubPd(a x86.M512d, k x86.Mmask8, b x86.M512d, c x86.M512d) x86.M512d {
	return x86.M512d(m512MaskFmaddsubPd([8]float64(a), uint8(k), [8]float64(b), [8]float64(c)))
}

func m512MaskFmaddsubPd(a [8]float64, k uint8, b [8]float64, c [8]float64) [8]float64


// M512Mask3FmaddsubPd: Multiply packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', alternatively add and subtract
// packed elements in 'c' to/from the intermediate result, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'c' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm512_mask3_fmaddsub_pd'.
// Requires AVX512F.
func M512Mask3FmaddsubPd(a x86.M512d, b x86.M512d, c x86.M512d, k x86.Mmask8) x86.M512d {
	return x86.M512d(m512Mask3FmaddsubPd([8]float64(a), [8]float64(b), [8]float64(c), uint8(k)))
}

func m512Mask3FmaddsubPd(a [8]float64, b [8]float64, c [8]float64, k uint8) [8]float64


// M512MaskzFmaddsubPd: Multiply packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', alternatively add and subtract
// packed elements in 'c' to/from the intermediate result, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm512_maskz_fmaddsub_pd'.
// Requires AVX512F.
func M512MaskzFmaddsubPd(k x86.Mmask8, a x86.M512d, b x86.M512d, c x86.M512d) x86.M512d {
	return x86.M512d(m512MaskzFmaddsubPd(uint8(k), [8]float64(a), [8]float64(b), [8]float64(c)))
}

func m512MaskzFmaddsubPd(k uint8, a [8]float64, b [8]float64, c [8]float64) [8]float64


// MaskFmaddsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm_mask_fmaddsub_ps'.
// Requires AVX512F.
func MaskFmaddsubPs(a x86.M128, k x86.Mmask8, b x86.M128, c x86.M128) x86.M128 {
	return x86.M128(maskFmaddsubPs([4]float32(a), uint8(k), [4]float32(b), [4]float32(c)))
}

func maskFmaddsubPs(a [4]float32, k uint8, b [4]float32, c [4]float32) [4]float32


// Mask3FmaddsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'c' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm_mask3_fmaddsub_ps'.
// Requires AVX512F.
func Mask3FmaddsubPs(a x86.M128, b x86.M128, c x86.M128, k x86.Mmask8) x86.M128 {
	return x86.M128(mask3FmaddsubPs([4]float32(a), [4]float32(b), [4]float32(c), uint8(k)))
}

func mask3FmaddsubPs(a [4]float32, b [4]float32, c [4]float32, k uint8) [4]float32


// MaskzFmaddsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm_maskz_fmaddsub_ps'.
// Requires AVX512F.
func MaskzFmaddsubPs(k x86.Mmask8, a x86.M128, b x86.M128, c x86.M128) x86.M128 {
	return x86.M128(maskzFmaddsubPs(uint8(k), [4]float32(a), [4]float32(b), [4]float32(c)))
}

func maskzFmaddsubPs(k uint8, a [4]float32, b [4]float32, c [4]float32) [4]float32


// M256MaskFmaddsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm256_mask_fmaddsub_ps'.
// Requires AVX512F.
func M256MaskFmaddsubPs(a x86.M256, k x86.Mmask8, b x86.M256, c x86.M256) x86.M256 {
	return x86.M256(m256MaskFmaddsubPs([8]float32(a), uint8(k), [8]float32(b), [8]float32(c)))
}

func m256MaskFmaddsubPs(a [8]float32, k uint8, b [8]float32, c [8]float32) [8]float32


// M256Mask3FmaddsubPs: Multiply packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', alternatively add and subtract
// packed elements in 'c' to/from the intermediate result, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'c' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm256_mask3_fmaddsub_ps'.
// Requires AVX512F.
func M256Mask3FmaddsubPs(a x86.M256, b x86.M256, c x86.M256, k x86.Mmask8) x86.M256 {
	return x86.M256(m256Mask3FmaddsubPs([8]float32(a), [8]float32(b), [8]float32(c), uint8(k)))
}

func m256Mask3FmaddsubPs(a [8]float32, b [8]float32, c [8]float32, k uint8) [8]float32


// M256MaskzFmaddsubPs: Multiply packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', alternatively add and subtract
// packed elements in 'c' to/from the intermediate result, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm256_maskz_fmaddsub_ps'.
// Requires AVX512F.
func M256MaskzFmaddsubPs(k x86.Mmask8, a x86.M256, b x86.M256, c x86.M256) x86.M256 {
	return x86.M256(m256MaskzFmaddsubPs(uint8(k), [8]float32(a), [8]float32(b), [8]float32(c)))
}

func m256MaskzFmaddsubPs(k uint8, a [8]float32, b [8]float32, c [8]float32) [8]float32


// M512FmaddsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF (j is even) 
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm512_fmaddsub_ps'.
// Requires AVX512F.
func M512FmaddsubPs(a x86.M512, b x86.M512, c x86.M512) x86.M512 {
	return x86.M512(m512FmaddsubPs([16]float32(a), [16]float32(b), [16]float32(c)))
}

func m512FmaddsubPs(a [16]float32, b [16]float32, c [16]float32) [16]float32


// M512MaskFmaddsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm512_mask_fmaddsub_ps'.
// Requires AVX512F.
func M512MaskFmaddsubPs(a x86.M512, k x86.Mmask16, b x86.M512, c x86.M512) x86.M512 {
	return x86.M512(m512MaskFmaddsubPs([16]float32(a), uint16(k), [16]float32(b), [16]float32(c)))
}

func m512MaskFmaddsubPs(a [16]float32, k uint16, b [16]float32, c [16]float32) [16]float32


// M512Mask3FmaddsubPs: Multiply packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', alternatively add and subtract
// packed elements in 'c' to/from the intermediate result, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'c' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm512_mask3_fmaddsub_ps'.
// Requires AVX512F.
func M512Mask3FmaddsubPs(a x86.M512, b x86.M512, c x86.M512, k x86.Mmask16) x86.M512 {
	return x86.M512(m512Mask3FmaddsubPs([16]float32(a), [16]float32(b), [16]float32(c), uint16(k)))
}

func m512Mask3FmaddsubPs(a [16]float32, b [16]float32, c [16]float32, k uint16) [16]float32


// M512MaskzFmaddsubPs: Multiply packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', alternatively add and subtract
// packed elements in 'c' to/from the intermediate result, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm512_maskz_fmaddsub_ps'.
// Requires AVX512F.
func M512MaskzFmaddsubPs(k x86.Mmask16, a x86.M512, b x86.M512, c x86.M512) x86.M512 {
	return x86.M512(m512MaskzFmaddsubPs(uint16(k), [16]float32(a), [16]float32(b), [16]float32(c)))
}

func m512MaskzFmaddsubPs(k uint16, a [16]float32, b [16]float32, c [16]float32) [16]float32


// M512FmaddsubRoundPd: Multiply packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', alternatively add and subtract
// packed elements in 'c' to/from the intermediate result, and store the
// results in 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF (j is even) 
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm512_fmaddsub_round_pd'.
// Requires AVX512F.
func M512FmaddsubRoundPd(a x86.M512d, b x86.M512d, c x86.M512d, rounding int) x86.M512d {
	return x86.M512d(m512FmaddsubRoundPd([8]float64(a), [8]float64(b), [8]float64(c), rounding))
}

func m512FmaddsubRoundPd(a [8]float64, b [8]float64, c [8]float64, rounding int) [8]float64


// M512MaskFmaddsubRoundPd: Multiply packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', alternatively add and subtract
// packed elements in 'c' to/from the intermediate result, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). Rounding is done according to the
// 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm512_mask_fmaddsub_round_pd'.
// Requires AVX512F.
func M512MaskFmaddsubRoundPd(a x86.M512d, k x86.Mmask8, b x86.M512d, c x86.M512d, rounding int) x86.M512d {
	return x86.M512d(m512MaskFmaddsubRoundPd([8]float64(a), uint8(k), [8]float64(b), [8]float64(c), rounding))
}

func m512MaskFmaddsubRoundPd(a [8]float64, k uint8, b [8]float64, c [8]float64, rounding int) [8]float64


// M512Mask3FmaddsubRoundPd: Multiply packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', alternatively add and subtract
// packed elements in 'c' to/from the intermediate result, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'c' when the
// corresponding mask bit is not set).  Rounding is done according to the
// 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				FI
//			ELSE 
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm512_mask3_fmaddsub_round_pd'.
// Requires AVX512F.
func M512Mask3FmaddsubRoundPd(a x86.M512d, b x86.M512d, c x86.M512d, k x86.Mmask8, rounding int) x86.M512d {
	return x86.M512d(m512Mask3FmaddsubRoundPd([8]float64(a), [8]float64(b), [8]float64(c), uint8(k), rounding))
}

func m512Mask3FmaddsubRoundPd(a [8]float64, b [8]float64, c [8]float64, k uint8, rounding int) [8]float64


// M512MaskzFmaddsubRoundPd: Multiply packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', alternatively add and subtract
// packed elements in 'c' to/from the intermediate result, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm512_maskz_fmaddsub_round_pd'.
// Requires AVX512F.
func M512MaskzFmaddsubRoundPd(k x86.Mmask8, a x86.M512d, b x86.M512d, c x86.M512d, rounding int) x86.M512d {
	return x86.M512d(m512MaskzFmaddsubRoundPd(uint8(k), [8]float64(a), [8]float64(b), [8]float64(c), rounding))
}

func m512MaskzFmaddsubRoundPd(k uint8, a [8]float64, b [8]float64, c [8]float64, rounding int) [8]float64


// M512FmaddsubRoundPs: Multiply packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', alternatively add and subtract
// packed elements in 'c' to/from the intermediate result, and store the
// results in 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF (j is even) 
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm512_fmaddsub_round_ps'.
// Requires AVX512F.
func M512FmaddsubRoundPs(a x86.M512, b x86.M512, c x86.M512, rounding int) x86.M512 {
	return x86.M512(m512FmaddsubRoundPs([16]float32(a), [16]float32(b), [16]float32(c), rounding))
}

func m512FmaddsubRoundPs(a [16]float32, b [16]float32, c [16]float32, rounding int) [16]float32


// M512MaskFmaddsubRoundPs: Multiply packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', alternatively add and subtract
// packed elements in 'c' to/from the intermediate result, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). Rounding is done according to the
// 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm512_mask_fmaddsub_round_ps'.
// Requires AVX512F.
func M512MaskFmaddsubRoundPs(a x86.M512, k x86.Mmask16, b x86.M512, c x86.M512, rounding int) x86.M512 {
	return x86.M512(m512MaskFmaddsubRoundPs([16]float32(a), uint16(k), [16]float32(b), [16]float32(c), rounding))
}

func m512MaskFmaddsubRoundPs(a [16]float32, k uint16, b [16]float32, c [16]float32, rounding int) [16]float32


// M512Mask3FmaddsubRoundPs: Multiply packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', alternatively add and subtract
// packed elements in 'c' to/from the intermediate result, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'c' when the
// corresponding mask bit is not set).  Rounding is done according to the
// 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm512_mask3_fmaddsub_round_ps'.
// Requires AVX512F.
func M512Mask3FmaddsubRoundPs(a x86.M512, b x86.M512, c x86.M512, k x86.Mmask16, rounding int) x86.M512 {
	return x86.M512(m512Mask3FmaddsubRoundPs([16]float32(a), [16]float32(b), [16]float32(c), uint16(k), rounding))
}

func m512Mask3FmaddsubRoundPs(a [16]float32, b [16]float32, c [16]float32, k uint16, rounding int) [16]float32


// M512MaskzFmaddsubRoundPs: Multiply packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', alternatively add and subtract
// packed elements in 'c' to/from the intermediate result, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm512_maskz_fmaddsub_round_ps'.
// Requires AVX512F.
func M512MaskzFmaddsubRoundPs(k x86.Mmask16, a x86.M512, b x86.M512, c x86.M512, rounding int) x86.M512 {
	return x86.M512(m512MaskzFmaddsubRoundPs(uint16(k), [16]float32(a), [16]float32(b), [16]float32(c), rounding))
}

func m512MaskzFmaddsubRoundPs(k uint16, a [16]float32, b [16]float32, c [16]float32, rounding int) [16]float32


// MaskFmsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132PD, VFMSUB213PD, VFMSUB231PD'. Intrinsic: '_mm_mask_fmsub_pd'.
// Requires AVX512F.
func MaskFmsubPd(a x86.M128d, k x86.Mmask8, b x86.M128d, c x86.M128d) x86.M128d {
	return x86.M128d(maskFmsubPd([2]float64(a), uint8(k), [2]float64(b), [2]float64(c)))
}

func maskFmsubPd(a [2]float64, k uint8, b [2]float64, c [2]float64) [2]float64


// Mask3FmsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132PD, VFMSUB213PD, VFMSUB231PD'. Intrinsic: '_mm_mask3_fmsub_pd'.
// Requires AVX512F.
func Mask3FmsubPd(a x86.M128d, b x86.M128d, c x86.M128d, k x86.Mmask8) x86.M128d {
	return x86.M128d(mask3FmsubPd([2]float64(a), [2]float64(b), [2]float64(c), uint8(k)))
}

func mask3FmsubPd(a [2]float64, b [2]float64, c [2]float64, k uint8) [2]float64


// MaskzFmsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132PD, VFMSUB213PD, VFMSUB231PD'. Intrinsic: '_mm_maskz_fmsub_pd'.
// Requires AVX512F.
func MaskzFmsubPd(k x86.Mmask8, a x86.M128d, b x86.M128d, c x86.M128d) x86.M128d {
	return x86.M128d(maskzFmsubPd(uint8(k), [2]float64(a), [2]float64(b), [2]float64(c)))
}

func maskzFmsubPd(k uint8, a [2]float64, b [2]float64, c [2]float64) [2]float64


// M256MaskFmsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUB132PD, VFMSUB213PD, VFMSUB231PD'. Intrinsic: '_mm256_mask_fmsub_pd'.
// Requires AVX512F.
func M256MaskFmsubPd(a x86.M256d, k x86.Mmask8, b x86.M256d, c x86.M256d) x86.M256d {
	return x86.M256d(m256MaskFmsubPd([4]float64(a), uint8(k), [4]float64(b), [4]float64(c)))
}

func m256MaskFmsubPd(a [4]float64, k uint8, b [4]float64, c [4]float64) [4]float64


// M256Mask3FmsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUB132PD, VFMSUB213PD, VFMSUB231PD'. Intrinsic: '_mm256_mask3_fmsub_pd'.
// Requires AVX512F.
func M256Mask3FmsubPd(a x86.M256d, b x86.M256d, c x86.M256d, k x86.Mmask8) x86.M256d {
	return x86.M256d(m256Mask3FmsubPd([4]float64(a), [4]float64(b), [4]float64(c), uint8(k)))
}

func m256Mask3FmsubPd(a [4]float64, b [4]float64, c [4]float64, k uint8) [4]float64


// M256MaskzFmsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUB132PD, VFMSUB213PD, VFMSUB231PD'. Intrinsic: '_mm256_maskz_fmsub_pd'.
// Requires AVX512F.
func M256MaskzFmsubPd(k x86.Mmask8, a x86.M256d, b x86.M256d, c x86.M256d) x86.M256d {
	return x86.M256d(m256MaskzFmsubPd(uint8(k), [4]float64(a), [4]float64(b), [4]float64(c)))
}

func m256MaskzFmsubPd(k uint8, a [4]float64, b [4]float64, c [4]float64) [4]float64


// M512MaskzFmsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUB132PD, VFMSUB213PD, VFMSUB231PD'. Intrinsic: '_mm512_maskz_fmsub_pd'.
// Requires AVX512F.
func M512MaskzFmsubPd(k x86.Mmask8, a x86.M512d, b x86.M512d, c x86.M512d) x86.M512d {
	return x86.M512d(m512MaskzFmsubPd(uint8(k), [8]float64(a), [8]float64(b), [8]float64(c)))
}

func m512MaskzFmsubPd(k uint8, a [8]float64, b [8]float64, c [8]float64) [8]float64


// MaskFmsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132PS, VFMSUB213PS, VFMSUB231PS'. Intrinsic: '_mm_mask_fmsub_ps'.
// Requires AVX512F.
func MaskFmsubPs(a x86.M128, k x86.Mmask8, b x86.M128, c x86.M128) x86.M128 {
	return x86.M128(maskFmsubPs([4]float32(a), uint8(k), [4]float32(b), [4]float32(c)))
}

func maskFmsubPs(a [4]float32, k uint8, b [4]float32, c [4]float32) [4]float32


// Mask3FmsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132PS, VFMSUB213PS, VFMSUB231PS'. Intrinsic: '_mm_mask3_fmsub_ps'.
// Requires AVX512F.
func Mask3FmsubPs(a x86.M128, b x86.M128, c x86.M128, k x86.Mmask8) x86.M128 {
	return x86.M128(mask3FmsubPs([4]float32(a), [4]float32(b), [4]float32(c), uint8(k)))
}

func mask3FmsubPs(a [4]float32, b [4]float32, c [4]float32, k uint8) [4]float32


// MaskzFmsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132PS, VFMSUB213PS, VFMSUB231PS'. Intrinsic: '_mm_maskz_fmsub_ps'.
// Requires AVX512F.
func MaskzFmsubPs(k x86.Mmask8, a x86.M128, b x86.M128, c x86.M128) x86.M128 {
	return x86.M128(maskzFmsubPs(uint8(k), [4]float32(a), [4]float32(b), [4]float32(c)))
}

func maskzFmsubPs(k uint8, a [4]float32, b [4]float32, c [4]float32) [4]float32


// M256MaskFmsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUB132PS, VFMSUB213PS, VFMSUB231PS'. Intrinsic: '_mm256_mask_fmsub_ps'.
// Requires AVX512F.
func M256MaskFmsubPs(a x86.M256, k x86.Mmask8, b x86.M256, c x86.M256) x86.M256 {
	return x86.M256(m256MaskFmsubPs([8]float32(a), uint8(k), [8]float32(b), [8]float32(c)))
}

func m256MaskFmsubPs(a [8]float32, k uint8, b [8]float32, c [8]float32) [8]float32


// M256Mask3FmsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUB132PS, VFMSUB213PS, VFMSUB231PS'. Intrinsic: '_mm256_mask3_fmsub_ps'.
// Requires AVX512F.
func M256Mask3FmsubPs(a x86.M256, b x86.M256, c x86.M256, k x86.Mmask8) x86.M256 {
	return x86.M256(m256Mask3FmsubPs([8]float32(a), [8]float32(b), [8]float32(c), uint8(k)))
}

func m256Mask3FmsubPs(a [8]float32, b [8]float32, c [8]float32, k uint8) [8]float32


// M256MaskzFmsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUB132PS, VFMSUB213PS, VFMSUB231PS'. Intrinsic: '_mm256_maskz_fmsub_ps'.
// Requires AVX512F.
func M256MaskzFmsubPs(k x86.Mmask8, a x86.M256, b x86.M256, c x86.M256) x86.M256 {
	return x86.M256(m256MaskzFmsubPs(uint8(k), [8]float32(a), [8]float32(b), [8]float32(c)))
}

func m256MaskzFmsubPs(k uint8, a [8]float32, b [8]float32, c [8]float32) [8]float32


// M512MaskzFmsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUB132PS, VFMSUB213PS, VFMSUB231PS'. Intrinsic: '_mm512_maskz_fmsub_ps'.
// Requires AVX512F.
func M512MaskzFmsubPs(k x86.Mmask16, a x86.M512, b x86.M512, c x86.M512) x86.M512 {
	return x86.M512(m512MaskzFmsubPs(uint16(k), [16]float32(a), [16]float32(b), [16]float32(c)))
}

func m512MaskzFmsubPs(k uint16, a [16]float32, b [16]float32, c [16]float32) [16]float32


// M512MaskzFmsubRoundPd: Multiply packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', subtract packed elements in 'c' from
// the intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUB132PD, VFMSUB213PD, VFMSUB231PD'. Intrinsic: '_mm512_maskz_fmsub_round_pd'.
// Requires AVX512F.
func M512MaskzFmsubRoundPd(k x86.Mmask8, a x86.M512d, b x86.M512d, c x86.M512d, rounding int) x86.M512d {
	return x86.M512d(m512MaskzFmsubRoundPd(uint8(k), [8]float64(a), [8]float64(b), [8]float64(c), rounding))
}

func m512MaskzFmsubRoundPd(k uint8, a [8]float64, b [8]float64, c [8]float64, rounding int) [8]float64


// M512MaskzFmsubRoundPs: Multiply packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', subtract packed elements in 'c' from
// the intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUB132PS, VFMSUB213PS, VFMSUB231PS'. Intrinsic: '_mm512_maskz_fmsub_round_ps'.
// Requires AVX512F.
func M512MaskzFmsubRoundPs(k x86.Mmask16, a x86.M512, b x86.M512, c x86.M512, rounding int) x86.M512 {
	return x86.M512(m512MaskzFmsubRoundPs(uint16(k), [16]float32(a), [16]float32(b), [16]float32(c), rounding))
}

func m512MaskzFmsubRoundPs(k uint16, a [16]float32, b [16]float32, c [16]float32, rounding int) [16]float32


// MaskFmsubRoundSd: Multiply the lower double-precision (64-bit)
// floating-point elements in 'a' and 'b', and subtract the lower element in
// 'c' from the intermediate result. Store the result in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'a' when mask bit 0 is
// not set), and copy the upper element from 'a' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := (a[63:0] * b[63:0]) - c[63:0]
//		ELSE
//			dst[63:0] := a[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132SD, VFMSUB213SD, VFMSUB231SD'. Intrinsic: '_mm_mask_fmsub_round_sd'.
// Requires AVX512F.
func MaskFmsubRoundSd(a x86.M128d, k x86.Mmask8, b x86.M128d, c x86.M128d, rounding int) x86.M128d {
	return x86.M128d(maskFmsubRoundSd([2]float64(a), uint8(k), [2]float64(b), [2]float64(c), rounding))
}

func maskFmsubRoundSd(a [2]float64, k uint8, b [2]float64, c [2]float64, rounding int) [2]float64


// Mask3FmsubRoundSd: Multiply the lower double-precision (64-bit)
// floating-point elements in 'a' and 'b', and subtract the lower element in
// 'c' from the intermediate result. Store the result in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'c' when mask bit 0 is
// not set), and copy the upper element from 'a' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := (a[63:0] * b[63:0]) - c[63:0]
//		ELSE
//			dst[63:0] := c[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132SD, VFMSUB213SD, VFMSUB231SD'. Intrinsic: '_mm_mask3_fmsub_round_sd'.
// Requires AVX512F.
func Mask3FmsubRoundSd(a x86.M128d, b x86.M128d, c x86.M128d, k x86.Mmask8, rounding int) x86.M128d {
	return x86.M128d(mask3FmsubRoundSd([2]float64(a), [2]float64(b), [2]float64(c), uint8(k), rounding))
}

func mask3FmsubRoundSd(a [2]float64, b [2]float64, c [2]float64, k uint8, rounding int) [2]float64


// MaskzFmsubRoundSd: Multiply the lower double-precision (64-bit)
// floating-point elements in 'a' and 'b', and subtract the lower element in
// 'c' from the intermediate result. Store the result in the lower element of
// 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is not
// set), and copy the upper element from 'a' to the upper element of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := (a[63:0] * b[63:0]) - c[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132SD, VFMSUB213SD, VFMSUB231SD'. Intrinsic: '_mm_maskz_fmsub_round_sd'.
// Requires AVX512F.
func MaskzFmsubRoundSd(k x86.Mmask8, a x86.M128d, b x86.M128d, c x86.M128d, rounding int) x86.M128d {
	return x86.M128d(maskzFmsubRoundSd(uint8(k), [2]float64(a), [2]float64(b), [2]float64(c), rounding))
}

func maskzFmsubRoundSd(k uint8, a [2]float64, b [2]float64, c [2]float64, rounding int) [2]float64


// MaskFmsubRoundSs: Multiply the lower single-precision (32-bit)
// floating-point elements in 'a' and 'b', and subtract the lower element in
// 'c' from the intermediate result. Store the result in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'a' when mask bit 0 is
// not set), and copy the upper 3 packed elements from 'a' to the upper
// elements of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := (a[31:0] * b[31:0]) - c[31:0]
//		ELSE
//			dst[31:0] := a[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132SS, VFMSUB213SS, VFMSUB231SS'. Intrinsic: '_mm_mask_fmsub_round_ss'.
// Requires AVX512F.
func MaskFmsubRoundSs(a x86.M128, k x86.Mmask8, b x86.M128, c x86.M128, rounding int) x86.M128 {
	return x86.M128(maskFmsubRoundSs([4]float32(a), uint8(k), [4]float32(b), [4]float32(c), rounding))
}

func maskFmsubRoundSs(a [4]float32, k uint8, b [4]float32, c [4]float32, rounding int) [4]float32


// Mask3FmsubRoundSs: Multiply the lower single-precision (32-bit)
// floating-point elements in 'a' and 'b', and subtract the lower element in
// 'c' from the intermediate result. Store the result in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'c' when mask bit 0 is
// not set), and copy the upper 3 packed elements from 'a' to the upper
// elements of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := (a[31:0] * b[31:0]) - c[31:0]
//		ELSE
//			dst[31:0] := c[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132SS, VFMSUB213SS, VFMSUB231SS'. Intrinsic: '_mm_mask3_fmsub_round_ss'.
// Requires AVX512F.
func Mask3FmsubRoundSs(a x86.M128, b x86.M128, c x86.M128, k x86.Mmask8, rounding int) x86.M128 {
	return x86.M128(mask3FmsubRoundSs([4]float32(a), [4]float32(b), [4]float32(c), uint8(k), rounding))
}

func mask3FmsubRoundSs(a [4]float32, b [4]float32, c [4]float32, k uint8, rounding int) [4]float32


// MaskzFmsubRoundSs: Multiply the lower single-precision (32-bit)
// floating-point elements in 'a' and 'b', and subtract the lower element in
// 'c' from the intermediate result. Store the result in the lower element of
// 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is not
// set), and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := (a[31:0] * b[31:0]) - c[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132SS, VFMSUB213SS, VFMSUB231SS'. Intrinsic: '_mm_maskz_fmsub_round_ss'.
// Requires AVX512F.
func MaskzFmsubRoundSs(k x86.Mmask8, a x86.M128, b x86.M128, c x86.M128, rounding int) x86.M128 {
	return x86.M128(maskzFmsubRoundSs(uint8(k), [4]float32(a), [4]float32(b), [4]float32(c), rounding))
}

func maskzFmsubRoundSs(k uint8, a [4]float32, b [4]float32, c [4]float32, rounding int) [4]float32


// MaskFmsubSd: Multiply the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', and subtract the lower element in 'c' from the
// intermediate result. Store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'a' when mask bit 0 is not set),
// and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := (a[63:0] * b[63:0]) - c[63:0]
//		ELSE
//			dst[63:0] := a[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132SD, VFMSUB213SD, VFMSUB231SD'. Intrinsic: '_mm_mask_fmsub_sd'.
// Requires AVX512F.
func MaskFmsubSd(a x86.M128d, k x86.Mmask8, b x86.M128d, c x86.M128d) x86.M128d {
	return x86.M128d(maskFmsubSd([2]float64(a), uint8(k), [2]float64(b), [2]float64(c)))
}

func maskFmsubSd(a [2]float64, k uint8, b [2]float64, c [2]float64) [2]float64


// Mask3FmsubSd: Multiply the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', and subtract the lower element in 'c' from the
// intermediate result. Store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'c' when mask bit 0 is not set),
// and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := (a[63:0] * b[63:0]) - c[63:0]
//		ELSE
//			dst[63:0] := c[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132SD, VFMSUB213SD, VFMSUB231SD'. Intrinsic: '_mm_mask3_fmsub_sd'.
// Requires AVX512F.
func Mask3FmsubSd(a x86.M128d, b x86.M128d, c x86.M128d, k x86.Mmask8) x86.M128d {
	return x86.M128d(mask3FmsubSd([2]float64(a), [2]float64(b), [2]float64(c), uint8(k)))
}

func mask3FmsubSd(a [2]float64, b [2]float64, c [2]float64, k uint8) [2]float64


// MaskzFmsubSd: Multiply the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', and subtract the lower element in 'c' from the
// intermediate result. Store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := (a[63:0] * b[63:0]) - c[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132SD, VFMSUB213SD, VFMSUB231SD'. Intrinsic: '_mm_maskz_fmsub_sd'.
// Requires AVX512F.
func MaskzFmsubSd(k x86.Mmask8, a x86.M128d, b x86.M128d, c x86.M128d) x86.M128d {
	return x86.M128d(maskzFmsubSd(uint8(k), [2]float64(a), [2]float64(b), [2]float64(c)))
}

func maskzFmsubSd(k uint8, a [2]float64, b [2]float64, c [2]float64) [2]float64


// MaskFmsubSs: Multiply the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', and subtract the lower element in 'c' from the
// intermediate result. Store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'a' when mask bit 0 is not set),
// and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'. 
//
//		IF k[0]
//			dst[31:0] := (a[31:0] * b[31:0]) - c[31:0]
//		ELSE
//			dst[31:0] := a[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132SS, VFMSUB213SS, VFMSUB231SS'. Intrinsic: '_mm_mask_fmsub_ss'.
// Requires AVX512F.
func MaskFmsubSs(a x86.M128, k x86.Mmask8, b x86.M128, c x86.M128) x86.M128 {
	return x86.M128(maskFmsubSs([4]float32(a), uint8(k), [4]float32(b), [4]float32(c)))
}

func maskFmsubSs(a [4]float32, k uint8, b [4]float32, c [4]float32) [4]float32


// Mask3FmsubSs: Multiply the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', and subtract the lower element in 'c' from the
// intermediate result. Store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'c' when mask bit 0 is not set),
// and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'. 
//
//		IF k[0]
//			dst[31:0] := (a[31:0] * b[31:0]) - c[31:0]
//		ELSE
//			dst[31:0] := c[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132SS, VFMSUB213SS, VFMSUB231SS'. Intrinsic: '_mm_mask3_fmsub_ss'.
// Requires AVX512F.
func Mask3FmsubSs(a x86.M128, b x86.M128, c x86.M128, k x86.Mmask8) x86.M128 {
	return x86.M128(mask3FmsubSs([4]float32(a), [4]float32(b), [4]float32(c), uint8(k)))
}

func mask3FmsubSs(a [4]float32, b [4]float32, c [4]float32, k uint8) [4]float32


// MaskzFmsubSs: Multiply the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', and subtract the lower element in 'c' from the
// intermediate result. Store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := (a[31:0] * b[31:0]) - c[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132SS, VFMSUB213SS, VFMSUB231SS'. Intrinsic: '_mm_maskz_fmsub_ss'.
// Requires AVX512F.
func MaskzFmsubSs(k x86.Mmask8, a x86.M128, b x86.M128, c x86.M128) x86.M128 {
	return x86.M128(maskzFmsubSs(uint8(k), [4]float32(a), [4]float32(b), [4]float32(c)))
}

func maskzFmsubSs(k uint8, a [4]float32, b [4]float32, c [4]float32) [4]float32


// MaskFmsubaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 1 
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm_mask_fmsubadd_pd'.
// Requires AVX512F.
func MaskFmsubaddPd(a x86.M128d, k x86.Mmask8, b x86.M128d, c x86.M128d) x86.M128d {
	return x86.M128d(maskFmsubaddPd([2]float64(a), uint8(k), [2]float64(b), [2]float64(c)))
}

func maskFmsubaddPd(a [2]float64, k uint8, b [2]float64, c [2]float64) [2]float64


// Mask3FmsubaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'c' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm_mask3_fmsubadd_pd'.
// Requires AVX512F.
func Mask3FmsubaddPd(a x86.M128d, b x86.M128d, c x86.M128d, k x86.Mmask8) x86.M128d {
	return x86.M128d(mask3FmsubaddPd([2]float64(a), [2]float64(b), [2]float64(c), uint8(k)))
}

func mask3FmsubaddPd(a [2]float64, b [2]float64, c [2]float64, k uint8) [2]float64


// MaskzFmsubaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm_maskz_fmsubadd_pd'.
// Requires AVX512F.
func MaskzFmsubaddPd(k x86.Mmask8, a x86.M128d, b x86.M128d, c x86.M128d) x86.M128d {
	return x86.M128d(maskzFmsubaddPd(uint8(k), [2]float64(a), [2]float64(b), [2]float64(c)))
}

func maskzFmsubaddPd(k uint8, a [2]float64, b [2]float64, c [2]float64) [2]float64


// M256MaskFmsubaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm256_mask_fmsubadd_pd'.
// Requires AVX512F.
func M256MaskFmsubaddPd(a x86.M256d, k x86.Mmask8, b x86.M256d, c x86.M256d) x86.M256d {
	return x86.M256d(m256MaskFmsubaddPd([4]float64(a), uint8(k), [4]float64(b), [4]float64(c)))
}

func m256MaskFmsubaddPd(a [4]float64, k uint8, b [4]float64, c [4]float64) [4]float64


// M256Mask3FmsubaddPd: Multiply packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', alternatively subtract and add
// packed elements in 'c' from/to the intermediate result, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'c' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm256_mask3_fmsubadd_pd'.
// Requires AVX512F.
func M256Mask3FmsubaddPd(a x86.M256d, b x86.M256d, c x86.M256d, k x86.Mmask8) x86.M256d {
	return x86.M256d(m256Mask3FmsubaddPd([4]float64(a), [4]float64(b), [4]float64(c), uint8(k)))
}

func m256Mask3FmsubaddPd(a [4]float64, b [4]float64, c [4]float64, k uint8) [4]float64


// M256MaskzFmsubaddPd: Multiply packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', alternatively subtract and add
// packed elements in 'c' from/to the intermediate result, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm256_maskz_fmsubadd_pd'.
// Requires AVX512F.
func M256MaskzFmsubaddPd(k x86.Mmask8, a x86.M256d, b x86.M256d, c x86.M256d) x86.M256d {
	return x86.M256d(m256MaskzFmsubaddPd(uint8(k), [4]float64(a), [4]float64(b), [4]float64(c)))
}

func m256MaskzFmsubaddPd(k uint8, a [4]float64, b [4]float64, c [4]float64) [4]float64


// M512FmsubaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF (j is even) 
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm512_fmsubadd_pd'.
// Requires AVX512F.
func M512FmsubaddPd(a x86.M512d, b x86.M512d, c x86.M512d) x86.M512d {
	return x86.M512d(m512FmsubaddPd([8]float64(a), [8]float64(b), [8]float64(c)))
}

func m512FmsubaddPd(a [8]float64, b [8]float64, c [8]float64) [8]float64


// M512MaskFmsubaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm512_mask_fmsubadd_pd'.
// Requires AVX512F.
func M512MaskFmsubaddPd(a x86.M512d, k x86.Mmask8, b x86.M512d, c x86.M512d) x86.M512d {
	return x86.M512d(m512MaskFmsubaddPd([8]float64(a), uint8(k), [8]float64(b), [8]float64(c)))
}

func m512MaskFmsubaddPd(a [8]float64, k uint8, b [8]float64, c [8]float64) [8]float64


// M512Mask3FmsubaddPd: Multiply packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', alternatively subtract and add
// packed elements in 'c' from/to the intermediate result, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'c' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm512_mask3_fmsubadd_pd'.
// Requires AVX512F.
func M512Mask3FmsubaddPd(a x86.M512d, b x86.M512d, c x86.M512d, k x86.Mmask8) x86.M512d {
	return x86.M512d(m512Mask3FmsubaddPd([8]float64(a), [8]float64(b), [8]float64(c), uint8(k)))
}

func m512Mask3FmsubaddPd(a [8]float64, b [8]float64, c [8]float64, k uint8) [8]float64


// M512MaskzFmsubaddPd: Multiply packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', alternatively subtract and add
// packed elements in 'c' from/to the intermediate result, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm512_maskz_fmsubadd_pd'.
// Requires AVX512F.
func M512MaskzFmsubaddPd(k x86.Mmask8, a x86.M512d, b x86.M512d, c x86.M512d) x86.M512d {
	return x86.M512d(m512MaskzFmsubaddPd(uint8(k), [8]float64(a), [8]float64(b), [8]float64(c)))
}

func m512MaskzFmsubaddPd(k uint8, a [8]float64, b [8]float64, c [8]float64) [8]float64


// MaskFmsubaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm_mask_fmsubadd_ps'.
// Requires AVX512F.
func MaskFmsubaddPs(a x86.M128, k x86.Mmask8, b x86.M128, c x86.M128) x86.M128 {
	return x86.M128(maskFmsubaddPs([4]float32(a), uint8(k), [4]float32(b), [4]float32(c)))
}

func maskFmsubaddPs(a [4]float32, k uint8, b [4]float32, c [4]float32) [4]float32


// Mask3FmsubaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'c' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm_mask3_fmsubadd_ps'.
// Requires AVX512F.
func Mask3FmsubaddPs(a x86.M128, b x86.M128, c x86.M128, k x86.Mmask8) x86.M128 {
	return x86.M128(mask3FmsubaddPs([4]float32(a), [4]float32(b), [4]float32(c), uint8(k)))
}

func mask3FmsubaddPs(a [4]float32, b [4]float32, c [4]float32, k uint8) [4]float32


// MaskzFmsubaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm_maskz_fmsubadd_ps'.
// Requires AVX512F.
func MaskzFmsubaddPs(k x86.Mmask8, a x86.M128, b x86.M128, c x86.M128) x86.M128 {
	return x86.M128(maskzFmsubaddPs(uint8(k), [4]float32(a), [4]float32(b), [4]float32(c)))
}

func maskzFmsubaddPs(k uint8, a [4]float32, b [4]float32, c [4]float32) [4]float32


// M256MaskFmsubaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm256_mask_fmsubadd_ps'.
// Requires AVX512F.
func M256MaskFmsubaddPs(a x86.M256, k x86.Mmask8, b x86.M256, c x86.M256) x86.M256 {
	return x86.M256(m256MaskFmsubaddPs([8]float32(a), uint8(k), [8]float32(b), [8]float32(c)))
}

func m256MaskFmsubaddPs(a [8]float32, k uint8, b [8]float32, c [8]float32) [8]float32


// M256Mask3FmsubaddPs: Multiply packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', alternatively subtract and add
// packed elements in 'c' from/to the intermediate result, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'c' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm256_mask3_fmsubadd_ps'.
// Requires AVX512F.
func M256Mask3FmsubaddPs(a x86.M256, b x86.M256, c x86.M256, k x86.Mmask8) x86.M256 {
	return x86.M256(m256Mask3FmsubaddPs([8]float32(a), [8]float32(b), [8]float32(c), uint8(k)))
}

func m256Mask3FmsubaddPs(a [8]float32, b [8]float32, c [8]float32, k uint8) [8]float32


// M256MaskzFmsubaddPs: Multiply packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', alternatively subtract and add
// packed elements in 'c' from/to the intermediate result, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm256_maskz_fmsubadd_ps'.
// Requires AVX512F.
func M256MaskzFmsubaddPs(k x86.Mmask8, a x86.M256, b x86.M256, c x86.M256) x86.M256 {
	return x86.M256(m256MaskzFmsubaddPs(uint8(k), [8]float32(a), [8]float32(b), [8]float32(c)))
}

func m256MaskzFmsubaddPs(k uint8, a [8]float32, b [8]float32, c [8]float32) [8]float32


// M512FmsubaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF (j is even) 
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm512_fmsubadd_ps'.
// Requires AVX512F.
func M512FmsubaddPs(a x86.M512, b x86.M512, c x86.M512) x86.M512 {
	return x86.M512(m512FmsubaddPs([16]float32(a), [16]float32(b), [16]float32(c)))
}

func m512FmsubaddPs(a [16]float32, b [16]float32, c [16]float32) [16]float32


// M512MaskFmsubaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm512_mask_fmsubadd_ps'.
// Requires AVX512F.
func M512MaskFmsubaddPs(a x86.M512, k x86.Mmask16, b x86.M512, c x86.M512) x86.M512 {
	return x86.M512(m512MaskFmsubaddPs([16]float32(a), uint16(k), [16]float32(b), [16]float32(c)))
}

func m512MaskFmsubaddPs(a [16]float32, k uint16, b [16]float32, c [16]float32) [16]float32


// M512Mask3FmsubaddPs: Multiply packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', alternatively subtract and add
// packed elements in 'c' from/to the intermediate result, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'c' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm512_mask3_fmsubadd_ps'.
// Requires AVX512F.
func M512Mask3FmsubaddPs(a x86.M512, b x86.M512, c x86.M512, k x86.Mmask16) x86.M512 {
	return x86.M512(m512Mask3FmsubaddPs([16]float32(a), [16]float32(b), [16]float32(c), uint16(k)))
}

func m512Mask3FmsubaddPs(a [16]float32, b [16]float32, c [16]float32, k uint16) [16]float32


// M512MaskzFmsubaddPs: Multiply packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', alternatively subtract and add
// packed elements in 'c' from/to the intermediate result, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm512_maskz_fmsubadd_ps'.
// Requires AVX512F.
func M512MaskzFmsubaddPs(k x86.Mmask16, a x86.M512, b x86.M512, c x86.M512) x86.M512 {
	return x86.M512(m512MaskzFmsubaddPs(uint16(k), [16]float32(a), [16]float32(b), [16]float32(c)))
}

func m512MaskzFmsubaddPs(k uint16, a [16]float32, b [16]float32, c [16]float32) [16]float32


// M512FmsubaddRoundPd: Multiply packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', alternatively subtract and add
// packed elements in 'c' from/to the intermediate result, and store the
// results in 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF (j is even) 
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm512_fmsubadd_round_pd'.
// Requires AVX512F.
func M512FmsubaddRoundPd(a x86.M512d, b x86.M512d, c x86.M512d, rounding int) x86.M512d {
	return x86.M512d(m512FmsubaddRoundPd([8]float64(a), [8]float64(b), [8]float64(c), rounding))
}

func m512FmsubaddRoundPd(a [8]float64, b [8]float64, c [8]float64, rounding int) [8]float64


// M512MaskFmsubaddRoundPd: Multiply packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', alternatively subtract and add
// packed elements in 'c' from/to the intermediate result, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). Rounding is done according to the
// 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm512_mask_fmsubadd_round_pd'.
// Requires AVX512F.
func M512MaskFmsubaddRoundPd(a x86.M512d, k x86.Mmask8, b x86.M512d, c x86.M512d, rounding int) x86.M512d {
	return x86.M512d(m512MaskFmsubaddRoundPd([8]float64(a), uint8(k), [8]float64(b), [8]float64(c), rounding))
}

func m512MaskFmsubaddRoundPd(a [8]float64, k uint8, b [8]float64, c [8]float64, rounding int) [8]float64


// M512Mask3FmsubaddRoundPd: Multiply packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', alternatively subtract and add
// packed elements in 'c' from/to the intermediate result, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'c' when the
// corresponding mask bit is not set).  Rounding is done according to the
// 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm512_mask3_fmsubadd_round_pd'.
// Requires AVX512F.
func M512Mask3FmsubaddRoundPd(a x86.M512d, b x86.M512d, c x86.M512d, k x86.Mmask8, rounding int) x86.M512d {
	return x86.M512d(m512Mask3FmsubaddRoundPd([8]float64(a), [8]float64(b), [8]float64(c), uint8(k), rounding))
}

func m512Mask3FmsubaddRoundPd(a [8]float64, b [8]float64, c [8]float64, k uint8, rounding int) [8]float64


// M512MaskzFmsubaddRoundPd: Multiply packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', alternatively subtract and add
// packed elements in 'c' from/to the intermediate result, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm512_maskz_fmsubadd_round_pd'.
// Requires AVX512F.
func M512MaskzFmsubaddRoundPd(k x86.Mmask8, a x86.M512d, b x86.M512d, c x86.M512d, rounding int) x86.M512d {
	return x86.M512d(m512MaskzFmsubaddRoundPd(uint8(k), [8]float64(a), [8]float64(b), [8]float64(c), rounding))
}

func m512MaskzFmsubaddRoundPd(k uint8, a [8]float64, b [8]float64, c [8]float64, rounding int) [8]float64


// M512FmsubaddRoundPs: Multiply packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', alternatively subtract and add
// packed elements in 'c' from/to the intermediate result, and store the
// results in 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF (j is even) 
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm512_fmsubadd_round_ps'.
// Requires AVX512F.
func M512FmsubaddRoundPs(a x86.M512, b x86.M512, c x86.M512, rounding int) x86.M512 {
	return x86.M512(m512FmsubaddRoundPs([16]float32(a), [16]float32(b), [16]float32(c), rounding))
}

func m512FmsubaddRoundPs(a [16]float32, b [16]float32, c [16]float32, rounding int) [16]float32


// M512MaskFmsubaddRoundPs: Multiply packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', alternatively subtract and add
// packed elements in 'c' from/to the intermediate result, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm512_mask_fmsubadd_round_ps'.
// Requires AVX512F.
func M512MaskFmsubaddRoundPs(a x86.M512, k x86.Mmask16, b x86.M512, c x86.M512, rounding int) x86.M512 {
	return x86.M512(m512MaskFmsubaddRoundPs([16]float32(a), uint16(k), [16]float32(b), [16]float32(c), rounding))
}

func m512MaskFmsubaddRoundPs(a [16]float32, k uint16, b [16]float32, c [16]float32, rounding int) [16]float32


// M512Mask3FmsubaddRoundPs: Multiply packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', alternatively subtract and add
// packed elements in 'c' from/to the intermediate result, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'c' when the
// corresponding mask bit is not set).  Rounding is done according to the
// 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm512_mask3_fmsubadd_round_ps'.
// Requires AVX512F.
func M512Mask3FmsubaddRoundPs(a x86.M512, b x86.M512, c x86.M512, k x86.Mmask16, rounding int) x86.M512 {
	return x86.M512(m512Mask3FmsubaddRoundPs([16]float32(a), [16]float32(b), [16]float32(c), uint16(k), rounding))
}

func m512Mask3FmsubaddRoundPs(a [16]float32, b [16]float32, c [16]float32, k uint16, rounding int) [16]float32


// M512MaskzFmsubaddRoundPs: Multiply packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', alternatively subtract and add
// packed elements in 'c' from/to the intermediate result, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm512_maskz_fmsubadd_round_ps'.
// Requires AVX512F.
func M512MaskzFmsubaddRoundPs(k x86.Mmask16, a x86.M512, b x86.M512, c x86.M512, rounding int) x86.M512 {
	return x86.M512(m512MaskzFmsubaddRoundPs(uint16(k), [16]float32(a), [16]float32(b), [16]float32(c), rounding))
}

func m512MaskzFmsubaddRoundPs(k uint16, a [16]float32, b [16]float32, c [16]float32, rounding int) [16]float32


// MaskFnmaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR	
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132PD, VFNMADD213PD, VFNMADD231PD'. Intrinsic: '_mm_mask_fnmadd_pd'.
// Requires AVX512F.
func MaskFnmaddPd(a x86.M128d, k x86.Mmask8, b x86.M128d, c x86.M128d) x86.M128d {
	return x86.M128d(maskFnmaddPd([2]float64(a), uint8(k), [2]float64(b), [2]float64(c)))
}

func maskFnmaddPd(a [2]float64, k uint8, b [2]float64, c [2]float64) [2]float64


// Mask3FnmaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR	
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132PD, VFNMADD213PD, VFNMADD231PD'. Intrinsic: '_mm_mask3_fnmadd_pd'.
// Requires AVX512F.
func Mask3FnmaddPd(a x86.M128d, b x86.M128d, c x86.M128d, k x86.Mmask8) x86.M128d {
	return x86.M128d(mask3FnmaddPd([2]float64(a), [2]float64(b), [2]float64(c), uint8(k)))
}

func mask3FnmaddPd(a [2]float64, b [2]float64, c [2]float64, k uint8) [2]float64


// MaskzFnmaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132PD, VFNMADD213PD, VFNMADD231PD'. Intrinsic: '_mm_maskz_fnmadd_pd'.
// Requires AVX512F.
func MaskzFnmaddPd(k x86.Mmask8, a x86.M128d, b x86.M128d, c x86.M128d) x86.M128d {
	return x86.M128d(maskzFnmaddPd(uint8(k), [2]float64(a), [2]float64(b), [2]float64(c)))
}

func maskzFnmaddPd(k uint8, a [2]float64, b [2]float64, c [2]float64) [2]float64


// M256MaskFnmaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMADD132PD, VFNMADD213PD, VFNMADD231PD'. Intrinsic: '_mm256_mask_fnmadd_pd'.
// Requires AVX512F.
func M256MaskFnmaddPd(a x86.M256d, k x86.Mmask8, b x86.M256d, c x86.M256d) x86.M256d {
	return x86.M256d(m256MaskFnmaddPd([4]float64(a), uint8(k), [4]float64(b), [4]float64(c)))
}

func m256MaskFnmaddPd(a [4]float64, k uint8, b [4]float64, c [4]float64) [4]float64


// M256Mask3FnmaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMADD132PD, VFNMADD213PD, VFNMADD231PD'. Intrinsic: '_mm256_mask3_fnmadd_pd'.
// Requires AVX512F.
func M256Mask3FnmaddPd(a x86.M256d, b x86.M256d, c x86.M256d, k x86.Mmask8) x86.M256d {
	return x86.M256d(m256Mask3FnmaddPd([4]float64(a), [4]float64(b), [4]float64(c), uint8(k)))
}

func m256Mask3FnmaddPd(a [4]float64, b [4]float64, c [4]float64, k uint8) [4]float64


// M256MaskzFnmaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMADD132PD, VFNMADD213PD, VFNMADD231PD'. Intrinsic: '_mm256_maskz_fnmadd_pd'.
// Requires AVX512F.
func M256MaskzFnmaddPd(k x86.Mmask8, a x86.M256d, b x86.M256d, c x86.M256d) x86.M256d {
	return x86.M256d(m256MaskzFnmaddPd(uint8(k), [4]float64(a), [4]float64(b), [4]float64(c)))
}

func m256MaskzFnmaddPd(k uint8, a [4]float64, b [4]float64, c [4]float64) [4]float64


// M512MaskzFnmaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMADD132PD, VFNMADD213PD, VFNMADD231PD'. Intrinsic: '_mm512_maskz_fnmadd_pd'.
// Requires AVX512F.
func M512MaskzFnmaddPd(k x86.Mmask8, a x86.M512d, b x86.M512d, c x86.M512d) x86.M512d {
	return x86.M512d(m512MaskzFnmaddPd(uint8(k), [8]float64(a), [8]float64(b), [8]float64(c)))
}

func m512MaskzFnmaddPd(k uint8, a [8]float64, b [8]float64, c [8]float64) [8]float64


// MaskFnmaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR	
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132PS, VFNMADD213PS, VFNMADD231PS'. Intrinsic: '_mm_mask_fnmadd_ps'.
// Requires AVX512F.
func MaskFnmaddPs(a x86.M128, k x86.Mmask8, b x86.M128, c x86.M128) x86.M128 {
	return x86.M128(maskFnmaddPs([4]float32(a), uint8(k), [4]float32(b), [4]float32(c)))
}

func maskFnmaddPs(a [4]float32, k uint8, b [4]float32, c [4]float32) [4]float32


// Mask3FnmaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR	
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132PS, VFNMADD213PS, VFNMADD231PS'. Intrinsic: '_mm_mask3_fnmadd_ps'.
// Requires AVX512F.
func Mask3FnmaddPs(a x86.M128, b x86.M128, c x86.M128, k x86.Mmask8) x86.M128 {
	return x86.M128(mask3FnmaddPs([4]float32(a), [4]float32(b), [4]float32(c), uint8(k)))
}

func mask3FnmaddPs(a [4]float32, b [4]float32, c [4]float32, k uint8) [4]float32


// MaskzFnmaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132PS, VFNMADD213PS, VFNMADD231PS'. Intrinsic: '_mm_maskz_fnmadd_ps'.
// Requires AVX512F.
func MaskzFnmaddPs(k x86.Mmask8, a x86.M128, b x86.M128, c x86.M128) x86.M128 {
	return x86.M128(maskzFnmaddPs(uint8(k), [4]float32(a), [4]float32(b), [4]float32(c)))
}

func maskzFnmaddPs(k uint8, a [4]float32, b [4]float32, c [4]float32) [4]float32


// M256MaskFnmaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMADD132PS, VFNMADD213PS, VFNMADD231PS'. Intrinsic: '_mm256_mask_fnmadd_ps'.
// Requires AVX512F.
func M256MaskFnmaddPs(a x86.M256, k x86.Mmask8, b x86.M256, c x86.M256) x86.M256 {
	return x86.M256(m256MaskFnmaddPs([8]float32(a), uint8(k), [8]float32(b), [8]float32(c)))
}

func m256MaskFnmaddPs(a [8]float32, k uint8, b [8]float32, c [8]float32) [8]float32


// M256Mask3FnmaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMADD132PS, VFNMADD213PS, VFNMADD231PS'. Intrinsic: '_mm256_mask3_fnmadd_ps'.
// Requires AVX512F.
func M256Mask3FnmaddPs(a x86.M256, b x86.M256, c x86.M256, k x86.Mmask8) x86.M256 {
	return x86.M256(m256Mask3FnmaddPs([8]float32(a), [8]float32(b), [8]float32(c), uint8(k)))
}

func m256Mask3FnmaddPs(a [8]float32, b [8]float32, c [8]float32, k uint8) [8]float32


// M256MaskzFnmaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMADD132PS, VFNMADD213PS, VFNMADD231PS'. Intrinsic: '_mm256_maskz_fnmadd_ps'.
// Requires AVX512F.
func M256MaskzFnmaddPs(k x86.Mmask8, a x86.M256, b x86.M256, c x86.M256) x86.M256 {
	return x86.M256(m256MaskzFnmaddPs(uint8(k), [8]float32(a), [8]float32(b), [8]float32(c)))
}

func m256MaskzFnmaddPs(k uint8, a [8]float32, b [8]float32, c [8]float32) [8]float32


// M512MaskzFnmaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMADD132PS, VFNMADD213PS, VFNMADD231PS'. Intrinsic: '_mm512_maskz_fnmadd_ps'.
// Requires AVX512F.
func M512MaskzFnmaddPs(k x86.Mmask16, a x86.M512, b x86.M512, c x86.M512) x86.M512 {
	return x86.M512(m512MaskzFnmaddPs(uint16(k), [16]float32(a), [16]float32(b), [16]float32(c)))
}

func m512MaskzFnmaddPs(k uint16, a [16]float32, b [16]float32, c [16]float32) [16]float32


// M512MaskzFnmaddRoundPd: Multiply packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', add the negated intermediate result
// to packed elements in 'c', and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set).
// Rounding is done according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMADD132PD, VFNMADD213PD, VFNMADD231PD'. Intrinsic: '_mm512_maskz_fnmadd_round_pd'.
// Requires AVX512F.
func M512MaskzFnmaddRoundPd(k x86.Mmask8, a x86.M512d, b x86.M512d, c x86.M512d, rounding int) x86.M512d {
	return x86.M512d(m512MaskzFnmaddRoundPd(uint8(k), [8]float64(a), [8]float64(b), [8]float64(c), rounding))
}

func m512MaskzFnmaddRoundPd(k uint8, a [8]float64, b [8]float64, c [8]float64, rounding int) [8]float64


// M512MaskzFnmaddRoundPs: Multiply packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', add the negated intermediate result
// to packed elements in 'c', and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set).
// Rounding is done according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMADD132PS, VFNMADD213PS, VFNMADD231PS'. Intrinsic: '_mm512_maskz_fnmadd_round_ps'.
// Requires AVX512F.
func M512MaskzFnmaddRoundPs(k x86.Mmask16, a x86.M512, b x86.M512, c x86.M512, rounding int) x86.M512 {
	return x86.M512(m512MaskzFnmaddRoundPs(uint16(k), [16]float32(a), [16]float32(b), [16]float32(c), rounding))
}

func m512MaskzFnmaddRoundPs(k uint16, a [16]float32, b [16]float32, c [16]float32, rounding int) [16]float32


// MaskFnmaddRoundSd: Multiply the lower double-precision (64-bit)
// floating-point elements in 'a' and 'b', and add the negated intermediate
// result to the lower element in 'c'. Store the result in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'a' when mask bit 0 is
// not set), and copy the upper element from 'a' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := -(a[63:0] * b[63:0]) + c[63:0]
//		ELSE
//			dst[63:0] := a[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132SD, VFNMADD213SD, VFNMADD231SD'. Intrinsic: '_mm_mask_fnmadd_round_sd'.
// Requires AVX512F.
func MaskFnmaddRoundSd(a x86.M128d, k x86.Mmask8, b x86.M128d, c x86.M128d, rounding int) x86.M128d {
	return x86.M128d(maskFnmaddRoundSd([2]float64(a), uint8(k), [2]float64(b), [2]float64(c), rounding))
}

func maskFnmaddRoundSd(a [2]float64, k uint8, b [2]float64, c [2]float64, rounding int) [2]float64


// Mask3FnmaddRoundSd: Multiply the lower double-precision (64-bit)
// floating-point elements in 'a' and 'b', and add the negated intermediate
// result to the lower element in 'c'. Store the result in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'c' when mask bit 0 is
// not set), and copy the upper element from 'a' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := -(a[63:0] * b[63:0]) + c[63:0]
//		ELSE
//			dst[63:0] := c[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132SD, VFNMADD213SD, VFNMADD231SD'. Intrinsic: '_mm_mask3_fnmadd_round_sd'.
// Requires AVX512F.
func Mask3FnmaddRoundSd(a x86.M128d, b x86.M128d, c x86.M128d, k x86.Mmask8, rounding int) x86.M128d {
	return x86.M128d(mask3FnmaddRoundSd([2]float64(a), [2]float64(b), [2]float64(c), uint8(k), rounding))
}

func mask3FnmaddRoundSd(a [2]float64, b [2]float64, c [2]float64, k uint8, rounding int) [2]float64


// MaskzFnmaddRoundSd: Multiply the lower double-precision (64-bit)
// floating-point elements in 'a' and 'b', and add the negated intermediate
// result to the lower element in 'c'. Store the result in the lower element of
// 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is not
// set), and copy the upper element from 'a' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := -(a[63:0] * b[63:0]) + c[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132SD, VFNMADD213SD, VFNMADD231SD'. Intrinsic: '_mm_maskz_fnmadd_round_sd'.
// Requires AVX512F.
func MaskzFnmaddRoundSd(k x86.Mmask8, a x86.M128d, b x86.M128d, c x86.M128d, rounding int) x86.M128d {
	return x86.M128d(maskzFnmaddRoundSd(uint8(k), [2]float64(a), [2]float64(b), [2]float64(c), rounding))
}

func maskzFnmaddRoundSd(k uint8, a [2]float64, b [2]float64, c [2]float64, rounding int) [2]float64


// MaskFnmaddRoundSs: Multiply the lower single-precision (32-bit)
// floating-point elements in 'a' and 'b', and add the negated intermediate
// result to the lower element in 'c'. Store the result in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'a' when mask bit 0 is
// not set), and copy the upper 3 packed elements from 'a' to the upper
// elements of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := -(a[31:0] * b[31:0]) + c[31:0]
//		ELSE
//			dst[31:0] := a[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132SS, VFNMADD213SS, VFNMADD231SS'. Intrinsic: '_mm_mask_fnmadd_round_ss'.
// Requires AVX512F.
func MaskFnmaddRoundSs(a x86.M128, k x86.Mmask8, b x86.M128, c x86.M128, rounding int) x86.M128 {
	return x86.M128(maskFnmaddRoundSs([4]float32(a), uint8(k), [4]float32(b), [4]float32(c), rounding))
}

func maskFnmaddRoundSs(a [4]float32, k uint8, b [4]float32, c [4]float32, rounding int) [4]float32


// Mask3FnmaddRoundSs: Multiply the lower single-precision (32-bit)
// floating-point elements in 'a' and 'b', and add the negated intermediate
// result to the lower element in 'c'. Store the result in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'c' when mask bit 0 is
// not set), and copy the upper 3 packed elements from 'a' to the upper
// elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := -(a[31:0] * b[31:0]) + c[31:0]
//		ELSE
//			dst[31:0] := c[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132SS, VFNMADD213SS, VFNMADD231SS'. Intrinsic: '_mm_mask3_fnmadd_round_ss'.
// Requires AVX512F.
func Mask3FnmaddRoundSs(a x86.M128, b x86.M128, c x86.M128, k x86.Mmask8, rounding int) x86.M128 {
	return x86.M128(mask3FnmaddRoundSs([4]float32(a), [4]float32(b), [4]float32(c), uint8(k), rounding))
}

func mask3FnmaddRoundSs(a [4]float32, b [4]float32, c [4]float32, k uint8, rounding int) [4]float32


// MaskzFnmaddRoundSs: Multiply the lower single-precision (32-bit)
// floating-point elements in 'a' and 'b', and add the negated intermediate
// result to the lower element in 'c'. Store the result in the lower element of
// 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is not
// set), and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := -(a[31:0] * b[31:0]) + c[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132SS, VFNMADD213SS, VFNMADD231SS'. Intrinsic: '_mm_maskz_fnmadd_round_ss'.
// Requires AVX512F.
func MaskzFnmaddRoundSs(k x86.Mmask8, a x86.M128, b x86.M128, c x86.M128, rounding int) x86.M128 {
	return x86.M128(maskzFnmaddRoundSs(uint8(k), [4]float32(a), [4]float32(b), [4]float32(c), rounding))
}

func maskzFnmaddRoundSs(k uint8, a [4]float32, b [4]float32, c [4]float32, rounding int) [4]float32


// MaskFnmaddSd: Multiply the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', and add the negated intermediate result to the
// lower element in 'c'. Store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'a' when mask bit 0 is not set),
// and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := -(a[63:0] * b[63:0]) + c[63:0]
//		ELSE
//			dst[63:0] := a[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132SD, VFNMADD213SD, VFNMADD231SD'. Intrinsic: '_mm_mask_fnmadd_sd'.
// Requires AVX512F.
func MaskFnmaddSd(a x86.M128d, k x86.Mmask8, b x86.M128d, c x86.M128d) x86.M128d {
	return x86.M128d(maskFnmaddSd([2]float64(a), uint8(k), [2]float64(b), [2]float64(c)))
}

func maskFnmaddSd(a [2]float64, k uint8, b [2]float64, c [2]float64) [2]float64


// Mask3FnmaddSd: Multiply the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', and add the negated intermediate result to the
// lower element in 'c'. Store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'c' when mask bit 0 is not set),
// and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := -(a[63:0] * b[63:0]) + c[63:0]
//		ELSE
//			dst[63:0] := c[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132SD, VFNMADD213SD, VFNMADD231SD'. Intrinsic: '_mm_mask3_fnmadd_sd'.
// Requires AVX512F.
func Mask3FnmaddSd(a x86.M128d, b x86.M128d, c x86.M128d, k x86.Mmask8) x86.M128d {
	return x86.M128d(mask3FnmaddSd([2]float64(a), [2]float64(b), [2]float64(c), uint8(k)))
}

func mask3FnmaddSd(a [2]float64, b [2]float64, c [2]float64, k uint8) [2]float64


// MaskzFnmaddSd: Multiply the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', and add the negated intermediate result to the
// lower element in 'c'. Store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := -(a[63:0] * b[63:0]) + c[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD213SD, VFNMADD231SD, VFNMADD132SD'. Intrinsic: '_mm_maskz_fnmadd_sd'.
// Requires AVX512F.
func MaskzFnmaddSd(k x86.Mmask8, a x86.M128d, b x86.M128d, c x86.M128d) x86.M128d {
	return x86.M128d(maskzFnmaddSd(uint8(k), [2]float64(a), [2]float64(b), [2]float64(c)))
}

func maskzFnmaddSd(k uint8, a [2]float64, b [2]float64, c [2]float64) [2]float64


// MaskFnmaddSs: Multiply the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', and add the negated intermediate result to the
// lower element in 'c'. Store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'a' when mask bit 0 is not set),
// and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'. 
//
//		IF k[0]
//			dst[31:0] := -(a[31:0] * b[31:0]) + c[31:0]
//		ELSE
//			dst[31:0] := a[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132SS, VFNMADD213SS, VFNMADD231SS'. Intrinsic: '_mm_mask_fnmadd_ss'.
// Requires AVX512F.
func MaskFnmaddSs(a x86.M128, k x86.Mmask8, b x86.M128, c x86.M128) x86.M128 {
	return x86.M128(maskFnmaddSs([4]float32(a), uint8(k), [4]float32(b), [4]float32(c)))
}

func maskFnmaddSs(a [4]float32, k uint8, b [4]float32, c [4]float32) [4]float32


// Mask3FnmaddSs: Multiply the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', and add the negated intermediate result to the
// lower element in 'c'. Store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'c' when mask bit 0 is not set),
// and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'. 
//
//		IF k[0]
//			dst[31:0] := -(a[31:0] * b[31:0]) + c[31:0]
//		ELSE
//			dst[31:0] := c[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132SS, VFNMADD213SS, VFNMADD231SS'. Intrinsic: '_mm_mask3_fnmadd_ss'.
// Requires AVX512F.
func Mask3FnmaddSs(a x86.M128, b x86.M128, c x86.M128, k x86.Mmask8) x86.M128 {
	return x86.M128(mask3FnmaddSs([4]float32(a), [4]float32(b), [4]float32(c), uint8(k)))
}

func mask3FnmaddSs(a [4]float32, b [4]float32, c [4]float32, k uint8) [4]float32


// MaskzFnmaddSs: Multiply the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', and add the negated intermediate result to the
// lower element in 'c'. Store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := -(a[31:0] * b[31:0]) + c[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132SS, VFNMADD213SS, VFNMADD231SS'. Intrinsic: '_mm_maskz_fnmadd_ss'.
// Requires AVX512F.
func MaskzFnmaddSs(k x86.Mmask8, a x86.M128, b x86.M128, c x86.M128) x86.M128 {
	return x86.M128(maskzFnmaddSs(uint8(k), [4]float32(a), [4]float32(b), [4]float32(c)))
}

func maskzFnmaddSs(k uint8, a [4]float32, b [4]float32, c [4]float32) [4]float32


// MaskFnmsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR	
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD'. Intrinsic: '_mm_mask_fnmsub_pd'.
// Requires AVX512F.
func MaskFnmsubPd(a x86.M128d, k x86.Mmask8, b x86.M128d, c x86.M128d) x86.M128d {
	return x86.M128d(maskFnmsubPd([2]float64(a), uint8(k), [2]float64(b), [2]float64(c)))
}

func maskFnmsubPd(a [2]float64, k uint8, b [2]float64, c [2]float64) [2]float64


// Mask3FnmsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR	
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD'. Intrinsic: '_mm_mask3_fnmsub_pd'.
// Requires AVX512F.
func Mask3FnmsubPd(a x86.M128d, b x86.M128d, c x86.M128d, k x86.Mmask8) x86.M128d {
	return x86.M128d(mask3FnmsubPd([2]float64(a), [2]float64(b), [2]float64(c), uint8(k)))
}

func mask3FnmsubPd(a [2]float64, b [2]float64, c [2]float64, k uint8) [2]float64


// MaskzFnmsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD'. Intrinsic: '_mm_maskz_fnmsub_pd'.
// Requires AVX512F.
func MaskzFnmsubPd(k x86.Mmask8, a x86.M128d, b x86.M128d, c x86.M128d) x86.M128d {
	return x86.M128d(maskzFnmsubPd(uint8(k), [2]float64(a), [2]float64(b), [2]float64(c)))
}

func maskzFnmsubPd(k uint8, a [2]float64, b [2]float64, c [2]float64) [2]float64


// M256MaskFnmsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD'. Intrinsic: '_mm256_mask_fnmsub_pd'.
// Requires AVX512F.
func M256MaskFnmsubPd(a x86.M256d, k x86.Mmask8, b x86.M256d, c x86.M256d) x86.M256d {
	return x86.M256d(m256MaskFnmsubPd([4]float64(a), uint8(k), [4]float64(b), [4]float64(c)))
}

func m256MaskFnmsubPd(a [4]float64, k uint8, b [4]float64, c [4]float64) [4]float64


// M256Mask3FnmsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD'. Intrinsic: '_mm256_mask3_fnmsub_pd'.
// Requires AVX512F.
func M256Mask3FnmsubPd(a x86.M256d, b x86.M256d, c x86.M256d, k x86.Mmask8) x86.M256d {
	return x86.M256d(m256Mask3FnmsubPd([4]float64(a), [4]float64(b), [4]float64(c), uint8(k)))
}

func m256Mask3FnmsubPd(a [4]float64, b [4]float64, c [4]float64, k uint8) [4]float64


// M256MaskzFnmsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD'. Intrinsic: '_mm256_maskz_fnmsub_pd'.
// Requires AVX512F.
func M256MaskzFnmsubPd(k x86.Mmask8, a x86.M256d, b x86.M256d, c x86.M256d) x86.M256d {
	return x86.M256d(m256MaskzFnmsubPd(uint8(k), [4]float64(a), [4]float64(b), [4]float64(c)))
}

func m256MaskzFnmsubPd(k uint8, a [4]float64, b [4]float64, c [4]float64) [4]float64


// M512MaskzFnmsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD'. Intrinsic: '_mm512_maskz_fnmsub_pd'.
// Requires AVX512F.
func M512MaskzFnmsubPd(k x86.Mmask8, a x86.M512d, b x86.M512d, c x86.M512d) x86.M512d {
	return x86.M512d(m512MaskzFnmsubPd(uint8(k), [8]float64(a), [8]float64(b), [8]float64(c)))
}

func m512MaskzFnmsubPd(k uint8, a [8]float64, b [8]float64, c [8]float64) [8]float64


// MaskFnmsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR	
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS'. Intrinsic: '_mm_mask_fnmsub_ps'.
// Requires AVX512F.
func MaskFnmsubPs(a x86.M128, k x86.Mmask8, b x86.M128, c x86.M128) x86.M128 {
	return x86.M128(maskFnmsubPs([4]float32(a), uint8(k), [4]float32(b), [4]float32(c)))
}

func maskFnmsubPs(a [4]float32, k uint8, b [4]float32, c [4]float32) [4]float32


// Mask3FnmsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR	
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS'. Intrinsic: '_mm_mask3_fnmsub_ps'.
// Requires AVX512F.
func Mask3FnmsubPs(a x86.M128, b x86.M128, c x86.M128, k x86.Mmask8) x86.M128 {
	return x86.M128(mask3FnmsubPs([4]float32(a), [4]float32(b), [4]float32(c), uint8(k)))
}

func mask3FnmsubPs(a [4]float32, b [4]float32, c [4]float32, k uint8) [4]float32


// MaskzFnmsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS'. Intrinsic: '_mm_maskz_fnmsub_ps'.
// Requires AVX512F.
func MaskzFnmsubPs(k x86.Mmask8, a x86.M128, b x86.M128, c x86.M128) x86.M128 {
	return x86.M128(maskzFnmsubPs(uint8(k), [4]float32(a), [4]float32(b), [4]float32(c)))
}

func maskzFnmsubPs(k uint8, a [4]float32, b [4]float32, c [4]float32) [4]float32


// M256MaskFnmsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS'. Intrinsic: '_mm256_mask_fnmsub_ps'.
// Requires AVX512F.
func M256MaskFnmsubPs(a x86.M256, k x86.Mmask8, b x86.M256, c x86.M256) x86.M256 {
	return x86.M256(m256MaskFnmsubPs([8]float32(a), uint8(k), [8]float32(b), [8]float32(c)))
}

func m256MaskFnmsubPs(a [8]float32, k uint8, b [8]float32, c [8]float32) [8]float32


// M256Mask3FnmsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS'. Intrinsic: '_mm256_mask3_fnmsub_ps'.
// Requires AVX512F.
func M256Mask3FnmsubPs(a x86.M256, b x86.M256, c x86.M256, k x86.Mmask8) x86.M256 {
	return x86.M256(m256Mask3FnmsubPs([8]float32(a), [8]float32(b), [8]float32(c), uint8(k)))
}

func m256Mask3FnmsubPs(a [8]float32, b [8]float32, c [8]float32, k uint8) [8]float32


// M256MaskzFnmsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS'. Intrinsic: '_mm256_maskz_fnmsub_ps'.
// Requires AVX512F.
func M256MaskzFnmsubPs(k x86.Mmask8, a x86.M256, b x86.M256, c x86.M256) x86.M256 {
	return x86.M256(m256MaskzFnmsubPs(uint8(k), [8]float32(a), [8]float32(b), [8]float32(c)))
}

func m256MaskzFnmsubPs(k uint8, a [8]float32, b [8]float32, c [8]float32) [8]float32


// M512MaskzFnmsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS'. Intrinsic: '_mm512_maskz_fnmsub_ps'.
// Requires AVX512F.
func M512MaskzFnmsubPs(k x86.Mmask16, a x86.M512, b x86.M512, c x86.M512) x86.M512 {
	return x86.M512(m512MaskzFnmsubPs(uint16(k), [16]float32(a), [16]float32(b), [16]float32(c)))
}

func m512MaskzFnmsubPs(k uint16, a [16]float32, b [16]float32, c [16]float32) [16]float32


// M512MaskzFnmsubRoundPd: Multiply packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', subtract packed elements in 'c' from
// the negated intermediate result, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). Rounding is done according to the 'rounding' parameter, which can be
// one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD'. Intrinsic: '_mm512_maskz_fnmsub_round_pd'.
// Requires AVX512F.
func M512MaskzFnmsubRoundPd(k x86.Mmask8, a x86.M512d, b x86.M512d, c x86.M512d, rounding int) x86.M512d {
	return x86.M512d(m512MaskzFnmsubRoundPd(uint8(k), [8]float64(a), [8]float64(b), [8]float64(c), rounding))
}

func m512MaskzFnmsubRoundPd(k uint8, a [8]float64, b [8]float64, c [8]float64, rounding int) [8]float64


// M512MaskzFnmsubRoundPs: Multiply packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', subtract packed elements in 'c' from
// the negated intermediate result, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS'. Intrinsic: '_mm512_maskz_fnmsub_round_ps'.
// Requires AVX512F.
func M512MaskzFnmsubRoundPs(k x86.Mmask16, a x86.M512, b x86.M512, c x86.M512, rounding int) x86.M512 {
	return x86.M512(m512MaskzFnmsubRoundPs(uint16(k), [16]float32(a), [16]float32(b), [16]float32(c), rounding))
}

func m512MaskzFnmsubRoundPs(k uint16, a [16]float32, b [16]float32, c [16]float32, rounding int) [16]float32


// MaskFnmsubRoundSd: Multiply the lower double-precision (64-bit)
// floating-point elements in 'a' and 'b', and subtract the lower element in
// 'c' from the negated intermediate result. Store the result in the lower
// element of 'dst' using writemask 'k' (the element is copied from 'c' when
// mask bit 0 is not set), and copy the upper element from 'a' to the upper
// element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := -(a[63:0] * b[63:0]) - c[63:0]
//		ELSE
//			dst[63:0] := a[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132SD, VFNMSUB213SD, VFNMSUB231SD'. Intrinsic: '_mm_mask_fnmsub_round_sd'.
// Requires AVX512F.
func MaskFnmsubRoundSd(a x86.M128d, k x86.Mmask8, b x86.M128d, c x86.M128d, rounding int) x86.M128d {
	return x86.M128d(maskFnmsubRoundSd([2]float64(a), uint8(k), [2]float64(b), [2]float64(c), rounding))
}

func maskFnmsubRoundSd(a [2]float64, k uint8, b [2]float64, c [2]float64, rounding int) [2]float64


// Mask3FnmsubRoundSd: Multiply the lower double-precision (64-bit)
// floating-point elements in 'a' and 'b', and subtract the lower element in
// 'c' from the negated intermediate result. Store the result in the lower
// element of 'dst' using writemask 'k' (the element is copied from 'c' when
// mask bit 0 is not set), and copy the upper element from 'a' to the upper
// element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := -(a[63:0] * b[63:0]) - c[63:0]
//		ELSE
//			dst[63:0] := c[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132SD, VFNMSUB213SD, VFNMSUB231SD'. Intrinsic: '_mm_mask3_fnmsub_round_sd'.
// Requires AVX512F.
func Mask3FnmsubRoundSd(a x86.M128d, b x86.M128d, c x86.M128d, k x86.Mmask8, rounding int) x86.M128d {
	return x86.M128d(mask3FnmsubRoundSd([2]float64(a), [2]float64(b), [2]float64(c), uint8(k), rounding))
}

func mask3FnmsubRoundSd(a [2]float64, b [2]float64, c [2]float64, k uint8, rounding int) [2]float64


// MaskzFnmsubRoundSd: Multiply the lower double-precision (64-bit)
// floating-point elements in 'a' and 'b', and subtract the lower element in
// 'c' from the negated intermediate result. Store the result in 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper element from 'a' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := -(a[63:0] * b[63:0]) - c[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132SD, VFNMSUB213SD, VFNMSUB231SD'. Intrinsic: '_mm_maskz_fnmsub_round_sd'.
// Requires AVX512F.
func MaskzFnmsubRoundSd(k x86.Mmask8, a x86.M128d, b x86.M128d, c x86.M128d, rounding int) x86.M128d {
	return x86.M128d(maskzFnmsubRoundSd(uint8(k), [2]float64(a), [2]float64(b), [2]float64(c), rounding))
}

func maskzFnmsubRoundSd(k uint8, a [2]float64, b [2]float64, c [2]float64, rounding int) [2]float64


// MaskFnmsubRoundSs: Multiply the lower single-precision (32-bit)
// floating-point elements in 'a' and 'b', and subtract the lower element in
// 'c' from the negated intermediate result. Store the result in the lower
// element of 'dst' using writemask 'k' (the element is copied from 'c' when
// mask bit 0 is not set), and copy the upper 3 packed elements from 'a' to the
// upper elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := -(a[31:0] * b[31:0]) - c[31:0]
//		ELSE
//			dst[31:0] := a[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132SS, VFNMSUB213SS, VFNMSUB231SS'. Intrinsic: '_mm_mask_fnmsub_round_ss'.
// Requires AVX512F.
func MaskFnmsubRoundSs(a x86.M128, k x86.Mmask8, b x86.M128, c x86.M128, rounding int) x86.M128 {
	return x86.M128(maskFnmsubRoundSs([4]float32(a), uint8(k), [4]float32(b), [4]float32(c), rounding))
}

func maskFnmsubRoundSs(a [4]float32, k uint8, b [4]float32, c [4]float32, rounding int) [4]float32


// Mask3FnmsubRoundSs: Multiply the lower single-precision (32-bit)
// floating-point elements in 'a' and 'b', subtract the lower element in 'c'
// from the negated intermediate result, store the result in the lower element
// of 'dst', and copy the upper element from 'a' to the upper element of 'dst'
// using writemask 'k' (elements are copied from 'c' when the corresponding
// mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := -(a[31:0] * b[31:0]) - c[31:0]
//		ELSE
//			dst[31:0] := c[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132SS, VFNMSUB213SS, VFNMSUB231SS'. Intrinsic: '_mm_mask3_fnmsub_round_ss'.
// Requires AVX512F.
func Mask3FnmsubRoundSs(a x86.M128, b x86.M128, c x86.M128, k x86.Mmask8, rounding int) x86.M128 {
	return x86.M128(mask3FnmsubRoundSs([4]float32(a), [4]float32(b), [4]float32(c), uint8(k), rounding))
}

func mask3FnmsubRoundSs(a [4]float32, b [4]float32, c [4]float32, k uint8, rounding int) [4]float32


// MaskzFnmsubRoundSs: Multiply the lower single-precision (32-bit)
// floating-point elements in 'a' and 'b', and subtract the lower element in
// 'c' from the negated intermediate result. Store the result in the lower
// element of 'dst' using zeromask 'k' (the element is zeroed out when mask bit
// 0 is not set), and copy the upper 3 packed elements from 'a' to the upper
// elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := -(a[31:0] * b[31:0]) - c[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132SS, VFNMSUB213SS, VFNMSUB231SS'. Intrinsic: '_mm_maskz_fnmsub_round_ss'.
// Requires AVX512F.
func MaskzFnmsubRoundSs(k x86.Mmask8, a x86.M128, b x86.M128, c x86.M128, rounding int) x86.M128 {
	return x86.M128(maskzFnmsubRoundSs(uint8(k), [4]float32(a), [4]float32(b), [4]float32(c), rounding))
}

func maskzFnmsubRoundSs(k uint8, a [4]float32, b [4]float32, c [4]float32, rounding int) [4]float32


// MaskFnmsubSd: Multiply the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', and subtract the lower element in 'c' from the
// negated intermediate result. Store the result in the lower element of 'dst'
// using writemask 'k' (the element is copied from 'c' when mask bit 0 is not
// set), and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := -(a[63:0] * b[63:0]) - c[63:0]
//		ELSE
//			dst[63:0] := a[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132SD, VFNMSUB213SD, VFNMSUB231SD'. Intrinsic: '_mm_mask_fnmsub_sd'.
// Requires AVX512F.
func MaskFnmsubSd(a x86.M128d, k x86.Mmask8, b x86.M128d, c x86.M128d) x86.M128d {
	return x86.M128d(maskFnmsubSd([2]float64(a), uint8(k), [2]float64(b), [2]float64(c)))
}

func maskFnmsubSd(a [2]float64, k uint8, b [2]float64, c [2]float64) [2]float64


// Mask3FnmsubSd: Multiply the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', and subtract the lower element in 'c' from the
// negated intermediate result. Store the result in the lower element of 'dst'
// using writemask 'k' (the element is copied from 'c' when mask bit 0 is not
// set), and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := -(a[63:0] * b[63:0]) - c[63:0]
//		ELSE
//			dst[63:0] := c[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132SD, VFNMSUB213SD, VFNMSUB231SD'. Intrinsic: '_mm_mask3_fnmsub_sd'.
// Requires AVX512F.
func Mask3FnmsubSd(a x86.M128d, b x86.M128d, c x86.M128d, k x86.Mmask8) x86.M128d {
	return x86.M128d(mask3FnmsubSd([2]float64(a), [2]float64(b), [2]float64(c), uint8(k)))
}

func mask3FnmsubSd(a [2]float64, b [2]float64, c [2]float64, k uint8) [2]float64


// MaskzFnmsubSd: Multiply the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', and subtract the lower element in 'c' from the
// negated intermediate result. Store the result in 'dst' using zeromask 'k'
// (the element is zeroed out when mask bit 0 is not set), and copy the upper
// element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := -(a[63:0] * b[63:0]) - c[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132SD, VFNMSUB213SD, VFNMSUB231SD'. Intrinsic: '_mm_maskz_fnmsub_sd'.
// Requires AVX512F.
func MaskzFnmsubSd(k x86.Mmask8, a x86.M128d, b x86.M128d, c x86.M128d) x86.M128d {
	return x86.M128d(maskzFnmsubSd(uint8(k), [2]float64(a), [2]float64(b), [2]float64(c)))
}

func maskzFnmsubSd(k uint8, a [2]float64, b [2]float64, c [2]float64) [2]float64


// MaskFnmsubSs: Multiply the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', and subtract the lower element in 'c' from the
// negated intermediate result. Store the result in the lower element of 'dst'
// using writemask 'k' (the element is copied from 'c' when mask bit 0 is not
// set), and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'. 
//
//		IF k[0]
//			dst[31:0] := -(a[31:0] * b[31:0]) - c[31:0]
//		ELSE
//			dst[31:0] := a[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132SS, VFNMSUB213SS, VFNMSUB231SS'. Intrinsic: '_mm_mask_fnmsub_ss'.
// Requires AVX512F.
func MaskFnmsubSs(a x86.M128, k x86.Mmask8, b x86.M128, c x86.M128) x86.M128 {
	return x86.M128(maskFnmsubSs([4]float32(a), uint8(k), [4]float32(b), [4]float32(c)))
}

func maskFnmsubSs(a [4]float32, k uint8, b [4]float32, c [4]float32) [4]float32


// Mask3FnmsubSs: Multiply the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', and subtract the lower element in 'c' from the
// negated intermediate result. Store the result in the lower element of 'dst',
// and copy the upper element from 'a' to the upper element of 'dst' using
// writemask 'k' (elements are copied from 'c' when the corresponding mask bit
// is not set). 
//
//		IF k[0]
//			dst[31:0] := -(a[31:0] * b[31:0]) - c[31:0]
//		ELSE
//			dst[31:0] := c[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132SS, VFNMSUB213SS, VFNMSUB231SS'. Intrinsic: '_mm_mask3_fnmsub_ss'.
// Requires AVX512F.
func Mask3FnmsubSs(a x86.M128, b x86.M128, c x86.M128, k x86.Mmask8) x86.M128 {
	return x86.M128(mask3FnmsubSs([4]float32(a), [4]float32(b), [4]float32(c), uint8(k)))
}

func mask3FnmsubSs(a [4]float32, b [4]float32, c [4]float32, k uint8) [4]float32


// MaskzFnmsubSs: Multiply the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', and subtract the lower element in 'c' from the
// negated intermediate result. Store the result in the lower element of 'dst'
// using zeromask 'k' (the element is zeroed out when mask bit 0 is not set),
// and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'. 
//
//		IF k[0]
//			dst[31:0] := -(a[31:0] * b[31:0]) - c[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132SS, VFNMSUB213SS, VFNMSUB231SS'. Intrinsic: '_mm_maskz_fnmsub_ss'.
// Requires AVX512F.
func MaskzFnmsubSs(k x86.Mmask8, a x86.M128, b x86.M128, c x86.M128) x86.M128 {
	return x86.M128(maskzFnmsubSs(uint8(k), [4]float32(a), [4]float32(b), [4]float32(c)))
}

func maskzFnmsubSs(k uint8, a [4]float32, b [4]float32, c [4]float32) [4]float32


// GetexpPd: Convert the exponent of each packed double-precision (64-bit)
// floating-point element in 'a' to a double-precision (64-bit) floating-point
// number representing the integer exponent, and store the results in 'dst'.
// This intrinsic essentially calculates 'floor(log2(x))' for each element. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := ConvertExpFP64(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPPD'. Intrinsic: '_mm_getexp_pd'.
// Requires AVX512F.
func GetexpPd(a x86.M128d) x86.M128d {
	return x86.M128d(getexpPd([2]float64(a)))
}

func getexpPd(a [2]float64) [2]float64


// MaskGetexpPd: Convert the exponent of each packed double-precision (64-bit)
// floating-point element in 'a' to a double-precision (64-bit) floating-point
// number representing the integer exponent, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). This intrinsic essentially calculates 'floor(log2(x))'
// for each element. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ConvertExpFP64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPPD'. Intrinsic: '_mm_mask_getexp_pd'.
// Requires AVX512F.
func MaskGetexpPd(src x86.M128d, k x86.Mmask8, a x86.M128d) x86.M128d {
	return x86.M128d(maskGetexpPd([2]float64(src), uint8(k), [2]float64(a)))
}

func maskGetexpPd(src [2]float64, k uint8, a [2]float64) [2]float64


// MaskzGetexpPd: Convert the exponent of each packed double-precision (64-bit)
// floating-point element in 'a' to a double-precision (64-bit) floating-point
// number representing the integer exponent, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). This intrinsic essentially calculates 'floor(log2(x))' for each
// element. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ConvertExpFP64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPPD'. Intrinsic: '_mm_maskz_getexp_pd'.
// Requires AVX512F.
func MaskzGetexpPd(k x86.Mmask8, a x86.M128d) x86.M128d {
	return x86.M128d(maskzGetexpPd(uint8(k), [2]float64(a)))
}

func maskzGetexpPd(k uint8, a [2]float64) [2]float64


// M256GetexpPd: Convert the exponent of each packed double-precision (64-bit)
// floating-point element in 'a' to a double-precision (64-bit) floating-point
// number representing the integer exponent, and store the results in 'dst'.
// This intrinsic essentially calculates 'floor(log2(x))' for each element. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ConvertExpFP64(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETEXPPD'. Intrinsic: '_mm256_getexp_pd'.
// Requires AVX512F.
func M256GetexpPd(a x86.M256d) x86.M256d {
	return x86.M256d(m256GetexpPd([4]float64(a)))
}

func m256GetexpPd(a [4]float64) [4]float64


// M256MaskGetexpPd: Convert the exponent of each packed double-precision
// (64-bit) floating-point element in 'a' to a double-precision (64-bit)
// floating-point number representing the integer exponent, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). This intrinsic essentially
// calculates 'floor(log2(x))' for each element. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ConvertExpFP64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETEXPPD'. Intrinsic: '_mm256_mask_getexp_pd'.
// Requires AVX512F.
func M256MaskGetexpPd(src x86.M256d, k x86.Mmask8, a x86.M256d) x86.M256d {
	return x86.M256d(m256MaskGetexpPd([4]float64(src), uint8(k), [4]float64(a)))
}

func m256MaskGetexpPd(src [4]float64, k uint8, a [4]float64) [4]float64


// M256MaskzGetexpPd: Convert the exponent of each packed double-precision
// (64-bit) floating-point element in 'a' to a double-precision (64-bit)
// floating-point number representing the integer exponent, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). This intrinsic essentially calculates
// 'floor(log2(x))' for each element. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ConvertExpFP64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETEXPPD'. Intrinsic: '_mm256_maskz_getexp_pd'.
// Requires AVX512F.
func M256MaskzGetexpPd(k x86.Mmask8, a x86.M256d) x86.M256d {
	return x86.M256d(m256MaskzGetexpPd(uint8(k), [4]float64(a)))
}

func m256MaskzGetexpPd(k uint8, a [4]float64) [4]float64


// M512MaskzGetexpPd: Convert the exponent of each packed double-precision
// (64-bit) floating-point element in 'a' to a double-precision (64-bit)
// floating-point number representing the integer exponent, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). This intrinsic essentially calculates
// 'floor(log2(x))' for each element. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ConvertExpFP64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGETEXPPD'. Intrinsic: '_mm512_maskz_getexp_pd'.
// Requires AVX512F.
func M512MaskzGetexpPd(k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskzGetexpPd(uint8(k), [8]float64(a)))
}

func m512MaskzGetexpPd(k uint8, a [8]float64) [8]float64


// GetexpPs: Convert the exponent of each packed single-precision (32-bit)
// floating-point element in 'a' to a single-precision (32-bit) floating-point
// number representing the integer exponent, and store the results in 'dst'.
// This intrinsic essentially calculates 'floor(log2(x))' for each element. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := ConvertExpFP32(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPPS'. Intrinsic: '_mm_getexp_ps'.
// Requires AVX512F.
func GetexpPs(a x86.M128) x86.M128 {
	return x86.M128(getexpPs([4]float32(a)))
}

func getexpPs(a [4]float32) [4]float32


// MaskGetexpPs: Convert the exponent of each packed single-precision (32-bit)
// floating-point element in 'a' to a single-precision (32-bit) floating-point
// number representing the integer exponent, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). This intrinsic essentially calculates 'floor(log2(x))'
// for each element. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ConvertExpFP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPPS'. Intrinsic: '_mm_mask_getexp_ps'.
// Requires AVX512F.
func MaskGetexpPs(src x86.M128, k x86.Mmask8, a x86.M128) x86.M128 {
	return x86.M128(maskGetexpPs([4]float32(src), uint8(k), [4]float32(a)))
}

func maskGetexpPs(src [4]float32, k uint8, a [4]float32) [4]float32


// MaskzGetexpPs: Convert the exponent of each packed single-precision (32-bit)
// floating-point element in 'a' to a single-precision (32-bit) floating-point
// number representing the integer exponent, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). This intrinsic essentially calculates 'floor(log2(x))' for each
// element. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ConvertExpFP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPPS'. Intrinsic: '_mm_maskz_getexp_ps'.
// Requires AVX512F.
func MaskzGetexpPs(k x86.Mmask8, a x86.M128) x86.M128 {
	return x86.M128(maskzGetexpPs(uint8(k), [4]float32(a)))
}

func maskzGetexpPs(k uint8, a [4]float32) [4]float32


// M256GetexpPs: Convert the exponent of each packed single-precision (32-bit)
// floating-point element in 'a' to a single-precision (32-bit) floating-point
// number representing the integer exponent, and store the results in 'dst'.
// This intrinsic essentially calculates 'floor(log2(x))' for each element. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ConvertExpFP32(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETEXPPS'. Intrinsic: '_mm256_getexp_ps'.
// Requires AVX512F.
func M256GetexpPs(a x86.M256) x86.M256 {
	return x86.M256(m256GetexpPs([8]float32(a)))
}

func m256GetexpPs(a [8]float32) [8]float32


// M256MaskGetexpPs: Convert the exponent of each packed single-precision
// (32-bit) floating-point element in 'a' to a single-precision (32-bit)
// floating-point number representing the integer exponent, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). This intrinsic essentially
// calculates 'floor(log2(x))' for each element. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ConvertExpFP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETEXPPS'. Intrinsic: '_mm256_mask_getexp_ps'.
// Requires AVX512F.
func M256MaskGetexpPs(src x86.M256, k x86.Mmask8, a x86.M256) x86.M256 {
	return x86.M256(m256MaskGetexpPs([8]float32(src), uint8(k), [8]float32(a)))
}

func m256MaskGetexpPs(src [8]float32, k uint8, a [8]float32) [8]float32


// M256MaskzGetexpPs: Convert the exponent of each packed single-precision
// (32-bit) floating-point element in 'a' to a single-precision (32-bit)
// floating-point number representing the integer exponent, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). This intrinsic essentially calculates
// 'floor(log2(x))' for each element. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ConvertExpFP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETEXPPS'. Intrinsic: '_mm256_maskz_getexp_ps'.
// Requires AVX512F.
func M256MaskzGetexpPs(k x86.Mmask8, a x86.M256) x86.M256 {
	return x86.M256(m256MaskzGetexpPs(uint8(k), [8]float32(a)))
}

func m256MaskzGetexpPs(k uint8, a [8]float32) [8]float32


// M512MaskzGetexpPs: Convert the exponent of each packed single-precision
// (32-bit) floating-point element in 'a' to a single-precision (32-bit)
// floating-point number representing the integer exponent, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). This intrinsic essentially calculates
// 'floor(log2(x))' for each element. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ConvertExpFP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGETEXPPS'. Intrinsic: '_mm512_maskz_getexp_ps'.
// Requires AVX512F.
func M512MaskzGetexpPs(k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskzGetexpPs(uint16(k), [16]float32(a)))
}

func m512MaskzGetexpPs(k uint16, a [16]float32) [16]float32


// M512MaskzGetexpRoundPd: Convert the exponent of each packed double-precision
// (64-bit) floating-point element in 'a' to a double-precision (64-bit)
// floating-point number representing the integer exponent, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). This intrinsic essentially calculates
// 'floor(log2(x))' for each element.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ConvertExpFP64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGETEXPPD'. Intrinsic: '_mm512_maskz_getexp_round_pd'.
// Requires AVX512F.
func M512MaskzGetexpRoundPd(k x86.Mmask8, a x86.M512d, rounding int) x86.M512d {
	return x86.M512d(m512MaskzGetexpRoundPd(uint8(k), [8]float64(a), rounding))
}

func m512MaskzGetexpRoundPd(k uint8, a [8]float64, rounding int) [8]float64


// M512MaskzGetexpRoundPs: Convert the exponent of each packed single-precision
// (32-bit) floating-point element in 'a' to a single-precision (32-bit)
// floating-point number representing the integer exponent, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). This intrinsic essentially calculates
// 'floor(log2(x))' for each element.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ConvertExpFP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGETEXPPS'. Intrinsic: '_mm512_maskz_getexp_round_ps'.
// Requires AVX512F.
func M512MaskzGetexpRoundPs(k x86.Mmask16, a x86.M512, rounding int) x86.M512 {
	return x86.M512(m512MaskzGetexpRoundPs(uint16(k), [16]float32(a), rounding))
}

func m512MaskzGetexpRoundPs(k uint16, a [16]float32, rounding int) [16]float32


// GetexpRoundSd: Convert the exponent of the lower double-precision (64-bit)
// floating-point element in 'b' to a double-precision (64-bit) floating-point
// number representing the integer exponent, store the result in the lower
// element of 'dst', and copy the upper element from 'a' to the upper element
// of 'dst'. This intrinsic essentially calculates 'floor(log2(x))' for the
// lower element.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := ConvertExpFP64(b[63:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPSD'. Intrinsic: '_mm_getexp_round_sd'.
// Requires AVX512F.
func GetexpRoundSd(a x86.M128d, b x86.M128d, rounding int) x86.M128d {
	return x86.M128d(getexpRoundSd([2]float64(a), [2]float64(b), rounding))
}

func getexpRoundSd(a [2]float64, b [2]float64, rounding int) [2]float64


// MaskGetexpRoundSd: Convert the exponent of the lower double-precision
// (64-bit) floating-point element in 'b' to a double-precision (64-bit)
// floating-point number representing the integer exponent, store the result in
// the lower element of 'dst' using writemask 'k' (the element is copied from
// 'src' when mask bit 0 is not set), and copy the upper element from 'a' to
// the upper element of 'dst'. This intrinsic essentially calculates
// 'floor(log2(x))' for the lower element.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := ConvertExpFP64(b[63:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPSD'. Intrinsic: '_mm_mask_getexp_round_sd'.
// Requires AVX512F.
func MaskGetexpRoundSd(src x86.M128d, k x86.Mmask8, a x86.M128d, b x86.M128d, rounding int) x86.M128d {
	return x86.M128d(maskGetexpRoundSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), rounding))
}

func maskGetexpRoundSd(src [2]float64, k uint8, a [2]float64, b [2]float64, rounding int) [2]float64


// MaskzGetexpRoundSd: Convert the exponent of the lower double-precision
// (64-bit) floating-point element in 'b' to a double-precision (64-bit)
// floating-point number representing the integer exponent, store the result in
// the lower element of 'dst' using zeromask 'k' (the element is zeroed out
// when mask bit 0 is not set), and copy the upper element from 'a' to the
// upper element of 'dst'. This intrinsic essentially calculates
// 'floor(log2(x))' for the lower element.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := ConvertExpFP64(b[63:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPSD'. Intrinsic: '_mm_maskz_getexp_round_sd'.
// Requires AVX512F.
func MaskzGetexpRoundSd(k x86.Mmask8, a x86.M128d, b x86.M128d, rounding int) x86.M128d {
	return x86.M128d(maskzGetexpRoundSd(uint8(k), [2]float64(a), [2]float64(b), rounding))
}

func maskzGetexpRoundSd(k uint8, a [2]float64, b [2]float64, rounding int) [2]float64


// GetexpRoundSs: Convert the exponent of the lower single-precision (32-bit)
// floating-point element in 'b' to a single-precision (32-bit) floating-point
// number representing the integer exponent, store the result in the lower
// element of 'dst', and copy the upper 3 packed elements from 'a' to the upper
// elements of 'dst'. This intrinsic essentially calculates 'floor(log2(x))'
// for the lower element.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := ConvertExpFP32(b[31:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPSS'. Intrinsic: '_mm_getexp_round_ss'.
// Requires AVX512F.
func GetexpRoundSs(a x86.M128, b x86.M128, rounding int) x86.M128 {
	return x86.M128(getexpRoundSs([4]float32(a), [4]float32(b), rounding))
}

func getexpRoundSs(a [4]float32, b [4]float32, rounding int) [4]float32


// MaskGetexpRoundSs: Convert the exponent of the lower single-precision
// (32-bit) floating-point element in 'b' to a single-precision (32-bit)
// floating-point number representing the integer exponent, store the result in
// the lower element of 'dst' using writemask 'k' (the element is copied from
// 'src' when mask bit 0 is not set), and copy the upper 3 packed elements from
// 'a' to the upper elements of 'dst'. This intrinsic essentially calculates
// 'floor(log2(x))' for the lower element.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := ConvertExpFP32(b[31:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPSS'. Intrinsic: '_mm_mask_getexp_round_ss'.
// Requires AVX512F.
func MaskGetexpRoundSs(src x86.M128, k x86.Mmask8, a x86.M128, b x86.M128, rounding int) x86.M128 {
	return x86.M128(maskGetexpRoundSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), rounding))
}

func maskGetexpRoundSs(src [4]float32, k uint8, a [4]float32, b [4]float32, rounding int) [4]float32


// MaskzGetexpRoundSs: Convert the exponent of the lower single-precision
// (32-bit) floating-point element in 'b' to a single-precision (32-bit)
// floating-point number representing the integer exponent, store the result in
// the lower element of 'dst' using zeromask 'k' (the element is zeroed out
// when mask bit 0 is not set), and copy the upper 3 packed elements from 'a'
// to the upper elements of 'dst'. This intrinsic essentially calculates
// 'floor(log2(x))' for the lower element.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := ConvertExpFP32(b[31:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPSS'. Intrinsic: '_mm_maskz_getexp_round_ss'.
// Requires AVX512F.
func MaskzGetexpRoundSs(k x86.Mmask8, a x86.M128, b x86.M128, rounding int) x86.M128 {
	return x86.M128(maskzGetexpRoundSs(uint8(k), [4]float32(a), [4]float32(b), rounding))
}

func maskzGetexpRoundSs(k uint8, a [4]float32, b [4]float32, rounding int) [4]float32


// GetexpSd: Convert the exponent of the lower double-precision (64-bit)
// floating-point element in 'b' to a double-precision (64-bit) floating-point
// number representing the integer exponent, store the result in the lower
// element of 'dst', and copy the upper element from 'a' to the upper element
// of 'dst'. This intrinsic essentially calculates 'floor(log2(x))' for the
// lower element. 
//
//		dst[63:0] := ConvertExpFP64(b[63:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPSD'. Intrinsic: '_mm_getexp_sd'.
// Requires AVX512F.
func GetexpSd(a x86.M128d, b x86.M128d) x86.M128d {
	return x86.M128d(getexpSd([2]float64(a), [2]float64(b)))
}

func getexpSd(a [2]float64, b [2]float64) [2]float64


// MaskGetexpSd: Convert the exponent of the lower double-precision (64-bit)
// floating-point element in 'b' to a double-precision (64-bit) floating-point
// number representing the integer exponent, store the result in the lower
// element of 'dst' using writemask 'k' (the element is copied from 'src' when
// mask bit 0 is not set), and copy the upper element from 'a' to the upper
// element of 'dst'. This intrinsic essentially calculates 'floor(log2(x))' for
// the lower element. 
//
//		IF k[0]
//			dst[63:0] := ConvertExpFP64(b[63:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPSD'. Intrinsic: '_mm_mask_getexp_sd'.
// Requires AVX512F.
func MaskGetexpSd(src x86.M128d, k x86.Mmask8, a x86.M128d, b x86.M128d) x86.M128d {
	return x86.M128d(maskGetexpSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskGetexpSd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzGetexpSd: Convert the exponent of the lower double-precision (64-bit)
// floating-point element in 'b' to a double-precision (64-bit) floating-point
// number representing the integer exponent, store the result in the lower
// element of 'dst' using zeromask 'k' (the element is zeroed out when mask bit
// 0 is not set), and copy the upper element from 'a' to the upper element of
// 'dst'. This intrinsic essentially calculates 'floor(log2(x))' for the lower
// element. 
//
//		IF k[0]
//			dst[63:0] := ConvertExpFP64(b[63:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPSD'. Intrinsic: '_mm_maskz_getexp_sd'.
// Requires AVX512F.
func MaskzGetexpSd(k x86.Mmask8, a x86.M128d, b x86.M128d) x86.M128d {
	return x86.M128d(maskzGetexpSd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzGetexpSd(k uint8, a [2]float64, b [2]float64) [2]float64


// GetexpSs: Convert the exponent of the lower single-precision (32-bit)
// floating-point element in 'b' to a single-precision (32-bit) floating-point
// number representing the integer exponent, store the result in the lower
// element of 'dst', and copy the upper 3 packed elements from 'a' to the upper
// elements of 'dst'. This intrinsic essentially calculates 'floor(log2(x))'
// for the lower element. 
//
//		dst[31:0] := ConvertExpFP32(b[31:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPSS'. Intrinsic: '_mm_getexp_ss'.
// Requires AVX512F.
func GetexpSs(a x86.M128, b x86.M128) x86.M128 {
	return x86.M128(getexpSs([4]float32(a), [4]float32(b)))
}

func getexpSs(a [4]float32, b [4]float32) [4]float32


// MaskGetexpSs: Convert the exponent of the lower single-precision (32-bit)
// floating-point element in 'b' to a single-precision (32-bit) floating-point
// number representing the integer exponent, store the result in the lower
// element of 'dst' using writemask 'k' (the element is copied from 'src' when
// mask bit 0 is not set), and copy the upper 3 packed elements from 'a' to the
// upper elements of 'dst'. This intrinsic essentially calculates
// 'floor(log2(x))' for the lower element. 
//
//		IF k[0]
//			dst[31:0] := ConvertExpFP32(b[31:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPSS'. Intrinsic: '_mm_mask_getexp_ss'.
// Requires AVX512F.
func MaskGetexpSs(src x86.M128, k x86.Mmask8, a x86.M128, b x86.M128) x86.M128 {
	return x86.M128(maskGetexpSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskGetexpSs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzGetexpSs: Convert the exponent of the lower single-precision (32-bit)
// floating-point element in 'b' to a single-precision (32-bit) floating-point
// number representing the integer exponent, store the result in the lower
// element of 'dst' using zeromask 'k' (the element is zeroed out when mask bit
// 0 is not set), and copy the upper 3 packed elements from 'a' to the upper
// elements of 'dst'. This intrinsic essentially calculates 'floor(log2(x))'
// for the lower element. 
//
//		IF k[0]
//			dst[31:0] := ConvertExpFP32(b[31:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPSS'. Intrinsic: '_mm_maskz_getexp_ss'.
// Requires AVX512F.
func MaskzGetexpSs(k x86.Mmask8, a x86.M128, b x86.M128) x86.M128 {
	return x86.M128(maskzGetexpSs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzGetexpSs(k uint8, a [4]float32, b [4]float32) [4]float32


// GetmantPd: Normalize the mantissas of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. This
// intrinsic essentially calculates '(2^k)*|x.significand|', where 'k'
// depends on the interval range defined by 'interv' and the sign depends on
// 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTPD'. Intrinsic: '_mm_getmant_pd'.
// Requires AVX512F.
func GetmantPd(a x86.M128d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) x86.M128d {
	return x86.M128d(getmantPd([2]float64(a), interv, sc))
}

func getmantPd(a [2]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [2]float64


// MaskGetmantPd: Normalize the mantissas of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). This intrinsic essentially calculates
// '(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTPD'. Intrinsic: '_mm_mask_getmant_pd'.
// Requires AVX512F.
func MaskGetmantPd(src x86.M128d, k x86.Mmask8, a x86.M128d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) x86.M128d {
	return x86.M128d(maskGetmantPd([2]float64(src), uint8(k), [2]float64(a), interv, sc))
}

func maskGetmantPd(src [2]float64, k uint8, a [2]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [2]float64


// MaskzGetmantPd: Normalize the mantissas of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). This intrinsic essentially calculates '(2^k)*|x.significand|', where
// 'k' depends on the interval range defined by 'interv' and the sign depends
// on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTPD'. Intrinsic: '_mm_maskz_getmant_pd'.
// Requires AVX512F.
func MaskzGetmantPd(k x86.Mmask8, a x86.M128d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) x86.M128d {
	return x86.M128d(maskzGetmantPd(uint8(k), [2]float64(a), interv, sc))
}

func maskzGetmantPd(k uint8, a [2]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [2]float64


// M256GetmantPd: Normalize the mantissas of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. This
// intrinsic essentially calculates '(2^k)*|x.significand|', where 'k'
// depends on the interval range defined by 'interv' and the sign depends on
// 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETMANTPD'. Intrinsic: '_mm256_getmant_pd'.
// Requires AVX512F.
func M256GetmantPd(a x86.M256d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) x86.M256d {
	return x86.M256d(m256GetmantPd([4]float64(a), interv, sc))
}

func m256GetmantPd(a [4]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [4]float64


// M256MaskGetmantPd: Normalize the mantissas of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). This intrinsic essentially calculates
// '(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETMANTPD'. Intrinsic: '_mm256_mask_getmant_pd'.
// Requires AVX512F.
func M256MaskGetmantPd(src x86.M256d, k x86.Mmask8, a x86.M256d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) x86.M256d {
	return x86.M256d(m256MaskGetmantPd([4]float64(src), uint8(k), [4]float64(a), interv, sc))
}

func m256MaskGetmantPd(src [4]float64, k uint8, a [4]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [4]float64


// M256MaskzGetmantPd: Normalize the mantissas of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). This intrinsic essentially calculates
// '(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETMANTPD'. Intrinsic: '_mm256_maskz_getmant_pd'.
// Requires AVX512F.
func M256MaskzGetmantPd(k x86.Mmask8, a x86.M256d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) x86.M256d {
	return x86.M256d(m256MaskzGetmantPd(uint8(k), [4]float64(a), interv, sc))
}

func m256MaskzGetmantPd(k uint8, a [4]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [4]float64


// M512MaskzGetmantPd: Normalize the mantissas of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). This intrinsic essentially calculates
// '(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGETMANTPD'. Intrinsic: '_mm512_maskz_getmant_pd'.
// Requires AVX512F.
func M512MaskzGetmantPd(k x86.Mmask8, a x86.M512d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) x86.M512d {
	return x86.M512d(m512MaskzGetmantPd(uint8(k), [8]float64(a), interv, sc))
}

func m512MaskzGetmantPd(k uint8, a [8]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [8]float64


// GetmantPs: Normalize the mantissas of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. This
// intrinsic essentially calculates '(2^k)*|x.significand|', where 'k'
// depends on the interval range defined by 'interv' and the sign depends on
// 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTPS'. Intrinsic: '_mm_getmant_ps'.
// Requires AVX512F.
func GetmantPs(a x86.M128, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) x86.M128 {
	return x86.M128(getmantPs([4]float32(a), interv, sc))
}

func getmantPs(a [4]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [4]float32


// MaskGetmantPs: Normalize the mantissas of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). This intrinsic essentially calculates
// '(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTPS'. Intrinsic: '_mm_mask_getmant_ps'.
// Requires AVX512F.
func MaskGetmantPs(src x86.M128, k x86.Mmask8, a x86.M128, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) x86.M128 {
	return x86.M128(maskGetmantPs([4]float32(src), uint8(k), [4]float32(a), interv, sc))
}

func maskGetmantPs(src [4]float32, k uint8, a [4]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [4]float32


// MaskzGetmantPs: Normalize the mantissas of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). This intrinsic essentially calculates '(2^k)*|x.significand|', where
// 'k' depends on the interval range defined by 'interv' and the sign depends
// on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTPS'. Intrinsic: '_mm_maskz_getmant_ps'.
// Requires AVX512F.
func MaskzGetmantPs(k x86.Mmask8, a x86.M128, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) x86.M128 {
	return x86.M128(maskzGetmantPs(uint8(k), [4]float32(a), interv, sc))
}

func maskzGetmantPs(k uint8, a [4]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [4]float32


// M256GetmantPs: Normalize the mantissas of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. This
// intrinsic essentially calculates '(2^k)*|x.significand|', where 'k'
// depends on the interval range defined by 'interv' and the sign depends on
// 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETMANTPS'. Intrinsic: '_mm256_getmant_ps'.
// Requires AVX512F.
func M256GetmantPs(a x86.M256, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) x86.M256 {
	return x86.M256(m256GetmantPs([8]float32(a), interv, sc))
}

func m256GetmantPs(a [8]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [8]float32


// M256MaskGetmantPs: Normalize the mantissas of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). This intrinsic essentially calculates
// '(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETMANTPS'. Intrinsic: '_mm256_mask_getmant_ps'.
// Requires AVX512F.
func M256MaskGetmantPs(src x86.M256, k x86.Mmask8, a x86.M256, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) x86.M256 {
	return x86.M256(m256MaskGetmantPs([8]float32(src), uint8(k), [8]float32(a), interv, sc))
}

func m256MaskGetmantPs(src [8]float32, k uint8, a [8]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [8]float32


// M256MaskzGetmantPs: Normalize the mantissas of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). This intrinsic essentially calculates
// '(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETMANTPS'. Intrinsic: '_mm256_maskz_getmant_ps'.
// Requires AVX512F.
func M256MaskzGetmantPs(k x86.Mmask8, a x86.M256, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) x86.M256 {
	return x86.M256(m256MaskzGetmantPs(uint8(k), [8]float32(a), interv, sc))
}

func m256MaskzGetmantPs(k uint8, a [8]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [8]float32


// M512MaskzGetmantPs: Normalize the mantissas of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). This intrinsic essentially calculates
// '(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGETMANTPS'. Intrinsic: '_mm512_maskz_getmant_ps'.
// Requires AVX512F.
func M512MaskzGetmantPs(k x86.Mmask16, a x86.M512, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) x86.M512 {
	return x86.M512(m512MaskzGetmantPs(uint16(k), [16]float32(a), interv, sc))
}

func m512MaskzGetmantPs(k uint16, a [16]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [16]float32


// M512MaskzGetmantRoundPd: Normalize the mantissas of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). This intrinsic essentially calculates
// '(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1Rounding is done according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGETMANTPD'. Intrinsic: '_mm512_maskz_getmant_round_pd'.
// Requires AVX512F.
func M512MaskzGetmantRoundPd(k x86.Mmask8, a x86.M512d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) x86.M512d {
	return x86.M512d(m512MaskzGetmantRoundPd(uint8(k), [8]float64(a), interv, sc, rounding))
}

func m512MaskzGetmantRoundPd(k uint8, a [8]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) [8]float64


// M512MaskzGetmantRoundPs: Normalize the mantissas of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). This intrinsic essentially calculates
// '(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1Rounding is done according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGETMANTPS'. Intrinsic: '_mm512_maskz_getmant_round_ps'.
// Requires AVX512F.
func M512MaskzGetmantRoundPs(k x86.Mmask16, a x86.M512, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) x86.M512 {
	return x86.M512(m512MaskzGetmantRoundPs(uint16(k), [16]float32(a), interv, sc, rounding))
}

func m512MaskzGetmantRoundPs(k uint16, a [16]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) [16]float32


// GetmantRoundSd: Normalize the mantissas of the lower double-precision
// (64-bit) floating-point element in 'a', store the result in the lower
// element of 'dst', and copy the upper element from 'b' to the upper element
// of 'dst'. This intrinsic essentially calculates '(2^k)*|x.significand|',
// where 'k' depends on the interval range defined by 'interv' and the sign
// depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1Rounding is done according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := GetNormalizedMantissa(a[63:0], sc, interv)
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTSD'. Intrinsic: '_mm_getmant_round_sd'.
// Requires AVX512F.
func GetmantRoundSd(a x86.M128d, b x86.M128d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) x86.M128d {
	return x86.M128d(getmantRoundSd([2]float64(a), [2]float64(b), interv, sc, rounding))
}

func getmantRoundSd(a [2]float64, b [2]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) [2]float64


// MaskGetmantRoundSd: Normalize the mantissas of the lower double-precision
// (64-bit) floating-point element in 'a', store the result in the lower
// element of 'dst' using writemask 'k' (the element is copied from 'src' when
// mask bit 0 is not set), and copy the upper element from 'b' to the upper
// element of 'dst'. This intrinsic essentially calculates
// '(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1Rounding is done according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := GetNormalizedMantissa(a[63:0], sc, interv)
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTSD'. Intrinsic: '_mm_mask_getmant_round_sd'.
// Requires AVX512F.
func MaskGetmantRoundSd(src x86.M128d, k x86.Mmask8, a x86.M128d, b x86.M128d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) x86.M128d {
	return x86.M128d(maskGetmantRoundSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), interv, sc, rounding))
}

func maskGetmantRoundSd(src [2]float64, k uint8, a [2]float64, b [2]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) [2]float64


// MaskzGetmantRoundSd: Normalize the mantissas of the lower double-precision
// (64-bit) floating-point element in 'a', store the result in the lower
// element of 'dst' using zeromask 'k' (the element is zeroed out when mask bit
// 0 is not set), and copy the upper element from 'b' to the upper element of
// 'dst'. This intrinsic essentially calculates '(2^k)*|x.significand|',
// where 'k' depends on the interval range defined by 'interv' and the sign
// depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1Rounding is done according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := GetNormalizedMantissa(a[63:0], sc, interv)
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTSD'. Intrinsic: '_mm_maskz_getmant_round_sd'.
// Requires AVX512F.
func MaskzGetmantRoundSd(k x86.Mmask8, a x86.M128d, b x86.M128d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) x86.M128d {
	return x86.M128d(maskzGetmantRoundSd(uint8(k), [2]float64(a), [2]float64(b), interv, sc, rounding))
}

func maskzGetmantRoundSd(k uint8, a [2]float64, b [2]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) [2]float64


// GetmantRoundSs: Normalize the mantissas of the lower single-precision
// (32-bit) floating-point element in 'a', store the result in the lower
// element of 'dst', and copy the upper 3 packed elements from 'b' to the upper
// elements of 'dst'. This intrinsic essentially calculates
// '(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1Rounding is done according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := GetNormalizedMantissa(a[31:0], sc, interv)
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTSS'. Intrinsic: '_mm_getmant_round_ss'.
// Requires AVX512F.
func GetmantRoundSs(a x86.M128, b x86.M128, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) x86.M128 {
	return x86.M128(getmantRoundSs([4]float32(a), [4]float32(b), interv, sc, rounding))
}

func getmantRoundSs(a [4]float32, b [4]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) [4]float32


// MaskGetmantRoundSs: Normalize the mantissas of the lower single-precision
// (32-bit) floating-point element in 'a', store the result in the lower
// element of 'dst' using writemask 'k' (the element is copied from 'src' when
// mask bit 0 is not set), and copy the upper 3 packed elements from 'b' to the
// upper elements of 'dst'. This intrinsic essentially calculates
// '(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1Rounding is done according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := GetNormalizedMantissa(a[31:0], sc, interv)
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTSS'. Intrinsic: '_mm_mask_getmant_round_ss'.
// Requires AVX512F.
func MaskGetmantRoundSs(src x86.M128, k x86.Mmask8, a x86.M128, b x86.M128, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) x86.M128 {
	return x86.M128(maskGetmantRoundSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), interv, sc, rounding))
}

func maskGetmantRoundSs(src [4]float32, k uint8, a [4]float32, b [4]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) [4]float32


// MaskzGetmantRoundSs: Normalize the mantissas of the lower single-precision
// (32-bit) floating-point element in 'a', store the result in the lower
// element of 'dst' using zeromask 'k' (the element is zeroed out when mask bit
// 0 is not set), and copy the upper 3 packed elements from 'b' to the upper
// elements of 'dst'. This intrinsic essentially calculates
// '(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1Rounding is done according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := GetNormalizedMantissa(a[31:0], sc, interv)
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTSS'. Intrinsic: '_mm_maskz_getmant_round_ss'.
// Requires AVX512F.
func MaskzGetmantRoundSs(k x86.Mmask8, a x86.M128, b x86.M128, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) x86.M128 {
	return x86.M128(maskzGetmantRoundSs(uint8(k), [4]float32(a), [4]float32(b), interv, sc, rounding))
}

func maskzGetmantRoundSs(k uint8, a [4]float32, b [4]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) [4]float32


// GetmantSd: Normalize the mantissas of the lower double-precision (64-bit)
// floating-point element in 'a', store the result in the lower element of
// 'dst', and copy the upper element from 'b' to the upper element of 'dst'.
// This intrinsic essentially calculates '(2^k)*|x.significand|', where 'k'
// depends on the interval range defined by 'interv' and the sign depends on
// 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		dst[63:0] := GetNormalizedMantissa(a[63:0], sc, interv)
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTSD'. Intrinsic: '_mm_getmant_sd'.
// Requires AVX512F.
func GetmantSd(a x86.M128d, b x86.M128d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) x86.M128d {
	return x86.M128d(getmantSd([2]float64(a), [2]float64(b), interv, sc))
}

func getmantSd(a [2]float64, b [2]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [2]float64


// MaskGetmantSd: Normalize the mantissas of the lower double-precision
// (64-bit) floating-point element in 'a', store the result in the lower
// element of 'dst' using writemask 'k' (the element is copied from 'src' when
// mask bit 0 is not set), and copy the upper element from 'b' to the upper
// element of 'dst'. This intrinsic essentially calculates
// '(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		IF k[0]
//			dst[63:0] := GetNormalizedMantissa(a[63:0], sc, interv)
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTSD'. Intrinsic: '_mm_mask_getmant_sd'.
// Requires AVX512F.
func MaskGetmantSd(src x86.M128d, k x86.Mmask8, a x86.M128d, b x86.M128d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) x86.M128d {
	return x86.M128d(maskGetmantSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), interv, sc))
}

func maskGetmantSd(src [2]float64, k uint8, a [2]float64, b [2]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [2]float64


// MaskzGetmantSd: Normalize the mantissas of the lower double-precision
// (64-bit) floating-point element in 'a', store the result in the lower
// element of 'dst' using zeromask 'k' (the element is zeroed out when mask bit
// 0 is not set), and copy the upper element from 'b' to the upper element of
// 'dst'. This intrinsic essentially calculates '(2^k)*|x.significand|',
// where 'k' depends on the interval range defined by 'interv' and the sign
// depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		IF k[0]
//			dst[63:0] := GetNormalizedMantissa(a[63:0], sc, interv)
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTSD'. Intrinsic: '_mm_maskz_getmant_sd'.
// Requires AVX512F.
func MaskzGetmantSd(k x86.Mmask8, a x86.M128d, b x86.M128d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) x86.M128d {
	return x86.M128d(maskzGetmantSd(uint8(k), [2]float64(a), [2]float64(b), interv, sc))
}

func maskzGetmantSd(k uint8, a [2]float64, b [2]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [2]float64


// GetmantSs: Normalize the mantissas of the lower single-precision (32-bit)
// floating-point element in 'a', store the result in the lower element of
// 'dst', and copy the upper 3 packed elements from 'b' to the upper elements
// of 'dst'. This intrinsic essentially calculates '(2^k)*|x.significand|',
// where 'k' depends on the interval range defined by 'interv' and the sign
// depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		dst[31:0] := GetNormalizedMantissa(a[31:0], sc, interv)
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTSS'. Intrinsic: '_mm_getmant_ss'.
// Requires AVX512F.
func GetmantSs(a x86.M128, b x86.M128, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) x86.M128 {
	return x86.M128(getmantSs([4]float32(a), [4]float32(b), interv, sc))
}

func getmantSs(a [4]float32, b [4]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [4]float32


// MaskGetmantSs: Normalize the mantissas of the lower single-precision
// (32-bit) floating-point element in 'a', store the result in the lower
// element of 'dst' using writemask 'k' (the element is copied from 'src' when
// mask bit 0 is not set), and copy the upper 3 packed elements from 'b' to the
// upper elements of 'dst'. This intrinsic essentially calculates
// '(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		IF k[0]
//			dst[31:0] := GetNormalizedMantissa(a[31:0], sc, interv)
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTSS'. Intrinsic: '_mm_mask_getmant_ss'.
// Requires AVX512F.
func MaskGetmantSs(src x86.M128, k x86.Mmask8, a x86.M128, b x86.M128, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) x86.M128 {
	return x86.M128(maskGetmantSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), interv, sc))
}

func maskGetmantSs(src [4]float32, k uint8, a [4]float32, b [4]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [4]float32


// MaskzGetmantSs: Normalize the mantissas of the lower single-precision
// (32-bit) floating-point element in 'a', store the result in the lower
// element of 'dst' using zeromask 'k' (the element is zeroed out when mask bit
// 0 is not set), and copy the upper 3 packed elements from 'b' to the upper
// elements of 'dst'. This intrinsic essentially calculates
// '(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		IF k[0]
//			dst[31:0] := GetNormalizedMantissa(a[31:0], sc, interv)
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTSS'. Intrinsic: '_mm_maskz_getmant_ss'.
// Requires AVX512F.
func MaskzGetmantSs(k x86.Mmask8, a x86.M128, b x86.M128, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) x86.M128 {
	return x86.M128(maskzGetmantSs(uint8(k), [4]float32(a), [4]float32(b), interv, sc))
}

func maskzGetmantSs(k uint8, a [4]float32, b [4]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [4]float32


// M512HypotPd: Compute the length of the hypotenous of a right triangle, with
// the lengths of the other two sides of the triangle stored as packed
// double-precision (64-bit) floating-point elements in 'a' and 'b', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := SQRT(a[i+63:i]^2 + b[i+63:i]^2)
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_hypot_pd'.
// Requires AVX512F.
func M512HypotPd(a x86.M512d, b x86.M512d) x86.M512d {
	return x86.M512d(m512HypotPd([8]float64(a), [8]float64(b)))
}

func m512HypotPd(a [8]float64, b [8]float64) [8]float64


// M512MaskHypotPd: Compute the length of the hypotenous of a right triangle,
// with the lengths of the other two sides of the triangle stored as packed
// double-precision (64-bit) floating-point elements in 'a' and 'b', and store
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SQRT(a[i+63:i]^2 + b[i+63:i]^2)
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_hypot_pd'.
// Requires AVX512F.
func M512MaskHypotPd(src x86.M512d, k x86.Mmask8, a x86.M512d, b x86.M512d) x86.M512d {
	return x86.M512d(m512MaskHypotPd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b)))
}

func m512MaskHypotPd(src [8]float64, k uint8, a [8]float64, b [8]float64) [8]float64


// M512HypotPs: Compute the length of the hypotenous of a right triangle, with
// the lengths of the other two sides of the triangle stored as packed
// single-precision (32-bit) floating-point elements in 'a' and 'b', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := SQRT(a[i+31:i]^2 + b[i+31:i]^2)
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_hypot_ps'.
// Requires AVX512F.
func M512HypotPs(a x86.M512, b x86.M512) x86.M512 {
	return x86.M512(m512HypotPs([16]float32(a), [16]float32(b)))
}

func m512HypotPs(a [16]float32, b [16]float32) [16]float32


// M512MaskHypotPs: Compute the length of the hypotenous of a right triangle,
// with the lengths of the other two sides of the triangle stored as packed
// single-precision (32-bit) floating-point elements in 'a' and 'b', and store
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SQRT(a[i+31:i]^2 + b[i+31:i]^2)
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_hypot_ps'.
// Requires AVX512F.
func M512MaskHypotPs(src x86.M512, k x86.Mmask16, a x86.M512, b x86.M512) x86.M512 {
	return x86.M512(m512MaskHypotPs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b)))
}

func m512MaskHypotPs(src [16]float32, k uint16, a [16]float32, b [16]float32) [16]float32


// Skipped: _mm_mmask_i32gather_epi32. Contains pointer parameter.


// Skipped: _mm256_mmask_i32gather_epi32. Contains pointer parameter.


// Skipped: _mm_mmask_i32gather_epi64. Contains pointer parameter.


// Skipped: _mm256_mmask_i32gather_epi64. Contains pointer parameter.


// Skipped: _mm512_i32gather_epi64. Contains pointer parameter.


// Skipped: _mm512_mask_i32gather_epi64. Contains pointer parameter.


// Skipped: _mm_mmask_i32gather_pd. Contains pointer parameter.


// Skipped: _mm256_mmask_i32gather_pd. Contains pointer parameter.


// Skipped: _mm512_i32gather_pd. Contains pointer parameter.


// Skipped: _mm512_mask_i32gather_pd. Contains pointer parameter.


// Skipped: _mm_mmask_i32gather_ps. Contains pointer parameter.


// Skipped: _mm256_mmask_i32gather_ps. Contains pointer parameter.


// Skipped: _mm_i32scatter_epi32. Contains pointer parameter.


// Skipped: _mm_mask_i32scatter_epi32. Contains pointer parameter.


// Skipped: _mm256_i32scatter_epi32. Contains pointer parameter.


// Skipped: _mm256_mask_i32scatter_epi32. Contains pointer parameter.


// Skipped: _mm_i32scatter_epi64. Contains pointer parameter.


// Skipped: _mm_mask_i32scatter_epi64. Contains pointer parameter.


// Skipped: _mm256_i32scatter_epi64. Contains pointer parameter.


// Skipped: _mm256_mask_i32scatter_epi64. Contains pointer parameter.


// Skipped: _mm512_i32scatter_epi64. Contains pointer parameter.


// Skipped: _mm512_mask_i32scatter_epi64. Contains pointer parameter.


// Skipped: _mm_i32scatter_pd. Contains pointer parameter.


// Skipped: _mm_mask_i32scatter_pd. Contains pointer parameter.


// Skipped: _mm256_i32scatter_pd. Contains pointer parameter.


// Skipped: _mm256_mask_i32scatter_pd. Contains pointer parameter.


// Skipped: _mm512_i32scatter_pd. Contains pointer parameter.


// Skipped: _mm512_mask_i32scatter_pd. Contains pointer parameter.


// Skipped: _mm_i32scatter_ps. Contains pointer parameter.


// Skipped: _mm_mask_i32scatter_ps. Contains pointer parameter.


// Skipped: _mm256_i32scatter_ps. Contains pointer parameter.


// Skipped: _mm256_mask_i32scatter_ps. Contains pointer parameter.


// Skipped: _mm_mmask_i64gather_epi32. Contains pointer parameter.


// Skipped: _mm256_mmask_i64gather_epi32. Contains pointer parameter.


// Skipped: _mm512_i64gather_epi32. Contains pointer parameter.


// Skipped: _mm512_mask_i64gather_epi32. Contains pointer parameter.


// Skipped: _mm_mmask_i64gather_epi64. Contains pointer parameter.


// Skipped: _mm256_mmask_i64gather_epi64. Contains pointer parameter.


// Skipped: _mm512_i64gather_epi64. Contains pointer parameter.


// Skipped: _mm512_mask_i64gather_epi64. Contains pointer parameter.


// Skipped: _mm_mmask_i64gather_pd. Contains pointer parameter.


// Skipped: _mm256_mmask_i64gather_pd. Contains pointer parameter.


// Skipped: _mm512_i64gather_pd. Contains pointer parameter.


// Skipped: _mm512_mask_i64gather_pd. Contains pointer parameter.


// Skipped: _mm_mmask_i64gather_ps. Contains pointer parameter.


// Skipped: _mm256_mmask_i64gather_ps. Contains pointer parameter.


// Skipped: _mm512_i64gather_ps. Contains pointer parameter.


// Skipped: _mm512_mask_i64gather_ps. Contains pointer parameter.


// Skipped: _mm_i64scatter_epi32. Contains pointer parameter.


// Skipped: _mm_mask_i64scatter_epi32. Contains pointer parameter.


// Skipped: _mm256_i64scatter_epi32. Contains pointer parameter.


// Skipped: _mm256_mask_i64scatter_epi32. Contains pointer parameter.


// Skipped: _mm512_i64scatter_epi32. Contains pointer parameter.


// Skipped: _mm512_mask_i64scatter_epi32. Contains pointer parameter.


// Skipped: _mm_i64scatter_epi64. Contains pointer parameter.


// Skipped: _mm_mask_i64scatter_epi64. Contains pointer parameter.


// Skipped: _mm256_i64scatter_epi64. Contains pointer parameter.


// Skipped: _mm256_mask_i64scatter_epi64. Contains pointer parameter.


// Skipped: _mm512_i64scatter_epi64. Contains pointer parameter.


// Skipped: _mm512_mask_i64scatter_epi64. Contains pointer parameter.


// Skipped: _mm_i64scatter_pd. Contains pointer parameter.


// Skipped: _mm_mask_i64scatter_pd. Contains pointer parameter.


// Skipped: _mm256_i64scatter_pd. Contains pointer parameter.


// Skipped: _mm256_mask_i64scatter_pd. Contains pointer parameter.


// Skipped: _mm512_i64scatter_pd. Contains pointer parameter.


// Skipped: _mm512_mask_i64scatter_pd. Contains pointer parameter.


// Skipped: _mm_i64scatter_ps. Contains pointer parameter.


// Skipped: _mm_mask_i64scatter_ps. Contains pointer parameter.


// Skipped: _mm256_i64scatter_ps. Contains pointer parameter.


// Skipped: _mm256_mask_i64scatter_ps. Contains pointer parameter.


// Skipped: _mm512_i64scatter_ps. Contains pointer parameter.


// Skipped: _mm512_mask_i64scatter_ps. Contains pointer parameter.


// M256Insertf32x4: Copy 'a' to 'dst', then insert 128 bits (composed of 4
// packed single-precision (32-bit) floating-point elements) from 'b' into
// 'dst' at the location specified by 'imm8'. 
//
//		dst[255:0] := a[255:0]
//		CASE (imm8[1:0]) of
//		0: dst[127:0] := b[127:0]
//		1: dst[255:128] := b[127:0]
//		ESAC
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTF32X4'. Intrinsic: '_mm256_insertf32x4'.
// Requires AVX512F.
func M256Insertf32x4(a x86.M256, b x86.M128, imm8 int) x86.M256 {
	return x86.M256(m256Insertf32x4([8]float32(a), [4]float32(b), imm8))
}

func m256Insertf32x4(a [8]float32, b [4]float32, imm8 int) [8]float32


// M256MaskInsertf32x4: Copy 'a' to 'tmp', then insert 128 bits (composed of 4
// packed single-precision (32-bit) floating-point elements) from 'b' into
// 'tmp' at the location specified by 'imm8'.  Store 'tmp' to 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp[255:0] := a[255:0]
//		CASE (imm8[1:0]) of
//		0: tmp[127:0] := b[127:0]
//		1: tmp[255:128] := b[127:0]
//		ESAC
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTF32X4'. Intrinsic: '_mm256_mask_insertf32x4'.
// Requires AVX512F.
func M256MaskInsertf32x4(src x86.M256, k x86.Mmask8, a x86.M256, b x86.M128, imm8 int) x86.M256 {
	return x86.M256(m256MaskInsertf32x4([8]float32(src), uint8(k), [8]float32(a), [4]float32(b), imm8))
}

func m256MaskInsertf32x4(src [8]float32, k uint8, a [8]float32, b [4]float32, imm8 int) [8]float32


// M256MaskzInsertf32x4: Copy 'a' to 'tmp', then insert 128 bits (composed of 4
// packed single-precision (32-bit) floating-point elements) from 'b' into
// 'tmp' at the location specified by 'imm8'.  Store 'tmp' to 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp[255:0] := a[255:0]
//		CASE (imm8[1:0]) of
//		0: tmp[127:0] := b[127:0]
//		1: tmp[255:128] := b[127:0]
//		ESAC
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTF32X4'. Intrinsic: '_mm256_maskz_insertf32x4'.
// Requires AVX512F.
func M256MaskzInsertf32x4(k x86.Mmask8, a x86.M256, b x86.M128, imm8 int) x86.M256 {
	return x86.M256(m256MaskzInsertf32x4(uint8(k), [8]float32(a), [4]float32(b), imm8))
}

func m256MaskzInsertf32x4(k uint8, a [8]float32, b [4]float32, imm8 int) [8]float32


// M512Insertf32x4: Copy 'a' to 'dst', then insert 128 bits (composed of 4
// packed single-precision (32-bit) floating-point elements) from 'b' into
// 'dst' at the location specified by 'imm8'. 
//
//		dst[511:0] := a[511:0]
//		CASE (imm8[1:0]) of
//		0: dst[127:0] := b[127:0]
//		1: dst[255:128] := b[127:0]
//		2: dst[383:256] := b[127:0]
//		3: dst[511:384] := b[127:0]
//		ESAC
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTF32X4'. Intrinsic: '_mm512_insertf32x4'.
// Requires AVX512F.
func M512Insertf32x4(a x86.M512, b x86.M128, imm8 int) x86.M512 {
	return x86.M512(m512Insertf32x4([16]float32(a), [4]float32(b), imm8))
}

func m512Insertf32x4(a [16]float32, b [4]float32, imm8 int) [16]float32


// M512MaskInsertf32x4: Copy 'a' to 'tmp', then insert 128 bits (composed of 4
// packed single-precision (32-bit) floating-point elements) from 'b' into
// 'tmp' at the location specified by 'imm8'.  Store 'tmp' to 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp[511:0] := a[511:0]
//		CASE (imm8[1:0]) of
//		0: tmp[127:0] := b[127:0]
//		1: tmp[255:128] := b[127:0]
//		2: tmp[383:256] := b[127:0]
//		3: tmp[511:384] := b[127:0]
//		ESAC
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTF32X4'. Intrinsic: '_mm512_mask_insertf32x4'.
// Requires AVX512F.
func M512MaskInsertf32x4(src x86.M512, k x86.Mmask16, a x86.M512, b x86.M128, imm8 int) x86.M512 {
	return x86.M512(m512MaskInsertf32x4([16]float32(src), uint16(k), [16]float32(a), [4]float32(b), imm8))
}

func m512MaskInsertf32x4(src [16]float32, k uint16, a [16]float32, b [4]float32, imm8 int) [16]float32


// M512MaskzInsertf32x4: Copy 'a' to 'tmp', then insert 128 bits (composed of 4
// packed single-precision (32-bit) floating-point elements) from 'b' into
// 'tmp' at the location specified by 'imm8'.  Store 'tmp' to 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp[511:0] := a[511:0]
//		CASE (imm8[1:0]) of
//		0: tmp[127:0] := b[127:0]
//		1: tmp[255:128] := b[127:0]
//		2: tmp[383:256] := b[127:0]
//		3: tmp[511:384] := b[127:0]
//		ESAC
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTF32X4'. Intrinsic: '_mm512_maskz_insertf32x4'.
// Requires AVX512F.
func M512MaskzInsertf32x4(k x86.Mmask16, a x86.M512, b x86.M128, imm8 int) x86.M512 {
	return x86.M512(m512MaskzInsertf32x4(uint16(k), [16]float32(a), [4]float32(b), imm8))
}

func m512MaskzInsertf32x4(k uint16, a [16]float32, b [4]float32, imm8 int) [16]float32


// M512Insertf64x4: Copy 'a' to 'dst', then insert 256 bits (composed of 4
// packed double-precision (64-bit) floating-point elements) from 'b' into
// 'dst' at the location specified by 'imm8'. 
//
//		dst[511:0] := a[511:0]
//		CASE (imm8[0]) of
//		0: dst[255:0] := b[255:0]
//		1: dst[511:256] := b[255:0]
//		ESAC
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTF64X4'. Intrinsic: '_mm512_insertf64x4'.
// Requires AVX512F.
func M512Insertf64x4(a x86.M512d, b x86.M256d, imm8 int) x86.M512d {
	return x86.M512d(m512Insertf64x4([8]float64(a), [4]float64(b), imm8))
}

func m512Insertf64x4(a [8]float64, b [4]float64, imm8 int) [8]float64


// M512MaskInsertf64x4: Copy 'a' to 'tmp', then insert 256 bits (composed of 4
// packed double-precision (64-bit) floating-point elements) from 'b' into
// 'tmp' at the location specified by 'imm8'.  Store 'tmp' to 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp[511:0] := a[511:0]
//		CASE (imm8[0]) of
//		0: tmp[255:0] := b[255:0]
//		1: tmp[511:256] := b[255:0]
//		ESAC
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTF64X4'. Intrinsic: '_mm512_mask_insertf64x4'.
// Requires AVX512F.
func M512MaskInsertf64x4(src x86.M512d, k x86.Mmask8, a x86.M512d, b x86.M256d, imm8 int) x86.M512d {
	return x86.M512d(m512MaskInsertf64x4([8]float64(src), uint8(k), [8]float64(a), [4]float64(b), imm8))
}

func m512MaskInsertf64x4(src [8]float64, k uint8, a [8]float64, b [4]float64, imm8 int) [8]float64


// M512MaskzInsertf64x4: Copy 'a' to 'tmp', then insert 256 bits (composed of 4
// packed double-precision (64-bit) floating-point elements) from 'b' into
// 'tmp' at the location specified by 'imm8'.  Store 'tmp' to 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp[511:0] := a[511:0]
//		CASE (imm8[0]) of
//		0: tmp[255:0] := b[255:0]
//		1: tmp[511:256] := b[255:0]
//		ESAC
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTF64X4'. Intrinsic: '_mm512_maskz_insertf64x4'.
// Requires AVX512F.
func M512MaskzInsertf64x4(k x86.Mmask8, a x86.M512d, b x86.M256d, imm8 int) x86.M512d {
	return x86.M512d(m512MaskzInsertf64x4(uint8(k), [8]float64(a), [4]float64(b), imm8))
}

func m512MaskzInsertf64x4(k uint8, a [8]float64, b [4]float64, imm8 int) [8]float64


// M256Inserti32x4: Copy 'a' to 'dst', then insert 128 bits (composed of 4
// packed 32-bit integers) from 'b' into 'dst' at the location specified by
// 'imm8'. 
//
//		dst[255:0] := a[255:0]
//		CASE (imm8[1:0]) of
//		0: dst[127:0] := b[127:0]
//		1: dst[255:128] := b[127:0]
//		ESAC
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTI32X4'. Intrinsic: '_mm256_inserti32x4'.
// Requires AVX512F.
func M256Inserti32x4(a x86.M256i, b x86.M128i, imm8 int) x86.M256i {
	return x86.M256i(m256Inserti32x4([32]byte(a), [16]byte(b), imm8))
}

func m256Inserti32x4(a [32]byte, b [16]byte, imm8 int) [32]byte


// M256MaskInserti32x4: Copy 'a' to 'tmp', then insert 128 bits (composed of 4
// packed 32-bit integers) from 'b' into 'tmp' at the location specified by
// 'imm8'.  Store 'tmp' to 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		tmp[255:0] := a[255:0]
//		CASE (imm8[1:0]) of
//		0: tmp[127:0] := b[127:0]
//		1: tmp[255:128] := b[127:0]
//		ESAC
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTI32X4'. Intrinsic: '_mm256_mask_inserti32x4'.
// Requires AVX512F.
func M256MaskInserti32x4(src x86.M256i, k x86.Mmask8, a x86.M256i, b x86.M128i, imm8 int) x86.M256i {
	return x86.M256i(m256MaskInserti32x4([32]byte(src), uint8(k), [32]byte(a), [16]byte(b), imm8))
}

func m256MaskInserti32x4(src [32]byte, k uint8, a [32]byte, b [16]byte, imm8 int) [32]byte


// M256MaskzInserti32x4: Copy 'a' to 'tmp', then insert 128 bits (composed of 4
// packed 32-bit integers) from 'b' into 'tmp' at the location specified by
// 'imm8'.  Store 'tmp' to 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		tmp[255:0] := a[255:0]
//		CASE (imm8[1:0]) of
//		0: tmp[127:0] := b[127:0]
//		1: tmp[255:128] := b[127:0]
//		ESAC
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTI32X4'. Intrinsic: '_mm256_maskz_inserti32x4'.
// Requires AVX512F.
func M256MaskzInserti32x4(k x86.Mmask8, a x86.M256i, b x86.M128i, imm8 int) x86.M256i {
	return x86.M256i(m256MaskzInserti32x4(uint8(k), [32]byte(a), [16]byte(b), imm8))
}

func m256MaskzInserti32x4(k uint8, a [32]byte, b [16]byte, imm8 int) [32]byte


// M512Inserti32x4: Copy 'a' to 'dst', then insert 128 bits (composed of 4
// packed 32-bit integers) from 'b' into 'dst' at the location specified by
// 'imm8'. 
//
//		dst[511:0] := a[511:0]
//		CASE (imm8[1:0]) of
//		0: dst[127:0] := b[127:0]
//		1: dst[255:128] := b[127:0]
//		2: dst[383:256] := b[127:0]
//		3: dst[511:384] := b[127:0]
//		ESAC
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTI32X4'. Intrinsic: '_mm512_inserti32x4'.
// Requires AVX512F.
func M512Inserti32x4(a x86.M512i, b x86.M128i, imm8 int) x86.M512i {
	return x86.M512i(m512Inserti32x4([64]byte(a), [16]byte(b), imm8))
}

func m512Inserti32x4(a [64]byte, b [16]byte, imm8 int) [64]byte


// M512MaskInserti32x4: Copy 'a' to 'tmp', then insert 128 bits (composed of 4
// packed 32-bit integers) from 'b' into 'tmp' at the location specified by
// 'imm8'.  Store 'tmp' to 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		tmp[511:0] := a[511:0]
//		CASE (imm8[1:0]) of
//		0: tmp[127:0] := b[127:0]
//		1: tmp[255:128] := b[127:0]
//		2: tmp[383:256] := b[127:0]
//		3: tmp[511:384] := b[127:0]
//		ESAC
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTI32X4'. Intrinsic: '_mm512_mask_inserti32x4'.
// Requires AVX512F.
func M512MaskInserti32x4(src x86.M512i, k x86.Mmask16, a x86.M512i, b x86.M128i, imm8 int) x86.M512i {
	return x86.M512i(m512MaskInserti32x4([64]byte(src), uint16(k), [64]byte(a), [16]byte(b), imm8))
}

func m512MaskInserti32x4(src [64]byte, k uint16, a [64]byte, b [16]byte, imm8 int) [64]byte


// M512MaskzInserti32x4: Copy 'a' to 'tmp', then insert 128 bits (composed of 4
// packed 32-bit integers) from 'b' into 'tmp' at the location specified by
// 'imm8'.  Store 'tmp' to 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		tmp[511:0] := a[511:0]
//		CASE (imm8[1:0]) of
//		0: tmp[127:0] := b[127:0]
//		1: tmp[255:128] := b[127:0]
//		2: tmp[383:256] := b[127:0]
//		3: tmp[511:384] := b[127:0]
//		ESAC
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTI32X4'. Intrinsic: '_mm512_maskz_inserti32x4'.
// Requires AVX512F.
func M512MaskzInserti32x4(k x86.Mmask16, a x86.M512i, b x86.M128i, imm8 int) x86.M512i {
	return x86.M512i(m512MaskzInserti32x4(uint16(k), [64]byte(a), [16]byte(b), imm8))
}

func m512MaskzInserti32x4(k uint16, a [64]byte, b [16]byte, imm8 int) [64]byte


// M512Inserti64x4: Copy 'a' to 'dst', then insert 256 bits (composed of 4
// packed 64-bit integers) from 'b' into 'dst' at the location specified by
// 'imm8'. 
//
//		dst[511:0] := a[511:0]
//		CASE (imm8[7:0]) OF
//		0: dst[255:0] := b[255:0]
//		1: dst[511:256] := b[255:0]
//		ESAC
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTI64X4'. Intrinsic: '_mm512_inserti64x4'.
// Requires AVX512F.
func M512Inserti64x4(a x86.M512i, b x86.M256i, imm8 int) x86.M512i {
	return x86.M512i(m512Inserti64x4([64]byte(a), [32]byte(b), imm8))
}

func m512Inserti64x4(a [64]byte, b [32]byte, imm8 int) [64]byte


// M512MaskInserti64x4: Copy 'a' to 'tmp', then insert 256 bits (composed of 4
// packed 64-bit integers) from 'b' into 'tmp' at the location specified by
// 'imm8'.  Store 'tmp' to 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		tmp[511:0] := a[511:0]
//		CASE (imm8[0]) of
//		0: tmp[255:0] := b[255:0]
//		1: tmp[511:256] := b[255:0]
//		ESAC
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTI64X4'. Intrinsic: '_mm512_mask_inserti64x4'.
// Requires AVX512F.
func M512MaskInserti64x4(src x86.M512i, k x86.Mmask8, a x86.M512i, b x86.M256i, imm8 int) x86.M512i {
	return x86.M512i(m512MaskInserti64x4([64]byte(src), uint8(k), [64]byte(a), [32]byte(b), imm8))
}

func m512MaskInserti64x4(src [64]byte, k uint8, a [64]byte, b [32]byte, imm8 int) [64]byte


// M512MaskzInserti64x4: Copy 'a' to 'tmp', then insert 256 bits (composed of 4
// packed 64-bit integers) from 'b' into 'tmp' at the location specified by
// 'imm8'.  Store 'tmp' to 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		tmp[511:0] := a[511:0]
//		CASE (imm8[0]) of
//		0: tmp[255:0] := b[255:0]
//		1: tmp[511:256] := b[255:0]
//		ESAC
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTI64X4'. Intrinsic: '_mm512_maskz_inserti64x4'.
// Requires AVX512F.
func M512MaskzInserti64x4(k x86.Mmask8, a x86.M512i, b x86.M256i, imm8 int) x86.M512i {
	return x86.M512i(m512MaskzInserti64x4(uint8(k), [64]byte(a), [32]byte(b), imm8))
}

func m512MaskzInserti64x4(k uint8, a [64]byte, b [32]byte, imm8 int) [64]byte


// M512InvsqrtPd: Compute the inverse square root of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := InvSQRT(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_invsqrt_pd'.
// Requires AVX512F.
func M512InvsqrtPd(a x86.M512d) x86.M512d {
	return x86.M512d(m512InvsqrtPd([8]float64(a)))
}

func m512InvsqrtPd(a [8]float64) [8]float64


// M512MaskInvsqrtPd: Compute the inverse square root of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := InvSQRT(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_invsqrt_pd'.
// Requires AVX512F.
func M512MaskInvsqrtPd(src x86.M512d, k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskInvsqrtPd([8]float64(src), uint8(k), [8]float64(a)))
}

func m512MaskInvsqrtPd(src [8]float64, k uint8, a [8]float64) [8]float64


// M512InvsqrtPs: Compute the inverse square root of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := InvSQRT(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_invsqrt_ps'.
// Requires AVX512F.
func M512InvsqrtPs(a x86.M512) x86.M512 {
	return x86.M512(m512InvsqrtPs([16]float32(a)))
}

func m512InvsqrtPs(a [16]float32) [16]float32


// M512MaskInvsqrtPs: Compute the inverse square root of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := InvSQRT(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_invsqrt_ps'.
// Requires AVX512F.
func M512MaskInvsqrtPs(src x86.M512, k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskInvsqrtPs([16]float32(src), uint16(k), [16]float32(a)))
}

func m512MaskInvsqrtPs(src [16]float32, k uint16, a [16]float32) [16]float32


// M512Kand: Compute the bitwise AND of 16-bit masks 'a' and 'b', and store the
// result in 'k'. 
//
//		k[15:0] := a[15:0] AND b[15:0]
//		k[MAX:16] := 0
//
// Instruction: 'KANDW'. Intrinsic: '_mm512_kand'.
// Requires AVX512F.
func M512Kand(a x86.Mmask16, b x86.Mmask16) x86.Mmask16 {
	return x86.Mmask16(m512Kand(uint16(a), uint16(b)))
}

func m512Kand(a uint16, b uint16) uint16


// M512Kandn: Compute the bitwise AND NOT of 16-bit masks 'a' and 'b', and
// store the result in 'k'. 
//
//		k[15:0] := (NOT a[15:0]) AND b[15:0]
//		k[MAX:16] := 0
//
// Instruction: 'KANDNW'. Intrinsic: '_mm512_kandn'.
// Requires AVX512F.
func M512Kandn(a x86.Mmask16, b x86.Mmask16) x86.Mmask16 {
	return x86.Mmask16(m512Kandn(uint16(a), uint16(b)))
}

func m512Kandn(a uint16, b uint16) uint16


// M512Kmov: Copy 16-bit mask 'a' to 'k'. 
//
//		k[15:0] := a[15:0]
//		k[MAX:16] := 0
//
// Instruction: 'KMOVW'. Intrinsic: '_mm512_kmov'.
// Requires AVX512F.
func M512Kmov(a x86.Mmask16) x86.Mmask16 {
	return x86.Mmask16(m512Kmov(uint16(a)))
}

func m512Kmov(a uint16) uint16


// M512Knot: Compute the bitwise NOT of 16-bit mask 'a', and store the result
// in 'k'. 
//
//		k[15:0] := NOT a[15:0]
//		k[MAX:16] := 0
//
// Instruction: 'KNOTW'. Intrinsic: '_mm512_knot'.
// Requires AVX512F.
func M512Knot(a x86.Mmask16) x86.Mmask16 {
	return x86.Mmask16(m512Knot(uint16(a)))
}

func m512Knot(a uint16) uint16


// M512Kor: Compute the bitwise OR of 16-bit masks 'a' and 'b', and store the
// result in 'k'. 
//
//		k[15:0] := a[15:0] OR b[15:0]
//		k[MAX:16] := 0
//
// Instruction: 'KORW'. Intrinsic: '_mm512_kor'.
// Requires AVX512F.
func M512Kor(a x86.Mmask16, b x86.Mmask16) x86.Mmask16 {
	return x86.Mmask16(m512Kor(uint16(a), uint16(b)))
}

func m512Kor(a uint16, b uint16) uint16


// M512Kortestc: Performs bitwise OR between 'k1' and 'k2', storing the result
// in 'dst'. CF flag is set if 'dst' consists of all 1's. 
//
//		dst[15:0] := k1[15:0] | k2[15:0]
//		IF PopCount(dst[15:0]) = 16
//			SetCF()
//		FI
//
// Instruction: 'KORTESTW'. Intrinsic: '_mm512_kortestc'.
// Requires AVX512F.
func M512Kortestc(k1 x86.Mmask16, k2 x86.Mmask16) int {
	return int(m512Kortestc(uint16(k1), uint16(k2)))
}

func m512Kortestc(k1 uint16, k2 uint16) int


// M512Kortestz: Performs bitwise OR between 'k1' and 'k2', storing the result
// in 'dst'. ZF flag is set if 'dst' is 0. 
//
//		dst[15:0] := k1[15:0] | k2[15:0]
//		IF dst = 0
//			SetZF()
//		FI
//
// Instruction: 'KORTESTW'. Intrinsic: '_mm512_kortestz'.
// Requires AVX512F.
func M512Kortestz(k1 x86.Mmask16, k2 x86.Mmask16) int {
	return int(m512Kortestz(uint16(k1), uint16(k2)))
}

func m512Kortestz(k1 uint16, k2 uint16) int


// M512Kunpackb: Unpack and interleave 8 bits from masks 'a' and 'b', and store
// the 16-bit result in 'k'. 
//
//		k[7:0] := b[7:0]
//		k[15:8] := a[7:0]
//		k[MAX:16] := 0
//
// Instruction: 'KUNPCKBW'. Intrinsic: '_mm512_kunpackb'.
// Requires AVX512F.
func M512Kunpackb(a x86.Mmask16, b x86.Mmask16) x86.Mmask16 {
	return x86.Mmask16(m512Kunpackb(uint16(a), uint16(b)))
}

func m512Kunpackb(a uint16, b uint16) uint16


// M512Kxnor: Compute the bitwise XNOR of 16-bit masks 'a' and 'b', and store
// the result in 'k'. 
//
//		k[15:0] := NOT (a[15:0] XOR b[15:0])
//		k[MAX:16] := 0
//
// Instruction: 'KXNORW'. Intrinsic: '_mm512_kxnor'.
// Requires AVX512F.
func M512Kxnor(a x86.Mmask16, b x86.Mmask16) x86.Mmask16 {
	return x86.Mmask16(m512Kxnor(uint16(a), uint16(b)))
}

func m512Kxnor(a uint16, b uint16) uint16


// M512Kxor: Compute the bitwise XOR of 16-bit masks 'a' and 'b', and store the
// result in 'k'. 
//
//		k[15:0] := a[15:0] XOR b[15:0]
//		k[MAX:16] := 0
//
// Instruction: 'KXORW'. Intrinsic: '_mm512_kxor'.
// Requires AVX512F.
func M512Kxor(a x86.Mmask16, b x86.Mmask16) x86.Mmask16 {
	return x86.Mmask16(m512Kxor(uint16(a), uint16(b)))
}

func m512Kxor(a uint16, b uint16) uint16


// Skipped: _mm_mask_load_epi32. Contains pointer parameter.


// Skipped: _mm_maskz_load_epi32. Contains pointer parameter.


// Skipped: _mm256_mask_load_epi32. Contains pointer parameter.


// Skipped: _mm256_maskz_load_epi32. Contains pointer parameter.


// Skipped: _mm512_maskz_load_epi32. Contains pointer parameter.


// Skipped: _mm_mask_load_epi64. Contains pointer parameter.


// Skipped: _mm_maskz_load_epi64. Contains pointer parameter.


// Skipped: _mm256_mask_load_epi64. Contains pointer parameter.


// Skipped: _mm256_maskz_load_epi64. Contains pointer parameter.


// Skipped: _mm512_maskz_load_epi64. Contains pointer parameter.


// Skipped: _mm_mask_load_pd. Contains pointer parameter.


// Skipped: _mm_maskz_load_pd. Contains pointer parameter.


// Skipped: _mm256_mask_load_pd. Contains pointer parameter.


// Skipped: _mm256_maskz_load_pd. Contains pointer parameter.


// Skipped: _mm512_maskz_load_pd. Contains pointer parameter.


// Skipped: _mm_mask_load_ps. Contains pointer parameter.


// Skipped: _mm_maskz_load_ps. Contains pointer parameter.


// Skipped: _mm256_mask_load_ps. Contains pointer parameter.


// Skipped: _mm256_maskz_load_ps. Contains pointer parameter.


// Skipped: _mm512_maskz_load_ps. Contains pointer parameter.


// MaskLoadSd: Load a double-precision (64-bit) floating-point element from
// memory into the lower element of 'dst' using writemask 'k' (the element is
// copied from 'src' when mask bit 0 is not set), and set the upper element of
// 'dst' to zero. 'mem_addr' must be aligned on a 16-byte boundary or a
// general-protection exception may be generated. 
//
//		IF k[0]
//			dst[63:0] := MEM[mem_addr+63:mem_addr]
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[MAX:64] := 0
//
// Instruction: 'VMOVSD'. Intrinsic: '_mm_mask_load_sd'.
// Requires AVX512F.
// FIXME: Will likely need to be reworked.
func MaskLoadSd(src x86.M128d, k x86.Mmask8, mem_addr *float64) x86.M128d {
	// FIXME: Rework to avoid possible return value as parameter.
	return x86.M128d{}
}

// MaskzLoadSd: Load a double-precision (64-bit) floating-point element from
// memory into the lower element of 'dst' using zeromask 'k' (the element is
// zeroed out when mask bit 0 is not set), and set the upper element of 'dst'
// to zero. 'mem_addr' must be aligned on a 16-byte boundary or a
// general-protection exception may be generated. 
//
//		IF k[0]
//			dst[63:0] := MEM[mem_addr+63:mem_addr]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[MAX:64] := 0
//
// Instruction: 'VMOVSD'. Intrinsic: '_mm_maskz_load_sd'.
// Requires AVX512F.
// FIXME: Will likely need to be reworked.
func MaskzLoadSd(k x86.Mmask8, mem_addr *float64) x86.M128d {
	// FIXME: Rework to avoid possible return value as parameter.
	return x86.M128d{}
}

// MaskLoadSs: Load a single-precision (32-bit) floating-point element from
// memory into the lower element of 'dst' using writemask 'k' (the element is
// copied from 'src' when mask bit 0 is not set), and set the upper elements of
// 'dst' to zero. 'mem_addr' must be aligned on a 16-byte boundary or a
// general-protection exception may be generated. 
//
//		IF k[0]
//			dst[31:0] := MEM[mem_addr+31:mem_addr]
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[MAX:32] := 0
//
// Instruction: 'VMOVSS'. Intrinsic: '_mm_mask_load_ss'.
// Requires AVX512F.
// FIXME: Will likely need to be reworked.
func MaskLoadSs(src x86.M128, k x86.Mmask8, mem_addr *float32) x86.M128 {
	// FIXME: Rework to avoid possible return value as parameter.
	return x86.M128{}
}

// MaskzLoadSs: Load a single-precision (32-bit) floating-point element from
// memory into the lower element of 'dst' using zeromask 'k' (the element is
// zeroed out when mask bit 0 is not set), and set the upper elements of 'dst'
// to zero. 'mem_addr' must be aligned on a 16-byte boundary or a
// general-protection exception may be generated. 
//
//		IF k[0]
//			dst[31:0] := MEM[mem_addr+31:mem_addr]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[MAX:32] := 0
//
// Instruction: 'VMOVSS'. Intrinsic: '_mm_maskz_load_ss'.
// Requires AVX512F.
// FIXME: Will likely need to be reworked.
func MaskzLoadSs(k x86.Mmask8, mem_addr *float32) x86.M128 {
	// FIXME: Rework to avoid possible return value as parameter.
	return x86.M128{}
}

// Skipped: _mm_mask_loadu_epi32. Contains pointer parameter.


// Skipped: _mm_maskz_loadu_epi32. Contains pointer parameter.


// Skipped: _mm256_mask_loadu_epi32. Contains pointer parameter.


// Skipped: _mm256_maskz_loadu_epi32. Contains pointer parameter.


// Skipped: _mm512_mask_loadu_epi32. Contains pointer parameter.


// Skipped: _mm512_maskz_loadu_epi32. Contains pointer parameter.


// Skipped: _mm_mask_loadu_epi64. Contains pointer parameter.


// Skipped: _mm_maskz_loadu_epi64. Contains pointer parameter.


// Skipped: _mm256_mask_loadu_epi64. Contains pointer parameter.


// Skipped: _mm256_maskz_loadu_epi64. Contains pointer parameter.


// Skipped: _mm512_mask_loadu_epi64. Contains pointer parameter.


// Skipped: _mm512_maskz_loadu_epi64. Contains pointer parameter.


// Skipped: _mm_mask_loadu_pd. Contains pointer parameter.


// Skipped: _mm_maskz_loadu_pd. Contains pointer parameter.


// Skipped: _mm256_mask_loadu_pd. Contains pointer parameter.


// Skipped: _mm256_maskz_loadu_pd. Contains pointer parameter.


// Skipped: _mm512_loadu_pd. Contains pointer parameter.


// Skipped: _mm512_mask_loadu_pd. Contains pointer parameter.


// Skipped: _mm512_maskz_loadu_pd. Contains pointer parameter.


// Skipped: _mm_mask_loadu_ps. Contains pointer parameter.


// Skipped: _mm_maskz_loadu_ps. Contains pointer parameter.


// Skipped: _mm256_mask_loadu_ps. Contains pointer parameter.


// Skipped: _mm256_maskz_loadu_ps. Contains pointer parameter.


// Skipped: _mm512_loadu_ps. Contains pointer parameter.


// Skipped: _mm512_mask_loadu_ps. Contains pointer parameter.


// Skipped: _mm512_maskz_loadu_ps. Contains pointer parameter.


// Skipped: _mm512_loadu_si512. Contains pointer parameter.


// M512LogPd: Compute the natural logarithm of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ln(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_log_pd'.
// Requires AVX512F.
func M512LogPd(a x86.M512d) x86.M512d {
	return x86.M512d(m512LogPd([8]float64(a)))
}

func m512LogPd(a [8]float64) [8]float64


// M512MaskLogPd: Compute the natural logarithm of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ln(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_log_pd'.
// Requires AVX512F.
func M512MaskLogPd(src x86.M512d, k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskLogPd([8]float64(src), uint8(k), [8]float64(a)))
}

func m512MaskLogPd(src [8]float64, k uint8, a [8]float64) [8]float64


// M512LogPs: Compute the natural logarithm of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ln(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_log_ps'.
// Requires AVX512F.
func M512LogPs(a x86.M512) x86.M512 {
	return x86.M512(m512LogPs([16]float32(a)))
}

func m512LogPs(a [16]float32) [16]float32


// M512MaskLogPs: Compute the natural logarithm of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ln(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_log_ps'.
// Requires AVX512F.
func M512MaskLogPs(src x86.M512, k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskLogPs([16]float32(src), uint16(k), [16]float32(a)))
}

func m512MaskLogPs(src [16]float32, k uint16, a [16]float32) [16]float32


// M512Log10Pd: Compute the base-10 logarithm of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := log10(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_log10_pd'.
// Requires AVX512F.
func M512Log10Pd(a x86.M512d) x86.M512d {
	return x86.M512d(m512Log10Pd([8]float64(a)))
}

func m512Log10Pd(a [8]float64) [8]float64


// M512MaskLog10Pd: Compute the base-10 logarithm of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := log10(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_log10_pd'.
// Requires AVX512F.
func M512MaskLog10Pd(src x86.M512d, k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskLog10Pd([8]float64(src), uint8(k), [8]float64(a)))
}

func m512MaskLog10Pd(src [8]float64, k uint8, a [8]float64) [8]float64


// M512Log10Ps: Compute the base-10 logarithm of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := log10(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_log10_ps'.
// Requires AVX512F.
func M512Log10Ps(a x86.M512) x86.M512 {
	return x86.M512(m512Log10Ps([16]float32(a)))
}

func m512Log10Ps(a [16]float32) [16]float32


// M512MaskLog10Ps: Compute the base-10 logarithm of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := log10(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_log10_ps'.
// Requires AVX512F.
func M512MaskLog10Ps(src x86.M512, k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskLog10Ps([16]float32(src), uint16(k), [16]float32(a)))
}

func m512MaskLog10Ps(src [16]float32, k uint16, a [16]float32) [16]float32


// M512Log1pPd: Compute the natural logarithm of one plus packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ln(1.0 + a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_log1p_pd'.
// Requires AVX512F.
func M512Log1pPd(a x86.M512d) x86.M512d {
	return x86.M512d(m512Log1pPd([8]float64(a)))
}

func m512Log1pPd(a [8]float64) [8]float64


// M512MaskLog1pPd: Compute the natural logarithm of one plus packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ln(1.0 + a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_log1p_pd'.
// Requires AVX512F.
func M512MaskLog1pPd(src x86.M512d, k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskLog1pPd([8]float64(src), uint8(k), [8]float64(a)))
}

func m512MaskLog1pPd(src [8]float64, k uint8, a [8]float64) [8]float64


// M512Log1pPs: Compute the natural logarithm of one plus packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ln(1.0 + a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_log1p_ps'.
// Requires AVX512F.
func M512Log1pPs(a x86.M512) x86.M512 {
	return x86.M512(m512Log1pPs([16]float32(a)))
}

func m512Log1pPs(a [16]float32) [16]float32


// M512MaskLog1pPs: Compute the natural logarithm of one plus packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ln(1.0 + a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_log1p_ps'.
// Requires AVX512F.
func M512MaskLog1pPs(src x86.M512, k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskLog1pPs([16]float32(src), uint16(k), [16]float32(a)))
}

func m512MaskLog1pPs(src [16]float32, k uint16, a [16]float32) [16]float32


// M512Log2Pd: Compute the base-2 logarithm of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := log2(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_log2_pd'.
// Requires AVX512F.
func M512Log2Pd(a x86.M512d) x86.M512d {
	return x86.M512d(m512Log2Pd([8]float64(a)))
}

func m512Log2Pd(a [8]float64) [8]float64


// M512MaskLog2Pd: Compute the base-2 logarithm of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := log2(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_log2_pd'.
// Requires AVX512F.
func M512MaskLog2Pd(src x86.M512d, k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskLog2Pd([8]float64(src), uint8(k), [8]float64(a)))
}

func m512MaskLog2Pd(src [8]float64, k uint8, a [8]float64) [8]float64


// M512LogbPd: Convert the exponent of each packed double-precision (64-bit)
// floating-point element in 'a' to a double-precision floating-point number
// representing the integer exponent, and store the results in 'dst'. This
// intrinsic essentially calculates 'floor(log2(x))' for each element. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ConvertExpFP64(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_logb_pd'.
// Requires AVX512F.
func M512LogbPd(a x86.M512d) x86.M512d {
	return x86.M512d(m512LogbPd([8]float64(a)))
}

func m512LogbPd(a [8]float64) [8]float64


// M512MaskLogbPd: Convert the exponent of each packed double-precision
// (64-bit) floating-point element in 'a' to a double-precision floating-point
// number representing the integer exponent, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). This intrinsic essentially calculates 'floor(log2(x))'
// for each element. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ConvertExpFP64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_logb_pd'.
// Requires AVX512F.
func M512MaskLogbPd(src x86.M512d, k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskLogbPd([8]float64(src), uint8(k), [8]float64(a)))
}

func m512MaskLogbPd(src [8]float64, k uint8, a [8]float64) [8]float64


// M512LogbPs: Convert the exponent of each packed single-precision (32-bit)
// floating-point element in 'a' to a single-precision floating-point number
// representing the integer exponent, and store the results in 'dst'. This
// intrinsic essentially calculates 'floor(log2(x))' for each element. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ConvertExpFP32(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_logb_ps'.
// Requires AVX512F.
func M512LogbPs(a x86.M512) x86.M512 {
	return x86.M512(m512LogbPs([16]float32(a)))
}

func m512LogbPs(a [16]float32) [16]float32


// M512MaskLogbPs: Convert the exponent of each packed single-precision
// (32-bit) floating-point element in 'a' to a single-precision floating-point
// number representing the integer exponent, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). This intrinsic essentially calculates 'floor(log2(x))'
// for each element. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ConvertExpFP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_logb_ps'.
// Requires AVX512F.
func M512MaskLogbPs(src x86.M512, k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskLogbPs([16]float32(src), uint16(k), [16]float32(a)))
}

func m512MaskLogbPs(src [16]float32, k uint16, a [16]float32) [16]float32


// MaskMaxEpi32: Compare packed 32-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] > b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMAXSD'. Intrinsic: '_mm_mask_max_epi32'.
// Requires AVX512F.
func MaskMaxEpi32(src x86.M128i, k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskMaxEpi32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskMaxEpi32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzMaxEpi32: Compare packed 32-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] > b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMAXSD'. Intrinsic: '_mm_maskz_max_epi32'.
// Requires AVX512F.
func MaskzMaxEpi32(k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskzMaxEpi32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzMaxEpi32(k uint8, a [16]byte, b [16]byte) [16]byte


// M256MaskMaxEpi32: Compare packed 32-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] > b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXSD'. Intrinsic: '_mm256_mask_max_epi32'.
// Requires AVX512F.
func M256MaskMaxEpi32(src x86.M256i, k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskMaxEpi32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskMaxEpi32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// M256MaskzMaxEpi32: Compare packed 32-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] > b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXSD'. Intrinsic: '_mm256_maskz_max_epi32'.
// Requires AVX512F.
func M256MaskzMaxEpi32(k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzMaxEpi32(uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskzMaxEpi32(k uint8, a [32]byte, b [32]byte) [32]byte


// M512MaskzMaxEpi32: Compare packed 32-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] > b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0 
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMAXSD'. Intrinsic: '_mm512_maskz_max_epi32'.
// Requires AVX512F.
func M512MaskzMaxEpi32(k x86.Mmask16, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzMaxEpi32(uint16(k), [64]byte(a), [64]byte(b)))
}

func m512MaskzMaxEpi32(k uint16, a [64]byte, b [64]byte) [64]byte


// MaskMaxEpi64: Compare packed 64-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] > b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMAXSQ'. Intrinsic: '_mm_mask_max_epi64'.
// Requires AVX512F.
func MaskMaxEpi64(src x86.M128i, k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskMaxEpi64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskMaxEpi64(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzMaxEpi64: Compare packed 64-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] > b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMAXSQ'. Intrinsic: '_mm_maskz_max_epi64'.
// Requires AVX512F.
func MaskzMaxEpi64(k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskzMaxEpi64(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzMaxEpi64(k uint8, a [16]byte, b [16]byte) [16]byte


// MaxEpi64: Compare packed 64-bit integers in 'a' and 'b', and store packed
// maximum values in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF a[i+63:i] > b[i+63:i]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := b[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMAXSQ'. Intrinsic: '_mm_max_epi64'.
// Requires AVX512F.
func MaxEpi64(a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maxEpi64([16]byte(a), [16]byte(b)))
}

func maxEpi64(a [16]byte, b [16]byte) [16]byte


// M256MaskMaxEpi64: Compare packed 64-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] > b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXSQ'. Intrinsic: '_mm256_mask_max_epi64'.
// Requires AVX512F.
func M256MaskMaxEpi64(src x86.M256i, k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskMaxEpi64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskMaxEpi64(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// M256MaskzMaxEpi64: Compare packed 64-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] > b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXSQ'. Intrinsic: '_mm256_maskz_max_epi64'.
// Requires AVX512F.
func M256MaskzMaxEpi64(k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzMaxEpi64(uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskzMaxEpi64(k uint8, a [32]byte, b [32]byte) [32]byte


// M256MaxEpi64: Compare packed 64-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF a[i+63:i] > b[i+63:i]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := b[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXSQ'. Intrinsic: '_mm256_max_epi64'.
// Requires AVX512F.
func M256MaxEpi64(a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaxEpi64([32]byte(a), [32]byte(b)))
}

func m256MaxEpi64(a [32]byte, b [32]byte) [32]byte


// M512MaskMaxEpi64: Compare packed 64-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] > b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMAXSQ'. Intrinsic: '_mm512_mask_max_epi64'.
// Requires AVX512F.
func M512MaskMaxEpi64(src x86.M512i, k x86.Mmask8, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskMaxEpi64([64]byte(src), uint8(k), [64]byte(a), [64]byte(b)))
}

func m512MaskMaxEpi64(src [64]byte, k uint8, a [64]byte, b [64]byte) [64]byte


// M512MaskzMaxEpi64: Compare packed 64-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] > b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMAXSQ'. Intrinsic: '_mm512_maskz_max_epi64'.
// Requires AVX512F.
func M512MaskzMaxEpi64(k x86.Mmask8, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzMaxEpi64(uint8(k), [64]byte(a), [64]byte(b)))
}

func m512MaskzMaxEpi64(k uint8, a [64]byte, b [64]byte) [64]byte


// M512MaxEpi64: Compare packed 64-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF a[i+63:i] > b[i+63:i]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := b[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMAXSQ'. Intrinsic: '_mm512_max_epi64'.
// Requires AVX512F.
func M512MaxEpi64(a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaxEpi64([64]byte(a), [64]byte(b)))
}

func m512MaxEpi64(a [64]byte, b [64]byte) [64]byte


// MaskMaxEpu32: Compare packed unsigned 32-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] > b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMAXUD'. Intrinsic: '_mm_mask_max_epu32'.
// Requires AVX512F.
func MaskMaxEpu32(src x86.M128i, k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskMaxEpu32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskMaxEpu32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzMaxEpu32: Compare packed unsigned 32-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] > b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMAXUD'. Intrinsic: '_mm_maskz_max_epu32'.
// Requires AVX512F.
func MaskzMaxEpu32(k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskzMaxEpu32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzMaxEpu32(k uint8, a [16]byte, b [16]byte) [16]byte


// M256MaskMaxEpu32: Compare packed unsigned 32-bit integers in 'a' and 'b',
// and store packed maximum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] > b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXUD'. Intrinsic: '_mm256_mask_max_epu32'.
// Requires AVX512F.
func M256MaskMaxEpu32(src x86.M256i, k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskMaxEpu32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskMaxEpu32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// M256MaskzMaxEpu32: Compare packed unsigned 32-bit integers in 'a' and 'b',
// and store packed maximum values in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] > b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXUD'. Intrinsic: '_mm256_maskz_max_epu32'.
// Requires AVX512F.
func M256MaskzMaxEpu32(k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzMaxEpu32(uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskzMaxEpu32(k uint8, a [32]byte, b [32]byte) [32]byte


// M512MaskzMaxEpu32: Compare packed unsigned 32-bit integers in 'a' and 'b',
// and store packed maximum values in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] > b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMAXUD'. Intrinsic: '_mm512_maskz_max_epu32'.
// Requires AVX512F.
func M512MaskzMaxEpu32(k x86.Mmask16, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzMaxEpu32(uint16(k), [64]byte(a), [64]byte(b)))
}

func m512MaskzMaxEpu32(k uint16, a [64]byte, b [64]byte) [64]byte


// MaskMaxEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] > b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMAXUQ'. Intrinsic: '_mm_mask_max_epu64'.
// Requires AVX512F.
func MaskMaxEpu64(src x86.M128i, k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskMaxEpu64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskMaxEpu64(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzMaxEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] > b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMAXUQ'. Intrinsic: '_mm_maskz_max_epu64'.
// Requires AVX512F.
func MaskzMaxEpu64(k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskzMaxEpu64(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzMaxEpu64(k uint8, a [16]byte, b [16]byte) [16]byte


// MaxEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF a[i+63:i] > b[i+63:i]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := b[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMAXUQ'. Intrinsic: '_mm_max_epu64'.
// Requires AVX512F.
func MaxEpu64(a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maxEpu64([16]byte(a), [16]byte(b)))
}

func maxEpu64(a [16]byte, b [16]byte) [16]byte


// M256MaskMaxEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b',
// and store packed maximum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] > b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXUQ'. Intrinsic: '_mm256_mask_max_epu64'.
// Requires AVX512F.
func M256MaskMaxEpu64(src x86.M256i, k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskMaxEpu64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskMaxEpu64(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// M256MaskzMaxEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b',
// and store packed maximum values in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] > b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXUQ'. Intrinsic: '_mm256_maskz_max_epu64'.
// Requires AVX512F.
func M256MaskzMaxEpu64(k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzMaxEpu64(uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskzMaxEpu64(k uint8, a [32]byte, b [32]byte) [32]byte


// M256MaxEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF a[i+63:i] > b[i+63:i]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := b[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXUQ'. Intrinsic: '_mm256_max_epu64'.
// Requires AVX512F.
func M256MaxEpu64(a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaxEpu64([32]byte(a), [32]byte(b)))
}

func m256MaxEpu64(a [32]byte, b [32]byte) [32]byte


// M512MaskMaxEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b',
// and store packed maximum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] > b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMAXUQ'. Intrinsic: '_mm512_mask_max_epu64'.
// Requires AVX512F.
func M512MaskMaxEpu64(src x86.M512i, k x86.Mmask8, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskMaxEpu64([64]byte(src), uint8(k), [64]byte(a), [64]byte(b)))
}

func m512MaskMaxEpu64(src [64]byte, k uint8, a [64]byte, b [64]byte) [64]byte


// M512MaskzMaxEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b',
// and store packed maximum values in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] > b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMAXUQ'. Intrinsic: '_mm512_maskz_max_epu64'.
// Requires AVX512F.
func M512MaskzMaxEpu64(k x86.Mmask8, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzMaxEpu64(uint8(k), [64]byte(a), [64]byte(b)))
}

func m512MaskzMaxEpu64(k uint8, a [64]byte, b [64]byte) [64]byte


// M512MaxEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF a[i+63:i] > b[i+63:i]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := b[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMAXUQ'. Intrinsic: '_mm512_max_epu64'.
// Requires AVX512F.
func M512MaxEpu64(a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaxEpu64([64]byte(a), [64]byte(b)))
}

func m512MaxEpu64(a [64]byte, b [64]byte) [64]byte


// MaskMaxPd: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', and store packed maximum values in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMAXPD'. Intrinsic: '_mm_mask_max_pd'.
// Requires AVX512F.
func MaskMaxPd(src x86.M128d, k x86.Mmask8, a x86.M128d, b x86.M128d) x86.M128d {
	return x86.M128d(maskMaxPd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskMaxPd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzMaxPd: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', and store packed maximum values in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMAXPD'. Intrinsic: '_mm_maskz_max_pd'.
// Requires AVX512F.
func MaskzMaxPd(k x86.Mmask8, a x86.M128d, b x86.M128d) x86.M128d {
	return x86.M128d(maskzMaxPd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzMaxPd(k uint8, a [2]float64, b [2]float64) [2]float64


// M256MaskMaxPd: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store packed maximum values in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMAXPD'. Intrinsic: '_mm256_mask_max_pd'.
// Requires AVX512F.
func M256MaskMaxPd(src x86.M256d, k x86.Mmask8, a x86.M256d, b x86.M256d) x86.M256d {
	return x86.M256d(m256MaskMaxPd([4]float64(src), uint8(k), [4]float64(a), [4]float64(b)))
}

func m256MaskMaxPd(src [4]float64, k uint8, a [4]float64, b [4]float64) [4]float64


// M256MaskzMaxPd: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store packed maximum values in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMAXPD'. Intrinsic: '_mm256_maskz_max_pd'.
// Requires AVX512F.
func M256MaskzMaxPd(k x86.Mmask8, a x86.M256d, b x86.M256d) x86.M256d {
	return x86.M256d(m256MaskzMaxPd(uint8(k), [4]float64(a), [4]float64(b)))
}

func m256MaskzMaxPd(k uint8, a [4]float64, b [4]float64) [4]float64


// M512MaskMaxPd: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store packed maximum values in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMAXPD'. Intrinsic: '_mm512_mask_max_pd'.
// Requires AVX512F.
func M512MaskMaxPd(src x86.M512d, k x86.Mmask8, a x86.M512d, b x86.M512d) x86.M512d {
	return x86.M512d(m512MaskMaxPd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b)))
}

func m512MaskMaxPd(src [8]float64, k uint8, a [8]float64, b [8]float64) [8]float64


// M512MaskzMaxPd: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store packed maximum values in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMAXPD'. Intrinsic: '_mm512_maskz_max_pd'.
// Requires AVX512F.
func M512MaskzMaxPd(k x86.Mmask8, a x86.M512d, b x86.M512d) x86.M512d {
	return x86.M512d(m512MaskzMaxPd(uint8(k), [8]float64(a), [8]float64(b)))
}

func m512MaskzMaxPd(k uint8, a [8]float64, b [8]float64) [8]float64


// M512MaxPd: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', and store packed maximum values in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMAXPD'. Intrinsic: '_mm512_max_pd'.
// Requires AVX512F.
func M512MaxPd(a x86.M512d, b x86.M512d) x86.M512d {
	return x86.M512d(m512MaxPd([8]float64(a), [8]float64(b)))
}

func m512MaxPd(a [8]float64, b [8]float64) [8]float64


// MaskMaxPs: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', and store packed maximum values in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMAXPS'. Intrinsic: '_mm_mask_max_ps'.
// Requires AVX512F.
func MaskMaxPs(src x86.M128, k x86.Mmask8, a x86.M128, b x86.M128) x86.M128 {
	return x86.M128(maskMaxPs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskMaxPs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzMaxPs: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', and store packed maximum values in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMAXPS'. Intrinsic: '_mm_maskz_max_ps'.
// Requires AVX512F.
func MaskzMaxPs(k x86.Mmask8, a x86.M128, b x86.M128) x86.M128 {
	return x86.M128(maskzMaxPs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzMaxPs(k uint8, a [4]float32, b [4]float32) [4]float32


// M256MaskMaxPs: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store packed maximum values in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMAXPS'. Intrinsic: '_mm256_mask_max_ps'.
// Requires AVX512F.
func M256MaskMaxPs(src x86.M256, k x86.Mmask8, a x86.M256, b x86.M256) x86.M256 {
	return x86.M256(m256MaskMaxPs([8]float32(src), uint8(k), [8]float32(a), [8]float32(b)))
}

func m256MaskMaxPs(src [8]float32, k uint8, a [8]float32, b [8]float32) [8]float32


// M256MaskzMaxPs: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store packed maximum values in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMAXPS'. Intrinsic: '_mm256_maskz_max_ps'.
// Requires AVX512F.
func M256MaskzMaxPs(k x86.Mmask8, a x86.M256, b x86.M256) x86.M256 {
	return x86.M256(m256MaskzMaxPs(uint8(k), [8]float32(a), [8]float32(b)))
}

func m256MaskzMaxPs(k uint8, a [8]float32, b [8]float32) [8]float32


// M512MaskMaxPs: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store packed maximum values in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMAXPS'. Intrinsic: '_mm512_mask_max_ps'.
// Requires AVX512F.
func M512MaskMaxPs(src x86.M512, k x86.Mmask16, a x86.M512, b x86.M512) x86.M512 {
	return x86.M512(m512MaskMaxPs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b)))
}

func m512MaskMaxPs(src [16]float32, k uint16, a [16]float32, b [16]float32) [16]float32


// M512MaskzMaxPs: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store packed maximum values in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMAXPS'. Intrinsic: '_mm512_maskz_max_ps'.
// Requires AVX512F.
func M512MaskzMaxPs(k x86.Mmask16, a x86.M512, b x86.M512) x86.M512 {
	return x86.M512(m512MaskzMaxPs(uint16(k), [16]float32(a), [16]float32(b)))
}

func m512MaskzMaxPs(k uint16, a [16]float32, b [16]float32) [16]float32


// M512MaxPs: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', and store packed maximum values in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMAXPS'. Intrinsic: '_mm512_max_ps'.
// Requires AVX512F.
func M512MaxPs(a x86.M512, b x86.M512) x86.M512 {
	return x86.M512(m512MaxPs([16]float32(a), [16]float32(b)))
}

func m512MaxPs(a [16]float32, b [16]float32) [16]float32


// M512MaskMaxRoundPd: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store packed maximum values in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set).  
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMAXPD'. Intrinsic: '_mm512_mask_max_round_pd'.
// Requires AVX512F.
func M512MaskMaxRoundPd(src x86.M512d, k x86.Mmask8, a x86.M512d, b x86.M512d, sae int) x86.M512d {
	return x86.M512d(m512MaskMaxRoundPd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b), sae))
}

func m512MaskMaxRoundPd(src [8]float64, k uint8, a [8]float64, b [8]float64, sae int) [8]float64


// M512MaskzMaxRoundPd: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store packed maximum values in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMAXPD'. Intrinsic: '_mm512_maskz_max_round_pd'.
// Requires AVX512F.
func M512MaskzMaxRoundPd(k x86.Mmask8, a x86.M512d, b x86.M512d, sae int) x86.M512d {
	return x86.M512d(m512MaskzMaxRoundPd(uint8(k), [8]float64(a), [8]float64(b), sae))
}

func m512MaskzMaxRoundPd(k uint8, a [8]float64, b [8]float64, sae int) [8]float64


// M512MaxRoundPd: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store packed maximum values in 'dst'. 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMAXPD'. Intrinsic: '_mm512_max_round_pd'.
// Requires AVX512F.
func M512MaxRoundPd(a x86.M512d, b x86.M512d, sae int) x86.M512d {
	return x86.M512d(m512MaxRoundPd([8]float64(a), [8]float64(b), sae))
}

func m512MaxRoundPd(a [8]float64, b [8]float64, sae int) [8]float64


// M512MaskMaxRoundPs: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store packed maximum values in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set).  
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMAXPS'. Intrinsic: '_mm512_mask_max_round_ps'.
// Requires AVX512F.
func M512MaskMaxRoundPs(src x86.M512, k x86.Mmask16, a x86.M512, b x86.M512, sae int) x86.M512 {
	return x86.M512(m512MaskMaxRoundPs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b), sae))
}

func m512MaskMaxRoundPs(src [16]float32, k uint16, a [16]float32, b [16]float32, sae int) [16]float32


// M512MaskzMaxRoundPs: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store packed maximum values in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMAXPS'. Intrinsic: '_mm512_maskz_max_round_ps'.
// Requires AVX512F.
func M512MaskzMaxRoundPs(k x86.Mmask16, a x86.M512, b x86.M512, sae int) x86.M512 {
	return x86.M512(m512MaskzMaxRoundPs(uint16(k), [16]float32(a), [16]float32(b), sae))
}

func m512MaskzMaxRoundPs(k uint16, a [16]float32, b [16]float32, sae int) [16]float32


// M512MaxRoundPs: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store packed maximum values in 'dst'. 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMAXPS'. Intrinsic: '_mm512_max_round_ps'.
// Requires AVX512F.
func M512MaxRoundPs(a x86.M512, b x86.M512, sae int) x86.M512 {
	return x86.M512(m512MaxRoundPs([16]float32(a), [16]float32(b), sae))
}

func m512MaxRoundPs(a [16]float32, b [16]float32, sae int) [16]float32


// MaskMaxRoundSd: Compare the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', store the maximum value in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'src' when mask bit 0
// is not set), and copy the upper element from 'a' to the upper element of
// 'dst'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		IF k[0]
//			dst[63:0] := MAX(a[63:0], b[63:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMAXSD'. Intrinsic: '_mm_mask_max_round_sd'.
// Requires AVX512F.
func MaskMaxRoundSd(src x86.M128d, k x86.Mmask8, a x86.M128d, b x86.M128d, sae int) x86.M128d {
	return x86.M128d(maskMaxRoundSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), sae))
}

func maskMaxRoundSd(src [2]float64, k uint8, a [2]float64, b [2]float64, sae int) [2]float64


// MaskzMaxRoundSd: Compare the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', store the maximum value in the lower element of
// 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is not
// set), and copy the upper element from 'a' to the upper element of 'dst'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		IF k[0]
//			dst[63:0] := MAX(a[63:0], b[63:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMAXSD'. Intrinsic: '_mm_maskz_max_round_sd'.
// Requires AVX512F.
func MaskzMaxRoundSd(k x86.Mmask8, a x86.M128d, b x86.M128d, sae int) x86.M128d {
	return x86.M128d(maskzMaxRoundSd(uint8(k), [2]float64(a), [2]float64(b), sae))
}

func maskzMaxRoundSd(k uint8, a [2]float64, b [2]float64, sae int) [2]float64


// MaxRoundSd: Compare the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', store the maximum value in the lower element of
// 'dst', and copy the upper element from 'a' to the upper element of 'dst'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		dst[63:0] := MAX(a[63:0], b[63:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMAXSD'. Intrinsic: '_mm_max_round_sd'.
// Requires AVX512F.
func MaxRoundSd(a x86.M128d, b x86.M128d, sae int) x86.M128d {
	return x86.M128d(maxRoundSd([2]float64(a), [2]float64(b), sae))
}

func maxRoundSd(a [2]float64, b [2]float64, sae int) [2]float64


// MaskMaxRoundSs: Compare the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', store the maximum value in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'src' when mask bit 0
// is not set), and copy the upper element from 'a' to the upper element of
// 'dst'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		IF k[0]
//			dst[31:0] := MAX(a[31:0], b[31:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMAXSS'. Intrinsic: '_mm_mask_max_round_ss'.
// Requires AVX512F.
func MaskMaxRoundSs(src x86.M128, k x86.Mmask8, a x86.M128, b x86.M128, sae int) x86.M128 {
	return x86.M128(maskMaxRoundSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), sae))
}

func maskMaxRoundSs(src [4]float32, k uint8, a [4]float32, b [4]float32, sae int) [4]float32


// MaskzMaxRoundSs: Compare the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', store the maximum value in the lower element of
// 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is not
// set), and copy the upper element from 'a' to the upper element of 'dst'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		IF k[0]
//			dst[31:0] := MAX(a[31:0], b[31:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMAXSS'. Intrinsic: '_mm_maskz_max_round_ss'.
// Requires AVX512F.
func MaskzMaxRoundSs(k x86.Mmask8, a x86.M128, b x86.M128, sae int) x86.M128 {
	return x86.M128(maskzMaxRoundSs(uint8(k), [4]float32(a), [4]float32(b), sae))
}

func maskzMaxRoundSs(k uint8, a [4]float32, b [4]float32, sae int) [4]float32


// MaxRoundSs: Compare the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', store the maximum value in the lower element of
// 'dst', and copy the upper element from 'a' to the upper element of 'dst'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		dst[31:0] := MAX(a[31:0], b[31:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMAXSS'. Intrinsic: '_mm_max_round_ss'.
// Requires AVX512F.
func MaxRoundSs(a x86.M128, b x86.M128, sae int) x86.M128 {
	return x86.M128(maxRoundSs([4]float32(a), [4]float32(b), sae))
}

func maxRoundSs(a [4]float32, b [4]float32, sae int) [4]float32


// MaskMaxSd: Compare the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', store the maximum value in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'src' when mask bit 0
// is not set), and copy the upper element from 'a' to the upper element of
// 'dst'. 
//
//		IF k[0]
//			dst[63:0] := MAX(a[63:0], b[63:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMAXSD'. Intrinsic: '_mm_mask_max_sd'.
// Requires AVX512F.
func MaskMaxSd(src x86.M128d, k x86.Mmask8, a x86.M128d, b x86.M128d) x86.M128d {
	return x86.M128d(maskMaxSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskMaxSd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzMaxSd: Compare the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', store the maximum value in the lower element of
// 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is not
// set), and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := MAX(a[63:0], b[63:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMAXSD'. Intrinsic: '_mm_maskz_max_sd'.
// Requires AVX512F.
func MaskzMaxSd(k x86.Mmask8, a x86.M128d, b x86.M128d) x86.M128d {
	return x86.M128d(maskzMaxSd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzMaxSd(k uint8, a [2]float64, b [2]float64) [2]float64


// MaskMaxSs: Compare the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', store the maximum value in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'src' when mask bit 0
// is not set), and copy the upper element from 'a' to the upper element of
// 'dst'. 
//
//		IF k[0]
//			dst[31:0] := MAX(a[31:0], b[31:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMAXSS'. Intrinsic: '_mm_mask_max_ss'.
// Requires AVX512F.
func MaskMaxSs(src x86.M128, k x86.Mmask8, a x86.M128, b x86.M128) x86.M128 {
	return x86.M128(maskMaxSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskMaxSs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzMaxSs: Compare the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', store the maximum value in the lower element of
// 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is not
// set), and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := MAX(a[31:0], b[31:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMAXSS'. Intrinsic: '_mm_maskz_max_ss'.
// Requires AVX512F.
func MaskzMaxSs(k x86.Mmask8, a x86.M128, b x86.M128) x86.M128 {
	return x86.M128(maskzMaxSs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzMaxSs(k uint8, a [4]float32, b [4]float32) [4]float32


// MaskMinEpi32: Compare packed 32-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] < b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMINSD'. Intrinsic: '_mm_mask_min_epi32'.
// Requires AVX512F.
func MaskMinEpi32(src x86.M128i, k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskMinEpi32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskMinEpi32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzMinEpi32: Compare packed 32-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] < b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMINSD'. Intrinsic: '_mm_maskz_min_epi32'.
// Requires AVX512F.
func MaskzMinEpi32(k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskzMinEpi32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzMinEpi32(k uint8, a [16]byte, b [16]byte) [16]byte


// M256MaskMinEpi32: Compare packed 32-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] < b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINSD'. Intrinsic: '_mm256_mask_min_epi32'.
// Requires AVX512F.
func M256MaskMinEpi32(src x86.M256i, k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskMinEpi32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskMinEpi32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// M256MaskzMinEpi32: Compare packed 32-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] < b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINSD'. Intrinsic: '_mm256_maskz_min_epi32'.
// Requires AVX512F.
func M256MaskzMinEpi32(k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzMinEpi32(uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskzMinEpi32(k uint8, a [32]byte, b [32]byte) [32]byte


// M512MaskzMinEpi32: Compare packed 32-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] < b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMINSD'. Intrinsic: '_mm512_maskz_min_epi32'.
// Requires AVX512F.
func M512MaskzMinEpi32(k x86.Mmask16, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzMinEpi32(uint16(k), [64]byte(a), [64]byte(b)))
}

func m512MaskzMinEpi32(k uint16, a [64]byte, b [64]byte) [64]byte


// MaskMinEpi64: Compare packed 64-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] < b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMINSQ'. Intrinsic: '_mm_mask_min_epi64'.
// Requires AVX512F.
func MaskMinEpi64(src x86.M128i, k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskMinEpi64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskMinEpi64(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzMinEpi64: Compare packed 64-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] < b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMINSQ'. Intrinsic: '_mm_maskz_min_epi64'.
// Requires AVX512F.
func MaskzMinEpi64(k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskzMinEpi64(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzMinEpi64(k uint8, a [16]byte, b [16]byte) [16]byte


// MinEpi64: Compare packed 64-bit integers in 'a' and 'b', and store packed
// minimum values in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF a[i+63:i] < b[i+63:i]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := b[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMINSQ'. Intrinsic: '_mm_min_epi64'.
// Requires AVX512F.
func MinEpi64(a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(minEpi64([16]byte(a), [16]byte(b)))
}

func minEpi64(a [16]byte, b [16]byte) [16]byte


// M256MaskMinEpi64: Compare packed 64-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] < b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINSQ'. Intrinsic: '_mm256_mask_min_epi64'.
// Requires AVX512F.
func M256MaskMinEpi64(src x86.M256i, k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskMinEpi64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskMinEpi64(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// M256MaskzMinEpi64: Compare packed 64-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] < b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINSQ'. Intrinsic: '_mm256_maskz_min_epi64'.
// Requires AVX512F.
func M256MaskzMinEpi64(k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzMinEpi64(uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskzMinEpi64(k uint8, a [32]byte, b [32]byte) [32]byte


// M256MinEpi64: Compare packed 64-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF a[i+63:i] < b[i+63:i]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := b[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINSQ'. Intrinsic: '_mm256_min_epi64'.
// Requires AVX512F.
func M256MinEpi64(a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MinEpi64([32]byte(a), [32]byte(b)))
}

func m256MinEpi64(a [32]byte, b [32]byte) [32]byte


// M512MaskMinEpi64: Compare packed 64-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] < b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMINSQ'. Intrinsic: '_mm512_mask_min_epi64'.
// Requires AVX512F.
func M512MaskMinEpi64(src x86.M512i, k x86.Mmask8, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskMinEpi64([64]byte(src), uint8(k), [64]byte(a), [64]byte(b)))
}

func m512MaskMinEpi64(src [64]byte, k uint8, a [64]byte, b [64]byte) [64]byte


// M512MaskzMinEpi64: Compare packed 64-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] < b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMINSQ'. Intrinsic: '_mm512_maskz_min_epi64'.
// Requires AVX512F.
func M512MaskzMinEpi64(k x86.Mmask8, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzMinEpi64(uint8(k), [64]byte(a), [64]byte(b)))
}

func m512MaskzMinEpi64(k uint8, a [64]byte, b [64]byte) [64]byte


// M512MinEpi64: Compare packed 64-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF a[i+63:i] < b[i+63:i]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := b[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMINSQ'. Intrinsic: '_mm512_min_epi64'.
// Requires AVX512F.
func M512MinEpi64(a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MinEpi64([64]byte(a), [64]byte(b)))
}

func m512MinEpi64(a [64]byte, b [64]byte) [64]byte


// MaskMinEpu32: Compare packed unsigned 32-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] < b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMINUD'. Intrinsic: '_mm_mask_min_epu32'.
// Requires AVX512F.
func MaskMinEpu32(src x86.M128i, k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskMinEpu32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskMinEpu32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzMinEpu32: Compare packed unsigned 32-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] < b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMINUD'. Intrinsic: '_mm_maskz_min_epu32'.
// Requires AVX512F.
func MaskzMinEpu32(k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskzMinEpu32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzMinEpu32(k uint8, a [16]byte, b [16]byte) [16]byte


// M256MaskMinEpu32: Compare packed unsigned 32-bit integers in 'a' and 'b',
// and store packed minimum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] < b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINUD'. Intrinsic: '_mm256_mask_min_epu32'.
// Requires AVX512F.
func M256MaskMinEpu32(src x86.M256i, k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskMinEpu32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskMinEpu32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// M256MaskzMinEpu32: Compare packed unsigned 32-bit integers in 'a' and 'b',
// and store packed minimum values in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] < b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINUD'. Intrinsic: '_mm256_maskz_min_epu32'.
// Requires AVX512F.
func M256MaskzMinEpu32(k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzMinEpu32(uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskzMinEpu32(k uint8, a [32]byte, b [32]byte) [32]byte


// M512MaskzMinEpu32: Compare packed unsigned 32-bit integers in 'a' and 'b',
// and store packed minimum values in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] < b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMINUD'. Intrinsic: '_mm512_maskz_min_epu32'.
// Requires AVX512F.
func M512MaskzMinEpu32(k x86.Mmask16, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzMinEpu32(uint16(k), [64]byte(a), [64]byte(b)))
}

func m512MaskzMinEpu32(k uint16, a [64]byte, b [64]byte) [64]byte


// MaskMinEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] < b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMINUQ'. Intrinsic: '_mm_mask_min_epu64'.
// Requires AVX512F.
func MaskMinEpu64(src x86.M128i, k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskMinEpu64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskMinEpu64(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzMinEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] < b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMINUQ'. Intrinsic: '_mm_maskz_min_epu64'.
// Requires AVX512F.
func MaskzMinEpu64(k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskzMinEpu64(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzMinEpu64(k uint8, a [16]byte, b [16]byte) [16]byte


// MinEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF a[i+63:i] < b[i+63:i]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := b[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMINUQ'. Intrinsic: '_mm_min_epu64'.
// Requires AVX512F.
func MinEpu64(a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(minEpu64([16]byte(a), [16]byte(b)))
}

func minEpu64(a [16]byte, b [16]byte) [16]byte


// M256MaskMinEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b',
// and store packed minimum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] < b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINUQ'. Intrinsic: '_mm256_mask_min_epu64'.
// Requires AVX512F.
func M256MaskMinEpu64(src x86.M256i, k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskMinEpu64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskMinEpu64(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// M256MaskzMinEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b',
// and store packed minimum values in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] < b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINUQ'. Intrinsic: '_mm256_maskz_min_epu64'.
// Requires AVX512F.
func M256MaskzMinEpu64(k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzMinEpu64(uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskzMinEpu64(k uint8, a [32]byte, b [32]byte) [32]byte


// M256MinEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF a[i+63:i] < b[i+63:i]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := b[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINUQ'. Intrinsic: '_mm256_min_epu64'.
// Requires AVX512F.
func M256MinEpu64(a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MinEpu64([32]byte(a), [32]byte(b)))
}

func m256MinEpu64(a [32]byte, b [32]byte) [32]byte


// M512MaskMinEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b',
// and store packed minimum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] < b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMINUQ'. Intrinsic: '_mm512_mask_min_epu64'.
// Requires AVX512F.
func M512MaskMinEpu64(src x86.M512i, k x86.Mmask8, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskMinEpu64([64]byte(src), uint8(k), [64]byte(a), [64]byte(b)))
}

func m512MaskMinEpu64(src [64]byte, k uint8, a [64]byte, b [64]byte) [64]byte


// M512MaskzMinEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b',
// and store packed minimum values in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] < b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMINUQ'. Intrinsic: '_mm512_maskz_min_epu64'.
// Requires AVX512F.
func M512MaskzMinEpu64(k x86.Mmask8, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzMinEpu64(uint8(k), [64]byte(a), [64]byte(b)))
}

func m512MaskzMinEpu64(k uint8, a [64]byte, b [64]byte) [64]byte


// M512MinEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF a[i+63:i] < b[i+63:i]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := b[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMINUQ'. Intrinsic: '_mm512_min_epu64'.
// Requires AVX512F.
func M512MinEpu64(a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MinEpu64([64]byte(a), [64]byte(b)))
}

func m512MinEpu64(a [64]byte, b [64]byte) [64]byte


// MaskMinPd: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', and store packed minimum values in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMINPD'. Intrinsic: '_mm_mask_min_pd'.
// Requires AVX512F.
func MaskMinPd(src x86.M128d, k x86.Mmask8, a x86.M128d, b x86.M128d) x86.M128d {
	return x86.M128d(maskMinPd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskMinPd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzMinPd: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', and store packed minimum values in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMINPD'. Intrinsic: '_mm_maskz_min_pd'.
// Requires AVX512F.
func MaskzMinPd(k x86.Mmask8, a x86.M128d, b x86.M128d) x86.M128d {
	return x86.M128d(maskzMinPd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzMinPd(k uint8, a [2]float64, b [2]float64) [2]float64


// M256MaskMinPd: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store packed minimum values in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMINPD'. Intrinsic: '_mm256_mask_min_pd'.
// Requires AVX512F.
func M256MaskMinPd(src x86.M256d, k x86.Mmask8, a x86.M256d, b x86.M256d) x86.M256d {
	return x86.M256d(m256MaskMinPd([4]float64(src), uint8(k), [4]float64(a), [4]float64(b)))
}

func m256MaskMinPd(src [4]float64, k uint8, a [4]float64, b [4]float64) [4]float64


// M256MaskzMinPd: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store packed minimum values in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMINPD'. Intrinsic: '_mm256_maskz_min_pd'.
// Requires AVX512F.
func M256MaskzMinPd(k x86.Mmask8, a x86.M256d, b x86.M256d) x86.M256d {
	return x86.M256d(m256MaskzMinPd(uint8(k), [4]float64(a), [4]float64(b)))
}

func m256MaskzMinPd(k uint8, a [4]float64, b [4]float64) [4]float64


// M512MaskMinPd: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store packed minimum values in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMINPD'. Intrinsic: '_mm512_mask_min_pd'.
// Requires AVX512F.
func M512MaskMinPd(src x86.M512d, k x86.Mmask8, a x86.M512d, b x86.M512d) x86.M512d {
	return x86.M512d(m512MaskMinPd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b)))
}

func m512MaskMinPd(src [8]float64, k uint8, a [8]float64, b [8]float64) [8]float64


// M512MaskzMinPd: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store packed minimum values in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMINPD'. Intrinsic: '_mm512_maskz_min_pd'.
// Requires AVX512F.
func M512MaskzMinPd(k x86.Mmask8, a x86.M512d, b x86.M512d) x86.M512d {
	return x86.M512d(m512MaskzMinPd(uint8(k), [8]float64(a), [8]float64(b)))
}

func m512MaskzMinPd(k uint8, a [8]float64, b [8]float64) [8]float64


// M512MinPd: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', and store packed minimum values in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMINPD'. Intrinsic: '_mm512_min_pd'.
// Requires AVX512F.
func M512MinPd(a x86.M512d, b x86.M512d) x86.M512d {
	return x86.M512d(m512MinPd([8]float64(a), [8]float64(b)))
}

func m512MinPd(a [8]float64, b [8]float64) [8]float64


// MaskMinPs: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', and store packed minimum values in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMINPS'. Intrinsic: '_mm_mask_min_ps'.
// Requires AVX512F.
func MaskMinPs(src x86.M128, k x86.Mmask8, a x86.M128, b x86.M128) x86.M128 {
	return x86.M128(maskMinPs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskMinPs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzMinPs: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', and store packed minimum values in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMINPS'. Intrinsic: '_mm_maskz_min_ps'.
// Requires AVX512F.
func MaskzMinPs(k x86.Mmask8, a x86.M128, b x86.M128) x86.M128 {
	return x86.M128(maskzMinPs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzMinPs(k uint8, a [4]float32, b [4]float32) [4]float32


// M256MaskMinPs: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store packed minimum values in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMINPS'. Intrinsic: '_mm256_mask_min_ps'.
// Requires AVX512F.
func M256MaskMinPs(src x86.M256, k x86.Mmask8, a x86.M256, b x86.M256) x86.M256 {
	return x86.M256(m256MaskMinPs([8]float32(src), uint8(k), [8]float32(a), [8]float32(b)))
}

func m256MaskMinPs(src [8]float32, k uint8, a [8]float32, b [8]float32) [8]float32


// M256MaskzMinPs: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store packed minimum values in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMINPS'. Intrinsic: '_mm256_maskz_min_ps'.
// Requires AVX512F.
func M256MaskzMinPs(k x86.Mmask8, a x86.M256, b x86.M256) x86.M256 {
	return x86.M256(m256MaskzMinPs(uint8(k), [8]float32(a), [8]float32(b)))
}

func m256MaskzMinPs(k uint8, a [8]float32, b [8]float32) [8]float32


// M512MaskMinPs: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store packed minimum values in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMINPS'. Intrinsic: '_mm512_mask_min_ps'.
// Requires AVX512F.
func M512MaskMinPs(src x86.M512, k x86.Mmask16, a x86.M512, b x86.M512) x86.M512 {
	return x86.M512(m512MaskMinPs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b)))
}

func m512MaskMinPs(src [16]float32, k uint16, a [16]float32, b [16]float32) [16]float32


// M512MaskzMinPs: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store packed minimum values in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMINPS'. Intrinsic: '_mm512_maskz_min_ps'.
// Requires AVX512F.
func M512MaskzMinPs(k x86.Mmask16, a x86.M512, b x86.M512) x86.M512 {
	return x86.M512(m512MaskzMinPs(uint16(k), [16]float32(a), [16]float32(b)))
}

func m512MaskzMinPs(k uint16, a [16]float32, b [16]float32) [16]float32


// M512MinPs: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', and store packed minimum values in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMINPS'. Intrinsic: '_mm512_min_ps'.
// Requires AVX512F.
func M512MinPs(a x86.M512, b x86.M512) x86.M512 {
	return x86.M512(m512MinPs([16]float32(a), [16]float32(b)))
}

func m512MinPs(a [16]float32, b [16]float32) [16]float32


// M512MaskMinRoundPd: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store packed minimum values in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set).  
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMINPD'. Intrinsic: '_mm512_mask_min_round_pd'.
// Requires AVX512F.
func M512MaskMinRoundPd(src x86.M512d, k x86.Mmask8, a x86.M512d, b x86.M512d, sae int) x86.M512d {
	return x86.M512d(m512MaskMinRoundPd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b), sae))
}

func m512MaskMinRoundPd(src [8]float64, k uint8, a [8]float64, b [8]float64, sae int) [8]float64


// M512MaskzMinRoundPd: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store packed minimum values in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMINPD'. Intrinsic: '_mm512_maskz_min_round_pd'.
// Requires AVX512F.
func M512MaskzMinRoundPd(k x86.Mmask8, a x86.M512d, b x86.M512d, sae int) x86.M512d {
	return x86.M512d(m512MaskzMinRoundPd(uint8(k), [8]float64(a), [8]float64(b), sae))
}

func m512MaskzMinRoundPd(k uint8, a [8]float64, b [8]float64, sae int) [8]float64


// M512MinRoundPd: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store packed minimum values in 'dst'. 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMINPD'. Intrinsic: '_mm512_min_round_pd'.
// Requires AVX512F.
func M512MinRoundPd(a x86.M512d, b x86.M512d, sae int) x86.M512d {
	return x86.M512d(m512MinRoundPd([8]float64(a), [8]float64(b), sae))
}

func m512MinRoundPd(a [8]float64, b [8]float64, sae int) [8]float64


// M512MaskMinRoundPs: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store packed minimum values in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set).  
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMINPS'. Intrinsic: '_mm512_mask_min_round_ps'.
// Requires AVX512F.
func M512MaskMinRoundPs(src x86.M512, k x86.Mmask16, a x86.M512, b x86.M512, sae int) x86.M512 {
	return x86.M512(m512MaskMinRoundPs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b), sae))
}

func m512MaskMinRoundPs(src [16]float32, k uint16, a [16]float32, b [16]float32, sae int) [16]float32


// M512MaskzMinRoundPs: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store packed minimum values in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMINPS'. Intrinsic: '_mm512_maskz_min_round_ps'.
// Requires AVX512F.
func M512MaskzMinRoundPs(k x86.Mmask16, a x86.M512, b x86.M512, sae int) x86.M512 {
	return x86.M512(m512MaskzMinRoundPs(uint16(k), [16]float32(a), [16]float32(b), sae))
}

func m512MaskzMinRoundPs(k uint16, a [16]float32, b [16]float32, sae int) [16]float32


// M512MinRoundPs: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store packed minimum values in 'dst'. 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMINPS'. Intrinsic: '_mm512_min_round_ps'.
// Requires AVX512F.
func M512MinRoundPs(a x86.M512, b x86.M512, sae int) x86.M512 {
	return x86.M512(m512MinRoundPs([16]float32(a), [16]float32(b), sae))
}

func m512MinRoundPs(a [16]float32, b [16]float32, sae int) [16]float32


// MaskMinRoundSd: Compare the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', store the minimum value in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'src' when mask bit 0
// is not set), and copy the upper element from 'a' to the upper element of
// 'dst'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		IF k[0]
//			dst[63:0] := MIN(a[63:0], b[63:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMINSD'. Intrinsic: '_mm_mask_min_round_sd'.
// Requires AVX512F.
func MaskMinRoundSd(src x86.M128d, k x86.Mmask8, a x86.M128d, b x86.M128d, sae int) x86.M128d {
	return x86.M128d(maskMinRoundSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), sae))
}

func maskMinRoundSd(src [2]float64, k uint8, a [2]float64, b [2]float64, sae int) [2]float64


// MaskzMinRoundSd: Compare the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', store the minimum value in the lower element of
// 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is not
// set), and copy the upper element from 'a' to the upper element of 'dst'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		IF k[0]
//			dst[63:0] := MIN(a[63:0], b[63:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMINSD'. Intrinsic: '_mm_maskz_min_round_sd'.
// Requires AVX512F.
func MaskzMinRoundSd(k x86.Mmask8, a x86.M128d, b x86.M128d, sae int) x86.M128d {
	return x86.M128d(maskzMinRoundSd(uint8(k), [2]float64(a), [2]float64(b), sae))
}

func maskzMinRoundSd(k uint8, a [2]float64, b [2]float64, sae int) [2]float64


// MinRoundSd: Compare the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', store the minimum value in the lower element of
// 'dst' , and copy the upper element from 'a' to the upper element of 'dst'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		dst[63:0] := MIN(a[63:0], b[63:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMINSD'. Intrinsic: '_mm_min_round_sd'.
// Requires AVX512F.
func MinRoundSd(a x86.M128d, b x86.M128d, sae int) x86.M128d {
	return x86.M128d(minRoundSd([2]float64(a), [2]float64(b), sae))
}

func minRoundSd(a [2]float64, b [2]float64, sae int) [2]float64


// MaskMinRoundSs: Compare the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', store the minimum value in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'src' when mask bit 0
// is not set), and copy the upper element from 'a' to the upper element of
// 'dst'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		IF k[0]
//			dst[31:0] := MIN(a[31:0], b[31:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMINSS'. Intrinsic: '_mm_mask_min_round_ss'.
// Requires AVX512F.
func MaskMinRoundSs(src x86.M128, k x86.Mmask8, a x86.M128, b x86.M128, sae int) x86.M128 {
	return x86.M128(maskMinRoundSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), sae))
}

func maskMinRoundSs(src [4]float32, k uint8, a [4]float32, b [4]float32, sae int) [4]float32


// MaskzMinRoundSs: Compare the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', store the minimum value in the lower element of
// 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is not
// set), and copy the upper element from 'a' to the upper element of 'dst'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		IF k[0]
//			dst[31:0] := MIN(a[31:0], b[31:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMINSS'. Intrinsic: '_mm_maskz_min_round_ss'.
// Requires AVX512F.
func MaskzMinRoundSs(k x86.Mmask8, a x86.M128, b x86.M128, sae int) x86.M128 {
	return x86.M128(maskzMinRoundSs(uint8(k), [4]float32(a), [4]float32(b), sae))
}

func maskzMinRoundSs(k uint8, a [4]float32, b [4]float32, sae int) [4]float32


// MinRoundSs: Compare the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', store the minimum value in the lower element of
// 'dst', and copy the upper element from 'a' to the upper element of 'dst'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		dst[31:0] := MIN(a[31:0], b[31:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMINSS'. Intrinsic: '_mm_min_round_ss'.
// Requires AVX512F.
func MinRoundSs(a x86.M128, b x86.M128, sae int) x86.M128 {
	return x86.M128(minRoundSs([4]float32(a), [4]float32(b), sae))
}

func minRoundSs(a [4]float32, b [4]float32, sae int) [4]float32


// MaskMinSd: Compare the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', store the minimum value in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'src' when mask bit 0
// is not set), and copy the upper element from 'a' to the upper element of
// 'dst'. 
//
//		IF k[0]
//			dst[63:0] := MIN(a[63:0], b[63:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMINSD'. Intrinsic: '_mm_mask_min_sd'.
// Requires AVX512F.
func MaskMinSd(src x86.M128d, k x86.Mmask8, a x86.M128d, b x86.M128d) x86.M128d {
	return x86.M128d(maskMinSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskMinSd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzMinSd: Compare the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', store the minimum value in the lower element of
// 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is not
// set), and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := MIN(a[63:0], b[63:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMINSD'. Intrinsic: '_mm_maskz_min_sd'.
// Requires AVX512F.
func MaskzMinSd(k x86.Mmask8, a x86.M128d, b x86.M128d) x86.M128d {
	return x86.M128d(maskzMinSd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzMinSd(k uint8, a [2]float64, b [2]float64) [2]float64


// MaskMinSs: Compare the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', store the minimum value in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'src' when mask bit 0
// is not set), and copy the upper element from 'a' to the upper element of
// 'dst'. 
//
//		IF k[0]
//			dst[31:0] := MIN(a[31:0], b[31:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMINSS'. Intrinsic: '_mm_mask_min_ss'.
// Requires AVX512F.
func MaskMinSs(src x86.M128, k x86.Mmask8, a x86.M128, b x86.M128) x86.M128 {
	return x86.M128(maskMinSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskMinSs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzMinSs: Compare the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', store the minimum value in the lower element of
// 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is not
// set), and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := MIN(a[31:0], b[31:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMINSS'. Intrinsic: '_mm_maskz_min_ss'.
// Requires AVX512F.
func MaskzMinSs(k x86.Mmask8, a x86.M128, b x86.M128) x86.M128 {
	return x86.M128(maskzMinSs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzMinSs(k uint8, a [4]float32, b [4]float32) [4]float32


// MaskMovEpi32: Move packed 32-bit integers from 'a' to 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVDQA32'. Intrinsic: '_mm_mask_mov_epi32'.
// Requires AVX512F.
func MaskMovEpi32(src x86.M128i, k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskMovEpi32([16]byte(src), uint8(k), [16]byte(a)))
}

func maskMovEpi32(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzMovEpi32: Move packed 32-bit integers from 'a' into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVDQA32'. Intrinsic: '_mm_maskz_mov_epi32'.
// Requires AVX512F.
func MaskzMovEpi32(k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskzMovEpi32(uint8(k), [16]byte(a)))
}

func maskzMovEpi32(k uint8, a [16]byte) [16]byte


// M256MaskMovEpi32: Move packed 32-bit integers from 'a' to 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQA32'. Intrinsic: '_mm256_mask_mov_epi32'.
// Requires AVX512F.
func M256MaskMovEpi32(src x86.M256i, k x86.Mmask8, a x86.M256i) x86.M256i {
	return x86.M256i(m256MaskMovEpi32([32]byte(src), uint8(k), [32]byte(a)))
}

func m256MaskMovEpi32(src [32]byte, k uint8, a [32]byte) [32]byte


// M256MaskzMovEpi32: Move packed 32-bit integers from 'a' into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQA32'. Intrinsic: '_mm256_maskz_mov_epi32'.
// Requires AVX512F.
func M256MaskzMovEpi32(k x86.Mmask8, a x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzMovEpi32(uint8(k), [32]byte(a)))
}

func m256MaskzMovEpi32(k uint8, a [32]byte) [32]byte


// M512MaskzMovEpi32: Move packed 32-bit integers from 'a' into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDQA32'. Intrinsic: '_mm512_maskz_mov_epi32'.
// Requires AVX512F.
func M512MaskzMovEpi32(k x86.Mmask16, a x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzMovEpi32(uint16(k), [64]byte(a)))
}

func m512MaskzMovEpi32(k uint16, a [64]byte) [64]byte


// MaskMovEpi64: Move packed 64-bit integers from 'a' to 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVDQA64'. Intrinsic: '_mm_mask_mov_epi64'.
// Requires AVX512F.
func MaskMovEpi64(src x86.M128i, k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskMovEpi64([16]byte(src), uint8(k), [16]byte(a)))
}

func maskMovEpi64(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzMovEpi64: Move packed 64-bit integers from 'a' into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVDQA64'. Intrinsic: '_mm_maskz_mov_epi64'.
// Requires AVX512F.
func MaskzMovEpi64(k x86.Mmask8, a x86.M128i) x86.M128i {
	return x86.M128i(maskzMovEpi64(uint8(k), [16]byte(a)))
}

func maskzMovEpi64(k uint8, a [16]byte) [16]byte


// M256MaskMovEpi64: Move packed 64-bit integers from 'a' to 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQA64'. Intrinsic: '_mm256_mask_mov_epi64'.
// Requires AVX512F.
func M256MaskMovEpi64(src x86.M256i, k x86.Mmask8, a x86.M256i) x86.M256i {
	return x86.M256i(m256MaskMovEpi64([32]byte(src), uint8(k), [32]byte(a)))
}

func m256MaskMovEpi64(src [32]byte, k uint8, a [32]byte) [32]byte


// M256MaskzMovEpi64: Move packed 64-bit integers from 'a' into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQA64'. Intrinsic: '_mm256_maskz_mov_epi64'.
// Requires AVX512F.
func M256MaskzMovEpi64(k x86.Mmask8, a x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzMovEpi64(uint8(k), [32]byte(a)))
}

func m256MaskzMovEpi64(k uint8, a [32]byte) [32]byte


// M512MaskzMovEpi64: Move packed 64-bit integers from 'a' into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDQA64'. Intrinsic: '_mm512_maskz_mov_epi64'.
// Requires AVX512F.
func M512MaskzMovEpi64(k x86.Mmask8, a x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzMovEpi64(uint8(k), [64]byte(a)))
}

func m512MaskzMovEpi64(k uint8, a [64]byte) [64]byte


// MaskMovPd: Move packed double-precision (64-bit) floating-point elements
// from 'a' to 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVAPD'. Intrinsic: '_mm_mask_mov_pd'.
// Requires AVX512F.
func MaskMovPd(src x86.M128d, k x86.Mmask8, a x86.M128d) x86.M128d {
	return x86.M128d(maskMovPd([2]float64(src), uint8(k), [2]float64(a)))
}

func maskMovPd(src [2]float64, k uint8, a [2]float64) [2]float64


// MaskzMovPd: Move packed double-precision (64-bit) floating-point elements
// from 'a' into 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVAPD'. Intrinsic: '_mm_maskz_mov_pd'.
// Requires AVX512F.
func MaskzMovPd(k x86.Mmask8, a x86.M128d) x86.M128d {
	return x86.M128d(maskzMovPd(uint8(k), [2]float64(a)))
}

func maskzMovPd(k uint8, a [2]float64) [2]float64


// M256MaskMovPd: Move packed double-precision (64-bit) floating-point elements
// from 'a' to 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVAPD'. Intrinsic: '_mm256_mask_mov_pd'.
// Requires AVX512F.
func M256MaskMovPd(src x86.M256d, k x86.Mmask8, a x86.M256d) x86.M256d {
	return x86.M256d(m256MaskMovPd([4]float64(src), uint8(k), [4]float64(a)))
}

func m256MaskMovPd(src [4]float64, k uint8, a [4]float64) [4]float64


// M256MaskzMovPd: Move packed double-precision (64-bit) floating-point
// elements from 'a' into 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVAPD'. Intrinsic: '_mm256_maskz_mov_pd'.
// Requires AVX512F.
func M256MaskzMovPd(k x86.Mmask8, a x86.M256d) x86.M256d {
	return x86.M256d(m256MaskzMovPd(uint8(k), [4]float64(a)))
}

func m256MaskzMovPd(k uint8, a [4]float64) [4]float64


// M512MaskzMovPd: Move packed double-precision (64-bit) floating-point
// elements from 'a' into 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVAPD'. Intrinsic: '_mm512_maskz_mov_pd'.
// Requires AVX512F.
func M512MaskzMovPd(k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskzMovPd(uint8(k), [8]float64(a)))
}

func m512MaskzMovPd(k uint8, a [8]float64) [8]float64


// MaskMovPs: Move packed single-precision (32-bit) floating-point elements
// from 'a' to 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVAPS'. Intrinsic: '_mm_mask_mov_ps'.
// Requires AVX512F.
func MaskMovPs(src x86.M128, k x86.Mmask8, a x86.M128) x86.M128 {
	return x86.M128(maskMovPs([4]float32(src), uint8(k), [4]float32(a)))
}

func maskMovPs(src [4]float32, k uint8, a [4]float32) [4]float32


// MaskzMovPs: Move packed single-precision (32-bit) floating-point elements
// from 'a' into 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVAPS'. Intrinsic: '_mm_maskz_mov_ps'.
// Requires AVX512F.
func MaskzMovPs(k x86.Mmask8, a x86.M128) x86.M128 {
	return x86.M128(maskzMovPs(uint8(k), [4]float32(a)))
}

func maskzMovPs(k uint8, a [4]float32) [4]float32


// M256MaskMovPs: Move packed single-precision (32-bit) floating-point elements
// from 'a' to 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVAPS'. Intrinsic: '_mm256_mask_mov_ps'.
// Requires AVX512F.
func M256MaskMovPs(src x86.M256, k x86.Mmask8, a x86.M256) x86.M256 {
	return x86.M256(m256MaskMovPs([8]float32(src), uint8(k), [8]float32(a)))
}

func m256MaskMovPs(src [8]float32, k uint8, a [8]float32) [8]float32


// M256MaskzMovPs: Move packed single-precision (32-bit) floating-point
// elements from 'a' into 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVAPS'. Intrinsic: '_mm256_maskz_mov_ps'.
// Requires AVX512F.
func M256MaskzMovPs(k x86.Mmask8, a x86.M256) x86.M256 {
	return x86.M256(m256MaskzMovPs(uint8(k), [8]float32(a)))
}

func m256MaskzMovPs(k uint8, a [8]float32) [8]float32


// M512MaskzMovPs: Move packed single-precision (32-bit) floating-point
// elements from 'a' into 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVAPS'. Intrinsic: '_mm512_maskz_mov_ps'.
// Requires AVX512F.
func M512MaskzMovPs(k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskzMovPs(uint16(k), [16]float32(a)))
}

func m512MaskzMovPs(k uint16, a [16]float32) [16]float32


// MaskMoveSd: Move the lower double-precision (64-bit) floating-point element
// from 'b' to the lower element of 'dst' using writemask 'k' (the element is
// copied from 'src' when mask bit 0 is not set), and copy the upper element
// from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := b[63:0]
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVSD'. Intrinsic: '_mm_mask_move_sd'.
// Requires AVX512F.
func MaskMoveSd(src x86.M128d, k x86.Mmask8, a x86.M128d, b x86.M128d) x86.M128d {
	return x86.M128d(maskMoveSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskMoveSd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzMoveSd: Move the lower double-precision (64-bit) floating-point element
// from 'b' to the lower element of 'dst' using zeromask 'k' (the element is
// zeroed out when mask bit 0 is not set), and copy the upper element from 'a'
// to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := b[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVSD'. Intrinsic: '_mm_maskz_move_sd'.
// Requires AVX512F.
func MaskzMoveSd(k x86.Mmask8, a x86.M128d, b x86.M128d) x86.M128d {
	return x86.M128d(maskzMoveSd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzMoveSd(k uint8, a [2]float64, b [2]float64) [2]float64


// MaskMoveSs: Move the lower single-precision (32-bit) floating-point element
// from 'b' to the lower element of 'dst' using writemask 'k' (the element is
// copied from 'src' when mask bit 0 is not set), and copy the upper 3 packed
// elements from 'a' to the upper elements of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := b[31:0]
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVSS'. Intrinsic: '_mm_mask_move_ss'.
// Requires AVX512F.
func MaskMoveSs(src x86.M128, k x86.Mmask8, a x86.M128, b x86.M128) x86.M128 {
	return x86.M128(maskMoveSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskMoveSs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzMoveSs: Move the lower single-precision (32-bit) floating-point element
// from 'b' to the lower element of 'dst' using zeromask 'k' (the element is
// zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements
// from 'a' to the upper elements of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := b[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVSS'. Intrinsic: '_mm_maskz_move_ss'.
// Requires AVX512F.
func MaskzMoveSs(k x86.Mmask8, a x86.M128, b x86.M128) x86.M128 {
	return x86.M128(maskzMoveSs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzMoveSs(k uint8, a [4]float32, b [4]float32) [4]float32


// MaskMovedupPd: Duplicate even-indexed double-precision (64-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp[63:0] := a[63:0]
//		tmp[127:64] := a[63:0]
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVDDUP'. Intrinsic: '_mm_mask_movedup_pd'.
// Requires AVX512F.
func MaskMovedupPd(src x86.M128d, k x86.Mmask8, a x86.M128d) x86.M128d {
	return x86.M128d(maskMovedupPd([2]float64(src), uint8(k), [2]float64(a)))
}

func maskMovedupPd(src [2]float64, k uint8, a [2]float64) [2]float64


// MaskzMovedupPd: Duplicate even-indexed double-precision (64-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp[63:0] := a[63:0]
//		tmp[127:64] := a[63:0]
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVDDUP'. Intrinsic: '_mm_maskz_movedup_pd'.
// Requires AVX512F.
func MaskzMovedupPd(k x86.Mmask8, a x86.M128d) x86.M128d {
	return x86.M128d(maskzMovedupPd(uint8(k), [2]float64(a)))
}

func maskzMovedupPd(k uint8, a [2]float64) [2]float64


// M256MaskMovedupPd: Duplicate even-indexed double-precision (64-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp[63:0] := a[63:0]
//		tmp[127:64] := a[63:0]
//		tmp[191:128] := a[191:128]
//		tmp[255:192] := a[191:128]
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDDUP'. Intrinsic: '_mm256_mask_movedup_pd'.
// Requires AVX512F.
func M256MaskMovedupPd(src x86.M256d, k x86.Mmask8, a x86.M256d) x86.M256d {
	return x86.M256d(m256MaskMovedupPd([4]float64(src), uint8(k), [4]float64(a)))
}

func m256MaskMovedupPd(src [4]float64, k uint8, a [4]float64) [4]float64


// M256MaskzMovedupPd: Duplicate even-indexed double-precision (64-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp[63:0] := a[63:0]
//		tmp[127:64] := a[63:0]
//		tmp[191:128] := a[191:128]
//		tmp[255:192] := a[191:128]
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDDUP'. Intrinsic: '_mm256_maskz_movedup_pd'.
// Requires AVX512F.
func M256MaskzMovedupPd(k x86.Mmask8, a x86.M256d) x86.M256d {
	return x86.M256d(m256MaskzMovedupPd(uint8(k), [4]float64(a)))
}

func m256MaskzMovedupPd(k uint8, a [4]float64) [4]float64


// M512MaskMovedupPd: Duplicate even-indexed double-precision (64-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp[63:0] := a[63:0]
//		tmp[127:64] := a[63:0]
//		tmp[191:128] := a[191:128]
//		tmp[255:192] := a[191:128]
//		tmp[319:256] := a[319:256] 
//		tmp[383:320] := a[319:256] 
//		tmp[447:384] := a[447:384]
//		tmp[511:448] := a[447:384]
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDDUP'. Intrinsic: '_mm512_mask_movedup_pd'.
// Requires AVX512F.
func M512MaskMovedupPd(src x86.M512d, k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskMovedupPd([8]float64(src), uint8(k), [8]float64(a)))
}

func m512MaskMovedupPd(src [8]float64, k uint8, a [8]float64) [8]float64


// M512MaskzMovedupPd: Duplicate even-indexed double-precision (64-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp[63:0] := a[63:0]
//		tmp[127:64] := a[63:0]
//		tmp[191:128] := a[191:128]
//		tmp[255:192] := a[191:128]
//		tmp[319:256] := a[319:256] 
//		tmp[383:320] := a[319:256] 
//		tmp[447:384] := a[447:384]
//		tmp[511:448] := a[447:384]
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDDUP'. Intrinsic: '_mm512_maskz_movedup_pd'.
// Requires AVX512F.
func M512MaskzMovedupPd(k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskzMovedupPd(uint8(k), [8]float64(a)))
}

func m512MaskzMovedupPd(k uint8, a [8]float64) [8]float64


// M512MovedupPd: Duplicate even-indexed double-precision (64-bit)
// floating-point elements from 'a', and store the results in 'dst'. 
//
//		tmp[63:0] := a[63:0]
//		tmp[127:64] := a[63:0]
//		tmp[191:128] := a[191:128]
//		tmp[255:192] := a[191:128]
//		tmp[319:256] := a[319:256] 
//		tmp[383:320] := a[319:256] 
//		tmp[447:384] := a[447:384]
//		tmp[511:448] := a[447:384]
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDDUP'. Intrinsic: '_mm512_movedup_pd'.
// Requires AVX512F.
func M512MovedupPd(a x86.M512d) x86.M512d {
	return x86.M512d(m512MovedupPd([8]float64(a)))
}

func m512MovedupPd(a [8]float64) [8]float64


// MaskMovehdupPs: Duplicate odd-indexed single-precision (32-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp[31:0] := a[63:32] 
//		tmp[63:32] := a[63:32] 
//		tmp[95:64] := a[127:96] 
//		tmp[127:96] := a[127:96]
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVSHDUP'. Intrinsic: '_mm_mask_movehdup_ps'.
// Requires AVX512F.
func MaskMovehdupPs(src x86.M128, k x86.Mmask8, a x86.M128) x86.M128 {
	return x86.M128(maskMovehdupPs([4]float32(src), uint8(k), [4]float32(a)))
}

func maskMovehdupPs(src [4]float32, k uint8, a [4]float32) [4]float32


// MaskzMovehdupPs: Duplicate odd-indexed single-precision (32-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp[31:0] := a[63:32] 
//		tmp[63:32] := a[63:32] 
//		tmp[95:64] := a[127:96] 
//		tmp[127:96] := a[127:96]
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVSHDUP'. Intrinsic: '_mm_maskz_movehdup_ps'.
// Requires AVX512F.
func MaskzMovehdupPs(k x86.Mmask8, a x86.M128) x86.M128 {
	return x86.M128(maskzMovehdupPs(uint8(k), [4]float32(a)))
}

func maskzMovehdupPs(k uint8, a [4]float32) [4]float32


// M256MaskMovehdupPs: Duplicate odd-indexed single-precision (32-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp[31:0] := a[63:32] 
//		tmp[63:32] := a[63:32] 
//		tmp[95:64] := a[127:96] 
//		tmp[127:96] := a[127:96]
//		tmp[159:128] := a[191:160] 
//		tmp[191:160] := a[191:160] 
//		tmp[223:192] := a[255:224] 
//		tmp[255:224] := a[255:224]
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVSHDUP'. Intrinsic: '_mm256_mask_movehdup_ps'.
// Requires AVX512F.
func M256MaskMovehdupPs(src x86.M256, k x86.Mmask8, a x86.M256) x86.M256 {
	return x86.M256(m256MaskMovehdupPs([8]float32(src), uint8(k), [8]float32(a)))
}

func m256MaskMovehdupPs(src [8]float32, k uint8, a [8]float32) [8]float32


// M256MaskzMovehdupPs: Duplicate odd-indexed single-precision (32-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp[31:0] := a[63:32] 
//		tmp[63:32] := a[63:32] 
//		tmp[95:64] := a[127:96] 
//		tmp[127:96] := a[127:96]
//		tmp[159:128] := a[191:160] 
//		tmp[191:160] := a[191:160] 
//		tmp[223:192] := a[255:224] 
//		tmp[255:224] := a[255:224]
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVSHDUP'. Intrinsic: '_mm256_maskz_movehdup_ps'.
// Requires AVX512F.
func M256MaskzMovehdupPs(k x86.Mmask8, a x86.M256) x86.M256 {
	return x86.M256(m256MaskzMovehdupPs(uint8(k), [8]float32(a)))
}

func m256MaskzMovehdupPs(k uint8, a [8]float32) [8]float32


// M512MaskMovehdupPs: Duplicate odd-indexed single-precision (32-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp[31:0] := a[63:32] 
//		tmp[63:32] := a[63:32] 
//		tmp[95:64] := a[127:96] 
//		tmp[127:96] := a[127:96]
//		tmp[159:128] := a[191:160] 
//		tmp[191:160] := a[191:160] 
//		tmp[223:192] := a[255:224] 
//		tmp[255:224] := a[255:224]
//		tmp[287:256] := a[319:288] 
//		tmp[319:288] := a[319:288] 
//		tmp[351:320] := a[383:352] 
//		tmp[383:352] := a[383:352] 
//		tmp[415:384] := a[447:416] 
//		tmp[447:416] := a[447:416] 
//		tmp[479:448] := a[511:480]
//		tmp[511:480] := a[511:480]
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVSHDUP'. Intrinsic: '_mm512_mask_movehdup_ps'.
// Requires AVX512F.
func M512MaskMovehdupPs(src x86.M512, k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskMovehdupPs([16]float32(src), uint16(k), [16]float32(a)))
}

func m512MaskMovehdupPs(src [16]float32, k uint16, a [16]float32) [16]float32


// M512MaskzMovehdupPs: Duplicate odd-indexed single-precision (32-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp[31:0] := a[63:32] 
//		tmp[63:32] := a[63:32] 
//		tmp[95:64] := a[127:96] 
//		tmp[127:96] := a[127:96]
//		tmp[159:128] := a[191:160] 
//		tmp[191:160] := a[191:160] 
//		tmp[223:192] := a[255:224] 
//		tmp[255:224] := a[255:224]
//		tmp[287:256] := a[319:288] 
//		tmp[319:288] := a[319:288] 
//		tmp[351:320] := a[383:352] 
//		tmp[383:352] := a[383:352] 
//		tmp[415:384] := a[447:416] 
//		tmp[447:416] := a[447:416] 
//		tmp[479:448] := a[511:480]
//		tmp[511:480] := a[511:480]
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVSHDUP'. Intrinsic: '_mm512_maskz_movehdup_ps'.
// Requires AVX512F.
func M512MaskzMovehdupPs(k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskzMovehdupPs(uint16(k), [16]float32(a)))
}

func m512MaskzMovehdupPs(k uint16, a [16]float32) [16]float32


// M512MovehdupPs: Duplicate odd-indexed single-precision (32-bit)
// floating-point elements from 'a', and store the results in 'dst'. 
//
//		dst[31:0] := a[63:32] 
//		dst[63:32] := a[63:32] 
//		dst[95:64] := a[127:96] 
//		dst[127:96] := a[127:96]
//		dst[159:128] := a[191:160] 
//		dst[191:160] := a[191:160] 
//		dst[223:192] := a[255:224] 
//		dst[255:224] := a[255:224]
//		dst[287:256] := a[319:288] 
//		dst[319:288] := a[319:288] 
//		dst[351:320] := a[383:352] 
//		dst[383:352] := a[383:352] 
//		dst[415:384] := a[447:416] 
//		dst[447:416] := a[447:416] 
//		dst[479:448] := a[511:480]
//		dst[511:480] := a[511:480]
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVSHDUP'. Intrinsic: '_mm512_movehdup_ps'.
// Requires AVX512F.
func M512MovehdupPs(a x86.M512) x86.M512 {
	return x86.M512(m512MovehdupPs([16]float32(a)))
}

func m512MovehdupPs(a [16]float32) [16]float32


// MaskMoveldupPs: Duplicate even-indexed single-precision (32-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp[31:0] := a[31:0] 
//		tmp[63:32] := a[31:0] 
//		tmp[95:64] := a[95:64] 
//		tmp[127:96] := a[95:64]
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR	
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVSLDUP'. Intrinsic: '_mm_mask_moveldup_ps'.
// Requires AVX512F.
func MaskMoveldupPs(src x86.M128, k x86.Mmask8, a x86.M128) x86.M128 {
	return x86.M128(maskMoveldupPs([4]float32(src), uint8(k), [4]float32(a)))
}

func maskMoveldupPs(src [4]float32, k uint8, a [4]float32) [4]float32


// MaskzMoveldupPs: Duplicate even-indexed single-precision (32-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp[31:0] := a[31:0] 
//		tmp[63:32] := a[31:0] 
//		tmp[95:64] := a[95:64] 
//		tmp[127:96] := a[95:64]
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVSLDUP'. Intrinsic: '_mm_maskz_moveldup_ps'.
// Requires AVX512F.
func MaskzMoveldupPs(k x86.Mmask8, a x86.M128) x86.M128 {
	return x86.M128(maskzMoveldupPs(uint8(k), [4]float32(a)))
}

func maskzMoveldupPs(k uint8, a [4]float32) [4]float32


// M256MaskMoveldupPs: Duplicate even-indexed single-precision (32-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp[31:0] := a[31:0] 
//		tmp[63:32] := a[31:0] 
//		tmp[95:64] := a[95:64] 
//		tmp[127:96] := a[95:64]
//		tmp[159:128] := a[159:128] 
//		tmp[191:160] := a[159:128] 
//		tmp[223:192] := a[223:192] 
//		tmp[255:224] := a[223:192]
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVSLDUP'. Intrinsic: '_mm256_mask_moveldup_ps'.
// Requires AVX512F.
func M256MaskMoveldupPs(src x86.M256, k x86.Mmask8, a x86.M256) x86.M256 {
	return x86.M256(m256MaskMoveldupPs([8]float32(src), uint8(k), [8]float32(a)))
}

func m256MaskMoveldupPs(src [8]float32, k uint8, a [8]float32) [8]float32


// M256MaskzMoveldupPs: Duplicate even-indexed single-precision (32-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp[31:0] := a[31:0] 
//		tmp[63:32] := a[31:0] 
//		tmp[95:64] := a[95:64] 
//		tmp[127:96] := a[95:64]
//		tmp[159:128] := a[159:128] 
//		tmp[191:160] := a[159:128] 
//		tmp[223:192] := a[223:192] 
//		tmp[255:224] := a[223:192]
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVSLDUP'. Intrinsic: '_mm256_maskz_moveldup_ps'.
// Requires AVX512F.
func M256MaskzMoveldupPs(k x86.Mmask8, a x86.M256) x86.M256 {
	return x86.M256(m256MaskzMoveldupPs(uint8(k), [8]float32(a)))
}

func m256MaskzMoveldupPs(k uint8, a [8]float32) [8]float32


// M512MaskMoveldupPs: Duplicate even-indexed single-precision (32-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp[31:0] := a[31:0] 
//		tmp[63:32] := a[31:0] 
//		tmp[95:64] := a[95:64] 
//		tmp[127:96] := a[95:64]
//		tmp[159:128] := a[159:128] 
//		tmp[191:160] := a[159:128] 
//		tmp[223:192] := a[223:192] 
//		tmp[255:224] := a[223:192]
//		tmp[287:256] := a[287:256] 
//		tmp[319:288] := a[287:256] 
//		tmp[351:320] := a[351:320] 
//		tmp[383:352] := a[351:320] 
//		tmp[415:384] := a[415:384] 
//		tmp[447:416] := a[415:384] 
//		tmp[479:448] := a[479:448]
//		tmp[511:480] := a[479:448]
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVSLDUP'. Intrinsic: '_mm512_mask_moveldup_ps'.
// Requires AVX512F.
func M512MaskMoveldupPs(src x86.M512, k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskMoveldupPs([16]float32(src), uint16(k), [16]float32(a)))
}

func m512MaskMoveldupPs(src [16]float32, k uint16, a [16]float32) [16]float32


// M512MaskzMoveldupPs: Duplicate even-indexed single-precision (32-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp[31:0] := a[31:0] 
//		tmp[63:32] := a[31:0] 
//		tmp[95:64] := a[95:64] 
//		tmp[127:96] := a[95:64]
//		tmp[159:128] := a[159:128] 
//		tmp[191:160] := a[159:128] 
//		tmp[223:192] := a[223:192] 
//		tmp[255:224] := a[223:192]
//		tmp[287:256] := a[287:256] 
//		tmp[319:288] := a[287:256] 
//		tmp[351:320] := a[351:320] 
//		tmp[383:352] := a[351:320] 
//		tmp[415:384] := a[415:384] 
//		tmp[447:416] := a[415:384] 
//		tmp[479:448] := a[479:448]
//		tmp[511:480] := a[479:448]
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVSLDUP'. Intrinsic: '_mm512_maskz_moveldup_ps'.
// Requires AVX512F.
func M512MaskzMoveldupPs(k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskzMoveldupPs(uint16(k), [16]float32(a)))
}

func m512MaskzMoveldupPs(k uint16, a [16]float32) [16]float32


// M512MoveldupPs: Duplicate even-indexed single-precision (32-bit)
// floating-point elements from 'a', and store the results in 'dst'. 
//
//		dst[31:0] := a[31:0] 
//		dst[63:32] := a[31:0] 
//		dst[95:64] := a[95:64] 
//		dst[127:96] := a[95:64]
//		dst[159:128] := a[159:128] 
//		dst[191:160] := a[159:128] 
//		dst[223:192] := a[223:192] 
//		dst[255:224] := a[223:192]
//		dst[287:256] := a[287:256] 
//		dst[319:288] := a[287:256] 
//		dst[351:320] := a[351:320] 
//		dst[383:352] := a[351:320] 
//		dst[415:384] := a[415:384] 
//		dst[447:416] := a[415:384] 
//		dst[479:448] := a[479:448]
//		dst[511:480] := a[479:448]
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVSLDUP'. Intrinsic: '_mm512_moveldup_ps'.
// Requires AVX512F.
func M512MoveldupPs(a x86.M512) x86.M512 {
	return x86.M512(m512MoveldupPs([16]float32(a)))
}

func m512MoveldupPs(a [16]float32) [16]float32


// MaskMulEpi32: Multiply the low 32-bit integers from each packed 64-bit
// element in 'a' and 'b', and store the signed 64-bit results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMULDQ'. Intrinsic: '_mm_mask_mul_epi32'.
// Requires AVX512F.
func MaskMulEpi32(src x86.M128i, k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskMulEpi32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskMulEpi32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzMulEpi32: Multiply the low 32-bit integers from each packed 64-bit
// element in 'a' and 'b', and store the signed 64-bit results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMULDQ'. Intrinsic: '_mm_maskz_mul_epi32'.
// Requires AVX512F.
func MaskzMulEpi32(k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskzMulEpi32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzMulEpi32(k uint8, a [16]byte, b [16]byte) [16]byte


// M256MaskMulEpi32: Multiply the low 32-bit integers from each packed 64-bit
// element in 'a' and 'b', and store the signed 64-bit results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULDQ'. Intrinsic: '_mm256_mask_mul_epi32'.
// Requires AVX512F.
func M256MaskMulEpi32(src x86.M256i, k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskMulEpi32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskMulEpi32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// M256MaskzMulEpi32: Multiply the low 32-bit integers from each packed 64-bit
// element in 'a' and 'b', and store the signed 64-bit results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULDQ'. Intrinsic: '_mm256_maskz_mul_epi32'.
// Requires AVX512F.
func M256MaskzMulEpi32(k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzMulEpi32(uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskzMulEpi32(k uint8, a [32]byte, b [32]byte) [32]byte


// M512MaskMulEpi32: Multiply the low 32-bit integers from each packed 64-bit
// element in 'a' and 'b', and store the signed 64-bit results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMULDQ'. Intrinsic: '_mm512_mask_mul_epi32'.
// Requires AVX512F.
func M512MaskMulEpi32(src x86.M512i, k x86.Mmask8, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskMulEpi32([64]byte(src), uint8(k), [64]byte(a), [64]byte(b)))
}

func m512MaskMulEpi32(src [64]byte, k uint8, a [64]byte, b [64]byte) [64]byte


// M512MaskzMulEpi32: Multiply the low 32-bit integers from each packed 64-bit
// element in 'a' and 'b', and store the signed 64-bit results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMULDQ'. Intrinsic: '_mm512_maskz_mul_epi32'.
// Requires AVX512F.
func M512MaskzMulEpi32(k x86.Mmask8, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzMulEpi32(uint8(k), [64]byte(a), [64]byte(b)))
}

func m512MaskzMulEpi32(k uint8, a [64]byte, b [64]byte) [64]byte


// M512MulEpi32: Multiply the low 32-bit integers from each packed 64-bit
// element in 'a' and 'b', and store the signed 64-bit results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := a[i+31:i] * b[i+31:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMULDQ'. Intrinsic: '_mm512_mul_epi32'.
// Requires AVX512F.
func M512MulEpi32(a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MulEpi32([64]byte(a), [64]byte(b)))
}

func m512MulEpi32(a [64]byte, b [64]byte) [64]byte


// MaskMulEpu32: Multiply the low unsigned 32-bit integers from each packed
// 64-bit element in 'a' and 'b', and store the unsigned 64-bit results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMULUDQ'. Intrinsic: '_mm_mask_mul_epu32'.
// Requires AVX512F.
func MaskMulEpu32(src x86.M128i, k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskMulEpu32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskMulEpu32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzMulEpu32: Multiply the low unsigned 32-bit integers from each packed
// 64-bit element in 'a' and 'b', and store the unsigned 64-bit results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMULUDQ'. Intrinsic: '_mm_maskz_mul_epu32'.
// Requires AVX512F.
func MaskzMulEpu32(k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskzMulEpu32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzMulEpu32(k uint8, a [16]byte, b [16]byte) [16]byte


// M256MaskMulEpu32: Multiply the low unsigned 32-bit integers from each packed
// 64-bit element in 'a' and 'b', and store the unsigned 64-bit results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULUDQ'. Intrinsic: '_mm256_mask_mul_epu32'.
// Requires AVX512F.
func M256MaskMulEpu32(src x86.M256i, k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskMulEpu32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskMulEpu32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// M256MaskzMulEpu32: Multiply the low unsigned 32-bit integers from each
// packed 64-bit element in 'a' and 'b', and store the unsigned 64-bit results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULUDQ'. Intrinsic: '_mm256_maskz_mul_epu32'.
// Requires AVX512F.
func M256MaskzMulEpu32(k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzMulEpu32(uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskzMulEpu32(k uint8, a [32]byte, b [32]byte) [32]byte


// M512MaskMulEpu32: Multiply the low unsigned 32-bit integers from each packed
// 64-bit element in 'a' and 'b', and store the unsigned 64-bit results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMULUDQ'. Intrinsic: '_mm512_mask_mul_epu32'.
// Requires AVX512F.
func M512MaskMulEpu32(src x86.M512i, k x86.Mmask8, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskMulEpu32([64]byte(src), uint8(k), [64]byte(a), [64]byte(b)))
}

func m512MaskMulEpu32(src [64]byte, k uint8, a [64]byte, b [64]byte) [64]byte


// M512MaskzMulEpu32: Multiply the low unsigned 32-bit integers from each
// packed 64-bit element in 'a' and 'b', and store the unsigned 64-bit results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMULUDQ'. Intrinsic: '_mm512_maskz_mul_epu32'.
// Requires AVX512F.
func M512MaskzMulEpu32(k x86.Mmask8, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzMulEpu32(uint8(k), [64]byte(a), [64]byte(b)))
}

func m512MaskzMulEpu32(k uint8, a [64]byte, b [64]byte) [64]byte


// M512MulEpu32: Multiply the low unsigned 32-bit integers from each packed
// 64-bit element in 'a' and 'b', and store the unsigned 64-bit results in
// 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := a[i+31:i] * b[i+31:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMULUDQ'. Intrinsic: '_mm512_mul_epu32'.
// Requires AVX512F.
func M512MulEpu32(a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MulEpu32([64]byte(a), [64]byte(b)))
}

func m512MulEpu32(a [64]byte, b [64]byte) [64]byte


// MaskMulPd: Multiply packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] * b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMULPD'. Intrinsic: '_mm_mask_mul_pd'.
// Requires AVX512F.
func MaskMulPd(src x86.M128d, k x86.Mmask8, a x86.M128d, b x86.M128d) x86.M128d {
	return x86.M128d(maskMulPd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskMulPd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzMulPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] * b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMULPD'. Intrinsic: '_mm_maskz_mul_pd'.
// Requires AVX512F.
func MaskzMulPd(k x86.Mmask8, a x86.M128d, b x86.M128d) x86.M128d {
	return x86.M128d(maskzMulPd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzMulPd(k uint8, a [2]float64, b [2]float64) [2]float64


// M256MaskMulPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] * b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMULPD'. Intrinsic: '_mm256_mask_mul_pd'.
// Requires AVX512F.
func M256MaskMulPd(src x86.M256d, k x86.Mmask8, a x86.M256d, b x86.M256d) x86.M256d {
	return x86.M256d(m256MaskMulPd([4]float64(src), uint8(k), [4]float64(a), [4]float64(b)))
}

func m256MaskMulPd(src [4]float64, k uint8, a [4]float64, b [4]float64) [4]float64


// M256MaskzMulPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] * b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMULPD'. Intrinsic: '_mm256_maskz_mul_pd'.
// Requires AVX512F.
func M256MaskzMulPd(k x86.Mmask8, a x86.M256d, b x86.M256d) x86.M256d {
	return x86.M256d(m256MaskzMulPd(uint8(k), [4]float64(a), [4]float64(b)))
}

func m256MaskzMulPd(k uint8, a [4]float64, b [4]float64) [4]float64


// M512MaskzMulPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] * b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMULPD'. Intrinsic: '_mm512_maskz_mul_pd'.
// Requires AVX512F.
func M512MaskzMulPd(k x86.Mmask8, a x86.M512d, b x86.M512d) x86.M512d {
	return x86.M512d(m512MaskzMulPd(uint8(k), [8]float64(a), [8]float64(b)))
}

func m512MaskzMulPd(k uint8, a [8]float64, b [8]float64) [8]float64


// MaskMulPs: Multiply packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set).  RM. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMULPS'. Intrinsic: '_mm_mask_mul_ps'.
// Requires AVX512F.
func MaskMulPs(src x86.M128, k x86.Mmask8, a x86.M128, b x86.M128) x86.M128 {
	return x86.M128(maskMulPs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskMulPs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzMulPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMULPS'. Intrinsic: '_mm_maskz_mul_ps'.
// Requires AVX512F.
func MaskzMulPs(k x86.Mmask8, a x86.M128, b x86.M128) x86.M128 {
	return x86.M128(maskzMulPs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzMulPs(k uint8, a [4]float32, b [4]float32) [4]float32


// M256MaskMulPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set).
// RM. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMULPS'. Intrinsic: '_mm256_mask_mul_ps'.
// Requires AVX512F.
func M256MaskMulPs(src x86.M256, k x86.Mmask8, a x86.M256, b x86.M256) x86.M256 {
	return x86.M256(m256MaskMulPs([8]float32(src), uint8(k), [8]float32(a), [8]float32(b)))
}

func m256MaskMulPs(src [8]float32, k uint8, a [8]float32, b [8]float32) [8]float32


// M256MaskzMulPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMULPS'. Intrinsic: '_mm256_maskz_mul_ps'.
// Requires AVX512F.
func M256MaskzMulPs(k x86.Mmask8, a x86.M256, b x86.M256) x86.M256 {
	return x86.M256(m256MaskzMulPs(uint8(k), [8]float32(a), [8]float32(b)))
}

func m256MaskzMulPs(k uint8, a [8]float32, b [8]float32) [8]float32


// M512MaskzMulPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMULPS'. Intrinsic: '_mm512_maskz_mul_ps'.
// Requires AVX512F.
func M512MaskzMulPs(k x86.Mmask16, a x86.M512, b x86.M512) x86.M512 {
	return x86.M512(m512MaskzMulPs(uint16(k), [16]float32(a), [16]float32(b)))
}

func m512MaskzMulPs(k uint16, a [16]float32, b [16]float32) [16]float32


// M512MaskzMulRoundPd: Multiply packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] * b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMULPD'. Intrinsic: '_mm512_maskz_mul_round_pd'.
// Requires AVX512F.
func M512MaskzMulRoundPd(k x86.Mmask8, a x86.M512d, b x86.M512d, rounding int) x86.M512d {
	return x86.M512d(m512MaskzMulRoundPd(uint8(k), [8]float64(a), [8]float64(b), rounding))
}

func m512MaskzMulRoundPd(k uint8, a [8]float64, b [8]float64, rounding int) [8]float64


// M512MaskzMulRoundPs: Multiply packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMULPS'. Intrinsic: '_mm512_maskz_mul_round_ps'.
// Requires AVX512F.
func M512MaskzMulRoundPs(k x86.Mmask16, a x86.M512, b x86.M512, rounding int) x86.M512 {
	return x86.M512(m512MaskzMulRoundPs(uint16(k), [16]float32(a), [16]float32(b), rounding))
}

func m512MaskzMulRoundPs(k uint16, a [16]float32, b [16]float32, rounding int) [16]float32


// MaskMulRoundSd: Multiply the lower double-precision (64-bit) floating-point
// element in 'a' and 'b', store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'src' when mask bit 0 is not set),
// and copy the upper element from 'a' to the upper element of 'dst'.
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := a[63:0] * b[63:0]
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMULSD'. Intrinsic: '_mm_mask_mul_round_sd'.
// Requires AVX512F.
func MaskMulRoundSd(src x86.M128d, k x86.Mmask8, a x86.M128d, b x86.M128d, rounding int) x86.M128d {
	return x86.M128d(maskMulRoundSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), rounding))
}

func maskMulRoundSd(src [2]float64, k uint8, a [2]float64, b [2]float64, rounding int) [2]float64


// MaskzMulRoundSd: Multiply the lower double-precision (64-bit) floating-point
// element in 'a' and 'b', store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper element from 'a' to the upper element of 'dst'.
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := a[63:0] * b[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMULSD'. Intrinsic: '_mm_maskz_mul_round_sd'.
// Requires AVX512F.
func MaskzMulRoundSd(k x86.Mmask8, a x86.M128d, b x86.M128d, rounding int) x86.M128d {
	return x86.M128d(maskzMulRoundSd(uint8(k), [2]float64(a), [2]float64(b), rounding))
}

func maskzMulRoundSd(k uint8, a [2]float64, b [2]float64, rounding int) [2]float64


// MulRoundSd: Multiply the lower double-precision (64-bit) floating-point
// element in 'a' and 'b', store the result in the lower element of 'dst', and
// copy the upper element from 'a' to the upper element of 'dst'.
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := a[63:0] * b[63:0]
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMULSD'. Intrinsic: '_mm_mul_round_sd'.
// Requires AVX512F.
func MulRoundSd(a x86.M128d, b x86.M128d, rounding int) x86.M128d {
	return x86.M128d(mulRoundSd([2]float64(a), [2]float64(b), rounding))
}

func mulRoundSd(a [2]float64, b [2]float64, rounding int) [2]float64


// MaskMulRoundSs: Multiply the lower single-precision (32-bit) floating-point
// element in 'a' and 'b', store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'src' when mask bit 0 is not set),
// and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'.
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := a[31:0] * b[31:0]
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMULSS'. Intrinsic: '_mm_mask_mul_round_ss'.
// Requires AVX512F.
func MaskMulRoundSs(src x86.M128, k x86.Mmask8, a x86.M128, b x86.M128, rounding int) x86.M128 {
	return x86.M128(maskMulRoundSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), rounding))
}

func maskMulRoundSs(src [4]float32, k uint8, a [4]float32, b [4]float32, rounding int) [4]float32


// MaskzMulRoundSs: Multiply the lower single-precision (32-bit) floating-point
// element in 'a' and 'b', store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'.
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := a[31:0] * b[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMULSS'. Intrinsic: '_mm_maskz_mul_round_ss'.
// Requires AVX512F.
func MaskzMulRoundSs(k x86.Mmask8, a x86.M128, b x86.M128, rounding int) x86.M128 {
	return x86.M128(maskzMulRoundSs(uint8(k), [4]float32(a), [4]float32(b), rounding))
}

func maskzMulRoundSs(k uint8, a [4]float32, b [4]float32, rounding int) [4]float32


// MulRoundSs: Multiply the lower single-precision (32-bit) floating-point
// element in 'a' and 'b', store the result in the lower element of 'dst', and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'.
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := a[31:0] * b[31:0]
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMULSS'. Intrinsic: '_mm_mul_round_ss'.
// Requires AVX512F.
func MulRoundSs(a x86.M128, b x86.M128, rounding int) x86.M128 {
	return x86.M128(mulRoundSs([4]float32(a), [4]float32(b), rounding))
}

func mulRoundSs(a [4]float32, b [4]float32, rounding int) [4]float32


// MaskMulSd: Multiply the lower double-precision (64-bit) floating-point
// element in 'a' and 'b', store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'src' when mask bit 0 is not set),
// and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := a[63:0] * b[63:0]
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMULSD'. Intrinsic: '_mm_mask_mul_sd'.
// Requires AVX512F.
func MaskMulSd(src x86.M128d, k x86.Mmask8, a x86.M128d, b x86.M128d) x86.M128d {
	return x86.M128d(maskMulSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskMulSd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzMulSd: Multiply the lower double-precision (64-bit) floating-point
// element in 'a' and 'b', store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := a[63:0] * b[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMULSD'. Intrinsic: '_mm_maskz_mul_sd'.
// Requires AVX512F.
func MaskzMulSd(k x86.Mmask8, a x86.M128d, b x86.M128d) x86.M128d {
	return x86.M128d(maskzMulSd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzMulSd(k uint8, a [2]float64, b [2]float64) [2]float64


// MaskMulSs: Multiply the lower single-precision (32-bit) floating-point
// element in 'a' and 'b', store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'src' when mask bit 0 is not set),
// and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'. 
//
//		IF k[0]
//			dst[31:0] := a[31:0] * b[31:0]
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMULSS'. Intrinsic: '_mm_mask_mul_ss'.
// Requires AVX512F.
func MaskMulSs(src x86.M128, k x86.Mmask8, a x86.M128, b x86.M128) x86.M128 {
	return x86.M128(maskMulSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskMulSs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzMulSs: Multiply the lower single-precision (32-bit) floating-point
// element in 'a' and 'b', store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := a[31:0] * b[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMULSS'. Intrinsic: '_mm_maskz_mul_ss'.
// Requires AVX512F.
func MaskzMulSs(k x86.Mmask8, a x86.M128, b x86.M128) x86.M128 {
	return x86.M128(maskzMulSs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzMulSs(k uint8, a [4]float32, b [4]float32) [4]float32


// MaskMulloEpi32: Multiply the packed 32-bit integers in 'a' and 'b',
// producing intermediate 64-bit integers, and store the low 32 bits of the
// intermediate integers in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				tmp[63:0] := a[i+31:i] * b[i+31:i]
//				dst[i+31:i] := tmp[31:0]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMULLD'. Intrinsic: '_mm_mask_mullo_epi32'.
// Requires AVX512F.
func MaskMulloEpi32(src x86.M128i, k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskMulloEpi32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskMulloEpi32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzMulloEpi32: Multiply the packed 32-bit integers in 'a' and 'b',
// producing intermediate 64-bit integers, and store the low 32 bits of the
// intermediate integers in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				tmp[63:0] := a[i+31:i] * b[i+31:i]
//				dst[i+31:i] := tmp[31:0]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMULLD'. Intrinsic: '_mm_maskz_mullo_epi32'.
// Requires AVX512F.
func MaskzMulloEpi32(k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskzMulloEpi32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzMulloEpi32(k uint8, a [16]byte, b [16]byte) [16]byte


// M256MaskMulloEpi32: Multiply the packed 32-bit integers in 'a' and 'b',
// producing intermediate 64-bit integers, and store the low 32 bits of the
// intermediate integers in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				tmp[63:0] := a[i+31:i] * b[i+31:i]
//				dst[i+31:i] := tmp[31:0]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULLD'. Intrinsic: '_mm256_mask_mullo_epi32'.
// Requires AVX512F.
func M256MaskMulloEpi32(src x86.M256i, k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskMulloEpi32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskMulloEpi32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// M256MaskzMulloEpi32: Multiply the packed 32-bit integers in 'a' and 'b',
// producing intermediate 64-bit integers, and store the low 32 bits of the
// intermediate integers in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				tmp[63:0] := a[i+31:i] * b[i+31:i]
//				dst[i+31:i] := tmp[31:0]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULLD'. Intrinsic: '_mm256_maskz_mullo_epi32'.
// Requires AVX512F.
func M256MaskzMulloEpi32(k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzMulloEpi32(uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskzMulloEpi32(k uint8, a [32]byte, b [32]byte) [32]byte


// M512MaskzMulloEpi32: Multiply the packed 32-bit integers in 'a' and 'b',
// producing intermediate 64-bit integers, and store the low 32 bits of the
// intermediate integers in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				tmp[63:0] := a[i+31:i] * b[i+31:i]
//				dst[i+31:i] := tmp[31:0]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMULLD'. Intrinsic: '_mm512_maskz_mullo_epi32'.
// Requires AVX512F.
func M512MaskzMulloEpi32(k x86.Mmask16, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzMulloEpi32(uint16(k), [64]byte(a), [64]byte(b)))
}

func m512MaskzMulloEpi32(k uint16, a [64]byte, b [64]byte) [64]byte


// M512MaskMulloxEpi64: Multiplies elements in packed 64-bit integer vectors
// 'a' and 'b' together, storing the lower 64 bits of the result in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] * b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_mullox_epi64'.
// Requires AVX512F.
func M512MaskMulloxEpi64(src x86.M512i, k x86.Mmask8, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskMulloxEpi64([64]byte(src), uint8(k), [64]byte(a), [64]byte(b)))
}

func m512MaskMulloxEpi64(src [64]byte, k uint8, a [64]byte, b [64]byte) [64]byte


// M512MulloxEpi64: Multiplies elements in packed 64-bit integer vectors 'a'
// and 'b' together, storing the lower 64 bits of the result in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := a[i+63:i] * b[i+63:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mullox_epi64'.
// Requires AVX512F.
func M512MulloxEpi64(a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MulloxEpi64([64]byte(a), [64]byte(b)))
}

func m512MulloxEpi64(a [64]byte, b [64]byte) [64]byte


// M512MaskNearbyintPd: Rounds each packed double-precision (64-bit)
// floating-point element in 'a' to the nearest integer value and stores the
// results as packed double-precision floating-point elements in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := NearbyInt(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_nearbyint_pd'.
// Requires AVX512F.
func M512MaskNearbyintPd(src x86.M512d, k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskNearbyintPd([8]float64(src), uint8(k), [8]float64(a)))
}

func m512MaskNearbyintPd(src [8]float64, k uint8, a [8]float64) [8]float64


// M512NearbyintPd: Rounds each packed double-precision (64-bit) floating-point
// element in 'a' to the nearest integer value and stores the results as packed
// double-precision floating-point elements in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := NearbyInt(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_nearbyint_pd'.
// Requires AVX512F.
func M512NearbyintPd(a x86.M512d) x86.M512d {
	return x86.M512d(m512NearbyintPd([8]float64(a)))
}

func m512NearbyintPd(a [8]float64) [8]float64


// M512MaskNearbyintPs: Rounds each packed single-precision (32-bit)
// floating-point element in 'a' to the nearest integer value and stores the
// results as packed double-precision floating-point elements in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := NearbyInt(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_nearbyint_ps'.
// Requires AVX512F.
func M512MaskNearbyintPs(src x86.M512, k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskNearbyintPs([16]float32(src), uint16(k), [16]float32(a)))
}

func m512MaskNearbyintPs(src [16]float32, k uint16, a [16]float32) [16]float32


// M512NearbyintPs: Rounds each packed single-precision (32-bit) floating-point
// element in 'a' to the nearest integer value and stores the results as packed
// double-precision floating-point elements in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := NearbyInt(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_nearbyint_ps'.
// Requires AVX512F.
func M512NearbyintPs(a x86.M512) x86.M512 {
	return x86.M512(m512NearbyintPs([16]float32(a)))
}

func m512NearbyintPs(a [16]float32) [16]float32


// MaskOrEpi32: Compute the bitwise OR of packed 32-bit integers in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] OR b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPORD'. Intrinsic: '_mm_mask_or_epi32'.
// Requires AVX512F.
func MaskOrEpi32(src x86.M128i, k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskOrEpi32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskOrEpi32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzOrEpi32: Compute the bitwise OR of packed 32-bit integers in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] OR b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPORD'. Intrinsic: '_mm_maskz_or_epi32'.
// Requires AVX512F.
func MaskzOrEpi32(k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskzOrEpi32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzOrEpi32(k uint8, a [16]byte, b [16]byte) [16]byte


// M256MaskOrEpi32: Compute the bitwise OR of packed 32-bit integers in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] OR b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPORD'. Intrinsic: '_mm256_mask_or_epi32'.
// Requires AVX512F.
func M256MaskOrEpi32(src x86.M256i, k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskOrEpi32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskOrEpi32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// M256MaskzOrEpi32: Compute the bitwise OR of packed 32-bit integers in 'a'
// and 'b', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] OR b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPORD'. Intrinsic: '_mm256_maskz_or_epi32'.
// Requires AVX512F.
func M256MaskzOrEpi32(k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzOrEpi32(uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskzOrEpi32(k uint8, a [32]byte, b [32]byte) [32]byte


// M512MaskzOrEpi32: Compute the bitwise OR of packed 32-bit integers in 'a'
// and 'b', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] OR b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPORD'. Intrinsic: '_mm512_maskz_or_epi32'.
// Requires AVX512F.
func M512MaskzOrEpi32(k x86.Mmask16, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzOrEpi32(uint16(k), [64]byte(a), [64]byte(b)))
}

func m512MaskzOrEpi32(k uint16, a [64]byte, b [64]byte) [64]byte


// MaskOrEpi64: Compute the bitwise OR of packed 64-bit integers in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] OR b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPORQ'. Intrinsic: '_mm_mask_or_epi64'.
// Requires AVX512F.
func MaskOrEpi64(src x86.M128i, k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskOrEpi64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskOrEpi64(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzOrEpi64: Compute the bitwise OR of packed 64-bit integers in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] OR b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPORQ'. Intrinsic: '_mm_maskz_or_epi64'.
// Requires AVX512F.
func MaskzOrEpi64(k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskzOrEpi64(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzOrEpi64(k uint8, a [16]byte, b [16]byte) [16]byte


// M256MaskOrEpi64: Compute the bitwise OR of packed 64-bit integers in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] OR b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPORQ'. Intrinsic: '_mm256_mask_or_epi64'.
// Requires AVX512F.
func M256MaskOrEpi64(src x86.M256i, k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskOrEpi64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskOrEpi64(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// M256MaskzOrEpi64: Compute the bitwise OR of packed 64-bit integers in 'a'
// and 'b', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] OR b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPORQ'. Intrinsic: '_mm256_maskz_or_epi64'.
// Requires AVX512F.
func M256MaskzOrEpi64(k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzOrEpi64(uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskzOrEpi64(k uint8, a [32]byte, b [32]byte) [32]byte


// M512MaskzOrEpi64: Compute the bitwise OR of packed 64-bit integers in 'a'
// and 'b', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] OR b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPORQ'. Intrinsic: '_mm512_maskz_or_epi64'.
// Requires AVX512F.
func M512MaskzOrEpi64(k x86.Mmask8, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzOrEpi64(uint8(k), [64]byte(a), [64]byte(b)))
}

func m512MaskzOrEpi64(k uint8, a [64]byte, b [64]byte) [64]byte


// MaskPermutePd: Shuffle double-precision (64-bit) floating-point elements in
// 'a' using the control in 'imm8', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		IF (imm8[0] == 0) tmp_dst[63:0] := a[63:0]
//		IF (imm8[0] == 1) tmp_dst[63:0] := a[127:64]
//		IF (imm8[1] == 0) tmp_dst[127:64] := a[63:0]
//		IF (imm8[1] == 1) tmp_dst[127:64] := a[127:64]
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm_mask_permute_pd'.
// Requires AVX512F.
func MaskPermutePd(src x86.M128d, k x86.Mmask8, a x86.M128d, imm8 int) x86.M128d {
	return x86.M128d(maskPermutePd([2]float64(src), uint8(k), [2]float64(a), imm8))
}

func maskPermutePd(src [2]float64, k uint8, a [2]float64, imm8 int) [2]float64


// MaskzPermutePd: Shuffle double-precision (64-bit) floating-point elements in
// 'a' using the control in 'imm8', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		IF (imm8[0] == 0) tmp_dst[63:0] := a[63:0]
//		IF (imm8[0] == 1) tmp_dst[63:0] := a[127:64]
//		IF (imm8[1] == 0) tmp_dst[127:64] := a[63:0]
//		IF (imm8[1] == 1) tmp_dst[127:64] := a[127:64]
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm_maskz_permute_pd'.
// Requires AVX512F.
func MaskzPermutePd(k x86.Mmask8, a x86.M128d, imm8 int) x86.M128d {
	return x86.M128d(maskzPermutePd(uint8(k), [2]float64(a), imm8))
}

func maskzPermutePd(k uint8, a [2]float64, imm8 int) [2]float64


// M256MaskPermutePd: Shuffle double-precision (64-bit) floating-point elements
// in 'a' within 128-bit lanes using the control in 'imm8', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		IF (imm8[0] == 0) tmp_dst[63:0] := a[63:0]
//		IF (imm8[0] == 1) tmp_dst[63:0] := a[127:64]
//		IF (imm8[1] == 0) tmp_dst[127:64] := a[63:0]
//		IF (imm8[1] == 1) tmp_dst[127:64] := a[127:64]
//		IF (imm8[2] == 0) tmp_dst[191:128] := a[191:128]
//		IF (imm8[2] == 1) tmp_dst[191:128] := a[255:192]
//		IF (imm8[3] == 0) tmp_dst[255:192] := a[191:128]
//		IF (imm8[3] == 1) tmp_dst[255:192] := a[255:192]
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm256_mask_permute_pd'.
// Requires AVX512F.
func M256MaskPermutePd(src x86.M256d, k x86.Mmask8, a x86.M256d, imm8 int) x86.M256d {
	return x86.M256d(m256MaskPermutePd([4]float64(src), uint8(k), [4]float64(a), imm8))
}

func m256MaskPermutePd(src [4]float64, k uint8, a [4]float64, imm8 int) [4]float64


// M256MaskzPermutePd: Shuffle double-precision (64-bit) floating-point
// elements in 'a' within 128-bit lanes using the control in 'imm8', and store
// the results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		IF (imm8[0] == 0) tmp_dst[63:0] := a[63:0]
//		IF (imm8[0] == 1) tmp_dst[63:0] := a[127:64]
//		IF (imm8[1] == 0) tmp_dst[127:64] := a[63:0]
//		IF (imm8[1] == 1) tmp_dst[127:64] := a[127:64]
//		IF (imm8[2] == 0) tmp_dst[191:128] := a[191:128]
//		IF (imm8[2] == 1) tmp_dst[191:128] := a[255:192]
//		IF (imm8[3] == 0) tmp_dst[255:192] := a[191:128]
//		IF (imm8[3] == 1) tmp_dst[255:192] := a[255:192]
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm256_maskz_permute_pd'.
// Requires AVX512F.
func M256MaskzPermutePd(k x86.Mmask8, a x86.M256d, imm8 int) x86.M256d {
	return x86.M256d(m256MaskzPermutePd(uint8(k), [4]float64(a), imm8))
}

func m256MaskzPermutePd(k uint8, a [4]float64, imm8 int) [4]float64


// M512MaskPermutePd: Shuffle double-precision (64-bit) floating-point elements
// in 'a' within 128-bit lanes using the control in 'imm8', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		IF (imm8[0] == 0) tmp_dst[63:0] := a[63:0]
//		IF (imm8[0] == 1) tmp_dst[63:0] := a[127:64]
//		IF (imm8[1] == 0) tmp_dst[127:64] := a[63:0]
//		IF (imm8[1] == 1) tmp_dst[127:64] := a[127:64]
//		IF (imm8[2] == 0) tmp_dst[191:128] := a[191:128]
//		IF (imm8[2] == 1) tmp_dst[191:128] := a[255:192]
//		IF (imm8[3] == 0) tmp_dst[255:192] := a[191:128]
//		IF (imm8[3] == 1) tmp_dst[255:192] := a[255:192]
//		IF (imm8[4] == 0) tmp_dst[319:256] := a[319:256]
//		IF (imm8[4] == 1) tmp_dst[319:256] := a[383:320]
//		IF (imm8[5] == 0) tmp_dst[383:320] := a[319:256]
//		IF (imm8[5] == 1) tmp_dst[383:320] := a[383:320]
//		IF (imm8[6] == 0) tmp_dst[447:384] := a[447:384]
//		IF (imm8[6] == 1) tmp_dst[447:384] := a[511:448]
//		IF (imm8[7] == 0) tmp_dst[511:448] := a[447:384]
//		IF (imm8[7] == 1) tmp_dst[511:448] := a[511:448]
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm512_mask_permute_pd'.
// Requires AVX512F.
func M512MaskPermutePd(src x86.M512d, k x86.Mmask8, a x86.M512d, imm8 int) x86.M512d {
	return x86.M512d(m512MaskPermutePd([8]float64(src), uint8(k), [8]float64(a), imm8))
}

func m512MaskPermutePd(src [8]float64, k uint8, a [8]float64, imm8 int) [8]float64


// M512MaskzPermutePd: Shuffle double-precision (64-bit) floating-point
// elements in 'a' within 128-bit lanes using the control in 'imm8', and store
// the results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		IF (imm8[0] == 0) tmp_dst[63:0] := a[63:0]
//		IF (imm8[0] == 1) tmp_dst[63:0] := a[127:64]
//		IF (imm8[1] == 0) tmp_dst[127:64] := a[63:0]
//		IF (imm8[1] == 1) tmp_dst[127:64] := a[127:64]
//		IF (imm8[2] == 0) tmp_dst[191:128] := a[191:128]
//		IF (imm8[2] == 1) tmp_dst[191:128] := a[255:192]
//		IF (imm8[3] == 0) tmp_dst[255:192] := a[191:128]
//		IF (imm8[3] == 1) tmp_dst[255:192] := a[255:192]
//		IF (imm8[4] == 0) tmp_dst[319:256] := a[319:256]
//		IF (imm8[4] == 1) tmp_dst[319:256] := a[383:320]
//		IF (imm8[5] == 0) tmp_dst[383:320] := a[319:256]
//		IF (imm8[5] == 1) tmp_dst[383:320] := a[383:320]
//		IF (imm8[6] == 0) tmp_dst[447:384] := a[447:384]
//		IF (imm8[6] == 1) tmp_dst[447:384] := a[511:448]
//		IF (imm8[7] == 0) tmp_dst[511:448] := a[447:384]
//		IF (imm8[7] == 1) tmp_dst[511:448] := a[511:448]
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm512_maskz_permute_pd'.
// Requires AVX512F.
func M512MaskzPermutePd(k x86.Mmask8, a x86.M512d, imm8 int) x86.M512d {
	return x86.M512d(m512MaskzPermutePd(uint8(k), [8]float64(a), imm8))
}

func m512MaskzPermutePd(k uint8, a [8]float64, imm8 int) [8]float64


// M512PermutePd: Shuffle double-precision (64-bit) floating-point elements in
// 'a' within 128-bit lanes using the control in 'imm8', and store the results
// in 'dst'. 
//
//		IF (imm8[0] == 0) dst[63:0] := a[63:0]
//		IF (imm8[0] == 1) dst[63:0] := a[127:64]
//		IF (imm8[1] == 0) dst[127:64] := a[63:0]
//		IF (imm8[1] == 1) dst[127:64] := a[127:64]
//		IF (imm8[2] == 0) dst[191:128] := a[191:128]
//		IF (imm8[2] == 1) dst[191:128] := a[255:192]
//		IF (imm8[3] == 0) dst[255:192] := a[191:128]
//		IF (imm8[3] == 1) dst[255:192] := a[255:192]
//		IF (imm8[4] == 0) dst[319:256] := a[319:256]
//		IF (imm8[4] == 1) dst[319:256] := a[383:320]
//		IF (imm8[5] == 0) dst[383:320] := a[319:256]
//		IF (imm8[5] == 1) dst[383:320] := a[383:320]
//		IF (imm8[6] == 0) dst[447:384] := a[447:384]
//		IF (imm8[6] == 1) dst[447:384] := a[511:448]
//		IF (imm8[7] == 0) dst[511:448] := a[447:384]
//		IF (imm8[7] == 1) dst[511:448] := a[511:448]
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm512_permute_pd'.
// Requires AVX512F.
func M512PermutePd(a x86.M512d, imm8 int) x86.M512d {
	return x86.M512d(m512PermutePd([8]float64(a), imm8))
}

func m512PermutePd(a [8]float64, imm8 int) [8]float64


// MaskPermutePs: Shuffle single-precision (32-bit) floating-point elements in
// 'a' using the control in 'imm8', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm_mask_permute_ps'.
// Requires AVX512F.
func MaskPermutePs(src x86.M128, k x86.Mmask8, a x86.M128, imm8 int) x86.M128 {
	return x86.M128(maskPermutePs([4]float32(src), uint8(k), [4]float32(a), imm8))
}

func maskPermutePs(src [4]float32, k uint8, a [4]float32, imm8 int) [4]float32


// MaskzPermutePs: Shuffle single-precision (32-bit) floating-point elements in
// 'a' using the control in 'imm8', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm_maskz_permute_ps'.
// Requires AVX512F.
func MaskzPermutePs(k x86.Mmask8, a x86.M128, imm8 int) x86.M128 {
	return x86.M128(maskzPermutePs(uint8(k), [4]float32(a), imm8))
}

func maskzPermutePs(k uint8, a [4]float32, imm8 int) [4]float32


// M256MaskPermutePs: Shuffle single-precision (32-bit) floating-point elements
// in 'a' within 128-bit lanes using the control in 'imm8', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		tmp_dst[223:192] := SELECT4(a[255:128], imm8[5:4])
//		tmp_dst[255:224] := SELECT4(a[255:128], imm8[7:6])
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm256_mask_permute_ps'.
// Requires AVX512F.
func M256MaskPermutePs(src x86.M256, k x86.Mmask8, a x86.M256, imm8 int) x86.M256 {
	return x86.M256(m256MaskPermutePs([8]float32(src), uint8(k), [8]float32(a), imm8))
}

func m256MaskPermutePs(src [8]float32, k uint8, a [8]float32, imm8 int) [8]float32


// M256MaskzPermutePs: Shuffle single-precision (32-bit) floating-point
// elements in 'a' within 128-bit lanes using the control in 'imm8', and store
// the results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		tmp_dst[223:192] := SELECT4(a[255:128], imm8[5:4])
//		tmp_dst[255:224] := SELECT4(a[255:128], imm8[7:6])
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm256_maskz_permute_ps'.
// Requires AVX512F.
func M256MaskzPermutePs(k x86.Mmask8, a x86.M256, imm8 int) x86.M256 {
	return x86.M256(m256MaskzPermutePs(uint8(k), [8]float32(a), imm8))
}

func m256MaskzPermutePs(k uint8, a [8]float32, imm8 int) [8]float32


// M512MaskPermutePs: Shuffle single-precision (32-bit) floating-point elements
// in 'a' within 128-bit lanes using the control in 'imm8', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		tmp_dst[223:192] := SELECT4(a[255:128], imm8[5:4])
//		tmp_dst[255:224] := SELECT4(a[255:128], imm8[7:6])
//		tmp_dst[287:256] := SELECT4(a[383:256], imm8[1:0])
//		tmp_dst[319:288] := SELECT4(a[383:256], imm8[3:2])
//		tmp_dst[351:320] := SELECT4(a[383:256], imm8[5:4])
//		tmp_dst[383:352] := SELECT4(a[383:256], imm8[7:6])
//		tmp_dst[415:384] := SELECT4(a[511:384], imm8[1:0])
//		tmp_dst[447:416] := SELECT4(a[511:384], imm8[3:2])
//		tmp_dst[479:448] := SELECT4(a[511:384], imm8[5:4])
//		tmp_dst[511:480] := SELECT4(a[511:384], imm8[7:6])
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm512_mask_permute_ps'.
// Requires AVX512F.
func M512MaskPermutePs(src x86.M512, k x86.Mmask16, a x86.M512, imm8 int) x86.M512 {
	return x86.M512(m512MaskPermutePs([16]float32(src), uint16(k), [16]float32(a), imm8))
}

func m512MaskPermutePs(src [16]float32, k uint16, a [16]float32, imm8 int) [16]float32


// M512MaskzPermutePs: Shuffle single-precision (32-bit) floating-point
// elements in 'a' within 128-bit lanes using the control in 'imm8', and store
// the results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		tmp_dst[223:192] := SELECT4(a[255:128], imm8[5:4])
//		tmp_dst[255:224] := SELECT4(a[255:128], imm8[7:6])
//		tmp_dst[287:256] := SELECT4(a[383:256], imm8[1:0])
//		tmp_dst[319:288] := SELECT4(a[383:256], imm8[3:2])
//		tmp_dst[351:320] := SELECT4(a[383:256], imm8[5:4])
//		tmp_dst[383:352] := SELECT4(a[383:256], imm8[7:6])
//		tmp_dst[415:384] := SELECT4(a[511:384], imm8[1:0])
//		tmp_dst[447:416] := SELECT4(a[511:384], imm8[3:2])
//		tmp_dst[479:448] := SELECT4(a[511:384], imm8[5:4])
//		tmp_dst[511:480] := SELECT4(a[511:384], imm8[7:6])
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm512_maskz_permute_ps'.
// Requires AVX512F.
func M512MaskzPermutePs(k x86.Mmask16, a x86.M512, imm8 int) x86.M512 {
	return x86.M512(m512MaskzPermutePs(uint16(k), [16]float32(a), imm8))
}

func m512MaskzPermutePs(k uint16, a [16]float32, imm8 int) [16]float32


// M512PermutePs: Shuffle single-precision (32-bit) floating-point elements in
// 'a' within 128-bit lanes using the control in 'imm8', and store the results
// in 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		dst[223:192] := SELECT4(a[255:128], imm8[5:4])
//		dst[255:224] := SELECT4(a[255:128], imm8[7:6])
//		dst[287:256] := SELECT4(a[383:256], imm8[1:0])
//		dst[319:288] := SELECT4(a[383:256], imm8[3:2])
//		dst[351:320] := SELECT4(a[383:256], imm8[5:4])
//		dst[383:352] := SELECT4(a[383:256], imm8[7:6])
//		dst[415:384] := SELECT4(a[511:384], imm8[1:0])
//		dst[447:416] := SELECT4(a[511:384], imm8[3:2])
//		dst[479:448] := SELECT4(a[511:384], imm8[5:4])
//		dst[511:480] := SELECT4(a[511:384], imm8[7:6])
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm512_permute_ps'.
// Requires AVX512F.
func M512PermutePs(a x86.M512, imm8 int) x86.M512 {
	return x86.M512(m512PermutePs([16]float32(a), imm8))
}

func m512PermutePs(a [16]float32, imm8 int) [16]float32


// MaskPermutevarPd: Shuffle double-precision (64-bit) floating-point elements
// in 'a' using the control in 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		IF (b[1] == 0) tmp_dst[63:0] := a[63:0]
//		IF (b[1] == 1) tmp_dst[63:0] := a[127:64]
//		IF (b[65] == 0) tmp_dst[127:64] := a[63:0]
//		IF (b[65] == 1) tmp_dst[127:64] := a[127:64]
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm_mask_permutevar_pd'.
// Requires AVX512F.
func MaskPermutevarPd(src x86.M128d, k x86.Mmask8, a x86.M128d, b x86.M128i) x86.M128d {
	return x86.M128d(maskPermutevarPd([2]float64(src), uint8(k), [2]float64(a), [16]byte(b)))
}

func maskPermutevarPd(src [2]float64, k uint8, a [2]float64, b [16]byte) [2]float64


// MaskzPermutevarPd: Shuffle double-precision (64-bit) floating-point elements
// in 'a' using the control in 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		IF (b[1] == 0) tmp_dst[63:0] := a[63:0]
//		IF (b[1] == 1) tmp_dst[63:0] := a[127:64]
//		IF (b[65] == 0) tmp_dst[127:64] := a[63:0]
//		IF (b[65] == 1) tmp_dst[127:64] := a[127:64]
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm_maskz_permutevar_pd'.
// Requires AVX512F.
func MaskzPermutevarPd(k x86.Mmask8, a x86.M128d, b x86.M128i) x86.M128d {
	return x86.M128d(maskzPermutevarPd(uint8(k), [2]float64(a), [16]byte(b)))
}

func maskzPermutevarPd(k uint8, a [2]float64, b [16]byte) [2]float64


// M256MaskPermutevarPd: Shuffle double-precision (64-bit) floating-point
// elements in 'a' within 128-bit lanes using the control in 'b', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		IF (b[1] == 0) tmp_dst[63:0] := a[63:0]
//		IF (b[1] == 1) tmp_dst[63:0] := a[127:64]
//		IF (b[65] == 0) tmp_dst[127:64] := a[63:0]
//		IF (b[65] == 1) tmp_dst[127:64] := a[127:64]
//		IF (b[129] == 0) tmp_dst[191:128] := a[191:128]
//		IF (b[129] == 1) tmp_dst[191:128] := a[255:192]
//		IF (b[193] == 0) tmp_dst[255:192] := a[191:128]
//		IF (b[193] == 1) tmp_dst[255:192] := a[255:192]
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm256_mask_permutevar_pd'.
// Requires AVX512F.
func M256MaskPermutevarPd(src x86.M256d, k x86.Mmask8, a x86.M256d, b x86.M256i) x86.M256d {
	return x86.M256d(m256MaskPermutevarPd([4]float64(src), uint8(k), [4]float64(a), [32]byte(b)))
}

func m256MaskPermutevarPd(src [4]float64, k uint8, a [4]float64, b [32]byte) [4]float64


// M256MaskzPermutevarPd: Shuffle double-precision (64-bit) floating-point
// elements in 'a' within 128-bit lanes using the control in 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		IF (b[1] == 0) tmp_dst[63:0] := a[63:0]
//		IF (b[1] == 1) tmp_dst[63:0] := a[127:64]
//		IF (b[65] == 0) tmp_dst[127:64] := a[63:0]
//		IF (b[65] == 1) tmp_dst[127:64] := a[127:64]
//		IF (b[129] == 0) tmp_dst[191:128] := a[191:128]
//		IF (b[129] == 1) tmp_dst[191:128] := a[255:192]
//		IF (b[193] == 0) tmp_dst[255:192] := a[191:128]
//		IF (b[193] == 1) tmp_dst[255:192] := a[255:192]
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm256_maskz_permutevar_pd'.
// Requires AVX512F.
func M256MaskzPermutevarPd(k x86.Mmask8, a x86.M256d, b x86.M256i) x86.M256d {
	return x86.M256d(m256MaskzPermutevarPd(uint8(k), [4]float64(a), [32]byte(b)))
}

func m256MaskzPermutevarPd(k uint8, a [4]float64, b [32]byte) [4]float64


// M512MaskPermutevarPd: Shuffle double-precision (64-bit) floating-point
// elements in 'a' within 128-bit lanes using the control in 'b', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		IF (b[1] == 0) tmp_dst[63:0] := a[63:0]
//		IF (b[1] == 1) tmp_dst[63:0] := a[127:64]
//		IF (b[65] == 0) tmp_dst[127:64] := a[63:0]
//		IF (b[65] == 1) tmp_dst[127:64] := a[127:64]
//		IF (b[129] == 0) tmp_dst[191:128] := a[191:128]
//		IF (b[129] == 1) tmp_dst[191:128] := a[255:192]
//		IF (b[193] == 0) tmp_dst[255:192] := a[191:128]
//		IF (b[193] == 1) tmp_dst[255:192] := a[255:192]
//		IF (b[257] == 0) tmp_dst[319:256] := a[319:256]
//		IF (b[257] == 1) tmp_dst[319:256] := a[383:320]
//		IF (b[321] == 0) tmp_dst[383:320] := a[319:256]
//		IF (b[321] == 1) tmp_dst[383:320] := a[383:320]
//		IF (b[385] == 0) tmp_dst[447:384] := a[447:384]
//		IF (b[385] == 1) tmp_dst[447:384] := a[511:448]
//		IF (b[449] == 0) tmp_dst[511:448] := a[447:384]
//		IF (b[449] == 1) tmp_dst[511:448] := a[511:448]
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm512_mask_permutevar_pd'.
// Requires AVX512F.
func M512MaskPermutevarPd(src x86.M512d, k x86.Mmask8, a x86.M512d, b x86.M512i) x86.M512d {
	return x86.M512d(m512MaskPermutevarPd([8]float64(src), uint8(k), [8]float64(a), [64]byte(b)))
}

func m512MaskPermutevarPd(src [8]float64, k uint8, a [8]float64, b [64]byte) [8]float64


// M512MaskzPermutevarPd: Shuffle double-precision (64-bit) floating-point
// elements in 'a' within 128-bit lanes using the control in 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		IF (b[1] == 0) tmp_dst[63:0] := a[63:0]
//		IF (b[1] == 1) tmp_dst[63:0] := a[127:64]
//		IF (b[65] == 0) tmp_dst[127:64] := a[63:0]
//		IF (b[65] == 1) tmp_dst[127:64] := a[127:64]
//		IF (b[129] == 0) tmp_dst[191:128] := a[191:128]
//		IF (b[129] == 1) tmp_dst[191:128] := a[255:192]
//		IF (b[193] == 0) tmp_dst[255:192] := a[191:128]
//		IF (b[193] == 1) tmp_dst[255:192] := a[255:192]
//		IF (b[257] == 0) tmp_dst[319:256] := a[319:256]
//		IF (b[257] == 1) tmp_dst[319:256] := a[383:320]
//		IF (b[321] == 0) tmp_dst[383:320] := a[319:256]
//		IF (b[321] == 1) tmp_dst[383:320] := a[383:320]
//		IF (b[385] == 0) tmp_dst[447:384] := a[447:384]
//		IF (b[385] == 1) tmp_dst[447:384] := a[511:448]
//		IF (b[449] == 0) tmp_dst[511:448] := a[447:384]
//		IF (b[449] == 1) tmp_dst[511:448] := a[511:448]
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm512_maskz_permutevar_pd'.
// Requires AVX512F.
func M512MaskzPermutevarPd(k x86.Mmask8, a x86.M512d, b x86.M512i) x86.M512d {
	return x86.M512d(m512MaskzPermutevarPd(uint8(k), [8]float64(a), [64]byte(b)))
}

func m512MaskzPermutevarPd(k uint8, a [8]float64, b [64]byte) [8]float64


// M512PermutevarPd: Shuffle double-precision (64-bit) floating-point elements
// in 'a' within 128-bit lanes using the control in 'b', and store the results
// in 'dst'. 
//
//		IF (b[1] == 0) dst[63:0] := a[63:0]
//		IF (b[1] == 1) dst[63:0] := a[127:64]
//		IF (b[65] == 0) dst[127:64] := a[63:0]
//		IF (b[65] == 1) dst[127:64] := a[127:64]
//		IF (b[129] == 0) dst[191:128] := a[191:128]
//		IF (b[129] == 1) dst[191:128] := a[255:192]
//		IF (b[193] == 0) dst[255:192] := a[191:128]
//		IF (b[193] == 1) dst[255:192] := a[255:192]
//		IF (b[257] == 0) dst[319:256] := a[319:256]
//		IF (b[257] == 1) dst[319:256] := a[383:320]
//		IF (b[321] == 0) dst[383:320] := a[319:256]
//		IF (b[321] == 1) dst[383:320] := a[383:320]
//		IF (b[385] == 0) dst[447:384] := a[447:384]
//		IF (b[385] == 1) dst[447:384] := a[511:448]
//		IF (b[449] == 0) dst[511:448] := a[447:384]
//		IF (b[449] == 1) dst[511:448] := a[511:448]
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm512_permutevar_pd'.
// Requires AVX512F.
func M512PermutevarPd(a x86.M512d, b x86.M512i) x86.M512d {
	return x86.M512d(m512PermutevarPd([8]float64(a), [64]byte(b)))
}

func m512PermutevarPd(a [8]float64, b [64]byte) [8]float64


// MaskPermutevarPs: Shuffle single-precision (32-bit) floating-point elements
// in 'a' using the control in 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], b[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], b[33:32])
//		tmp_dst[95:64] := SELECT4(a[127:0], b[65:64])
//		tmp_dst[127:96] := SELECT4(a[127:0], b[97:96])
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm_mask_permutevar_ps'.
// Requires AVX512F.
func MaskPermutevarPs(src x86.M128, k x86.Mmask8, a x86.M128, b x86.M128i) x86.M128 {
	return x86.M128(maskPermutevarPs([4]float32(src), uint8(k), [4]float32(a), [16]byte(b)))
}

func maskPermutevarPs(src [4]float32, k uint8, a [4]float32, b [16]byte) [4]float32


// MaskzPermutevarPs: Shuffle single-precision (32-bit) floating-point elements
// in 'a' using the control in 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], b[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], b[33:32])
//		tmp_dst[95:64] := SELECT4(a[127:0], b[65:64])
//		tmp_dst[127:96] := SELECT4(a[127:0], b[97:96])
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm_maskz_permutevar_ps'.
// Requires AVX512F.
func MaskzPermutevarPs(k x86.Mmask8, a x86.M128, b x86.M128i) x86.M128 {
	return x86.M128(maskzPermutevarPs(uint8(k), [4]float32(a), [16]byte(b)))
}

func maskzPermutevarPs(k uint8, a [4]float32, b [16]byte) [4]float32


// M256MaskPermutevarPs: Shuffle single-precision (32-bit) floating-point
// elements in 'a' within 128-bit lanes using the control in 'b', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], b[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], b[33:32])
//		tmp_dst[95:64] := SELECT4(a[127:0], b[65:64])
//		tmp_dst[127:96] := SELECT4(a[127:0], b[97:96])
//		tmp_dst[159:128] := SELECT4(a[255:128], b[129:128])
//		tmp_dst[191:160] := SELECT4(a[255:128], b[161:160])
//		tmp_dst[223:192] := SELECT4(a[255:128], b[193:192])
//		tmp_dst[255:224] := SELECT4(a[255:128], b[225:224])
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm256_mask_permutevar_ps'.
// Requires AVX512F.
func M256MaskPermutevarPs(src x86.M256, k x86.Mmask8, a x86.M256, b x86.M256i) x86.M256 {
	return x86.M256(m256MaskPermutevarPs([8]float32(src), uint8(k), [8]float32(a), [32]byte(b)))
}

func m256MaskPermutevarPs(src [8]float32, k uint8, a [8]float32, b [32]byte) [8]float32


// M256MaskzPermutevarPs: Shuffle single-precision (32-bit) floating-point
// elements in 'a' within 128-bit lanes using the control in 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], b[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], b[33:32])
//		tmp_dst[95:64] := SELECT4(a[127:0], b[65:64])
//		tmp_dst[127:96] := SELECT4(a[127:0], b[97:96])
//		tmp_dst[159:128] := SELECT4(a[255:128], b[129:128])
//		tmp_dst[191:160] := SELECT4(a[255:128], b[161:160])
//		tmp_dst[223:192] := SELECT4(a[255:128], b[193:192])
//		tmp_dst[255:224] := SELECT4(a[255:128], b[225:224])
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm256_maskz_permutevar_ps'.
// Requires AVX512F.
func M256MaskzPermutevarPs(k x86.Mmask8, a x86.M256, b x86.M256i) x86.M256 {
	return x86.M256(m256MaskzPermutevarPs(uint8(k), [8]float32(a), [32]byte(b)))
}

func m256MaskzPermutevarPs(k uint8, a [8]float32, b [32]byte) [8]float32


// M512MaskPermutevarPs: Shuffle single-precision (32-bit) floating-point
// elements in 'a' within 128-bit lanes using the control in 'b', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], b[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], b[33:32])
//		tmp_dst[95:64] := SELECT4(a[127:0], b[65:64])
//		tmp_dst[127:96] := SELECT4(a[127:0], b[97:96])
//		tmp_dst[159:128] := SELECT4(a[255:128], b[129:128])
//		tmp_dst[191:160] := SELECT4(a[255:128], b[161:160])
//		tmp_dst[223:192] := SELECT4(a[255:128], b[193:192])
//		tmp_dst[255:224] := SELECT4(a[255:128], b[225:224])
//		tmp_dst[287:256] := SELECT4(a[383:256], b[257:256])
//		tmp_dst[319:288] := SELECT4(a[383:256], b[289:288])
//		tmp_dst[351:320] := SELECT4(a[383:256], b[321:320])
//		tmp_dst[383:352] := SELECT4(a[383:256], b[353:352])
//		tmp_dst[415:384] := SELECT4(a[511:384], b[385:384])
//		tmp_dst[447:416] := SELECT4(a[511:384], b[417:416])
//		tmp_dst[479:448] := SELECT4(a[511:384], b[449:448])
//		tmp_dst[511:480] := SELECT4(a[511:384], b[481:480])
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm512_mask_permutevar_ps'.
// Requires AVX512F.
func M512MaskPermutevarPs(src x86.M512, k x86.Mmask16, a x86.M512, b x86.M512i) x86.M512 {
	return x86.M512(m512MaskPermutevarPs([16]float32(src), uint16(k), [16]float32(a), [64]byte(b)))
}

func m512MaskPermutevarPs(src [16]float32, k uint16, a [16]float32, b [64]byte) [16]float32


// M512MaskzPermutevarPs: Shuffle single-precision (32-bit) floating-point
// elements in 'a' within 128-bit lanes using the control in 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], b[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], b[33:32])
//		tmp_dst[95:64] := SELECT4(a[127:0], b[65:64])
//		tmp_dst[127:96] := SELECT4(a[127:0], b[97:96])
//		tmp_dst[159:128] := SELECT4(a[255:128], b[129:128])
//		tmp_dst[191:160] := SELECT4(a[255:128], b[161:160])
//		tmp_dst[223:192] := SELECT4(a[255:128], b[193:192])
//		tmp_dst[255:224] := SELECT4(a[255:128], b[225:224])
//		tmp_dst[287:256] := SELECT4(a[383:256], b[257:256])
//		tmp_dst[319:288] := SELECT4(a[383:256], b[289:288])
//		tmp_dst[351:320] := SELECT4(a[383:256], b[321:320])
//		tmp_dst[383:352] := SELECT4(a[383:256], b[353:352])
//		tmp_dst[415:384] := SELECT4(a[511:384], b[385:384])
//		tmp_dst[447:416] := SELECT4(a[511:384], b[417:416])
//		tmp_dst[479:448] := SELECT4(a[511:384], b[449:448])
//		tmp_dst[511:480] := SELECT4(a[511:384], b[481:480])
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm512_maskz_permutevar_ps'.
// Requires AVX512F.
func M512MaskzPermutevarPs(k x86.Mmask16, a x86.M512, b x86.M512i) x86.M512 {
	return x86.M512(m512MaskzPermutevarPs(uint16(k), [16]float32(a), [64]byte(b)))
}

func m512MaskzPermutevarPs(k uint16, a [16]float32, b [64]byte) [16]float32


// M512PermutevarPs: Shuffle single-precision (32-bit) floating-point elements
// in 'a' within 128-bit lanes using the control in 'b', and store the results
// in 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		dst[31:0] := SELECT4(a[127:0], b[1:0])
//		dst[63:32] := SELECT4(a[127:0], b[33:32])
//		dst[95:64] := SELECT4(a[127:0], b[65:64])
//		dst[127:96] := SELECT4(a[127:0], b[97:96])
//		dst[159:128] := SELECT4(a[255:128], b[129:128])
//		dst[191:160] := SELECT4(a[255:128], b[161:160])
//		dst[223:192] := SELECT4(a[255:128], b[193:192])
//		dst[255:224] := SELECT4(a[255:128], b[225:224])
//		dst[287:256] := SELECT4(a[383:256], b[257:256])
//		dst[319:288] := SELECT4(a[383:256], b[289:288])
//		dst[351:320] := SELECT4(a[383:256], b[321:320])
//		dst[383:352] := SELECT4(a[383:256], b[353:352])
//		dst[415:384] := SELECT4(a[511:384], b[385:384])
//		dst[447:416] := SELECT4(a[511:384], b[417:416])
//		dst[479:448] := SELECT4(a[511:384], b[449:448])
//		dst[511:480] := SELECT4(a[511:384], b[481:480])
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm512_permutevar_ps'.
// Requires AVX512F.
func M512PermutevarPs(a x86.M512, b x86.M512i) x86.M512 {
	return x86.M512(m512PermutevarPs([16]float32(a), [64]byte(b)))
}

func m512PermutevarPs(a [16]float32, b [64]byte) [16]float32


// M256MaskPermutexEpi64: Shuffle 64-bit integers in 'a' across lanes lanes
// using the control in 'imm8', and store the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[63:0] := src[63:0]
//			1:	tmp[63:0] := src[127:64]
//			2:	tmp[63:0] := src[191:128]
//			3:	tmp[63:0] := src[255:192]
//			ESAC
//			RETURN tmp[63:0]
//		}
//		
//		tmp_dst[63:0] := SELECT4(a[255:0], imm8[1:0])
//		tmp_dst[127:64] := SELECT4(a[255:0], imm8[3:2])
//		tmp_dst[191:128] := SELECT4(a[255:0], imm8[5:4])
//		tmp_dst[255:192] := SELECT4(a[255:0], imm8[7:6])
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMQ'. Intrinsic: '_mm256_mask_permutex_epi64'.
// Requires AVX512F.
func M256MaskPermutexEpi64(src x86.M256i, k x86.Mmask8, a x86.M256i, imm8 int) x86.M256i {
	return x86.M256i(m256MaskPermutexEpi64([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func m256MaskPermutexEpi64(src [32]byte, k uint8, a [32]byte, imm8 int) [32]byte


// M256MaskzPermutexEpi64: Shuffle 64-bit integers in 'a' across lanes using
// the control in 'imm8', and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[63:0] := src[63:0]
//			1:	tmp[63:0] := src[127:64]
//			2:	tmp[63:0] := src[191:128]
//			3:	tmp[63:0] := src[255:192]
//			ESAC
//			RETURN tmp[63:0]
//		}
//		
//		tmp_dst[63:0] := SELECT4(a[255:0], imm8[1:0])
//		tmp_dst[127:64] := SELECT4(a[255:0], imm8[3:2])
//		tmp_dst[191:128] := SELECT4(a[255:0], imm8[5:4])
//		tmp_dst[255:192] := SELECT4(a[255:0], imm8[7:6])
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMQ'. Intrinsic: '_mm256_maskz_permutex_epi64'.
// Requires AVX512F.
func M256MaskzPermutexEpi64(k x86.Mmask8, a x86.M256i, imm8 int) x86.M256i {
	return x86.M256i(m256MaskzPermutexEpi64(uint8(k), [32]byte(a), imm8))
}

func m256MaskzPermutexEpi64(k uint8, a [32]byte, imm8 int) [32]byte


// M256PermutexEpi64: Shuffle 64-bit integers in 'a' across lanes using the
// control in 'imm8', and store the results in 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[63:0] := src[63:0]
//			1:	tmp[63:0] := src[127:64]
//			2:	tmp[63:0] := src[191:128]
//			3:	tmp[63:0] := src[255:192]
//			ESAC
//			RETURN tmp[63:0]
//		}
//		
//		dst[63:0] := SELECT4(a[255:0], imm8[1:0])
//		dst[127:64] := SELECT4(a[255:0], imm8[3:2])
//		dst[191:128] := SELECT4(a[255:0], imm8[5:4])
//		dst[255:192] := SELECT4(a[255:0], imm8[7:6])
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMQ'. Intrinsic: '_mm256_permutex_epi64'.
// Requires AVX512F.
func M256PermutexEpi64(a x86.M256i, imm8 int) x86.M256i {
	return x86.M256i(m256PermutexEpi64([32]byte(a), imm8))
}

func m256PermutexEpi64(a [32]byte, imm8 int) [32]byte


// M512MaskPermutexEpi64: Shuffle 64-bit integers in 'a' within 256-bit lanes
// using the control in 'imm8', and store the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[63:0] := src[63:0]
//			1:	tmp[63:0] := src[127:64]
//			2:	tmp[63:0] := src[191:128]
//			3:	tmp[63:0] := src[255:192]
//			ESAC
//			RETURN tmp[63:0]
//		}
//		
//		tmp_dst[63:0] := SELECT4(a[255:0], imm8[1:0])
//		tmp_dst[127:64] := SELECT4(a[255:0], imm8[3:2])
//		tmp_dst[191:128] := SELECT4(a[255:0], imm8[5:4])
//		tmp_dst[255:192] := SELECT4(a[255:0], imm8[7:6])
//		tmp_dst[319:256] := SELECT4(a[511:256], imm8[1:0])
//		tmp_dst[383:320] := SELECT4(a[511:256], imm8[3:2])
//		tmp_dst[447:384] := SELECT4(a[511:256], imm8[5:4])
//		tmp_dst[511:448] := SELECT4(a[511:256], imm8[7:6])
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMQ'. Intrinsic: '_mm512_mask_permutex_epi64'.
// Requires AVX512F.
func M512MaskPermutexEpi64(src x86.M512i, k x86.Mmask8, a x86.M512i, imm8 int) x86.M512i {
	return x86.M512i(m512MaskPermutexEpi64([64]byte(src), uint8(k), [64]byte(a), imm8))
}

func m512MaskPermutexEpi64(src [64]byte, k uint8, a [64]byte, imm8 int) [64]byte


// M512MaskzPermutexEpi64: Shuffle 64-bit integers in 'a' within 256-bit lanes
// using the control in 'imm8', and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[63:0] := src[63:0]
//			1:	tmp[63:0] := src[127:64]
//			2:	tmp[63:0] := src[191:128]
//			3:	tmp[63:0] := src[255:192]
//			ESAC
//			RETURN tmp[63:0]
//		}
//		
//		tmp_dst[63:0] := SELECT4(a[255:0], imm8[1:0])
//		tmp_dst[127:64] := SELECT4(a[255:0], imm8[3:2])
//		tmp_dst[191:128] := SELECT4(a[255:0], imm8[5:4])
//		tmp_dst[255:192] := SELECT4(a[255:0], imm8[7:6])
//		tmp_dst[319:256] := SELECT4(a[511:256], imm8[1:0])
//		tmp_dst[383:320] := SELECT4(a[511:256], imm8[3:2])
//		tmp_dst[447:384] := SELECT4(a[511:256], imm8[5:4])
//		tmp_dst[511:448] := SELECT4(a[511:256], imm8[7:6])
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMQ'. Intrinsic: '_mm512_maskz_permutex_epi64'.
// Requires AVX512F.
func M512MaskzPermutexEpi64(k x86.Mmask8, a x86.M512i, imm8 int) x86.M512i {
	return x86.M512i(m512MaskzPermutexEpi64(uint8(k), [64]byte(a), imm8))
}

func m512MaskzPermutexEpi64(k uint8, a [64]byte, imm8 int) [64]byte


// M512PermutexEpi64: Shuffle 64-bit integers in 'a' within 256-bit lanes using
// the control in 'imm8', and store the results in 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[63:0] := src[63:0]
//			1:	tmp[63:0] := src[127:64]
//			2:	tmp[63:0] := src[191:128]
//			3:	tmp[63:0] := src[255:192]
//			ESAC
//			RETURN tmp[63:0]
//		}
//		
//		dst[63:0] := SELECT4(a[255:0], imm8[1:0])
//		dst[127:64] := SELECT4(a[255:0], imm8[3:2])
//		dst[191:128] := SELECT4(a[255:0], imm8[5:4])
//		dst[255:192] := SELECT4(a[255:0], imm8[7:6])
//		dst[319:256] := SELECT4(a[511:256], imm8[1:0])
//		dst[383:320] := SELECT4(a[511:256], imm8[3:2])
//		dst[447:384] := SELECT4(a[511:256], imm8[5:4])
//		dst[511:448] := SELECT4(a[511:256], imm8[7:6])
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMQ'. Intrinsic: '_mm512_permutex_epi64'.
// Requires AVX512F.
func M512PermutexEpi64(a x86.M512i, imm8 int) x86.M512i {
	return x86.M512i(m512PermutexEpi64([64]byte(a), imm8))
}

func m512PermutexEpi64(a [64]byte, imm8 int) [64]byte


// M256MaskPermutexPd: Shuffle double-precision (64-bit) floating-point
// elements in 'a' across lanes using the control in 'imm8', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[63:0] := src[63:0]
//			1:	tmp[63:0] := src[127:64]
//			2:	tmp[63:0] := src[191:128]
//			3:	tmp[63:0] := src[255:192]
//			ESAC
//			RETURN tmp[63:0]
//		}
//		
//		tmp_dst[63:0] := SELECT4(a[255:0], imm8[1:0])
//		tmp_dst[127:64] := SELECT4(a[255:0], imm8[3:2])
//		tmp_dst[191:128] := SELECT4(a[255:0], imm8[5:4])
//		tmp_dst[255:192] := SELECT4(a[255:0], imm8[7:6])
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMPD'. Intrinsic: '_mm256_mask_permutex_pd'.
// Requires AVX512F.
func M256MaskPermutexPd(src x86.M256d, k x86.Mmask8, a x86.M256d, imm8 int) x86.M256d {
	return x86.M256d(m256MaskPermutexPd([4]float64(src), uint8(k), [4]float64(a), imm8))
}

func m256MaskPermutexPd(src [4]float64, k uint8, a [4]float64, imm8 int) [4]float64


// M256MaskzPermutexPd: Shuffle double-precision (64-bit) floating-point
// elements in 'a' across lanes using the control in 'imm8', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[63:0] := src[63:0]
//			1:	tmp[63:0] := src[127:64]
//			2:	tmp[63:0] := src[191:128]
//			3:	tmp[63:0] := src[255:192]
//			ESAC
//			RETURN tmp[63:0]
//		}
//		
//		tmp_dst[63:0] := SELECT4(a[255:0], imm8[1:0])
//		tmp_dst[127:64] := SELECT4(a[255:0], imm8[3:2])
//		tmp_dst[191:128] := SELECT4(a[255:0], imm8[5:4])
//		tmp_dst[255:192] := SELECT4(a[255:0], imm8[7:6])
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMPD'. Intrinsic: '_mm256_maskz_permutex_pd'.
// Requires AVX512F.
func M256MaskzPermutexPd(k x86.Mmask8, a x86.M256d, imm8 int) x86.M256d {
	return x86.M256d(m256MaskzPermutexPd(uint8(k), [4]float64(a), imm8))
}

func m256MaskzPermutexPd(k uint8, a [4]float64, imm8 int) [4]float64


// M256PermutexPd: Shuffle double-precision (64-bit) floating-point elements in
// 'a' across lanes using the control in 'imm8', and store the results in
// 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[63:0] := src[63:0]
//			1:	tmp[63:0] := src[127:64]
//			2:	tmp[63:0] := src[191:128]
//			3:	tmp[63:0] := src[255:192]
//			ESAC
//			RETURN tmp[63:0]
//		}
//		
//		dst[63:0] := SELECT4(a[255:0], imm8[1:0])
//		dst[127:64] := SELECT4(a[255:0], imm8[3:2])
//		dst[191:128] := SELECT4(a[255:0], imm8[5:4])
//		dst[255:192] := SELECT4(a[255:0], imm8[7:6])
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMPD'. Intrinsic: '_mm256_permutex_pd'.
// Requires AVX512F.
func M256PermutexPd(a x86.M256d, imm8 int) x86.M256d {
	return x86.M256d(m256PermutexPd([4]float64(a), imm8))
}

func m256PermutexPd(a [4]float64, imm8 int) [4]float64


// M512MaskPermutexPd: Shuffle double-precision (64-bit) floating-point
// elements in 'a' within 256-bit lanes using the control in 'imm8', and store
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[63:0] := src[63:0]
//			1:	tmp[63:0] := src[127:64]
//			2:	tmp[63:0] := src[191:128]
//			3:	tmp[63:0] := src[255:192]
//			ESAC
//			RETURN tmp[63:0]
//		}
//		
//		tmp_dst[63:0] := SELECT4(a[255:0], imm8[1:0])
//		tmp_dst[127:64] := SELECT4(a[255:0], imm8[3:2])
//		tmp_dst[191:128] := SELECT4(a[255:0], imm8[5:4])
//		tmp_dst[255:192] := SELECT4(a[255:0], imm8[7:6])
//		tmp_dst[319:256] := SELECT4(a[511:256], imm8[1:0])
//		tmp_dst[383:320] := SELECT4(a[511:256], imm8[3:2])
//		tmp_dst[447:384] := SELECT4(a[511:256], imm8[5:4])
//		tmp_dst[511:448] := SELECT4(a[511:256], imm8[7:6])
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMPD'. Intrinsic: '_mm512_mask_permutex_pd'.
// Requires AVX512F.
func M512MaskPermutexPd(src x86.M512d, k x86.Mmask8, a x86.M512d, imm8 int) x86.M512d {
	return x86.M512d(m512MaskPermutexPd([8]float64(src), uint8(k), [8]float64(a), imm8))
}

func m512MaskPermutexPd(src [8]float64, k uint8, a [8]float64, imm8 int) [8]float64


// M512MaskzPermutexPd: Shuffle double-precision (64-bit) floating-point
// elements in 'a' within 256-bit lanes using the control in 'imm8', and store
// the results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[63:0] := src[63:0]
//			1:	tmp[63:0] := src[127:64]
//			2:	tmp[63:0] := src[191:128]
//			3:	tmp[63:0] := src[255:192]
//			ESAC
//			RETURN tmp[63:0]
//		}
//		
//		tmp_dst[63:0] := SELECT4(a[255:0], imm8[1:0])
//		tmp_dst[127:64] := SELECT4(a[255:0], imm8[3:2])
//		tmp_dst[191:128] := SELECT4(a[255:0], imm8[5:4])
//		tmp_dst[255:192] := SELECT4(a[255:0], imm8[7:6])
//		tmp_dst[319:256] := SELECT4(a[511:256], imm8[1:0])
//		tmp_dst[383:320] := SELECT4(a[511:256], imm8[3:2])
//		tmp_dst[447:384] := SELECT4(a[511:256], imm8[5:4])
//		tmp_dst[511:448] := SELECT4(a[511:256], imm8[7:6])
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMPD'. Intrinsic: '_mm512_maskz_permutex_pd'.
// Requires AVX512F.
func M512MaskzPermutexPd(k x86.Mmask8, a x86.M512d, imm8 int) x86.M512d {
	return x86.M512d(m512MaskzPermutexPd(uint8(k), [8]float64(a), imm8))
}

func m512MaskzPermutexPd(k uint8, a [8]float64, imm8 int) [8]float64


// M512PermutexPd: Shuffle double-precision (64-bit) floating-point elements in
// 'a' within 256-bit lanes using the control in 'imm8', and store the results
// in 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[63:0] := src[63:0]
//			1:	tmp[63:0] := src[127:64]
//			2:	tmp[63:0] := src[191:128]
//			3:	tmp[63:0] := src[255:192]
//			ESAC
//			RETURN tmp[63:0]
//		}
//		
//		dst[63:0] := SELECT4(a[255:0], imm8[1:0])
//		dst[127:64] := SELECT4(a[255:0], imm8[3:2])
//		dst[191:128] := SELECT4(a[255:0], imm8[5:4])
//		dst[255:192] := SELECT4(a[255:0], imm8[7:6])
//		dst[319:256] := SELECT4(a[511:256], imm8[1:0])
//		dst[383:320] := SELECT4(a[511:256], imm8[3:2])
//		dst[447:384] := SELECT4(a[511:256], imm8[5:4])
//		dst[511:448] := SELECT4(a[511:256], imm8[7:6])
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMPD'. Intrinsic: '_mm512_permutex_pd'.
// Requires AVX512F.
func M512PermutexPd(a x86.M512d, imm8 int) x86.M512d {
	return x86.M512d(m512PermutexPd([8]float64(a), imm8))
}

func m512PermutexPd(a [8]float64, imm8 int) [8]float64


// MaskPermutex2varEpi32: Shuffle 32-bit integers in 'a' and 'b' using the
// corresponding selector and index in 'idx', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'a' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			off := idx[i+1:i]*32
//			IF k[j]
//				dst[i+31:i] := idx[i+2] ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMT2D'. Intrinsic: '_mm_mask_permutex2var_epi32'.
// Requires AVX512F.
func MaskPermutex2varEpi32(a x86.M128i, k x86.Mmask8, idx x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskPermutex2varEpi32([16]byte(a), uint8(k), [16]byte(idx), [16]byte(b)))
}

func maskPermutex2varEpi32(a [16]byte, k uint8, idx [16]byte, b [16]byte) [16]byte


// Mask2Permutex2varEpi32: Shuffle 32-bit integers in 'a' and 'b' using the
// corresponding selector and index in 'idx', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'idx' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			off := idx[i+1:i]*32
//			IF k[j]
//				dst[i+31:i] := idx[i+2] ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := idx[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMI2D'. Intrinsic: '_mm_mask2_permutex2var_epi32'.
// Requires AVX512F.
func Mask2Permutex2varEpi32(a x86.M128i, idx x86.M128i, k x86.Mmask8, b x86.M128i) x86.M128i {
	return x86.M128i(mask2Permutex2varEpi32([16]byte(a), [16]byte(idx), uint8(k), [16]byte(b)))
}

func mask2Permutex2varEpi32(a [16]byte, idx [16]byte, k uint8, b [16]byte) [16]byte


// MaskzPermutex2varEpi32: Shuffle 32-bit integers in 'a' and 'b' using the
// corresponding selector and index in 'idx', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			off := idx[i+1:i]*32
//			IF k[j]
//				dst[i+31:i] := (idx[i+2]) ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMI2D, VPERMT2D'. Intrinsic: '_mm_maskz_permutex2var_epi32'.
// Requires AVX512F.
func MaskzPermutex2varEpi32(k x86.Mmask8, a x86.M128i, idx x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskzPermutex2varEpi32(uint8(k), [16]byte(a), [16]byte(idx), [16]byte(b)))
}

func maskzPermutex2varEpi32(k uint8, a [16]byte, idx [16]byte, b [16]byte) [16]byte


// Permutex2varEpi32: Shuffle 32-bit integers in 'a' and 'b' using the
// corresponding selector and index in 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			off := idx[i+2:i]*32
//			dst[i+31:i] := idx[i+3] ? b[off+31:off] : a[off+31:off]
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMI2D, VPERMT2D'. Intrinsic: '_mm_permutex2var_epi32'.
// Requires AVX512F.
func Permutex2varEpi32(a x86.M128i, idx x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(permutex2varEpi32([16]byte(a), [16]byte(idx), [16]byte(b)))
}

func permutex2varEpi32(a [16]byte, idx [16]byte, b [16]byte) [16]byte


// M256MaskPermutex2varEpi32: Shuffle 32-bit integers in 'a' and 'b' across
// lanes using the corresponding selector and index in 'idx', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			off := idx[i+3:i]*32
//			IF k[j]
//				dst[i+31:i] := idx[i+4] ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMT2D'. Intrinsic: '_mm256_mask_permutex2var_epi32'.
// Requires AVX512F.
func M256MaskPermutex2varEpi32(a x86.M256i, k x86.Mmask8, idx x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskPermutex2varEpi32([32]byte(a), uint8(k), [32]byte(idx), [32]byte(b)))
}

func m256MaskPermutex2varEpi32(a [32]byte, k uint8, idx [32]byte, b [32]byte) [32]byte


// M256Mask2Permutex2varEpi32: Shuffle 32-bit integers in 'a' and 'b' across
// lanes using the corresponding selector and index in 'idx', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'idx' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			off := idx[i+2:i]*32
//			IF k[j]
//				dst[i+31:i] := idx[i+3] ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := idx[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2D'. Intrinsic: '_mm256_mask2_permutex2var_epi32'.
// Requires AVX512F.
func M256Mask2Permutex2varEpi32(a x86.M256i, idx x86.M256i, k x86.Mmask8, b x86.M256i) x86.M256i {
	return x86.M256i(m256Mask2Permutex2varEpi32([32]byte(a), [32]byte(idx), uint8(k), [32]byte(b)))
}

func m256Mask2Permutex2varEpi32(a [32]byte, idx [32]byte, k uint8, b [32]byte) [32]byte


// M256MaskzPermutex2varEpi32: Shuffle 32-bit integers in 'a' and 'b' across
// lanes using the corresponding selector and index in 'idx', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			off := idx[i+2:i]*32
//			IF k[j]
//				dst[i+31:i] := (idx[i+3]) ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2D, VPERMT2D'. Intrinsic: '_mm256_maskz_permutex2var_epi32'.
// Requires AVX512F.
func M256MaskzPermutex2varEpi32(k x86.Mmask8, a x86.M256i, idx x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzPermutex2varEpi32(uint8(k), [32]byte(a), [32]byte(idx), [32]byte(b)))
}

func m256MaskzPermutex2varEpi32(k uint8, a [32]byte, idx [32]byte, b [32]byte) [32]byte


// M256Permutex2varEpi32: Shuffle 32-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			off := idx[i+2:i]*32
//			dst[i+31:i] := idx[i+3] ? b[off+31:off] : a[off+31:off]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2D, VPERMT2D'. Intrinsic: '_mm256_permutex2var_epi32'.
// Requires AVX512F.
func M256Permutex2varEpi32(a x86.M256i, idx x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256Permutex2varEpi32([32]byte(a), [32]byte(idx), [32]byte(b)))
}

func m256Permutex2varEpi32(a [32]byte, idx [32]byte, b [32]byte) [32]byte


// M512MaskPermutex2varEpi32: Shuffle 32-bit integers in 'a' and 'b' across
// lanes using the corresponding selector and index in 'idx', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			off := idx[i+3:i]*32
//			IF k[j]
//				dst[i+31:i] := idx[i+4] ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMT2D'. Intrinsic: '_mm512_mask_permutex2var_epi32'.
// Requires AVX512F.
func M512MaskPermutex2varEpi32(a x86.M512i, k x86.Mmask16, idx x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskPermutex2varEpi32([64]byte(a), uint16(k), [64]byte(idx), [64]byte(b)))
}

func m512MaskPermutex2varEpi32(a [64]byte, k uint16, idx [64]byte, b [64]byte) [64]byte


// M512Mask2Permutex2varEpi32: Shuffle 32-bit integers in 'a' and 'b' across
// lanes using the corresponding selector and index in 'idx', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'idx' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			off := idx[i+3:i]*32
//			IF k[j]
//				dst[i+31:i] := idx[i+4] ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := idx[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMI2D'. Intrinsic: '_mm512_mask2_permutex2var_epi32'.
// Requires AVX512F.
func M512Mask2Permutex2varEpi32(a x86.M512i, idx x86.M512i, k x86.Mmask16, b x86.M512i) x86.M512i {
	return x86.M512i(m512Mask2Permutex2varEpi32([64]byte(a), [64]byte(idx), uint16(k), [64]byte(b)))
}

func m512Mask2Permutex2varEpi32(a [64]byte, idx [64]byte, k uint16, b [64]byte) [64]byte


// M512MaskzPermutex2varEpi32: Shuffle 32-bit integers in 'a' and 'b' across
// lanes using the corresponding selector and index in 'idx', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			off := idx[i+3:i]*32
//			IF k[j]
//				dst[i+31:i] := (idx[i+4]) ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMI2D, VPERMT2D'. Intrinsic: '_mm512_maskz_permutex2var_epi32'.
// Requires AVX512F.
func M512MaskzPermutex2varEpi32(k x86.Mmask16, a x86.M512i, idx x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzPermutex2varEpi32(uint16(k), [64]byte(a), [64]byte(idx), [64]byte(b)))
}

func m512MaskzPermutex2varEpi32(k uint16, a [64]byte, idx [64]byte, b [64]byte) [64]byte


// M512Permutex2varEpi32: Shuffle 32-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			off := idx[i+3:i]*32
//			dst[i+31:i] := idx[i+4] ? b[off+31:off] : a[off+31:off]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMI2D, VPERMT2D'. Intrinsic: '_mm512_permutex2var_epi32'.
// Requires AVX512F.
func M512Permutex2varEpi32(a x86.M512i, idx x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512Permutex2varEpi32([64]byte(a), [64]byte(idx), [64]byte(b)))
}

func m512Permutex2varEpi32(a [64]byte, idx [64]byte, b [64]byte) [64]byte


// MaskPermutex2varEpi64: Shuffle 64-bit integers in 'a' and 'b' using the
// corresponding selector and index in 'idx', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'a' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			off := idx[i]*64
//			IF k[j]
//				dst[i+63:i] := idx[i+1] ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMT2Q'. Intrinsic: '_mm_mask_permutex2var_epi64'.
// Requires AVX512F.
func MaskPermutex2varEpi64(a x86.M128i, k x86.Mmask8, idx x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskPermutex2varEpi64([16]byte(a), uint8(k), [16]byte(idx), [16]byte(b)))
}

func maskPermutex2varEpi64(a [16]byte, k uint8, idx [16]byte, b [16]byte) [16]byte


// Mask2Permutex2varEpi64: Shuffle 64-bit integers in 'a' and 'b' using the
// corresponding selector and index in 'idx', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'idx' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			off := idx[i]*64
//			IF k[j]
//				dst[i+63:i] := idx[i+1] ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := idx[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMI2Q'. Intrinsic: '_mm_mask2_permutex2var_epi64'.
// Requires AVX512F.
func Mask2Permutex2varEpi64(a x86.M128i, idx x86.M128i, k x86.Mmask8, b x86.M128i) x86.M128i {
	return x86.M128i(mask2Permutex2varEpi64([16]byte(a), [16]byte(idx), uint8(k), [16]byte(b)))
}

func mask2Permutex2varEpi64(a [16]byte, idx [16]byte, k uint8, b [16]byte) [16]byte


// MaskzPermutex2varEpi64: Shuffle 64-bit integers in 'a' and 'b' using the
// corresponding selector and index in 'idx', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			off := idx[i]*64
//			IF k[j]
//				dst[i+63:i] := (idx[i+1]) ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMI2Q, VPERMT2Q'. Intrinsic: '_mm_maskz_permutex2var_epi64'.
// Requires AVX512F.
func MaskzPermutex2varEpi64(k x86.Mmask8, a x86.M128i, idx x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskzPermutex2varEpi64(uint8(k), [16]byte(a), [16]byte(idx), [16]byte(b)))
}

func maskzPermutex2varEpi64(k uint8, a [16]byte, idx [16]byte, b [16]byte) [16]byte


// Permutex2varEpi64: Shuffle 64-bit integers in 'a' and 'b' using the
// corresponding selector and index in 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			off := idx[i]*64
//			dst[i+63:i] := idx[i+1] ? b[off+63:off] : a[off+63:off]
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMI2Q, VPERMT2Q'. Intrinsic: '_mm_permutex2var_epi64'.
// Requires AVX512F.
func Permutex2varEpi64(a x86.M128i, idx x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(permutex2varEpi64([16]byte(a), [16]byte(idx), [16]byte(b)))
}

func permutex2varEpi64(a [16]byte, idx [16]byte, b [16]byte) [16]byte


// M256MaskPermutex2varEpi64: Shuffle 64-bit integers in 'a' and 'b' across
// lanes using the corresponding selector and index in 'idx', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			off := idx[i+1:i]*64
//			IF k[j]
//				dst[i+63:i] := idx[i+2] ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMT2Q'. Intrinsic: '_mm256_mask_permutex2var_epi64'.
// Requires AVX512F.
func M256MaskPermutex2varEpi64(a x86.M256i, k x86.Mmask8, idx x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskPermutex2varEpi64([32]byte(a), uint8(k), [32]byte(idx), [32]byte(b)))
}

func m256MaskPermutex2varEpi64(a [32]byte, k uint8, idx [32]byte, b [32]byte) [32]byte


// M256Mask2Permutex2varEpi64: Shuffle 64-bit integers in 'a' and 'b' across
// lanes using the corresponding selector and index in 'idx', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'idx' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			off := idx[i+1:i]*64
//			IF k[j]
//				dst[i+63:i] := idx[i+2] ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := idx[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2Q'. Intrinsic: '_mm256_mask2_permutex2var_epi64'.
// Requires AVX512F.
func M256Mask2Permutex2varEpi64(a x86.M256i, idx x86.M256i, k x86.Mmask8, b x86.M256i) x86.M256i {
	return x86.M256i(m256Mask2Permutex2varEpi64([32]byte(a), [32]byte(idx), uint8(k), [32]byte(b)))
}

func m256Mask2Permutex2varEpi64(a [32]byte, idx [32]byte, k uint8, b [32]byte) [32]byte


// M256MaskzPermutex2varEpi64: Shuffle 64-bit integers in 'a' and 'b' across
// lanes using the corresponding selector and index in 'idx', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			off := idx[i+1:i]*64
//			IF k[j]
//				dst[i+63:i] := (idx[i+2]) ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2Q, VPERMT2Q'. Intrinsic: '_mm256_maskz_permutex2var_epi64'.
// Requires AVX512F.
func M256MaskzPermutex2varEpi64(k x86.Mmask8, a x86.M256i, idx x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzPermutex2varEpi64(uint8(k), [32]byte(a), [32]byte(idx), [32]byte(b)))
}

func m256MaskzPermutex2varEpi64(k uint8, a [32]byte, idx [32]byte, b [32]byte) [32]byte


// M256Permutex2varEpi64: Shuffle 64-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			off := idx[i+1:i]*64
//			dst[i+63:i] := idx[i+2] ? b[off+63:off] : a[off+63:off]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2Q, VPERMT2Q'. Intrinsic: '_mm256_permutex2var_epi64'.
// Requires AVX512F.
func M256Permutex2varEpi64(a x86.M256i, idx x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256Permutex2varEpi64([32]byte(a), [32]byte(idx), [32]byte(b)))
}

func m256Permutex2varEpi64(a [32]byte, idx [32]byte, b [32]byte) [32]byte


// M512MaskPermutex2varEpi64: Shuffle 64-bit integers in 'a' and 'b' across
// lanes using the corresponding selector and index in 'idx', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			off := idx[i+2:i]*64
//			IF k[j]
//				dst[i+63:i] := idx[i+3] ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMT2Q'. Intrinsic: '_mm512_mask_permutex2var_epi64'.
// Requires AVX512F.
func M512MaskPermutex2varEpi64(a x86.M512i, k x86.Mmask8, idx x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskPermutex2varEpi64([64]byte(a), uint8(k), [64]byte(idx), [64]byte(b)))
}

func m512MaskPermutex2varEpi64(a [64]byte, k uint8, idx [64]byte, b [64]byte) [64]byte


// M512Mask2Permutex2varEpi64: Shuffle 64-bit integers in 'a' and 'b' across
// lanes using the corresponding selector and index in 'idx', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'idx' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			off := idx[i+2:i]*64
//			IF k[j]
//				dst[i+63:i] := idx[i+3] ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := idx[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMI2Q'. Intrinsic: '_mm512_mask2_permutex2var_epi64'.
// Requires AVX512F.
func M512Mask2Permutex2varEpi64(a x86.M512i, idx x86.M512i, k x86.Mmask8, b x86.M512i) x86.M512i {
	return x86.M512i(m512Mask2Permutex2varEpi64([64]byte(a), [64]byte(idx), uint8(k), [64]byte(b)))
}

func m512Mask2Permutex2varEpi64(a [64]byte, idx [64]byte, k uint8, b [64]byte) [64]byte


// M512MaskzPermutex2varEpi64: Shuffle 64-bit integers in 'a' and 'b' across
// lanes using the corresponding selector and index in 'idx', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			off := idx[i+2:i]*64
//			IF k[j]
//				dst[i+63:i] := (idx[i+3]) ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMI2Q, VPERMT2Q'. Intrinsic: '_mm512_maskz_permutex2var_epi64'.
// Requires AVX512F.
func M512MaskzPermutex2varEpi64(k x86.Mmask8, a x86.M512i, idx x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzPermutex2varEpi64(uint8(k), [64]byte(a), [64]byte(idx), [64]byte(b)))
}

func m512MaskzPermutex2varEpi64(k uint8, a [64]byte, idx [64]byte, b [64]byte) [64]byte


// M512Permutex2varEpi64: Shuffle 64-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			off := idx[i+2:i]*64
//			dst[i+63:i] := idx[i+3] ? b[off+63:off] : a[off+63:off]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMI2Q, VPERMT2Q'. Intrinsic: '_mm512_permutex2var_epi64'.
// Requires AVX512F.
func M512Permutex2varEpi64(a x86.M512i, idx x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512Permutex2varEpi64([64]byte(a), [64]byte(idx), [64]byte(b)))
}

func m512Permutex2varEpi64(a [64]byte, idx [64]byte, b [64]byte) [64]byte


// MaskPermutex2varPd: Shuffle double-precision (64-bit) floating-point
// elements in 'a' and 'b' using the corresponding selector and index in 'idx',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			off := idx[i]*64
//			IF k[j]
//				dst[i+63:i] := idx[i+1] ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMT2PD'. Intrinsic: '_mm_mask_permutex2var_pd'.
// Requires AVX512F.
func MaskPermutex2varPd(a x86.M128d, k x86.Mmask8, idx x86.M128i, b x86.M128d) x86.M128d {
	return x86.M128d(maskPermutex2varPd([2]float64(a), uint8(k), [16]byte(idx), [2]float64(b)))
}

func maskPermutex2varPd(a [2]float64, k uint8, idx [16]byte, b [2]float64) [2]float64


// Mask2Permutex2varPd: Shuffle double-precision (64-bit) floating-point
// elements in 'a' and 'b' using the corresponding selector and index in 'idx',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'idx' when the corresponding mask bit is not set) 
//
//		FOR j := 0 to 1
//			i := j*64
//			off := idx[i]*64
//			IF k[j]
//				dst[i+63:i] := idx[i+1] ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := idx[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMI2PD'. Intrinsic: '_mm_mask2_permutex2var_pd'.
// Requires AVX512F.
func Mask2Permutex2varPd(a x86.M128d, idx x86.M128i, k x86.Mmask8, b x86.M128d) x86.M128d {
	return x86.M128d(mask2Permutex2varPd([2]float64(a), [16]byte(idx), uint8(k), [2]float64(b)))
}

func mask2Permutex2varPd(a [2]float64, idx [16]byte, k uint8, b [2]float64) [2]float64


// MaskzPermutex2varPd: Shuffle double-precision (64-bit) floating-point
// elements in 'a' and 'b' using the corresponding selector and index in 'idx',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			off := idx[i]*64
//			IF k[j]
//				dst[i+63:i] := (idx[i+1]) ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMI2PD, VPERMT2PD'. Intrinsic: '_mm_maskz_permutex2var_pd'.
// Requires AVX512F.
func MaskzPermutex2varPd(k x86.Mmask8, a x86.M128d, idx x86.M128i, b x86.M128d) x86.M128d {
	return x86.M128d(maskzPermutex2varPd(uint8(k), [2]float64(a), [16]byte(idx), [2]float64(b)))
}

func maskzPermutex2varPd(k uint8, a [2]float64, idx [16]byte, b [2]float64) [2]float64


// Permutex2varPd: Shuffle double-precision (64-bit) floating-point elements in
// 'a' and 'b' using the corresponding selector and index in 'idx', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			off := idx[i]*64
//			dst[i+63:i] := idx[i+1] ? b[off+63:off] : a[off+63:off]
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMI2PD, VPERMT2PD'. Intrinsic: '_mm_permutex2var_pd'.
// Requires AVX512F.
func Permutex2varPd(a x86.M128d, idx x86.M128i, b x86.M128d) x86.M128d {
	return x86.M128d(permutex2varPd([2]float64(a), [16]byte(idx), [2]float64(b)))
}

func permutex2varPd(a [2]float64, idx [16]byte, b [2]float64) [2]float64


// M256MaskPermutex2varPd: Shuffle double-precision (64-bit) floating-point
// elements in 'a' and 'b' across lanes using the corresponding selector and
// index in 'idx', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			off := idx[i+1:i]*64
//			IF k[j]
//				dst[i+63:i] := idx[i+2] ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMT2PD'. Intrinsic: '_mm256_mask_permutex2var_pd'.
// Requires AVX512F.
func M256MaskPermutex2varPd(a x86.M256d, k x86.Mmask8, idx x86.M256i, b x86.M256d) x86.M256d {
	return x86.M256d(m256MaskPermutex2varPd([4]float64(a), uint8(k), [32]byte(idx), [4]float64(b)))
}

func m256MaskPermutex2varPd(a [4]float64, k uint8, idx [32]byte, b [4]float64) [4]float64


// M256Mask2Permutex2varPd: Shuffle double-precision (64-bit) floating-point
// elements in 'a' and 'b' across lanes using the corresponding selector and
// index in 'idx', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'idx' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			off := idx[i+1:i]*64
//			IF k[j]
//				dst[i+63:i] := idx[i+2] ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := idx[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2PD'. Intrinsic: '_mm256_mask2_permutex2var_pd'.
// Requires AVX512F.
func M256Mask2Permutex2varPd(a x86.M256d, idx x86.M256i, k x86.Mmask8, b x86.M256d) x86.M256d {
	return x86.M256d(m256Mask2Permutex2varPd([4]float64(a), [32]byte(idx), uint8(k), [4]float64(b)))
}

func m256Mask2Permutex2varPd(a [4]float64, idx [32]byte, k uint8, b [4]float64) [4]float64


// M256MaskzPermutex2varPd: Shuffle double-precision (64-bit) floating-point
// elements in 'a' and 'b' across lanes using the corresponding selector and
// index in 'idx', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			off := idx[i+1:i]*64
//			IF k[j]
//				dst[i+63:i] := (idx[i+2]) ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2PD, VPERMT2PD'. Intrinsic: '_mm256_maskz_permutex2var_pd'.
// Requires AVX512F.
func M256MaskzPermutex2varPd(k x86.Mmask8, a x86.M256d, idx x86.M256i, b x86.M256d) x86.M256d {
	return x86.M256d(m256MaskzPermutex2varPd(uint8(k), [4]float64(a), [32]byte(idx), [4]float64(b)))
}

func m256MaskzPermutex2varPd(k uint8, a [4]float64, idx [32]byte, b [4]float64) [4]float64


// M256Permutex2varPd: Shuffle double-precision (64-bit) floating-point
// elements in 'a' and 'b' across lanes using the corresponding selector and
// index in 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			off := idx[i+1:i]*64
//			dst[i+63:i] := idx[i+2] ? b[off+63:off] : a[off+63:off]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2PD, VPERMT2PD'. Intrinsic: '_mm256_permutex2var_pd'.
// Requires AVX512F.
func M256Permutex2varPd(a x86.M256d, idx x86.M256i, b x86.M256d) x86.M256d {
	return x86.M256d(m256Permutex2varPd([4]float64(a), [32]byte(idx), [4]float64(b)))
}

func m256Permutex2varPd(a [4]float64, idx [32]byte, b [4]float64) [4]float64


// M512MaskPermutex2varPd: Shuffle double-precision (64-bit) floating-point
// elements in 'a' and 'b' across lanes using the corresponding selector and
// index in 'idx', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			off := idx[i+2:i]*64
//			IF k[j]
//				dst[i+63:i] := idx[i+3] ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMT2PD'. Intrinsic: '_mm512_mask_permutex2var_pd'.
// Requires AVX512F.
func M512MaskPermutex2varPd(a x86.M512d, k x86.Mmask8, idx x86.M512i, b x86.M512d) x86.M512d {
	return x86.M512d(m512MaskPermutex2varPd([8]float64(a), uint8(k), [64]byte(idx), [8]float64(b)))
}

func m512MaskPermutex2varPd(a [8]float64, k uint8, idx [64]byte, b [8]float64) [8]float64


// M512Mask2Permutex2varPd: Shuffle double-precision (64-bit) floating-point
// elements in 'a' and 'b' across lanes using the corresponding selector and
// index in 'idx', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'idx' when the corresponding mask bit is not set) 
//
//		FOR j := 0 to 7
//			i := j*64
//			off := idx[i+2:i]*64
//			IF k[j]
//				dst[i+63:i] := idx[i+3] ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := idx[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMI2PD'. Intrinsic: '_mm512_mask2_permutex2var_pd'.
// Requires AVX512F.
func M512Mask2Permutex2varPd(a x86.M512d, idx x86.M512i, k x86.Mmask8, b x86.M512d) x86.M512d {
	return x86.M512d(m512Mask2Permutex2varPd([8]float64(a), [64]byte(idx), uint8(k), [8]float64(b)))
}

func m512Mask2Permutex2varPd(a [8]float64, idx [64]byte, k uint8, b [8]float64) [8]float64


// M512MaskzPermutex2varPd: Shuffle double-precision (64-bit) floating-point
// elements in 'a' and 'b' across lanes using the corresponding selector and
// index in 'idx', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			off := idx[i+2:i]*64
//			IF k[j]
//				dst[i+63:i] := (idx[i+3]) ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMI2PD, VPERMT2PD'. Intrinsic: '_mm512_maskz_permutex2var_pd'.
// Requires AVX512F.
func M512MaskzPermutex2varPd(k x86.Mmask8, a x86.M512d, idx x86.M512i, b x86.M512d) x86.M512d {
	return x86.M512d(m512MaskzPermutex2varPd(uint8(k), [8]float64(a), [64]byte(idx), [8]float64(b)))
}

func m512MaskzPermutex2varPd(k uint8, a [8]float64, idx [64]byte, b [8]float64) [8]float64


// M512Permutex2varPd: Shuffle double-precision (64-bit) floating-point
// elements in 'a' and 'b' across lanes using the corresponding selector and
// index in 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			off := idx[i+2:i]*64
//			dst[i+63:i] := idx[i+3] ? b[off+63:off] : a[off+63:off]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMI2PD, VPERMT2PD'. Intrinsic: '_mm512_permutex2var_pd'.
// Requires AVX512F.
func M512Permutex2varPd(a x86.M512d, idx x86.M512i, b x86.M512d) x86.M512d {
	return x86.M512d(m512Permutex2varPd([8]float64(a), [64]byte(idx), [8]float64(b)))
}

func m512Permutex2varPd(a [8]float64, idx [64]byte, b [8]float64) [8]float64


// MaskPermutex2varPs: Shuffle single-precision (32-bit) floating-point
// elements in 'a' and 'b' using the corresponding selector and index in 'idx',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			off := idx[i+1:i]*32
//			IF k[j]
//				dst[i+31:i] := idx[i+2] ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMT2PS'. Intrinsic: '_mm_mask_permutex2var_ps'.
// Requires AVX512F.
func MaskPermutex2varPs(a x86.M128, k x86.Mmask8, idx x86.M128i, b x86.M128) x86.M128 {
	return x86.M128(maskPermutex2varPs([4]float32(a), uint8(k), [16]byte(idx), [4]float32(b)))
}

func maskPermutex2varPs(a [4]float32, k uint8, idx [16]byte, b [4]float32) [4]float32


// Mask2Permutex2varPs: Shuffle single-precision (32-bit) floating-point
// elements in 'a' and 'b' using the corresponding selector and index in 'idx',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'idx' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			off := idx[i+1:i]*32
//			IF k[j]
//				dst[i+31:i] := idx[i+2] ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := idx[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMI2PS'. Intrinsic: '_mm_mask2_permutex2var_ps'.
// Requires AVX512F.
func Mask2Permutex2varPs(a x86.M128, idx x86.M128i, k x86.Mmask8, b x86.M128) x86.M128 {
	return x86.M128(mask2Permutex2varPs([4]float32(a), [16]byte(idx), uint8(k), [4]float32(b)))
}

func mask2Permutex2varPs(a [4]float32, idx [16]byte, k uint8, b [4]float32) [4]float32


// MaskzPermutex2varPs: Shuffle single-precision (32-bit) floating-point
// elements in 'a' and 'b' using the corresponding selector and index in 'idx',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			off := idx[i+1:i]*32
//			IF k[j]
//				dst[i+31:i] := (idx[i+2]) ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMI2PS, VPERMT2PS'. Intrinsic: '_mm_maskz_permutex2var_ps'.
// Requires AVX512F.
func MaskzPermutex2varPs(k x86.Mmask8, a x86.M128, idx x86.M128i, b x86.M128) x86.M128 {
	return x86.M128(maskzPermutex2varPs(uint8(k), [4]float32(a), [16]byte(idx), [4]float32(b)))
}

func maskzPermutex2varPs(k uint8, a [4]float32, idx [16]byte, b [4]float32) [4]float32


// Permutex2varPs: Shuffle single-precision (32-bit) floating-point elements in
// 'a' and 'b' using the corresponding selector and index in 'idx', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			off := idx[i+1:i]*32
//			dst[i+31:i] := idx[i+2] ? b[off+31:off] : a[off+31:off]
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMI2PS, VPERMT2PS'. Intrinsic: '_mm_permutex2var_ps'.
// Requires AVX512F.
func Permutex2varPs(a x86.M128, idx x86.M128i, b x86.M128) x86.M128 {
	return x86.M128(permutex2varPs([4]float32(a), [16]byte(idx), [4]float32(b)))
}

func permutex2varPs(a [4]float32, idx [16]byte, b [4]float32) [4]float32


// M256MaskPermutex2varPs: Shuffle single-precision (32-bit) floating-point
// elements in 'a' and 'b' across lanes using the corresponding selector and
// index in 'idx', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			off := idx[i+2:i]*32
//			IF k[j]
//				dst[i+31:i] := idx[i+3] ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMT2PS'. Intrinsic: '_mm256_mask_permutex2var_ps'.
// Requires AVX512F.
func M256MaskPermutex2varPs(a x86.M256, k x86.Mmask8, idx x86.M256i, b x86.M256) x86.M256 {
	return x86.M256(m256MaskPermutex2varPs([8]float32(a), uint8(k), [32]byte(idx), [8]float32(b)))
}

func m256MaskPermutex2varPs(a [8]float32, k uint8, idx [32]byte, b [8]float32) [8]float32


// M256Mask2Permutex2varPs: Shuffle single-precision (32-bit) floating-point
// elements in 'a' and 'b' across lanes using the corresponding selector and
// index in 'idx', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'idx' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			off := idx[i+2:i]*32
//			IF k[j]
//				dst[i+31:i] := idx[i+3] ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := idx[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2PS'. Intrinsic: '_mm256_mask2_permutex2var_ps'.
// Requires AVX512F.
func M256Mask2Permutex2varPs(a x86.M256, idx x86.M256i, k x86.Mmask8, b x86.M256) x86.M256 {
	return x86.M256(m256Mask2Permutex2varPs([8]float32(a), [32]byte(idx), uint8(k), [8]float32(b)))
}

func m256Mask2Permutex2varPs(a [8]float32, idx [32]byte, k uint8, b [8]float32) [8]float32


// M256MaskzPermutex2varPs: Shuffle single-precision (32-bit) floating-point
// elements in 'a' and 'b' across lanes using the corresponding selector and
// index in 'idx', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			off := idx[i+2:i]*32
//			IF k[j]
//				dst[i+31:i] := (idx[i+3]) ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2PS, VPERMT2PS'. Intrinsic: '_mm256_maskz_permutex2var_ps'.
// Requires AVX512F.
func M256MaskzPermutex2varPs(k x86.Mmask8, a x86.M256, idx x86.M256i, b x86.M256) x86.M256 {
	return x86.M256(m256MaskzPermutex2varPs(uint8(k), [8]float32(a), [32]byte(idx), [8]float32(b)))
}

func m256MaskzPermutex2varPs(k uint8, a [8]float32, idx [32]byte, b [8]float32) [8]float32


// M256Permutex2varPs: Shuffle single-precision (32-bit) floating-point
// elements in 'a' and 'b' across lanes using the corresponding selector and
// index in 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			off := idx[i+2:i]*32
//			dst[i+31:i] := idx[i+3] ? b[off+31:off] : a[off+31:off]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2PS, VPERMT2PS'. Intrinsic: '_mm256_permutex2var_ps'.
// Requires AVX512F.
func M256Permutex2varPs(a x86.M256, idx x86.M256i, b x86.M256) x86.M256 {
	return x86.M256(m256Permutex2varPs([8]float32(a), [32]byte(idx), [8]float32(b)))
}

func m256Permutex2varPs(a [8]float32, idx [32]byte, b [8]float32) [8]float32


// M512MaskPermutex2varPs: Shuffle single-precision (32-bit) floating-point
// elements in 'a' and 'b' across lanes using the corresponding selector and
// index in 'idx', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			off := idx[i+3:i]*32
//			IF k[j]
//				dst[i+31:i] := idx[i+4] ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMT2PS'. Intrinsic: '_mm512_mask_permutex2var_ps'.
// Requires AVX512F.
func M512MaskPermutex2varPs(a x86.M512, k x86.Mmask16, idx x86.M512i, b x86.M512) x86.M512 {
	return x86.M512(m512MaskPermutex2varPs([16]float32(a), uint16(k), [64]byte(idx), [16]float32(b)))
}

func m512MaskPermutex2varPs(a [16]float32, k uint16, idx [64]byte, b [16]float32) [16]float32


// M512Mask2Permutex2varPs: Shuffle single-precision (32-bit) floating-point
// elements in 'a' and 'b' across lanes using the corresponding selector and
// index in 'idx', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'idx' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			off := idx[i+3:i]*32
//			IF k[j]
//				dst[i+31:i] := idx[i+4] ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := idx[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMI2PS'. Intrinsic: '_mm512_mask2_permutex2var_ps'.
// Requires AVX512F.
func M512Mask2Permutex2varPs(a x86.M512, idx x86.M512i, k x86.Mmask16, b x86.M512) x86.M512 {
	return x86.M512(m512Mask2Permutex2varPs([16]float32(a), [64]byte(idx), uint16(k), [16]float32(b)))
}

func m512Mask2Permutex2varPs(a [16]float32, idx [64]byte, k uint16, b [16]float32) [16]float32


// M512MaskzPermutex2varPs: Shuffle single-precision (32-bit) floating-point
// elements in 'a' and 'b' across lanes using the corresponding selector and
// index in 'idx', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			off := idx[i+3:i]*32
//			IF k[j]
//				dst[i+31:i] := (idx[i+4]) ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMI2PS, VPERMT2PS'. Intrinsic: '_mm512_maskz_permutex2var_ps'.
// Requires AVX512F.
func M512MaskzPermutex2varPs(k x86.Mmask16, a x86.M512, idx x86.M512i, b x86.M512) x86.M512 {
	return x86.M512(m512MaskzPermutex2varPs(uint16(k), [16]float32(a), [64]byte(idx), [16]float32(b)))
}

func m512MaskzPermutex2varPs(k uint16, a [16]float32, idx [64]byte, b [16]float32) [16]float32


// M512Permutex2varPs: Shuffle single-precision (32-bit) floating-point
// elements in 'a' and 'b' across lanes using the corresponding selector and
// index in 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			off := idx[i+3:i]*32
//			dst[i+31:i] := idx[i+4] ? b[off+31:off] : a[off+31:off]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMI2PS, VPERMT2PS'. Intrinsic: '_mm512_permutex2var_ps'.
// Requires AVX512F.
func M512Permutex2varPs(a x86.M512, idx x86.M512i, b x86.M512) x86.M512 {
	return x86.M512(m512Permutex2varPs([16]float32(a), [64]byte(idx), [16]float32(b)))
}

func m512Permutex2varPs(a [16]float32, idx [64]byte, b [16]float32) [16]float32


// M256MaskPermutexvarEpi32: Shuffle 32-bit integers in 'a' across lanes using
// the corresponding index in 'idx', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			id := idx[i+2:i]*32
//			IF k[j]
//				dst[i+31:i] := a[id+31:id]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMD'. Intrinsic: '_mm256_mask_permutexvar_epi32'.
// Requires AVX512F.
func M256MaskPermutexvarEpi32(src x86.M256i, k x86.Mmask8, idx x86.M256i, a x86.M256i) x86.M256i {
	return x86.M256i(m256MaskPermutexvarEpi32([32]byte(src), uint8(k), [32]byte(idx), [32]byte(a)))
}

func m256MaskPermutexvarEpi32(src [32]byte, k uint8, idx [32]byte, a [32]byte) [32]byte


// M256MaskzPermutexvarEpi32: Shuffle 32-bit integers in 'a' across lanes using
// the corresponding index in 'idx', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			id := idx[i+2:i]*32
//			IF k[j]
//				dst[i+31:i] := a[id+31:id]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMD'. Intrinsic: '_mm256_maskz_permutexvar_epi32'.
// Requires AVX512F.
func M256MaskzPermutexvarEpi32(k x86.Mmask8, idx x86.M256i, a x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzPermutexvarEpi32(uint8(k), [32]byte(idx), [32]byte(a)))
}

func m256MaskzPermutexvarEpi32(k uint8, idx [32]byte, a [32]byte) [32]byte


// M256PermutexvarEpi32: Shuffle 32-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			id := idx[i+2:i]*32
//			dst[i+31:i] := a[id+31:id]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMD'. Intrinsic: '_mm256_permutexvar_epi32'.
// Requires AVX512F.
func M256PermutexvarEpi32(idx x86.M256i, a x86.M256i) x86.M256i {
	return x86.M256i(m256PermutexvarEpi32([32]byte(idx), [32]byte(a)))
}

func m256PermutexvarEpi32(idx [32]byte, a [32]byte) [32]byte


// M512MaskPermutexvarEpi32: Shuffle 32-bit integers in 'a' across lanes using
// the corresponding index in 'idx', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			id := idx[i+3:i]*32
//			IF k[j]
//				dst[i+31:i] := a[id+31:id]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMD'. Intrinsic: '_mm512_mask_permutexvar_epi32'.
// Requires AVX512F.
func M512MaskPermutexvarEpi32(src x86.M512i, k x86.Mmask16, idx x86.M512i, a x86.M512i) x86.M512i {
	return x86.M512i(m512MaskPermutexvarEpi32([64]byte(src), uint16(k), [64]byte(idx), [64]byte(a)))
}

func m512MaskPermutexvarEpi32(src [64]byte, k uint16, idx [64]byte, a [64]byte) [64]byte


// M512MaskzPermutexvarEpi32: Shuffle 32-bit integers in 'a' across lanes using
// the corresponding index in 'idx', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			id := idx[i+3:i]*32
//			IF k[j]
//				dst[i+31:i] := a[id+31:id]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMD'. Intrinsic: '_mm512_maskz_permutexvar_epi32'.
// Requires AVX512F.
func M512MaskzPermutexvarEpi32(k x86.Mmask16, idx x86.M512i, a x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzPermutexvarEpi32(uint16(k), [64]byte(idx), [64]byte(a)))
}

func m512MaskzPermutexvarEpi32(k uint16, idx [64]byte, a [64]byte) [64]byte


// M512PermutexvarEpi32: Shuffle 32-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			id := idx[i+3:i]*32
//			dst[i+31:i] := a[id+31:id]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMD'. Intrinsic: '_mm512_permutexvar_epi32'.
// Requires AVX512F.
func M512PermutexvarEpi32(idx x86.M512i, a x86.M512i) x86.M512i {
	return x86.M512i(m512PermutexvarEpi32([64]byte(idx), [64]byte(a)))
}

func m512PermutexvarEpi32(idx [64]byte, a [64]byte) [64]byte


// M256MaskPermutexvarEpi64: Shuffle 64-bit integers in 'a' across lanes using
// the corresponding index in 'idx', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			id := idx[i+1:i]*64
//			IF k[j]
//				dst[i+63:i] := a[id+63:id]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMQ'. Intrinsic: '_mm256_mask_permutexvar_epi64'.
// Requires AVX512F.
func M256MaskPermutexvarEpi64(src x86.M256i, k x86.Mmask8, idx x86.M256i, a x86.M256i) x86.M256i {
	return x86.M256i(m256MaskPermutexvarEpi64([32]byte(src), uint8(k), [32]byte(idx), [32]byte(a)))
}

func m256MaskPermutexvarEpi64(src [32]byte, k uint8, idx [32]byte, a [32]byte) [32]byte


// M256MaskzPermutexvarEpi64: Shuffle 64-bit integers in 'a' across lanes using
// the corresponding index in 'idx', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			id := idx[i+1:i]*64
//			IF k[j]
//				dst[i+63:i] := a[id+63:id]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMQ'. Intrinsic: '_mm256_maskz_permutexvar_epi64'.
// Requires AVX512F.
func M256MaskzPermutexvarEpi64(k x86.Mmask8, idx x86.M256i, a x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzPermutexvarEpi64(uint8(k), [32]byte(idx), [32]byte(a)))
}

func m256MaskzPermutexvarEpi64(k uint8, idx [32]byte, a [32]byte) [32]byte


// M256PermutexvarEpi64: Shuffle 64-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			id := idx[i+1:i]*64
//			dst[i+63:i] := a[id+63:id]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMQ'. Intrinsic: '_mm256_permutexvar_epi64'.
// Requires AVX512F.
func M256PermutexvarEpi64(idx x86.M256i, a x86.M256i) x86.M256i {
	return x86.M256i(m256PermutexvarEpi64([32]byte(idx), [32]byte(a)))
}

func m256PermutexvarEpi64(idx [32]byte, a [32]byte) [32]byte


// M512MaskPermutexvarEpi64: Shuffle 64-bit integers in 'a' across lanes using
// the corresponding index in 'idx', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			id := idx[i+2:i]*64
//			IF k[j]
//				dst[i+63:i] := a[id+63:id]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMQ'. Intrinsic: '_mm512_mask_permutexvar_epi64'.
// Requires AVX512F.
func M512MaskPermutexvarEpi64(src x86.M512i, k x86.Mmask8, idx x86.M512i, a x86.M512i) x86.M512i {
	return x86.M512i(m512MaskPermutexvarEpi64([64]byte(src), uint8(k), [64]byte(idx), [64]byte(a)))
}

func m512MaskPermutexvarEpi64(src [64]byte, k uint8, idx [64]byte, a [64]byte) [64]byte


// M512MaskzPermutexvarEpi64: Shuffle 64-bit integers in 'a' across lanes using
// the corresponding index in 'idx', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			id := idx[i+2:i]*64
//			IF k[j]
//				dst[i+63:i] := a[id+63:id]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMQ'. Intrinsic: '_mm512_maskz_permutexvar_epi64'.
// Requires AVX512F.
func M512MaskzPermutexvarEpi64(k x86.Mmask8, idx x86.M512i, a x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzPermutexvarEpi64(uint8(k), [64]byte(idx), [64]byte(a)))
}

func m512MaskzPermutexvarEpi64(k uint8, idx [64]byte, a [64]byte) [64]byte


// M512PermutexvarEpi64: Shuffle 64-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			id := idx[i+2:i]*64
//			dst[i+63:i] := a[id+63:id]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMQ'. Intrinsic: '_mm512_permutexvar_epi64'.
// Requires AVX512F.
func M512PermutexvarEpi64(idx x86.M512i, a x86.M512i) x86.M512i {
	return x86.M512i(m512PermutexvarEpi64([64]byte(idx), [64]byte(a)))
}

func m512PermutexvarEpi64(idx [64]byte, a [64]byte) [64]byte


// M256MaskPermutexvarPd: Shuffle double-precision (64-bit) floating-point
// elements in 'a' across lanes using the corresponding index in 'idx', and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			id := idx[i+1:i]*64
//			IF k[j]
//				dst[i+63:i] := a[id+63:id]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMPD'. Intrinsic: '_mm256_mask_permutexvar_pd'.
// Requires AVX512F.
func M256MaskPermutexvarPd(src x86.M256d, k x86.Mmask8, idx x86.M256i, a x86.M256d) x86.M256d {
	return x86.M256d(m256MaskPermutexvarPd([4]float64(src), uint8(k), [32]byte(idx), [4]float64(a)))
}

func m256MaskPermutexvarPd(src [4]float64, k uint8, idx [32]byte, a [4]float64) [4]float64


// M256MaskzPermutexvarPd: Shuffle double-precision (64-bit) floating-point
// elements in 'a' across lanes using the corresponding index in 'idx', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			id := idx[i+1:i]*64
//			IF k[j]
//				dst[i+63:i] := a[id+63:id]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMPD'. Intrinsic: '_mm256_maskz_permutexvar_pd'.
// Requires AVX512F.
func M256MaskzPermutexvarPd(k x86.Mmask8, idx x86.M256i, a x86.M256d) x86.M256d {
	return x86.M256d(m256MaskzPermutexvarPd(uint8(k), [32]byte(idx), [4]float64(a)))
}

func m256MaskzPermutexvarPd(k uint8, idx [32]byte, a [4]float64) [4]float64


// M256PermutexvarPd: Shuffle double-precision (64-bit) floating-point elements
// in 'a' across lanes using the corresponding index in 'idx', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			id := idx[i+1:i]*64
//			dst[i+63:i] := a[id+63:id]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMPD'. Intrinsic: '_mm256_permutexvar_pd'.
// Requires AVX512F.
func M256PermutexvarPd(idx x86.M256i, a x86.M256d) x86.M256d {
	return x86.M256d(m256PermutexvarPd([32]byte(idx), [4]float64(a)))
}

func m256PermutexvarPd(idx [32]byte, a [4]float64) [4]float64


// M512MaskPermutexvarPd: Shuffle double-precision (64-bit) floating-point
// elements in 'a' across lanes using the corresponding index in 'idx', and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			id := idx[i+2:i]*64
//			IF k[j]
//				dst[i+63:i] := a[id+63:id]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMPD'. Intrinsic: '_mm512_mask_permutexvar_pd'.
// Requires AVX512F.
func M512MaskPermutexvarPd(src x86.M512d, k x86.Mmask8, idx x86.M512i, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskPermutexvarPd([8]float64(src), uint8(k), [64]byte(idx), [8]float64(a)))
}

func m512MaskPermutexvarPd(src [8]float64, k uint8, idx [64]byte, a [8]float64) [8]float64


// M512MaskzPermutexvarPd: Shuffle double-precision (64-bit) floating-point
// elements in 'a' across lanes using the corresponding index in 'idx', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			id := idx[i+2:i]*64
//			IF k[j]
//				dst[i+63:i] := a[id+63:id]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMPD'. Intrinsic: '_mm512_maskz_permutexvar_pd'.
// Requires AVX512F.
func M512MaskzPermutexvarPd(k x86.Mmask8, idx x86.M512i, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskzPermutexvarPd(uint8(k), [64]byte(idx), [8]float64(a)))
}

func m512MaskzPermutexvarPd(k uint8, idx [64]byte, a [8]float64) [8]float64


// M512PermutexvarPd: Shuffle double-precision (64-bit) floating-point elements
// in 'a' across lanes using the corresponding index in 'idx', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			id := idx[i+2:i]*64
//			dst[i+63:i] := a[id+63:id]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMPD'. Intrinsic: '_mm512_permutexvar_pd'.
// Requires AVX512F.
func M512PermutexvarPd(idx x86.M512i, a x86.M512d) x86.M512d {
	return x86.M512d(m512PermutexvarPd([64]byte(idx), [8]float64(a)))
}

func m512PermutexvarPd(idx [64]byte, a [8]float64) [8]float64


// M256MaskPermutexvarPs: Shuffle single-precision (32-bit) floating-point
// elements in 'a' across lanes using the corresponding index in 'idx', and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			id := idx[i+2:i]*32
//			IF k[j]
//				dst[i+31:i] := a[id+31:id]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMPS'. Intrinsic: '_mm256_mask_permutexvar_ps'.
// Requires AVX512F.
func M256MaskPermutexvarPs(src x86.M256, k x86.Mmask8, idx x86.M256i, a x86.M256) x86.M256 {
	return x86.M256(m256MaskPermutexvarPs([8]float32(src), uint8(k), [32]byte(idx), [8]float32(a)))
}

func m256MaskPermutexvarPs(src [8]float32, k uint8, idx [32]byte, a [8]float32) [8]float32


// M256MaskzPermutexvarPs: Shuffle single-precision (32-bit) floating-point
// elements in 'a' across lanes using the corresponding index in 'idx', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			id := idx[i+2:i]*32
//			IF k[j]
//				dst[i+31:i] := a[id+31:id]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMPS'. Intrinsic: '_mm256_maskz_permutexvar_ps'.
// Requires AVX512F.
func M256MaskzPermutexvarPs(k x86.Mmask8, idx x86.M256i, a x86.M256) x86.M256 {
	return x86.M256(m256MaskzPermutexvarPs(uint8(k), [32]byte(idx), [8]float32(a)))
}

func m256MaskzPermutexvarPs(k uint8, idx [32]byte, a [8]float32) [8]float32


// M256PermutexvarPs: Shuffle single-precision (32-bit) floating-point elements
// in 'a' across lanes using the corresponding index in 'idx'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			id := idx[i+2:i]*32
//			dst[i+31:i] := a[id+31:id]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMPS'. Intrinsic: '_mm256_permutexvar_ps'.
// Requires AVX512F.
func M256PermutexvarPs(idx x86.M256i, a x86.M256) x86.M256 {
	return x86.M256(m256PermutexvarPs([32]byte(idx), [8]float32(a)))
}

func m256PermutexvarPs(idx [32]byte, a [8]float32) [8]float32


// M512MaskPermutexvarPs: Shuffle single-precision (32-bit) floating-point
// elements in 'a' across lanes using the corresponding index in 'idx', and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			id := idx[i+3:i]*32
//			IF k[j]
//				dst[i+31:i] := a[id+31:id]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMPS'. Intrinsic: '_mm512_mask_permutexvar_ps'.
// Requires AVX512F.
func M512MaskPermutexvarPs(src x86.M512, k x86.Mmask16, idx x86.M512i, a x86.M512) x86.M512 {
	return x86.M512(m512MaskPermutexvarPs([16]float32(src), uint16(k), [64]byte(idx), [16]float32(a)))
}

func m512MaskPermutexvarPs(src [16]float32, k uint16, idx [64]byte, a [16]float32) [16]float32


// M512MaskzPermutexvarPs: Shuffle single-precision (32-bit) floating-point
// elements in 'a' across lanes using the corresponding index in 'idx', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			id := idx[i+3:i]*32
//			IF k[j]
//				dst[i+31:i] := a[id+31:id]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMPS'. Intrinsic: '_mm512_maskz_permutexvar_ps'.
// Requires AVX512F.
func M512MaskzPermutexvarPs(k x86.Mmask16, idx x86.M512i, a x86.M512) x86.M512 {
	return x86.M512(m512MaskzPermutexvarPs(uint16(k), [64]byte(idx), [16]float32(a)))
}

func m512MaskzPermutexvarPs(k uint16, idx [64]byte, a [16]float32) [16]float32


// M512PermutexvarPs: Shuffle single-precision (32-bit) floating-point elements
// in 'a' across lanes using the corresponding index in 'idx'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			id := idx[i+3:i]*32
//			dst[i+31:i] := a[id+31:id]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMPS'. Intrinsic: '_mm512_permutexvar_ps'.
// Requires AVX512F.
func M512PermutexvarPs(idx x86.M512i, a x86.M512) x86.M512 {
	return x86.M512(m512PermutexvarPs([64]byte(idx), [16]float32(a)))
}

func m512PermutexvarPs(idx [64]byte, a [16]float32) [16]float32


// M512MaskPowPd: Compute the exponential value of packed double-precision
// (64-bit) floating-point elements in 'a' raised by packed elements in 'b',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i])^(b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_pow_pd'.
// Requires AVX512F.
func M512MaskPowPd(src x86.M512d, k x86.Mmask8, a x86.M512d, b x86.M512d) x86.M512d {
	return x86.M512d(m512MaskPowPd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b)))
}

func m512MaskPowPd(src [8]float64, k uint8, a [8]float64, b [8]float64) [8]float64


// M512PowPd: Compute the exponential value of packed double-precision (64-bit)
// floating-point elements in 'a' raised by packed elements in 'b', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := (a[i+63:i])^(b[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_pow_pd'.
// Requires AVX512F.
func M512PowPd(a x86.M512d, b x86.M512d) x86.M512d {
	return x86.M512d(m512PowPd([8]float64(a), [8]float64(b)))
}

func m512PowPd(a [8]float64, b [8]float64) [8]float64


// M512MaskPowPs: Compute the exponential value of packed single-precision
// (32-bit) floating-point elements in 'a' raised by packed elements in 'b',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i])^(b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_pow_ps'.
// Requires AVX512F.
func M512MaskPowPs(src x86.M512, k x86.Mmask16, a x86.M512, b x86.M512) x86.M512 {
	return x86.M512(m512MaskPowPs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b)))
}

func m512MaskPowPs(src [16]float32, k uint16, a [16]float32, b [16]float32) [16]float32


// M512PowPs: Compute the exponential value of packed single-precision (32-bit)
// floating-point elements in 'a' raised by packed elements in 'b', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := (a[i+31:i])^(b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_pow_ps'.
// Requires AVX512F.
func M512PowPs(a x86.M512, b x86.M512) x86.M512 {
	return x86.M512(m512PowPs([16]float32(a), [16]float32(b)))
}

func m512PowPs(a [16]float32, b [16]float32) [16]float32


// MaskRcp14Pd: Compute the approximate reciprocal of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). The maximum relative error for this approximation is
// less than 2^-14. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := APPROXIMATE(1.0/a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRCP14PD'. Intrinsic: '_mm_mask_rcp14_pd'.
// Requires AVX512F.
func MaskRcp14Pd(src x86.M128d, k x86.Mmask8, a x86.M128d) x86.M128d {
	return x86.M128d(maskRcp14Pd([2]float64(src), uint8(k), [2]float64(a)))
}

func maskRcp14Pd(src [2]float64, k uint8, a [2]float64) [2]float64


// MaskzRcp14Pd: Compute the approximate reciprocal of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). The maximum relative error for this approximation is less than
// 2^-14. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := APPROXIMATE(1.0/a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRCP14PD'. Intrinsic: '_mm_maskz_rcp14_pd'.
// Requires AVX512F.
func MaskzRcp14Pd(k x86.Mmask8, a x86.M128d) x86.M128d {
	return x86.M128d(maskzRcp14Pd(uint8(k), [2]float64(a)))
}

func maskzRcp14Pd(k uint8, a [2]float64) [2]float64


// Rcp14Pd: Compute the approximate reciprocal of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'. The
// maximum relative error for this approximation is less than 2^-14. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := APPROXIMATE(1.0/a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRCP14PD'. Intrinsic: '_mm_rcp14_pd'.
// Requires AVX512F.
func Rcp14Pd(a x86.M128d) x86.M128d {
	return x86.M128d(rcp14Pd([2]float64(a)))
}

func rcp14Pd(a [2]float64) [2]float64


// M256MaskRcp14Pd: Compute the approximate reciprocal of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := APPROXIMATE(1.0/a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRCP14PD'. Intrinsic: '_mm256_mask_rcp14_pd'.
// Requires AVX512F.
func M256MaskRcp14Pd(src x86.M256d, k x86.Mmask8, a x86.M256d) x86.M256d {
	return x86.M256d(m256MaskRcp14Pd([4]float64(src), uint8(k), [4]float64(a)))
}

func m256MaskRcp14Pd(src [4]float64, k uint8, a [4]float64) [4]float64


// M256MaskzRcp14Pd: Compute the approximate reciprocal of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := APPROXIMATE(1.0/a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRCP14PD'. Intrinsic: '_mm256_maskz_rcp14_pd'.
// Requires AVX512F.
func M256MaskzRcp14Pd(k x86.Mmask8, a x86.M256d) x86.M256d {
	return x86.M256d(m256MaskzRcp14Pd(uint8(k), [4]float64(a)))
}

func m256MaskzRcp14Pd(k uint8, a [4]float64) [4]float64


// M256Rcp14Pd: Compute the approximate reciprocal of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'. The
// maximum relative error for this approximation is less than 2^-14. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := APPROXIMATE(1.0/a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRCP14PD'. Intrinsic: '_mm256_rcp14_pd'.
// Requires AVX512F.
func M256Rcp14Pd(a x86.M256d) x86.M256d {
	return x86.M256d(m256Rcp14Pd([4]float64(a)))
}

func m256Rcp14Pd(a [4]float64) [4]float64


// M512MaskRcp14Pd: Compute the approximate reciprocal of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := APPROXIMATE(1.0/a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRCP14PD'. Intrinsic: '_mm512_mask_rcp14_pd'.
// Requires AVX512F.
func M512MaskRcp14Pd(src x86.M512d, k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskRcp14Pd([8]float64(src), uint8(k), [8]float64(a)))
}

func m512MaskRcp14Pd(src [8]float64, k uint8, a [8]float64) [8]float64


// M512MaskzRcp14Pd: Compute the approximate reciprocal of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := APPROXIMATE(1.0/a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRCP14PD'. Intrinsic: '_mm512_maskz_rcp14_pd'.
// Requires AVX512F.
func M512MaskzRcp14Pd(k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskzRcp14Pd(uint8(k), [8]float64(a)))
}

func m512MaskzRcp14Pd(k uint8, a [8]float64) [8]float64


// M512Rcp14Pd: Compute the approximate reciprocal of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'. The
// maximum relative error for this approximation is less than 2^-14. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := APPROXIMATE(1.0/a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRCP14PD'. Intrinsic: '_mm512_rcp14_pd'.
// Requires AVX512F.
func M512Rcp14Pd(a x86.M512d) x86.M512d {
	return x86.M512d(m512Rcp14Pd([8]float64(a)))
}

func m512Rcp14Pd(a [8]float64) [8]float64


// MaskRcp14Ps: Compute the approximate reciprocal of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). The maximum relative error for this approximation is
// less than 2^-14. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRCP14PS'. Intrinsic: '_mm_mask_rcp14_ps'.
// Requires AVX512F.
func MaskRcp14Ps(src x86.M128, k x86.Mmask8, a x86.M128) x86.M128 {
	return x86.M128(maskRcp14Ps([4]float32(src), uint8(k), [4]float32(a)))
}

func maskRcp14Ps(src [4]float32, k uint8, a [4]float32) [4]float32


// MaskzRcp14Ps: Compute the approximate reciprocal of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). The maximum relative error for this approximation is less than
// 2^-14. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRCP14PS'. Intrinsic: '_mm_maskz_rcp14_ps'.
// Requires AVX512F.
func MaskzRcp14Ps(k x86.Mmask8, a x86.M128) x86.M128 {
	return x86.M128(maskzRcp14Ps(uint8(k), [4]float32(a)))
}

func maskzRcp14Ps(k uint8, a [4]float32) [4]float32


// Rcp14Ps: Compute the approximate reciprocal of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'. The
// maximum relative error for this approximation is less than 2^-14. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRCP14PS'. Intrinsic: '_mm_rcp14_ps'.
// Requires AVX512F.
func Rcp14Ps(a x86.M128) x86.M128 {
	return x86.M128(rcp14Ps([4]float32(a)))
}

func rcp14Ps(a [4]float32) [4]float32


// M256MaskRcp14Ps: Compute the approximate reciprocal of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRCP14PS'. Intrinsic: '_mm256_mask_rcp14_ps'.
// Requires AVX512F.
func M256MaskRcp14Ps(src x86.M256, k x86.Mmask8, a x86.M256) x86.M256 {
	return x86.M256(m256MaskRcp14Ps([8]float32(src), uint8(k), [8]float32(a)))
}

func m256MaskRcp14Ps(src [8]float32, k uint8, a [8]float32) [8]float32


// M256MaskzRcp14Ps: Compute the approximate reciprocal of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRCP14PS'. Intrinsic: '_mm256_maskz_rcp14_ps'.
// Requires AVX512F.
func M256MaskzRcp14Ps(k x86.Mmask8, a x86.M256) x86.M256 {
	return x86.M256(m256MaskzRcp14Ps(uint8(k), [8]float32(a)))
}

func m256MaskzRcp14Ps(k uint8, a [8]float32) [8]float32


// M256Rcp14Ps: Compute the approximate reciprocal of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'. The
// maximum relative error for this approximation is less than 2^-14. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRCP14PS'. Intrinsic: '_mm256_rcp14_ps'.
// Requires AVX512F.
func M256Rcp14Ps(a x86.M256) x86.M256 {
	return x86.M256(m256Rcp14Ps([8]float32(a)))
}

func m256Rcp14Ps(a [8]float32) [8]float32


// M512MaskRcp14Ps: Compute the approximate reciprocal of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRCP14PS'. Intrinsic: '_mm512_mask_rcp14_ps'.
// Requires AVX512F.
func M512MaskRcp14Ps(src x86.M512, k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskRcp14Ps([16]float32(src), uint16(k), [16]float32(a)))
}

func m512MaskRcp14Ps(src [16]float32, k uint16, a [16]float32) [16]float32


// M512MaskzRcp14Ps: Compute the approximate reciprocal of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRCP14PS'. Intrinsic: '_mm512_maskz_rcp14_ps'.
// Requires AVX512F.
func M512MaskzRcp14Ps(k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskzRcp14Ps(uint16(k), [16]float32(a)))
}

func m512MaskzRcp14Ps(k uint16, a [16]float32) [16]float32


// M512Rcp14Ps: Compute the approximate reciprocal of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'. The
// maximum relative error for this approximation is less than 2^-14. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRCP14PS'. Intrinsic: '_mm512_rcp14_ps'.
// Requires AVX512F.
func M512Rcp14Ps(a x86.M512) x86.M512 {
	return x86.M512(m512Rcp14Ps([16]float32(a)))
}

func m512Rcp14Ps(a [16]float32) [16]float32


// MaskRcp14Sd: Compute the approximate reciprocal of the lower
// double-precision (64-bit) floating-point element in 'b', store the result in
// the lower element of 'dst' using writemask 'k' (the element is copied from
// 'src' when mask bit 0 is not set), and copy the upper element from 'a' to
// the upper element of 'dst'. The maximum relative error for this
// approximation is less than 2^-14. 
//
//		IF k[0]
//			dst[63:0] := APPROXIMATE(1.0/b[63:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VRCP14SD'. Intrinsic: '_mm_mask_rcp14_sd'.
// Requires AVX512F.
func MaskRcp14Sd(src x86.M128d, k x86.Mmask8, a x86.M128d, b x86.M128d) x86.M128d {
	return x86.M128d(maskRcp14Sd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskRcp14Sd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzRcp14Sd: Compute the approximate reciprocal of the lower
// double-precision (64-bit) floating-point element in 'b', store the result in
// the lower element of 'dst' using zeromask 'k' (the element is zeroed out
// when mask bit 0 is not set), and copy the upper element from 'a' to the
// upper element of 'dst'. The maximum relative error for this approximation is
// less than 2^-14. 
//
//		IF k[0]
//			dst[63:0] := APPROXIMATE(1.0/b[63:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VRCP14SD'. Intrinsic: '_mm_maskz_rcp14_sd'.
// Requires AVX512F.
func MaskzRcp14Sd(k x86.Mmask8, a x86.M128d, b x86.M128d) x86.M128d {
	return x86.M128d(maskzRcp14Sd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzRcp14Sd(k uint8, a [2]float64, b [2]float64) [2]float64


// Rcp14Sd: Compute the approximate reciprocal of the lower double-precision
// (64-bit) floating-point element in 'b', store the result in the lower
// element of 'dst', and copy the upper element from 'a' to the upper element
// of 'dst'. The maximum relative error for this approximation is less than
// 2^-14. 
//
//		dst[63:0] := APPROXIMATE(1.0/b[63:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VRCP14SD'. Intrinsic: '_mm_rcp14_sd'.
// Requires AVX512F.
func Rcp14Sd(a x86.M128d, b x86.M128d) x86.M128d {
	return x86.M128d(rcp14Sd([2]float64(a), [2]float64(b)))
}

func rcp14Sd(a [2]float64, b [2]float64) [2]float64


// MaskRcp14Ss: Compute the approximate reciprocal of the lower
// single-precision (32-bit) floating-point element in 'b', store the result in
// the lower element of 'dst' using writemask 'k' (the element is copied from
// 'src' when mask bit 0 is not set), and copy the upper 3 packed elements from
// 'a' to the upper elements of 'dst'. The maximum relative error for this
// approximation is less than 2^-14. 
//
//		IF k[0]
//			dst[31:0] := APPROXIMATE(1.0/b[31:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VRCP14SS'. Intrinsic: '_mm_mask_rcp14_ss'.
// Requires AVX512F.
func MaskRcp14Ss(src x86.M128, k x86.Mmask8, a x86.M128, b x86.M128) x86.M128 {
	return x86.M128(maskRcp14Ss([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskRcp14Ss(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzRcp14Ss: Compute the approximate reciprocal of the lower
// single-precision (32-bit) floating-point element in 'b', store the result in
// the lower element of 'dst' using zeromask 'k' (the element is zeroed out
// when mask bit 0 is not set), and copy the upper 3 packed elements from 'a'
// to the upper elements of 'dst'. The maximum relative error for this
// approximation is less than 2^-14. 
//
//		IF k[0]
//			dst[31:0] := APPROXIMATE(1.0/b[31:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VRCP14SS'. Intrinsic: '_mm_maskz_rcp14_ss'.
// Requires AVX512F.
func MaskzRcp14Ss(k x86.Mmask8, a x86.M128, b x86.M128) x86.M128 {
	return x86.M128(maskzRcp14Ss(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzRcp14Ss(k uint8, a [4]float32, b [4]float32) [4]float32


// Rcp14Ss: Compute the approximate reciprocal of the lower single-precision
// (32-bit) floating-point element in 'b', store the result in the lower
// element of 'dst', and copy the upper 3 packed elements from 'a' to the upper
// elements of 'dst'. The maximum relative error for this approximation is less
// than 2^-14. 
//
//		dst[31:0] := APPROXIMATE(1.0/b[31:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VRCP14SS'. Intrinsic: '_mm_rcp14_ss'.
// Requires AVX512F.
func Rcp14Ss(a x86.M128, b x86.M128) x86.M128 {
	return x86.M128(rcp14Ss([4]float32(a), [4]float32(b)))
}

func rcp14Ss(a [4]float32, b [4]float32) [4]float32


// M512MaskRecipPd: Computes the reciprocal of packed double-precision (64-bit)
// floating-point elements in 'a', storing the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (1 / a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_recip_pd'.
// Requires AVX512F.
func M512MaskRecipPd(src x86.M512d, k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskRecipPd([8]float64(src), uint8(k), [8]float64(a)))
}

func m512MaskRecipPd(src [8]float64, k uint8, a [8]float64) [8]float64


// M512RecipPd: Computes the reciprocal of packed double-precision (64-bit)
// floating-point elements in 'a', storing the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := (1 / a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_recip_pd'.
// Requires AVX512F.
func M512RecipPd(a x86.M512d) x86.M512d {
	return x86.M512d(m512RecipPd([8]float64(a)))
}

func m512RecipPd(a [8]float64) [8]float64


// M512MaskRecipPs: Computes the reciprocal of packed single-precision (32-bit)
// floating-point elements in 'a', storing the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (1 / a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_recip_ps'.
// Requires AVX512F.
func M512MaskRecipPs(src x86.M512, k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskRecipPs([16]float32(src), uint16(k), [16]float32(a)))
}

func m512MaskRecipPs(src [16]float32, k uint16, a [16]float32) [16]float32


// M512RecipPs: Computes the reciprocal of packed single-precision (32-bit)
// floating-point elements in 'a', storing the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := (1 / a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_recip_ps'.
// Requires AVX512F.
func M512RecipPs(a x86.M512) x86.M512 {
	return x86.M512(m512RecipPs([16]float32(a)))
}

func m512RecipPs(a [16]float32) [16]float32


// M512RemEpi16: Divide packed 16-bit integers in 'a' by packed elements in
// 'b', and store the remainders as packed 32-bit integers in 'dst'. 
//
//		FOR j := 0 to 31
//			i := 16*j
//			dst[i+15:i] := REMAINDER(a[i+15:i] / b[i+15:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_rem_epi16'.
// Requires AVX512F.
func M512RemEpi16(a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512RemEpi16([64]byte(a), [64]byte(b)))
}

func m512RemEpi16(a [64]byte, b [64]byte) [64]byte


// M512MaskRemEpi32: Divide packed 32-bit integers in 'a' by packed elements in
// 'b', and store the remainders as packed 32-bit integers in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := REMAINDER(a[i+31:i] / b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_rem_epi32'.
// Requires AVX512F.
func M512MaskRemEpi32(src x86.M512i, k x86.Mmask16, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskRemEpi32([64]byte(src), uint16(k), [64]byte(a), [64]byte(b)))
}

func m512MaskRemEpi32(src [64]byte, k uint16, a [64]byte, b [64]byte) [64]byte


// M512RemEpi32: Divide packed 32-bit integers in 'a' by packed elements in
// 'b', and store the remainders as packed 32-bit integers in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := REMAINDER(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_rem_epi32'.
// Requires AVX512F.
func M512RemEpi32(a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512RemEpi32([64]byte(a), [64]byte(b)))
}

func m512RemEpi32(a [64]byte, b [64]byte) [64]byte


// M512RemEpi64: Divide packed 64-bit integers in 'a' by packed elements in
// 'b', and store the remainders as packed 32-bit integers in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			dst[i+63:i] := REMAINDER(a[i+63:i] / b[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_rem_epi64'.
// Requires AVX512F.
func M512RemEpi64(a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512RemEpi64([64]byte(a), [64]byte(b)))
}

func m512RemEpi64(a [64]byte, b [64]byte) [64]byte


// M512RemEpi8: Divide packed 8-bit integers in 'a' by packed elements in 'b',
// and store the remainders as packed 32-bit integers in 'dst'. 
//
//		FOR j := 0 to 63
//			i := 8*j
//			dst[i+7:i] := REMAINDER(a[i+7:i] / b[i+7:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_rem_epi8'.
// Requires AVX512F.
func M512RemEpi8(a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512RemEpi8([64]byte(a), [64]byte(b)))
}

func m512RemEpi8(a [64]byte, b [64]byte) [64]byte


// M512RemEpu16: Divide packed unsigned 16-bit integers in 'a' by packed
// elements in 'b', and store the remainders as packed unsigned 32-bit integers
// in 'dst'. 
//
//		FOR j := 0 to 31
//			i := 16*j
//			dst[i+15:i] := REMAINDER(a[i+15:i] / b[i+15:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_rem_epu16'.
// Requires AVX512F.
func M512RemEpu16(a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512RemEpu16([64]byte(a), [64]byte(b)))
}

func m512RemEpu16(a [64]byte, b [64]byte) [64]byte


// M512MaskRemEpu32: Divide packed unsigned 32-bit integers in 'a' by packed
// elements in 'b', and store the remainders as packed unsigned 32-bit integers
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := REMAINDER(a[i+31:i] / b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_rem_epu32'.
// Requires AVX512F.
func M512MaskRemEpu32(src x86.M512i, k x86.Mmask16, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskRemEpu32([64]byte(src), uint16(k), [64]byte(a), [64]byte(b)))
}

func m512MaskRemEpu32(src [64]byte, k uint16, a [64]byte, b [64]byte) [64]byte


// M512RemEpu32: Divide packed unsigned 32-bit integers in 'a' by packed
// elements in 'b', and store the remainders as packed unsigned 32-bit integers
// in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := REMAINDER(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_rem_epu32'.
// Requires AVX512F.
func M512RemEpu32(a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512RemEpu32([64]byte(a), [64]byte(b)))
}

func m512RemEpu32(a [64]byte, b [64]byte) [64]byte


// M512RemEpu64: Divide packed unsigned 64-bit integers in 'a' by packed
// elements in 'b', and store the remainders as packed unsigned 32-bit integers
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			dst[i+63:i] := REMAINDER(a[i+63:i] / b[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_rem_epu64'.
// Requires AVX512F.
func M512RemEpu64(a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512RemEpu64([64]byte(a), [64]byte(b)))
}

func m512RemEpu64(a [64]byte, b [64]byte) [64]byte


// M512RemEpu8: Divide packed unsigned 8-bit integers in 'a' by packed elements
// in 'b', and store the remainders as packed unsigned 32-bit integers in
// 'dst'. 
//
//		FOR j := 0 to 63
//			i := 8*j
//			dst[i+7:i] := REMAINDER(a[i+7:i] / b[i+7:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_rem_epu8'.
// Requires AVX512F.
func M512RemEpu8(a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512RemEpu8([64]byte(a), [64]byte(b)))
}

func m512RemEpu8(a [64]byte, b [64]byte) [64]byte


// M512MaskRintPd: Rounds the packed double-precision (64-bit) floating-point
// elements in 'a' to the nearest even integer value and stores the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RoundToNearestEven(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_rint_pd'.
// Requires AVX512F.
func M512MaskRintPd(src x86.M512d, k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskRintPd([8]float64(src), uint8(k), [8]float64(a)))
}

func m512MaskRintPd(src [8]float64, k uint8, a [8]float64) [8]float64


// M512RintPd: Rounds the packed double-precision (64-bit) floating-point
// elements in 'a' to the nearest even integer value and stores the results in
// 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := RoundToNearestEven(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_rint_pd'.
// Requires AVX512F.
func M512RintPd(a x86.M512d) x86.M512d {
	return x86.M512d(m512RintPd([8]float64(a)))
}

func m512RintPd(a [8]float64) [8]float64


// M512MaskRintPs: Rounds the packed single-precision (32-bit) floating-point
// elements in 'a' to the nearest even integer value and stores the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RoundToNearestEven(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_rint_ps'.
// Requires AVX512F.
func M512MaskRintPs(src x86.M512, k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskRintPs([16]float32(src), uint16(k), [16]float32(a)))
}

func m512MaskRintPs(src [16]float32, k uint16, a [16]float32) [16]float32


// M512RintPs: Rounds the packed single-precision (32-bit) floating-point
// elements in 'a' to the nearest even integer value and stores the results in
// 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := RoundToNearestEven(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_rint_ps'.
// Requires AVX512F.
func M512RintPs(a x86.M512) x86.M512 {
	return x86.M512(m512RintPs([16]float32(a)))
}

func m512RintPs(a [16]float32) [16]float32


// MaskRolEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the
// left by the number of bits specified in 'imm8', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPROLD'. Intrinsic: '_mm_mask_rol_epi32'.
// Requires AVX512F.
func MaskRolEpi32(src x86.M128i, k x86.Mmask8, a x86.M128i, imm8 int) x86.M128i {
	return x86.M128i(maskRolEpi32([16]byte(src), uint8(k), [16]byte(a), imm8))
}

func maskRolEpi32(src [16]byte, k uint8, a [16]byte, imm8 int) [16]byte


// MaskzRolEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the
// left by the number of bits specified in 'imm8', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPROLD'. Intrinsic: '_mm_maskz_rol_epi32'.
// Requires AVX512F.
func MaskzRolEpi32(k x86.Mmask8, a x86.M128i, imm8 int) x86.M128i {
	return x86.M128i(maskzRolEpi32(uint8(k), [16]byte(a), imm8))
}

func maskzRolEpi32(k uint8, a [16]byte, imm8 int) [16]byte


// RolEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the left
// by the number of bits specified in 'imm8', and store the results in 'dst'. 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPROLD'. Intrinsic: '_mm_rol_epi32'.
// Requires AVX512F.
func RolEpi32(a x86.M128i, imm8 int) x86.M128i {
	return x86.M128i(rolEpi32([16]byte(a), imm8))
}

func rolEpi32(a [16]byte, imm8 int) [16]byte


// M256MaskRolEpi32: Rotate the bits in each packed 32-bit integer in 'a' to
// the left by the number of bits specified in 'imm8', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLD'. Intrinsic: '_mm256_mask_rol_epi32'.
// Requires AVX512F.
func M256MaskRolEpi32(src x86.M256i, k x86.Mmask8, a x86.M256i, imm8 int) x86.M256i {
	return x86.M256i(m256MaskRolEpi32([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func m256MaskRolEpi32(src [32]byte, k uint8, a [32]byte, imm8 int) [32]byte


// M256MaskzRolEpi32: Rotate the bits in each packed 32-bit integer in 'a' to
// the left by the number of bits specified in 'imm8', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLD'. Intrinsic: '_mm256_maskz_rol_epi32'.
// Requires AVX512F.
func M256MaskzRolEpi32(k x86.Mmask8, a x86.M256i, imm8 int) x86.M256i {
	return x86.M256i(m256MaskzRolEpi32(uint8(k), [32]byte(a), imm8))
}

func m256MaskzRolEpi32(k uint8, a [32]byte, imm8 int) [32]byte


// M256RolEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the
// left by the number of bits specified in 'imm8', and store the results in
// 'dst'. 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLD'. Intrinsic: '_mm256_rol_epi32'.
// Requires AVX512F.
func M256RolEpi32(a x86.M256i, imm8 int) x86.M256i {
	return x86.M256i(m256RolEpi32([32]byte(a), imm8))
}

func m256RolEpi32(a [32]byte, imm8 int) [32]byte


// M512MaskRolEpi32: Rotate the bits in each packed 32-bit integer in 'a' to
// the left by the number of bits specified in 'imm8', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPROLD'. Intrinsic: '_mm512_mask_rol_epi32'.
// Requires AVX512F.
func M512MaskRolEpi32(src x86.M512i, k x86.Mmask16, a x86.M512i, imm8 int) x86.M512i {
	return x86.M512i(m512MaskRolEpi32([64]byte(src), uint16(k), [64]byte(a), imm8))
}

func m512MaskRolEpi32(src [64]byte, k uint16, a [64]byte, imm8 int) [64]byte


// M512MaskzRolEpi32: Rotate the bits in each packed 32-bit integer in 'a' to
// the left by the number of bits specified in 'imm8', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPROLD'. Intrinsic: '_mm512_maskz_rol_epi32'.
// Requires AVX512F.
func M512MaskzRolEpi32(k x86.Mmask16, a x86.M512i, imm8 int) x86.M512i {
	return x86.M512i(m512MaskzRolEpi32(uint16(k), [64]byte(a), imm8))
}

func m512MaskzRolEpi32(k uint16, a [64]byte, imm8 int) [64]byte


// M512RolEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the
// left by the number of bits specified in 'imm8', and store the results in
// 'dst'. 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPROLD'. Intrinsic: '_mm512_rol_epi32'.
// Requires AVX512F.
func M512RolEpi32(a x86.M512i, imm8 int) x86.M512i {
	return x86.M512i(m512RolEpi32([64]byte(a), imm8))
}

func m512RolEpi32(a [64]byte, imm8 int) [64]byte


// MaskRolEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the
// left by the number of bits specified in 'imm8', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPROLQ'. Intrinsic: '_mm_mask_rol_epi64'.
// Requires AVX512F.
func MaskRolEpi64(src x86.M128i, k x86.Mmask8, a x86.M128i, imm8 int) x86.M128i {
	return x86.M128i(maskRolEpi64([16]byte(src), uint8(k), [16]byte(a), imm8))
}

func maskRolEpi64(src [16]byte, k uint8, a [16]byte, imm8 int) [16]byte


// MaskzRolEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the
// left by the number of bits specified in 'imm8', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPROLQ'. Intrinsic: '_mm_maskz_rol_epi64'.
// Requires AVX512F.
func MaskzRolEpi64(k x86.Mmask8, a x86.M128i, imm8 int) x86.M128i {
	return x86.M128i(maskzRolEpi64(uint8(k), [16]byte(a), imm8))
}

func maskzRolEpi64(k uint8, a [16]byte, imm8 int) [16]byte


// RolEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the left
// by the number of bits specified in 'imm8', and store the results in 'dst'. 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPROLQ'. Intrinsic: '_mm_rol_epi64'.
// Requires AVX512F.
func RolEpi64(a x86.M128i, imm8 int) x86.M128i {
	return x86.M128i(rolEpi64([16]byte(a), imm8))
}

func rolEpi64(a [16]byte, imm8 int) [16]byte


// M256MaskRolEpi64: Rotate the bits in each packed 64-bit integer in 'a' to
// the left by the number of bits specified in 'imm8', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLQ'. Intrinsic: '_mm256_mask_rol_epi64'.
// Requires AVX512F.
func M256MaskRolEpi64(src x86.M256i, k x86.Mmask8, a x86.M256i, imm8 int) x86.M256i {
	return x86.M256i(m256MaskRolEpi64([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func m256MaskRolEpi64(src [32]byte, k uint8, a [32]byte, imm8 int) [32]byte


// M256MaskzRolEpi64: Rotate the bits in each packed 64-bit integer in 'a' to
// the left by the number of bits specified in 'imm8', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLQ'. Intrinsic: '_mm256_maskz_rol_epi64'.
// Requires AVX512F.
func M256MaskzRolEpi64(k x86.Mmask8, a x86.M256i, imm8 int) x86.M256i {
	return x86.M256i(m256MaskzRolEpi64(uint8(k), [32]byte(a), imm8))
}

func m256MaskzRolEpi64(k uint8, a [32]byte, imm8 int) [32]byte


// M256RolEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the
// left by the number of bits specified in 'imm8', and store the results in
// 'dst'. 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLQ'. Intrinsic: '_mm256_rol_epi64'.
// Requires AVX512F.
func M256RolEpi64(a x86.M256i, imm8 int) x86.M256i {
	return x86.M256i(m256RolEpi64([32]byte(a), imm8))
}

func m256RolEpi64(a [32]byte, imm8 int) [32]byte


// M512MaskRolEpi64: Rotate the bits in each packed 64-bit integer in 'a' to
// the left by the number of bits specified in 'imm8', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPROLQ'. Intrinsic: '_mm512_mask_rol_epi64'.
// Requires AVX512F.
func M512MaskRolEpi64(src x86.M512i, k x86.Mmask8, a x86.M512i, imm8 int) x86.M512i {
	return x86.M512i(m512MaskRolEpi64([64]byte(src), uint8(k), [64]byte(a), imm8))
}

func m512MaskRolEpi64(src [64]byte, k uint8, a [64]byte, imm8 int) [64]byte


// M512MaskzRolEpi64: Rotate the bits in each packed 64-bit integer in 'a' to
// the left by the number of bits specified in 'imm8', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPROLQ'. Intrinsic: '_mm512_maskz_rol_epi64'.
// Requires AVX512F.
func M512MaskzRolEpi64(k x86.Mmask8, a x86.M512i, imm8 int) x86.M512i {
	return x86.M512i(m512MaskzRolEpi64(uint8(k), [64]byte(a), imm8))
}

func m512MaskzRolEpi64(k uint8, a [64]byte, imm8 int) [64]byte


// M512RolEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the
// left by the number of bits specified in 'imm8', and store the results in
// 'dst'. 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPROLQ'. Intrinsic: '_mm512_rol_epi64'.
// Requires AVX512F.
func M512RolEpi64(a x86.M512i, imm8 int) x86.M512i {
	return x86.M512i(m512RolEpi64([64]byte(a), imm8))
}

func m512RolEpi64(a [64]byte, imm8 int) [64]byte


// MaskRolvEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the
// left by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPROLVD'. Intrinsic: '_mm_mask_rolv_epi32'.
// Requires AVX512F.
func MaskRolvEpi32(src x86.M128i, k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskRolvEpi32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskRolvEpi32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzRolvEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the
// left by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPROLVD'. Intrinsic: '_mm_maskz_rolv_epi32'.
// Requires AVX512F.
func MaskzRolvEpi32(k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskzRolvEpi32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzRolvEpi32(k uint8, a [16]byte, b [16]byte) [16]byte


// RolvEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the left
// by the number of bits specified in the corresponding element of 'b', and
// store the results in 'dst'. 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPROLVD'. Intrinsic: '_mm_rolv_epi32'.
// Requires AVX512F.
func RolvEpi32(a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(rolvEpi32([16]byte(a), [16]byte(b)))
}

func rolvEpi32(a [16]byte, b [16]byte) [16]byte


// M256MaskRolvEpi32: Rotate the bits in each packed 32-bit integer in 'a' to
// the left by the number of bits specified in the corresponding element of
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLVD'. Intrinsic: '_mm256_mask_rolv_epi32'.
// Requires AVX512F.
func M256MaskRolvEpi32(src x86.M256i, k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskRolvEpi32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskRolvEpi32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// M256MaskzRolvEpi32: Rotate the bits in each packed 32-bit integer in 'a' to
// the left by the number of bits specified in the corresponding element of
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLVD'. Intrinsic: '_mm256_maskz_rolv_epi32'.
// Requires AVX512F.
func M256MaskzRolvEpi32(k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzRolvEpi32(uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskzRolvEpi32(k uint8, a [32]byte, b [32]byte) [32]byte


// M256RolvEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the
// left by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst'. 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLVD'. Intrinsic: '_mm256_rolv_epi32'.
// Requires AVX512F.
func M256RolvEpi32(a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256RolvEpi32([32]byte(a), [32]byte(b)))
}

func m256RolvEpi32(a [32]byte, b [32]byte) [32]byte


// M512MaskRolvEpi32: Rotate the bits in each packed 32-bit integer in 'a' to
// the left by the number of bits specified in the corresponding element of
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPROLVD'. Intrinsic: '_mm512_mask_rolv_epi32'.
// Requires AVX512F.
func M512MaskRolvEpi32(src x86.M512i, k x86.Mmask16, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskRolvEpi32([64]byte(src), uint16(k), [64]byte(a), [64]byte(b)))
}

func m512MaskRolvEpi32(src [64]byte, k uint16, a [64]byte, b [64]byte) [64]byte


// M512MaskzRolvEpi32: Rotate the bits in each packed 32-bit integer in 'a' to
// the left by the number of bits specified in the corresponding element of
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPROLVD'. Intrinsic: '_mm512_maskz_rolv_epi32'.
// Requires AVX512F.
func M512MaskzRolvEpi32(k x86.Mmask16, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzRolvEpi32(uint16(k), [64]byte(a), [64]byte(b)))
}

func m512MaskzRolvEpi32(k uint16, a [64]byte, b [64]byte) [64]byte


// M512RolvEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the
// left by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst'. 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPROLVD'. Intrinsic: '_mm512_rolv_epi32'.
// Requires AVX512F.
func M512RolvEpi32(a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512RolvEpi32([64]byte(a), [64]byte(b)))
}

func m512RolvEpi32(a [64]byte, b [64]byte) [64]byte


// MaskRolvEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the
// left by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPROLVQ'. Intrinsic: '_mm_mask_rolv_epi64'.
// Requires AVX512F.
func MaskRolvEpi64(src x86.M128i, k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskRolvEpi64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskRolvEpi64(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzRolvEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the
// left by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPROLVQ'. Intrinsic: '_mm_maskz_rolv_epi64'.
// Requires AVX512F.
func MaskzRolvEpi64(k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskzRolvEpi64(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzRolvEpi64(k uint8, a [16]byte, b [16]byte) [16]byte


// RolvEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the left
// by the number of bits specified in the corresponding element of 'b', and
// store the results in 'dst'. 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPROLVQ'. Intrinsic: '_mm_rolv_epi64'.
// Requires AVX512F.
func RolvEpi64(a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(rolvEpi64([16]byte(a), [16]byte(b)))
}

func rolvEpi64(a [16]byte, b [16]byte) [16]byte


// M256MaskRolvEpi64: Rotate the bits in each packed 64-bit integer in 'a' to
// the left by the number of bits specified in the corresponding element of
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLVQ'. Intrinsic: '_mm256_mask_rolv_epi64'.
// Requires AVX512F.
func M256MaskRolvEpi64(src x86.M256i, k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskRolvEpi64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskRolvEpi64(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// M256MaskzRolvEpi64: Rotate the bits in each packed 64-bit integer in 'a' to
// the left by the number of bits specified in the corresponding element of
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLVQ'. Intrinsic: '_mm256_maskz_rolv_epi64'.
// Requires AVX512F.
func M256MaskzRolvEpi64(k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzRolvEpi64(uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskzRolvEpi64(k uint8, a [32]byte, b [32]byte) [32]byte


// M256RolvEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the
// left by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst'. 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLVQ'. Intrinsic: '_mm256_rolv_epi64'.
// Requires AVX512F.
func M256RolvEpi64(a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256RolvEpi64([32]byte(a), [32]byte(b)))
}

func m256RolvEpi64(a [32]byte, b [32]byte) [32]byte


// M512MaskRolvEpi64: Rotate the bits in each packed 64-bit integer in 'a' to
// the left by the number of bits specified in the corresponding element of
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPROLVQ'. Intrinsic: '_mm512_mask_rolv_epi64'.
// Requires AVX512F.
func M512MaskRolvEpi64(src x86.M512i, k x86.Mmask8, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskRolvEpi64([64]byte(src), uint8(k), [64]byte(a), [64]byte(b)))
}

func m512MaskRolvEpi64(src [64]byte, k uint8, a [64]byte, b [64]byte) [64]byte


// M512MaskzRolvEpi64: Rotate the bits in each packed 64-bit integer in 'a' to
// the left by the number of bits specified in the corresponding element of
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPROLVQ'. Intrinsic: '_mm512_maskz_rolv_epi64'.
// Requires AVX512F.
func M512MaskzRolvEpi64(k x86.Mmask8, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzRolvEpi64(uint8(k), [64]byte(a), [64]byte(b)))
}

func m512MaskzRolvEpi64(k uint8, a [64]byte, b [64]byte) [64]byte


// M512RolvEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the
// left by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst'. 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPROLVQ'. Intrinsic: '_mm512_rolv_epi64'.
// Requires AVX512F.
func M512RolvEpi64(a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512RolvEpi64([64]byte(a), [64]byte(b)))
}

func m512RolvEpi64(a [64]byte, b [64]byte) [64]byte


// MaskRorEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the
// right by the number of bits specified in 'imm8', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPRORD'. Intrinsic: '_mm_mask_ror_epi32'.
// Requires AVX512F.
func MaskRorEpi32(src x86.M128i, k x86.Mmask8, a x86.M128i, imm8 int) x86.M128i {
	return x86.M128i(maskRorEpi32([16]byte(src), uint8(k), [16]byte(a), imm8))
}

func maskRorEpi32(src [16]byte, k uint8, a [16]byte, imm8 int) [16]byte


// MaskzRorEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the
// right by the number of bits specified in 'imm8', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPRORD'. Intrinsic: '_mm_maskz_ror_epi32'.
// Requires AVX512F.
func MaskzRorEpi32(k x86.Mmask8, a x86.M128i, imm8 int) x86.M128i {
	return x86.M128i(maskzRorEpi32(uint8(k), [16]byte(a), imm8))
}

func maskzRorEpi32(k uint8, a [16]byte, imm8 int) [16]byte


// RorEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the right
// by the number of bits specified in 'imm8', and store the results in 'dst'. 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPRORD'. Intrinsic: '_mm_ror_epi32'.
// Requires AVX512F.
func RorEpi32(a x86.M128i, imm8 int) x86.M128i {
	return x86.M128i(rorEpi32([16]byte(a), imm8))
}

func rorEpi32(a [16]byte, imm8 int) [16]byte


// M256MaskRorEpi32: Rotate the bits in each packed 32-bit integer in 'a' to
// the right by the number of bits specified in 'imm8', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORD'. Intrinsic: '_mm256_mask_ror_epi32'.
// Requires AVX512F.
func M256MaskRorEpi32(src x86.M256i, k x86.Mmask8, a x86.M256i, imm8 int) x86.M256i {
	return x86.M256i(m256MaskRorEpi32([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func m256MaskRorEpi32(src [32]byte, k uint8, a [32]byte, imm8 int) [32]byte


// M256MaskzRorEpi32: Rotate the bits in each packed 32-bit integer in 'a' to
// the right by the number of bits specified in 'imm8', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORD'. Intrinsic: '_mm256_maskz_ror_epi32'.
// Requires AVX512F.
func M256MaskzRorEpi32(k x86.Mmask8, a x86.M256i, imm8 int) x86.M256i {
	return x86.M256i(m256MaskzRorEpi32(uint8(k), [32]byte(a), imm8))
}

func m256MaskzRorEpi32(k uint8, a [32]byte, imm8 int) [32]byte


// M256RorEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the
// right by the number of bits specified in 'imm8', and store the results in
// 'dst'. 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORD'. Intrinsic: '_mm256_ror_epi32'.
// Requires AVX512F.
func M256RorEpi32(a x86.M256i, imm8 int) x86.M256i {
	return x86.M256i(m256RorEpi32([32]byte(a), imm8))
}

func m256RorEpi32(a [32]byte, imm8 int) [32]byte


// M512MaskRorEpi32: Rotate the bits in each packed 32-bit integer in 'a' to
// the right by the number of bits specified in 'imm8', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPRORD'. Intrinsic: '_mm512_mask_ror_epi32'.
// Requires AVX512F.
func M512MaskRorEpi32(src x86.M512i, k x86.Mmask16, a x86.M512i, imm8 int) x86.M512i {
	return x86.M512i(m512MaskRorEpi32([64]byte(src), uint16(k), [64]byte(a), imm8))
}

func m512MaskRorEpi32(src [64]byte, k uint16, a [64]byte, imm8 int) [64]byte


// M512MaskzRorEpi32: Rotate the bits in each packed 32-bit integer in 'a' to
// the right by the number of bits specified in 'imm8', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPRORD'. Intrinsic: '_mm512_maskz_ror_epi32'.
// Requires AVX512F.
func M512MaskzRorEpi32(k x86.Mmask16, a x86.M512i, imm8 int) x86.M512i {
	return x86.M512i(m512MaskzRorEpi32(uint16(k), [64]byte(a), imm8))
}

func m512MaskzRorEpi32(k uint16, a [64]byte, imm8 int) [64]byte


// M512RorEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the
// right by the number of bits specified in 'imm8', and store the results in
// 'dst'. 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPRORD'. Intrinsic: '_mm512_ror_epi32'.
// Requires AVX512F.
func M512RorEpi32(a x86.M512i, imm8 int) x86.M512i {
	return x86.M512i(m512RorEpi32([64]byte(a), imm8))
}

func m512RorEpi32(a [64]byte, imm8 int) [64]byte


// MaskRorEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the
// right by the number of bits specified in 'imm8', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPRORQ'. Intrinsic: '_mm_mask_ror_epi64'.
// Requires AVX512F.
func MaskRorEpi64(src x86.M128i, k x86.Mmask8, a x86.M128i, imm8 int) x86.M128i {
	return x86.M128i(maskRorEpi64([16]byte(src), uint8(k), [16]byte(a), imm8))
}

func maskRorEpi64(src [16]byte, k uint8, a [16]byte, imm8 int) [16]byte


// MaskzRorEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the
// right by the number of bits specified in 'imm8', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPRORQ'. Intrinsic: '_mm_maskz_ror_epi64'.
// Requires AVX512F.
func MaskzRorEpi64(k x86.Mmask8, a x86.M128i, imm8 int) x86.M128i {
	return x86.M128i(maskzRorEpi64(uint8(k), [16]byte(a), imm8))
}

func maskzRorEpi64(k uint8, a [16]byte, imm8 int) [16]byte


// RorEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the right
// by the number of bits specified in 'imm8', and store the results in 'dst'. 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPRORQ'. Intrinsic: '_mm_ror_epi64'.
// Requires AVX512F.
func RorEpi64(a x86.M128i, imm8 int) x86.M128i {
	return x86.M128i(rorEpi64([16]byte(a), imm8))
}

func rorEpi64(a [16]byte, imm8 int) [16]byte


// M256MaskRorEpi64: Rotate the bits in each packed 64-bit integer in 'a' to
// the right by the number of bits specified in 'imm8', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORQ'. Intrinsic: '_mm256_mask_ror_epi64'.
// Requires AVX512F.
func M256MaskRorEpi64(src x86.M256i, k x86.Mmask8, a x86.M256i, imm8 int) x86.M256i {
	return x86.M256i(m256MaskRorEpi64([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func m256MaskRorEpi64(src [32]byte, k uint8, a [32]byte, imm8 int) [32]byte


// M256MaskzRorEpi64: Rotate the bits in each packed 64-bit integer in 'a' to
// the right by the number of bits specified in 'imm8', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORQ'. Intrinsic: '_mm256_maskz_ror_epi64'.
// Requires AVX512F.
func M256MaskzRorEpi64(k x86.Mmask8, a x86.M256i, imm8 int) x86.M256i {
	return x86.M256i(m256MaskzRorEpi64(uint8(k), [32]byte(a), imm8))
}

func m256MaskzRorEpi64(k uint8, a [32]byte, imm8 int) [32]byte


// M256RorEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the
// right by the number of bits specified in 'imm8', and store the results in
// 'dst'. 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORQ'. Intrinsic: '_mm256_ror_epi64'.
// Requires AVX512F.
func M256RorEpi64(a x86.M256i, imm8 int) x86.M256i {
	return x86.M256i(m256RorEpi64([32]byte(a), imm8))
}

func m256RorEpi64(a [32]byte, imm8 int) [32]byte


// M512MaskRorEpi64: Rotate the bits in each packed 64-bit integer in 'a' to
// the right by the number of bits specified in 'imm8', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPRORQ'. Intrinsic: '_mm512_mask_ror_epi64'.
// Requires AVX512F.
func M512MaskRorEpi64(src x86.M512i, k x86.Mmask8, a x86.M512i, imm8 int) x86.M512i {
	return x86.M512i(m512MaskRorEpi64([64]byte(src), uint8(k), [64]byte(a), imm8))
}

func m512MaskRorEpi64(src [64]byte, k uint8, a [64]byte, imm8 int) [64]byte


// M512MaskzRorEpi64: Rotate the bits in each packed 64-bit integer in 'a' to
// the right by the number of bits specified in 'imm8', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPRORQ'. Intrinsic: '_mm512_maskz_ror_epi64'.
// Requires AVX512F.
func M512MaskzRorEpi64(k x86.Mmask8, a x86.M512i, imm8 int) x86.M512i {
	return x86.M512i(m512MaskzRorEpi64(uint8(k), [64]byte(a), imm8))
}

func m512MaskzRorEpi64(k uint8, a [64]byte, imm8 int) [64]byte


// M512RorEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the
// right by the number of bits specified in 'imm8', and store the results in
// 'dst'. 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPRORQ'. Intrinsic: '_mm512_ror_epi64'.
// Requires AVX512F.
func M512RorEpi64(a x86.M512i, imm8 int) x86.M512i {
	return x86.M512i(m512RorEpi64([64]byte(a), imm8))
}

func m512RorEpi64(a [64]byte, imm8 int) [64]byte


// MaskRorvEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the
// right by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPRORVD'. Intrinsic: '_mm_mask_rorv_epi32'.
// Requires AVX512F.
func MaskRorvEpi32(src x86.M128i, k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskRorvEpi32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskRorvEpi32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzRorvEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the
// right by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPRORVD'. Intrinsic: '_mm_maskz_rorv_epi32'.
// Requires AVX512F.
func MaskzRorvEpi32(k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskzRorvEpi32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzRorvEpi32(k uint8, a [16]byte, b [16]byte) [16]byte


// RorvEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the right
// by the number of bits specified in the corresponding element of 'b', and
// store the results in 'dst'. 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPRORVD'. Intrinsic: '_mm_rorv_epi32'.
// Requires AVX512F.
func RorvEpi32(a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(rorvEpi32([16]byte(a), [16]byte(b)))
}

func rorvEpi32(a [16]byte, b [16]byte) [16]byte


// M256MaskRorvEpi32: Rotate the bits in each packed 32-bit integer in 'a' to
// the right by the number of bits specified in the corresponding element of
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORVD'. Intrinsic: '_mm256_mask_rorv_epi32'.
// Requires AVX512F.
func M256MaskRorvEpi32(src x86.M256i, k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskRorvEpi32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskRorvEpi32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// M256MaskzRorvEpi32: Rotate the bits in each packed 32-bit integer in 'a' to
// the right by the number of bits specified in the corresponding element of
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORVD'. Intrinsic: '_mm256_maskz_rorv_epi32'.
// Requires AVX512F.
func M256MaskzRorvEpi32(k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzRorvEpi32(uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskzRorvEpi32(k uint8, a [32]byte, b [32]byte) [32]byte


// M256RorvEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the
// right by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst'. 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORVD'. Intrinsic: '_mm256_rorv_epi32'.
// Requires AVX512F.
func M256RorvEpi32(a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256RorvEpi32([32]byte(a), [32]byte(b)))
}

func m256RorvEpi32(a [32]byte, b [32]byte) [32]byte


// M512MaskRorvEpi32: Rotate the bits in each packed 32-bit integer in 'a' to
// the right by the number of bits specified in the corresponding element of
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPRORVD'. Intrinsic: '_mm512_mask_rorv_epi32'.
// Requires AVX512F.
func M512MaskRorvEpi32(src x86.M512i, k x86.Mmask16, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskRorvEpi32([64]byte(src), uint16(k), [64]byte(a), [64]byte(b)))
}

func m512MaskRorvEpi32(src [64]byte, k uint16, a [64]byte, b [64]byte) [64]byte


// M512MaskzRorvEpi32: Rotate the bits in each packed 32-bit integer in 'a' to
// the right by the number of bits specified in the corresponding element of
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPRORVD'. Intrinsic: '_mm512_maskz_rorv_epi32'.
// Requires AVX512F.
func M512MaskzRorvEpi32(k x86.Mmask16, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzRorvEpi32(uint16(k), [64]byte(a), [64]byte(b)))
}

func m512MaskzRorvEpi32(k uint16, a [64]byte, b [64]byte) [64]byte


// M512RorvEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the
// right by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst'. 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPRORVD'. Intrinsic: '_mm512_rorv_epi32'.
// Requires AVX512F.
func M512RorvEpi32(a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512RorvEpi32([64]byte(a), [64]byte(b)))
}

func m512RorvEpi32(a [64]byte, b [64]byte) [64]byte


// MaskRorvEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the
// right by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPRORVQ'. Intrinsic: '_mm_mask_rorv_epi64'.
// Requires AVX512F.
func MaskRorvEpi64(src x86.M128i, k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskRorvEpi64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskRorvEpi64(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzRorvEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the
// right by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPRORVQ'. Intrinsic: '_mm_maskz_rorv_epi64'.
// Requires AVX512F.
func MaskzRorvEpi64(k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskzRorvEpi64(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzRorvEpi64(k uint8, a [16]byte, b [16]byte) [16]byte


// RorvEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the right
// by the number of bits specified in the corresponding element of 'b', and
// store the results in 'dst'. 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPRORVQ'. Intrinsic: '_mm_rorv_epi64'.
// Requires AVX512F.
func RorvEpi64(a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(rorvEpi64([16]byte(a), [16]byte(b)))
}

func rorvEpi64(a [16]byte, b [16]byte) [16]byte


// M256MaskRorvEpi64: Rotate the bits in each packed 64-bit integer in 'a' to
// the right by the number of bits specified in the corresponding element of
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORVQ'. Intrinsic: '_mm256_mask_rorv_epi64'.
// Requires AVX512F.
func M256MaskRorvEpi64(src x86.M256i, k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskRorvEpi64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskRorvEpi64(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// M256MaskzRorvEpi64: Rotate the bits in each packed 64-bit integer in 'a' to
// the right by the number of bits specified in the corresponding element of
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORVQ'. Intrinsic: '_mm256_maskz_rorv_epi64'.
// Requires AVX512F.
func M256MaskzRorvEpi64(k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzRorvEpi64(uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskzRorvEpi64(k uint8, a [32]byte, b [32]byte) [32]byte


// M256RorvEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the
// right by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst'. 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORVQ'. Intrinsic: '_mm256_rorv_epi64'.
// Requires AVX512F.
func M256RorvEpi64(a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256RorvEpi64([32]byte(a), [32]byte(b)))
}

func m256RorvEpi64(a [32]byte, b [32]byte) [32]byte


// M512MaskRorvEpi64: Rotate the bits in each packed 64-bit integer in 'a' to
// the right by the number of bits specified in the corresponding element of
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPRORVQ'. Intrinsic: '_mm512_mask_rorv_epi64'.
// Requires AVX512F.
func M512MaskRorvEpi64(src x86.M512i, k x86.Mmask8, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskRorvEpi64([64]byte(src), uint8(k), [64]byte(a), [64]byte(b)))
}

func m512MaskRorvEpi64(src [64]byte, k uint8, a [64]byte, b [64]byte) [64]byte


// M512MaskzRorvEpi64: Rotate the bits in each packed 64-bit integer in 'a' to
// the right by the number of bits specified in the corresponding element of
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPRORVQ'. Intrinsic: '_mm512_maskz_rorv_epi64'.
// Requires AVX512F.
func M512MaskzRorvEpi64(k x86.Mmask8, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzRorvEpi64(uint8(k), [64]byte(a), [64]byte(b)))
}

func m512MaskzRorvEpi64(k uint8, a [64]byte, b [64]byte) [64]byte


// M512RorvEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the
// right by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst'. 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPRORVQ'. Intrinsic: '_mm512_rorv_epi64'.
// Requires AVX512F.
func M512RorvEpi64(a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512RorvEpi64([64]byte(a), [64]byte(b)))
}

func m512RorvEpi64(a [64]byte, b [64]byte) [64]byte


// MaskRoundscalePd: Round packed double-precision (64-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}	
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALEPD'. Intrinsic: '_mm_mask_roundscale_pd'.
// Requires AVX512F.
func MaskRoundscalePd(src x86.M128d, k x86.Mmask8, a x86.M128d, imm8 int) x86.M128d {
	return x86.M128d(maskRoundscalePd([2]float64(src), uint8(k), [2]float64(a), imm8))
}

func maskRoundscalePd(src [2]float64, k uint8, a [2]float64, imm8 int) [2]float64


// MaskzRoundscalePd: Round packed double-precision (64-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}	
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALEPD'. Intrinsic: '_mm_maskz_roundscale_pd'.
// Requires AVX512F.
func MaskzRoundscalePd(k x86.Mmask8, a x86.M128d, imm8 int) x86.M128d {
	return x86.M128d(maskzRoundscalePd(uint8(k), [2]float64(a), imm8))
}

func maskzRoundscalePd(k uint8, a [2]float64, imm8 int) [2]float64


// RoundscalePd: Round packed double-precision (64-bit) floating-point elements
// in 'a' to the number of fraction bits specified by 'imm8', and store the
// results in 'dst'. 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}	
//		
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALEPD'. Intrinsic: '_mm_roundscale_pd'.
// Requires AVX512F.
func RoundscalePd(a x86.M128d, imm8 int) x86.M128d {
	return x86.M128d(roundscalePd([2]float64(a), imm8))
}

func roundscalePd(a [2]float64, imm8 int) [2]float64


// M256MaskRoundscalePd: Round packed double-precision (64-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}	
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRNDSCALEPD'. Intrinsic: '_mm256_mask_roundscale_pd'.
// Requires AVX512F.
func M256MaskRoundscalePd(src x86.M256d, k x86.Mmask8, a x86.M256d, imm8 int) x86.M256d {
	return x86.M256d(m256MaskRoundscalePd([4]float64(src), uint8(k), [4]float64(a), imm8))
}

func m256MaskRoundscalePd(src [4]float64, k uint8, a [4]float64, imm8 int) [4]float64


// M256MaskzRoundscalePd: Round packed double-precision (64-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}	
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRNDSCALEPD'. Intrinsic: '_mm256_maskz_roundscale_pd'.
// Requires AVX512F.
func M256MaskzRoundscalePd(k x86.Mmask8, a x86.M256d, imm8 int) x86.M256d {
	return x86.M256d(m256MaskzRoundscalePd(uint8(k), [4]float64(a), imm8))
}

func m256MaskzRoundscalePd(k uint8, a [4]float64, imm8 int) [4]float64


// M256RoundscalePd: Round packed double-precision (64-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst'. 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}	
//		
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRNDSCALEPD'. Intrinsic: '_mm256_roundscale_pd'.
// Requires AVX512F.
func M256RoundscalePd(a x86.M256d, imm8 int) x86.M256d {
	return x86.M256d(m256RoundscalePd([4]float64(a), imm8))
}

func m256RoundscalePd(a [4]float64, imm8 int) [4]float64


// M512MaskRoundscalePd: Round packed double-precision (64-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}	
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRNDSCALEPD'. Intrinsic: '_mm512_mask_roundscale_pd'.
// Requires AVX512F.
func M512MaskRoundscalePd(src x86.M512d, k x86.Mmask8, a x86.M512d, imm8 int) x86.M512d {
	return x86.M512d(m512MaskRoundscalePd([8]float64(src), uint8(k), [8]float64(a), imm8))
}

func m512MaskRoundscalePd(src [8]float64, k uint8, a [8]float64, imm8 int) [8]float64


// M512MaskzRoundscalePd: Round packed double-precision (64-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}	
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRNDSCALEPD'. Intrinsic: '_mm512_maskz_roundscale_pd'.
// Requires AVX512F.
func M512MaskzRoundscalePd(k x86.Mmask8, a x86.M512d, imm8 int) x86.M512d {
	return x86.M512d(m512MaskzRoundscalePd(uint8(k), [8]float64(a), imm8))
}

func m512MaskzRoundscalePd(k uint8, a [8]float64, imm8 int) [8]float64


// M512RoundscalePd: Round packed double-precision (64-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst'. 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}	
//		
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRNDSCALEPD'. Intrinsic: '_mm512_roundscale_pd'.
// Requires AVX512F.
func M512RoundscalePd(a x86.M512d, imm8 int) x86.M512d {
	return x86.M512d(m512RoundscalePd([8]float64(a), imm8))
}

func m512RoundscalePd(a [8]float64, imm8 int) [8]float64


// MaskRoundscalePs: Round packed single-precision (32-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}	
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALEPS'. Intrinsic: '_mm_mask_roundscale_ps'.
// Requires AVX512F.
func MaskRoundscalePs(src x86.M128, k x86.Mmask8, a x86.M128, imm8 int) x86.M128 {
	return x86.M128(maskRoundscalePs([4]float32(src), uint8(k), [4]float32(a), imm8))
}

func maskRoundscalePs(src [4]float32, k uint8, a [4]float32, imm8 int) [4]float32


// MaskzRoundscalePs: Round packed single-precision (32-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}	
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALEPS'. Intrinsic: '_mm_maskz_roundscale_ps'.
// Requires AVX512F.
func MaskzRoundscalePs(k x86.Mmask8, a x86.M128, imm8 int) x86.M128 {
	return x86.M128(maskzRoundscalePs(uint8(k), [4]float32(a), imm8))
}

func maskzRoundscalePs(k uint8, a [4]float32, imm8 int) [4]float32


// RoundscalePs: Round packed single-precision (32-bit) floating-point elements
// in 'a' to the number of fraction bits specified by 'imm8', and store the
// results in 'dst'. 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}	
//		
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALEPS'. Intrinsic: '_mm_roundscale_ps'.
// Requires AVX512F.
func RoundscalePs(a x86.M128, imm8 int) x86.M128 {
	return x86.M128(roundscalePs([4]float32(a), imm8))
}

func roundscalePs(a [4]float32, imm8 int) [4]float32


// M256MaskRoundscalePs: Round packed single-precision (32-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}	
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRNDSCALEPS'. Intrinsic: '_mm256_mask_roundscale_ps'.
// Requires AVX512F.
func M256MaskRoundscalePs(src x86.M256, k x86.Mmask8, a x86.M256, imm8 int) x86.M256 {
	return x86.M256(m256MaskRoundscalePs([8]float32(src), uint8(k), [8]float32(a), imm8))
}

func m256MaskRoundscalePs(src [8]float32, k uint8, a [8]float32, imm8 int) [8]float32


// M256MaskzRoundscalePs: Round packed single-precision (32-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}	
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRNDSCALEPS'. Intrinsic: '_mm256_maskz_roundscale_ps'.
// Requires AVX512F.
func M256MaskzRoundscalePs(k x86.Mmask8, a x86.M256, imm8 int) x86.M256 {
	return x86.M256(m256MaskzRoundscalePs(uint8(k), [8]float32(a), imm8))
}

func m256MaskzRoundscalePs(k uint8, a [8]float32, imm8 int) [8]float32


// M256RoundscalePs: Round packed single-precision (32-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst'. 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}	
//		
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRNDSCALEPS'. Intrinsic: '_mm256_roundscale_ps'.
// Requires AVX512F.
func M256RoundscalePs(a x86.M256, imm8 int) x86.M256 {
	return x86.M256(m256RoundscalePs([8]float32(a), imm8))
}

func m256RoundscalePs(a [8]float32, imm8 int) [8]float32


// M512MaskRoundscalePs: Round packed single-precision (32-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}	
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRNDSCALEPS'. Intrinsic: '_mm512_mask_roundscale_ps'.
// Requires AVX512F.
func M512MaskRoundscalePs(src x86.M512, k x86.Mmask16, a x86.M512, imm8 int) x86.M512 {
	return x86.M512(m512MaskRoundscalePs([16]float32(src), uint16(k), [16]float32(a), imm8))
}

func m512MaskRoundscalePs(src [16]float32, k uint16, a [16]float32, imm8 int) [16]float32


// M512MaskzRoundscalePs: Round packed single-precision (32-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}	
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRNDSCALEPS'. Intrinsic: '_mm512_maskz_roundscale_ps'.
// Requires AVX512F.
func M512MaskzRoundscalePs(k x86.Mmask16, a x86.M512, imm8 int) x86.M512 {
	return x86.M512(m512MaskzRoundscalePs(uint16(k), [16]float32(a), imm8))
}

func m512MaskzRoundscalePs(k uint16, a [16]float32, imm8 int) [16]float32


// M512RoundscalePs: Round packed single-precision (32-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst'. 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}	
//		
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRNDSCALEPS'. Intrinsic: '_mm512_roundscale_ps'.
// Requires AVX512F.
func M512RoundscalePs(a x86.M512, imm8 int) x86.M512 {
	return x86.M512(m512RoundscalePs([16]float32(a), imm8))
}

func m512RoundscalePs(a [16]float32, imm8 int) [16]float32


// M512MaskRoundscaleRoundPd: Round packed double-precision (64-bit)
// floating-point elements in 'a' to the number of fraction bits specified by
// 'imm8', and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}	
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRNDSCALEPD'. Intrinsic: '_mm512_mask_roundscale_round_pd'.
// Requires AVX512F.
func M512MaskRoundscaleRoundPd(src x86.M512d, k x86.Mmask8, a x86.M512d, imm8 int, rounding int) x86.M512d {
	return x86.M512d(m512MaskRoundscaleRoundPd([8]float64(src), uint8(k), [8]float64(a), imm8, rounding))
}

func m512MaskRoundscaleRoundPd(src [8]float64, k uint8, a [8]float64, imm8 int, rounding int) [8]float64


// M512MaskzRoundscaleRoundPd: Round packed double-precision (64-bit)
// floating-point elements in 'a' to the number of fraction bits specified by
// 'imm8', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}	
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRNDSCALEPD'. Intrinsic: '_mm512_maskz_roundscale_round_pd'.
// Requires AVX512F.
func M512MaskzRoundscaleRoundPd(k x86.Mmask8, a x86.M512d, imm8 int, rounding int) x86.M512d {
	return x86.M512d(m512MaskzRoundscaleRoundPd(uint8(k), [8]float64(a), imm8, rounding))
}

func m512MaskzRoundscaleRoundPd(k uint8, a [8]float64, imm8 int, rounding int) [8]float64


// M512RoundscaleRoundPd: Round packed double-precision (64-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}	
//		
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRNDSCALEPD'. Intrinsic: '_mm512_roundscale_round_pd'.
// Requires AVX512F.
func M512RoundscaleRoundPd(a x86.M512d, imm8 int, rounding int) x86.M512d {
	return x86.M512d(m512RoundscaleRoundPd([8]float64(a), imm8, rounding))
}

func m512RoundscaleRoundPd(a [8]float64, imm8 int, rounding int) [8]float64


// M512MaskRoundscaleRoundPs: Round packed single-precision (32-bit)
// floating-point elements in 'a' to the number of fraction bits specified by
// 'imm8', and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}	
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRNDSCALEPS'. Intrinsic: '_mm512_mask_roundscale_round_ps'.
// Requires AVX512F.
func M512MaskRoundscaleRoundPs(src x86.M512, k x86.Mmask16, a x86.M512, imm8 int, rounding int) x86.M512 {
	return x86.M512(m512MaskRoundscaleRoundPs([16]float32(src), uint16(k), [16]float32(a), imm8, rounding))
}

func m512MaskRoundscaleRoundPs(src [16]float32, k uint16, a [16]float32, imm8 int, rounding int) [16]float32


// M512MaskzRoundscaleRoundPs: Round packed single-precision (32-bit)
// floating-point elements in 'a' to the number of fraction bits specified by
// 'imm8', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}	
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRNDSCALEPS'. Intrinsic: '_mm512_maskz_roundscale_round_ps'.
// Requires AVX512F.
func M512MaskzRoundscaleRoundPs(k x86.Mmask16, a x86.M512, imm8 int, rounding int) x86.M512 {
	return x86.M512(m512MaskzRoundscaleRoundPs(uint16(k), [16]float32(a), imm8, rounding))
}

func m512MaskzRoundscaleRoundPs(k uint16, a [16]float32, imm8 int, rounding int) [16]float32


// M512RoundscaleRoundPs: Round packed single-precision (32-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}	
//		
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRNDSCALEPS'. Intrinsic: '_mm512_roundscale_round_ps'.
// Requires AVX512F.
func M512RoundscaleRoundPs(a x86.M512, imm8 int, rounding int) x86.M512 {
	return x86.M512(m512RoundscaleRoundPs([16]float32(a), imm8, rounding))
}

func m512RoundscaleRoundPs(a [16]float32, imm8 int, rounding int) [16]float32


// MaskRoundscaleRoundSd: Round the lower double-precision (64-bit)
// floating-point element in 'a' to the number of fraction bits specified by
// 'imm8', store the result in the lower element of 'dst' using writemask 'k'
// (the element is copied from 'src' when mask bit 0 is not set), and copy the
// upper element from 'b' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}		
//		
//		IF k[0]
//			dst[63:0] := RoundTo_IntegerPD(a[63:0], imm8[7:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALESD'. Intrinsic: '_mm_mask_roundscale_round_sd'.
// Requires AVX512F.
func MaskRoundscaleRoundSd(src x86.M128d, k x86.Mmask8, a x86.M128d, b x86.M128d, imm8 int, rounding int) x86.M128d {
	return x86.M128d(maskRoundscaleRoundSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), imm8, rounding))
}

func maskRoundscaleRoundSd(src [2]float64, k uint8, a [2]float64, b [2]float64, imm8 int, rounding int) [2]float64


// MaskzRoundscaleRoundSd: Round the lower double-precision (64-bit)
// floating-point element in 'a' to the number of fraction bits specified by
// 'imm8', store the result in the lower element of 'dst' using zeromask 'k'
// (the element is zeroed out when mask bit 0 is not set), and copy the upper
// element from 'b' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}		
//		
//		IF k[0]
//			dst[63:0] := RoundTo_IntegerPD(a[63:0], imm8[7:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALESD'. Intrinsic: '_mm_maskz_roundscale_round_sd'.
// Requires AVX512F.
func MaskzRoundscaleRoundSd(k x86.Mmask8, a x86.M128d, b x86.M128d, imm8 int, rounding int) x86.M128d {
	return x86.M128d(maskzRoundscaleRoundSd(uint8(k), [2]float64(a), [2]float64(b), imm8, rounding))
}

func maskzRoundscaleRoundSd(k uint8, a [2]float64, b [2]float64, imm8 int, rounding int) [2]float64


// RoundscaleRoundSd: Round the lower double-precision (64-bit) floating-point
// element in 'a' to the number of fraction bits specified by 'imm8', store the
// result in the lower element of 'dst', and copy the upper element from 'b' to
// the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}		
//		
//		dst[63:0] := RoundTo_IntegerPD(a[63:0], imm8[7:0])
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALESD'. Intrinsic: '_mm_roundscale_round_sd'.
// Requires AVX512F.
func RoundscaleRoundSd(a x86.M128d, b x86.M128d, imm8 int, rounding int) x86.M128d {
	return x86.M128d(roundscaleRoundSd([2]float64(a), [2]float64(b), imm8, rounding))
}

func roundscaleRoundSd(a [2]float64, b [2]float64, imm8 int, rounding int) [2]float64


// MaskRoundscaleRoundSs: Round the lower single-precision (32-bit)
// floating-point element in 'a' to the number of fraction bits specified by
// 'imm8', store the result in the lower element of 'dst' using writemask 'k'
// (the element is copied from 'src' when mask bit 0 is not set), and copy the
// upper 3 packed elements from 'b' to the upper elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}
//		
//		IF k[0]
//			dst[31:0] := RoundTo_IntegerPS(a[31:0], imm8[7:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALESS'. Intrinsic: '_mm_mask_roundscale_round_ss'.
// Requires AVX512F.
func MaskRoundscaleRoundSs(src x86.M128, k x86.Mmask8, a x86.M128, b x86.M128, imm8 int, rounding int) x86.M128 {
	return x86.M128(maskRoundscaleRoundSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), imm8, rounding))
}

func maskRoundscaleRoundSs(src [4]float32, k uint8, a [4]float32, b [4]float32, imm8 int, rounding int) [4]float32


// MaskzRoundscaleRoundSs: Round the lower single-precision (32-bit)
// floating-point element in 'a' to the number of fraction bits specified by
// 'imm8', store the result in the lower element of 'dst' using zeromask 'k'
// (the element is zeroed out when mask bit 0 is not set), and copy the upper 3
// packed elements from 'b' to the upper elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}
//		
//		IF k[0]
//			dst[31:0] := RoundTo_IntegerPS(a[31:0], imm8[7:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALESS'. Intrinsic: '_mm_maskz_roundscale_round_ss'.
// Requires AVX512F.
func MaskzRoundscaleRoundSs(k x86.Mmask8, a x86.M128, b x86.M128, imm8 int, rounding int) x86.M128 {
	return x86.M128(maskzRoundscaleRoundSs(uint8(k), [4]float32(a), [4]float32(b), imm8, rounding))
}

func maskzRoundscaleRoundSs(k uint8, a [4]float32, b [4]float32, imm8 int, rounding int) [4]float32


// RoundscaleRoundSs: Round the lower single-precision (32-bit) floating-point
// element in 'a' to the number of fraction bits specified by 'imm8', store the
// result in the lower element of 'dst', and copy the upper 3 packed elements
// from 'b' to the upper elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}
//		
//		dst[31:0] := RoundTo_IntegerPS(a[31:0], imm8[7:0])
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALESS'. Intrinsic: '_mm_roundscale_round_ss'.
// Requires AVX512F.
func RoundscaleRoundSs(a x86.M128, b x86.M128, imm8 int, rounding int) x86.M128 {
	return x86.M128(roundscaleRoundSs([4]float32(a), [4]float32(b), imm8, rounding))
}

func roundscaleRoundSs(a [4]float32, b [4]float32, imm8 int, rounding int) [4]float32


// MaskRoundscaleSd: Round the lower double-precision (64-bit) floating-point
// element in 'a' to the number of fraction bits specified by 'imm8', store the
// result in the lower element of 'dst' using writemask 'k' (the element is
// copied from 'src' when mask bit 0 is not set), and copy the upper element
// from 'b' to the upper element of 'dst'. 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}		
//		
//		IF k[0]
//			dst[63:0] := RoundTo_IntegerPD(a[63:0], imm8[7:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALESD'. Intrinsic: '_mm_mask_roundscale_sd'.
// Requires AVX512F.
func MaskRoundscaleSd(src x86.M128d, k x86.Mmask8, a x86.M128d, b x86.M128d, imm8 int) x86.M128d {
	return x86.M128d(maskRoundscaleSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), imm8))
}

func maskRoundscaleSd(src [2]float64, k uint8, a [2]float64, b [2]float64, imm8 int) [2]float64


// MaskzRoundscaleSd: Round the lower double-precision (64-bit) floating-point
// element in 'a' to the number of fraction bits specified by 'imm8', store the
// result in the lower element of 'dst' using zeromask 'k' (the element is
// zeroed out when mask bit 0 is not set), and copy the upper element from 'b'
// to the upper element of 'dst'. 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}		
//		
//		IF k[0]
//			dst[63:0] := RoundTo_IntegerPD(a[63:0], imm8[7:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALESD'. Intrinsic: '_mm_maskz_roundscale_sd'.
// Requires AVX512F.
func MaskzRoundscaleSd(k x86.Mmask8, a x86.M128d, b x86.M128d, imm8 int) x86.M128d {
	return x86.M128d(maskzRoundscaleSd(uint8(k), [2]float64(a), [2]float64(b), imm8))
}

func maskzRoundscaleSd(k uint8, a [2]float64, b [2]float64, imm8 int) [2]float64


// RoundscaleSd: Round the lower double-precision (64-bit) floating-point
// element in 'a' to the number of fraction bits specified by 'imm8', store the
// result in the lower element of 'dst', and copy the upper element from 'b' to
// the upper element of 'dst'. 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}		
//		
//		dst[63:0] := RoundTo_IntegerPD(a[63:0], imm8[7:0])
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALESD'. Intrinsic: '_mm_roundscale_sd'.
// Requires AVX512F.
func RoundscaleSd(a x86.M128d, b x86.M128d, imm8 int) x86.M128d {
	return x86.M128d(roundscaleSd([2]float64(a), [2]float64(b), imm8))
}

func roundscaleSd(a [2]float64, b [2]float64, imm8 int) [2]float64


// MaskRoundscaleSs: Round the lower single-precision (32-bit) floating-point
// element in 'a' to the number of fraction bits specified by 'imm8', store the
// result in the lower element of 'dst' using writemask 'k' (the element is
// copied from 'src' when mask bit 0 is not set), and copy the upper 3 packed
// elements from 'b' to the upper elements of 'dst'. 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}
//		
//		IF k[0]
//			dst[31:0] := RoundTo_IntegerPS(a[31:0], imm8[7:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALESS'. Intrinsic: '_mm_mask_roundscale_ss'.
// Requires AVX512F.
func MaskRoundscaleSs(src x86.M128, k x86.Mmask8, a x86.M128, b x86.M128, imm8 int) x86.M128 {
	return x86.M128(maskRoundscaleSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), imm8))
}

func maskRoundscaleSs(src [4]float32, k uint8, a [4]float32, b [4]float32, imm8 int) [4]float32


// MaskzRoundscaleSs: Round the lower single-precision (32-bit) floating-point
// element in 'a' to the number of fraction bits specified by 'imm8', store the
// result in the lower element of 'dst' using zeromask 'k' (the element is
// zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements
// from 'b' to the upper elements of 'dst'. 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}
//		
//		IF k[0]
//			dst[31:0] := RoundTo_IntegerPS(a[31:0], imm8[7:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALESS'. Intrinsic: '_mm_maskz_roundscale_ss'.
// Requires AVX512F.
func MaskzRoundscaleSs(k x86.Mmask8, a x86.M128, b x86.M128, imm8 int) x86.M128 {
	return x86.M128(maskzRoundscaleSs(uint8(k), [4]float32(a), [4]float32(b), imm8))
}

func maskzRoundscaleSs(k uint8, a [4]float32, b [4]float32, imm8 int) [4]float32


// RoundscaleSs: Round the lower single-precision (32-bit) floating-point
// element in 'a' to the number of fraction bits specified by 'imm8', store the
// result in the lower element of 'dst', and copy the upper 3 packed elements
// from 'b' to the upper elements of 'dst'. 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}
//		
//		dst[31:0] := RoundTo_IntegerPS(a[31:0], imm8[7:0])
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALESS'. Intrinsic: '_mm_roundscale_ss'.
// Requires AVX512F.
func RoundscaleSs(a x86.M128, b x86.M128, imm8 int) x86.M128 {
	return x86.M128(roundscaleSs([4]float32(a), [4]float32(b), imm8))
}

func roundscaleSs(a [4]float32, b [4]float32, imm8 int) [4]float32


// MaskRsqrt14Pd: Compute the approximate reciprocal square root of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := APPROXIMATE(1.0 / SQRT(a[i+63:i]))
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRSQRT14PD'. Intrinsic: '_mm_mask_rsqrt14_pd'.
// Requires AVX512F.
func MaskRsqrt14Pd(src x86.M128d, k x86.Mmask8, a x86.M128d) x86.M128d {
	return x86.M128d(maskRsqrt14Pd([2]float64(src), uint8(k), [2]float64(a)))
}

func maskRsqrt14Pd(src [2]float64, k uint8, a [2]float64) [2]float64


// MaskzRsqrt14Pd: Compute the approximate reciprocal square root of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := APPROXIMATE(1.0 / SQRT(a[i+63:i]))
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRSQRT14PD'. Intrinsic: '_mm_maskz_rsqrt14_pd'.
// Requires AVX512F.
func MaskzRsqrt14Pd(k x86.Mmask8, a x86.M128d) x86.M128d {
	return x86.M128d(maskzRsqrt14Pd(uint8(k), [2]float64(a)))
}

func maskzRsqrt14Pd(k uint8, a [2]float64) [2]float64


// M256MaskRsqrt14Pd: Compute the approximate reciprocal square root of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := APPROXIMATE(1.0 / SQRT(a[i+63:i]))
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRSQRT14PD'. Intrinsic: '_mm256_mask_rsqrt14_pd'.
// Requires AVX512F.
func M256MaskRsqrt14Pd(src x86.M256d, k x86.Mmask8, a x86.M256d) x86.M256d {
	return x86.M256d(m256MaskRsqrt14Pd([4]float64(src), uint8(k), [4]float64(a)))
}

func m256MaskRsqrt14Pd(src [4]float64, k uint8, a [4]float64) [4]float64


// M256MaskzRsqrt14Pd: Compute the approximate reciprocal square root of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := APPROXIMATE(1.0 / SQRT(a[i+63:i]))
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRSQRT14PD'. Intrinsic: '_mm256_maskz_rsqrt14_pd'.
// Requires AVX512F.
func M256MaskzRsqrt14Pd(k x86.Mmask8, a x86.M256d) x86.M256d {
	return x86.M256d(m256MaskzRsqrt14Pd(uint8(k), [4]float64(a)))
}

func m256MaskzRsqrt14Pd(k uint8, a [4]float64) [4]float64


// M512MaskRsqrt14Pd: Compute the approximate reciprocal square root of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := APPROXIMATE(1.0 / SQRT(a[i+63:i]))
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRSQRT14PD'. Intrinsic: '_mm512_mask_rsqrt14_pd'.
// Requires AVX512F.
func M512MaskRsqrt14Pd(src x86.M512d, k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskRsqrt14Pd([8]float64(src), uint8(k), [8]float64(a)))
}

func m512MaskRsqrt14Pd(src [8]float64, k uint8, a [8]float64) [8]float64


// M512MaskzRsqrt14Pd: Compute the approximate reciprocal square root of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := APPROXIMATE(1.0 / SQRT(a[i+63:i]))
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRSQRT14PD'. Intrinsic: '_mm512_maskz_rsqrt14_pd'.
// Requires AVX512F.
func M512MaskzRsqrt14Pd(k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskzRsqrt14Pd(uint8(k), [8]float64(a)))
}

func m512MaskzRsqrt14Pd(k uint8, a [8]float64) [8]float64


// M512Rsqrt14Pd: Compute the approximate reciprocal square root of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst'. The maximum relative error for this approximation is less
// than 2^-14. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := APPROXIMATE(1.0 / SQRT(a[i+63:i]))
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRSQRT14PD'. Intrinsic: '_mm512_rsqrt14_pd'.
// Requires AVX512F.
func M512Rsqrt14Pd(a x86.M512d) x86.M512d {
	return x86.M512d(m512Rsqrt14Pd([8]float64(a)))
}

func m512Rsqrt14Pd(a [8]float64) [8]float64


// MaskRsqrt14Ps: Compute the approximate reciprocal square root of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := APPROXIMATE(1.0 / SQRT(a[i+31:i]))
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRSQRT14PS'. Intrinsic: '_mm_mask_rsqrt14_ps'.
// Requires AVX512F.
func MaskRsqrt14Ps(src x86.M128, k x86.Mmask8, a x86.M128) x86.M128 {
	return x86.M128(maskRsqrt14Ps([4]float32(src), uint8(k), [4]float32(a)))
}

func maskRsqrt14Ps(src [4]float32, k uint8, a [4]float32) [4]float32


// MaskzRsqrt14Ps: Compute the approximate reciprocal square root of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := APPROXIMATE(1.0 / SQRT(a[i+31:i]))
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRSQRT14PS'. Intrinsic: '_mm_maskz_rsqrt14_ps'.
// Requires AVX512F.
func MaskzRsqrt14Ps(k x86.Mmask8, a x86.M128) x86.M128 {
	return x86.M128(maskzRsqrt14Ps(uint8(k), [4]float32(a)))
}

func maskzRsqrt14Ps(k uint8, a [4]float32) [4]float32


// M256MaskRsqrt14Ps: Compute the approximate reciprocal square root of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := APPROXIMATE(1.0 / SQRT(a[i+31:i]))
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRSQRT14PS'. Intrinsic: '_mm256_mask_rsqrt14_ps'.
// Requires AVX512F.
func M256MaskRsqrt14Ps(src x86.M256, k x86.Mmask8, a x86.M256) x86.M256 {
	return x86.M256(m256MaskRsqrt14Ps([8]float32(src), uint8(k), [8]float32(a)))
}

func m256MaskRsqrt14Ps(src [8]float32, k uint8, a [8]float32) [8]float32


// M256MaskzRsqrt14Ps: Compute the approximate reciprocal square root of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := APPROXIMATE(1.0 / SQRT(a[i+31:i]))
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRSQRT14PS'. Intrinsic: '_mm256_maskz_rsqrt14_ps'.
// Requires AVX512F.
func M256MaskzRsqrt14Ps(k x86.Mmask8, a x86.M256) x86.M256 {
	return x86.M256(m256MaskzRsqrt14Ps(uint8(k), [8]float32(a)))
}

func m256MaskzRsqrt14Ps(k uint8, a [8]float32) [8]float32


// M512MaskRsqrt14Ps: Compute the approximate reciprocal square root of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := APPROXIMATE(1.0 / SQRT(a[i+31:i]))
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRSQRT14PS'. Intrinsic: '_mm512_mask_rsqrt14_ps'.
// Requires AVX512F.
func M512MaskRsqrt14Ps(src x86.M512, k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskRsqrt14Ps([16]float32(src), uint16(k), [16]float32(a)))
}

func m512MaskRsqrt14Ps(src [16]float32, k uint16, a [16]float32) [16]float32


// M512MaskzRsqrt14Ps: Compute the approximate reciprocal square root of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := APPROXIMATE(1.0 / SQRT(a[i+31:i]))
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRSQRT14PS'. Intrinsic: '_mm512_maskz_rsqrt14_ps'.
// Requires AVX512F.
func M512MaskzRsqrt14Ps(k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskzRsqrt14Ps(uint16(k), [16]float32(a)))
}

func m512MaskzRsqrt14Ps(k uint16, a [16]float32) [16]float32


// M512Rsqrt14Ps: Compute the approximate reciprocal square root of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst'. The maximum relative error for this approximation is less
// than 2^-14. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := APPROXIMATE(1.0 / SQRT(a[i+31:i]))
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRSQRT14PS'. Intrinsic: '_mm512_rsqrt14_ps'.
// Requires AVX512F.
func M512Rsqrt14Ps(a x86.M512) x86.M512 {
	return x86.M512(m512Rsqrt14Ps([16]float32(a)))
}

func m512Rsqrt14Ps(a [16]float32) [16]float32


// MaskRsqrt14Sd: Compute the approximate reciprocal square root of the lower
// double-precision (64-bit) floating-point element in 'b', store the result in
// the lower element of 'dst' using writemask 'k' (the element is copied from
// 'src' when mask bit 0 is not set), and copy the upper element from 'a' to
// the upper element of 'dst'. The maximum relative error for this
// approximation is less than 2^-14. 
//
//		IF k[0]
//			dst[63:0] := APPROXIMATE(1.0 / SQRT(b[63:0]))
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VRSQRT14SD'. Intrinsic: '_mm_mask_rsqrt14_sd'.
// Requires AVX512F.
func MaskRsqrt14Sd(src x86.M128d, k x86.Mmask8, a x86.M128d, b x86.M128d) x86.M128d {
	return x86.M128d(maskRsqrt14Sd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskRsqrt14Sd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzRsqrt14Sd: Compute the approximate reciprocal square root of the lower
// double-precision (64-bit) floating-point element in 'b', store the result in
// the lower element of 'dst' using zeromask 'k' (the element is zeroed out
// when mask bit 0 is not set), and copy the upper element from 'a' to the
// upper element of 'dst'. The maximum relative error for this approximation is
// less than 2^-14. 
//
//		IF k[0]
//			dst[63:0] := APPROXIMATE(1.0 / SQRT(b[63:0]))
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VRSQRT14SD'. Intrinsic: '_mm_maskz_rsqrt14_sd'.
// Requires AVX512F.
func MaskzRsqrt14Sd(k x86.Mmask8, a x86.M128d, b x86.M128d) x86.M128d {
	return x86.M128d(maskzRsqrt14Sd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzRsqrt14Sd(k uint8, a [2]float64, b [2]float64) [2]float64


// Rsqrt14Sd: Compute the approximate reciprocal square root of the lower
// double-precision (64-bit) floating-point element in 'b', store the result in
// the lower element of 'dst', and copy the upper element from 'a' to the upper
// element of 'dst'. The maximum relative error for this approximation is less
// than 2^-14. 
//
//		dst[63:0] := APPROXIMATE(1.0 / SQRT(b[63:0]))
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VRSQRT14SD'. Intrinsic: '_mm_rsqrt14_sd'.
// Requires AVX512F.
func Rsqrt14Sd(a x86.M128d, b x86.M128d) x86.M128d {
	return x86.M128d(rsqrt14Sd([2]float64(a), [2]float64(b)))
}

func rsqrt14Sd(a [2]float64, b [2]float64) [2]float64


// MaskRsqrt14Ss: Compute the approximate reciprocal square root of the lower
// single-precision (32-bit) floating-point element in 'b', store the result in
// the lower element of 'dst' using writemask 'k' (the element is copied from
// 'src' when mask bit 0 is not set), and copy the upper 3 packed elements from
// 'a' to the upper elements of 'dst'. The maximum relative error for this
// approximation is less than 2^-14. 
//
//		IF k[0]
//			dst[31:0] := APPROXIMATE(1.0 / SQRT(b[31:0]))
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VRSQRT14SS'. Intrinsic: '_mm_mask_rsqrt14_ss'.
// Requires AVX512F.
func MaskRsqrt14Ss(src x86.M128, k x86.Mmask8, a x86.M128, b x86.M128) x86.M128 {
	return x86.M128(maskRsqrt14Ss([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskRsqrt14Ss(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzRsqrt14Ss: Compute the approximate reciprocal square root of the lower
// single-precision (32-bit) floating-point element in 'b', store the result in
// the lower element of 'dst' using zeromask 'k' (the element is zeroed out
// when mask bit 0 is not set), and copy the upper 3 packed elements from 'a'
// to the upper elements of 'dst'. The maximum relative error for this
// approximation is less than 2^-14. 
//
//		IF k[0]
//			dst[31:0] := APPROXIMATE(1.0 / SQRT(b[31:0]))
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VRSQRT14SS'. Intrinsic: '_mm_maskz_rsqrt14_ss'.
// Requires AVX512F.
func MaskzRsqrt14Ss(k x86.Mmask8, a x86.M128, b x86.M128) x86.M128 {
	return x86.M128(maskzRsqrt14Ss(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzRsqrt14Ss(k uint8, a [4]float32, b [4]float32) [4]float32


// Rsqrt14Ss: Compute the approximate reciprocal square root of the lower
// single-precision (32-bit) floating-point element in 'b', store the result in
// the lower element of 'dst', and copy the upper 3 packed elements from 'a' to
// the upper elements of 'dst'. The maximum relative error for this
// approximation is less than 2^-14. 
//
//		dst[31:0] := APPROXIMATE(1.0 / SQRT(b[31:0]))
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VRSQRT14SS'. Intrinsic: '_mm_rsqrt14_ss'.
// Requires AVX512F.
func Rsqrt14Ss(a x86.M128, b x86.M128) x86.M128 {
	return x86.M128(rsqrt14Ss([4]float32(a), [4]float32(b)))
}

func rsqrt14Ss(a [4]float32, b [4]float32) [4]float32


// MaskScalefPd: Scale the packed double-precision (64-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFPD'. Intrinsic: '_mm_mask_scalef_pd'.
// Requires AVX512F.
func MaskScalefPd(src x86.M128d, k x86.Mmask8, a x86.M128d, b x86.M128d) x86.M128d {
	return x86.M128d(maskScalefPd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskScalefPd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzScalefPd: Scale the packed double-precision (64-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFPD'. Intrinsic: '_mm_maskz_scalef_pd'.
// Requires AVX512F.
func MaskzScalefPd(k x86.Mmask8, a x86.M128d, b x86.M128d) x86.M128d {
	return x86.M128d(maskzScalefPd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzScalefPd(k uint8, a [2]float64, b [2]float64) [2]float64


// ScalefPd: Scale the packed double-precision (64-bit) floating-point elements
// in 'a' using values from 'b', and store the results in 'dst'. 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFPD'. Intrinsic: '_mm_scalef_pd'.
// Requires AVX512F.
func ScalefPd(a x86.M128d, b x86.M128d) x86.M128d {
	return x86.M128d(scalefPd([2]float64(a), [2]float64(b)))
}

func scalefPd(a [2]float64, b [2]float64) [2]float64


// M256MaskScalefPd: Scale the packed double-precision (64-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSCALEFPD'. Intrinsic: '_mm256_mask_scalef_pd'.
// Requires AVX512F.
func M256MaskScalefPd(src x86.M256d, k x86.Mmask8, a x86.M256d, b x86.M256d) x86.M256d {
	return x86.M256d(m256MaskScalefPd([4]float64(src), uint8(k), [4]float64(a), [4]float64(b)))
}

func m256MaskScalefPd(src [4]float64, k uint8, a [4]float64, b [4]float64) [4]float64


// M256MaskzScalefPd: Scale the packed double-precision (64-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSCALEFPD'. Intrinsic: '_mm256_maskz_scalef_pd'.
// Requires AVX512F.
func M256MaskzScalefPd(k x86.Mmask8, a x86.M256d, b x86.M256d) x86.M256d {
	return x86.M256d(m256MaskzScalefPd(uint8(k), [4]float64(a), [4]float64(b)))
}

func m256MaskzScalefPd(k uint8, a [4]float64, b [4]float64) [4]float64


// M256ScalefPd: Scale the packed double-precision (64-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst'. 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSCALEFPD'. Intrinsic: '_mm256_scalef_pd'.
// Requires AVX512F.
func M256ScalefPd(a x86.M256d, b x86.M256d) x86.M256d {
	return x86.M256d(m256ScalefPd([4]float64(a), [4]float64(b)))
}

func m256ScalefPd(a [4]float64, b [4]float64) [4]float64


// M512MaskScalefPd: Scale the packed double-precision (64-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSCALEFPD'. Intrinsic: '_mm512_mask_scalef_pd'.
// Requires AVX512F.
func M512MaskScalefPd(src x86.M512d, k x86.Mmask8, a x86.M512d, b x86.M512d) x86.M512d {
	return x86.M512d(m512MaskScalefPd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b)))
}

func m512MaskScalefPd(src [8]float64, k uint8, a [8]float64, b [8]float64) [8]float64


// M512MaskzScalefPd: Scale the packed double-precision (64-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSCALEFPD'. Intrinsic: '_mm512_maskz_scalef_pd'.
// Requires AVX512F.
func M512MaskzScalefPd(k x86.Mmask8, a x86.M512d, b x86.M512d) x86.M512d {
	return x86.M512d(m512MaskzScalefPd(uint8(k), [8]float64(a), [8]float64(b)))
}

func m512MaskzScalefPd(k uint8, a [8]float64, b [8]float64) [8]float64


// M512ScalefPd: Scale the packed double-precision (64-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst'. 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSCALEFPD'. Intrinsic: '_mm512_scalef_pd'.
// Requires AVX512F.
func M512ScalefPd(a x86.M512d, b x86.M512d) x86.M512d {
	return x86.M512d(m512ScalefPd([8]float64(a), [8]float64(b)))
}

func m512ScalefPd(a [8]float64, b [8]float64) [8]float64


// MaskScalefPs: Scale the packed single-precision (32-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[31:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFPS'. Intrinsic: '_mm_mask_scalef_ps'.
// Requires AVX512F.
func MaskScalefPs(src x86.M128, k x86.Mmask8, a x86.M128, b x86.M128) x86.M128 {
	return x86.M128(maskScalefPs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskScalefPs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzScalefPs: Scale the packed single-precision (32-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[31:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFPS'. Intrinsic: '_mm_maskz_scalef_ps'.
// Requires AVX512F.
func MaskzScalefPs(k x86.Mmask8, a x86.M128, b x86.M128) x86.M128 {
	return x86.M128(maskzScalefPs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzScalefPs(k uint8, a [4]float32, b [4]float32) [4]float32


// ScalefPs: Scale the packed single-precision (32-bit) floating-point elements
// in 'a' using values from 'b', and store the results in 'dst'. 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[31:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFPS'. Intrinsic: '_mm_scalef_ps'.
// Requires AVX512F.
func ScalefPs(a x86.M128, b x86.M128) x86.M128 {
	return x86.M128(scalefPs([4]float32(a), [4]float32(b)))
}

func scalefPs(a [4]float32, b [4]float32) [4]float32


// M256MaskScalefPs: Scale the packed single-precision (32-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[31:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSCALEFPS'. Intrinsic: '_mm256_mask_scalef_ps'.
// Requires AVX512F.
func M256MaskScalefPs(src x86.M256, k x86.Mmask8, a x86.M256, b x86.M256) x86.M256 {
	return x86.M256(m256MaskScalefPs([8]float32(src), uint8(k), [8]float32(a), [8]float32(b)))
}

func m256MaskScalefPs(src [8]float32, k uint8, a [8]float32, b [8]float32) [8]float32


// M256MaskzScalefPs: Scale the packed single-precision (32-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[31:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSCALEFPS'. Intrinsic: '_mm256_maskz_scalef_ps'.
// Requires AVX512F.
func M256MaskzScalefPs(k x86.Mmask8, a x86.M256, b x86.M256) x86.M256 {
	return x86.M256(m256MaskzScalefPs(uint8(k), [8]float32(a), [8]float32(b)))
}

func m256MaskzScalefPs(k uint8, a [8]float32, b [8]float32) [8]float32


// M256ScalefPs: Scale the packed single-precision (32-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst'. 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[31:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSCALEFPS'. Intrinsic: '_mm256_scalef_ps'.
// Requires AVX512F.
func M256ScalefPs(a x86.M256, b x86.M256) x86.M256 {
	return x86.M256(m256ScalefPs([8]float32(a), [8]float32(b)))
}

func m256ScalefPs(a [8]float32, b [8]float32) [8]float32


// M512MaskScalefPs: Scale the packed single-precision (32-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[31:0]
//		}
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSCALEFPS'. Intrinsic: '_mm512_mask_scalef_ps'.
// Requires AVX512F.
func M512MaskScalefPs(src x86.M512, k x86.Mmask16, a x86.M512, b x86.M512) x86.M512 {
	return x86.M512(m512MaskScalefPs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b)))
}

func m512MaskScalefPs(src [16]float32, k uint16, a [16]float32, b [16]float32) [16]float32


// M512MaskzScalefPs: Scale the packed single-precision (32-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[31:0]
//		}
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSCALEFPS'. Intrinsic: '_mm512_maskz_scalef_ps'.
// Requires AVX512F.
func M512MaskzScalefPs(k x86.Mmask16, a x86.M512, b x86.M512) x86.M512 {
	return x86.M512(m512MaskzScalefPs(uint16(k), [16]float32(a), [16]float32(b)))
}

func m512MaskzScalefPs(k uint16, a [16]float32, b [16]float32) [16]float32


// M512ScalefPs: Scale the packed single-precision (32-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst'. 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[31:0]
//		}
//		
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSCALEFPS'. Intrinsic: '_mm512_scalef_ps'.
// Requires AVX512F.
func M512ScalefPs(a x86.M512, b x86.M512) x86.M512 {
	return x86.M512(m512ScalefPs([16]float32(a), [16]float32(b)))
}

func m512ScalefPs(a [16]float32, b [16]float32) [16]float32


// M512MaskScalefRoundPd: Scale the packed double-precision (64-bit)
// floating-point elements in 'a' using values from 'b', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSCALEFPD'. Intrinsic: '_mm512_mask_scalef_round_pd'.
// Requires AVX512F.
func M512MaskScalefRoundPd(src x86.M512d, k x86.Mmask8, a x86.M512d, b x86.M512d, rounding int) x86.M512d {
	return x86.M512d(m512MaskScalefRoundPd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b), rounding))
}

func m512MaskScalefRoundPd(src [8]float64, k uint8, a [8]float64, b [8]float64, rounding int) [8]float64


// M512MaskzScalefRoundPd: Scale the packed double-precision (64-bit)
// floating-point elements in 'a' using values from 'b', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSCALEFPD'. Intrinsic: '_mm512_maskz_scalef_round_pd'.
// Requires AVX512F.
func M512MaskzScalefRoundPd(k x86.Mmask8, a x86.M512d, b x86.M512d, rounding int) x86.M512d {
	return x86.M512d(m512MaskzScalefRoundPd(uint8(k), [8]float64(a), [8]float64(b), rounding))
}

func m512MaskzScalefRoundPd(k uint8, a [8]float64, b [8]float64, rounding int) [8]float64


// M512ScalefRoundPd: Scale the packed double-precision (64-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSCALEFPD'. Intrinsic: '_mm512_scalef_round_pd'.
// Requires AVX512F.
func M512ScalefRoundPd(a x86.M512d, b x86.M512d, rounding int) x86.M512d {
	return x86.M512d(m512ScalefRoundPd([8]float64(a), [8]float64(b), rounding))
}

func m512ScalefRoundPd(a [8]float64, b [8]float64, rounding int) [8]float64


// M512MaskScalefRoundPs: Scale the packed single-precision (32-bit)
// floating-point elements in 'a' using values from 'b', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[31:0]
//		}
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSCALEFPS'. Intrinsic: '_mm512_mask_scalef_round_ps'.
// Requires AVX512F.
func M512MaskScalefRoundPs(src x86.M512, k x86.Mmask16, a x86.M512, b x86.M512, rounding int) x86.M512 {
	return x86.M512(m512MaskScalefRoundPs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b), rounding))
}

func m512MaskScalefRoundPs(src [16]float32, k uint16, a [16]float32, b [16]float32, rounding int) [16]float32


// M512MaskzScalefRoundPs: Scale the packed single-precision (32-bit)
// floating-point elements in 'a' using values from 'b', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[31:0]
//		}
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSCALEFPS'. Intrinsic: '_mm512_maskz_scalef_round_ps'.
// Requires AVX512F.
func M512MaskzScalefRoundPs(k x86.Mmask16, a x86.M512, b x86.M512, rounding int) x86.M512 {
	return x86.M512(m512MaskzScalefRoundPs(uint16(k), [16]float32(a), [16]float32(b), rounding))
}

func m512MaskzScalefRoundPs(k uint16, a [16]float32, b [16]float32, rounding int) [16]float32


// M512ScalefRoundPs: Scale the packed single-precision (32-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[31:0]
//		}
//		
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSCALEFPS'. Intrinsic: '_mm512_scalef_round_ps'.
// Requires AVX512F.
func M512ScalefRoundPs(a x86.M512, b x86.M512, rounding int) x86.M512 {
	return x86.M512(m512ScalefRoundPs([16]float32(a), [16]float32(b), rounding))
}

func m512ScalefRoundPs(a [16]float32, b [16]float32, rounding int) [16]float32


// MaskScalefRoundSd: Scale the packed double-precision (64-bit) floating-point
// elements in 'a' using values from 'b', store the result in the lower element
// of 'dst' using writemask 'k' (the element is copied from 'src' when mask bit
// 0 is not set), and copy the upper element from 'b' to the upper element of
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		IF k[0]
//			dst[63:0] := SCALE(a[63:0], b[63:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFSD'. Intrinsic: '_mm_mask_scalef_round_sd'.
// Requires AVX512F.
func MaskScalefRoundSd(src x86.M128d, k x86.Mmask8, a x86.M128d, b x86.M128d, rounding int) x86.M128d {
	return x86.M128d(maskScalefRoundSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), rounding))
}

func maskScalefRoundSd(src [2]float64, k uint8, a [2]float64, b [2]float64, rounding int) [2]float64


// MaskzScalefRoundSd: Scale the packed double-precision (64-bit)
// floating-point elements in 'a' using values from 'b', store the result in
// the lower element of 'dst' using zeromask 'k' (the element is zeroed out
// when mask bit 0 is not set), and copy the upper element from 'b' to the
// upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		IF k[0]
//			dst[63:0] := SCALE(a[63:0], b[63:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFSD'. Intrinsic: '_mm_maskz_scalef_round_sd'.
// Requires AVX512F.
func MaskzScalefRoundSd(k x86.Mmask8, a x86.M128d, b x86.M128d, rounding int) x86.M128d {
	return x86.M128d(maskzScalefRoundSd(uint8(k), [2]float64(a), [2]float64(b), rounding))
}

func maskzScalefRoundSd(k uint8, a [2]float64, b [2]float64, rounding int) [2]float64


// ScalefRoundSd: Scale the packed double-precision (64-bit) floating-point
// elements in 'a' using values from 'b', store the result in the lower element
// of 'dst', and copy the upper element from 'b' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		dst[63:0] := SCALE(a[63:0], b[63:0])
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFSD'. Intrinsic: '_mm_scalef_round_sd'.
// Requires AVX512F.
func ScalefRoundSd(a x86.M128d, b x86.M128d, rounding int) x86.M128d {
	return x86.M128d(scalefRoundSd([2]float64(a), [2]float64(b), rounding))
}

func scalefRoundSd(a [2]float64, b [2]float64, rounding int) [2]float64


// MaskScalefRoundSs: Scale the packed single-precision (32-bit) floating-point
// elements in 'a' using values from 'b', store the result in the lower element
// of 'dst' using writemask 'k' (the element is copied from 'src' when mask bit
// 0 is not set), and copy the upper 3 packed elements from 'b' to the upper
// elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[63:0]
//		}
//		
//		IF k[0]
//			dst[31:0] := SCALE(a[31:0], b[31:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFSS'. Intrinsic: '_mm_mask_scalef_round_ss'.
// Requires AVX512F.
func MaskScalefRoundSs(src x86.M128, k x86.Mmask8, a x86.M128, b x86.M128, rounding int) x86.M128 {
	return x86.M128(maskScalefRoundSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), rounding))
}

func maskScalefRoundSs(src [4]float32, k uint8, a [4]float32, b [4]float32, rounding int) [4]float32


// MaskzScalefRoundSs: Scale the packed single-precision (32-bit)
// floating-point elements in 'a' using values from 'b', store the result in
// the lower element of 'dst' using zeromask 'k' (the element is zeroed out
// when mask bit 0 is not set), and copy the upper 3 packed elements from 'b'
// to the upper elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[63:0]
//		}
//		
//		IF k[0]
//			dst[31:0] := SCALE(a[31:0], b[31:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFSS'. Intrinsic: '_mm_maskz_scalef_round_ss'.
// Requires AVX512F.
func MaskzScalefRoundSs(k x86.Mmask8, a x86.M128, b x86.M128, rounding int) x86.M128 {
	return x86.M128(maskzScalefRoundSs(uint8(k), [4]float32(a), [4]float32(b), rounding))
}

func maskzScalefRoundSs(k uint8, a [4]float32, b [4]float32, rounding int) [4]float32


// ScalefRoundSs: Scale the packed single-precision (32-bit) floating-point
// elements in 'a' using values from 'b', store the result in the lower element
// of 'dst', and copy the upper 3 packed elements from 'b' to the upper
// elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[63:0]
//		}
//		
//		dst[31:0] := SCALE(a[31:0], b[31:0])
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFSS'. Intrinsic: '_mm_scalef_round_ss'.
// Requires AVX512F.
func ScalefRoundSs(a x86.M128, b x86.M128, rounding int) x86.M128 {
	return x86.M128(scalefRoundSs([4]float32(a), [4]float32(b), rounding))
}

func scalefRoundSs(a [4]float32, b [4]float32, rounding int) [4]float32


// MaskScalefSd: Scale the packed double-precision (64-bit) floating-point
// elements in 'a' using values from 'b', store the result in the lower element
// of 'dst' using writemask 'k' (the element is copied from 'src' when mask bit
// 0 is not set), and copy the upper element from 'b' to the upper element of
// 'dst'. 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		IF k[0]
//			dst[63:0] := SCALE(a[63:0], b[63:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFSD'. Intrinsic: '_mm_mask_scalef_sd'.
// Requires AVX512F.
func MaskScalefSd(src x86.M128d, k x86.Mmask8, a x86.M128d, b x86.M128d) x86.M128d {
	return x86.M128d(maskScalefSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskScalefSd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzScalefSd: Scale the packed double-precision (64-bit) floating-point
// elements in 'a' using values from 'b', store the result in the lower element
// of 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is
// not set), and copy the upper element from 'b' to the upper element of 'dst'. 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		IF k[0]
//			dst[63:0] := SCALE(a[63:0], b[63:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFSD'. Intrinsic: '_mm_maskz_scalef_sd'.
// Requires AVX512F.
func MaskzScalefSd(k x86.Mmask8, a x86.M128d, b x86.M128d) x86.M128d {
	return x86.M128d(maskzScalefSd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzScalefSd(k uint8, a [2]float64, b [2]float64) [2]float64


// ScalefSd: Scale the packed double-precision (64-bit) floating-point elements
// in 'a' using values from 'b', store the result in the lower element of
// 'dst', and copy the upper element from 'b' to the upper element of 'dst'. 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		dst[63:0] := SCALE(a[63:0], b[63:0])
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFSD'. Intrinsic: '_mm_scalef_sd'.
// Requires AVX512F.
func ScalefSd(a x86.M128d, b x86.M128d) x86.M128d {
	return x86.M128d(scalefSd([2]float64(a), [2]float64(b)))
}

func scalefSd(a [2]float64, b [2]float64) [2]float64


// MaskScalefSs: Scale the packed single-precision (32-bit) floating-point
// elements in 'a' using values from 'b', store the result in the lower element
// of 'dst' using writemask 'k' (the element is copied from 'src' when mask bit
// 0 is not set), and copy the upper 3 packed elements from 'b' to the upper
// elements of 'dst'. 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[63:0]
//		}
//		
//		IF k[0]
//			dst[31:0] := SCALE(a[31:0], b[31:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFSS'. Intrinsic: '_mm_mask_scalef_ss'.
// Requires AVX512F.
func MaskScalefSs(src x86.M128, k x86.Mmask8, a x86.M128, b x86.M128) x86.M128 {
	return x86.M128(maskScalefSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskScalefSs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzScalefSs: Scale the packed single-precision (32-bit) floating-point
// elements in 'a' using values from 'b', store the result in the lower element
// of 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is
// not set), and copy the upper 3 packed elements from 'b' to the upper
// elements of 'dst'. 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[63:0]
//		}
//		
//		IF k[0]
//			dst[31:0] := SCALE(a[31:0], b[31:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFSS'. Intrinsic: '_mm_maskz_scalef_ss'.
// Requires AVX512F.
func MaskzScalefSs(k x86.Mmask8, a x86.M128, b x86.M128) x86.M128 {
	return x86.M128(maskzScalefSs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzScalefSs(k uint8, a [4]float32, b [4]float32) [4]float32


// ScalefSs: Scale the packed single-precision (32-bit) floating-point elements
// in 'a' using values from 'b', store the result in the lower element of
// 'dst', and copy the upper 3 packed elements from 'b' to the upper elements
// of 'dst'. 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[63:0]
//		}
//		
//		dst[31:0] := SCALE(a[31:0], b[31:0])
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFSS'. Intrinsic: '_mm_scalef_ss'.
// Requires AVX512F.
func ScalefSs(a x86.M128, b x86.M128) x86.M128 {
	return x86.M128(scalefSs([4]float32(a), [4]float32(b)))
}

func scalefSs(a [4]float32, b [4]float32) [4]float32


// M512SetEpi32: Set packed 32-bit integers in 'dst' with the supplied values. 
//
//		dst[31:0] := e0
//		dst[63:32] := e1
//		dst[95:64] := e2
//		dst[127:96] := e3
//		dst[159:128] := e4
//		dst[191:160] := e5
//		dst[223:192] := e6
//		dst[255:224] := e7
//		dst[287:256] := e8
//		dst[319:288] := e9
//		dst[351:320] := e10
//		dst[383:352] := e11
//		dst[415:384] := e12
//		dst[447:416] := e13
//		dst[479:448] := e14
//		dst[511:480] := e15
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_set_epi32'.
// Requires AVX512F.
func M512SetEpi32(e15 int, e14 int, e13 int, e12 int, e11 int, e10 int, e9 int, e8 int, e7 int, e6 int, e5 int, e4 int, e3 int, e2 int, e1 int, e0 int) x86.M512i {
	return x86.M512i(m512SetEpi32(e15, e14, e13, e12, e11, e10, e9, e8, e7, e6, e5, e4, e3, e2, e1, e0))
}

func m512SetEpi32(e15 int, e14 int, e13 int, e12 int, e11 int, e10 int, e9 int, e8 int, e7 int, e6 int, e5 int, e4 int, e3 int, e2 int, e1 int, e0 int) [64]byte


// M512SetEpi64: Set packed 64-bit integers in 'dst' with the supplied values. 
//
//		dst[63:0] := e0
//		dst[127:64] := e1
//		dst[191:128] := e2
//		dst[255:192] := e3
//		dst[319:256] := e4
//		dst[383:320] := e5
//		dst[447:384] := e6
//		dst[511:448] := e7
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_set_epi64'.
// Requires AVX512F.
func M512SetEpi64(e7 int64, e6 int64, e5 int64, e4 int64, e3 int64, e2 int64, e1 int64, e0 int64) x86.M512i {
	return x86.M512i(m512SetEpi64(e7, e6, e5, e4, e3, e2, e1, e0))
}

func m512SetEpi64(e7 int64, e6 int64, e5 int64, e4 int64, e3 int64, e2 int64, e1 int64, e0 int64) [64]byte


// M512SetPd: Set packed double-precision (64-bit) floating-point elements in
// 'dst' with the supplied values. 
//
//		dst[63:0] := e0
//		dst[127:64] := e1
//		dst[191:128] := e2
//		dst[255:192] := e3
//		dst[319:256] := e4
//		dst[383:320] := e5
//		dst[447:384] := e6
//		dst[511:448] := e7
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_set_pd'.
// Requires AVX512F.
func M512SetPd(e7 float64, e6 float64, e5 float64, e4 float64, e3 float64, e2 float64, e1 float64, e0 float64) x86.M512d {
	return x86.M512d(m512SetPd(e7, e6, e5, e4, e3, e2, e1, e0))
}

func m512SetPd(e7 float64, e6 float64, e5 float64, e4 float64, e3 float64, e2 float64, e1 float64, e0 float64) [8]float64


// M512SetPs: Set packed single-precision (32-bit) floating-point elements in
// 'dst' with the supplied values. 
//
//		dst[31:0] := e0
//		dst[63:32] := e1
//		dst[95:64] := e2
//		dst[127:96] := e3
//		dst[159:128] := e4
//		dst[191:160] := e5
//		dst[223:192] := e6
//		dst[255:224] := e7
//		dst[287:256] := e8
//		dst[319:288] := e9
//		dst[351:320] := e10
//		dst[383:352] := e11
//		dst[415:384] := e12
//		dst[447:416] := e13
//		dst[479:448] := e14
//		dst[511:480] := e15
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_set_ps'.
// Requires AVX512F.
func M512SetPs(e15 float32, e14 float32, e13 float32, e12 float32, e11 float32, e10 float32, e9 float32, e8 float32, e7 float32, e6 float32, e5 float32, e4 float32, e3 float32, e2 float32, e1 float32, e0 float32) x86.M512 {
	return x86.M512(m512SetPs(e15, e14, e13, e12, e11, e10, e9, e8, e7, e6, e5, e4, e3, e2, e1, e0))
}

func m512SetPs(e15 float32, e14 float32, e13 float32, e12 float32, e11 float32, e10 float32, e9 float32, e8 float32, e7 float32, e6 float32, e5 float32, e4 float32, e3 float32, e2 float32, e1 float32, e0 float32) [16]float32


// M512Set1Epi16: Broadcast the low packed 16-bit integer from 'a' to all all
// elements of 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			dst[i+15:i] := a[15:0]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_set1_epi16'.
// Requires AVX512F.
func M512Set1Epi16(a int16) x86.M512i {
	return x86.M512i(m512Set1Epi16(a))
}

func m512Set1Epi16(a int16) [64]byte


// MaskSet1Epi32: Broadcast 32-bit integer 'a' to all elements of 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm_mask_set1_epi32'.
// Requires AVX512F.
func MaskSet1Epi32(src x86.M128i, k x86.Mmask8, a int) x86.M128i {
	return x86.M128i(maskSet1Epi32([16]byte(src), uint8(k), a))
}

func maskSet1Epi32(src [16]byte, k uint8, a int) [16]byte


// MaskzSet1Epi32: Broadcast 32-bit integer 'a' to all elements of 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm_maskz_set1_epi32'.
// Requires AVX512F.
func MaskzSet1Epi32(k x86.Mmask8, a int) x86.M128i {
	return x86.M128i(maskzSet1Epi32(uint8(k), a))
}

func maskzSet1Epi32(k uint8, a int) [16]byte


// M256MaskSet1Epi32: Broadcast 32-bit integer 'a' to all elements of 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm256_mask_set1_epi32'.
// Requires AVX512F.
func M256MaskSet1Epi32(src x86.M256i, k x86.Mmask8, a int) x86.M256i {
	return x86.M256i(m256MaskSet1Epi32([32]byte(src), uint8(k), a))
}

func m256MaskSet1Epi32(src [32]byte, k uint8, a int) [32]byte


// M256MaskzSet1Epi32: Broadcast 32-bit integer 'a' to all elements of 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm256_maskz_set1_epi32'.
// Requires AVX512F.
func M256MaskzSet1Epi32(k x86.Mmask8, a int) x86.M256i {
	return x86.M256i(m256MaskzSet1Epi32(uint8(k), a))
}

func m256MaskzSet1Epi32(k uint8, a int) [32]byte


// M512MaskSet1Epi32: Broadcast 32-bit integer 'a' to all elements of 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm512_mask_set1_epi32'.
// Requires AVX512F.
func M512MaskSet1Epi32(src x86.M512i, k x86.Mmask16, a int) x86.M512i {
	return x86.M512i(m512MaskSet1Epi32([64]byte(src), uint16(k), a))
}

func m512MaskSet1Epi32(src [64]byte, k uint16, a int) [64]byte


// M512MaskzSet1Epi32: Broadcast 32-bit integer 'a' to all elements of 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm512_maskz_set1_epi32'.
// Requires AVX512F.
func M512MaskzSet1Epi32(k x86.Mmask16, a int) x86.M512i {
	return x86.M512i(m512MaskzSet1Epi32(uint16(k), a))
}

func m512MaskzSet1Epi32(k uint16, a int) [64]byte


// M512Set1Epi32: Broadcast 32-bit integer 'a' to all elements of 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := a[31:0]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm512_set1_epi32'.
// Requires AVX512F.
func M512Set1Epi32(a int) x86.M512i {
	return x86.M512i(m512Set1Epi32(a))
}

func m512Set1Epi32(a int) [64]byte


// MaskSet1Epi64: Broadcast 64-bit integer 'a' to all elements of 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm_mask_set1_epi64'.
// Requires AVX512F.
func MaskSet1Epi64(src x86.M128i, k x86.Mmask8, a int64) x86.M128i {
	return x86.M128i(maskSet1Epi64([16]byte(src), uint8(k), a))
}

func maskSet1Epi64(src [16]byte, k uint8, a int64) [16]byte


// MaskzSet1Epi64: Broadcast 64-bit integer 'a' to all elements of 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm_maskz_set1_epi64'.
// Requires AVX512F.
func MaskzSet1Epi64(k x86.Mmask8, a int64) x86.M128i {
	return x86.M128i(maskzSet1Epi64(uint8(k), a))
}

func maskzSet1Epi64(k uint8, a int64) [16]byte


// M256MaskSet1Epi64: Broadcast 64-bit integer 'a' to all elements of 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm256_mask_set1_epi64'.
// Requires AVX512F.
func M256MaskSet1Epi64(src x86.M256i, k x86.Mmask8, a int64) x86.M256i {
	return x86.M256i(m256MaskSet1Epi64([32]byte(src), uint8(k), a))
}

func m256MaskSet1Epi64(src [32]byte, k uint8, a int64) [32]byte


// M256MaskzSet1Epi64: Broadcast 64-bit integer 'a' to all elements of 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm256_maskz_set1_epi64'.
// Requires AVX512F.
func M256MaskzSet1Epi64(k x86.Mmask8, a int64) x86.M256i {
	return x86.M256i(m256MaskzSet1Epi64(uint8(k), a))
}

func m256MaskzSet1Epi64(k uint8, a int64) [32]byte


// M512MaskSet1Epi64: Broadcast 64-bit integer 'a' to all elements of 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm512_mask_set1_epi64'.
// Requires AVX512F.
func M512MaskSet1Epi64(src x86.M512i, k x86.Mmask8, a int64) x86.M512i {
	return x86.M512i(m512MaskSet1Epi64([64]byte(src), uint8(k), a))
}

func m512MaskSet1Epi64(src [64]byte, k uint8, a int64) [64]byte


// M512MaskzSet1Epi64: Broadcast 64-bit integer 'a' to all elements of 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm512_maskz_set1_epi64'.
// Requires AVX512F.
func M512MaskzSet1Epi64(k x86.Mmask8, a int64) x86.M512i {
	return x86.M512i(m512MaskzSet1Epi64(uint8(k), a))
}

func m512MaskzSet1Epi64(k uint8, a int64) [64]byte


// M512Set1Epi64: Broadcast 64-bit integer 'a' to all elements of 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := a[63:0]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm512_set1_epi64'.
// Requires AVX512F.
func M512Set1Epi64(a int64) x86.M512i {
	return x86.M512i(m512Set1Epi64(a))
}

func m512Set1Epi64(a int64) [64]byte


// M512Set1Epi8: Broadcast 8-bit integer 'a' to all elements of 'dst'. 
//
//		FOR j := 0 to 63
//			i := j*8
//			dst[i+7:i] := a[7:0]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_set1_epi8'.
// Requires AVX512F.
func M512Set1Epi8(a byte) x86.M512i {
	return x86.M512i(m512Set1Epi8(a))
}

func m512Set1Epi8(a byte) [64]byte


// M512Set1Pd: Broadcast double-precision (64-bit) floating-point value 'a' to
// all elements of 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := a[63:0]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_set1_pd'.
// Requires AVX512F.
func M512Set1Pd(a float64) x86.M512d {
	return x86.M512d(m512Set1Pd(a))
}

func m512Set1Pd(a float64) [8]float64


// M512Set1Ps: Broadcast single-precision (32-bit) floating-point value 'a' to
// all elements of 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := a[31:0]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_set1_ps'.
// Requires AVX512F.
func M512Set1Ps(a float32) x86.M512 {
	return x86.M512(m512Set1Ps(a))
}

func m512Set1Ps(a float32) [16]float32


// M512Set4Epi32: Set packed 32-bit integers in 'dst' with the repeated 4
// element sequence. 
//
//		dst[31:0] := d
//		dst[63:32] := c
//		dst[95:64] := b
//		dst[127:96] := a
//		dst[159:128] := d
//		dst[191:160] := c
//		dst[223:192] := b
//		dst[255:224] := a
//		dst[287:256] := d
//		dst[319:288] := c
//		dst[351:320] := b
//		dst[383:352] := a
//		dst[415:384] := d
//		dst[447:416] := c
//		dst[479:448] := b
//		dst[511:480] := a
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_set4_epi32'.
// Requires AVX512F.
func M512Set4Epi32(d int, c int, b int, a int) x86.M512i {
	return x86.M512i(m512Set4Epi32(d, c, b, a))
}

func m512Set4Epi32(d int, c int, b int, a int) [64]byte


// M512Set4Epi64: Set packed 64-bit integers in 'dst' with the repeated 4
// element sequence. 
//
//		dst[63:0] := d
//		dst[127:64] := c
//		dst[191:128] := b
//		dst[255:192] := a
//		dst[319:256] := d
//		dst[383:320] := c
//		dst[447:384] := b
//		dst[511:448] := a
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_set4_epi64'.
// Requires AVX512F.
func M512Set4Epi64(d int64, c int64, b int64, a int64) x86.M512i {
	return x86.M512i(m512Set4Epi64(d, c, b, a))
}

func m512Set4Epi64(d int64, c int64, b int64, a int64) [64]byte


// M512Set4Pd: Set packed double-precision (64-bit) floating-point elements in
// 'dst' with the repeated 4 element sequence. 
//
//		dst[63:0] := d
//		dst[127:64] := c
//		dst[191:128] := b
//		dst[255:192] := a
//		dst[319:256] := d
//		dst[383:320] := c
//		dst[447:384] := b
//		dst[511:448] := a
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_set4_pd'.
// Requires AVX512F.
func M512Set4Pd(d float64, c float64, b float64, a float64) x86.M512d {
	return x86.M512d(m512Set4Pd(d, c, b, a))
}

func m512Set4Pd(d float64, c float64, b float64, a float64) [8]float64


// M512Set4Ps: Set packed single-precision (32-bit) floating-point elements in
// 'dst' with the repeated 4 element sequence. 
//
//		dst[31:0] := d
//		dst[63:32] := c
//		dst[95:64] := b
//		dst[127:96] := a
//		dst[159:128] := d
//		dst[191:160] := c
//		dst[223:192] := b
//		dst[255:224] := a
//		dst[287:256] := d
//		dst[319:288] := c
//		dst[351:320] := b
//		dst[383:352] := a
//		dst[415:384] := d
//		dst[447:416] := c
//		dst[479:448] := b
//		dst[511:480] := a
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_set4_ps'.
// Requires AVX512F.
func M512Set4Ps(d float32, c float32, b float32, a float32) x86.M512 {
	return x86.M512(m512Set4Ps(d, c, b, a))
}

func m512Set4Ps(d float32, c float32, b float32, a float32) [16]float32


// M512SetrEpi32: Set packed 32-bit integers in 'dst' with the supplied values
// in reverse order. 
//
//		dst[31:0] := e15
//		dst[63:32] := e14
//		dst[95:64] := e13
//		dst[127:96] := e12
//		dst[159:128] := e11
//		dst[191:160] := e10
//		dst[223:192] := e9
//		dst[255:224] := e8
//		dst[287:256] := e7
//		dst[319:288] := e6
//		dst[351:320] := e5
//		dst[383:352] := e4
//		dst[415:384] := e3
//		dst[447:416] := e2
//		dst[479:448] := e1
//		dst[511:480] := e0
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_setr_epi32'.
// Requires AVX512F.
func M512SetrEpi32(e15 int, e14 int, e13 int, e12 int, e11 int, e10 int, e9 int, e8 int, e7 int, e6 int, e5 int, e4 int, e3 int, e2 int, e1 int, e0 int) x86.M512i {
	return x86.M512i(m512SetrEpi32(e15, e14, e13, e12, e11, e10, e9, e8, e7, e6, e5, e4, e3, e2, e1, e0))
}

func m512SetrEpi32(e15 int, e14 int, e13 int, e12 int, e11 int, e10 int, e9 int, e8 int, e7 int, e6 int, e5 int, e4 int, e3 int, e2 int, e1 int, e0 int) [64]byte


// M512SetrEpi64: Set packed 64-bit integers in 'dst' with the supplied values
// in reverse order. 
//
//		dst[63:0] := e7
//		dst[127:64] := e6
//		dst[191:128] := e5
//		dst[255:192] := e4
//		dst[319:256] := e3
//		dst[383:320] := e2
//		dst[447:384] := e1
//		dst[511:448] := e0
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_setr_epi64'.
// Requires AVX512F.
func M512SetrEpi64(e7 int64, e6 int64, e5 int64, e4 int64, e3 int64, e2 int64, e1 int64, e0 int64) x86.M512i {
	return x86.M512i(m512SetrEpi64(e7, e6, e5, e4, e3, e2, e1, e0))
}

func m512SetrEpi64(e7 int64, e6 int64, e5 int64, e4 int64, e3 int64, e2 int64, e1 int64, e0 int64) [64]byte


// M512SetrPd: Set packed double-precision (64-bit) floating-point elements in
// 'dst' with the supplied values in reverse order. 
//
//		dst[63:0] := e7
//		dst[127:64] := e6
//		dst[191:128] := e5
//		dst[255:192] := e4
//		dst[319:256] := e3
//		dst[383:320] := e2
//		dst[447:384] := e1
//		dst[511:448] := e0
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_setr_pd'.
// Requires AVX512F.
func M512SetrPd(e7 float64, e6 float64, e5 float64, e4 float64, e3 float64, e2 float64, e1 float64, e0 float64) x86.M512d {
	return x86.M512d(m512SetrPd(e7, e6, e5, e4, e3, e2, e1, e0))
}

func m512SetrPd(e7 float64, e6 float64, e5 float64, e4 float64, e3 float64, e2 float64, e1 float64, e0 float64) [8]float64


// M512SetrPs: Set packed single-precision (32-bit) floating-point elements in
// 'dst' with the supplied values in reverse order. 
//
//		dst[31:0] := e15
//		dst[63:32] := e14
//		dst[95:64] := e13
//		dst[127:96] := e12
//		dst[159:128] := e11
//		dst[191:160] := e10
//		dst[223:192] := e9
//		dst[255:224] := e8
//		dst[287:256] := e7
//		dst[319:288] := e6
//		dst[351:320] := e5
//		dst[383:352] := e4
//		dst[415:384] := e3
//		dst[447:416] := e2
//		dst[479:448] := e1
//		dst[511:480] := e0
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_setr_ps'.
// Requires AVX512F.
func M512SetrPs(e15 float32, e14 float32, e13 float32, e12 float32, e11 float32, e10 float32, e9 float32, e8 float32, e7 float32, e6 float32, e5 float32, e4 float32, e3 float32, e2 float32, e1 float32, e0 float32) x86.M512 {
	return x86.M512(m512SetrPs(e15, e14, e13, e12, e11, e10, e9, e8, e7, e6, e5, e4, e3, e2, e1, e0))
}

func m512SetrPs(e15 float32, e14 float32, e13 float32, e12 float32, e11 float32, e10 float32, e9 float32, e8 float32, e7 float32, e6 float32, e5 float32, e4 float32, e3 float32, e2 float32, e1 float32, e0 float32) [16]float32


// M512Setr4Epi32: Set packed 32-bit integers in 'dst' with the repeated 4
// element sequence in reverse order. 
//
//		dst[31:0] := a
//		dst[63:32] := b
//		dst[95:64] := c
//		dst[127:96] := d
//		dst[159:128] := a
//		dst[191:160] := b
//		dst[223:192] := c
//		dst[255:224] := d
//		dst[287:256] := a
//		dst[319:288] := b
//		dst[351:320] := c
//		dst[383:352] := d
//		dst[415:384] := a
//		dst[447:416] := b
//		dst[479:448] := c
//		dst[511:480] := d
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_setr4_epi32'.
// Requires AVX512F.
func M512Setr4Epi32(d int, c int, b int, a int) x86.M512i {
	return x86.M512i(m512Setr4Epi32(d, c, b, a))
}

func m512Setr4Epi32(d int, c int, b int, a int) [64]byte


// M512Setr4Epi64: Set packed 64-bit integers in 'dst' with the repeated 4
// element sequence in reverse order. 
//
//		dst[63:0] := a
//		dst[127:64] := b
//		dst[191:128] := c
//		dst[255:192] := d
//		dst[319:256] := a
//		dst[383:320] := b
//		dst[447:384] := c
//		dst[511:448] := d
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_setr4_epi64'.
// Requires AVX512F.
func M512Setr4Epi64(d int64, c int64, b int64, a int64) x86.M512i {
	return x86.M512i(m512Setr4Epi64(d, c, b, a))
}

func m512Setr4Epi64(d int64, c int64, b int64, a int64) [64]byte


// M512Setr4Pd: Set packed double-precision (64-bit) floating-point elements in
// 'dst' with the repeated 4 element sequence in reverse order. 
//
//		dst[63:0] := a
//		dst[127:64] := b
//		dst[191:128] := c
//		dst[255:192] := d
//		dst[319:256] := a
//		dst[383:320] := b
//		dst[447:384] := c
//		dst[511:448] := d
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_setr4_pd'.
// Requires AVX512F.
func M512Setr4Pd(d float64, c float64, b float64, a float64) x86.M512d {
	return x86.M512d(m512Setr4Pd(d, c, b, a))
}

func m512Setr4Pd(d float64, c float64, b float64, a float64) [8]float64


// M512Setr4Ps: Set packed single-precision (32-bit) floating-point elements in
// 'dst' with the repeated 4 element sequence in reverse order. 
//
//		dst[31:0] := a
//		dst[63:32] := b
//		dst[95:64] := c
//		dst[127:96] := d
//		dst[159:128] := a
//		dst[191:160] := b
//		dst[223:192] := c
//		dst[255:224] := d
//		dst[287:256] := a
//		dst[319:288] := b
//		dst[351:320] := c
//		dst[383:352] := d
//		dst[415:384] := a
//		dst[447:416] := b
//		dst[479:448] := c
//		dst[511:480] := d
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_setr4_ps'.
// Requires AVX512F.
func M512Setr4Ps(d float32, c float32, b float32, a float32) x86.M512 {
	return x86.M512(m512Setr4Ps(d, c, b, a))
}

func m512Setr4Ps(d float32, c float32, b float32, a float32) [16]float32


// M512Setzero: Return vector of type __m512 with all elements set to zero. 
//
//		dst[MAX:0] := 0
//
// Instruction: 'VPXORQ'. Intrinsic: '_mm512_setzero'.
// Requires AVX512F.
func M512Setzero() x86.M512 {
	return x86.M512(m512Setzero())
}

func m512Setzero() [16]float32


// M512SetzeroEpi32: Return vector of type __m512i with all elements set to
// zero. 
//
//		dst[MAX:0] := 0
//
// Instruction: 'VPXORQ'. Intrinsic: '_mm512_setzero_epi32'.
// Requires AVX512F.
func M512SetzeroEpi32() x86.M512i {
	return x86.M512i(m512SetzeroEpi32())
}

func m512SetzeroEpi32() [64]byte


// M512SetzeroPd: Return vector of type __m512d with all elements set to zero. 
//
//		dst[MAX:0] := 0
//
// Instruction: 'VPXORQ'. Intrinsic: '_mm512_setzero_pd'.
// Requires AVX512F.
func M512SetzeroPd() x86.M512d {
	return x86.M512d(m512SetzeroPd())
}

func m512SetzeroPd() [8]float64


// M512SetzeroPs: Return vector of type __m512 with all elements set to zero. 
//
//		dst[MAX:0] := 0
//
// Instruction: 'VPXORQ'. Intrinsic: '_mm512_setzero_ps'.
// Requires AVX512F.
func M512SetzeroPs() x86.M512 {
	return x86.M512(m512SetzeroPs())
}

func m512SetzeroPs() [16]float32


// M512SetzeroSi512: Return vector of type __m512i with all elements set to
// zero. 
//
//		dst[MAX:0] := 0
//
// Instruction: 'VPXORQ'. Intrinsic: '_mm512_setzero_si512'.
// Requires AVX512F.
func M512SetzeroSi512() x86.M512i {
	return x86.M512i(m512SetzeroSi512())
}

func m512SetzeroSi512() [64]byte


// MaskShuffleEpi32: Shuffle 32-bit integers in 'a' using the control in
// 'imm8', and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSHUFD'. Intrinsic: '_mm_mask_shuffle_epi32'.
// Requires AVX512F.
func MaskShuffleEpi32(src x86.M128i, k x86.Mmask8, a x86.M128i, imm8 MMPERMENUM) x86.M128i {
	return x86.M128i(maskShuffleEpi32([16]byte(src), uint8(k), [16]byte(a), imm8))
}

func maskShuffleEpi32(src [16]byte, k uint8, a [16]byte, imm8 MMPERMENUM) [16]byte


// MaskzShuffleEpi32: Shuffle 32-bit integers in 'a' using the control in
// 'imm8', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSHUFD'. Intrinsic: '_mm_maskz_shuffle_epi32'.
// Requires AVX512F.
func MaskzShuffleEpi32(k x86.Mmask8, a x86.M128i, imm8 MMPERMENUM) x86.M128i {
	return x86.M128i(maskzShuffleEpi32(uint8(k), [16]byte(a), imm8))
}

func maskzShuffleEpi32(k uint8, a [16]byte, imm8 MMPERMENUM) [16]byte


// M256MaskShuffleEpi32: Shuffle 32-bit integers in 'a' within 128-bit lanes
// using the control in 'imm8', and store the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		tmp_dst[223:192] := SELECT4(a[255:128], imm8[5:4])
//		tmp_dst[255:224] := SELECT4(a[255:128], imm8[7:6])
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSHUFD'. Intrinsic: '_mm256_mask_shuffle_epi32'.
// Requires AVX512F.
func M256MaskShuffleEpi32(src x86.M256i, k x86.Mmask8, a x86.M256i, imm8 MMPERMENUM) x86.M256i {
	return x86.M256i(m256MaskShuffleEpi32([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func m256MaskShuffleEpi32(src [32]byte, k uint8, a [32]byte, imm8 MMPERMENUM) [32]byte


// M256MaskzShuffleEpi32: Shuffle 32-bit integers in 'a' within 128-bit lanes
// using the control in 'imm8', and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		tmp_dst[223:192] := SELECT4(a[255:128], imm8[5:4])
//		tmp_dst[255:224] := SELECT4(a[255:128], imm8[7:6])
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSHUFD'. Intrinsic: '_mm256_maskz_shuffle_epi32'.
// Requires AVX512F.
func M256MaskzShuffleEpi32(k x86.Mmask8, a x86.M256i, imm8 MMPERMENUM) x86.M256i {
	return x86.M256i(m256MaskzShuffleEpi32(uint8(k), [32]byte(a), imm8))
}

func m256MaskzShuffleEpi32(k uint8, a [32]byte, imm8 MMPERMENUM) [32]byte


// M512MaskzShuffleEpi32: Shuffle 32-bit integers in 'a' within 128-bit lanes
// using the control in 'imm8', and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		tmp_dst[223:192] := SELECT4(a[255:128], imm8[5:4])
//		tmp_dst[255:224] := SELECT4(a[255:128], imm8[7:6])
//		tmp_dst[287:256] := SELECT4(a[383:256], imm8[1:0])
//		tmp_dst[319:288] := SELECT4(a[383:256], imm8[3:2])
//		tmp_dst[351:320] := SELECT4(a[383:256], imm8[5:4])
//		tmp_dst[383:352] := SELECT4(a[383:256], imm8[7:6])
//		tmp_dst[415:384] := SELECT4(a[511:384], imm8[1:0])
//		tmp_dst[447:416] := SELECT4(a[511:384], imm8[3:2])
//		tmp_dst[479:448] := SELECT4(a[511:384], imm8[5:4])
//		tmp_dst[511:480] := SELECT4(a[511:384], imm8[7:6])
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSHUFD'. Intrinsic: '_mm512_maskz_shuffle_epi32'.
// Requires AVX512F.
func M512MaskzShuffleEpi32(k x86.Mmask16, a x86.M512i, imm8 MMPERMENUM) x86.M512i {
	return x86.M512i(m512MaskzShuffleEpi32(uint16(k), [64]byte(a), imm8))
}

func m512MaskzShuffleEpi32(k uint16, a [64]byte, imm8 MMPERMENUM) [64]byte


// M256MaskShuffleF32x4: Shuffle 128-bits (composed of 4 single-precision
// (32-bit) floating-point elements) selected by 'imm8' from 'a' and 'b', and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		SELECT2(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT2(a[255:0], imm8[0])
//		tmp_dst[255:128] := SELECT2(b[255:0], imm8[1])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFF32X4'. Intrinsic: '_mm256_mask_shuffle_f32x4'.
// Requires AVX512F.
func M256MaskShuffleF32x4(src x86.M256, k x86.Mmask8, a x86.M256, b x86.M256, imm8 int) x86.M256 {
	return x86.M256(m256MaskShuffleF32x4([8]float32(src), uint8(k), [8]float32(a), [8]float32(b), imm8))
}

func m256MaskShuffleF32x4(src [8]float32, k uint8, a [8]float32, b [8]float32, imm8 int) [8]float32


// M256MaskzShuffleF32x4: Shuffle 128-bits (composed of 4 single-precision
// (32-bit) floating-point elements) selected by 'imm8' from 'a' and 'b', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		SELECT2(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT2(a[255:0], imm8[0])
//		tmp_dst[255:128] := SELECT2(b[255:0], imm8[1])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFF32X4'. Intrinsic: '_mm256_maskz_shuffle_f32x4'.
// Requires AVX512F.
func M256MaskzShuffleF32x4(k x86.Mmask8, a x86.M256, b x86.M256, imm8 int) x86.M256 {
	return x86.M256(m256MaskzShuffleF32x4(uint8(k), [8]float32(a), [8]float32(b), imm8))
}

func m256MaskzShuffleF32x4(k uint8, a [8]float32, b [8]float32, imm8 int) [8]float32


// M256ShuffleF32x4: Shuffle 128-bits (composed of 4 single-precision (32-bit)
// floating-point elements) selected by 'imm8' from 'a' and 'b', and store the
// results in 'dst'. 
//
//		SELECT2(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		dst[127:0] := SELECT2(a[255:0], imm8[0])
//		dst[255:128] := SELECT2(b[255:0], imm8[1])
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFF32X4'. Intrinsic: '_mm256_shuffle_f32x4'.
// Requires AVX512F.
func M256ShuffleF32x4(a x86.M256, b x86.M256, imm8 int) x86.M256 {
	return x86.M256(m256ShuffleF32x4([8]float32(a), [8]float32(b), imm8))
}

func m256ShuffleF32x4(a [8]float32, b [8]float32, imm8 int) [8]float32


// M512MaskShuffleF32x4: Shuffle 128-bits (composed of 4 single-precision
// (32-bit) floating-point elements) selected by 'imm8' from 'a' and 'b', and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			2:	tmp[127:0] := src[383:256]
//			3:	tmp[127:0] := src[511:384]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT4(a[511:0], imm8[1:0])
//		tmp_dst[255:128] := SELECT4(a[511:0], imm8[3:2])
//		tmp_dst[383:256] := SELECT4(b[511:0], imm8[5:4])
//		tmp_dst[511:384] := SELECT4(b[511:0], imm8[7:6])
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFF32X4'. Intrinsic: '_mm512_mask_shuffle_f32x4'.
// Requires AVX512F.
func M512MaskShuffleF32x4(src x86.M512, k x86.Mmask16, a x86.M512, b x86.M512, imm8 int) x86.M512 {
	return x86.M512(m512MaskShuffleF32x4([16]float32(src), uint16(k), [16]float32(a), [16]float32(b), imm8))
}

func m512MaskShuffleF32x4(src [16]float32, k uint16, a [16]float32, b [16]float32, imm8 int) [16]float32


// M512MaskzShuffleF32x4: Shuffle 128-bits (composed of 4 single-precision
// (32-bit) floating-point elements) selected by 'imm8' from 'a' and 'b', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			2:	tmp[127:0] := src[383:256]
//			3:	tmp[127:0] := src[511:384]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT4(a[511:0], imm8[1:0])
//		tmp_dst[255:128] := SELECT4(a[511:0], imm8[3:2])
//		tmp_dst[383:256] := SELECT4(b[511:0], imm8[5:4])
//		tmp_dst[511:384] := SELECT4(b[511:0], imm8[7:6])
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFF32X4'. Intrinsic: '_mm512_maskz_shuffle_f32x4'.
// Requires AVX512F.
func M512MaskzShuffleF32x4(k x86.Mmask16, a x86.M512, b x86.M512, imm8 int) x86.M512 {
	return x86.M512(m512MaskzShuffleF32x4(uint16(k), [16]float32(a), [16]float32(b), imm8))
}

func m512MaskzShuffleF32x4(k uint16, a [16]float32, b [16]float32, imm8 int) [16]float32


// M512ShuffleF32x4: Shuffle 128-bits (composed of 4 single-precision (32-bit)
// floating-point elements) selected by 'imm8' from 'a' and 'b', and store the
// results in 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			2:	tmp[127:0] := src[383:256]
//			3:	tmp[127:0] := src[511:384]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		dst[127:0] := SELECT4(a[511:0], imm8[1:0])
//		dst[255:128] := SELECT4(a[511:0], imm8[3:2])
//		dst[383:256] := SELECT4(b[511:0], imm8[5:4])
//		dst[511:384] := SELECT4(b[511:0], imm8[7:6])
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFF32X4'. Intrinsic: '_mm512_shuffle_f32x4'.
// Requires AVX512F.
func M512ShuffleF32x4(a x86.M512, b x86.M512, imm8 int) x86.M512 {
	return x86.M512(m512ShuffleF32x4([16]float32(a), [16]float32(b), imm8))
}

func m512ShuffleF32x4(a [16]float32, b [16]float32, imm8 int) [16]float32


// M256MaskShuffleF64x2: Shuffle 128-bits (composed of 2 double-precision
// (64-bit) floating-point elements) selected by 'imm8' from 'a' and 'b', and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT2(a[255:0], imm8[0])
//		tmp_dst[255:128] := SELECT2(b[255:0], imm8[1])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFF64X2'. Intrinsic: '_mm256_mask_shuffle_f64x2'.
// Requires AVX512F.
func M256MaskShuffleF64x2(src x86.M256d, k x86.Mmask8, a x86.M256d, b x86.M256d, imm8 int) x86.M256d {
	return x86.M256d(m256MaskShuffleF64x2([4]float64(src), uint8(k), [4]float64(a), [4]float64(b), imm8))
}

func m256MaskShuffleF64x2(src [4]float64, k uint8, a [4]float64, b [4]float64, imm8 int) [4]float64


// M256MaskzShuffleF64x2: Shuffle 128-bits (composed of 2 double-precision
// (64-bit) floating-point elements) selected by 'imm8' from 'a' and 'b', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT2(a[255:0], imm8[0])
//		tmp_dst[255:128] := SELECT2(b[255:0], imm8[1])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFF64X2'. Intrinsic: '_mm256_maskz_shuffle_f64x2'.
// Requires AVX512F.
func M256MaskzShuffleF64x2(k x86.Mmask8, a x86.M256d, b x86.M256d, imm8 int) x86.M256d {
	return x86.M256d(m256MaskzShuffleF64x2(uint8(k), [4]float64(a), [4]float64(b), imm8))
}

func m256MaskzShuffleF64x2(k uint8, a [4]float64, b [4]float64, imm8 int) [4]float64


// M256ShuffleF64x2: Shuffle 128-bits (composed of 2 double-precision (64-bit)
// floating-point elements) selected by 'imm8' from 'a' and 'b', and store the
// results in 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		dst[127:0] := SELECT2(a[255:0], imm8[0])
//		dst[255:128] := SELECT2(b[255:0], imm8[1])
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFF64X2'. Intrinsic: '_mm256_shuffle_f64x2'.
// Requires AVX512F.
func M256ShuffleF64x2(a x86.M256d, b x86.M256d, imm8 int) x86.M256d {
	return x86.M256d(m256ShuffleF64x2([4]float64(a), [4]float64(b), imm8))
}

func m256ShuffleF64x2(a [4]float64, b [4]float64, imm8 int) [4]float64


// M512MaskShuffleF64x2: Shuffle 128-bits (composed of 2 double-precision
// (64-bit) floating-point elements) selected by 'imm8' from 'a' and 'b', and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			2:	tmp[127:0] := src[383:256]
//			3:	tmp[127:0] := src[511:384]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT4(a[511:0], imm8[1:0])
//		tmp_dst[255:128] := SELECT4(a[511:0], imm8[3:2])
//		tmp_dst[383:256] := SELECT4(b[511:0], imm8[5:4])
//		tmp_dst[511:384] := SELECT4(b[511:0], imm8[7:6])
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFF64X2'. Intrinsic: '_mm512_mask_shuffle_f64x2'.
// Requires AVX512F.
func M512MaskShuffleF64x2(src x86.M512d, k x86.Mmask8, a x86.M512d, b x86.M512d, imm8 int) x86.M512d {
	return x86.M512d(m512MaskShuffleF64x2([8]float64(src), uint8(k), [8]float64(a), [8]float64(b), imm8))
}

func m512MaskShuffleF64x2(src [8]float64, k uint8, a [8]float64, b [8]float64, imm8 int) [8]float64


// M512MaskzShuffleF64x2: Shuffle 128-bits (composed of 2 double-precision
// (64-bit) floating-point elements) selected by 'imm8' from 'a' and 'b', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			2:	tmp[127:0] := src[383:256]
//			3:	tmp[127:0] := src[511:384]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT4(a[511:0], imm8[1:0])
//		tmp_dst[255:128] := SELECT4(a[511:0], imm8[3:2])
//		tmp_dst[383:256] := SELECT4(b[511:0], imm8[5:4])
//		tmp_dst[511:384] := SELECT4(b[511:0], imm8[7:6])
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFF64X2'. Intrinsic: '_mm512_maskz_shuffle_f64x2'.
// Requires AVX512F.
func M512MaskzShuffleF64x2(k x86.Mmask8, a x86.M512d, b x86.M512d, imm8 int) x86.M512d {
	return x86.M512d(m512MaskzShuffleF64x2(uint8(k), [8]float64(a), [8]float64(b), imm8))
}

func m512MaskzShuffleF64x2(k uint8, a [8]float64, b [8]float64, imm8 int) [8]float64


// M512ShuffleF64x2: Shuffle 128-bits (composed of 2 double-precision (64-bit)
// floating-point elements) selected by 'imm8' from 'a' and 'b', and store the
// results in 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			2:	tmp[127:0] := src[383:256]
//			3:	tmp[127:0] := src[511:384]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		dst[127:0] := SELECT4(a[511:0], imm8[1:0])
//		dst[255:128] := SELECT4(a[511:0], imm8[3:2])
//		dst[383:256] := SELECT4(b[511:0], imm8[5:4])
//		dst[511:384] := SELECT4(b[511:0], imm8[7:6])
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFF64X2'. Intrinsic: '_mm512_shuffle_f64x2'.
// Requires AVX512F.
func M512ShuffleF64x2(a x86.M512d, b x86.M512d, imm8 int) x86.M512d {
	return x86.M512d(m512ShuffleF64x2([8]float64(a), [8]float64(b), imm8))
}

func m512ShuffleF64x2(a [8]float64, b [8]float64, imm8 int) [8]float64


// M256MaskShuffleI32x4: Shuffle 128-bits (composed of 4 32-bit integers)
// selected by 'imm8' from 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		SELECT2(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT2(a[255:0], imm8[1:0])
//		tmp_dst[255:128] := SELECT2(b[255:0], imm8[3:2])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFI32X4'. Intrinsic: '_mm256_mask_shuffle_i32x4'.
// Requires AVX512F.
func M256MaskShuffleI32x4(src x86.M256i, k x86.Mmask8, a x86.M256i, b x86.M256i, imm8 int) x86.M256i {
	return x86.M256i(m256MaskShuffleI32x4([32]byte(src), uint8(k), [32]byte(a), [32]byte(b), imm8))
}

func m256MaskShuffleI32x4(src [32]byte, k uint8, a [32]byte, b [32]byte, imm8 int) [32]byte


// M256MaskzShuffleI32x4: Shuffle 128-bits (composed of 4 32-bit integers)
// selected by 'imm8' from 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		SELECT2(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT2(a[255:0], imm8[1:0])
//		tmp_dst[255:128] := SELECT2(b[255:0], imm8[3:2])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFI32X4'. Intrinsic: '_mm256_maskz_shuffle_i32x4'.
// Requires AVX512F.
func M256MaskzShuffleI32x4(k x86.Mmask8, a x86.M256i, b x86.M256i, imm8 int) x86.M256i {
	return x86.M256i(m256MaskzShuffleI32x4(uint8(k), [32]byte(a), [32]byte(b), imm8))
}

func m256MaskzShuffleI32x4(k uint8, a [32]byte, b [32]byte, imm8 int) [32]byte


// M256ShuffleI32x4: Shuffle 128-bits (composed of 4 32-bit integers) selected
// by 'imm8' from 'a' and 'b', and store the results in 'dst'. 
//
//		SELECT2(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		dst[127:0] := SELECT2(a[255:0], imm8[1:0])
//		dst[255:128] := SELECT2(b[255:0], imm8[3:2])
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFI32X4'. Intrinsic: '_mm256_shuffle_i32x4'.
// Requires AVX512F.
func M256ShuffleI32x4(a x86.M256i, b x86.M256i, imm8 int) x86.M256i {
	return x86.M256i(m256ShuffleI32x4([32]byte(a), [32]byte(b), imm8))
}

func m256ShuffleI32x4(a [32]byte, b [32]byte, imm8 int) [32]byte


// M512MaskShuffleI32x4: Shuffle 128-bits (composed of 4 32-bit integers)
// selected by 'imm8' from 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			2:	tmp[127:0] := src[383:256]
//			3:	tmp[127:0] := src[511:384]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT4(a[511:0], imm8[1:0])
//		tmp_dst[255:128] := SELECT4(a[511:0], imm8[3:2])
//		tmp_dst[383:256] := SELECT4(b[511:0], imm8[5:4])
//		tmp_dst[511:384] := SELECT4(b[511:0], imm8[7:6])
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFI32X4'. Intrinsic: '_mm512_mask_shuffle_i32x4'.
// Requires AVX512F.
func M512MaskShuffleI32x4(src x86.M512i, k x86.Mmask16, a x86.M512i, b x86.M512i, imm8 int) x86.M512i {
	return x86.M512i(m512MaskShuffleI32x4([64]byte(src), uint16(k), [64]byte(a), [64]byte(b), imm8))
}

func m512MaskShuffleI32x4(src [64]byte, k uint16, a [64]byte, b [64]byte, imm8 int) [64]byte


// M512MaskzShuffleI32x4: Shuffle 128-bits (composed of 4 32-bit integers)
// selected by 'imm8' from 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			2:	tmp[127:0] := src[383:256]
//			3:	tmp[127:0] := src[511:384]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT4(a[511:0], imm8[1:0])
//		tmp_dst[255:128] := SELECT4(a[511:0], imm8[3:2])
//		tmp_dst[383:256] := SELECT4(b[511:0], imm8[5:4])
//		tmp_dst[511:384] := SELECT4(b[511:0], imm8[7:6])
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFI32X4'. Intrinsic: '_mm512_maskz_shuffle_i32x4'.
// Requires AVX512F.
func M512MaskzShuffleI32x4(k x86.Mmask16, a x86.M512i, b x86.M512i, imm8 int) x86.M512i {
	return x86.M512i(m512MaskzShuffleI32x4(uint16(k), [64]byte(a), [64]byte(b), imm8))
}

func m512MaskzShuffleI32x4(k uint16, a [64]byte, b [64]byte, imm8 int) [64]byte


// M512ShuffleI32x4: Shuffle 128-bits (composed of 4 32-bit integers) selected
// by 'imm8' from 'a' and 'b', and store the results in 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			2:	tmp[127:0] := src[383:256]
//			3:	tmp[127:0] := src[511:384]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		dst[127:0] := SELECT4(a[511:0], imm8[1:0])
//		dst[255:128] := SELECT4(a[511:0], imm8[3:2])
//		dst[383:256] := SELECT4(b[511:0], imm8[5:4])
//		dst[511:384] := SELECT4(b[511:0], imm8[7:6])
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFI32X4'. Intrinsic: '_mm512_shuffle_i32x4'.
// Requires AVX512F.
func M512ShuffleI32x4(a x86.M512i, b x86.M512i, imm8 int) x86.M512i {
	return x86.M512i(m512ShuffleI32x4([64]byte(a), [64]byte(b), imm8))
}

func m512ShuffleI32x4(a [64]byte, b [64]byte, imm8 int) [64]byte


// M256MaskShuffleI64x2: Shuffle 128-bits (composed of 2 64-bit integers)
// selected by 'imm8' from 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		SELECT2(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT2(a[255:0], imm8[1:0])
//		tmp_dst[255:128] := SELECT2(b[255:0], imm8[3:2])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFI64X2'. Intrinsic: '_mm256_mask_shuffle_i64x2'.
// Requires AVX512F.
func M256MaskShuffleI64x2(src x86.M256i, k x86.Mmask8, a x86.M256i, b x86.M256i, imm8 int) x86.M256i {
	return x86.M256i(m256MaskShuffleI64x2([32]byte(src), uint8(k), [32]byte(a), [32]byte(b), imm8))
}

func m256MaskShuffleI64x2(src [32]byte, k uint8, a [32]byte, b [32]byte, imm8 int) [32]byte


// M256MaskzShuffleI64x2: Shuffle 128-bits (composed of 2 64-bit integers)
// selected by 'imm8' from 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		SELECT2(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT2(a[255:0], imm8[1:0])
//		tmp_dst[255:128] := SELECT2(b[255:0], imm8[3:2])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFI64X2'. Intrinsic: '_mm256_maskz_shuffle_i64x2'.
// Requires AVX512F.
func M256MaskzShuffleI64x2(k x86.Mmask8, a x86.M256i, b x86.M256i, imm8 int) x86.M256i {
	return x86.M256i(m256MaskzShuffleI64x2(uint8(k), [32]byte(a), [32]byte(b), imm8))
}

func m256MaskzShuffleI64x2(k uint8, a [32]byte, b [32]byte, imm8 int) [32]byte


// M256ShuffleI64x2: Shuffle 128-bits (composed of 2 64-bit integers) selected
// by 'imm8' from 'a' and 'b', and store the results in 'dst'. 
//
//		SELECT2(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		dst[127:0] := SELECT2(a[255:0], imm8[1:0])
//		dst[255:128] := SELECT2(b[255:0], imm8[3:2])
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFI64X2'. Intrinsic: '_mm256_shuffle_i64x2'.
// Requires AVX512F.
func M256ShuffleI64x2(a x86.M256i, b x86.M256i, imm8 int) x86.M256i {
	return x86.M256i(m256ShuffleI64x2([32]byte(a), [32]byte(b), imm8))
}

func m256ShuffleI64x2(a [32]byte, b [32]byte, imm8 int) [32]byte


// M512MaskShuffleI64x2: Shuffle 128-bits (composed of 2 64-bit integers)
// selected by 'imm8' from 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			2:	tmp[127:0] := src[383:256]
//			3:	tmp[127:0] := src[511:384]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT4(a[511:0], imm8[1:0])
//		tmp_dst[255:128] := SELECT4(a[511:0], imm8[3:2])
//		tmp_dst[383:256] := SELECT4(b[511:0], imm8[5:4])
//		tmp_dst[511:384] := SELECT4(b[511:0], imm8[7:6])
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFI64X2'. Intrinsic: '_mm512_mask_shuffle_i64x2'.
// Requires AVX512F.
func M512MaskShuffleI64x2(src x86.M512i, k x86.Mmask8, a x86.M512i, b x86.M512i, imm8 int) x86.M512i {
	return x86.M512i(m512MaskShuffleI64x2([64]byte(src), uint8(k), [64]byte(a), [64]byte(b), imm8))
}

func m512MaskShuffleI64x2(src [64]byte, k uint8, a [64]byte, b [64]byte, imm8 int) [64]byte


// M512MaskzShuffleI64x2: Shuffle 128-bits (composed of 2 64-bit integers)
// selected by 'imm8' from 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			2:	tmp[127:0] := src[383:256]
//			3:	tmp[127:0] := src[511:384]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT4(a[511:0], imm8[1:0])
//		tmp_dst[255:128] := SELECT4(a[511:0], imm8[3:2])
//		tmp_dst[383:256] := SELECT4(b[511:0], imm8[5:4])
//		tmp_dst[511:384] := SELECT4(b[511:0], imm8[7:6])
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFI64X2'. Intrinsic: '_mm512_maskz_shuffle_i64x2'.
// Requires AVX512F.
func M512MaskzShuffleI64x2(k x86.Mmask8, a x86.M512i, b x86.M512i, imm8 int) x86.M512i {
	return x86.M512i(m512MaskzShuffleI64x2(uint8(k), [64]byte(a), [64]byte(b), imm8))
}

func m512MaskzShuffleI64x2(k uint8, a [64]byte, b [64]byte, imm8 int) [64]byte


// M512ShuffleI64x2: Shuffle 128-bits (composed of 2 64-bit integers) selected
// by 'imm8' from 'a' and 'b', and store the results in 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			2:	tmp[127:0] := src[383:256]
//			3:	tmp[127:0] := src[511:384]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		dst[127:0] := SELECT4(a[511:0], imm8[1:0])
//		dst[255:128] := SELECT4(a[511:0], imm8[3:2])
//		dst[383:256] := SELECT4(b[511:0], imm8[5:4])
//		dst[511:384] := SELECT4(b[511:0], imm8[7:6])
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFI64X2'. Intrinsic: '_mm512_shuffle_i64x2'.
// Requires AVX512F.
func M512ShuffleI64x2(a x86.M512i, b x86.M512i, imm8 int) x86.M512i {
	return x86.M512i(m512ShuffleI64x2([64]byte(a), [64]byte(b), imm8))
}

func m512ShuffleI64x2(a [64]byte, b [64]byte, imm8 int) [64]byte


// MaskShufflePd: Shuffle double-precision (64-bit) floating-point elements
// using the control in 'imm8', and store the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		tmp_dst[63:0] := (imm8[0] == 0) ? a[63:0] : a[127:64]
//		tmp_dst[127:64] := (imm8[1] == 0) ? b[63:0] : b[127:64]
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSHUFPD'. Intrinsic: '_mm_mask_shuffle_pd'.
// Requires AVX512F.
func MaskShufflePd(src x86.M128d, k x86.Mmask8, a x86.M128d, b x86.M128d, imm8 int) x86.M128d {
	return x86.M128d(maskShufflePd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), imm8))
}

func maskShufflePd(src [2]float64, k uint8, a [2]float64, b [2]float64, imm8 int) [2]float64


// MaskzShufflePd: Shuffle double-precision (64-bit) floating-point elements
// using the control in 'imm8', and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		tmp_dst[63:0] := (imm8[0] == 0) ? a[63:0] : a[127:64]
//		tmp_dst[127:64] := (imm8[1] == 0) ? b[63:0] : b[127:64]
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSHUFPD'. Intrinsic: '_mm_maskz_shuffle_pd'.
// Requires AVX512F.
func MaskzShufflePd(k x86.Mmask8, a x86.M128d, b x86.M128d, imm8 int) x86.M128d {
	return x86.M128d(maskzShufflePd(uint8(k), [2]float64(a), [2]float64(b), imm8))
}

func maskzShufflePd(k uint8, a [2]float64, b [2]float64, imm8 int) [2]float64


// M256MaskShufflePd: Shuffle double-precision (64-bit) floating-point elements
// within 128-bit lanes using the control in 'imm8', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		tmp_dst[63:0] := (imm8[0] == 0) ? a[63:0] : a[127:64]
//		tmp_dst[127:64] := (imm8[1] == 0) ? b[63:0] : b[127:64]
//		tmp_dst[191:128] := (imm8[2] == 0) ? a[191:128] : a[255:192]
//		tmp_dst[255:192] := (imm8[3] == 0) ? b[191:128] : b[255:192]
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFPD'. Intrinsic: '_mm256_mask_shuffle_pd'.
// Requires AVX512F.
func M256MaskShufflePd(src x86.M256d, k x86.Mmask8, a x86.M256d, b x86.M256d, imm8 int) x86.M256d {
	return x86.M256d(m256MaskShufflePd([4]float64(src), uint8(k), [4]float64(a), [4]float64(b), imm8))
}

func m256MaskShufflePd(src [4]float64, k uint8, a [4]float64, b [4]float64, imm8 int) [4]float64


// M256MaskzShufflePd: Shuffle double-precision (64-bit) floating-point
// elements within 128-bit lanes using the control in 'imm8', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		tmp_dst[63:0] := (imm8[0] == 0) ? a[63:0] : a[127:64]
//		tmp_dst[127:64] := (imm8[1] == 0) ? b[63:0] : b[127:64]
//		tmp_dst[191:128] := (imm8[2] == 0) ? a[191:128] : a[255:192]
//		tmp_dst[255:192] := (imm8[3] == 0) ? b[191:128] : b[255:192]
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFPD'. Intrinsic: '_mm256_maskz_shuffle_pd'.
// Requires AVX512F.
func M256MaskzShufflePd(k x86.Mmask8, a x86.M256d, b x86.M256d, imm8 int) x86.M256d {
	return x86.M256d(m256MaskzShufflePd(uint8(k), [4]float64(a), [4]float64(b), imm8))
}

func m256MaskzShufflePd(k uint8, a [4]float64, b [4]float64, imm8 int) [4]float64


// M512MaskShufflePd: Shuffle double-precision (64-bit) floating-point elements
// within 128-bit lanes using the control in 'imm8', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		tmp_dst[63:0] := (imm8[0] == 0) ? a[63:0] : a[127:64]
//		tmp_dst[127:64] := (imm8[1] == 0) ? b[63:0] : b[127:64]
//		tmp_dst[191:128] := (imm8[2] == 0) ? a[191:128] : a[255:192]
//		tmp_dst[255:192] := (imm8[3] == 0) ? b[191:128] : b[255:192]
//		tmp_dst[319:256] := (imm8[4] == 0) ? a[319:256] : a[383:320]
//		tmp_dst[383:320] := (imm8[5] == 0) ? b[319:256] : b[383:320]
//		tmp_dst[447:384] := (imm8[6] == 0) ? a[447:384] : a[511:448]
//		tmp_dst[511:448] := (imm8[7] == 0) ? b[447:384] : b[511:448]
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFPD'. Intrinsic: '_mm512_mask_shuffle_pd'.
// Requires AVX512F.
func M512MaskShufflePd(src x86.M512d, k x86.Mmask8, a x86.M512d, b x86.M512d, imm8 int) x86.M512d {
	return x86.M512d(m512MaskShufflePd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b), imm8))
}

func m512MaskShufflePd(src [8]float64, k uint8, a [8]float64, b [8]float64, imm8 int) [8]float64


// M512MaskzShufflePd: Shuffle double-precision (64-bit) floating-point
// elements within 128-bit lanes using the control in 'imm8', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		tmp_dst[63:0] := (imm8[0] == 0) ? a[63:0] : a[127:64]
//		tmp_dst[127:64] := (imm8[1] == 0) ? b[63:0] : b[127:64]
//		tmp_dst[191:128] := (imm8[2] == 0) ? a[191:128] : a[255:192]
//		tmp_dst[255:192] := (imm8[3] == 0) ? b[191:128] : b[255:192]
//		tmp_dst[319:256] := (imm8[4] == 0) ? a[319:256] : a[383:320]
//		tmp_dst[383:320] := (imm8[5] == 0) ? b[319:256] : b[383:320]
//		tmp_dst[447:384] := (imm8[6] == 0) ? a[447:384] : a[511:448]
//		tmp_dst[511:448] := (imm8[7] == 0) ? b[447:384] : b[511:448]
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFPD'. Intrinsic: '_mm512_maskz_shuffle_pd'.
// Requires AVX512F.
func M512MaskzShufflePd(k x86.Mmask8, a x86.M512d, b x86.M512d, imm8 int) x86.M512d {
	return x86.M512d(m512MaskzShufflePd(uint8(k), [8]float64(a), [8]float64(b), imm8))
}

func m512MaskzShufflePd(k uint8, a [8]float64, b [8]float64, imm8 int) [8]float64


// M512ShufflePd: Shuffle double-precision (64-bit) floating-point elements
// within 128-bit lanes using the control in 'imm8', and store the results in
// 'dst'. 
//
//		dst[63:0] := (imm8[0] == 0) ? a[63:0] : a[127:64]
//		dst[127:64] := (imm8[1] == 0) ? b[63:0] : b[127:64]
//		dst[191:128] := (imm8[2] == 0) ? a[191:128] : a[255:192]
//		dst[255:192] := (imm8[3] == 0) ? b[191:128] : b[255:192]
//		dst[319:256] := (imm8[4] == 0) ? a[319:256] : a[383:320]
//		dst[383:320] := (imm8[5] == 0) ? b[319:256] : b[383:320]
//		dst[447:384] := (imm8[6] == 0) ? a[447:384] : a[511:448]
//		dst[511:448] := (imm8[7] == 0) ? b[447:384] : b[511:448]
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFPD'. Intrinsic: '_mm512_shuffle_pd'.
// Requires AVX512F.
func M512ShufflePd(a x86.M512d, b x86.M512d, imm8 int) x86.M512d {
	return x86.M512d(m512ShufflePd([8]float64(a), [8]float64(b), imm8))
}

func m512ShufflePd(a [8]float64, b [8]float64, imm8 int) [8]float64


// MaskShufflePs: Shuffle single-precision (32-bit) floating-point elements in
// 'a' using the control in 'imm8', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(b[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(b[127:0], imm8[7:6])
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSHUFPS'. Intrinsic: '_mm_mask_shuffle_ps'.
// Requires AVX512F.
func MaskShufflePs(src x86.M128, k x86.Mmask8, a x86.M128, b x86.M128, imm8 int) x86.M128 {
	return x86.M128(maskShufflePs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), imm8))
}

func maskShufflePs(src [4]float32, k uint8, a [4]float32, b [4]float32, imm8 int) [4]float32


// MaskzShufflePs: Shuffle single-precision (32-bit) floating-point elements in
// 'a' using the control in 'imm8', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(b[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(b[127:0], imm8[7:6])
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSHUFPS'. Intrinsic: '_mm_maskz_shuffle_ps'.
// Requires AVX512F.
func MaskzShufflePs(k x86.Mmask8, a x86.M128, b x86.M128, imm8 int) x86.M128 {
	return x86.M128(maskzShufflePs(uint8(k), [4]float32(a), [4]float32(b), imm8))
}

func maskzShufflePs(k uint8, a [4]float32, b [4]float32, imm8 int) [4]float32


// M256MaskShufflePs: Shuffle single-precision (32-bit) floating-point elements
// in 'a' within 128-bit lanes using the control in 'imm8', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(b[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(b[127:0], imm8[7:6])
//		tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		tmp_dst[223:192] := SELECT4(b[255:128], imm8[5:4])
//		tmp_dst[255:224] := SELECT4(b[255:128], imm8[7:6])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFPS'. Intrinsic: '_mm256_mask_shuffle_ps'.
// Requires AVX512F.
func M256MaskShufflePs(src x86.M256, k x86.Mmask8, a x86.M256, b x86.M256, imm8 int) x86.M256 {
	return x86.M256(m256MaskShufflePs([8]float32(src), uint8(k), [8]float32(a), [8]float32(b), imm8))
}

func m256MaskShufflePs(src [8]float32, k uint8, a [8]float32, b [8]float32, imm8 int) [8]float32


// M256MaskzShufflePs: Shuffle single-precision (32-bit) floating-point
// elements in 'a' within 128-bit lanes using the control in 'imm8', and store
// the results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(b[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(b[127:0], imm8[7:6])
//		tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		tmp_dst[223:192] := SELECT4(b[255:128], imm8[5:4])
//		tmp_dst[255:224] := SELECT4(b[255:128], imm8[7:6])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFPS'. Intrinsic: '_mm256_maskz_shuffle_ps'.
// Requires AVX512F.
func M256MaskzShufflePs(k x86.Mmask8, a x86.M256, b x86.M256, imm8 int) x86.M256 {
	return x86.M256(m256MaskzShufflePs(uint8(k), [8]float32(a), [8]float32(b), imm8))
}

func m256MaskzShufflePs(k uint8, a [8]float32, b [8]float32, imm8 int) [8]float32


// M512MaskShufflePs: Shuffle single-precision (32-bit) floating-point elements
// in 'a' within 128-bit lanes using the control in 'imm8', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(b[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(b[127:0], imm8[7:6])
//		tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		tmp_dst[223:192] := SELECT4(b[255:128], imm8[5:4])
//		tmp_dst[255:224] := SELECT4(b[255:128], imm8[7:6])
//		tmp_dst[287:256] := SELECT4(a[383:256], imm8[1:0])
//		tmp_dst[319:288] := SELECT4(a[383:256], imm8[3:2])
//		tmp_dst[351:320] := SELECT4(b[383:256], imm8[5:4])
//		tmp_dst[383:352] := SELECT4(b[383:256], imm8[7:6])
//		tmp_dst[415:384] := SELECT4(a[511:384], imm8[1:0])
//		tmp_dst[447:416] := SELECT4(a[511:384], imm8[3:2])
//		tmp_dst[479:448] := SELECT4(b[511:384], imm8[5:4])
//		tmp_dst[511:480] := SELECT4(b[511:384], imm8[7:6])
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFPS'. Intrinsic: '_mm512_mask_shuffle_ps'.
// Requires AVX512F.
func M512MaskShufflePs(src x86.M512, k x86.Mmask16, a x86.M512, b x86.M512, imm8 int) x86.M512 {
	return x86.M512(m512MaskShufflePs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b), imm8))
}

func m512MaskShufflePs(src [16]float32, k uint16, a [16]float32, b [16]float32, imm8 int) [16]float32


// M512MaskzShufflePs: Shuffle single-precision (32-bit) floating-point
// elements in 'a' within 128-bit lanes using the control in 'imm8', and store
// the results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(b[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(b[127:0], imm8[7:6])
//		tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		tmp_dst[223:192] := SELECT4(b[255:128], imm8[5:4])
//		tmp_dst[255:224] := SELECT4(b[255:128], imm8[7:6])
//		tmp_dst[287:256] := SELECT4(a[383:256], imm8[1:0])
//		tmp_dst[319:288] := SELECT4(a[383:256], imm8[3:2])
//		tmp_dst[351:320] := SELECT4(b[383:256], imm8[5:4])
//		tmp_dst[383:352] := SELECT4(b[383:256], imm8[7:6])
//		tmp_dst[415:384] := SELECT4(a[511:384], imm8[1:0])
//		tmp_dst[447:416] := SELECT4(a[511:384], imm8[3:2])
//		tmp_dst[479:448] := SELECT4(b[511:384], imm8[5:4])
//		tmp_dst[511:480] := SELECT4(b[511:384], imm8[7:6])
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFPS'. Intrinsic: '_mm512_maskz_shuffle_ps'.
// Requires AVX512F.
func M512MaskzShufflePs(k x86.Mmask16, a x86.M512, b x86.M512, imm8 int) x86.M512 {
	return x86.M512(m512MaskzShufflePs(uint16(k), [16]float32(a), [16]float32(b), imm8))
}

func m512MaskzShufflePs(k uint16, a [16]float32, b [16]float32, imm8 int) [16]float32


// M512ShufflePs: Shuffle single-precision (32-bit) floating-point elements in
// 'a' within 128-bit lanes using the control in 'imm8', and store the results
// in 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		dst[95:64] := SELECT4(b[127:0], imm8[5:4])
//		dst[127:96] := SELECT4(b[127:0], imm8[7:6])
//		dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		dst[223:192] := SELECT4(b[255:128], imm8[5:4])
//		dst[255:224] := SELECT4(b[255:128], imm8[7:6])
//		dst[287:256] := SELECT4(a[383:256], imm8[1:0])
//		dst[319:288] := SELECT4(a[383:256], imm8[3:2])
//		dst[351:320] := SELECT4(b[383:256], imm8[5:4])
//		dst[383:352] := SELECT4(b[383:256], imm8[7:6])
//		dst[415:384] := SELECT4(a[511:384], imm8[1:0])
//		dst[447:416] := SELECT4(a[511:384], imm8[3:2])
//		dst[479:448] := SELECT4(b[511:384], imm8[5:4])
//		dst[511:480] := SELECT4(b[511:384], imm8[7:6])
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFPS'. Intrinsic: '_mm512_shuffle_ps'.
// Requires AVX512F.
func M512ShufflePs(a x86.M512, b x86.M512, imm8 int) x86.M512 {
	return x86.M512(m512ShufflePs([16]float32(a), [16]float32(b), imm8))
}

func m512ShufflePs(a [16]float32, b [16]float32, imm8 int) [16]float32


// M512MaskSinPd: Compute the sine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SIN(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_sin_pd'.
// Requires AVX512F.
func M512MaskSinPd(src x86.M512d, k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskSinPd([8]float64(src), uint8(k), [8]float64(a)))
}

func m512MaskSinPd(src [8]float64, k uint8, a [8]float64) [8]float64


// M512SinPd: Compute the sine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := SIN(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_sin_pd'.
// Requires AVX512F.
func M512SinPd(a x86.M512d) x86.M512d {
	return x86.M512d(m512SinPd([8]float64(a)))
}

func m512SinPd(a [8]float64) [8]float64


// M512MaskSinPs: Compute the sine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SIN(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_sin_ps'.
// Requires AVX512F.
func M512MaskSinPs(src x86.M512, k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskSinPs([16]float32(src), uint16(k), [16]float32(a)))
}

func m512MaskSinPs(src [16]float32, k uint16, a [16]float32) [16]float32


// M512SinPs: Compute the sine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := SIN(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_sin_ps'.
// Requires AVX512F.
func M512SinPs(a x86.M512) x86.M512 {
	return x86.M512(m512SinPs([16]float32(a)))
}

func m512SinPs(a [16]float32) [16]float32


// M512MaskSincosPd: Computes the sine and cosine of the packed
// double-precision (64-bit) floating-point elements in 'a' and stores the
// results of the sine computation in 'dst' and the results of the cosine
// computation in 'cos_res'. Elements are written to their respective locations
// using writemask 'k' (elements are copied from 'sin_src' or 'cos_src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SIN(a[i+63:i])
//				cos_res[i+63:i] := COS(a[i+63:i])
//			ELSE
//				dst[i+63:i] := sin_src[i+63:i]
//				cos_res[i+63:i] := cos_src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//		cos_res[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_sincos_pd'.
// Requires AVX512F.
// FIXME: Will likely need to be reworked.
func M512MaskSincosPd(cos_res *x86.M512d, sin_src x86.M512d, cos_src x86.M512d, k x86.Mmask8, a x86.M512d) x86.M512d {
	// FIXME: Rework to avoid possible return value as parameter.
	return x86.M512d{}
}

// M512SincosPd: Computes the sine and cosine of the packed double-precision
// (64-bit) floating-point elements in 'a' and stores the results of the sine
// computation in 'dst' and the results of the cosine computation in 'cos_res'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := SIN(a[i+63:i])
//			cos_res[i+63:i] := COS(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//		cos_res[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_sincos_pd'.
// Requires AVX512F.
// FIXME: Will likely need to be reworked.
func M512SincosPd(cos_res *x86.M512d, a x86.M512d) x86.M512d {
	// FIXME: Rework to avoid possible return value as parameter.
	return x86.M512d{}
}

// M512MaskSincosPs: Computes the sine and cosine of the packed
// single-precision (32-bit) floating-point elements in 'a' and stores the
// results of the sine computation in 'dst' and the results of the cosine
// computation in 'cos_res'. Elements are written to their respective locations
// using writemask 'k' (elements are copied from 'sin_src' or 'cos_src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SIN(a[i+31:i])
//				cos_res[i+31:i] := COS(a[i+31:i])
//			ELSE
//				dst[i+31:i] := sin_src[i+31:i]
//				cos_res[i+31:i] := cos_src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//		cos_res[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_sincos_ps'.
// Requires AVX512F.
// FIXME: Will likely need to be reworked.
func M512MaskSincosPs(cos_res *x86.M512, sin_src x86.M512, cos_src x86.M512, k x86.Mmask16, a x86.M512) x86.M512 {
	// FIXME: Rework to avoid possible return value as parameter.
	return x86.M512{}
}

// M512SincosPs: Computes the sine and cosine of the packed single-precision
// (32-bit) floating-point elements in 'a' and stores the results of the sine
// computation in 'dst' and the results of the cosine computation in 'cos_res'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := SIN(a[i+31:i])
//			cos_res[i+31:i] := COS(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//		cos_res[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_sincos_ps'.
// Requires AVX512F.
// FIXME: Will likely need to be reworked.
func M512SincosPs(cos_res *x86.M512, a x86.M512) x86.M512 {
	// FIXME: Rework to avoid possible return value as parameter.
	return x86.M512{}
}

// M512MaskSindPd: Compute the sine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in degrees, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SIND(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_sind_pd'.
// Requires AVX512F.
func M512MaskSindPd(src x86.M512d, k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskSindPd([8]float64(src), uint8(k), [8]float64(a)))
}

func m512MaskSindPd(src [8]float64, k uint8, a [8]float64) [8]float64


// M512SindPd: Compute the sine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in degrees, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := SIND(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_sind_pd'.
// Requires AVX512F.
func M512SindPd(a x86.M512d) x86.M512d {
	return x86.M512d(m512SindPd([8]float64(a)))
}

func m512SindPd(a [8]float64) [8]float64


// M512MaskSindPs: Compute the sine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in degrees, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SIND(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_sind_ps'.
// Requires AVX512F.
func M512MaskSindPs(src x86.M512, k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskSindPs([16]float32(src), uint16(k), [16]float32(a)))
}

func m512MaskSindPs(src [16]float32, k uint16, a [16]float32) [16]float32


// M512SindPs: Compute the sine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in degrees, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := SIND(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_sind_ps'.
// Requires AVX512F.
func M512SindPs(a x86.M512) x86.M512 {
	return x86.M512(m512SindPs([16]float32(a)))
}

func m512SindPs(a [16]float32) [16]float32


// M512MaskSinhPd: Compute the hyperbolic sine of packed double-precision
// (64-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SINH(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_sinh_pd'.
// Requires AVX512F.
func M512MaskSinhPd(src x86.M512d, k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskSinhPd([8]float64(src), uint8(k), [8]float64(a)))
}

func m512MaskSinhPd(src [8]float64, k uint8, a [8]float64) [8]float64


// M512SinhPd: Compute the hyperbolic sine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := SINH(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_sinh_pd'.
// Requires AVX512F.
func M512SinhPd(a x86.M512d) x86.M512d {
	return x86.M512d(m512SinhPd([8]float64(a)))
}

func m512SinhPd(a [8]float64) [8]float64


// M512MaskSinhPs: Compute the hyperbolic sine of packed single-precision
// (32-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SINH(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_sinh_ps'.
// Requires AVX512F.
func M512MaskSinhPs(src x86.M512, k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskSinhPs([16]float32(src), uint16(k), [16]float32(a)))
}

func m512MaskSinhPs(src [16]float32, k uint16, a [16]float32) [16]float32


// M512SinhPs: Compute the hyperbolic sine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := SINH(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_sinh_ps'.
// Requires AVX512F.
func M512SinhPs(a x86.M512) x86.M512 {
	return x86.M512(m512SinhPs([16]float32(a)))
}

func m512SinhPs(a [16]float32) [16]float32


// MaskSllEpi32: Shift packed 32-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSLLD'. Intrinsic: '_mm_mask_sll_epi32'.
// Requires AVX512F.
func MaskSllEpi32(src x86.M128i, k x86.Mmask8, a x86.M128i, count x86.M128i) x86.M128i {
	return x86.M128i(maskSllEpi32([16]byte(src), uint8(k), [16]byte(a), [16]byte(count)))
}

func maskSllEpi32(src [16]byte, k uint8, a [16]byte, count [16]byte) [16]byte


// MaskzSllEpi32: Shift packed 32-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSLLD'. Intrinsic: '_mm_maskz_sll_epi32'.
// Requires AVX512F.
func MaskzSllEpi32(k x86.Mmask8, a x86.M128i, count x86.M128i) x86.M128i {
	return x86.M128i(maskzSllEpi32(uint8(k), [16]byte(a), [16]byte(count)))
}

func maskzSllEpi32(k uint8, a [16]byte, count [16]byte) [16]byte


// M256MaskSllEpi32: Shift packed 32-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLD'. Intrinsic: '_mm256_mask_sll_epi32'.
// Requires AVX512F.
func M256MaskSllEpi32(src x86.M256i, k x86.Mmask8, a x86.M256i, count x86.M128i) x86.M256i {
	return x86.M256i(m256MaskSllEpi32([32]byte(src), uint8(k), [32]byte(a), [16]byte(count)))
}

func m256MaskSllEpi32(src [32]byte, k uint8, a [32]byte, count [16]byte) [32]byte


// M256MaskzSllEpi32: Shift packed 32-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLD'. Intrinsic: '_mm256_maskz_sll_epi32'.
// Requires AVX512F.
func M256MaskzSllEpi32(k x86.Mmask8, a x86.M256i, count x86.M128i) x86.M256i {
	return x86.M256i(m256MaskzSllEpi32(uint8(k), [32]byte(a), [16]byte(count)))
}

func m256MaskzSllEpi32(k uint8, a [32]byte, count [16]byte) [32]byte


// M512MaskSllEpi32: Shift packed 32-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLD'. Intrinsic: '_mm512_mask_sll_epi32'.
// Requires AVX512F.
func M512MaskSllEpi32(src x86.M512i, k x86.Mmask16, a x86.M512i, count x86.M128i) x86.M512i {
	return x86.M512i(m512MaskSllEpi32([64]byte(src), uint16(k), [64]byte(a), [16]byte(count)))
}

func m512MaskSllEpi32(src [64]byte, k uint16, a [64]byte, count [16]byte) [64]byte


// M512MaskzSllEpi32: Shift packed 32-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLD'. Intrinsic: '_mm512_maskz_sll_epi32'.
// Requires AVX512F.
func M512MaskzSllEpi32(k x86.Mmask16, a x86.M512i, count x86.M128i) x86.M512i {
	return x86.M512i(m512MaskzSllEpi32(uint16(k), [64]byte(a), [16]byte(count)))
}

func m512MaskzSllEpi32(k uint16, a [64]byte, count [16]byte) [64]byte


// M512SllEpi32: Shift packed 32-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF count[63:0] > 31
//				dst[i+31:i] := 0
//			ELSE
//				dst[i+31:i] := ZeroExtend(a[i+31:i] << count[63:0])
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLD'. Intrinsic: '_mm512_sll_epi32'.
// Requires AVX512F.
func M512SllEpi32(a x86.M512i, count x86.M128i) x86.M512i {
	return x86.M512i(m512SllEpi32([64]byte(a), [16]byte(count)))
}

func m512SllEpi32(a [64]byte, count [16]byte) [64]byte


// MaskSllEpi64: Shift packed 64-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm_mask_sll_epi64'.
// Requires AVX512F.
func MaskSllEpi64(src x86.M128i, k x86.Mmask8, a x86.M128i, count x86.M128i) x86.M128i {
	return x86.M128i(maskSllEpi64([16]byte(src), uint8(k), [16]byte(a), [16]byte(count)))
}

func maskSllEpi64(src [16]byte, k uint8, a [16]byte, count [16]byte) [16]byte


// MaskzSllEpi64: Shift packed 64-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm_maskz_sll_epi64'.
// Requires AVX512F.
func MaskzSllEpi64(k x86.Mmask8, a x86.M128i, count x86.M128i) x86.M128i {
	return x86.M128i(maskzSllEpi64(uint8(k), [16]byte(a), [16]byte(count)))
}

func maskzSllEpi64(k uint8, a [16]byte, count [16]byte) [16]byte


// M256MaskSllEpi64: Shift packed 64-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm256_mask_sll_epi64'.
// Requires AVX512F.
func M256MaskSllEpi64(src x86.M256i, k x86.Mmask8, a x86.M256i, count x86.M128i) x86.M256i {
	return x86.M256i(m256MaskSllEpi64([32]byte(src), uint8(k), [32]byte(a), [16]byte(count)))
}

func m256MaskSllEpi64(src [32]byte, k uint8, a [32]byte, count [16]byte) [32]byte


// M256MaskzSllEpi64: Shift packed 64-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm256_maskz_sll_epi64'.
// Requires AVX512F.
func M256MaskzSllEpi64(k x86.Mmask8, a x86.M256i, count x86.M128i) x86.M256i {
	return x86.M256i(m256MaskzSllEpi64(uint8(k), [32]byte(a), [16]byte(count)))
}

func m256MaskzSllEpi64(k uint8, a [32]byte, count [16]byte) [32]byte


// M512MaskSllEpi64: Shift packed 64-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm512_mask_sll_epi64'.
// Requires AVX512F.
func M512MaskSllEpi64(src x86.M512i, k x86.Mmask8, a x86.M512i, count x86.M128i) x86.M512i {
	return x86.M512i(m512MaskSllEpi64([64]byte(src), uint8(k), [64]byte(a), [16]byte(count)))
}

func m512MaskSllEpi64(src [64]byte, k uint8, a [64]byte, count [16]byte) [64]byte


// M512MaskzSllEpi64: Shift packed 64-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm512_maskz_sll_epi64'.
// Requires AVX512F.
func M512MaskzSllEpi64(k x86.Mmask8, a x86.M512i, count x86.M128i) x86.M512i {
	return x86.M512i(m512MaskzSllEpi64(uint8(k), [64]byte(a), [16]byte(count)))
}

func m512MaskzSllEpi64(k uint8, a [64]byte, count [16]byte) [64]byte


// M512SllEpi64: Shift packed 64-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF count[63:0] > 63
//				dst[i+63:i] := 0
//			ELSE
//				dst[i+63:i] := ZeroExtend(a[i+63:i] << count[63:0])
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm512_sll_epi64'.
// Requires AVX512F.
func M512SllEpi64(a x86.M512i, count x86.M128i) x86.M512i {
	return x86.M512i(m512SllEpi64([64]byte(a), [16]byte(count)))
}

func m512SllEpi64(a [64]byte, count [16]byte) [64]byte


// MaskSlliEpi32: Shift packed 32-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSLLD'. Intrinsic: '_mm_mask_slli_epi32'.
// Requires AVX512F.
func MaskSlliEpi32(src x86.M128i, k x86.Mmask8, a x86.M128i, imm8 uint32) x86.M128i {
	return x86.M128i(maskSlliEpi32([16]byte(src), uint8(k), [16]byte(a), imm8))
}

func maskSlliEpi32(src [16]byte, k uint8, a [16]byte, imm8 uint32) [16]byte


// MaskzSlliEpi32: Shift packed 32-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSLLD'. Intrinsic: '_mm_maskz_slli_epi32'.
// Requires AVX512F.
func MaskzSlliEpi32(k x86.Mmask8, a x86.M128i, imm8 uint32) x86.M128i {
	return x86.M128i(maskzSlliEpi32(uint8(k), [16]byte(a), imm8))
}

func maskzSlliEpi32(k uint8, a [16]byte, imm8 uint32) [16]byte


// M256MaskSlliEpi32: Shift packed 32-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLD'. Intrinsic: '_mm256_mask_slli_epi32'.
// Requires AVX512F.
func M256MaskSlliEpi32(src x86.M256i, k x86.Mmask8, a x86.M256i, imm8 uint32) x86.M256i {
	return x86.M256i(m256MaskSlliEpi32([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func m256MaskSlliEpi32(src [32]byte, k uint8, a [32]byte, imm8 uint32) [32]byte


// M256MaskzSlliEpi32: Shift packed 32-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLD'. Intrinsic: '_mm256_maskz_slli_epi32'.
// Requires AVX512F.
func M256MaskzSlliEpi32(k x86.Mmask8, a x86.M256i, imm8 uint32) x86.M256i {
	return x86.M256i(m256MaskzSlliEpi32(uint8(k), [32]byte(a), imm8))
}

func m256MaskzSlliEpi32(k uint8, a [32]byte, imm8 uint32) [32]byte


// M512MaskzSlliEpi32: Shift packed 32-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLD'. Intrinsic: '_mm512_maskz_slli_epi32'.
// Requires AVX512F.
func M512MaskzSlliEpi32(k x86.Mmask16, a x86.M512i, imm8 uint32) x86.M512i {
	return x86.M512i(m512MaskzSlliEpi32(uint16(k), [64]byte(a), imm8))
}

func m512MaskzSlliEpi32(k uint16, a [64]byte, imm8 uint32) [64]byte


// MaskSlliEpi64: Shift packed 64-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm_mask_slli_epi64'.
// Requires AVX512F.
func MaskSlliEpi64(src x86.M128i, k x86.Mmask8, a x86.M128i, imm8 uint32) x86.M128i {
	return x86.M128i(maskSlliEpi64([16]byte(src), uint8(k), [16]byte(a), imm8))
}

func maskSlliEpi64(src [16]byte, k uint8, a [16]byte, imm8 uint32) [16]byte


// MaskzSlliEpi64: Shift packed 64-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm_maskz_slli_epi64'.
// Requires AVX512F.
func MaskzSlliEpi64(k x86.Mmask8, a x86.M128i, imm8 uint32) x86.M128i {
	return x86.M128i(maskzSlliEpi64(uint8(k), [16]byte(a), imm8))
}

func maskzSlliEpi64(k uint8, a [16]byte, imm8 uint32) [16]byte


// M256MaskSlliEpi64: Shift packed 64-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm256_mask_slli_epi64'.
// Requires AVX512F.
func M256MaskSlliEpi64(src x86.M256i, k x86.Mmask8, a x86.M256i, imm8 uint32) x86.M256i {
	return x86.M256i(m256MaskSlliEpi64([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func m256MaskSlliEpi64(src [32]byte, k uint8, a [32]byte, imm8 uint32) [32]byte


// M256MaskzSlliEpi64: Shift packed 64-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm256_maskz_slli_epi64'.
// Requires AVX512F.
func M256MaskzSlliEpi64(k x86.Mmask8, a x86.M256i, imm8 uint32) x86.M256i {
	return x86.M256i(m256MaskzSlliEpi64(uint8(k), [32]byte(a), imm8))
}

func m256MaskzSlliEpi64(k uint8, a [32]byte, imm8 uint32) [32]byte


// M512MaskSlliEpi64: Shift packed 64-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm512_mask_slli_epi64'.
// Requires AVX512F.
func M512MaskSlliEpi64(src x86.M512i, k x86.Mmask8, a x86.M512i, imm8 uint32) x86.M512i {
	return x86.M512i(m512MaskSlliEpi64([64]byte(src), uint8(k), [64]byte(a), imm8))
}

func m512MaskSlliEpi64(src [64]byte, k uint8, a [64]byte, imm8 uint32) [64]byte


// M512MaskzSlliEpi64: Shift packed 64-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm512_maskz_slli_epi64'.
// Requires AVX512F.
func M512MaskzSlliEpi64(k x86.Mmask8, a x86.M512i, imm8 uint32) x86.M512i {
	return x86.M512i(m512MaskzSlliEpi64(uint8(k), [64]byte(a), imm8))
}

func m512MaskzSlliEpi64(k uint8, a [64]byte, imm8 uint32) [64]byte


// M512SlliEpi64: Shift packed 64-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF imm8[7:0] > 63
//				dst[i+63:i] := 0
//			ELSE
//				dst[i+63:i] := ZeroExtend(a[i+63:i] << imm8[7:0])
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm512_slli_epi64'.
// Requires AVX512F.
func M512SlliEpi64(a x86.M512i, imm8 uint32) x86.M512i {
	return x86.M512i(m512SlliEpi64([64]byte(a), imm8))
}

func m512SlliEpi64(a [64]byte, imm8 uint32) [64]byte


// MaskSllvEpi32: Shift packed 32-bit integers in 'a' left by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[i+31:i] << count[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSLLVD'. Intrinsic: '_mm_mask_sllv_epi32'.
// Requires AVX512F.
func MaskSllvEpi32(src x86.M128i, k x86.Mmask8, a x86.M128i, count x86.M128i) x86.M128i {
	return x86.M128i(maskSllvEpi32([16]byte(src), uint8(k), [16]byte(a), [16]byte(count)))
}

func maskSllvEpi32(src [16]byte, k uint8, a [16]byte, count [16]byte) [16]byte


// MaskzSllvEpi32: Shift packed 32-bit integers in 'a' left by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[i+31:i] << count[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSLLVD'. Intrinsic: '_mm_maskz_sllv_epi32'.
// Requires AVX512F.
func MaskzSllvEpi32(k x86.Mmask8, a x86.M128i, count x86.M128i) x86.M128i {
	return x86.M128i(maskzSllvEpi32(uint8(k), [16]byte(a), [16]byte(count)))
}

func maskzSllvEpi32(k uint8, a [16]byte, count [16]byte) [16]byte


// M256MaskSllvEpi32: Shift packed 32-bit integers in 'a' left by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[i+31:i] << count[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLVD'. Intrinsic: '_mm256_mask_sllv_epi32'.
// Requires AVX512F.
func M256MaskSllvEpi32(src x86.M256i, k x86.Mmask8, a x86.M256i, count x86.M256i) x86.M256i {
	return x86.M256i(m256MaskSllvEpi32([32]byte(src), uint8(k), [32]byte(a), [32]byte(count)))
}

func m256MaskSllvEpi32(src [32]byte, k uint8, a [32]byte, count [32]byte) [32]byte


// M256MaskzSllvEpi32: Shift packed 32-bit integers in 'a' left by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[i+31:i] << count[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLVD'. Intrinsic: '_mm256_maskz_sllv_epi32'.
// Requires AVX512F.
func M256MaskzSllvEpi32(k x86.Mmask8, a x86.M256i, count x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzSllvEpi32(uint8(k), [32]byte(a), [32]byte(count)))
}

func m256MaskzSllvEpi32(k uint8, a [32]byte, count [32]byte) [32]byte


// M512MaskzSllvEpi32: Shift packed 32-bit integers in 'a' left by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[i+31:i] << count[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLVD'. Intrinsic: '_mm512_maskz_sllv_epi32'.
// Requires AVX512F.
func M512MaskzSllvEpi32(k x86.Mmask16, a x86.M512i, count x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzSllvEpi32(uint16(k), [64]byte(a), [64]byte(count)))
}

func m512MaskzSllvEpi32(k uint16, a [64]byte, count [64]byte) [64]byte


// MaskSllvEpi64: Shift packed 64-bit integers in 'a' left by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[i+63:i] << count[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSLLVQ'. Intrinsic: '_mm_mask_sllv_epi64'.
// Requires AVX512F.
func MaskSllvEpi64(src x86.M128i, k x86.Mmask8, a x86.M128i, count x86.M128i) x86.M128i {
	return x86.M128i(maskSllvEpi64([16]byte(src), uint8(k), [16]byte(a), [16]byte(count)))
}

func maskSllvEpi64(src [16]byte, k uint8, a [16]byte, count [16]byte) [16]byte


// MaskzSllvEpi64: Shift packed 64-bit integers in 'a' left by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[i+63:i] << count[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSLLVQ'. Intrinsic: '_mm_maskz_sllv_epi64'.
// Requires AVX512F.
func MaskzSllvEpi64(k x86.Mmask8, a x86.M128i, count x86.M128i) x86.M128i {
	return x86.M128i(maskzSllvEpi64(uint8(k), [16]byte(a), [16]byte(count)))
}

func maskzSllvEpi64(k uint8, a [16]byte, count [16]byte) [16]byte


// M256MaskSllvEpi64: Shift packed 64-bit integers in 'a' left by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[i+63:i] << count[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLVQ'. Intrinsic: '_mm256_mask_sllv_epi64'.
// Requires AVX512F.
func M256MaskSllvEpi64(src x86.M256i, k x86.Mmask8, a x86.M256i, count x86.M256i) x86.M256i {
	return x86.M256i(m256MaskSllvEpi64([32]byte(src), uint8(k), [32]byte(a), [32]byte(count)))
}

func m256MaskSllvEpi64(src [32]byte, k uint8, a [32]byte, count [32]byte) [32]byte


// M256MaskzSllvEpi64: Shift packed 64-bit integers in 'a' left by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[i+63:i] << count[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLVQ'. Intrinsic: '_mm256_maskz_sllv_epi64'.
// Requires AVX512F.
func M256MaskzSllvEpi64(k x86.Mmask8, a x86.M256i, count x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzSllvEpi64(uint8(k), [32]byte(a), [32]byte(count)))
}

func m256MaskzSllvEpi64(k uint8, a [32]byte, count [32]byte) [32]byte


// M512MaskSllvEpi64: Shift packed 64-bit integers in 'a' left by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[i+63:i] << count[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLVQ'. Intrinsic: '_mm512_mask_sllv_epi64'.
// Requires AVX512F.
func M512MaskSllvEpi64(src x86.M512i, k x86.Mmask8, a x86.M512i, count x86.M512i) x86.M512i {
	return x86.M512i(m512MaskSllvEpi64([64]byte(src), uint8(k), [64]byte(a), [64]byte(count)))
}

func m512MaskSllvEpi64(src [64]byte, k uint8, a [64]byte, count [64]byte) [64]byte


// M512MaskzSllvEpi64: Shift packed 64-bit integers in 'a' left by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[i+63:i] << count[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLVQ'. Intrinsic: '_mm512_maskz_sllv_epi64'.
// Requires AVX512F.
func M512MaskzSllvEpi64(k x86.Mmask8, a x86.M512i, count x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzSllvEpi64(uint8(k), [64]byte(a), [64]byte(count)))
}

func m512MaskzSllvEpi64(k uint8, a [64]byte, count [64]byte) [64]byte


// M512SllvEpi64: Shift packed 64-bit integers in 'a' left by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ZeroExtend(a[i+63:i] << count[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLVQ'. Intrinsic: '_mm512_sllv_epi64'.
// Requires AVX512F.
func M512SllvEpi64(a x86.M512i, count x86.M512i) x86.M512i {
	return x86.M512i(m512SllvEpi64([64]byte(a), [64]byte(count)))
}

func m512SllvEpi64(a [64]byte, count [64]byte) [64]byte


// MaskSqrtPd: Compute the square root of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SQRT(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSQRTPD'. Intrinsic: '_mm_mask_sqrt_pd'.
// Requires AVX512F.
func MaskSqrtPd(src x86.M128d, k x86.Mmask8, a x86.M128d) x86.M128d {
	return x86.M128d(maskSqrtPd([2]float64(src), uint8(k), [2]float64(a)))
}

func maskSqrtPd(src [2]float64, k uint8, a [2]float64) [2]float64


// MaskzSqrtPd: Compute the square root of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SQRT(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSQRTPD'. Intrinsic: '_mm_maskz_sqrt_pd'.
// Requires AVX512F.
func MaskzSqrtPd(k x86.Mmask8, a x86.M128d) x86.M128d {
	return x86.M128d(maskzSqrtPd(uint8(k), [2]float64(a)))
}

func maskzSqrtPd(k uint8, a [2]float64) [2]float64


// M256MaskSqrtPd: Compute the square root of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SQRT(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSQRTPD'. Intrinsic: '_mm256_mask_sqrt_pd'.
// Requires AVX512F.
func M256MaskSqrtPd(src x86.M256d, k x86.Mmask8, a x86.M256d) x86.M256d {
	return x86.M256d(m256MaskSqrtPd([4]float64(src), uint8(k), [4]float64(a)))
}

func m256MaskSqrtPd(src [4]float64, k uint8, a [4]float64) [4]float64


// M256MaskzSqrtPd: Compute the square root of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SQRT(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSQRTPD'. Intrinsic: '_mm256_maskz_sqrt_pd'.
// Requires AVX512F.
func M256MaskzSqrtPd(k x86.Mmask8, a x86.M256d) x86.M256d {
	return x86.M256d(m256MaskzSqrtPd(uint8(k), [4]float64(a)))
}

func m256MaskzSqrtPd(k uint8, a [4]float64) [4]float64


// M512MaskSqrtPd: Compute the square root of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SQRT(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSQRTPD'. Intrinsic: '_mm512_mask_sqrt_pd'.
// Requires AVX512F.
func M512MaskSqrtPd(src x86.M512d, k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskSqrtPd([8]float64(src), uint8(k), [8]float64(a)))
}

func m512MaskSqrtPd(src [8]float64, k uint8, a [8]float64) [8]float64


// M512MaskzSqrtPd: Compute the square root of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SQRT(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSQRTPD'. Intrinsic: '_mm512_maskz_sqrt_pd'.
// Requires AVX512F.
func M512MaskzSqrtPd(k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskzSqrtPd(uint8(k), [8]float64(a)))
}

func m512MaskzSqrtPd(k uint8, a [8]float64) [8]float64


// M512SqrtPd: Compute the square root of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := SQRT(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSQRTPD'. Intrinsic: '_mm512_sqrt_pd'.
// Requires AVX512F.
func M512SqrtPd(a x86.M512d) x86.M512d {
	return x86.M512d(m512SqrtPd([8]float64(a)))
}

func m512SqrtPd(a [8]float64) [8]float64


// MaskSqrtPs: Compute the square root of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SQRT(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSQRTPS'. Intrinsic: '_mm_mask_sqrt_ps'.
// Requires AVX512F.
func MaskSqrtPs(src x86.M128, k x86.Mmask8, a x86.M128) x86.M128 {
	return x86.M128(maskSqrtPs([4]float32(src), uint8(k), [4]float32(a)))
}

func maskSqrtPs(src [4]float32, k uint8, a [4]float32) [4]float32


// MaskzSqrtPs: Compute the square root of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SQRT(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSQRTPS'. Intrinsic: '_mm_maskz_sqrt_ps'.
// Requires AVX512F.
func MaskzSqrtPs(k x86.Mmask8, a x86.M128) x86.M128 {
	return x86.M128(maskzSqrtPs(uint8(k), [4]float32(a)))
}

func maskzSqrtPs(k uint8, a [4]float32) [4]float32


// M256MaskSqrtPs: Compute the square root of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SQRT(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSQRTPS'. Intrinsic: '_mm256_mask_sqrt_ps'.
// Requires AVX512F.
func M256MaskSqrtPs(src x86.M256, k x86.Mmask8, a x86.M256) x86.M256 {
	return x86.M256(m256MaskSqrtPs([8]float32(src), uint8(k), [8]float32(a)))
}

func m256MaskSqrtPs(src [8]float32, k uint8, a [8]float32) [8]float32


// M256MaskzSqrtPs: Compute the square root of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SQRT(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSQRTPS'. Intrinsic: '_mm256_maskz_sqrt_ps'.
// Requires AVX512F.
func M256MaskzSqrtPs(k x86.Mmask8, a x86.M256) x86.M256 {
	return x86.M256(m256MaskzSqrtPs(uint8(k), [8]float32(a)))
}

func m256MaskzSqrtPs(k uint8, a [8]float32) [8]float32


// M512MaskSqrtPs: Compute the square root of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SQRT(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSQRTPS'. Intrinsic: '_mm512_mask_sqrt_ps'.
// Requires AVX512F.
func M512MaskSqrtPs(src x86.M512, k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskSqrtPs([16]float32(src), uint16(k), [16]float32(a)))
}

func m512MaskSqrtPs(src [16]float32, k uint16, a [16]float32) [16]float32


// M512MaskzSqrtPs: Compute the square root of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SQRT(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSQRTPS'. Intrinsic: '_mm512_maskz_sqrt_ps'.
// Requires AVX512F.
func M512MaskzSqrtPs(k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskzSqrtPs(uint16(k), [16]float32(a)))
}

func m512MaskzSqrtPs(k uint16, a [16]float32) [16]float32


// M512SqrtPs: Compute the square root of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := SQRT(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSQRTPS'. Intrinsic: '_mm512_sqrt_ps'.
// Requires AVX512F.
func M512SqrtPs(a x86.M512) x86.M512 {
	return x86.M512(m512SqrtPs([16]float32(a)))
}

func m512SqrtPs(a [16]float32) [16]float32


// M512MaskSqrtRoundPd: Compute the square root of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SQRT(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSQRTPD'. Intrinsic: '_mm512_mask_sqrt_round_pd'.
// Requires AVX512F.
func M512MaskSqrtRoundPd(src x86.M512d, k x86.Mmask8, a x86.M512d, rounding int) x86.M512d {
	return x86.M512d(m512MaskSqrtRoundPd([8]float64(src), uint8(k), [8]float64(a), rounding))
}

func m512MaskSqrtRoundPd(src [8]float64, k uint8, a [8]float64, rounding int) [8]float64


// M512MaskzSqrtRoundPd: Compute the square root of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SQRT(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSQRTPD'. Intrinsic: '_mm512_maskz_sqrt_round_pd'.
// Requires AVX512F.
func M512MaskzSqrtRoundPd(k x86.Mmask8, a x86.M512d, rounding int) x86.M512d {
	return x86.M512d(m512MaskzSqrtRoundPd(uint8(k), [8]float64(a), rounding))
}

func m512MaskzSqrtRoundPd(k uint8, a [8]float64, rounding int) [8]float64


// M512SqrtRoundPd: Compute the square root of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := SQRT(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSQRTPD'. Intrinsic: '_mm512_sqrt_round_pd'.
// Requires AVX512F.
func M512SqrtRoundPd(a x86.M512d, rounding int) x86.M512d {
	return x86.M512d(m512SqrtRoundPd([8]float64(a), rounding))
}

func m512SqrtRoundPd(a [8]float64, rounding int) [8]float64


// M512MaskSqrtRoundPs: Compute the square root of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SQRT(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSQRTPS'. Intrinsic: '_mm512_mask_sqrt_round_ps'.
// Requires AVX512F.
func M512MaskSqrtRoundPs(src x86.M512, k x86.Mmask16, a x86.M512, rounding int) x86.M512 {
	return x86.M512(m512MaskSqrtRoundPs([16]float32(src), uint16(k), [16]float32(a), rounding))
}

func m512MaskSqrtRoundPs(src [16]float32, k uint16, a [16]float32, rounding int) [16]float32


// M512MaskzSqrtRoundPs: Compute the square root of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SQRT(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSQRTPS'. Intrinsic: '_mm512_maskz_sqrt_round_ps'.
// Requires AVX512F.
func M512MaskzSqrtRoundPs(k x86.Mmask16, a x86.M512, rounding int) x86.M512 {
	return x86.M512(m512MaskzSqrtRoundPs(uint16(k), [16]float32(a), rounding))
}

func m512MaskzSqrtRoundPs(k uint16, a [16]float32, rounding int) [16]float32


// M512SqrtRoundPs: Compute the square root of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := SQRT(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSQRTPS'. Intrinsic: '_mm512_sqrt_round_ps'.
// Requires AVX512F.
func M512SqrtRoundPs(a x86.M512, rounding int) x86.M512 {
	return x86.M512(m512SqrtRoundPs([16]float32(a), rounding))
}

func m512SqrtRoundPs(a [16]float32, rounding int) [16]float32


// MaskSqrtRoundSd: Compute the square root of the lower double-precision
// (64-bit) floating-point element in 'a', store the result in the lower
// element of 'dst' using writemask 'k' (the element is copied from 'src' when
// mask bit 0 is not set), and copy the upper element from 'b' to the upper
// element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := SQRT(a[63:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSQRTSD'. Intrinsic: '_mm_mask_sqrt_round_sd'.
// Requires AVX512F.
func MaskSqrtRoundSd(src x86.M128d, k x86.Mmask8, a x86.M128d, b x86.M128d, rounding int) x86.M128d {
	return x86.M128d(maskSqrtRoundSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), rounding))
}

func maskSqrtRoundSd(src [2]float64, k uint8, a [2]float64, b [2]float64, rounding int) [2]float64


// MaskzSqrtRoundSd: Compute the square root of the lower double-precision
// (64-bit) floating-point element in 'a', store the result in the lower
// element of 'dst' using zeromask 'k' (the element is zeroed out when mask bit
// 0 is not set), and copy the upper element from 'b' to the upper element of
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := SQRT(a[63:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSQRTSD'. Intrinsic: '_mm_maskz_sqrt_round_sd'.
// Requires AVX512F.
func MaskzSqrtRoundSd(k x86.Mmask8, a x86.M128d, b x86.M128d, rounding int) x86.M128d {
	return x86.M128d(maskzSqrtRoundSd(uint8(k), [2]float64(a), [2]float64(b), rounding))
}

func maskzSqrtRoundSd(k uint8, a [2]float64, b [2]float64, rounding int) [2]float64


// SqrtRoundSd: Compute the square root of the lower double-precision (64-bit)
// floating-point element in 'a', store the result in the lower element of
// 'dst', and copy the upper element from 'b' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := SQRT(a[63:0])
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSQRTSD'. Intrinsic: '_mm_sqrt_round_sd'.
// Requires AVX512F.
func SqrtRoundSd(a x86.M128d, b x86.M128d, rounding int) x86.M128d {
	return x86.M128d(sqrtRoundSd([2]float64(a), [2]float64(b), rounding))
}

func sqrtRoundSd(a [2]float64, b [2]float64, rounding int) [2]float64


// MaskSqrtRoundSs: Compute the square root of the lower single-precision
// (32-bit) floating-point element in 'a', store the result in the lower
// element of 'dst' using writemask 'k' (the element is copied from 'src' when
// mask bit 0 is not set), and copy the upper 3 packed elements from 'b' to the
// upper elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := SQRT(a[31:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSQRTSS'. Intrinsic: '_mm_mask_sqrt_round_ss'.
// Requires AVX512F.
func MaskSqrtRoundSs(src x86.M128, k x86.Mmask8, a x86.M128, b x86.M128, rounding int) x86.M128 {
	return x86.M128(maskSqrtRoundSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), rounding))
}

func maskSqrtRoundSs(src [4]float32, k uint8, a [4]float32, b [4]float32, rounding int) [4]float32


// MaskzSqrtRoundSs: Compute the square root of the lower single-precision
// (32-bit) floating-point element in 'a', store the result in the lower
// element of 'dst' using zeromask 'k' (the element is zeroed out when mask bit
// 0 is not set), and copy the upper 3 packed elements from 'b' to the upper
// elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := SQRT(a[31:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSQRTSS'. Intrinsic: '_mm_maskz_sqrt_round_ss'.
// Requires AVX512F.
func MaskzSqrtRoundSs(k x86.Mmask8, a x86.M128, b x86.M128, rounding int) x86.M128 {
	return x86.M128(maskzSqrtRoundSs(uint8(k), [4]float32(a), [4]float32(b), rounding))
}

func maskzSqrtRoundSs(k uint8, a [4]float32, b [4]float32, rounding int) [4]float32


// SqrtRoundSs: Compute the square root of the lower single-precision (32-bit)
// floating-point element in 'a', store the result in the lower element of
// 'dst', and copy the upper 3 packed elements from 'b' to the upper elements
// of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := SQRT(a[31:0])
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSQRTSS'. Intrinsic: '_mm_sqrt_round_ss'.
// Requires AVX512F.
func SqrtRoundSs(a x86.M128, b x86.M128, rounding int) x86.M128 {
	return x86.M128(sqrtRoundSs([4]float32(a), [4]float32(b), rounding))
}

func sqrtRoundSs(a [4]float32, b [4]float32, rounding int) [4]float32


// MaskSqrtSd: Compute the square root of the lower double-precision (64-bit)
// floating-point element in 'a', store the result in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'src' when mask bit 0
// is not set), and copy the upper element from 'b' to the upper element of
// 'dst'. 
//
//		IF k[0]
//			dst[63:0] := SQRT(a[63:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSQRTSD'. Intrinsic: '_mm_mask_sqrt_sd'.
// Requires AVX512F.
func MaskSqrtSd(src x86.M128d, k x86.Mmask8, a x86.M128d, b x86.M128d) x86.M128d {
	return x86.M128d(maskSqrtSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskSqrtSd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzSqrtSd: Compute the square root of the lower double-precision (64-bit)
// floating-point element in 'a', store the result in the lower element of
// 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is not
// set), and copy the upper element from 'b' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := SQRT(a[63:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSQRTSD'. Intrinsic: '_mm_maskz_sqrt_sd'.
// Requires AVX512F.
func MaskzSqrtSd(k x86.Mmask8, a x86.M128d, b x86.M128d) x86.M128d {
	return x86.M128d(maskzSqrtSd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzSqrtSd(k uint8, a [2]float64, b [2]float64) [2]float64


// MaskSqrtSs: Compute the square root of the lower single-precision (32-bit)
// floating-point element in 'a', store the result in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'src' when mask bit 0
// is not set), and copy the upper 3 packed elements from 'b' to the upper
// elements of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := SQRT(a[31:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSQRTSS'. Intrinsic: '_mm_mask_sqrt_ss'.
// Requires AVX512F.
func MaskSqrtSs(src x86.M128, k x86.Mmask8, a x86.M128, b x86.M128) x86.M128 {
	return x86.M128(maskSqrtSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskSqrtSs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzSqrtSs: Compute the square root of the lower single-precision (32-bit)
// floating-point element in 'a', store the result in the lower element of
// 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is not
// set), and copy the upper 3 packed elements from 'b' to the upper elements of
// 'dst'. 
//
//		IF k[0]
//			dst[31:0] := SQRT(a[31:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSQRTSS'. Intrinsic: '_mm_maskz_sqrt_ss'.
// Requires AVX512F.
func MaskzSqrtSs(k x86.Mmask8, a x86.M128, b x86.M128) x86.M128 {
	return x86.M128(maskzSqrtSs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzSqrtSs(k uint8, a [4]float32, b [4]float32) [4]float32


// MaskSraEpi32: Shift packed 32-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := SignBit
//				ELSE
//					dst[i+31:i] := SignExtend(a[i+31:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAD'. Intrinsic: '_mm_mask_sra_epi32'.
// Requires AVX512F.
func MaskSraEpi32(src x86.M128i, k x86.Mmask8, a x86.M128i, count x86.M128i) x86.M128i {
	return x86.M128i(maskSraEpi32([16]byte(src), uint8(k), [16]byte(a), [16]byte(count)))
}

func maskSraEpi32(src [16]byte, k uint8, a [16]byte, count [16]byte) [16]byte


// MaskzSraEpi32: Shift packed 32-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := SignBit
//				ELSE
//					dst[i+31:i] := SignExtend(a[i+31:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAD'. Intrinsic: '_mm_maskz_sra_epi32'.
// Requires AVX512F.
func MaskzSraEpi32(k x86.Mmask8, a x86.M128i, count x86.M128i) x86.M128i {
	return x86.M128i(maskzSraEpi32(uint8(k), [16]byte(a), [16]byte(count)))
}

func maskzSraEpi32(k uint8, a [16]byte, count [16]byte) [16]byte


// M256MaskSraEpi32: Shift packed 32-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := SignBit
//				ELSE
//					dst[i+31:i] := SignExtend(a[i+31:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAD'. Intrinsic: '_mm256_mask_sra_epi32'.
// Requires AVX512F.
func M256MaskSraEpi32(src x86.M256i, k x86.Mmask8, a x86.M256i, count x86.M128i) x86.M256i {
	return x86.M256i(m256MaskSraEpi32([32]byte(src), uint8(k), [32]byte(a), [16]byte(count)))
}

func m256MaskSraEpi32(src [32]byte, k uint8, a [32]byte, count [16]byte) [32]byte


// M256MaskzSraEpi32: Shift packed 32-bit integers in 'a' right by 'count'
// while shifting in sign bits, and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := SignBit
//				ELSE
//					dst[i+31:i] := SignExtend(a[i+31:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAD'. Intrinsic: '_mm256_maskz_sra_epi32'.
// Requires AVX512F.
func M256MaskzSraEpi32(k x86.Mmask8, a x86.M256i, count x86.M128i) x86.M256i {
	return x86.M256i(m256MaskzSraEpi32(uint8(k), [32]byte(a), [16]byte(count)))
}

func m256MaskzSraEpi32(k uint8, a [32]byte, count [16]byte) [32]byte


// M512MaskSraEpi32: Shift packed 32-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := SignBit
//				ELSE
//					dst[i+31:i] := SignExtend(a[i+31:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAD'. Intrinsic: '_mm512_mask_sra_epi32'.
// Requires AVX512F.
func M512MaskSraEpi32(src x86.M512i, k x86.Mmask16, a x86.M512i, count x86.M128i) x86.M512i {
	return x86.M512i(m512MaskSraEpi32([64]byte(src), uint16(k), [64]byte(a), [16]byte(count)))
}

func m512MaskSraEpi32(src [64]byte, k uint16, a [64]byte, count [16]byte) [64]byte


// M512MaskzSraEpi32: Shift packed 32-bit integers in 'a' right by 'count'
// while shifting in sign bits, and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := SignBit
//				ELSE
//					dst[i+31:i] := SignExtend(a[i+31:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAD'. Intrinsic: '_mm512_maskz_sra_epi32'.
// Requires AVX512F.
func M512MaskzSraEpi32(k x86.Mmask16, a x86.M512i, count x86.M128i) x86.M512i {
	return x86.M512i(m512MaskzSraEpi32(uint16(k), [64]byte(a), [16]byte(count)))
}

func m512MaskzSraEpi32(k uint16, a [64]byte, count [16]byte) [64]byte


// M512SraEpi32: Shift packed 32-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF count[63:0] > 31
//				dst[i+31:i] := SignBit
//			ELSE
//				dst[i+31:i] := SignExtend(a[i+31:i] >> count[63:0])
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAD'. Intrinsic: '_mm512_sra_epi32'.
// Requires AVX512F.
func M512SraEpi32(a x86.M512i, count x86.M128i) x86.M512i {
	return x86.M512i(m512SraEpi32([64]byte(a), [16]byte(count)))
}

func m512SraEpi32(a [64]byte, count [16]byte) [64]byte


// MaskSraEpi64: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := SignBit
//				ELSE
//					dst[i+63:i] := SignExtend(a[i+63:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm_mask_sra_epi64'.
// Requires AVX512F.
func MaskSraEpi64(src x86.M128i, k x86.Mmask8, a x86.M128i, count x86.M128i) x86.M128i {
	return x86.M128i(maskSraEpi64([16]byte(src), uint8(k), [16]byte(a), [16]byte(count)))
}

func maskSraEpi64(src [16]byte, k uint8, a [16]byte, count [16]byte) [16]byte


// MaskzSraEpi64: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := SignBit
//				ELSE
//					dst[i+63:i] := SignExtend(a[i+63:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm_maskz_sra_epi64'.
// Requires AVX512F.
func MaskzSraEpi64(k x86.Mmask8, a x86.M128i, count x86.M128i) x86.M128i {
	return x86.M128i(maskzSraEpi64(uint8(k), [16]byte(a), [16]byte(count)))
}

func maskzSraEpi64(k uint8, a [16]byte, count [16]byte) [16]byte


// SraEpi64: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF count[63:0] > 63
//				dst[i+63:i] := SignBit
//			ELSE
//				dst[i+63:i] := SignExtend(a[i+63:i] >> count[63:0])
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm_sra_epi64'.
// Requires AVX512F.
func SraEpi64(a x86.M128i, count x86.M128i) x86.M128i {
	return x86.M128i(sraEpi64([16]byte(a), [16]byte(count)))
}

func sraEpi64(a [16]byte, count [16]byte) [16]byte


// M256MaskSraEpi64: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := SignBit
//				ELSE
//					dst[i+63:i] := SignExtend(a[i+63:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm256_mask_sra_epi64'.
// Requires AVX512F.
func M256MaskSraEpi64(src x86.M256i, k x86.Mmask8, a x86.M256i, count x86.M128i) x86.M256i {
	return x86.M256i(m256MaskSraEpi64([32]byte(src), uint8(k), [32]byte(a), [16]byte(count)))
}

func m256MaskSraEpi64(src [32]byte, k uint8, a [32]byte, count [16]byte) [32]byte


// M256MaskzSraEpi64: Shift packed 64-bit integers in 'a' right by 'count'
// while shifting in sign bits, and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := SignBit
//				ELSE
//					dst[i+63:i] := SignExtend(a[i+63:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm256_maskz_sra_epi64'.
// Requires AVX512F.
func M256MaskzSraEpi64(k x86.Mmask8, a x86.M256i, count x86.M128i) x86.M256i {
	return x86.M256i(m256MaskzSraEpi64(uint8(k), [32]byte(a), [16]byte(count)))
}

func m256MaskzSraEpi64(k uint8, a [32]byte, count [16]byte) [32]byte


// M256SraEpi64: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF count[63:0] > 63
//				dst[i+63:i] := SignBit
//			ELSE
//				dst[i+63:i] := SignExtend(a[i+63:i] >> count[63:0])
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm256_sra_epi64'.
// Requires AVX512F.
func M256SraEpi64(a x86.M256i, count x86.M128i) x86.M256i {
	return x86.M256i(m256SraEpi64([32]byte(a), [16]byte(count)))
}

func m256SraEpi64(a [32]byte, count [16]byte) [32]byte


// M512MaskSraEpi64: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := SignBit
//				ELSE
//					dst[i+63:i] := SignExtend(a[i+63:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm512_mask_sra_epi64'.
// Requires AVX512F.
func M512MaskSraEpi64(src x86.M512i, k x86.Mmask8, a x86.M512i, count x86.M128i) x86.M512i {
	return x86.M512i(m512MaskSraEpi64([64]byte(src), uint8(k), [64]byte(a), [16]byte(count)))
}

func m512MaskSraEpi64(src [64]byte, k uint8, a [64]byte, count [16]byte) [64]byte


// M512MaskzSraEpi64: Shift packed 64-bit integers in 'a' right by 'count'
// while shifting in sign bits, and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := SignBit
//				ELSE
//					dst[i+63:i] := SignExtend(a[i+63:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm512_maskz_sra_epi64'.
// Requires AVX512F.
func M512MaskzSraEpi64(k x86.Mmask8, a x86.M512i, count x86.M128i) x86.M512i {
	return x86.M512i(m512MaskzSraEpi64(uint8(k), [64]byte(a), [16]byte(count)))
}

func m512MaskzSraEpi64(k uint8, a [64]byte, count [16]byte) [64]byte


// M512SraEpi64: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF count[63:0] > 63
//				dst[i+63:i] := SignBit
//			ELSE
//				dst[i+63:i] := SignExtend(a[i+63:i] >> count[63:0])
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm512_sra_epi64'.
// Requires AVX512F.
func M512SraEpi64(a x86.M512i, count x86.M128i) x86.M512i {
	return x86.M512i(m512SraEpi64([64]byte(a), [16]byte(count)))
}

func m512SraEpi64(a [64]byte, count [16]byte) [64]byte


// MaskSraiEpi32: Shift packed 32-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := SignBit
//				ELSE
//					dst[i+31:i] := SignExtend(a[i+31:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAD'. Intrinsic: '_mm_mask_srai_epi32'.
// Requires AVX512F.
func MaskSraiEpi32(src x86.M128i, k x86.Mmask8, a x86.M128i, imm8 uint32) x86.M128i {
	return x86.M128i(maskSraiEpi32([16]byte(src), uint8(k), [16]byte(a), imm8))
}

func maskSraiEpi32(src [16]byte, k uint8, a [16]byte, imm8 uint32) [16]byte


// MaskzSraiEpi32: Shift packed 32-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := SignBit
//				ELSE
//					dst[i+31:i] := SignExtend(a[i+31:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAD'. Intrinsic: '_mm_maskz_srai_epi32'.
// Requires AVX512F.
func MaskzSraiEpi32(k x86.Mmask8, a x86.M128i, imm8 uint32) x86.M128i {
	return x86.M128i(maskzSraiEpi32(uint8(k), [16]byte(a), imm8))
}

func maskzSraiEpi32(k uint8, a [16]byte, imm8 uint32) [16]byte


// M256MaskSraiEpi32: Shift packed 32-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := SignBit
//				ELSE
//					dst[i+31:i] := SignExtend(a[i+31:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAD'. Intrinsic: '_mm256_mask_srai_epi32'.
// Requires AVX512F.
func M256MaskSraiEpi32(src x86.M256i, k x86.Mmask8, a x86.M256i, imm8 uint32) x86.M256i {
	return x86.M256i(m256MaskSraiEpi32([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func m256MaskSraiEpi32(src [32]byte, k uint8, a [32]byte, imm8 uint32) [32]byte


// M256MaskzSraiEpi32: Shift packed 32-bit integers in 'a' right by 'imm8'
// while shifting in sign bits, and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := SignBit
//				ELSE
//					dst[i+31:i] := SignExtend(a[i+31:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAD'. Intrinsic: '_mm256_maskz_srai_epi32'.
// Requires AVX512F.
func M256MaskzSraiEpi32(k x86.Mmask8, a x86.M256i, imm8 uint32) x86.M256i {
	return x86.M256i(m256MaskzSraiEpi32(uint8(k), [32]byte(a), imm8))
}

func m256MaskzSraiEpi32(k uint8, a [32]byte, imm8 uint32) [32]byte


// M512MaskzSraiEpi32: Shift packed 32-bit integers in 'a' right by 'imm8'
// while shifting in sign bits, and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := SignBit
//				ELSE
//					dst[i+31:i] := SignExtend(a[i+31:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAD'. Intrinsic: '_mm512_maskz_srai_epi32'.
// Requires AVX512F.
func M512MaskzSraiEpi32(k x86.Mmask16, a x86.M512i, imm8 uint32) x86.M512i {
	return x86.M512i(m512MaskzSraiEpi32(uint16(k), [64]byte(a), imm8))
}

func m512MaskzSraiEpi32(k uint16, a [64]byte, imm8 uint32) [64]byte


// MaskSraiEpi64: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := SignBit
//				ELSE
//					dst[i+63:i] := SignExtend(a[i+63:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm_mask_srai_epi64'.
// Requires AVX512F.
func MaskSraiEpi64(src x86.M128i, k x86.Mmask8, a x86.M128i, imm8 uint32) x86.M128i {
	return x86.M128i(maskSraiEpi64([16]byte(src), uint8(k), [16]byte(a), imm8))
}

func maskSraiEpi64(src [16]byte, k uint8, a [16]byte, imm8 uint32) [16]byte


// MaskzSraiEpi64: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := SignBit
//				ELSE
//					dst[i+63:i] := SignExtend(a[i+63:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm_maskz_srai_epi64'.
// Requires AVX512F.
func MaskzSraiEpi64(k x86.Mmask8, a x86.M128i, imm8 uint32) x86.M128i {
	return x86.M128i(maskzSraiEpi64(uint8(k), [16]byte(a), imm8))
}

func maskzSraiEpi64(k uint8, a [16]byte, imm8 uint32) [16]byte


// SraiEpi64: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF imm8[7:0] > 63
//				dst[i+63:i] := SignBit
//			ELSE
//				dst[i+63:i] := SignExtend(a[i+63:i] << imm8[7:0])
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm_srai_epi64'.
// Requires AVX512F.
func SraiEpi64(a x86.M128i, imm8 uint32) x86.M128i {
	return x86.M128i(sraiEpi64([16]byte(a), imm8))
}

func sraiEpi64(a [16]byte, imm8 uint32) [16]byte


// M256MaskSraiEpi64: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := SignBit
//				ELSE
//					dst[i+63:i] := SignExtend(a[i+63:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm256_mask_srai_epi64'.
// Requires AVX512F.
func M256MaskSraiEpi64(src x86.M256i, k x86.Mmask8, a x86.M256i, imm8 uint32) x86.M256i {
	return x86.M256i(m256MaskSraiEpi64([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func m256MaskSraiEpi64(src [32]byte, k uint8, a [32]byte, imm8 uint32) [32]byte


// M256MaskzSraiEpi64: Shift packed 64-bit integers in 'a' right by 'imm8'
// while shifting in sign bits, and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := SignBit
//				ELSE
//					dst[i+63:i] := SignExtend(a[i+63:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm256_maskz_srai_epi64'.
// Requires AVX512F.
func M256MaskzSraiEpi64(k x86.Mmask8, a x86.M256i, imm8 uint32) x86.M256i {
	return x86.M256i(m256MaskzSraiEpi64(uint8(k), [32]byte(a), imm8))
}

func m256MaskzSraiEpi64(k uint8, a [32]byte, imm8 uint32) [32]byte


// M256SraiEpi64: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF imm8[7:0] > 63
//				dst[i+63:i] := SignBit
//			ELSE
//				dst[i+63:i] := SignExtend(a[i+63:i] << imm8[7:0])
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm256_srai_epi64'.
// Requires AVX512F.
func M256SraiEpi64(a x86.M256i, imm8 uint32) x86.M256i {
	return x86.M256i(m256SraiEpi64([32]byte(a), imm8))
}

func m256SraiEpi64(a [32]byte, imm8 uint32) [32]byte


// M512MaskSraiEpi64: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := SignBit
//				ELSE
//					dst[i+63:i] := SignExtend(a[i+63:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm512_mask_srai_epi64'.
// Requires AVX512F.
func M512MaskSraiEpi64(src x86.M512i, k x86.Mmask8, a x86.M512i, imm8 uint32) x86.M512i {
	return x86.M512i(m512MaskSraiEpi64([64]byte(src), uint8(k), [64]byte(a), imm8))
}

func m512MaskSraiEpi64(src [64]byte, k uint8, a [64]byte, imm8 uint32) [64]byte


// M512MaskzSraiEpi64: Shift packed 64-bit integers in 'a' right by 'imm8'
// while shifting in sign bits, and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := SignBit
//				ELSE
//					dst[i+63:i] := SignExtend(a[i+63:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm512_maskz_srai_epi64'.
// Requires AVX512F.
func M512MaskzSraiEpi64(k x86.Mmask8, a x86.M512i, imm8 uint32) x86.M512i {
	return x86.M512i(m512MaskzSraiEpi64(uint8(k), [64]byte(a), imm8))
}

func m512MaskzSraiEpi64(k uint8, a [64]byte, imm8 uint32) [64]byte


// M512SraiEpi64: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF imm8[7:0] > 63
//				dst[i+63:i] := SignBit
//			ELSE
//				dst[i+63:i] := SignExtend(a[i+63:i] << imm8[7:0])
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm512_srai_epi64'.
// Requires AVX512F.
func M512SraiEpi64(a x86.M512i, imm8 uint32) x86.M512i {
	return x86.M512i(m512SraiEpi64([64]byte(a), imm8))
}

func m512SraiEpi64(a [64]byte, imm8 uint32) [64]byte


// MaskSravEpi32: Shift packed 32-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[i+31:i] >> count[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAVD'. Intrinsic: '_mm_mask_srav_epi32'.
// Requires AVX512F.
func MaskSravEpi32(src x86.M128i, k x86.Mmask8, a x86.M128i, count x86.M128i) x86.M128i {
	return x86.M128i(maskSravEpi32([16]byte(src), uint8(k), [16]byte(a), [16]byte(count)))
}

func maskSravEpi32(src [16]byte, k uint8, a [16]byte, count [16]byte) [16]byte


// MaskzSravEpi32: Shift packed 32-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[i+31:i] >> count[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAVD'. Intrinsic: '_mm_maskz_srav_epi32'.
// Requires AVX512F.
func MaskzSravEpi32(k x86.Mmask8, a x86.M128i, count x86.M128i) x86.M128i {
	return x86.M128i(maskzSravEpi32(uint8(k), [16]byte(a), [16]byte(count)))
}

func maskzSravEpi32(k uint8, a [16]byte, count [16]byte) [16]byte


// M256MaskSravEpi32: Shift packed 32-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[i+31:i] >> count[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAVD'. Intrinsic: '_mm256_mask_srav_epi32'.
// Requires AVX512F.
func M256MaskSravEpi32(src x86.M256i, k x86.Mmask8, a x86.M256i, count x86.M256i) x86.M256i {
	return x86.M256i(m256MaskSravEpi32([32]byte(src), uint8(k), [32]byte(a), [32]byte(count)))
}

func m256MaskSravEpi32(src [32]byte, k uint8, a [32]byte, count [32]byte) [32]byte


// M256MaskzSravEpi32: Shift packed 32-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[i+31:i] >> count[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAVD'. Intrinsic: '_mm256_maskz_srav_epi32'.
// Requires AVX512F.
func M256MaskzSravEpi32(k x86.Mmask8, a x86.M256i, count x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzSravEpi32(uint8(k), [32]byte(a), [32]byte(count)))
}

func m256MaskzSravEpi32(k uint8, a [32]byte, count [32]byte) [32]byte


// M512MaskzSravEpi32: Shift packed 32-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[i+31:i] >> count[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAVD'. Intrinsic: '_mm512_maskz_srav_epi32'.
// Requires AVX512F.
func M512MaskzSravEpi32(k x86.Mmask16, a x86.M512i, count x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzSravEpi32(uint16(k), [64]byte(a), [64]byte(count)))
}

func m512MaskzSravEpi32(k uint16, a [64]byte, count [64]byte) [64]byte


// MaskSravEpi64: Shift packed 64-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[i+63:i] >> count[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAVQ'. Intrinsic: '_mm_mask_srav_epi64'.
// Requires AVX512F.
func MaskSravEpi64(src x86.M128i, k x86.Mmask8, a x86.M128i, count x86.M128i) x86.M128i {
	return x86.M128i(maskSravEpi64([16]byte(src), uint8(k), [16]byte(a), [16]byte(count)))
}

func maskSravEpi64(src [16]byte, k uint8, a [16]byte, count [16]byte) [16]byte


// MaskzSravEpi64: Shift packed 64-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[i+63:i] >> count[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAVQ'. Intrinsic: '_mm_maskz_srav_epi64'.
// Requires AVX512F.
func MaskzSravEpi64(k x86.Mmask8, a x86.M128i, count x86.M128i) x86.M128i {
	return x86.M128i(maskzSravEpi64(uint8(k), [16]byte(a), [16]byte(count)))
}

func maskzSravEpi64(k uint8, a [16]byte, count [16]byte) [16]byte


// SravEpi64: Shift packed 64-bit integers in 'a' right by the amount specified
// by the corresponding element in 'count' while shifting in sign bits, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := SignExtend(a[i+63:i] >> count[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAVQ'. Intrinsic: '_mm_srav_epi64'.
// Requires AVX512F.
func SravEpi64(a x86.M128i, count x86.M128i) x86.M128i {
	return x86.M128i(sravEpi64([16]byte(a), [16]byte(count)))
}

func sravEpi64(a [16]byte, count [16]byte) [16]byte


// M256MaskSravEpi64: Shift packed 64-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[i+63:i] >> count[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAVQ'. Intrinsic: '_mm256_mask_srav_epi64'.
// Requires AVX512F.
func M256MaskSravEpi64(src x86.M256i, k x86.Mmask8, a x86.M256i, count x86.M256i) x86.M256i {
	return x86.M256i(m256MaskSravEpi64([32]byte(src), uint8(k), [32]byte(a), [32]byte(count)))
}

func m256MaskSravEpi64(src [32]byte, k uint8, a [32]byte, count [32]byte) [32]byte


// M256MaskzSravEpi64: Shift packed 64-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[i+63:i] >> count[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAVQ'. Intrinsic: '_mm256_maskz_srav_epi64'.
// Requires AVX512F.
func M256MaskzSravEpi64(k x86.Mmask8, a x86.M256i, count x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzSravEpi64(uint8(k), [32]byte(a), [32]byte(count)))
}

func m256MaskzSravEpi64(k uint8, a [32]byte, count [32]byte) [32]byte


// M256SravEpi64: Shift packed 64-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := SignExtend(a[i+63:i] >> count[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAVQ'. Intrinsic: '_mm256_srav_epi64'.
// Requires AVX512F.
func M256SravEpi64(a x86.M256i, count x86.M256i) x86.M256i {
	return x86.M256i(m256SravEpi64([32]byte(a), [32]byte(count)))
}

func m256SravEpi64(a [32]byte, count [32]byte) [32]byte


// M512MaskSravEpi64: Shift packed 64-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[i+63:i] >> count[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAVQ'. Intrinsic: '_mm512_mask_srav_epi64'.
// Requires AVX512F.
func M512MaskSravEpi64(src x86.M512i, k x86.Mmask8, a x86.M512i, count x86.M512i) x86.M512i {
	return x86.M512i(m512MaskSravEpi64([64]byte(src), uint8(k), [64]byte(a), [64]byte(count)))
}

func m512MaskSravEpi64(src [64]byte, k uint8, a [64]byte, count [64]byte) [64]byte


// M512MaskzSravEpi64: Shift packed 64-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[i+63:i] >> count[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAVQ'. Intrinsic: '_mm512_maskz_srav_epi64'.
// Requires AVX512F.
func M512MaskzSravEpi64(k x86.Mmask8, a x86.M512i, count x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzSravEpi64(uint8(k), [64]byte(a), [64]byte(count)))
}

func m512MaskzSravEpi64(k uint8, a [64]byte, count [64]byte) [64]byte


// M512SravEpi64: Shift packed 64-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := SignExtend(a[i+63:i] >> count[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAVQ'. Intrinsic: '_mm512_srav_epi64'.
// Requires AVX512F.
func M512SravEpi64(a x86.M512i, count x86.M512i) x86.M512i {
	return x86.M512i(m512SravEpi64([64]byte(a), [64]byte(count)))
}

func m512SravEpi64(a [64]byte, count [64]byte) [64]byte


// MaskSrlEpi32: Shift packed 32-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRLD'. Intrinsic: '_mm_mask_srl_epi32'.
// Requires AVX512F.
func MaskSrlEpi32(src x86.M128i, k x86.Mmask8, a x86.M128i, count x86.M128i) x86.M128i {
	return x86.M128i(maskSrlEpi32([16]byte(src), uint8(k), [16]byte(a), [16]byte(count)))
}

func maskSrlEpi32(src [16]byte, k uint8, a [16]byte, count [16]byte) [16]byte


// MaskzSrlEpi32: Shift packed 32-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRLD'. Intrinsic: '_mm_maskz_srl_epi32'.
// Requires AVX512F.
func MaskzSrlEpi32(k x86.Mmask8, a x86.M128i, count x86.M128i) x86.M128i {
	return x86.M128i(maskzSrlEpi32(uint8(k), [16]byte(a), [16]byte(count)))
}

func maskzSrlEpi32(k uint8, a [16]byte, count [16]byte) [16]byte


// M256MaskSrlEpi32: Shift packed 32-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLD'. Intrinsic: '_mm256_mask_srl_epi32'.
// Requires AVX512F.
func M256MaskSrlEpi32(src x86.M256i, k x86.Mmask8, a x86.M256i, count x86.M128i) x86.M256i {
	return x86.M256i(m256MaskSrlEpi32([32]byte(src), uint8(k), [32]byte(a), [16]byte(count)))
}

func m256MaskSrlEpi32(src [32]byte, k uint8, a [32]byte, count [16]byte) [32]byte


// M256MaskzSrlEpi32: Shift packed 32-bit integers in 'a' right by 'count'
// while shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLD'. Intrinsic: '_mm256_maskz_srl_epi32'.
// Requires AVX512F.
func M256MaskzSrlEpi32(k x86.Mmask8, a x86.M256i, count x86.M128i) x86.M256i {
	return x86.M256i(m256MaskzSrlEpi32(uint8(k), [32]byte(a), [16]byte(count)))
}

func m256MaskzSrlEpi32(k uint8, a [32]byte, count [16]byte) [32]byte


// M512MaskSrlEpi32: Shift packed 32-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLD'. Intrinsic: '_mm512_mask_srl_epi32'.
// Requires AVX512F.
func M512MaskSrlEpi32(src x86.M512i, k x86.Mmask16, a x86.M512i, count x86.M128i) x86.M512i {
	return x86.M512i(m512MaskSrlEpi32([64]byte(src), uint16(k), [64]byte(a), [16]byte(count)))
}

func m512MaskSrlEpi32(src [64]byte, k uint16, a [64]byte, count [16]byte) [64]byte


// M512MaskzSrlEpi32: Shift packed 32-bit integers in 'a' right by 'count'
// while shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLD'. Intrinsic: '_mm512_maskz_srl_epi32'.
// Requires AVX512F.
func M512MaskzSrlEpi32(k x86.Mmask16, a x86.M512i, count x86.M128i) x86.M512i {
	return x86.M512i(m512MaskzSrlEpi32(uint16(k), [64]byte(a), [16]byte(count)))
}

func m512MaskzSrlEpi32(k uint16, a [64]byte, count [16]byte) [64]byte


// M512SrlEpi32: Shift packed 32-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF count[63:0] > 31
//				dst[i+31:i] := 0
//			ELSE
//				dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[63:0])
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLD'. Intrinsic: '_mm512_srl_epi32'.
// Requires AVX512F.
func M512SrlEpi32(a x86.M512i, count x86.M128i) x86.M512i {
	return x86.M512i(m512SrlEpi32([64]byte(a), [16]byte(count)))
}

func m512SrlEpi32(a [64]byte, count [16]byte) [64]byte


// MaskSrlEpi64: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm_mask_srl_epi64'.
// Requires AVX512F.
func MaskSrlEpi64(src x86.M128i, k x86.Mmask8, a x86.M128i, count x86.M128i) x86.M128i {
	return x86.M128i(maskSrlEpi64([16]byte(src), uint8(k), [16]byte(a), [16]byte(count)))
}

func maskSrlEpi64(src [16]byte, k uint8, a [16]byte, count [16]byte) [16]byte


// MaskzSrlEpi64: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm_maskz_srl_epi64'.
// Requires AVX512F.
func MaskzSrlEpi64(k x86.Mmask8, a x86.M128i, count x86.M128i) x86.M128i {
	return x86.M128i(maskzSrlEpi64(uint8(k), [16]byte(a), [16]byte(count)))
}

func maskzSrlEpi64(k uint8, a [16]byte, count [16]byte) [16]byte


// M256MaskSrlEpi64: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm256_mask_srl_epi64'.
// Requires AVX512F.
func M256MaskSrlEpi64(src x86.M256i, k x86.Mmask8, a x86.M256i, count x86.M128i) x86.M256i {
	return x86.M256i(m256MaskSrlEpi64([32]byte(src), uint8(k), [32]byte(a), [16]byte(count)))
}

func m256MaskSrlEpi64(src [32]byte, k uint8, a [32]byte, count [16]byte) [32]byte


// M256MaskzSrlEpi64: Shift packed 64-bit integers in 'a' right by 'count'
// while shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm256_maskz_srl_epi64'.
// Requires AVX512F.
func M256MaskzSrlEpi64(k x86.Mmask8, a x86.M256i, count x86.M128i) x86.M256i {
	return x86.M256i(m256MaskzSrlEpi64(uint8(k), [32]byte(a), [16]byte(count)))
}

func m256MaskzSrlEpi64(k uint8, a [32]byte, count [16]byte) [32]byte


// M512MaskSrlEpi64: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm512_mask_srl_epi64'.
// Requires AVX512F.
func M512MaskSrlEpi64(src x86.M512i, k x86.Mmask8, a x86.M512i, count x86.M128i) x86.M512i {
	return x86.M512i(m512MaskSrlEpi64([64]byte(src), uint8(k), [64]byte(a), [16]byte(count)))
}

func m512MaskSrlEpi64(src [64]byte, k uint8, a [64]byte, count [16]byte) [64]byte


// M512MaskzSrlEpi64: Shift packed 64-bit integers in 'a' right by 'count'
// while shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm512_maskz_srl_epi64'.
// Requires AVX512F.
func M512MaskzSrlEpi64(k x86.Mmask8, a x86.M512i, count x86.M128i) x86.M512i {
	return x86.M512i(m512MaskzSrlEpi64(uint8(k), [64]byte(a), [16]byte(count)))
}

func m512MaskzSrlEpi64(k uint8, a [64]byte, count [16]byte) [64]byte


// M512SrlEpi64: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF count[63:0] > 63
//				dst[i+63:i] := 0
//			ELSE
//				dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[63:0])
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm512_srl_epi64'.
// Requires AVX512F.
func M512SrlEpi64(a x86.M512i, count x86.M128i) x86.M512i {
	return x86.M512i(m512SrlEpi64([64]byte(a), [16]byte(count)))
}

func m512SrlEpi64(a [64]byte, count [16]byte) [64]byte


// MaskSrliEpi32: Shift packed 32-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRLD'. Intrinsic: '_mm_mask_srli_epi32'.
// Requires AVX512F.
func MaskSrliEpi32(src x86.M128i, k x86.Mmask8, a x86.M128i, imm8 uint32) x86.M128i {
	return x86.M128i(maskSrliEpi32([16]byte(src), uint8(k), [16]byte(a), imm8))
}

func maskSrliEpi32(src [16]byte, k uint8, a [16]byte, imm8 uint32) [16]byte


// MaskzSrliEpi32: Shift packed 32-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRLD'. Intrinsic: '_mm_maskz_srli_epi32'.
// Requires AVX512F.
func MaskzSrliEpi32(k x86.Mmask8, a x86.M128i, imm8 uint32) x86.M128i {
	return x86.M128i(maskzSrliEpi32(uint8(k), [16]byte(a), imm8))
}

func maskzSrliEpi32(k uint8, a [16]byte, imm8 uint32) [16]byte


// M256MaskSrliEpi32: Shift packed 32-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLD'. Intrinsic: '_mm256_mask_srli_epi32'.
// Requires AVX512F.
func M256MaskSrliEpi32(src x86.M256i, k x86.Mmask8, a x86.M256i, imm8 uint32) x86.M256i {
	return x86.M256i(m256MaskSrliEpi32([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func m256MaskSrliEpi32(src [32]byte, k uint8, a [32]byte, imm8 uint32) [32]byte


// M256MaskzSrliEpi32: Shift packed 32-bit integers in 'a' right by 'imm8'
// while shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLD'. Intrinsic: '_mm256_maskz_srli_epi32'.
// Requires AVX512F.
func M256MaskzSrliEpi32(k x86.Mmask8, a x86.M256i, imm8 uint32) x86.M256i {
	return x86.M256i(m256MaskzSrliEpi32(uint8(k), [32]byte(a), imm8))
}

func m256MaskzSrliEpi32(k uint8, a [32]byte, imm8 uint32) [32]byte


// M512MaskzSrliEpi32: Shift packed 32-bit integers in 'a' right by 'imm8'
// while shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLD'. Intrinsic: '_mm512_maskz_srli_epi32'.
// Requires AVX512F.
func M512MaskzSrliEpi32(k x86.Mmask16, a x86.M512i, imm8 uint32) x86.M512i {
	return x86.M512i(m512MaskzSrliEpi32(uint16(k), [64]byte(a), imm8))
}

func m512MaskzSrliEpi32(k uint16, a [64]byte, imm8 uint32) [64]byte


// MaskSrliEpi64: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm_mask_srli_epi64'.
// Requires AVX512F.
func MaskSrliEpi64(src x86.M128i, k x86.Mmask8, a x86.M128i, imm8 uint32) x86.M128i {
	return x86.M128i(maskSrliEpi64([16]byte(src), uint8(k), [16]byte(a), imm8))
}

func maskSrliEpi64(src [16]byte, k uint8, a [16]byte, imm8 uint32) [16]byte


// MaskzSrliEpi64: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm_maskz_srli_epi64'.
// Requires AVX512F.
func MaskzSrliEpi64(k x86.Mmask8, a x86.M128i, imm8 uint32) x86.M128i {
	return x86.M128i(maskzSrliEpi64(uint8(k), [16]byte(a), imm8))
}

func maskzSrliEpi64(k uint8, a [16]byte, imm8 uint32) [16]byte


// M256MaskSrliEpi64: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm256_mask_srli_epi64'.
// Requires AVX512F.
func M256MaskSrliEpi64(src x86.M256i, k x86.Mmask8, a x86.M256i, imm8 uint32) x86.M256i {
	return x86.M256i(m256MaskSrliEpi64([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func m256MaskSrliEpi64(src [32]byte, k uint8, a [32]byte, imm8 uint32) [32]byte


// M256MaskzSrliEpi64: Shift packed 64-bit integers in 'a' right by 'imm8'
// while shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm256_maskz_srli_epi64'.
// Requires AVX512F.
func M256MaskzSrliEpi64(k x86.Mmask8, a x86.M256i, imm8 uint32) x86.M256i {
	return x86.M256i(m256MaskzSrliEpi64(uint8(k), [32]byte(a), imm8))
}

func m256MaskzSrliEpi64(k uint8, a [32]byte, imm8 uint32) [32]byte


// M512MaskSrliEpi64: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm512_mask_srli_epi64'.
// Requires AVX512F.
func M512MaskSrliEpi64(src x86.M512i, k x86.Mmask8, a x86.M512i, imm8 uint32) x86.M512i {
	return x86.M512i(m512MaskSrliEpi64([64]byte(src), uint8(k), [64]byte(a), imm8))
}

func m512MaskSrliEpi64(src [64]byte, k uint8, a [64]byte, imm8 uint32) [64]byte


// M512MaskzSrliEpi64: Shift packed 64-bit integers in 'a' right by 'imm8'
// while shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm512_maskz_srli_epi64'.
// Requires AVX512F.
func M512MaskzSrliEpi64(k x86.Mmask8, a x86.M512i, imm8 uint32) x86.M512i {
	return x86.M512i(m512MaskzSrliEpi64(uint8(k), [64]byte(a), imm8))
}

func m512MaskzSrliEpi64(k uint8, a [64]byte, imm8 uint32) [64]byte


// M512SrliEpi64: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF imm8[7:0] > 63
//				dst[i+63:i] := 0
//			ELSE
//				dst[i+63:i] := ZeroExtend(a[i+63:i] >> imm8[7:0])
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm512_srli_epi64'.
// Requires AVX512F.
func M512SrliEpi64(a x86.M512i, imm8 uint32) x86.M512i {
	return x86.M512i(m512SrliEpi64([64]byte(a), imm8))
}

func m512SrliEpi64(a [64]byte, imm8 uint32) [64]byte


// MaskSrlvEpi32: Shift packed 32-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRLVD'. Intrinsic: '_mm_mask_srlv_epi32'.
// Requires AVX512F.
func MaskSrlvEpi32(src x86.M128i, k x86.Mmask8, a x86.M128i, count x86.M128i) x86.M128i {
	return x86.M128i(maskSrlvEpi32([16]byte(src), uint8(k), [16]byte(a), [16]byte(count)))
}

func maskSrlvEpi32(src [16]byte, k uint8, a [16]byte, count [16]byte) [16]byte


// MaskzSrlvEpi32: Shift packed 32-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRLVD'. Intrinsic: '_mm_maskz_srlv_epi32'.
// Requires AVX512F.
func MaskzSrlvEpi32(k x86.Mmask8, a x86.M128i, count x86.M128i) x86.M128i {
	return x86.M128i(maskzSrlvEpi32(uint8(k), [16]byte(a), [16]byte(count)))
}

func maskzSrlvEpi32(k uint8, a [16]byte, count [16]byte) [16]byte


// M256MaskSrlvEpi32: Shift packed 32-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLVD'. Intrinsic: '_mm256_mask_srlv_epi32'.
// Requires AVX512F.
func M256MaskSrlvEpi32(src x86.M256i, k x86.Mmask8, a x86.M256i, count x86.M256i) x86.M256i {
	return x86.M256i(m256MaskSrlvEpi32([32]byte(src), uint8(k), [32]byte(a), [32]byte(count)))
}

func m256MaskSrlvEpi32(src [32]byte, k uint8, a [32]byte, count [32]byte) [32]byte


// M256MaskzSrlvEpi32: Shift packed 32-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLVD'. Intrinsic: '_mm256_maskz_srlv_epi32'.
// Requires AVX512F.
func M256MaskzSrlvEpi32(k x86.Mmask8, a x86.M256i, count x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzSrlvEpi32(uint8(k), [32]byte(a), [32]byte(count)))
}

func m256MaskzSrlvEpi32(k uint8, a [32]byte, count [32]byte) [32]byte


// M512MaskzSrlvEpi32: Shift packed 32-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLVD'. Intrinsic: '_mm512_maskz_srlv_epi32'.
// Requires AVX512F.
func M512MaskzSrlvEpi32(k x86.Mmask16, a x86.M512i, count x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzSrlvEpi32(uint16(k), [64]byte(a), [64]byte(count)))
}

func m512MaskzSrlvEpi32(k uint16, a [64]byte, count [64]byte) [64]byte


// MaskSrlvEpi64: Shift packed 64-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRLVQ'. Intrinsic: '_mm_mask_srlv_epi64'.
// Requires AVX512F.
func MaskSrlvEpi64(src x86.M128i, k x86.Mmask8, a x86.M128i, count x86.M128i) x86.M128i {
	return x86.M128i(maskSrlvEpi64([16]byte(src), uint8(k), [16]byte(a), [16]byte(count)))
}

func maskSrlvEpi64(src [16]byte, k uint8, a [16]byte, count [16]byte) [16]byte


// MaskzSrlvEpi64: Shift packed 64-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRLVQ'. Intrinsic: '_mm_maskz_srlv_epi64'.
// Requires AVX512F.
func MaskzSrlvEpi64(k x86.Mmask8, a x86.M128i, count x86.M128i) x86.M128i {
	return x86.M128i(maskzSrlvEpi64(uint8(k), [16]byte(a), [16]byte(count)))
}

func maskzSrlvEpi64(k uint8, a [16]byte, count [16]byte) [16]byte


// M256MaskSrlvEpi64: Shift packed 64-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLVQ'. Intrinsic: '_mm256_mask_srlv_epi64'.
// Requires AVX512F.
func M256MaskSrlvEpi64(src x86.M256i, k x86.Mmask8, a x86.M256i, count x86.M256i) x86.M256i {
	return x86.M256i(m256MaskSrlvEpi64([32]byte(src), uint8(k), [32]byte(a), [32]byte(count)))
}

func m256MaskSrlvEpi64(src [32]byte, k uint8, a [32]byte, count [32]byte) [32]byte


// M256MaskzSrlvEpi64: Shift packed 64-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLVQ'. Intrinsic: '_mm256_maskz_srlv_epi64'.
// Requires AVX512F.
func M256MaskzSrlvEpi64(k x86.Mmask8, a x86.M256i, count x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzSrlvEpi64(uint8(k), [32]byte(a), [32]byte(count)))
}

func m256MaskzSrlvEpi64(k uint8, a [32]byte, count [32]byte) [32]byte


// M512MaskSrlvEpi64: Shift packed 64-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLVQ'. Intrinsic: '_mm512_mask_srlv_epi64'.
// Requires AVX512F.
func M512MaskSrlvEpi64(src x86.M512i, k x86.Mmask8, a x86.M512i, count x86.M512i) x86.M512i {
	return x86.M512i(m512MaskSrlvEpi64([64]byte(src), uint8(k), [64]byte(a), [64]byte(count)))
}

func m512MaskSrlvEpi64(src [64]byte, k uint8, a [64]byte, count [64]byte) [64]byte


// M512MaskzSrlvEpi64: Shift packed 64-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLVQ'. Intrinsic: '_mm512_maskz_srlv_epi64'.
// Requires AVX512F.
func M512MaskzSrlvEpi64(k x86.Mmask8, a x86.M512i, count x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzSrlvEpi64(uint8(k), [64]byte(a), [64]byte(count)))
}

func m512MaskzSrlvEpi64(k uint8, a [64]byte, count [64]byte) [64]byte


// M512SrlvEpi64: Shift packed 64-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLVQ'. Intrinsic: '_mm512_srlv_epi64'.
// Requires AVX512F.
func M512SrlvEpi64(a x86.M512i, count x86.M512i) x86.M512i {
	return x86.M512i(m512SrlvEpi64([64]byte(a), [64]byte(count)))
}

func m512SrlvEpi64(a [64]byte, count [64]byte) [64]byte


// Skipped: _mm_mask_store_epi32. Contains pointer parameter.


// Skipped: _mm256_mask_store_epi32. Contains pointer parameter.


// Skipped: _mm_mask_store_epi64. Contains pointer parameter.


// Skipped: _mm256_mask_store_epi64. Contains pointer parameter.


// Skipped: _mm_mask_store_pd. Contains pointer parameter.


// Skipped: _mm256_mask_store_pd. Contains pointer parameter.


// Skipped: _mm_mask_store_ps. Contains pointer parameter.


// Skipped: _mm256_mask_store_ps. Contains pointer parameter.


// MaskStoreSd: Store the lower double-precision (64-bit) floating-point
// element from 'a' into memory using writemask 'k'.
// 	'mem_addr' must be aligned on a 16-byte boundary or a general-protection
// exception may be generated. 
//
//		IF k[0]
//			MEM[mem_addr+63:mem_addr] := a[63:0]
//		FI
//
// Instruction: 'VMOVSD'. Intrinsic: '_mm_mask_store_sd'.
// Requires AVX512F.
// FIXME: Will likely need to be reworked.
func MaskStoreSd(mem_addr *float64, k x86.Mmask8, a x86.M128d)  {
	// FIXME: Rework to avoid possible return value as parameter.

}

// MaskStoreSs: Store the lower single-precision (32-bit) floating-point
// element from 'a' into memory using writemask 'k'.
// 	'mem_addr' must be aligned on a 16-byte boundary or a general-protection
// exception may be generated. 
//
//		IF k[0]
//			MEM[mem_addr+31:mem_addr] := a[31:0]
//		FI
//
// Instruction: 'VMOVSS'. Intrinsic: '_mm_mask_store_ss'.
// Requires AVX512F.
// FIXME: Will likely need to be reworked.
func MaskStoreSs(mem_addr *float32, k x86.Mmask8, a x86.M128)  {
	// FIXME: Rework to avoid possible return value as parameter.

}

// Skipped: _mm_mask_storeu_epi32. Contains pointer parameter.


// Skipped: _mm256_mask_storeu_epi32. Contains pointer parameter.


// Skipped: _mm512_mask_storeu_epi32. Contains pointer parameter.


// Skipped: _mm_mask_storeu_epi64. Contains pointer parameter.


// Skipped: _mm256_mask_storeu_epi64. Contains pointer parameter.


// Skipped: _mm512_mask_storeu_epi64. Contains pointer parameter.


// Skipped: _mm_mask_storeu_pd. Contains pointer parameter.


// Skipped: _mm256_mask_storeu_pd. Contains pointer parameter.


// Skipped: _mm512_mask_storeu_pd. Contains pointer parameter.


// Skipped: _mm512_storeu_pd. Contains pointer parameter.


// Skipped: _mm_mask_storeu_ps. Contains pointer parameter.


// Skipped: _mm256_mask_storeu_ps. Contains pointer parameter.


// Skipped: _mm512_mask_storeu_ps. Contains pointer parameter.


// Skipped: _mm512_storeu_ps. Contains pointer parameter.


// Skipped: _mm512_storeu_si512. Contains pointer parameter.


// Skipped: _mm512_stream_load_si512. Contains pointer parameter.


// Skipped: _mm512_stream_pd. Contains pointer parameter.


// Skipped: _mm512_stream_ps. Contains pointer parameter.


// Skipped: _mm512_stream_si512. Contains pointer parameter.


// MaskSubEpi32: Subtract packed 32-bit integers in 'b' from packed 32-bit
// integers in 'a', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] - b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSUBD'. Intrinsic: '_mm_mask_sub_epi32'.
// Requires AVX512F.
func MaskSubEpi32(src x86.M128i, k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskSubEpi32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskSubEpi32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzSubEpi32: Subtract packed 32-bit integers in 'b' from packed 32-bit
// integers in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] - b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSUBD'. Intrinsic: '_mm_maskz_sub_epi32'.
// Requires AVX512F.
func MaskzSubEpi32(k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskzSubEpi32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzSubEpi32(k uint8, a [16]byte, b [16]byte) [16]byte


// M256MaskSubEpi32: Subtract packed 32-bit integers in 'b' from packed 32-bit
// integers in 'a', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] - b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBD'. Intrinsic: '_mm256_mask_sub_epi32'.
// Requires AVX512F.
func M256MaskSubEpi32(src x86.M256i, k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskSubEpi32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskSubEpi32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// M256MaskzSubEpi32: Subtract packed 32-bit integers in 'b' from packed 32-bit
// integers in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] - b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBD'. Intrinsic: '_mm256_maskz_sub_epi32'.
// Requires AVX512F.
func M256MaskzSubEpi32(k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzSubEpi32(uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskzSubEpi32(k uint8, a [32]byte, b [32]byte) [32]byte


// M512MaskzSubEpi32: Subtract packed 32-bit integers in 'b' from packed 32-bit
// integers in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] - b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSUBD'. Intrinsic: '_mm512_maskz_sub_epi32'.
// Requires AVX512F.
func M512MaskzSubEpi32(k x86.Mmask16, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzSubEpi32(uint16(k), [64]byte(a), [64]byte(b)))
}

func m512MaskzSubEpi32(k uint16, a [64]byte, b [64]byte) [64]byte


// MaskSubEpi64: Subtract packed 64-bit integers in 'b' from packed 64-bit
// integers in 'a', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] - b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSUBQ'. Intrinsic: '_mm_mask_sub_epi64'.
// Requires AVX512F.
func MaskSubEpi64(src x86.M128i, k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskSubEpi64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskSubEpi64(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzSubEpi64: Subtract packed 64-bit integers in 'b' from packed 64-bit
// integers in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] - b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSUBQ'. Intrinsic: '_mm_maskz_sub_epi64'.
// Requires AVX512F.
func MaskzSubEpi64(k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskzSubEpi64(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzSubEpi64(k uint8, a [16]byte, b [16]byte) [16]byte


// M256MaskSubEpi64: Subtract packed 64-bit integers in 'b' from packed 64-bit
// integers in 'a', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] - b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBQ'. Intrinsic: '_mm256_mask_sub_epi64'.
// Requires AVX512F.
func M256MaskSubEpi64(src x86.M256i, k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskSubEpi64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskSubEpi64(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// M256MaskzSubEpi64: Subtract packed 64-bit integers in 'b' from packed 64-bit
// integers in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] - b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBQ'. Intrinsic: '_mm256_maskz_sub_epi64'.
// Requires AVX512F.
func M256MaskzSubEpi64(k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzSubEpi64(uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskzSubEpi64(k uint8, a [32]byte, b [32]byte) [32]byte


// M512MaskSubEpi64: Subtract packed 64-bit integers in 'b' from packed 64-bit
// integers in 'a', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] - b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSUBQ'. Intrinsic: '_mm512_mask_sub_epi64'.
// Requires AVX512F.
func M512MaskSubEpi64(src x86.M512i, k x86.Mmask8, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskSubEpi64([64]byte(src), uint8(k), [64]byte(a), [64]byte(b)))
}

func m512MaskSubEpi64(src [64]byte, k uint8, a [64]byte, b [64]byte) [64]byte


// M512MaskzSubEpi64: Subtract packed 64-bit integers in 'b' from packed 64-bit
// integers in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] - b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSUBQ'. Intrinsic: '_mm512_maskz_sub_epi64'.
// Requires AVX512F.
func M512MaskzSubEpi64(k x86.Mmask8, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzSubEpi64(uint8(k), [64]byte(a), [64]byte(b)))
}

func m512MaskzSubEpi64(k uint8, a [64]byte, b [64]byte) [64]byte


// M512SubEpi64: Subtract packed 64-bit integers in 'b' from packed 64-bit
// integers in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := a[i+63:i] - b[i+63:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSUBQ'. Intrinsic: '_mm512_sub_epi64'.
// Requires AVX512F.
func M512SubEpi64(a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512SubEpi64([64]byte(a), [64]byte(b)))
}

func m512SubEpi64(a [64]byte, b [64]byte) [64]byte


// MaskSubPd: Subtract packed double-precision (64-bit) floating-point elements
// in 'b' from packed double-precision (64-bit) floating-point elements in 'a',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] - b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSUBPD'. Intrinsic: '_mm_mask_sub_pd'.
// Requires AVX512F.
func MaskSubPd(src x86.M128d, k x86.Mmask8, a x86.M128d, b x86.M128d) x86.M128d {
	return x86.M128d(maskSubPd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskSubPd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzSubPd: Subtract packed double-precision (64-bit) floating-point
// elements in 'b' from packed double-precision (64-bit) floating-point
// elements in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] - b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSUBPD'. Intrinsic: '_mm_maskz_sub_pd'.
// Requires AVX512F.
func MaskzSubPd(k x86.Mmask8, a x86.M128d, b x86.M128d) x86.M128d {
	return x86.M128d(maskzSubPd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzSubPd(k uint8, a [2]float64, b [2]float64) [2]float64


// M256MaskSubPd: Subtract packed double-precision (64-bit) floating-point
// elements in 'b' from packed double-precision (64-bit) floating-point
// elements in 'a', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] - b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSUBPD'. Intrinsic: '_mm256_mask_sub_pd'.
// Requires AVX512F.
func M256MaskSubPd(src x86.M256d, k x86.Mmask8, a x86.M256d, b x86.M256d) x86.M256d {
	return x86.M256d(m256MaskSubPd([4]float64(src), uint8(k), [4]float64(a), [4]float64(b)))
}

func m256MaskSubPd(src [4]float64, k uint8, a [4]float64, b [4]float64) [4]float64


// M256MaskzSubPd: Subtract packed double-precision (64-bit) floating-point
// elements in 'b' from packed double-precision (64-bit) floating-point
// elements in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] - b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSUBPD'. Intrinsic: '_mm256_maskz_sub_pd'.
// Requires AVX512F.
func M256MaskzSubPd(k x86.Mmask8, a x86.M256d, b x86.M256d) x86.M256d {
	return x86.M256d(m256MaskzSubPd(uint8(k), [4]float64(a), [4]float64(b)))
}

func m256MaskzSubPd(k uint8, a [4]float64, b [4]float64) [4]float64


// M512MaskzSubPd: Subtract packed double-precision (64-bit) floating-point
// elements in 'b' from packed double-precision (64-bit) floating-point
// elements in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] - b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSUBPD'. Intrinsic: '_mm512_maskz_sub_pd'.
// Requires AVX512F.
func M512MaskzSubPd(k x86.Mmask8, a x86.M512d, b x86.M512d) x86.M512d {
	return x86.M512d(m512MaskzSubPd(uint8(k), [8]float64(a), [8]float64(b)))
}

func m512MaskzSubPd(k uint8, a [8]float64, b [8]float64) [8]float64


// MaskSubPs: Subtract packed single-precision (32-bit) floating-point elements
// in 'b' from packed single-precision (32-bit) floating-point elements in 'a',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] - b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSUBPS'. Intrinsic: '_mm_mask_sub_ps'.
// Requires AVX512F.
func MaskSubPs(src x86.M128, k x86.Mmask8, a x86.M128, b x86.M128) x86.M128 {
	return x86.M128(maskSubPs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskSubPs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzSubPs: Subtract packed single-precision (32-bit) floating-point
// elements in 'b' from packed single-precision (32-bit) floating-point
// elements in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] - b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSUBPS'. Intrinsic: '_mm_maskz_sub_ps'.
// Requires AVX512F.
func MaskzSubPs(k x86.Mmask8, a x86.M128, b x86.M128) x86.M128 {
	return x86.M128(maskzSubPs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzSubPs(k uint8, a [4]float32, b [4]float32) [4]float32


// M256MaskSubPs: Subtract packed single-precision (32-bit) floating-point
// elements in 'b' from packed single-precision (32-bit) floating-point
// elements in 'a', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] - b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSUBPS'. Intrinsic: '_mm256_mask_sub_ps'.
// Requires AVX512F.
func M256MaskSubPs(src x86.M256, k x86.Mmask8, a x86.M256, b x86.M256) x86.M256 {
	return x86.M256(m256MaskSubPs([8]float32(src), uint8(k), [8]float32(a), [8]float32(b)))
}

func m256MaskSubPs(src [8]float32, k uint8, a [8]float32, b [8]float32) [8]float32


// M256MaskzSubPs: Subtract packed single-precision (32-bit) floating-point
// elements in 'b' from packed single-precision (32-bit) floating-point
// elements in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] - b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSUBPS'. Intrinsic: '_mm256_maskz_sub_ps'.
// Requires AVX512F.
func M256MaskzSubPs(k x86.Mmask8, a x86.M256, b x86.M256) x86.M256 {
	return x86.M256(m256MaskzSubPs(uint8(k), [8]float32(a), [8]float32(b)))
}

func m256MaskzSubPs(k uint8, a [8]float32, b [8]float32) [8]float32


// M512MaskzSubPs: Subtract packed single-precision (32-bit) floating-point
// elements in 'b' from packed single-precision (32-bit) floating-point
// elements in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] - b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSUBPS'. Intrinsic: '_mm512_maskz_sub_ps'.
// Requires AVX512F.
func M512MaskzSubPs(k x86.Mmask16, a x86.M512, b x86.M512) x86.M512 {
	return x86.M512(m512MaskzSubPs(uint16(k), [16]float32(a), [16]float32(b)))
}

func m512MaskzSubPs(k uint16, a [16]float32, b [16]float32) [16]float32


// M512MaskzSubRoundPd: Subtract packed double-precision (64-bit)
// floating-point elements in 'b' from packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] - b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSUBPD'. Intrinsic: '_mm512_maskz_sub_round_pd'.
// Requires AVX512F.
func M512MaskzSubRoundPd(k x86.Mmask8, a x86.M512d, b x86.M512d, rounding int) x86.M512d {
	return x86.M512d(m512MaskzSubRoundPd(uint8(k), [8]float64(a), [8]float64(b), rounding))
}

func m512MaskzSubRoundPd(k uint8, a [8]float64, b [8]float64, rounding int) [8]float64


// M512MaskzSubRoundPs: Subtract packed single-precision (32-bit)
// floating-point elements in 'b' from packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] - b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSUBPS'. Intrinsic: '_mm512_maskz_sub_round_ps'.
// Requires AVX512F.
func M512MaskzSubRoundPs(k x86.Mmask16, a x86.M512, b x86.M512, rounding int) x86.M512 {
	return x86.M512(m512MaskzSubRoundPs(uint16(k), [16]float32(a), [16]float32(b), rounding))
}

func m512MaskzSubRoundPs(k uint16, a [16]float32, b [16]float32, rounding int) [16]float32


// MaskSubRoundSd: Subtract the lower double-precision (64-bit) floating-point
// element in 'b' from the lower double-precision (64-bit) floating-point
// element in 'a', store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'src' when mask bit 0 is not set),
// and copy the upper element from 'a' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := a[63:0] - b[63:0]
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSUBSD'. Intrinsic: '_mm_mask_sub_round_sd'.
// Requires AVX512F.
func MaskSubRoundSd(src x86.M128d, k x86.Mmask8, a x86.M128d, b x86.M128d, rounding int) x86.M128d {
	return x86.M128d(maskSubRoundSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), rounding))
}

func maskSubRoundSd(src [2]float64, k uint8, a [2]float64, b [2]float64, rounding int) [2]float64


// MaskzSubRoundSd: Subtract the lower double-precision (64-bit) floating-point
// element in 'b' from the lower double-precision (64-bit) floating-point
// element in 'a', store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper element from 'a' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := a[63:0] - b[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSUBSD'. Intrinsic: '_mm_maskz_sub_round_sd'.
// Requires AVX512F.
func MaskzSubRoundSd(k x86.Mmask8, a x86.M128d, b x86.M128d, rounding int) x86.M128d {
	return x86.M128d(maskzSubRoundSd(uint8(k), [2]float64(a), [2]float64(b), rounding))
}

func maskzSubRoundSd(k uint8, a [2]float64, b [2]float64, rounding int) [2]float64


// SubRoundSd: Subtract the lower double-precision (64-bit) floating-point
// element in 'b' from the lower double-precision (64-bit) floating-point
// element in 'a', store the result in the lower element of 'dst', and copy the
// upper element from 'a' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := a[63:0] - b[63:0]
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSUBSD'. Intrinsic: '_mm_sub_round_sd'.
// Requires AVX512F.
func SubRoundSd(a x86.M128d, b x86.M128d, rounding int) x86.M128d {
	return x86.M128d(subRoundSd([2]float64(a), [2]float64(b), rounding))
}

func subRoundSd(a [2]float64, b [2]float64, rounding int) [2]float64


// MaskSubRoundSs: Subtract the lower single-precision (32-bit) floating-point
// element in 'b' from the lower single-precision (32-bit) floating-point
// element in 'a', store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'src' when mask bit 0 is not set),
// and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := a[31:0] - b[31:0]
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSUBSS'. Intrinsic: '_mm_mask_sub_round_ss'.
// Requires AVX512F.
func MaskSubRoundSs(src x86.M128, k x86.Mmask8, a x86.M128, b x86.M128, rounding int) x86.M128 {
	return x86.M128(maskSubRoundSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), rounding))
}

func maskSubRoundSs(src [4]float32, k uint8, a [4]float32, b [4]float32, rounding int) [4]float32


// MaskzSubRoundSs: Subtract the lower single-precision (32-bit) floating-point
// element in 'b' from the lower single-precision (32-bit) floating-point
// element in 'a', store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := a[31:0] - b[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSUBSS'. Intrinsic: '_mm_maskz_sub_round_ss'.
// Requires AVX512F.
func MaskzSubRoundSs(k x86.Mmask8, a x86.M128, b x86.M128, rounding int) x86.M128 {
	return x86.M128(maskzSubRoundSs(uint8(k), [4]float32(a), [4]float32(b), rounding))
}

func maskzSubRoundSs(k uint8, a [4]float32, b [4]float32, rounding int) [4]float32


// SubRoundSs: Subtract the lower single-precision (32-bit) floating-point
// element in 'b' from the lower single-precision (32-bit) floating-point
// element in 'a', store the result in the lower element of 'dst', and copy the
// upper 3 packed elements from 'a' to the upper elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := a[31:0] - b[31:0]
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSUBSS'. Intrinsic: '_mm_sub_round_ss'.
// Requires AVX512F.
func SubRoundSs(a x86.M128, b x86.M128, rounding int) x86.M128 {
	return x86.M128(subRoundSs([4]float32(a), [4]float32(b), rounding))
}

func subRoundSs(a [4]float32, b [4]float32, rounding int) [4]float32


// MaskSubSd: Subtract the lower double-precision (64-bit) floating-point
// element in 'b' from the lower double-precision (64-bit) floating-point
// element in 'a', store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'src' when mask bit 0 is not set),
// and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := a[63:0] - b[63:0]
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSUBSD'. Intrinsic: '_mm_mask_sub_sd'.
// Requires AVX512F.
func MaskSubSd(src x86.M128d, k x86.Mmask8, a x86.M128d, b x86.M128d) x86.M128d {
	return x86.M128d(maskSubSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskSubSd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzSubSd: Subtract the lower double-precision (64-bit) floating-point
// element in 'b' from the lower double-precision (64-bit) floating-point
// element in 'a', store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := a[63:0] - b[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSUBSD'. Intrinsic: '_mm_maskz_sub_sd'.
// Requires AVX512F.
func MaskzSubSd(k x86.Mmask8, a x86.M128d, b x86.M128d) x86.M128d {
	return x86.M128d(maskzSubSd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzSubSd(k uint8, a [2]float64, b [2]float64) [2]float64


// MaskSubSs: Subtract the lower single-precision (32-bit) floating-point
// element in 'b' from the lower single-precision (32-bit) floating-point
// element in 'a', store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'src' when mask bit 0 is not set),
// and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'. 
//
//		IF k[0]
//			dst[31:0] := a[31:0] - b[31:0]
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSUBSS'. Intrinsic: '_mm_mask_sub_ss'.
// Requires AVX512F.
func MaskSubSs(src x86.M128, k x86.Mmask8, a x86.M128, b x86.M128) x86.M128 {
	return x86.M128(maskSubSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskSubSs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzSubSs: Subtract the lower single-precision (32-bit) floating-point
// element in 'b' from the lower single-precision (32-bit) floating-point
// element in 'a', store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := a[31:0] - b[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSUBSS'. Intrinsic: '_mm_maskz_sub_ss'.
// Requires AVX512F.
func MaskzSubSs(k x86.Mmask8, a x86.M128, b x86.M128) x86.M128 {
	return x86.M128(maskzSubSs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzSubSs(k uint8, a [4]float32, b [4]float32) [4]float32


// M512MaskSvmlRoundPd: Round the packed double-precision (64-bit)
// floating-point elements in 'a' to the nearest integer value, and store the
// results as packed double-precision floating-point elements in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ROUND(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i] 
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_svml_round_pd'.
// Requires AVX512F.
func M512MaskSvmlRoundPd(src x86.M512d, k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskSvmlRoundPd([8]float64(src), uint8(k), [8]float64(a)))
}

func m512MaskSvmlRoundPd(src [8]float64, k uint8, a [8]float64) [8]float64


// M512SvmlRoundPd: Round the packed double-precision (64-bit) floating-point
// elements in 'a' to the nearest integer value, and store the results as
// packed double-precision floating-point elements in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ROUND(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_svml_round_pd'.
// Requires AVX512F.
func M512SvmlRoundPd(a x86.M512d) x86.M512d {
	return x86.M512d(m512SvmlRoundPd([8]float64(a)))
}

func m512SvmlRoundPd(a [8]float64) [8]float64


// M512MaskTanPd: Compute the tangent of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := TAN(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_tan_pd'.
// Requires AVX512F.
func M512MaskTanPd(src x86.M512d, k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskTanPd([8]float64(src), uint8(k), [8]float64(a)))
}

func m512MaskTanPd(src [8]float64, k uint8, a [8]float64) [8]float64


// M512TanPd: Compute the tangent of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := TAN(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_tan_pd'.
// Requires AVX512F.
func M512TanPd(a x86.M512d) x86.M512d {
	return x86.M512d(m512TanPd([8]float64(a)))
}

func m512TanPd(a [8]float64) [8]float64


// M512MaskTanPs: Compute the tangent of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := TAN(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_tan_ps'.
// Requires AVX512F.
func M512MaskTanPs(src x86.M512, k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskTanPs([16]float32(src), uint16(k), [16]float32(a)))
}

func m512MaskTanPs(src [16]float32, k uint16, a [16]float32) [16]float32


// M512TanPs: Compute the tangent of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := TAN(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_tan_ps'.
// Requires AVX512F.
func M512TanPs(a x86.M512) x86.M512 {
	return x86.M512(m512TanPs([16]float32(a)))
}

func m512TanPs(a [16]float32) [16]float32


// M512MaskTandPd: Compute the tangent of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in degrees, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := TAND(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_tand_pd'.
// Requires AVX512F.
func M512MaskTandPd(src x86.M512d, k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskTandPd([8]float64(src), uint8(k), [8]float64(a)))
}

func m512MaskTandPd(src [8]float64, k uint8, a [8]float64) [8]float64


// M512TandPd: Compute the tangent of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in degrees, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := TAND(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_tand_pd'.
// Requires AVX512F.
func M512TandPd(a x86.M512d) x86.M512d {
	return x86.M512d(m512TandPd([8]float64(a)))
}

func m512TandPd(a [8]float64) [8]float64


// M512MaskTandPs: Compute the tangent of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in degrees, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := TAND(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_tand_ps'.
// Requires AVX512F.
func M512MaskTandPs(src x86.M512, k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskTandPs([16]float32(src), uint16(k), [16]float32(a)))
}

func m512MaskTandPs(src [16]float32, k uint16, a [16]float32) [16]float32


// M512TandPs: Compute the tangent of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in degrees, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := TAND(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_tand_ps'.
// Requires AVX512F.
func M512TandPs(a x86.M512) x86.M512 {
	return x86.M512(m512TandPs([16]float32(a)))
}

func m512TandPs(a [16]float32) [16]float32


// M512MaskTanhPd: Compute the hyperbolic tangent of packed double-precision
// (64-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := TANH(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_tanh_pd'.
// Requires AVX512F.
func M512MaskTanhPd(src x86.M512d, k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskTanhPd([8]float64(src), uint8(k), [8]float64(a)))
}

func m512MaskTanhPd(src [8]float64, k uint8, a [8]float64) [8]float64


// M512TanhPd: Compute the hyperbolic tangent of packed double-precision
// (64-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := TANH(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_tanh_pd'.
// Requires AVX512F.
func M512TanhPd(a x86.M512d) x86.M512d {
	return x86.M512d(m512TanhPd([8]float64(a)))
}

func m512TanhPd(a [8]float64) [8]float64


// M512MaskTanhPs: Compute the hyperbolic tangent of packed single-precision
// (32-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := TANH(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_tanh_ps'.
// Requires AVX512F.
func M512MaskTanhPs(src x86.M512, k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskTanhPs([16]float32(src), uint16(k), [16]float32(a)))
}

func m512MaskTanhPs(src [16]float32, k uint16, a [16]float32) [16]float32


// M512TanhPs: Compute the hyperbolic tangent of packed single-precision
// (32-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := TANH(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_tanh_ps'.
// Requires AVX512F.
func M512TanhPs(a x86.M512) x86.M512 {
	return x86.M512(m512TanhPs([16]float32(a)))
}

func m512TanhPs(a [16]float32) [16]float32


// MaskTernarylogicEpi32: Bitwise ternary logic that provides the capability to
// implement any three-operand binary function; the specific binary function is
// specified by value in 'imm8'. For each bit in each packed 32-bit integer,
// the corresponding bit from 'src', 'a', and 'b' are used to form a 3 bit
// index into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst' using writemask 'k' at 32-bit granularity (32-bit
// elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				FOR h := 0 to 31
//					index[2:0] := (src[i+h] << 2) OR (a[i+h] << 1) OR b[i+h]
//					dst[i+h] := imm8[index[2:0]]
//				ENDFOR
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPTERNLOGD'. Intrinsic: '_mm_mask_ternarylogic_epi32'.
// Requires AVX512F.
func MaskTernarylogicEpi32(src x86.M128i, k x86.Mmask8, a x86.M128i, b x86.M128i, imm8 int) x86.M128i {
	return x86.M128i(maskTernarylogicEpi32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b), imm8))
}

func maskTernarylogicEpi32(src [16]byte, k uint8, a [16]byte, b [16]byte, imm8 int) [16]byte


// MaskzTernarylogicEpi32: Bitwise ternary logic that provides the capability
// to implement any three-operand binary function; the specific binary function
// is specified by value in 'imm8'. For each bit in each packed 32-bit integer,
// the corresponding bit from 'a', 'b', and 'c' are used to form a 3 bit index
// into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst' using zeromask 'k' at 32-bit granularity (32-bit
// elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				FOR h := 0 to 31
//					index[2:0] := (a[i+h] << 2) OR (b[i+h] << 1) OR c[i+h]
//					dst[i+h] := imm8[index[2:0]]
//				ENDFOR
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPTERNLOGD'. Intrinsic: '_mm_maskz_ternarylogic_epi32'.
// Requires AVX512F.
func MaskzTernarylogicEpi32(k x86.Mmask8, a x86.M128i, b x86.M128i, c x86.M128i, imm8 int) x86.M128i {
	return x86.M128i(maskzTernarylogicEpi32(uint8(k), [16]byte(a), [16]byte(b), [16]byte(c), imm8))
}

func maskzTernarylogicEpi32(k uint8, a [16]byte, b [16]byte, c [16]byte, imm8 int) [16]byte


// TernarylogicEpi32: Bitwise ternary logic that provides the capability to
// implement any three-operand binary function; the specific binary function is
// specified by value in 'imm8'. For each bit in each packed 32-bit integer,
// the corresponding bit from 'a', 'b', and 'c' are used to form a 3 bit index
// into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			FOR h := 0 to 31
//				index[2:0] := (a[i+h] << 2) OR (b[i+h] << 1) OR c[i+h]
//				dst[i+h] := imm8[index[2:0]]
//			ENDFOR
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPTERNLOGD'. Intrinsic: '_mm_ternarylogic_epi32'.
// Requires AVX512F.
func TernarylogicEpi32(a x86.M128i, b x86.M128i, c x86.M128i, imm8 int) x86.M128i {
	return x86.M128i(ternarylogicEpi32([16]byte(a), [16]byte(b), [16]byte(c), imm8))
}

func ternarylogicEpi32(a [16]byte, b [16]byte, c [16]byte, imm8 int) [16]byte


// M256MaskTernarylogicEpi32: Bitwise ternary logic that provides the
// capability to implement any three-operand binary function; the specific
// binary function is specified by value in 'imm8'. For each bit in each packed
// 32-bit integer, the corresponding bit from 'src', 'a', and 'b' are used to
// form a 3 bit index into 'imm8', and the value at that bit in 'imm8' is
// written to the corresponding bit in 'dst' using writemask 'k' at 32-bit
// granularity (32-bit elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				FOR h := 0 to 31
//					index[2:0] := (src[i+h] << 2) OR (a[i+h] << 1) OR b[i+h]
//					dst[i+h] := imm8[index[2:0]]
//				ENDFOR
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPTERNLOGD'. Intrinsic: '_mm256_mask_ternarylogic_epi32'.
// Requires AVX512F.
func M256MaskTernarylogicEpi32(src x86.M256i, k x86.Mmask8, a x86.M256i, b x86.M256i, imm8 int) x86.M256i {
	return x86.M256i(m256MaskTernarylogicEpi32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b), imm8))
}

func m256MaskTernarylogicEpi32(src [32]byte, k uint8, a [32]byte, b [32]byte, imm8 int) [32]byte


// M256MaskzTernarylogicEpi32: Bitwise ternary logic that provides the
// capability to implement any three-operand binary function; the specific
// binary function is specified by value in 'imm8'. For each bit in each packed
// 32-bit integer, the corresponding bit from 'a', 'b', and 'c' are used to
// form a 3 bit index into 'imm8', and the value at that bit in 'imm8' is
// written to the corresponding bit in 'dst' using zeromask 'k' at 32-bit
// granularity (32-bit elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				FOR h := 0 to 31
//					index[2:0] := (a[i+h] << 2) OR (b[i+h] << 1) OR c[i+h]
//					dst[i+h] := imm8[index[2:0]]
//				ENDFOR
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPTERNLOGD'. Intrinsic: '_mm256_maskz_ternarylogic_epi32'.
// Requires AVX512F.
func M256MaskzTernarylogicEpi32(k x86.Mmask8, a x86.M256i, b x86.M256i, c x86.M256i, imm8 int) x86.M256i {
	return x86.M256i(m256MaskzTernarylogicEpi32(uint8(k), [32]byte(a), [32]byte(b), [32]byte(c), imm8))
}

func m256MaskzTernarylogicEpi32(k uint8, a [32]byte, b [32]byte, c [32]byte, imm8 int) [32]byte


// M256TernarylogicEpi32: Bitwise ternary logic that provides the capability to
// implement any three-operand binary function; the specific binary function is
// specified by value in 'imm8'. For each bit in each packed 32-bit integer,
// the corresponding bit from 'a', 'b', and 'c' are used to form a 3 bit index
// into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			FOR h := 0 to 31
//				index[2:0] := (a[i+h] << 2) OR (b[i+h] << 1) OR c[i+h]
//				dst[i+h] := imm8[index[2:0]]
//			ENDFOR
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPTERNLOGD'. Intrinsic: '_mm256_ternarylogic_epi32'.
// Requires AVX512F.
func M256TernarylogicEpi32(a x86.M256i, b x86.M256i, c x86.M256i, imm8 int) x86.M256i {
	return x86.M256i(m256TernarylogicEpi32([32]byte(a), [32]byte(b), [32]byte(c), imm8))
}

func m256TernarylogicEpi32(a [32]byte, b [32]byte, c [32]byte, imm8 int) [32]byte


// M512MaskTernarylogicEpi32: Bitwise ternary logic that provides the
// capability to implement any three-operand binary function; the specific
// binary function is specified by value in 'imm8'. For each bit in each packed
// 32-bit integer, the corresponding bit from 'src', 'a', and 'b' are used to
// form a 3 bit index into 'imm8', and the value at that bit in 'imm8' is
// written to the corresponding bit in 'dst' using writemask 'k' at 32-bit
// granularity (32-bit elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				FOR h := 0 to 31
//					index[2:0] := (src[i+h] << 2) OR (a[i+h] << 1) OR b[i+h]
//					dst[i+h] := imm8[index[2:0]]
//				ENDFOR
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPTERNLOGD'. Intrinsic: '_mm512_mask_ternarylogic_epi32'.
// Requires AVX512F.
func M512MaskTernarylogicEpi32(src x86.M512i, k x86.Mmask16, a x86.M512i, b x86.M512i, imm8 int) x86.M512i {
	return x86.M512i(m512MaskTernarylogicEpi32([64]byte(src), uint16(k), [64]byte(a), [64]byte(b), imm8))
}

func m512MaskTernarylogicEpi32(src [64]byte, k uint16, a [64]byte, b [64]byte, imm8 int) [64]byte


// M512MaskzTernarylogicEpi32: Bitwise ternary logic that provides the
// capability to implement any three-operand binary function; the specific
// binary function is specified by value in 'imm8'. For each bit in each packed
// 32-bit integer, the corresponding bit from 'a', 'b', and 'c' are used to
// form a 3 bit index into 'imm8', and the value at that bit in 'imm8' is
// written to the corresponding bit in 'dst' using zeromask 'k' at 32-bit
// granularity (32-bit elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				FOR h := 0 to 31
//					index[2:0] := (a[i+h] << 2) OR (b[i+h] << 1) OR c[i+h]
//					dst[i+h] := imm8[index[2:0]]
//				ENDFOR
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPTERNLOGD'. Intrinsic: '_mm512_maskz_ternarylogic_epi32'.
// Requires AVX512F.
func M512MaskzTernarylogicEpi32(k x86.Mmask16, a x86.M512i, b x86.M512i, c x86.M512i, imm8 int) x86.M512i {
	return x86.M512i(m512MaskzTernarylogicEpi32(uint16(k), [64]byte(a), [64]byte(b), [64]byte(c), imm8))
}

func m512MaskzTernarylogicEpi32(k uint16, a [64]byte, b [64]byte, c [64]byte, imm8 int) [64]byte


// M512TernarylogicEpi32: Bitwise ternary logic that provides the capability to
// implement any three-operand binary function; the specific binary function is
// specified by value in 'imm8'. For each bit in each packed 32-bit integer,
// the corresponding bit from 'a', 'b', and 'c' are used to form a 3 bit index
// into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			FOR h := 0 to 31
//				index[2:0] := (a[i+h] << 2) OR (b[i+h] << 1) OR c[i+h]
//				dst[i+h] := imm8[index[2:0]]
//			ENDFOR
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPTERNLOGD'. Intrinsic: '_mm512_ternarylogic_epi32'.
// Requires AVX512F.
func M512TernarylogicEpi32(a x86.M512i, b x86.M512i, c x86.M512i, imm8 int) x86.M512i {
	return x86.M512i(m512TernarylogicEpi32([64]byte(a), [64]byte(b), [64]byte(c), imm8))
}

func m512TernarylogicEpi32(a [64]byte, b [64]byte, c [64]byte, imm8 int) [64]byte


// MaskTernarylogicEpi64: Bitwise ternary logic that provides the capability to
// implement any three-operand binary function; the specific binary function is
// specified by value in 'imm8'. For each bit in each packed 64-bit integer,
// the corresponding bit from 'src', 'a', and 'b' are used to form a 3 bit
// index into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst' using writemask 'k' at 64-bit granularity (64-bit
// elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				FOR h := 0 to 63
//					index[2:0] := (src[i+h] << 2) OR (a[i+h] << 1) OR b[i+h]
//					dst[i+h] := imm8[index[2:0]]
//				ENDFOR
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPTERNLOGQ'. Intrinsic: '_mm_mask_ternarylogic_epi64'.
// Requires AVX512F.
func MaskTernarylogicEpi64(src x86.M128i, k x86.Mmask8, a x86.M128i, b x86.M128i, imm8 int) x86.M128i {
	return x86.M128i(maskTernarylogicEpi64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b), imm8))
}

func maskTernarylogicEpi64(src [16]byte, k uint8, a [16]byte, b [16]byte, imm8 int) [16]byte


// MaskzTernarylogicEpi64: Bitwise ternary logic that provides the capability
// to implement any three-operand binary function; the specific binary function
// is specified by value in 'imm8'. For each bit in each packed 64-bit integer,
// the corresponding bit from 'a', 'b', and 'c' are used to form a 3 bit index
// into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst' using zeromask 'k' at 64-bit granularity (64-bit
// elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				FOR h := 0 to 63
//					index[2:0] := (a[i+h] << 2) OR (b[i+h] << 1) OR c[i+h]
//					dst[i+h] := imm8[index[2:0]]
//				ENDFOR
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPTERNLOGQ'. Intrinsic: '_mm_maskz_ternarylogic_epi64'.
// Requires AVX512F.
func MaskzTernarylogicEpi64(k x86.Mmask8, a x86.M128i, b x86.M128i, c x86.M128i, imm8 int) x86.M128i {
	return x86.M128i(maskzTernarylogicEpi64(uint8(k), [16]byte(a), [16]byte(b), [16]byte(c), imm8))
}

func maskzTernarylogicEpi64(k uint8, a [16]byte, b [16]byte, c [16]byte, imm8 int) [16]byte


// TernarylogicEpi64: Bitwise ternary logic that provides the capability to
// implement any three-operand binary function; the specific binary function is
// specified by value in 'imm8'. For each bit in each packed 64-bit integer,
// the corresponding bit from 'a', 'b', and 'c' are used to form a 3 bit index
// into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			FOR h := 0 to 63
//				index[2:0] := (a[i+h] << 2) OR (b[i+h] << 1) OR c[i+h]
//				dst[i+h] := imm8[index[2:0]]
//			ENDFOR
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPTERNLOGQ'. Intrinsic: '_mm_ternarylogic_epi64'.
// Requires AVX512F.
func TernarylogicEpi64(a x86.M128i, b x86.M128i, c x86.M128i, imm8 int) x86.M128i {
	return x86.M128i(ternarylogicEpi64([16]byte(a), [16]byte(b), [16]byte(c), imm8))
}

func ternarylogicEpi64(a [16]byte, b [16]byte, c [16]byte, imm8 int) [16]byte


// M256MaskTernarylogicEpi64: Bitwise ternary logic that provides the
// capability to implement any three-operand binary function; the specific
// binary function is specified by value in 'imm8'. For each bit in each packed
// 64-bit integer, the corresponding bit from 'src', 'a', and 'b' are used to
// form a 3 bit index into 'imm8', and the value at that bit in 'imm8' is
// written to the corresponding bit in 'dst' using writemask 'k' at 64-bit
// granularity (64-bit elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				FOR h := 0 to 63
//					index[2:0] := (src[i+h] << 2) OR (a[i+h] << 1) OR b[i+h]
//					dst[i+h] := imm8[index[2:0]]
//				ENDFOR
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPTERNLOGQ'. Intrinsic: '_mm256_mask_ternarylogic_epi64'.
// Requires AVX512F.
func M256MaskTernarylogicEpi64(src x86.M256i, k x86.Mmask8, a x86.M256i, b x86.M256i, imm8 int) x86.M256i {
	return x86.M256i(m256MaskTernarylogicEpi64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b), imm8))
}

func m256MaskTernarylogicEpi64(src [32]byte, k uint8, a [32]byte, b [32]byte, imm8 int) [32]byte


// M256MaskzTernarylogicEpi64: Bitwise ternary logic that provides the
// capability to implement any three-operand binary function; the specific
// binary function is specified by value in 'imm8'. For each bit in each packed
// 64-bit integer, the corresponding bit from 'a', 'b', and 'c' are used to
// form a 3 bit index into 'imm8', and the value at that bit in 'imm8' is
// written to the corresponding bit in 'dst' using zeromask 'k' at 64-bit
// granularity (64-bit elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				FOR h := 0 to 63
//					index[2:0] := (a[i+h] << 2) OR (b[i+h] << 1) OR c[i+h]
//					dst[i+h] := imm8[index[2:0]]
//				ENDFOR
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPTERNLOGQ'. Intrinsic: '_mm256_maskz_ternarylogic_epi64'.
// Requires AVX512F.
func M256MaskzTernarylogicEpi64(k x86.Mmask8, a x86.M256i, b x86.M256i, c x86.M256i, imm8 int) x86.M256i {
	return x86.M256i(m256MaskzTernarylogicEpi64(uint8(k), [32]byte(a), [32]byte(b), [32]byte(c), imm8))
}

func m256MaskzTernarylogicEpi64(k uint8, a [32]byte, b [32]byte, c [32]byte, imm8 int) [32]byte


// M256TernarylogicEpi64: Bitwise ternary logic that provides the capability to
// implement any three-operand binary function; the specific binary function is
// specified by value in 'imm8'. For each bit in each packed 64-bit integer,
// the corresponding bit from 'a', 'b', and 'c' are used to form a 3 bit index
// into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			FOR h := 0 to 63
//				index[2:0] := (a[i+h] << 2) OR (b[i+h] << 1) OR c[i+h]
//				dst[i+h] := imm8[index[2:0]]
//			ENDFOR
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPTERNLOGQ'. Intrinsic: '_mm256_ternarylogic_epi64'.
// Requires AVX512F.
func M256TernarylogicEpi64(a x86.M256i, b x86.M256i, c x86.M256i, imm8 int) x86.M256i {
	return x86.M256i(m256TernarylogicEpi64([32]byte(a), [32]byte(b), [32]byte(c), imm8))
}

func m256TernarylogicEpi64(a [32]byte, b [32]byte, c [32]byte, imm8 int) [32]byte


// M512MaskTernarylogicEpi64: Bitwise ternary logic that provides the
// capability to implement any three-operand binary function; the specific
// binary function is specified by value in 'imm8'. For each bit in each packed
// 64-bit integer, the corresponding bit from 'src', 'a', and 'b' are used to
// form a 3 bit index into 'imm8', and the value at that bit in 'imm8' is
// written to the corresponding bit in 'dst' using writemask 'k' at 64-bit
// granularity (64-bit elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				FOR h := 0 to 63
//					index[2:0] := (src[i+h] << 2) OR (a[i+h] << 1) OR b[i+h]
//					dst[i+h] := imm8[index[2:0]]
//				ENDFOR
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPTERNLOGQ'. Intrinsic: '_mm512_mask_ternarylogic_epi64'.
// Requires AVX512F.
func M512MaskTernarylogicEpi64(src x86.M512i, k x86.Mmask8, a x86.M512i, b x86.M512i, imm8 int) x86.M512i {
	return x86.M512i(m512MaskTernarylogicEpi64([64]byte(src), uint8(k), [64]byte(a), [64]byte(b), imm8))
}

func m512MaskTernarylogicEpi64(src [64]byte, k uint8, a [64]byte, b [64]byte, imm8 int) [64]byte


// M512MaskzTernarylogicEpi64: Bitwise ternary logic that provides the
// capability to implement any three-operand binary function; the specific
// binary function is specified by value in 'imm8'. For each bit in each packed
// 64-bit integer, the corresponding bit from 'a', 'b', and 'c' are used to
// form a 3 bit index into 'imm8', and the value at that bit in 'imm8' is
// written to the corresponding bit in 'dst' using zeromask 'k' at 64-bit
// granularity (64-bit elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				FOR h := 0 to 63
//					index[2:0] := (a[i+h] << 2) OR (b[i+h] << 1) OR c[i+h]
//					dst[i+h] := imm8[index[2:0]]
//				ENDFOR
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPTERNLOGQ'. Intrinsic: '_mm512_maskz_ternarylogic_epi64'.
// Requires AVX512F.
func M512MaskzTernarylogicEpi64(k x86.Mmask8, a x86.M512i, b x86.M512i, c x86.M512i, imm8 int) x86.M512i {
	return x86.M512i(m512MaskzTernarylogicEpi64(uint8(k), [64]byte(a), [64]byte(b), [64]byte(c), imm8))
}

func m512MaskzTernarylogicEpi64(k uint8, a [64]byte, b [64]byte, c [64]byte, imm8 int) [64]byte


// M512TernarylogicEpi64: Bitwise ternary logic that provides the capability to
// implement any three-operand binary function; the specific binary function is
// specified by value in 'imm8'. For each bit in each packed 64-bit integer,
// the corresponding bit from 'a', 'b', and 'c' are used to form a 3 bit index
// into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			FOR h := 0 to 63
//				index[2:0] := (a[i+h] << 2) OR (b[i+h] << 1) OR c[i+h]
//				dst[i+h] := imm8[index[2:0]]
//			ENDFOR
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPTERNLOGQ'. Intrinsic: '_mm512_ternarylogic_epi64'.
// Requires AVX512F.
func M512TernarylogicEpi64(a x86.M512i, b x86.M512i, c x86.M512i, imm8 int) x86.M512i {
	return x86.M512i(m512TernarylogicEpi64([64]byte(a), [64]byte(b), [64]byte(c), imm8))
}

func m512TernarylogicEpi64(a [64]byte, b [64]byte, c [64]byte, imm8 int) [64]byte


// MaskTestEpi32Mask: Compute the bitwise AND of packed 32-bit integers in 'a'
// and 'b', producing intermediate 32-bit values, and set the corresponding bit
// in result mask 'k' (subject to writemask 'k') if the intermediate value is
// non-zero. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ((a[i+31:i] AND b[i+31:i]) != 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPTESTMD'. Intrinsic: '_mm_mask_test_epi32_mask'.
// Requires AVX512F.
func MaskTestEpi32Mask(k1 x86.Mmask8, a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(maskTestEpi32Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskTestEpi32Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// TestEpi32Mask: Compute the bitwise AND of packed 32-bit integers in 'a' and
// 'b', producing intermediate 32-bit values, and set the corresponding bit in
// result mask 'k' if the intermediate value is non-zero. 
//
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ((a[i+31:i] AND b[i+31:i]) != 0) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPTESTMD'. Intrinsic: '_mm_test_epi32_mask'.
// Requires AVX512F.
func TestEpi32Mask(a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(testEpi32Mask([16]byte(a), [16]byte(b)))
}

func testEpi32Mask(a [16]byte, b [16]byte) uint8


// M256MaskTestEpi32Mask: Compute the bitwise AND of packed 32-bit integers in
// 'a' and 'b', producing intermediate 32-bit values, and set the corresponding
// bit in result mask 'k' (subject to writemask 'k') if the intermediate value
// is non-zero. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ((a[i+31:i] AND b[i+31:i]) != 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPTESTMD'. Intrinsic: '_mm256_mask_test_epi32_mask'.
// Requires AVX512F.
func M256MaskTestEpi32Mask(k1 x86.Mmask8, a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256MaskTestEpi32Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func m256MaskTestEpi32Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// M256TestEpi32Mask: Compute the bitwise AND of packed 32-bit integers in 'a'
// and 'b', producing intermediate 32-bit values, and set the corresponding bit
// in result mask 'k' if the intermediate value is non-zero. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ((a[i+31:i] AND b[i+31:i]) != 0) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPTESTMD'. Intrinsic: '_mm256_test_epi32_mask'.
// Requires AVX512F.
func M256TestEpi32Mask(a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256TestEpi32Mask([32]byte(a), [32]byte(b)))
}

func m256TestEpi32Mask(a [32]byte, b [32]byte) uint8


// MaskTestEpi64Mask: Compute the bitwise AND of packed 64-bit integers in 'a'
// and 'b', producing intermediate 64-bit values, and set the corresponding bit
// in result mask 'k' (subject to writemask 'k') if the intermediate value is
// non-zero. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ((a[i+63:i] AND b[i+63:i]) != 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPTESTMQ'. Intrinsic: '_mm_mask_test_epi64_mask'.
// Requires AVX512F.
func MaskTestEpi64Mask(k1 x86.Mmask8, a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(maskTestEpi64Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskTestEpi64Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// TestEpi64Mask: Compute the bitwise AND of packed 64-bit integers in 'a' and
// 'b', producing intermediate 64-bit values, and set the corresponding bit in
// result mask 'k' if the intermediate value is non-zero. 
//
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ((a[i+63:i] AND b[i+63:i]) != 0) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPTESTMQ'. Intrinsic: '_mm_test_epi64_mask'.
// Requires AVX512F.
func TestEpi64Mask(a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(testEpi64Mask([16]byte(a), [16]byte(b)))
}

func testEpi64Mask(a [16]byte, b [16]byte) uint8


// M256MaskTestEpi64Mask: Compute the bitwise AND of packed 64-bit integers in
// 'a' and 'b', producing intermediate 64-bit values, and set the corresponding
// bit in result mask 'k' (subject to writemask 'k') if the intermediate value
// is non-zero. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ((a[i+63:i] AND b[i+63:i]) != 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPTESTMQ'. Intrinsic: '_mm256_mask_test_epi64_mask'.
// Requires AVX512F.
func M256MaskTestEpi64Mask(k1 x86.Mmask8, a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256MaskTestEpi64Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func m256MaskTestEpi64Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// M256TestEpi64Mask: Compute the bitwise AND of packed 64-bit integers in 'a'
// and 'b', producing intermediate 64-bit values, and set the corresponding bit
// in result mask 'k' if the intermediate value is non-zero. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ((a[i+63:i] AND b[i+63:i]) != 0) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPTESTMQ'. Intrinsic: '_mm256_test_epi64_mask'.
// Requires AVX512F.
func M256TestEpi64Mask(a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256TestEpi64Mask([32]byte(a), [32]byte(b)))
}

func m256TestEpi64Mask(a [32]byte, b [32]byte) uint8


// M512MaskTestEpi64Mask: Compute the bitwise AND of packed 64-bit integers in
// 'a' and 'b', producing intermediate 64-bit values, and set the corresponding
// bit in result mask 'k' (subject to writemask 'k') if the intermediate value
// is non-zero. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ((a[i+63:i] AND b[i+63:i]) != 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPTESTMQ'. Intrinsic: '_mm512_mask_test_epi64_mask'.
// Requires AVX512F.
func M512MaskTestEpi64Mask(k1 x86.Mmask8, a x86.M512i, b x86.M512i) x86.Mmask8 {
	return x86.Mmask8(m512MaskTestEpi64Mask(uint8(k1), [64]byte(a), [64]byte(b)))
}

func m512MaskTestEpi64Mask(k1 uint8, a [64]byte, b [64]byte) uint8


// M512TestEpi64Mask: Compute the bitwise AND of packed 64-bit integers in 'a'
// and 'b', producing intermediate 64-bit values, and set the corresponding bit
// in result mask 'k' if the intermediate value is non-zero. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ((a[i+63:i] AND b[i+63:i]) != 0) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPTESTMQ'. Intrinsic: '_mm512_test_epi64_mask'.
// Requires AVX512F.
func M512TestEpi64Mask(a x86.M512i, b x86.M512i) x86.Mmask8 {
	return x86.Mmask8(m512TestEpi64Mask([64]byte(a), [64]byte(b)))
}

func m512TestEpi64Mask(a [64]byte, b [64]byte) uint8


// MaskTestnEpi32Mask: Compute the bitwise NAND of packed 32-bit integers in
// 'a' and 'b', producing intermediate 32-bit values, and set the corresponding
// bit in result mask 'k' (subject to writemask 'k') if the intermediate value
// is zero. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ((a[i+31:i] AND b[i+31:i]) == 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPTESTNMD'. Intrinsic: '_mm_mask_testn_epi32_mask'.
// Requires AVX512F.
func MaskTestnEpi32Mask(k1 x86.Mmask8, a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(maskTestnEpi32Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskTestnEpi32Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// TestnEpi32Mask: Compute the bitwise NAND of packed 32-bit integers in 'a'
// and 'b', producing intermediate 32-bit values, and set the corresponding bit
// in result mask 'k' if the intermediate value is zero. 
//
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ((a[i+31:i] NAND b[i+31:i]) == 0) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPTESTNMD'. Intrinsic: '_mm_testn_epi32_mask'.
// Requires AVX512F.
func TestnEpi32Mask(a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(testnEpi32Mask([16]byte(a), [16]byte(b)))
}

func testnEpi32Mask(a [16]byte, b [16]byte) uint8


// M256MaskTestnEpi32Mask: Compute the bitwise NAND of packed 32-bit integers
// in 'a' and 'b', producing intermediate 32-bit values, and set the
// corresponding bit in result mask 'k' (subject to writemask 'k') if the
// intermediate value is zero. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ((a[i+31:i] AND b[i+31:i]) == 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPTESTNMD'. Intrinsic: '_mm256_mask_testn_epi32_mask'.
// Requires AVX512F.
func M256MaskTestnEpi32Mask(k1 x86.Mmask8, a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256MaskTestnEpi32Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func m256MaskTestnEpi32Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// M256TestnEpi32Mask: Compute the bitwise NAND of packed 32-bit integers in
// 'a' and 'b', producing intermediate 32-bit values, and set the corresponding
// bit in result mask 'k' if the intermediate value is zero. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ((a[i+31:i] AND b[i+31:i]) == 0) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPTESTNMD'. Intrinsic: '_mm256_testn_epi32_mask'.
// Requires AVX512F.
func M256TestnEpi32Mask(a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256TestnEpi32Mask([32]byte(a), [32]byte(b)))
}

func m256TestnEpi32Mask(a [32]byte, b [32]byte) uint8


// M512MaskTestnEpi32Mask: Compute the bitwise NAND of packed 32-bit integers
// in 'a' and 'b', producing intermediate 32-bit values, and set the
// corresponding bit in result mask 'k' (subject to writemask 'k') if the
// intermediate value is zero. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k1[j]
//				k[j] := ((a[i+31:i] AND b[i+31:i]) == 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPTESTNMD'. Intrinsic: '_mm512_mask_testn_epi32_mask'.
// Requires AVX512F.
func M512MaskTestnEpi32Mask(k1 x86.Mmask16, a x86.M512i, b x86.M512i) x86.Mmask16 {
	return x86.Mmask16(m512MaskTestnEpi32Mask(uint16(k1), [64]byte(a), [64]byte(b)))
}

func m512MaskTestnEpi32Mask(k1 uint16, a [64]byte, b [64]byte) uint16


// M512TestnEpi32Mask: Compute the bitwise NAND of packed 32-bit integers in
// 'a' and 'b', producing intermediate 32-bit values, and set the corresponding
// bit in result mask 'k' if the intermediate value is zero. 
//
//		FOR j := 0 to 15
//			i := j*32
//			k[j] := ((a[i+31:i] AND b[i+31:i]) == 0) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPTESTNMD'. Intrinsic: '_mm512_testn_epi32_mask'.
// Requires AVX512F.
func M512TestnEpi32Mask(a x86.M512i, b x86.M512i) x86.Mmask16 {
	return x86.Mmask16(m512TestnEpi32Mask([64]byte(a), [64]byte(b)))
}

func m512TestnEpi32Mask(a [64]byte, b [64]byte) uint16


// MaskTestnEpi64Mask: Compute the bitwise NAND of packed 64-bit integers in
// 'a' and 'b', producing intermediate 64-bit values, and set the corresponding
// bit in result mask 'k' (subject to writemask 'k') if the intermediate value
// is zero. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ((a[i+63:i] AND b[i+63:i]) == 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPTESTNMQ'. Intrinsic: '_mm_mask_testn_epi64_mask'.
// Requires AVX512F.
func MaskTestnEpi64Mask(k1 x86.Mmask8, a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(maskTestnEpi64Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskTestnEpi64Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// TestnEpi64Mask: Compute the bitwise NAND of packed 64-bit integers in 'a'
// and 'b', producing intermediate 64-bit values, and set the corresponding bit
// in result mask 'k' if the intermediate value is zero. 
//
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ((a[i+63:i] AND b[i+63:i]) == 0) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPTESTNMQ'. Intrinsic: '_mm_testn_epi64_mask'.
// Requires AVX512F.
func TestnEpi64Mask(a x86.M128i, b x86.M128i) x86.Mmask8 {
	return x86.Mmask8(testnEpi64Mask([16]byte(a), [16]byte(b)))
}

func testnEpi64Mask(a [16]byte, b [16]byte) uint8


// M256MaskTestnEpi64Mask: Compute the bitwise NAND of packed 64-bit integers
// in 'a' and 'b', producing intermediate 64-bit values, and set the
// corresponding bit in result mask 'k' (subject to writemask 'k') if the
// intermediate value is zero. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ((a[i+63:i] AND b[i+63:i]) == 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPTESTNMQ'. Intrinsic: '_mm256_mask_testn_epi64_mask'.
// Requires AVX512F.
func M256MaskTestnEpi64Mask(k1 x86.Mmask8, a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256MaskTestnEpi64Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func m256MaskTestnEpi64Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// M256TestnEpi64Mask: Compute the bitwise NAND of packed 64-bit integers in
// 'a' and 'b', producing intermediate 64-bit values, and set the corresponding
// bit in result mask 'k' if the intermediate value is zero. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ((a[i+63:i] AND b[i+63:i]) == 0) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPTESTNMQ'. Intrinsic: '_mm256_testn_epi64_mask'.
// Requires AVX512F.
func M256TestnEpi64Mask(a x86.M256i, b x86.M256i) x86.Mmask8 {
	return x86.Mmask8(m256TestnEpi64Mask([32]byte(a), [32]byte(b)))
}

func m256TestnEpi64Mask(a [32]byte, b [32]byte) uint8


// M512MaskTestnEpi64Mask: Compute the bitwise NAND of packed 64-bit integers
// in 'a' and 'b', producing intermediate 64-bit values, and set the
// corresponding bit in result mask 'k' (subject to writemask 'k') if the
// intermediate value is zero. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ((a[i+63:i] AND b[i+63:i]) == 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPTESTNMQ'. Intrinsic: '_mm512_mask_testn_epi64_mask'.
// Requires AVX512F.
func M512MaskTestnEpi64Mask(k1 x86.Mmask8, a x86.M512i, b x86.M512i) x86.Mmask8 {
	return x86.Mmask8(m512MaskTestnEpi64Mask(uint8(k1), [64]byte(a), [64]byte(b)))
}

func m512MaskTestnEpi64Mask(k1 uint8, a [64]byte, b [64]byte) uint8


// M512TestnEpi64Mask: Compute the bitwise NAND of packed 64-bit integers in
// 'a' and 'b', producing intermediate 64-bit values, and set the corresponding
// bit in result mask 'k' if the intermediate value is zero. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ((a[i+63:i] AND b[i+63:i]) == 0) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPTESTNMQ'. Intrinsic: '_mm512_testn_epi64_mask'.
// Requires AVX512F.
func M512TestnEpi64Mask(a x86.M512i, b x86.M512i) x86.Mmask8 {
	return x86.Mmask8(m512TestnEpi64Mask([64]byte(a), [64]byte(b)))
}

func m512TestnEpi64Mask(a [64]byte, b [64]byte) uint8


// M512MaskTruncPd: Truncate the packed double-precision (64-bit)
// floating-point elements in 'a', and store the results as packed
// double-precision floating-point elements in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := TRUNCATE(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_trunc_pd'.
// Requires AVX512F.
func M512MaskTruncPd(src x86.M512d, k x86.Mmask8, a x86.M512d) x86.M512d {
	return x86.M512d(m512MaskTruncPd([8]float64(src), uint8(k), [8]float64(a)))
}

func m512MaskTruncPd(src [8]float64, k uint8, a [8]float64) [8]float64


// M512TruncPd: Truncate the packed double-precision (64-bit) floating-point
// elements in 'a', and store the results as packed double-precision
// floating-point elements in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := TRUNCATE(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_trunc_pd'.
// Requires AVX512F.
func M512TruncPd(a x86.M512d) x86.M512d {
	return x86.M512d(m512TruncPd([8]float64(a)))
}

func m512TruncPd(a [8]float64) [8]float64


// M512MaskTruncPs: Truncate the packed single-precision (32-bit)
// floating-point elements in 'a', and store the results as packed
// single-precision floating-point elements in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := TRUNCATE(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_trunc_ps'.
// Requires AVX512F.
func M512MaskTruncPs(src x86.M512, k x86.Mmask16, a x86.M512) x86.M512 {
	return x86.M512(m512MaskTruncPs([16]float32(src), uint16(k), [16]float32(a)))
}

func m512MaskTruncPs(src [16]float32, k uint16, a [16]float32) [16]float32


// M512TruncPs: Truncate the packed single-precision (32-bit) floating-point
// elements in 'a', and store the results as packed single-precision
// floating-point elements in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := TRUNCATE(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_trunc_ps'.
// Requires AVX512F.
func M512TruncPs(a x86.M512) x86.M512 {
	return x86.M512(m512TruncPs([16]float32(a)))
}

func m512TruncPs(a [16]float32) [16]float32


// M512Undefined: Return vector of type __m512 with undefined elements. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_undefined'.
// Requires AVX512F.
func M512Undefined() x86.M512 {
	return x86.M512(m512Undefined())
}

func m512Undefined() [16]float32


// M512UndefinedEpi32: Return vector of type __m512i with undefined elements. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_undefined_epi32'.
// Requires AVX512F.
func M512UndefinedEpi32() x86.M512i {
	return x86.M512i(m512UndefinedEpi32())
}

func m512UndefinedEpi32() [64]byte


// M512UndefinedPd: Return vector of type __m512d with undefined elements. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_undefined_pd'.
// Requires AVX512F.
func M512UndefinedPd() x86.M512d {
	return x86.M512d(m512UndefinedPd())
}

func m512UndefinedPd() [8]float64


// M512UndefinedPs: Return vector of type __m512 with undefined elements. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_undefined_ps'.
// Requires AVX512F.
func M512UndefinedPs() x86.M512 {
	return x86.M512(m512UndefinedPs())
}

func m512UndefinedPs() [16]float32


// MaskUnpackhiEpi32: Unpack and interleave 32-bit integers from the high half
// of 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPUNPCKHDQ'. Intrinsic: '_mm_mask_unpackhi_epi32'.
// Requires AVX512F.
func MaskUnpackhiEpi32(src x86.M128i, k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskUnpackhiEpi32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskUnpackhiEpi32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzUnpackhiEpi32: Unpack and interleave 32-bit integers from the high half
// of 'a' and 'b', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPUNPCKHDQ'. Intrinsic: '_mm_maskz_unpackhi_epi32'.
// Requires AVX512F.
func MaskzUnpackhiEpi32(k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskzUnpackhiEpi32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzUnpackhiEpi32(k uint8, a [16]byte, b [16]byte) [16]byte


// M256MaskUnpackhiEpi32: Unpack and interleave 32-bit integers from the high
// half of each 128-bit lane in 'a' and 'b', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKHDQ'. Intrinsic: '_mm256_mask_unpackhi_epi32'.
// Requires AVX512F.
func M256MaskUnpackhiEpi32(src x86.M256i, k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskUnpackhiEpi32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskUnpackhiEpi32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// M256MaskzUnpackhiEpi32: Unpack and interleave 32-bit integers from the high
// half of each 128-bit lane in 'a' and 'b', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKHDQ'. Intrinsic: '_mm256_maskz_unpackhi_epi32'.
// Requires AVX512F.
func M256MaskzUnpackhiEpi32(k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzUnpackhiEpi32(uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskzUnpackhiEpi32(k uint8, a [32]byte, b [32]byte) [32]byte


// M512MaskUnpackhiEpi32: Unpack and interleave 32-bit integers from the high
// half of each 128-bit lane in 'a' and 'b', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_HIGH_DWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_HIGH_DWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKHDQ'. Intrinsic: '_mm512_mask_unpackhi_epi32'.
// Requires AVX512F.
func M512MaskUnpackhiEpi32(src x86.M512i, k x86.Mmask16, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskUnpackhiEpi32([64]byte(src), uint16(k), [64]byte(a), [64]byte(b)))
}

func m512MaskUnpackhiEpi32(src [64]byte, k uint16, a [64]byte, b [64]byte) [64]byte


// M512MaskzUnpackhiEpi32: Unpack and interleave 32-bit integers from the high
// half of each 128-bit lane in 'a' and 'b', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_HIGH_DWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_HIGH_DWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKHDQ'. Intrinsic: '_mm512_maskz_unpackhi_epi32'.
// Requires AVX512F.
func M512MaskzUnpackhiEpi32(k x86.Mmask16, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzUnpackhiEpi32(uint16(k), [64]byte(a), [64]byte(b)))
}

func m512MaskzUnpackhiEpi32(k uint16, a [64]byte, b [64]byte) [64]byte


// M512UnpackhiEpi32: Unpack and interleave 32-bit integers from the high half
// of each 128-bit lane in 'a' and 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}	
//		
//		dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
//		dst[383:256] := INTERLEAVE_HIGH_DWORDS(a[383:256], b[383:256])
//		dst[511:384] := INTERLEAVE_HIGH_DWORDS(a[511:384], b[511:384])
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKHDQ'. Intrinsic: '_mm512_unpackhi_epi32'.
// Requires AVX512F.
func M512UnpackhiEpi32(a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512UnpackhiEpi32([64]byte(a), [64]byte(b)))
}

func m512UnpackhiEpi32(a [64]byte, b [64]byte) [64]byte


// MaskUnpackhiEpi64: Unpack and interleave 64-bit integers from the high half
// of 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPUNPCKHQDQ'. Intrinsic: '_mm_mask_unpackhi_epi64'.
// Requires AVX512F.
func MaskUnpackhiEpi64(src x86.M128i, k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskUnpackhiEpi64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskUnpackhiEpi64(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzUnpackhiEpi64: Unpack and interleave 64-bit integers from the high half
// of 'a' and 'b', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPUNPCKHQDQ'. Intrinsic: '_mm_maskz_unpackhi_epi64'.
// Requires AVX512F.
func MaskzUnpackhiEpi64(k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskzUnpackhiEpi64(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzUnpackhiEpi64(k uint8, a [16]byte, b [16]byte) [16]byte


// M256MaskUnpackhiEpi64: Unpack and interleave 64-bit integers from the high
// half of each 128-bit lane in 'a' and 'b', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKHQDQ'. Intrinsic: '_mm256_mask_unpackhi_epi64'.
// Requires AVX512F.
func M256MaskUnpackhiEpi64(src x86.M256i, k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskUnpackhiEpi64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskUnpackhiEpi64(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// M256MaskzUnpackhiEpi64: Unpack and interleave 64-bit integers from the high
// half of each 128-bit lane in 'a' and 'b', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKHQDQ'. Intrinsic: '_mm256_maskz_unpackhi_epi64'.
// Requires AVX512F.
func M256MaskzUnpackhiEpi64(k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzUnpackhiEpi64(uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskzUnpackhiEpi64(k uint8, a [32]byte, b [32]byte) [32]byte


// M512MaskUnpackhiEpi64: Unpack and interleave 64-bit integers from the high
// half of each 128-bit lane in 'a' and 'b', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_HIGH_QWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_HIGH_QWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKHQDQ'. Intrinsic: '_mm512_mask_unpackhi_epi64'.
// Requires AVX512F.
func M512MaskUnpackhiEpi64(src x86.M512i, k x86.Mmask8, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskUnpackhiEpi64([64]byte(src), uint8(k), [64]byte(a), [64]byte(b)))
}

func m512MaskUnpackhiEpi64(src [64]byte, k uint8, a [64]byte, b [64]byte) [64]byte


// M512MaskzUnpackhiEpi64: Unpack and interleave 64-bit integers from the high
// half of each 128-bit lane in 'a' and 'b', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_HIGH_QWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_HIGH_QWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKHQDQ'. Intrinsic: '_mm512_maskz_unpackhi_epi64'.
// Requires AVX512F.
func M512MaskzUnpackhiEpi64(k x86.Mmask8, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzUnpackhiEpi64(uint8(k), [64]byte(a), [64]byte(b)))
}

func m512MaskzUnpackhiEpi64(k uint8, a [64]byte, b [64]byte) [64]byte


// M512UnpackhiEpi64: Unpack and interleave 64-bit integers from the high half
// of each 128-bit lane in 'a' and 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
//		dst[383:256] := INTERLEAVE_HIGH_QWORDS(a[383:256], b[383:256])
//		dst[511:384] := INTERLEAVE_HIGH_QWORDS(a[511:384], b[511:384])
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKHQDQ'. Intrinsic: '_mm512_unpackhi_epi64'.
// Requires AVX512F.
func M512UnpackhiEpi64(a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512UnpackhiEpi64([64]byte(a), [64]byte(b)))
}

func m512UnpackhiEpi64(a [64]byte, b [64]byte) [64]byte


// MaskUnpackhiPd: Unpack and interleave double-precision (64-bit)
// floating-point elements from the high half of 'a' and 'b', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VUNPCKHPD'. Intrinsic: '_mm_mask_unpackhi_pd'.
// Requires AVX512F.
func MaskUnpackhiPd(src x86.M128d, k x86.Mmask8, a x86.M128d, b x86.M128d) x86.M128d {
	return x86.M128d(maskUnpackhiPd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskUnpackhiPd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzUnpackhiPd: Unpack and interleave double-precision (64-bit)
// floating-point elements from the high half of 'a' and 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VUNPCKHPD'. Intrinsic: '_mm_maskz_unpackhi_pd'.
// Requires AVX512F.
func MaskzUnpackhiPd(k x86.Mmask8, a x86.M128d, b x86.M128d) x86.M128d {
	return x86.M128d(maskzUnpackhiPd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzUnpackhiPd(k uint8, a [2]float64, b [2]float64) [2]float64


// M256MaskUnpackhiPd: Unpack and interleave double-precision (64-bit)
// floating-point elements from the high half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VUNPCKHPD'. Intrinsic: '_mm256_mask_unpackhi_pd'.
// Requires AVX512F.
func M256MaskUnpackhiPd(src x86.M256d, k x86.Mmask8, a x86.M256d, b x86.M256d) x86.M256d {
	return x86.M256d(m256MaskUnpackhiPd([4]float64(src), uint8(k), [4]float64(a), [4]float64(b)))
}

func m256MaskUnpackhiPd(src [4]float64, k uint8, a [4]float64, b [4]float64) [4]float64


// M256MaskzUnpackhiPd: Unpack and interleave double-precision (64-bit)
// floating-point elements from the high half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VUNPCKHPD'. Intrinsic: '_mm256_maskz_unpackhi_pd'.
// Requires AVX512F.
func M256MaskzUnpackhiPd(k x86.Mmask8, a x86.M256d, b x86.M256d) x86.M256d {
	return x86.M256d(m256MaskzUnpackhiPd(uint8(k), [4]float64(a), [4]float64(b)))
}

func m256MaskzUnpackhiPd(k uint8, a [4]float64, b [4]float64) [4]float64


// M512MaskUnpackhiPd: Unpack and interleave double-precision (64-bit)
// floating-point elements from the high half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_HIGH_QWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_HIGH_QWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VUNPCKHPD'. Intrinsic: '_mm512_mask_unpackhi_pd'.
// Requires AVX512F.
func M512MaskUnpackhiPd(src x86.M512d, k x86.Mmask8, a x86.M512d, b x86.M512d) x86.M512d {
	return x86.M512d(m512MaskUnpackhiPd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b)))
}

func m512MaskUnpackhiPd(src [8]float64, k uint8, a [8]float64, b [8]float64) [8]float64


// M512MaskzUnpackhiPd: Unpack and interleave double-precision (64-bit)
// floating-point elements from the high half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_HIGH_QWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_HIGH_QWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VUNPCKHPD'. Intrinsic: '_mm512_maskz_unpackhi_pd'.
// Requires AVX512F.
func M512MaskzUnpackhiPd(k x86.Mmask8, a x86.M512d, b x86.M512d) x86.M512d {
	return x86.M512d(m512MaskzUnpackhiPd(uint8(k), [8]float64(a), [8]float64(b)))
}

func m512MaskzUnpackhiPd(k uint8, a [8]float64, b [8]float64) [8]float64


// M512UnpackhiPd: Unpack and interleave double-precision (64-bit)
// floating-point elements from the high half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
//		dst[383:256] := INTERLEAVE_HIGH_QWORDS(a[383:256], b[383:256])
//		dst[511:384] := INTERLEAVE_HIGH_QWORDS(a[511:384], b[511:384])
//		dst[MAX:512] := 0
//
// Instruction: 'VUNPCKHPD'. Intrinsic: '_mm512_unpackhi_pd'.
// Requires AVX512F.
func M512UnpackhiPd(a x86.M512d, b x86.M512d) x86.M512d {
	return x86.M512d(m512UnpackhiPd([8]float64(a), [8]float64(b)))
}

func m512UnpackhiPd(a [8]float64, b [8]float64) [8]float64


// MaskUnpackhiPs: Unpack and interleave single-precision (32-bit)
// floating-point elements from the high half of 'a' and 'b', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VUNPCKHPS'. Intrinsic: '_mm_mask_unpackhi_ps'.
// Requires AVX512F.
func MaskUnpackhiPs(src x86.M128, k x86.Mmask8, a x86.M128, b x86.M128) x86.M128 {
	return x86.M128(maskUnpackhiPs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskUnpackhiPs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzUnpackhiPs: Unpack and interleave single-precision (32-bit)
// floating-point elements from the high half of 'a' and 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VUNPCKHPS'. Intrinsic: '_mm_maskz_unpackhi_ps'.
// Requires AVX512F.
func MaskzUnpackhiPs(k x86.Mmask8, a x86.M128, b x86.M128) x86.M128 {
	return x86.M128(maskzUnpackhiPs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzUnpackhiPs(k uint8, a [4]float32, b [4]float32) [4]float32


// M256MaskUnpackhiPs: Unpack and interleave single-precision (32-bit)
// floating-point elements from the high half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VUNPCKHPS'. Intrinsic: '_mm256_mask_unpackhi_ps'.
// Requires AVX512F.
func M256MaskUnpackhiPs(src x86.M256, k x86.Mmask8, a x86.M256, b x86.M256) x86.M256 {
	return x86.M256(m256MaskUnpackhiPs([8]float32(src), uint8(k), [8]float32(a), [8]float32(b)))
}

func m256MaskUnpackhiPs(src [8]float32, k uint8, a [8]float32, b [8]float32) [8]float32


// M256MaskzUnpackhiPs: Unpack and interleave single-precision (32-bit)
// floating-point elements from the high half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VUNPCKHPS'. Intrinsic: '_mm256_maskz_unpackhi_ps'.
// Requires AVX512F.
func M256MaskzUnpackhiPs(k x86.Mmask8, a x86.M256, b x86.M256) x86.M256 {
	return x86.M256(m256MaskzUnpackhiPs(uint8(k), [8]float32(a), [8]float32(b)))
}

func m256MaskzUnpackhiPs(k uint8, a [8]float32, b [8]float32) [8]float32


// M512MaskUnpackhiPs: Unpack and interleave single-precision (32-bit)
// floating-point elements from the high half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_HIGH_DWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_HIGH_DWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VUNPCKHPS'. Intrinsic: '_mm512_mask_unpackhi_ps'.
// Requires AVX512F.
func M512MaskUnpackhiPs(src x86.M512, k x86.Mmask16, a x86.M512, b x86.M512) x86.M512 {
	return x86.M512(m512MaskUnpackhiPs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b)))
}

func m512MaskUnpackhiPs(src [16]float32, k uint16, a [16]float32, b [16]float32) [16]float32


// M512MaskzUnpackhiPs: Unpack and interleave single-precision (32-bit)
// floating-point elements from the high half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_HIGH_DWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_HIGH_DWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VUNPCKHPS'. Intrinsic: '_mm512_maskz_unpackhi_ps'.
// Requires AVX512F.
func M512MaskzUnpackhiPs(k x86.Mmask16, a x86.M512, b x86.M512) x86.M512 {
	return x86.M512(m512MaskzUnpackhiPs(uint16(k), [16]float32(a), [16]float32(b)))
}

func m512MaskzUnpackhiPs(k uint16, a [16]float32, b [16]float32) [16]float32


// M512UnpackhiPs: Unpack and interleave single-precision (32-bit)
// floating-point elements from the high half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}	
//		
//		dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
//		dst[383:256] := INTERLEAVE_HIGH_DWORDS(a[383:256], b[383:256])
//		dst[511:384] := INTERLEAVE_HIGH_DWORDS(a[511:384], b[511:384])
//		dst[MAX:512] := 0
//
// Instruction: 'VUNPCKHPS'. Intrinsic: '_mm512_unpackhi_ps'.
// Requires AVX512F.
func M512UnpackhiPs(a x86.M512, b x86.M512) x86.M512 {
	return x86.M512(m512UnpackhiPs([16]float32(a), [16]float32(b)))
}

func m512UnpackhiPs(a [16]float32, b [16]float32) [16]float32


// MaskUnpackloEpi32: Unpack and interleave 32-bit integers from the low half
// of 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPUNPCKLDQ'. Intrinsic: '_mm_mask_unpacklo_epi32'.
// Requires AVX512F.
func MaskUnpackloEpi32(src x86.M128i, k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskUnpackloEpi32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskUnpackloEpi32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzUnpackloEpi32: Unpack and interleave 32-bit integers from the low half
// of 'a' and 'b', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPUNPCKLDQ'. Intrinsic: '_mm_maskz_unpacklo_epi32'.
// Requires AVX512F.
func MaskzUnpackloEpi32(k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskzUnpackloEpi32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzUnpackloEpi32(k uint8, a [16]byte, b [16]byte) [16]byte


// M256MaskUnpackloEpi32: Unpack and interleave 32-bit integers from the low
// half of each 128-bit lane in 'a' and 'b', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKLDQ'. Intrinsic: '_mm256_mask_unpacklo_epi32'.
// Requires AVX512F.
func M256MaskUnpackloEpi32(src x86.M256i, k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskUnpackloEpi32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskUnpackloEpi32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// M256MaskzUnpackloEpi32: Unpack and interleave 32-bit integers from the low
// half of each 128-bit lane in 'a' and 'b', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKLDQ'. Intrinsic: '_mm256_maskz_unpacklo_epi32'.
// Requires AVX512F.
func M256MaskzUnpackloEpi32(k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzUnpackloEpi32(uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskzUnpackloEpi32(k uint8, a [32]byte, b [32]byte) [32]byte


// M512MaskUnpackloEpi32: Unpack and interleave 32-bit integers from the low
// half of each 128-bit lane in 'a' and 'b', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_DWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_DWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKLDQ'. Intrinsic: '_mm512_mask_unpacklo_epi32'.
// Requires AVX512F.
func M512MaskUnpackloEpi32(src x86.M512i, k x86.Mmask16, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskUnpackloEpi32([64]byte(src), uint16(k), [64]byte(a), [64]byte(b)))
}

func m512MaskUnpackloEpi32(src [64]byte, k uint16, a [64]byte, b [64]byte) [64]byte


// M512MaskzUnpackloEpi32: Unpack and interleave 32-bit integers from the low
// half of each 128-bit lane in 'a' and 'b', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_DWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_DWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKLDQ'. Intrinsic: '_mm512_maskz_unpacklo_epi32'.
// Requires AVX512F.
func M512MaskzUnpackloEpi32(k x86.Mmask16, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzUnpackloEpi32(uint16(k), [64]byte(a), [64]byte(b)))
}

func m512MaskzUnpackloEpi32(k uint16, a [64]byte, b [64]byte) [64]byte


// M512UnpackloEpi32: Unpack and interleave 32-bit integers from the low half
// of each 128-bit lane in 'a' and 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
//		dst[383:256] := INTERLEAVE_DWORDS(a[383:256], b[383:256])
//		dst[511:384] := INTERLEAVE_DWORDS(a[511:384], b[511:384])
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKLDQ'. Intrinsic: '_mm512_unpacklo_epi32'.
// Requires AVX512F.
func M512UnpackloEpi32(a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512UnpackloEpi32([64]byte(a), [64]byte(b)))
}

func m512UnpackloEpi32(a [64]byte, b [64]byte) [64]byte


// MaskUnpackloEpi64: Unpack and interleave 64-bit integers from the low half
// of 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPUNPCKLQDQ'. Intrinsic: '_mm_mask_unpacklo_epi64'.
// Requires AVX512F.
func MaskUnpackloEpi64(src x86.M128i, k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskUnpackloEpi64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskUnpackloEpi64(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzUnpackloEpi64: Unpack and interleave 64-bit integers from the low half
// of 'a' and 'b', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPUNPCKLQDQ'. Intrinsic: '_mm_maskz_unpacklo_epi64'.
// Requires AVX512F.
func MaskzUnpackloEpi64(k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskzUnpackloEpi64(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzUnpackloEpi64(k uint8, a [16]byte, b [16]byte) [16]byte


// M256MaskUnpackloEpi64: Unpack and interleave 64-bit integers from the low
// half of each 128-bit lane in 'a' and 'b', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKLQDQ'. Intrinsic: '_mm256_mask_unpacklo_epi64'.
// Requires AVX512F.
func M256MaskUnpackloEpi64(src x86.M256i, k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskUnpackloEpi64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskUnpackloEpi64(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// M256MaskzUnpackloEpi64: Unpack and interleave 64-bit integers from the low
// half of each 128-bit lane in 'a' and 'b', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKLQDQ'. Intrinsic: '_mm256_maskz_unpacklo_epi64'.
// Requires AVX512F.
func M256MaskzUnpackloEpi64(k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzUnpackloEpi64(uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskzUnpackloEpi64(k uint8, a [32]byte, b [32]byte) [32]byte


// M512MaskUnpackloEpi64: Unpack and interleave 64-bit integers from the low
// half of each 128-bit lane in 'a' and 'b', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_QWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_QWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKLQDQ'. Intrinsic: '_mm512_mask_unpacklo_epi64'.
// Requires AVX512F.
func M512MaskUnpackloEpi64(src x86.M512i, k x86.Mmask8, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskUnpackloEpi64([64]byte(src), uint8(k), [64]byte(a), [64]byte(b)))
}

func m512MaskUnpackloEpi64(src [64]byte, k uint8, a [64]byte, b [64]byte) [64]byte


// M512MaskzUnpackloEpi64: Unpack and interleave 64-bit integers from the low
// half of each 128-bit lane in 'a' and 'b', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_QWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_QWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKLQDQ'. Intrinsic: '_mm512_maskz_unpacklo_epi64'.
// Requires AVX512F.
func M512MaskzUnpackloEpi64(k x86.Mmask8, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzUnpackloEpi64(uint8(k), [64]byte(a), [64]byte(b)))
}

func m512MaskzUnpackloEpi64(k uint8, a [64]byte, b [64]byte) [64]byte


// M512UnpackloEpi64: Unpack and interleave 64-bit integers from the low half
// of each 128-bit lane in 'a' and 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
//		dst[383:256] := INTERLEAVE_QWORDS(a[383:256], b[383:256])
//		dst[511:384] := INTERLEAVE_QWORDS(a[511:384], b[511:384])
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKLQDQ'. Intrinsic: '_mm512_unpacklo_epi64'.
// Requires AVX512F.
func M512UnpackloEpi64(a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512UnpackloEpi64([64]byte(a), [64]byte(b)))
}

func m512UnpackloEpi64(a [64]byte, b [64]byte) [64]byte


// MaskUnpackloPd: Unpack and interleave double-precision (64-bit)
// floating-point elements from the low half of 'a' and 'b', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VUNPCKLPD'. Intrinsic: '_mm_mask_unpacklo_pd'.
// Requires AVX512F.
func MaskUnpackloPd(src x86.M128d, k x86.Mmask8, a x86.M128d, b x86.M128d) x86.M128d {
	return x86.M128d(maskUnpackloPd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskUnpackloPd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzUnpackloPd: Unpack and interleave double-precision (64-bit)
// floating-point elements from the low half of 'a' and 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VUNPCKLPD'. Intrinsic: '_mm_maskz_unpacklo_pd'.
// Requires AVX512F.
func MaskzUnpackloPd(k x86.Mmask8, a x86.M128d, b x86.M128d) x86.M128d {
	return x86.M128d(maskzUnpackloPd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzUnpackloPd(k uint8, a [2]float64, b [2]float64) [2]float64


// M256MaskUnpackloPd: Unpack and interleave double-precision (64-bit)
// floating-point elements from the low half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VUNPCKLPD'. Intrinsic: '_mm256_mask_unpacklo_pd'.
// Requires AVX512F.
func M256MaskUnpackloPd(src x86.M256d, k x86.Mmask8, a x86.M256d, b x86.M256d) x86.M256d {
	return x86.M256d(m256MaskUnpackloPd([4]float64(src), uint8(k), [4]float64(a), [4]float64(b)))
}

func m256MaskUnpackloPd(src [4]float64, k uint8, a [4]float64, b [4]float64) [4]float64


// M256MaskzUnpackloPd: Unpack and interleave double-precision (64-bit)
// floating-point elements from the low half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VUNPCKLPD'. Intrinsic: '_mm256_maskz_unpacklo_pd'.
// Requires AVX512F.
func M256MaskzUnpackloPd(k x86.Mmask8, a x86.M256d, b x86.M256d) x86.M256d {
	return x86.M256d(m256MaskzUnpackloPd(uint8(k), [4]float64(a), [4]float64(b)))
}

func m256MaskzUnpackloPd(k uint8, a [4]float64, b [4]float64) [4]float64


// M512MaskUnpackloPd: Unpack and interleave double-precision (64-bit)
// floating-point elements from the low half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_QWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_QWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VUNPCKLPD'. Intrinsic: '_mm512_mask_unpacklo_pd'.
// Requires AVX512F.
func M512MaskUnpackloPd(src x86.M512d, k x86.Mmask8, a x86.M512d, b x86.M512d) x86.M512d {
	return x86.M512d(m512MaskUnpackloPd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b)))
}

func m512MaskUnpackloPd(src [8]float64, k uint8, a [8]float64, b [8]float64) [8]float64


// M512MaskzUnpackloPd: Unpack and interleave double-precision (64-bit)
// floating-point elements from the low half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_QWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_QWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VUNPCKLPD'. Intrinsic: '_mm512_maskz_unpacklo_pd'.
// Requires AVX512F.
func M512MaskzUnpackloPd(k x86.Mmask8, a x86.M512d, b x86.M512d) x86.M512d {
	return x86.M512d(m512MaskzUnpackloPd(uint8(k), [8]float64(a), [8]float64(b)))
}

func m512MaskzUnpackloPd(k uint8, a [8]float64, b [8]float64) [8]float64


// M512UnpackloPd: Unpack and interleave double-precision (64-bit)
// floating-point elements from the low half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
//		dst[383:256] := INTERLEAVE_QWORDS(a[383:256], b[383:256])
//		dst[511:384] := INTERLEAVE_QWORDS(a[511:384], b[511:384])
//		dst[MAX:512] := 0
//
// Instruction: 'VUNPCKLPD'. Intrinsic: '_mm512_unpacklo_pd'.
// Requires AVX512F.
func M512UnpackloPd(a x86.M512d, b x86.M512d) x86.M512d {
	return x86.M512d(m512UnpackloPd([8]float64(a), [8]float64(b)))
}

func m512UnpackloPd(a [8]float64, b [8]float64) [8]float64


// MaskUnpackloPs: Unpack and interleave single-precision (32-bit)
// floating-point elements from the low half of 'a' and 'b', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VUNPCKLPS'. Intrinsic: '_mm_mask_unpacklo_ps'.
// Requires AVX512F.
func MaskUnpackloPs(src x86.M128, k x86.Mmask8, a x86.M128, b x86.M128) x86.M128 {
	return x86.M128(maskUnpackloPs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskUnpackloPs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzUnpackloPs: Unpack and interleave single-precision (32-bit)
// floating-point elements from the low half of 'a' and 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VUNPCKLPS'. Intrinsic: '_mm_maskz_unpacklo_ps'.
// Requires AVX512F.
func MaskzUnpackloPs(k x86.Mmask8, a x86.M128, b x86.M128) x86.M128 {
	return x86.M128(maskzUnpackloPs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzUnpackloPs(k uint8, a [4]float32, b [4]float32) [4]float32


// M256MaskUnpackloPs: Unpack and interleave single-precision (32-bit)
// floating-point elements from the low half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VUNPCKLPS'. Intrinsic: '_mm256_mask_unpacklo_ps'.
// Requires AVX512F.
func M256MaskUnpackloPs(src x86.M256, k x86.Mmask8, a x86.M256, b x86.M256) x86.M256 {
	return x86.M256(m256MaskUnpackloPs([8]float32(src), uint8(k), [8]float32(a), [8]float32(b)))
}

func m256MaskUnpackloPs(src [8]float32, k uint8, a [8]float32, b [8]float32) [8]float32


// M256MaskzUnpackloPs: Unpack and interleave single-precision (32-bit)
// floating-point elements from the low half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VUNPCKLPS'. Intrinsic: '_mm256_maskz_unpacklo_ps'.
// Requires AVX512F.
func M256MaskzUnpackloPs(k x86.Mmask8, a x86.M256, b x86.M256) x86.M256 {
	return x86.M256(m256MaskzUnpackloPs(uint8(k), [8]float32(a), [8]float32(b)))
}

func m256MaskzUnpackloPs(k uint8, a [8]float32, b [8]float32) [8]float32


// M512MaskUnpackloPs: Unpack and interleave single-precision (32-bit)
// floating-point elements from the low half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_DWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_DWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VUNPCKLPS'. Intrinsic: '_mm512_mask_unpacklo_ps'.
// Requires AVX512F.
func M512MaskUnpackloPs(src x86.M512, k x86.Mmask16, a x86.M512, b x86.M512) x86.M512 {
	return x86.M512(m512MaskUnpackloPs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b)))
}

func m512MaskUnpackloPs(src [16]float32, k uint16, a [16]float32, b [16]float32) [16]float32


// M512MaskzUnpackloPs: Unpack and interleave single-precision (32-bit)
// floating-point elements from the low half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_DWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_DWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VUNPCKLPS'. Intrinsic: '_mm512_maskz_unpacklo_ps'.
// Requires AVX512F.
func M512MaskzUnpackloPs(k x86.Mmask16, a x86.M512, b x86.M512) x86.M512 {
	return x86.M512(m512MaskzUnpackloPs(uint16(k), [16]float32(a), [16]float32(b)))
}

func m512MaskzUnpackloPs(k uint16, a [16]float32, b [16]float32) [16]float32


// M512UnpackloPs: Unpack and interleave single-precision (32-bit)
// floating-point elements from the low half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
//		dst[383:256] := INTERLEAVE_DWORDS(a[383:256], b[383:256])
//		dst[511:384] := INTERLEAVE_DWORDS(a[511:384], b[511:384])
//		dst[MAX:512] := 0
//
// Instruction: 'VUNPCKLPS'. Intrinsic: '_mm512_unpacklo_ps'.
// Requires AVX512F.
func M512UnpackloPs(a x86.M512, b x86.M512) x86.M512 {
	return x86.M512(m512UnpackloPs([16]float32(a), [16]float32(b)))
}

func m512UnpackloPs(a [16]float32, b [16]float32) [16]float32


// MaskXorEpi32: Compute the bitwise XOR of packed 32-bit integers in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPXORD'. Intrinsic: '_mm_mask_xor_epi32'.
// Requires AVX512F.
func MaskXorEpi32(src x86.M128i, k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskXorEpi32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskXorEpi32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzXorEpi32: Compute the bitwise XOR of packed 32-bit integers in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPXORD'. Intrinsic: '_mm_maskz_xor_epi32'.
// Requires AVX512F.
func MaskzXorEpi32(k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskzXorEpi32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzXorEpi32(k uint8, a [16]byte, b [16]byte) [16]byte


// M256MaskXorEpi32: Compute the bitwise XOR of packed 32-bit integers in 'a'
// and 'b', and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPXORD'. Intrinsic: '_mm256_mask_xor_epi32'.
// Requires AVX512F.
func M256MaskXorEpi32(src x86.M256i, k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskXorEpi32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskXorEpi32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// M256MaskzXorEpi32: Compute the bitwise XOR of packed 32-bit integers in 'a'
// and 'b', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPXORD'. Intrinsic: '_mm256_maskz_xor_epi32'.
// Requires AVX512F.
func M256MaskzXorEpi32(k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzXorEpi32(uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskzXorEpi32(k uint8, a [32]byte, b [32]byte) [32]byte


// M512MaskzXorEpi32: Compute the bitwise XOR of packed 32-bit integers in 'a'
// and 'b', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPXORD'. Intrinsic: '_mm512_maskz_xor_epi32'.
// Requires AVX512F.
func M512MaskzXorEpi32(k x86.Mmask16, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzXorEpi32(uint16(k), [64]byte(a), [64]byte(b)))
}

func m512MaskzXorEpi32(k uint16, a [64]byte, b [64]byte) [64]byte


// MaskXorEpi64: Compute the bitwise XOR of packed 64-bit integers in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPXORQ'. Intrinsic: '_mm_mask_xor_epi64'.
// Requires AVX512F.
func MaskXorEpi64(src x86.M128i, k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskXorEpi64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskXorEpi64(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzXorEpi64: Compute the bitwise XOR of packed 64-bit integers in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPXORQ'. Intrinsic: '_mm_maskz_xor_epi64'.
// Requires AVX512F.
func MaskzXorEpi64(k x86.Mmask8, a x86.M128i, b x86.M128i) x86.M128i {
	return x86.M128i(maskzXorEpi64(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzXorEpi64(k uint8, a [16]byte, b [16]byte) [16]byte


// M256MaskXorEpi64: Compute the bitwise XOR of packed 64-bit integers in 'a'
// and 'b', and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPXORQ'. Intrinsic: '_mm256_mask_xor_epi64'.
// Requires AVX512F.
func M256MaskXorEpi64(src x86.M256i, k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskXorEpi64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskXorEpi64(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// M256MaskzXorEpi64: Compute the bitwise XOR of packed 64-bit integers in 'a'
// and 'b', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPXORQ'. Intrinsic: '_mm256_maskz_xor_epi64'.
// Requires AVX512F.
func M256MaskzXorEpi64(k x86.Mmask8, a x86.M256i, b x86.M256i) x86.M256i {
	return x86.M256i(m256MaskzXorEpi64(uint8(k), [32]byte(a), [32]byte(b)))
}

func m256MaskzXorEpi64(k uint8, a [32]byte, b [32]byte) [32]byte


// M512MaskzXorEpi64: Compute the bitwise XOR of packed 64-bit integers in 'a'
// and 'b', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPXORQ'. Intrinsic: '_mm512_maskz_xor_epi64'.
// Requires AVX512F.
func M512MaskzXorEpi64(k x86.Mmask8, a x86.M512i, b x86.M512i) x86.M512i {
	return x86.M512i(m512MaskzXorEpi64(uint8(k), [64]byte(a), [64]byte(b)))
}

func m512MaskzXorEpi64(k uint8, a [64]byte, b [64]byte) [64]byte

