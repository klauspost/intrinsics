package avx512f

import . "github.com/klauspost/intrinsics/x86"


// MaskAbsEpi32: Compute the absolute value of packed 32-bit integers in 'a',
// and store the unsigned results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ABS(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPABSD'. Intrinsic: '_mm_mask_abs_epi32'.
// Requires AVX512F.
func MaskAbsEpi32(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskAbsEpi32([16]byte(src), uint8(k), [16]byte(a)))
}

func maskAbsEpi32(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzAbsEpi32: Compute the absolute value of packed 32-bit integers in 'a',
// and store the unsigned results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ABS(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPABSD'. Intrinsic: '_mm_maskz_abs_epi32'.
// Requires AVX512F.
func MaskzAbsEpi32(k Mmask8, a M128i) M128i {
	return M128i(maskzAbsEpi32(uint8(k), [16]byte(a)))
}

func maskzAbsEpi32(k uint8, a [16]byte) [16]byte


// MaskAbsEpi321: Compute the absolute value of packed 32-bit integers in 'a',
// and store the unsigned results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ABS(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPABSD'. Intrinsic: '_mm256_mask_abs_epi32'.
// Requires AVX512F.
func MaskAbsEpi321(src M256i, k Mmask8, a M256i) M256i {
	return M256i(maskAbsEpi321([32]byte(src), uint8(k), [32]byte(a)))
}

func maskAbsEpi321(src [32]byte, k uint8, a [32]byte) [32]byte


// MaskzAbsEpi321: Compute the absolute value of packed 32-bit integers in 'a',
// and store the unsigned results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ABS(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPABSD'. Intrinsic: '_mm256_maskz_abs_epi32'.
// Requires AVX512F.
func MaskzAbsEpi321(k Mmask8, a M256i) M256i {
	return M256i(maskzAbsEpi321(uint8(k), [32]byte(a)))
}

func maskzAbsEpi321(k uint8, a [32]byte) [32]byte


// AbsEpi32: Compute the absolute value of packed 32-bit integers in 'a', and
// store the unsigned results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ABS(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPABSD'. Intrinsic: '_mm512_abs_epi32'.
// Requires AVX512F.
func AbsEpi32(a M512i) M512i {
	return M512i(absEpi32([64]byte(a)))
}

func absEpi32(a [64]byte) [64]byte


// MaskAbsEpi322: Compute the absolute value of packed 32-bit integers in 'a',
// and store the unsigned results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ABS(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPABSD'. Intrinsic: '_mm512_mask_abs_epi32'.
// Requires AVX512F.
func MaskAbsEpi322(src M512i, k Mmask16, a M512i) M512i {
	return M512i(maskAbsEpi322([64]byte(src), uint16(k), [64]byte(a)))
}

func maskAbsEpi322(src [64]byte, k uint16, a [64]byte) [64]byte


// MaskzAbsEpi322: Compute the absolute value of packed 32-bit integers in 'a',
// and store the unsigned results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ABS(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPABSD'. Intrinsic: '_mm512_maskz_abs_epi32'.
// Requires AVX512F.
func MaskzAbsEpi322(k Mmask16, a M512i) M512i {
	return M512i(maskzAbsEpi322(uint16(k), [64]byte(a)))
}

func maskzAbsEpi322(k uint16, a [64]byte) [64]byte


// AbsEpi64: Compute the absolute value of packed 64-bit integers in 'a', and
// store the unsigned results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := ABS(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPABSQ'. Intrinsic: '_mm_abs_epi64'.
// Requires AVX512F.
func AbsEpi64(a M128i) M128i {
	return M128i(absEpi64([16]byte(a)))
}

func absEpi64(a [16]byte) [16]byte


// MaskAbsEpi64: Compute the absolute value of packed 64-bit integers in 'a',
// and store the unsigned results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ABS(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPABSQ'. Intrinsic: '_mm_mask_abs_epi64'.
// Requires AVX512F.
func MaskAbsEpi64(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskAbsEpi64([16]byte(src), uint8(k), [16]byte(a)))
}

func maskAbsEpi64(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzAbsEpi64: Compute the absolute value of packed 64-bit integers in 'a',
// and store the unsigned results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ABS(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPABSQ'. Intrinsic: '_mm_maskz_abs_epi64'.
// Requires AVX512F.
func MaskzAbsEpi64(k Mmask8, a M128i) M128i {
	return M128i(maskzAbsEpi64(uint8(k), [16]byte(a)))
}

func maskzAbsEpi64(k uint8, a [16]byte) [16]byte


// AbsEpi641: Compute the absolute value of packed 64-bit integers in 'a', and
// store the unsigned results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ABS(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPABSQ'. Intrinsic: '_mm256_abs_epi64'.
// Requires AVX512F.
func AbsEpi641(a M256i) M256i {
	return M256i(absEpi641([32]byte(a)))
}

func absEpi641(a [32]byte) [32]byte


// MaskAbsEpi641: Compute the absolute value of packed 64-bit integers in 'a',
// and store the unsigned results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ABS(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPABSQ'. Intrinsic: '_mm256_mask_abs_epi64'.
// Requires AVX512F.
func MaskAbsEpi641(src M256i, k Mmask8, a M256i) M256i {
	return M256i(maskAbsEpi641([32]byte(src), uint8(k), [32]byte(a)))
}

func maskAbsEpi641(src [32]byte, k uint8, a [32]byte) [32]byte


// MaskzAbsEpi641: Compute the absolute value of packed 64-bit integers in 'a',
// and store the unsigned results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ABS(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPABSQ'. Intrinsic: '_mm256_maskz_abs_epi64'.
// Requires AVX512F.
func MaskzAbsEpi641(k Mmask8, a M256i) M256i {
	return M256i(maskzAbsEpi641(uint8(k), [32]byte(a)))
}

func maskzAbsEpi641(k uint8, a [32]byte) [32]byte


// AbsEpi642: Compute the absolute value of packed 64-bit integers in 'a', and
// store the unsigned results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ABS(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPABSQ'. Intrinsic: '_mm512_abs_epi64'.
// Requires AVX512F.
func AbsEpi642(a M512i) M512i {
	return M512i(absEpi642([64]byte(a)))
}

func absEpi642(a [64]byte) [64]byte


// MaskAbsEpi642: Compute the absolute value of packed 64-bit integers in 'a',
// and store the unsigned results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ABS(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPABSQ'. Intrinsic: '_mm512_mask_abs_epi64'.
// Requires AVX512F.
func MaskAbsEpi642(src M512i, k Mmask8, a M512i) M512i {
	return M512i(maskAbsEpi642([64]byte(src), uint8(k), [64]byte(a)))
}

func maskAbsEpi642(src [64]byte, k uint8, a [64]byte) [64]byte


// MaskzAbsEpi642: Compute the absolute value of packed 64-bit integers in 'a',
// and store the unsigned results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ABS(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPABSQ'. Intrinsic: '_mm512_maskz_abs_epi64'.
// Requires AVX512F.
func MaskzAbsEpi642(k Mmask8, a M512i) M512i {
	return M512i(maskzAbsEpi642(uint8(k), [64]byte(a)))
}

func maskzAbsEpi642(k uint8, a [64]byte) [64]byte


// AcosPd: Compute the inverse cosine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ACOS(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_acos_pd'.
// Requires AVX512F.
func AcosPd(a M512d) M512d {
	return M512d(acosPd([8]float64(a)))
}

func acosPd(a [8]float64) [8]float64


// MaskAcosPd: Compute the inverse cosine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ACOS(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_acos_pd'.
// Requires AVX512F.
func MaskAcosPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskAcosPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskAcosPd(src [8]float64, k uint8, a [8]float64) [8]float64


// AcosPs: Compute the inverse cosine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ACOS(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_acos_ps'.
// Requires AVX512F.
func AcosPs(a M512) M512 {
	return M512(acosPs([16]float32(a)))
}

func acosPs(a [16]float32) [16]float32


// MaskAcosPs: Compute the inverse cosine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ACOS(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_acos_ps'.
// Requires AVX512F.
func MaskAcosPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskAcosPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskAcosPs(src [16]float32, k uint16, a [16]float32) [16]float32


// AcoshPd: Compute the inverse hyperbolic cosine of packed double-precision
// (64-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ACOSH(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_acosh_pd'.
// Requires AVX512F.
func AcoshPd(a M512d) M512d {
	return M512d(acoshPd([8]float64(a)))
}

func acoshPd(a [8]float64) [8]float64


// MaskAcoshPd: Compute the inverse hyperbolic cosine of packed
// double-precision (64-bit) floating-point elements in 'a' expressed in
// radians, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ACOSH(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_acosh_pd'.
// Requires AVX512F.
func MaskAcoshPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskAcoshPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskAcoshPd(src [8]float64, k uint8, a [8]float64) [8]float64


// AcoshPs: Compute the inverse hyperbolic cosine of packed single-precision
// (32-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ACOSH(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_acosh_ps'.
// Requires AVX512F.
func AcoshPs(a M512) M512 {
	return M512(acoshPs([16]float32(a)))
}

func acoshPs(a [16]float32) [16]float32


// MaskAcoshPs: Compute the inverse hyperbolic cosine of packed
// single-precision (32-bit) floating-point elements in 'a' expressed in
// radians, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ACOSH(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_acosh_ps'.
// Requires AVX512F.
func MaskAcoshPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskAcoshPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskAcoshPs(src [16]float32, k uint16, a [16]float32) [16]float32


// MaskAddEpi32: Add packed 32-bit integers in 'a' and 'b', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] + b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPADDD'. Intrinsic: '_mm_mask_add_epi32'.
// Requires AVX512F.
func MaskAddEpi32(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskAddEpi32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskAddEpi32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzAddEpi32: Add packed 32-bit integers in 'a' and 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] + b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPADDD'. Intrinsic: '_mm_maskz_add_epi32'.
// Requires AVX512F.
func MaskzAddEpi32(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzAddEpi32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzAddEpi32(k uint8, a [16]byte, b [16]byte) [16]byte


// MaskAddEpi321: Add packed 32-bit integers in 'a' and 'b', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] + b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDD'. Intrinsic: '_mm256_mask_add_epi32'.
// Requires AVX512F.
func MaskAddEpi321(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskAddEpi321([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskAddEpi321(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzAddEpi321: Add packed 32-bit integers in 'a' and 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] + b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDD'. Intrinsic: '_mm256_maskz_add_epi32'.
// Requires AVX512F.
func MaskzAddEpi321(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzAddEpi321(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzAddEpi321(k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzAddEpi322: Add packed 32-bit integers in 'a' and 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] + b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPADDD'. Intrinsic: '_mm512_maskz_add_epi32'.
// Requires AVX512F.
func MaskzAddEpi322(k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskzAddEpi322(uint16(k), [64]byte(a), [64]byte(b)))
}

func maskzAddEpi322(k uint16, a [64]byte, b [64]byte) [64]byte


// MaskAddEpi64: Add packed 64-bit integers in 'a' and 'b', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] + b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPADDQ'. Intrinsic: '_mm_mask_add_epi64'.
// Requires AVX512F.
func MaskAddEpi64(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskAddEpi64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskAddEpi64(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzAddEpi64: Add packed 64-bit integers in 'a' and 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] + b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPADDQ'. Intrinsic: '_mm_maskz_add_epi64'.
// Requires AVX512F.
func MaskzAddEpi64(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzAddEpi64(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzAddEpi64(k uint8, a [16]byte, b [16]byte) [16]byte


// MaskAddEpi641: Add packed 64-bit integers in 'a' and 'b', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] + b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDQ'. Intrinsic: '_mm256_mask_add_epi64'.
// Requires AVX512F.
func MaskAddEpi641(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskAddEpi641([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskAddEpi641(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzAddEpi641: Add packed 64-bit integers in 'a' and 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] + b[i+63:i]
//			ELSE
//				dst[i+63:i] :=0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDQ'. Intrinsic: '_mm256_maskz_add_epi64'.
// Requires AVX512F.
func MaskzAddEpi641(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzAddEpi641(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzAddEpi641(k uint8, a [32]byte, b [32]byte) [32]byte


// AddEpi64: Add packed 64-bit integers in 'a' and 'b', and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := a[i+63:i] + b[i+63:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPADDQ'. Intrinsic: '_mm512_add_epi64'.
// Requires AVX512F.
func AddEpi64(a M512i, b M512i) M512i {
	return M512i(addEpi64([64]byte(a), [64]byte(b)))
}

func addEpi64(a [64]byte, b [64]byte) [64]byte


// MaskAddEpi642: Add packed 64-bit integers in 'a' and 'b', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] + b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPADDQ'. Intrinsic: '_mm512_mask_add_epi64'.
// Requires AVX512F.
func MaskAddEpi642(src M512i, k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskAddEpi642([64]byte(src), uint8(k), [64]byte(a), [64]byte(b)))
}

func maskAddEpi642(src [64]byte, k uint8, a [64]byte, b [64]byte) [64]byte


// MaskzAddEpi642: Add packed 64-bit integers in 'a' and 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] + b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPADDQ'. Intrinsic: '_mm512_maskz_add_epi64'.
// Requires AVX512F.
func MaskzAddEpi642(k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskzAddEpi642(uint8(k), [64]byte(a), [64]byte(b)))
}

func maskzAddEpi642(k uint8, a [64]byte, b [64]byte) [64]byte


// MaskzAddPd: Add packed double-precision (64-bit) floating-point elements in
// 'a' and 'b', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] + b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VADDPD'. Intrinsic: '_mm512_maskz_add_pd'.
// Requires AVX512F.
func MaskzAddPd(k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskzAddPd(uint8(k), [8]float64(a), [8]float64(b)))
}

func maskzAddPd(k uint8, a [8]float64, b [8]float64) [8]float64


// MaskzAddPs: Add packed single-precision (32-bit) floating-point elements in
// 'a' and 'b', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] + b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VADDPS'. Intrinsic: '_mm512_maskz_add_ps'.
// Requires AVX512F.
func MaskzAddPs(k Mmask16, a M512, b M512) M512 {
	return M512(maskzAddPs(uint16(k), [16]float32(a), [16]float32(b)))
}

func maskzAddPs(k uint16, a [16]float32, b [16]float32) [16]float32


// MaskzAddRoundPd: Add packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] + b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VADDPD'. Intrinsic: '_mm512_maskz_add_round_pd'.
// Requires AVX512F.
func MaskzAddRoundPd(k Mmask8, a M512d, b M512d, rounding int) M512d {
	return M512d(maskzAddRoundPd(uint8(k), [8]float64(a), [8]float64(b), rounding))
}

func maskzAddRoundPd(k uint8, a [8]float64, b [8]float64, rounding int) [8]float64


// MaskzAddRoundPs: Add packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] + b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VADDPS'. Intrinsic: '_mm512_maskz_add_round_ps'.
// Requires AVX512F.
func MaskzAddRoundPs(k Mmask16, a M512, b M512, rounding int) M512 {
	return M512(maskzAddRoundPs(uint16(k), [16]float32(a), [16]float32(b), rounding))
}

func maskzAddRoundPs(k uint16, a [16]float32, b [16]float32, rounding int) [16]float32


// AddRoundSd: Add the lower double-precision (64-bit) floating-point element
// in 'a' and 'b', store the result in the lower element of 'dst', and copy the
// upper element from 'a' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := a[63:0] + b[63:0]
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VADDSD'. Intrinsic: '_mm_add_round_sd'.
// Requires AVX512F.
func AddRoundSd(a M128d, b M128d, rounding int) M128d {
	return M128d(addRoundSd([2]float64(a), [2]float64(b), rounding))
}

func addRoundSd(a [2]float64, b [2]float64, rounding int) [2]float64


// MaskAddRoundSd: Add the lower double-precision (64-bit) floating-point
// element in 'a' and 'b', store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'src' when mask bit 0 is not set),
// and copy the upper element from 'a' to the upper element of 'dst'.
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := a[63:0] + b[63:0]
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VADDSD'. Intrinsic: '_mm_mask_add_round_sd'.
// Requires AVX512F.
func MaskAddRoundSd(src M128d, k Mmask8, a M128d, b M128d, rounding int) M128d {
	return M128d(maskAddRoundSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), rounding))
}

func maskAddRoundSd(src [2]float64, k uint8, a [2]float64, b [2]float64, rounding int) [2]float64


// MaskzAddRoundSd: Add the lower double-precision (64-bit) floating-point
// element in 'a' and 'b', store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper element from 'a' to the upper element of 'dst'.
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := a[63:0] + b[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VADDSD'. Intrinsic: '_mm_maskz_add_round_sd'.
// Requires AVX512F.
func MaskzAddRoundSd(k Mmask8, a M128d, b M128d, rounding int) M128d {
	return M128d(maskzAddRoundSd(uint8(k), [2]float64(a), [2]float64(b), rounding))
}

func maskzAddRoundSd(k uint8, a [2]float64, b [2]float64, rounding int) [2]float64


// AddRoundSs: Add the lower single-precision (32-bit) floating-point element
// in 'a' and 'b', store the result in the lower element of 'dst', and copy the
// upper 3 packed elements from 'a' to the upper elements of 'dst'.
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := a[31:0] + b[31:0]
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VADDSS'. Intrinsic: '_mm_add_round_ss'.
// Requires AVX512F.
func AddRoundSs(a M128, b M128, rounding int) M128 {
	return M128(addRoundSs([4]float32(a), [4]float32(b), rounding))
}

func addRoundSs(a [4]float32, b [4]float32, rounding int) [4]float32


// MaskAddRoundSs: Add the lower single-precision (32-bit) floating-point
// element in 'a' and 'b', store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'src' when mask bit 0 is not set),
// and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'. 
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := a[31:0] + b[31:0]
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VADDSS'. Intrinsic: '_mm_mask_add_round_ss'.
// Requires AVX512F.
func MaskAddRoundSs(src M128, k Mmask8, a M128, b M128, rounding int) M128 {
	return M128(maskAddRoundSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), rounding))
}

func maskAddRoundSs(src [4]float32, k uint8, a [4]float32, b [4]float32, rounding int) [4]float32


// MaskzAddRoundSs: Add the lower single-precision (32-bit) floating-point
// element in 'a' and 'b', store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := a[31:0] + b[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VADDSS'. Intrinsic: '_mm_maskz_add_round_ss'.
// Requires AVX512F.
func MaskzAddRoundSs(k Mmask8, a M128, b M128, rounding int) M128 {
	return M128(maskzAddRoundSs(uint8(k), [4]float32(a), [4]float32(b), rounding))
}

func maskzAddRoundSs(k uint8, a [4]float32, b [4]float32, rounding int) [4]float32


// MaskAddSd: Add the lower double-precision (64-bit) floating-point element in
// 'a' and 'b', store the result in the lower element of 'dst' using writemask
// 'k' (the element is copied from 'src' when mask bit 0 is not set), and copy
// the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := a[63:0] + b[63:0]
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VADDSD'. Intrinsic: '_mm_mask_add_sd'.
// Requires AVX512F.
func MaskAddSd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskAddSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskAddSd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzAddSd: Add the lower double-precision (64-bit) floating-point element
// in 'a' and 'b', store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := a[63:0] + b[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VADDSD'. Intrinsic: '_mm_maskz_add_sd'.
// Requires AVX512F.
func MaskzAddSd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzAddSd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzAddSd(k uint8, a [2]float64, b [2]float64) [2]float64


// MaskAddSs: Add the lower single-precision (32-bit) floating-point element in
// 'a' and 'b', store the result in the lower element of 'dst' using writemask
// 'k' (the element is copied from 'src' when mask bit 0 is not set), and copy
// the upper 3 packed elements from 'a' to the upper elements of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := a[31:0] + b[31:0]
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VADDSS'. Intrinsic: '_mm_mask_add_ss'.
// Requires AVX512F.
func MaskAddSs(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskAddSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskAddSs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzAddSs: Add the lower single-precision (32-bit) floating-point element
// in 'a' and 'b', store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := a[31:0] + b[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VADDSS'. Intrinsic: '_mm_maskz_add_ss'.
// Requires AVX512F.
func MaskzAddSs(k Mmask8, a M128, b M128) M128 {
	return M128(maskzAddSs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzAddSs(k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzAlignrEpi32: Concatenate 'a' and 'b' into a 128-byte immediate result,
// shift the result right by 'count' 32-bit elements, and stores the low 64
// bytes (16 elements) in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		temp[1023:512] := a[511:0]
//		temp[511:0] := b[511:0]
//		temp[1023:0] := temp[1023:0] >> (32*count)
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := temp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VALIGND'. Intrinsic: '_mm512_maskz_alignr_epi32'.
// Requires AVX512F.
func MaskzAlignrEpi32(k Mmask16, a M512i, b M512i, count int) M512i {
	return M512i(maskzAlignrEpi32(uint16(k), [64]byte(a), [64]byte(b), count))
}

func maskzAlignrEpi32(k uint16, a [64]byte, b [64]byte, count int) [64]byte


// AlignrEpi64: Concatenate 'a' and 'b' into a 128-byte immediate result, shift
// the result right by 'count' 64-bit elements, and store the low 64 bytes (8
// elements) in 'dst'. 
//
//		temp[1023:512] := a[511:0]
//		temp[511:0] := b[511:0]
//		temp[1023:0] := temp[1023:0] >> (64*count)
//		dst[511:0] := temp[511:0]
//		dst[MAX:512] := 0
//
// Instruction: 'VALIGNQ'. Intrinsic: '_mm512_alignr_epi64'.
// Requires AVX512F.
func AlignrEpi64(a M512i, b M512i, count int) M512i {
	return M512i(alignrEpi64([64]byte(a), [64]byte(b), count))
}

func alignrEpi64(a [64]byte, b [64]byte, count int) [64]byte


// MaskAlignrEpi64: Concatenate 'a' and 'b' into a 128-byte immediate result,
// shift the result right by 'count' 64-bit elements, and store the low 64
// bytes (8 elements) in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		temp[1023:512] := a[511:0]
//		temp[511:0] := b[511:0]
//		temp[1023:0] := temp[1023:0] >> (64*count)
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := temp[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VALIGNQ'. Intrinsic: '_mm512_mask_alignr_epi64'.
// Requires AVX512F.
func MaskAlignrEpi64(src M512i, k Mmask8, a M512i, b M512i, count int) M512i {
	return M512i(maskAlignrEpi64([64]byte(src), uint8(k), [64]byte(a), [64]byte(b), count))
}

func maskAlignrEpi64(src [64]byte, k uint8, a [64]byte, b [64]byte, count int) [64]byte


// MaskzAlignrEpi64: Concatenate 'a' and 'b' into a 128-byte immediate result,
// shift the result right by 'count' 64-bit elements, and stores the low 64
// bytes (8 elements) in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		temp[1023:512] := a[511:0]
//		temp[511:0] := b[511:0]
//		temp[1023:0] := temp[1023:0] >> (64*count)
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := temp[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VALIGNQ'. Intrinsic: '_mm512_maskz_alignr_epi64'.
// Requires AVX512F.
func MaskzAlignrEpi64(k Mmask8, a M512i, b M512i, count int) M512i {
	return M512i(maskzAlignrEpi64(uint8(k), [64]byte(a), [64]byte(b), count))
}

func maskzAlignrEpi64(k uint8, a [64]byte, b [64]byte, count int) [64]byte


// MaskAndEpi32: Compute the bitwise AND of packed 32-bit integers in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] AND b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPANDD'. Intrinsic: '_mm_mask_and_epi32'.
// Requires AVX512F.
func MaskAndEpi32(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskAndEpi32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskAndEpi32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzAndEpi32: Compute the bitwise AND of packed 32-bit integers in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] AND b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPANDD'. Intrinsic: '_mm_maskz_and_epi32'.
// Requires AVX512F.
func MaskzAndEpi32(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzAndEpi32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzAndEpi32(k uint8, a [16]byte, b [16]byte) [16]byte


// MaskAndEpi321: Compute the bitwise AND of packed 32-bit integers in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] BITWISE AND b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPANDD'. Intrinsic: '_mm256_mask_and_epi32'.
// Requires AVX512F.
func MaskAndEpi321(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskAndEpi321([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskAndEpi321(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzAndEpi321: Compute the bitwise AND of packed 32-bit integers in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] BITWISE AND b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPANDD'. Intrinsic: '_mm256_maskz_and_epi32'.
// Requires AVX512F.
func MaskzAndEpi321(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzAndEpi321(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzAndEpi321(k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzAndEpi322: Compute the bitwise AND of packed 32-bit integers in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] AND b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPANDD'. Intrinsic: '_mm512_maskz_and_epi32'.
// Requires AVX512F.
func MaskzAndEpi322(k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskzAndEpi322(uint16(k), [64]byte(a), [64]byte(b)))
}

func maskzAndEpi322(k uint16, a [64]byte, b [64]byte) [64]byte


// MaskAndEpi64: Compute the bitwise AND of packed 64-bit integers in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] AND b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPANDQ'. Intrinsic: '_mm_mask_and_epi64'.
// Requires AVX512F.
func MaskAndEpi64(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskAndEpi64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskAndEpi64(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzAndEpi64: Compute the bitwise AND of packed 64-bit integers in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] AND b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPANDQ'. Intrinsic: '_mm_maskz_and_epi64'.
// Requires AVX512F.
func MaskzAndEpi64(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzAndEpi64(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzAndEpi64(k uint8, a [16]byte, b [16]byte) [16]byte


// MaskAndEpi641: Compute the bitwise AND of packed 64-bit integers in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] AND b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPANDQ'. Intrinsic: '_mm256_mask_and_epi64'.
// Requires AVX512F.
func MaskAndEpi641(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskAndEpi641([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskAndEpi641(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzAndEpi641: Compute the bitwise AND of packed 64-bit integers in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] AND b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPANDQ'. Intrinsic: '_mm256_maskz_and_epi64'.
// Requires AVX512F.
func MaskzAndEpi641(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzAndEpi641(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzAndEpi641(k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzAndEpi642: Compute the bitwise AND of packed 64-bit integers in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] AND b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPANDQ'. Intrinsic: '_mm512_maskz_and_epi64'.
// Requires AVX512F.
func MaskzAndEpi642(k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskzAndEpi642(uint8(k), [64]byte(a), [64]byte(b)))
}

func maskzAndEpi642(k uint8, a [64]byte, b [64]byte) [64]byte


// MaskAndnotEpi32: Compute the bitwise AND NOT of packed 32-bit integers in
// 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ((NOT a[i+31:i]) AND b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPANDND'. Intrinsic: '_mm_mask_andnot_epi32'.
// Requires AVX512F.
func MaskAndnotEpi32(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskAndnotEpi32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskAndnotEpi32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzAndnotEpi32: Compute the bitwise AND NOT of packed 32-bit integers in
// 'a' and 'b', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (NOT a[i+31:i]) AND b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPANDND'. Intrinsic: '_mm_maskz_andnot_epi32'.
// Requires AVX512F.
func MaskzAndnotEpi32(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzAndnotEpi32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzAndnotEpi32(k uint8, a [16]byte, b [16]byte) [16]byte


// MaskAndnotEpi321: Compute the bitwise AND NOT of packed 32-bit integers in
// 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ((NOT a[i+31:i]) AND b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPANDND'. Intrinsic: '_mm256_mask_andnot_epi32'.
// Requires AVX512F.
func MaskAndnotEpi321(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskAndnotEpi321([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskAndnotEpi321(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzAndnotEpi321: Compute the bitwise AND NOT of packed 32-bit integers in
// 'a' and 'b', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (NOT a[i+31:i]) AND b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPANDND'. Intrinsic: '_mm256_maskz_andnot_epi32'.
// Requires AVX512F.
func MaskzAndnotEpi321(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzAndnotEpi321(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzAndnotEpi321(k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzAndnotEpi322: Compute the bitwise AND NOT of packed 32-bit integers in
// 'a' and 'b', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (NOT a[i+31:i]) AND b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPANDND'. Intrinsic: '_mm512_maskz_andnot_epi32'.
// Requires AVX512F.
func MaskzAndnotEpi322(k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskzAndnotEpi322(uint16(k), [64]byte(a), [64]byte(b)))
}

func maskzAndnotEpi322(k uint16, a [64]byte, b [64]byte) [64]byte


// MaskAndnotEpi64: Compute the bitwise AND NOT of packed 64-bit integers in
// 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ((NOT a[i+63:i]) AND b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPANDNQ'. Intrinsic: '_mm_mask_andnot_epi64'.
// Requires AVX512F.
func MaskAndnotEpi64(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskAndnotEpi64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskAndnotEpi64(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzAndnotEpi64: Compute the bitwise AND NOT of packed 64-bit integers in
// 'a' and 'b', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (NOT a[i+63:i]) AND b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPANDNQ'. Intrinsic: '_mm_maskz_andnot_epi64'.
// Requires AVX512F.
func MaskzAndnotEpi64(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzAndnotEpi64(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzAndnotEpi64(k uint8, a [16]byte, b [16]byte) [16]byte


// MaskAndnotEpi641: Compute the bitwise AND NOT of packed 64-bit integers in
// 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ((NOT a[i+63:i]) AND b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPANDNQ'. Intrinsic: '_mm256_mask_andnot_epi64'.
// Requires AVX512F.
func MaskAndnotEpi641(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskAndnotEpi641([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskAndnotEpi641(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzAndnotEpi641: Compute the bitwise AND NOT of packed 64-bit integers in
// 'a' and 'b', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (NOT a[i+63:i]) AND b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPANDNQ'. Intrinsic: '_mm256_maskz_andnot_epi64'.
// Requires AVX512F.
func MaskzAndnotEpi641(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzAndnotEpi641(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzAndnotEpi641(k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzAndnotEpi642: Compute the bitwise AND NOT of packed 64-bit integers in
// 'a' and 'b', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (NOT a[i+63:i]) AND b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPANDNQ'. Intrinsic: '_mm512_maskz_andnot_epi64'.
// Requires AVX512F.
func MaskzAndnotEpi642(k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskzAndnotEpi642(uint8(k), [64]byte(a), [64]byte(b)))
}

func maskzAndnotEpi642(k uint8, a [64]byte, b [64]byte) [64]byte


// AsinPd: Compute the inverse sine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ASIN(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_asin_pd'.
// Requires AVX512F.
func AsinPd(a M512d) M512d {
	return M512d(asinPd([8]float64(a)))
}

func asinPd(a [8]float64) [8]float64


// MaskAsinPd: Compute the inverse sine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ASIN(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_asin_pd'.
// Requires AVX512F.
func MaskAsinPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskAsinPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskAsinPd(src [8]float64, k uint8, a [8]float64) [8]float64


// AsinPs: Compute the inverse sine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ASIN(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_asin_ps'.
// Requires AVX512F.
func AsinPs(a M512) M512 {
	return M512(asinPs([16]float32(a)))
}

func asinPs(a [16]float32) [16]float32


// MaskAsinPs: Compute the inverse sine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ASIN(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_asin_ps'.
// Requires AVX512F.
func MaskAsinPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskAsinPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskAsinPs(src [16]float32, k uint16, a [16]float32) [16]float32


// AsinhPd: Compute the inverse hyperbolic sine of packed double-precision
// (64-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ASINH(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_asinh_pd'.
// Requires AVX512F.
func AsinhPd(a M512d) M512d {
	return M512d(asinhPd([8]float64(a)))
}

func asinhPd(a [8]float64) [8]float64


// MaskAsinhPd: Compute the inverse hyperbolic sine of packed double-precision
// (64-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ASINH(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_asinh_pd'.
// Requires AVX512F.
func MaskAsinhPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskAsinhPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskAsinhPd(src [8]float64, k uint8, a [8]float64) [8]float64


// AsinhPs: Compute the inverse hyperbolic sine of packed single-precision
// (32-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ASINH(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_asinh_ps'.
// Requires AVX512F.
func AsinhPs(a M512) M512 {
	return M512(asinhPs([16]float32(a)))
}

func asinhPs(a [16]float32) [16]float32


// MaskAsinhPs: Compute the inverse hyperbolic sine of packed single-precision
// (32-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ASINH(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_asinh_ps'.
// Requires AVX512F.
func MaskAsinhPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskAsinhPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskAsinhPs(src [16]float32, k uint16, a [16]float32) [16]float32


// AtanPd: Compute the inverse tangent of packed double-precision (64-bit)
// floating-point elements in 'a' and store the results in 'dst' expressed in
// radians. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ATAN(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_atan_pd'.
// Requires AVX512F.
func AtanPd(a M512d) M512d {
	return M512d(atanPd([8]float64(a)))
}

func atanPd(a [8]float64) [8]float64


// MaskAtanPd: Compute the inverse tangent of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' expressed in
// radians using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ATAN(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_atan_pd'.
// Requires AVX512F.
func MaskAtanPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskAtanPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskAtanPd(src [8]float64, k uint8, a [8]float64) [8]float64


// AtanPs: Compute the inverse tangent of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' expressed in
// radians. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ATAN(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_atan_ps'.
// Requires AVX512F.
func AtanPs(a M512) M512 {
	return M512(atanPs([16]float32(a)))
}

func atanPs(a [16]float32) [16]float32


// MaskAtanPs: Compute the inverse tangent of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ATAN(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_atan_ps'.
// Requires AVX512F.
func MaskAtanPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskAtanPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskAtanPs(src [16]float32, k uint16, a [16]float32) [16]float32


// Atan2Pd: Compute the inverse tangent of packed double-precision (64-bit)
// floating-point elements in 'a' divided by packed elements in 'b', and store
// the results in 'dst' expressed in radians. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ATAN(a[i+63:i] / b[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_atan2_pd'.
// Requires AVX512F.
func Atan2Pd(a M512d, b M512d) M512d {
	return M512d(atan2Pd([8]float64(a), [8]float64(b)))
}

func atan2Pd(a [8]float64, b [8]float64) [8]float64


// MaskAtan2Pd: Compute the inverse tangent of packed double-precision (64-bit)
// floating-point elements in 'a' divided by packed elements in 'b', and store
// the results in 'dst' expressed in radians using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ATAN(a[i+63:i] / b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_atan2_pd'.
// Requires AVX512F.
func MaskAtan2Pd(src M512d, k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskAtan2Pd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b)))
}

func maskAtan2Pd(src [8]float64, k uint8, a [8]float64, b [8]float64) [8]float64


// Atan2Ps: Compute the inverse tangent of packed single-precision (32-bit)
// floating-point elements in 'a' divided by packed elements in 'b', and store
// the results in 'dst' expressed in radians. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ATAN(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_atan2_ps'.
// Requires AVX512F.
func Atan2Ps(a M512, b M512) M512 {
	return M512(atan2Ps([16]float32(a), [16]float32(b)))
}

func atan2Ps(a [16]float32, b [16]float32) [16]float32


// MaskAtan2Ps: Compute the inverse tangent of packed single-precision (32-bit)
// floating-point elements in 'a' divided by packed elements in 'b', and store
// the results in 'dst' expressed in radians using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ATAN(a[i+31:i] / b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_atan2_ps'.
// Requires AVX512F.
func MaskAtan2Ps(src M512, k Mmask16, a M512, b M512) M512 {
	return M512(maskAtan2Ps([16]float32(src), uint16(k), [16]float32(a), [16]float32(b)))
}

func maskAtan2Ps(src [16]float32, k uint16, a [16]float32, b [16]float32) [16]float32


// AtanhPd: Compute the inverse hyperbolic tangent of packed double-precision
// (64-bit) floating-point elements in 'a' and store the results in 'dst'
// expressed in radians. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ATANH(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_atanh_pd'.
// Requires AVX512F.
func AtanhPd(a M512d) M512d {
	return M512d(atanhPd([8]float64(a)))
}

func atanhPd(a [8]float64) [8]float64


// MaskAtanhPd: Compute the inverse hyperbolic tangent of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst' expressed in radians using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ATANH(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_atanh_pd'.
// Requires AVX512F.
func MaskAtanhPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskAtanhPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskAtanhPd(src [8]float64, k uint8, a [8]float64) [8]float64


// AtanhPs: Compute the inverse hyperblic tangent of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'
// expressed in radians. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ATANH(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_atanh_ps'.
// Requires AVX512F.
func AtanhPs(a M512) M512 {
	return M512(atanhPs([16]float32(a)))
}

func atanhPs(a [16]float32) [16]float32


// MaskAtanhPs: Compute the inverse hyperbolic tangent of packed
// single-precision (32-bit) floating-point elements in 'a' expressed in
// radians, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ATANH(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_atanh_ps'.
// Requires AVX512F.
func MaskAtanhPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskAtanhPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskAtanhPs(src [16]float32, k uint16, a [16]float32) [16]float32


// MaskBlendEpi32: Blend packed 32-bit integers from 'a' and 'b' using control
// mask 'k', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := b[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBLENDMD'. Intrinsic: '_mm_mask_blend_epi32'.
// Requires AVX512F.
func MaskBlendEpi32(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskBlendEpi32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskBlendEpi32(k uint8, a [16]byte, b [16]byte) [16]byte


// MaskBlendEpi321: Blend packed 32-bit integers from 'a' and 'b' using control
// mask 'k', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := b[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBLENDMD'. Intrinsic: '_mm256_mask_blend_epi32'.
// Requires AVX512F.
func MaskBlendEpi321(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskBlendEpi321(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskBlendEpi321(k uint8, a [32]byte, b [32]byte) [32]byte


// MaskBlendEpi64: Blend packed 64-bit integers from 'a' and 'b' using control
// mask 'k', and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := b[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBLENDMQ'. Intrinsic: '_mm_mask_blend_epi64'.
// Requires AVX512F.
func MaskBlendEpi64(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskBlendEpi64(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskBlendEpi64(k uint8, a [16]byte, b [16]byte) [16]byte


// MaskBlendEpi641: Blend packed 64-bit integers from 'a' and 'b' using control
// mask 'k', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := b[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBLENDMQ'. Intrinsic: '_mm256_mask_blend_epi64'.
// Requires AVX512F.
func MaskBlendEpi641(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskBlendEpi641(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskBlendEpi641(k uint8, a [32]byte, b [32]byte) [32]byte


// MaskBlendPd: Blend packed double-precision (64-bit) floating-point elements
// from 'a' and 'b' using control mask 'k', and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := b[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VBLENDMPD'. Intrinsic: '_mm_mask_blend_pd'.
// Requires AVX512F.
func MaskBlendPd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskBlendPd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskBlendPd(k uint8, a [2]float64, b [2]float64) [2]float64


// MaskBlendPd1: Blend packed double-precision (64-bit) floating-point elements
// from 'a' and 'b' using control mask 'k', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := b[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBLENDMPD'. Intrinsic: '_mm256_mask_blend_pd'.
// Requires AVX512F.
func MaskBlendPd1(k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskBlendPd1(uint8(k), [4]float64(a), [4]float64(b)))
}

func maskBlendPd1(k uint8, a [4]float64, b [4]float64) [4]float64


// MaskBlendPs: Blend packed single-precision (32-bit) floating-point elements
// from 'a' and 'b' using control mask 'k', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := b[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VBLENDMPS'. Intrinsic: '_mm_mask_blend_ps'.
// Requires AVX512F.
func MaskBlendPs(k Mmask8, a M128, b M128) M128 {
	return M128(maskBlendPs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskBlendPs(k uint8, a [4]float32, b [4]float32) [4]float32


// MaskBlendPs1: Blend packed single-precision (32-bit) floating-point elements
// from 'a' and 'b' using control mask 'k', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := b[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBLENDMPS'. Intrinsic: '_mm256_mask_blend_ps'.
// Requires AVX512F.
func MaskBlendPs1(k Mmask8, a M256, b M256) M256 {
	return M256(maskBlendPs1(uint8(k), [8]float32(a), [8]float32(b)))
}

func maskBlendPs1(k uint8, a [8]float32, b [8]float32) [8]float32


// BroadcastF32x4: Broadcast the 4 packed single-precision (32-bit)
// floating-point elements from 'a' to all elements of 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			n := (j mod 4)*32
//			dst[i+31:i] := a[n+31:n]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTF32X4'. Intrinsic: '_mm256_broadcast_f32x4'.
// Requires AVX512F.
func BroadcastF32x4(a M128) M256 {
	return M256(broadcastF32x4([4]float32(a)))
}

func broadcastF32x4(a [4]float32) [8]float32


// MaskBroadcastF32x4: Broadcast the 4 packed single-precision (32-bit)
// floating-point elements from 'a' to all elements of 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			n := (j mod 4)*32
//			IF k[j]
//				dst[i+31:i] := a[n+31:n]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTF32X4'. Intrinsic: '_mm256_mask_broadcast_f32x4'.
// Requires AVX512F.
func MaskBroadcastF32x4(src M256, k Mmask8, a M128) M256 {
	return M256(maskBroadcastF32x4([8]float32(src), uint8(k), [4]float32(a)))
}

func maskBroadcastF32x4(src [8]float32, k uint8, a [4]float32) [8]float32


// MaskzBroadcastF32x4: Broadcast the 4 packed single-precision (32-bit)
// floating-point elements from 'a' to all elements of 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			n := (j mod 4)*32
//			IF k[j]
//				dst[i+31:i] := a[n+31:n]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTF32X4'. Intrinsic: '_mm256_maskz_broadcast_f32x4'.
// Requires AVX512F.
func MaskzBroadcastF32x4(k Mmask8, a M128) M256 {
	return M256(maskzBroadcastF32x4(uint8(k), [4]float32(a)))
}

func maskzBroadcastF32x4(k uint8, a [4]float32) [8]float32


// BroadcastF32x41: Broadcast the 4 packed single-precision (32-bit)
// floating-point elements from 'a' to all elements of 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			n := (j mod 4)*32
//			dst[i+31:i] := a[n+31:n]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTF32X4'. Intrinsic: '_mm512_broadcast_f32x4'.
// Requires AVX512F.
func BroadcastF32x41(a M128) M512 {
	return M512(broadcastF32x41([4]float32(a)))
}

func broadcastF32x41(a [4]float32) [16]float32


// MaskBroadcastF32x41: Broadcast the 4 packed single-precision (32-bit)
// floating-point elements from 'a' to all elements of 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			n := (j mod 4)*32
//			IF k[j]
//				dst[i+31:i] := a[n+31:n]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTF32X4'. Intrinsic: '_mm512_mask_broadcast_f32x4'.
// Requires AVX512F.
func MaskBroadcastF32x41(src M512, k Mmask16, a M128) M512 {
	return M512(maskBroadcastF32x41([16]float32(src), uint16(k), [4]float32(a)))
}

func maskBroadcastF32x41(src [16]float32, k uint16, a [4]float32) [16]float32


// MaskzBroadcastF32x41: Broadcast the 4 packed single-precision (32-bit)
// floating-point elements from 'a' to all elements of 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			n := (j mod 4)*32
//			IF k[j]
//				dst[i+31:i] := a[n+31:n]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTF32X4'. Intrinsic: '_mm512_maskz_broadcast_f32x4'.
// Requires AVX512F.
func MaskzBroadcastF32x41(k Mmask16, a M128) M512 {
	return M512(maskzBroadcastF32x41(uint16(k), [4]float32(a)))
}

func maskzBroadcastF32x41(k uint16, a [4]float32) [16]float32


// BroadcastF64x4: Broadcast the 4 packed double-precision (64-bit)
// floating-point elements from 'a' to all elements of 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			n := (j mod 4)*64
//			dst[i+63:i] := a[n+63:n]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTF64X4'. Intrinsic: '_mm512_broadcast_f64x4'.
// Requires AVX512F.
func BroadcastF64x4(a M256d) M512d {
	return M512d(broadcastF64x4([4]float64(a)))
}

func broadcastF64x4(a [4]float64) [8]float64


// MaskBroadcastF64x4: Broadcast the 4 packed double-precision (64-bit)
// floating-point elements from 'a' to all elements of 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			n := (j mod 4)*64
//			IF k[j]
//				dst[i+63:i] := a[n+63:n]
//			ELSE
//				dst[i+63:i] := src[n+63:n]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTF64X4'. Intrinsic: '_mm512_mask_broadcast_f64x4'.
// Requires AVX512F.
func MaskBroadcastF64x4(src M512d, k Mmask8, a M256d) M512d {
	return M512d(maskBroadcastF64x4([8]float64(src), uint8(k), [4]float64(a)))
}

func maskBroadcastF64x4(src [8]float64, k uint8, a [4]float64) [8]float64


// MaskzBroadcastF64x4: Broadcast the 4 packed double-precision (64-bit)
// floating-point elements from 'a' to all elements of 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			n := (j mod 4)*64
//			IF k[j]
//				dst[i+63:i] := a[n+63:n]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTF64X4'. Intrinsic: '_mm512_maskz_broadcast_f64x4'.
// Requires AVX512F.
func MaskzBroadcastF64x4(k Mmask8, a M256d) M512d {
	return M512d(maskzBroadcastF64x4(uint8(k), [4]float64(a)))
}

func maskzBroadcastF64x4(k uint8, a [4]float64) [8]float64


// BroadcastI32x4: Broadcast the 4 packed 32-bit integers from 'a' to all
// elements of 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			n := (j mod 4)*32
//			dst[i+31:i] := a[n+31:n]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTI32X4'. Intrinsic: '_mm256_broadcast_i32x4'.
// Requires AVX512F.
func BroadcastI32x4(a M128i) M256i {
	return M256i(broadcastI32x4([16]byte(a)))
}

func broadcastI32x4(a [16]byte) [32]byte


// MaskBroadcastI32x4: Broadcast the 4 packed 32-bit integers from 'a' to all
// elements of 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			n := (j mod 4)*32
//			IF k[j]
//				dst[i+31:i] := a[n+31:n]
//			ELSE
//				dst[i+31:i] := src[n+31:n]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTI32X4'. Intrinsic: '_mm256_mask_broadcast_i32x4'.
// Requires AVX512F.
func MaskBroadcastI32x4(src M256i, k Mmask8, a M128i) M256i {
	return M256i(maskBroadcastI32x4([32]byte(src), uint8(k), [16]byte(a)))
}

func maskBroadcastI32x4(src [32]byte, k uint8, a [16]byte) [32]byte


// MaskzBroadcastI32x4: Broadcast the 4 packed 32-bit integers from 'a' to all
// elements of 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			n := (j mod 4)*32
//			IF k[j]
//				dst[i+31:i] := a[n+31:n]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTI32X4'. Intrinsic: '_mm256_maskz_broadcast_i32x4'.
// Requires AVX512F.
func MaskzBroadcastI32x4(k Mmask8, a M128i) M256i {
	return M256i(maskzBroadcastI32x4(uint8(k), [16]byte(a)))
}

func maskzBroadcastI32x4(k uint8, a [16]byte) [32]byte


// BroadcastI32x41: Broadcast the 4 packed 32-bit integers from 'a' to all
// elements of 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			n := (j mod 4)*32
//			dst[i+31:i] := a[n+31:n]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTI32X4'. Intrinsic: '_mm512_broadcast_i32x4'.
// Requires AVX512F.
func BroadcastI32x41(a M128i) M512i {
	return M512i(broadcastI32x41([16]byte(a)))
}

func broadcastI32x41(a [16]byte) [64]byte


// MaskBroadcastI32x41: Broadcast the 4 packed 32-bit integers from 'a' to all
// elements of 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			n := (j mod 4)*32
//			IF k[j]
//				dst[i+31:i] := a[n+31:n]
//			ELSE
//				dst[i+31:i] := src[n+31:n]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTI32X4'. Intrinsic: '_mm512_mask_broadcast_i32x4'.
// Requires AVX512F.
func MaskBroadcastI32x41(src M512i, k Mmask16, a M128i) M512i {
	return M512i(maskBroadcastI32x41([64]byte(src), uint16(k), [16]byte(a)))
}

func maskBroadcastI32x41(src [64]byte, k uint16, a [16]byte) [64]byte


// MaskzBroadcastI32x41: Broadcast the 4 packed 32-bit integers from 'a' to all
// elements of 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			n := (j mod 4)*32
//			IF k[j]
//				dst[i+31:i] := a[n+31:n]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTI32X4'. Intrinsic: '_mm512_maskz_broadcast_i32x4'.
// Requires AVX512F.
func MaskzBroadcastI32x41(k Mmask16, a M128i) M512i {
	return M512i(maskzBroadcastI32x41(uint16(k), [16]byte(a)))
}

func maskzBroadcastI32x41(k uint16, a [16]byte) [64]byte


// BroadcastI64x4: Broadcast the 4 packed 64-bit integers from 'a' to all
// elements of 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			n := (j mod 4)*64
//			dst[i+63:i] := a[n+63:n]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTI64X4'. Intrinsic: '_mm512_broadcast_i64x4'.
// Requires AVX512F.
func BroadcastI64x4(a M256i) M512i {
	return M512i(broadcastI64x4([32]byte(a)))
}

func broadcastI64x4(a [32]byte) [64]byte


// MaskBroadcastI64x4: Broadcast the 4 packed 64-bit integers from 'a' to all
// elements of 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			n := (j mod 4)*64
//			IF k[j]
//				dst[i+63:i] := a[n+63:n]
//			ELSE
//				dst[i+63:i] := src[n+63:n]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTI64X4'. Intrinsic: '_mm512_mask_broadcast_i64x4'.
// Requires AVX512F.
func MaskBroadcastI64x4(src M512i, k Mmask8, a M256i) M512i {
	return M512i(maskBroadcastI64x4([64]byte(src), uint8(k), [32]byte(a)))
}

func maskBroadcastI64x4(src [64]byte, k uint8, a [32]byte) [64]byte


// MaskzBroadcastI64x4: Broadcast the 4 packed 64-bit integers from 'a' to all
// elements of 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			n := (j mod 4)*64
//			IF k[j]
//				dst[i+63:i] := a[n+63:n]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTI64X4'. Intrinsic: '_mm512_maskz_broadcast_i64x4'.
// Requires AVX512F.
func MaskzBroadcastI64x4(k Mmask8, a M256i) M512i {
	return M512i(maskzBroadcastI64x4(uint8(k), [32]byte(a)))
}

func maskzBroadcastI64x4(k uint8, a [32]byte) [64]byte


// MaskBroadcastdEpi32: Broadcast the low packed 32-bit integer from 'a' to all
// elements of 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm_mask_broadcastd_epi32'.
// Requires AVX512F.
func MaskBroadcastdEpi32(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskBroadcastdEpi32([16]byte(src), uint8(k), [16]byte(a)))
}

func maskBroadcastdEpi32(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzBroadcastdEpi32: Broadcast the low packed 32-bit integer from 'a' to
// all elements of 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm_maskz_broadcastd_epi32'.
// Requires AVX512F.
func MaskzBroadcastdEpi32(k Mmask8, a M128i) M128i {
	return M128i(maskzBroadcastdEpi32(uint8(k), [16]byte(a)))
}

func maskzBroadcastdEpi32(k uint8, a [16]byte) [16]byte


// MaskBroadcastdEpi321: Broadcast the low packed 32-bit integer from 'a' to
// all elements of 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm256_mask_broadcastd_epi32'.
// Requires AVX512F.
func MaskBroadcastdEpi321(src M256i, k Mmask8, a M128i) M256i {
	return M256i(maskBroadcastdEpi321([32]byte(src), uint8(k), [16]byte(a)))
}

func maskBroadcastdEpi321(src [32]byte, k uint8, a [16]byte) [32]byte


// MaskzBroadcastdEpi321: Broadcast the low packed 32-bit integer from 'a' to
// all elements of 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm256_maskz_broadcastd_epi32'.
// Requires AVX512F.
func MaskzBroadcastdEpi321(k Mmask8, a M128i) M256i {
	return M256i(maskzBroadcastdEpi321(uint8(k), [16]byte(a)))
}

func maskzBroadcastdEpi321(k uint8, a [16]byte) [32]byte


// BroadcastdEpi32: Broadcast the low packed 32-bit integer from 'a' to all
// elements of 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := a[31:0]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm512_broadcastd_epi32'.
// Requires AVX512F.
func BroadcastdEpi32(a M128i) M512i {
	return M512i(broadcastdEpi32([16]byte(a)))
}

func broadcastdEpi32(a [16]byte) [64]byte


// MaskBroadcastdEpi322: Broadcast the low packed 32-bit integer from 'a' to
// all elements of 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm512_mask_broadcastd_epi32'.
// Requires AVX512F.
func MaskBroadcastdEpi322(src M512i, k Mmask16, a M128i) M512i {
	return M512i(maskBroadcastdEpi322([64]byte(src), uint16(k), [16]byte(a)))
}

func maskBroadcastdEpi322(src [64]byte, k uint16, a [16]byte) [64]byte


// MaskzBroadcastdEpi322: Broadcast the low packed 32-bit integer from 'a' to
// all elements of 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm512_maskz_broadcastd_epi32'.
// Requires AVX512F.
func MaskzBroadcastdEpi322(k Mmask16, a M128i) M512i {
	return M512i(maskzBroadcastdEpi322(uint16(k), [16]byte(a)))
}

func maskzBroadcastdEpi322(k uint16, a [16]byte) [64]byte


// MaskBroadcastqEpi64: Broadcast the low packed 64-bit integer from 'a' to all
// elements of 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm_mask_broadcastq_epi64'.
// Requires AVX512F.
func MaskBroadcastqEpi64(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskBroadcastqEpi64([16]byte(src), uint8(k), [16]byte(a)))
}

func maskBroadcastqEpi64(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzBroadcastqEpi64: Broadcast the low packed 64-bit integer from 'a' to
// all elements of 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm_maskz_broadcastq_epi64'.
// Requires AVX512F.
func MaskzBroadcastqEpi64(k Mmask8, a M128i) M128i {
	return M128i(maskzBroadcastqEpi64(uint8(k), [16]byte(a)))
}

func maskzBroadcastqEpi64(k uint8, a [16]byte) [16]byte


// MaskBroadcastqEpi641: Broadcast the low packed 64-bit integer from 'a' to
// all elements of 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm256_mask_broadcastq_epi64'.
// Requires AVX512F.
func MaskBroadcastqEpi641(src M256i, k Mmask8, a M128i) M256i {
	return M256i(maskBroadcastqEpi641([32]byte(src), uint8(k), [16]byte(a)))
}

func maskBroadcastqEpi641(src [32]byte, k uint8, a [16]byte) [32]byte


// MaskzBroadcastqEpi641: Broadcast the low packed 64-bit integer from 'a' to
// all elements of 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm256_maskz_broadcastq_epi64'.
// Requires AVX512F.
func MaskzBroadcastqEpi641(k Mmask8, a M128i) M256i {
	return M256i(maskzBroadcastqEpi641(uint8(k), [16]byte(a)))
}

func maskzBroadcastqEpi641(k uint8, a [16]byte) [32]byte


// BroadcastqEpi64: Broadcast the low packed 64-bit integer from 'a' to all
// elements of 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := a[63:0]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm512_broadcastq_epi64'.
// Requires AVX512F.
func BroadcastqEpi64(a M128i) M512i {
	return M512i(broadcastqEpi64([16]byte(a)))
}

func broadcastqEpi64(a [16]byte) [64]byte


// MaskBroadcastqEpi642: Broadcast the low packed 64-bit integer from 'a' to
// all elements of 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm512_mask_broadcastq_epi64'.
// Requires AVX512F.
func MaskBroadcastqEpi642(src M512i, k Mmask8, a M128i) M512i {
	return M512i(maskBroadcastqEpi642([64]byte(src), uint8(k), [16]byte(a)))
}

func maskBroadcastqEpi642(src [64]byte, k uint8, a [16]byte) [64]byte


// MaskzBroadcastqEpi642: Broadcast the low packed 64-bit integer from 'a' to
// all elements of 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm512_maskz_broadcastq_epi64'.
// Requires AVX512F.
func MaskzBroadcastqEpi642(k Mmask8, a M128i) M512i {
	return M512i(maskzBroadcastqEpi642(uint8(k), [16]byte(a)))
}

func maskzBroadcastqEpi642(k uint8, a [16]byte) [64]byte


// MaskBroadcastsdPd: Broadcast the low double-precision (64-bit)
// floating-point element from 'a' to all elements of 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTSD'. Intrinsic: '_mm256_mask_broadcastsd_pd'.
// Requires AVX512F.
func MaskBroadcastsdPd(src M256d, k Mmask8, a M128d) M256d {
	return M256d(maskBroadcastsdPd([4]float64(src), uint8(k), [2]float64(a)))
}

func maskBroadcastsdPd(src [4]float64, k uint8, a [2]float64) [4]float64


// MaskzBroadcastsdPd: Broadcast the low double-precision (64-bit)
// floating-point element from 'a' to all elements of 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTSD'. Intrinsic: '_mm256_maskz_broadcastsd_pd'.
// Requires AVX512F.
func MaskzBroadcastsdPd(k Mmask8, a M128d) M256d {
	return M256d(maskzBroadcastsdPd(uint8(k), [2]float64(a)))
}

func maskzBroadcastsdPd(k uint8, a [2]float64) [4]float64


// BroadcastsdPd: Broadcast the low double-precision (64-bit) floating-point
// element from 'a' to all elements of 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := a[63:0]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTSD'. Intrinsic: '_mm512_broadcastsd_pd'.
// Requires AVX512F.
func BroadcastsdPd(a M128d) M512d {
	return M512d(broadcastsdPd([2]float64(a)))
}

func broadcastsdPd(a [2]float64) [8]float64


// MaskBroadcastsdPd1: Broadcast the low double-precision (64-bit)
// floating-point element from 'a' to all elements of 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTSD'. Intrinsic: '_mm512_mask_broadcastsd_pd'.
// Requires AVX512F.
func MaskBroadcastsdPd1(src M512d, k Mmask8, a M128d) M512d {
	return M512d(maskBroadcastsdPd1([8]float64(src), uint8(k), [2]float64(a)))
}

func maskBroadcastsdPd1(src [8]float64, k uint8, a [2]float64) [8]float64


// MaskzBroadcastsdPd1: Broadcast the low double-precision (64-bit)
// floating-point element from 'a' to all elements of 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTSD'. Intrinsic: '_mm512_maskz_broadcastsd_pd'.
// Requires AVX512F.
func MaskzBroadcastsdPd1(k Mmask8, a M128d) M512d {
	return M512d(maskzBroadcastsdPd1(uint8(k), [2]float64(a)))
}

func maskzBroadcastsdPd1(k uint8, a [2]float64) [8]float64


// MaskBroadcastssPs: Broadcast the low single-precision (32-bit)
// floating-point element from 'a' to all elements of 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VBROADCASTSS'. Intrinsic: '_mm_mask_broadcastss_ps'.
// Requires AVX512F.
func MaskBroadcastssPs(src M128, k Mmask8, a M128) M128 {
	return M128(maskBroadcastssPs([4]float32(src), uint8(k), [4]float32(a)))
}

func maskBroadcastssPs(src [4]float32, k uint8, a [4]float32) [4]float32


// MaskzBroadcastssPs: Broadcast the low single-precision (32-bit)
// floating-point element from 'a' to all elements of 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VBROADCASTSS'. Intrinsic: '_mm_maskz_broadcastss_ps'.
// Requires AVX512F.
func MaskzBroadcastssPs(k Mmask8, a M128) M128 {
	return M128(maskzBroadcastssPs(uint8(k), [4]float32(a)))
}

func maskzBroadcastssPs(k uint8, a [4]float32) [4]float32


// MaskBroadcastssPs1: Broadcast the low single-precision (32-bit)
// floating-point element from 'a' to all elements of 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTSS'. Intrinsic: '_mm256_mask_broadcastss_ps'.
// Requires AVX512F.
func MaskBroadcastssPs1(src M256, k Mmask8, a M128) M256 {
	return M256(maskBroadcastssPs1([8]float32(src), uint8(k), [4]float32(a)))
}

func maskBroadcastssPs1(src [8]float32, k uint8, a [4]float32) [8]float32


// MaskzBroadcastssPs1: Broadcast the low single-precision (32-bit)
// floating-point element from 'a' to all elements of 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTSS'. Intrinsic: '_mm256_maskz_broadcastss_ps'.
// Requires AVX512F.
func MaskzBroadcastssPs1(k Mmask8, a M128) M256 {
	return M256(maskzBroadcastssPs1(uint8(k), [4]float32(a)))
}

func maskzBroadcastssPs1(k uint8, a [4]float32) [8]float32


// BroadcastssPs: Broadcast the low single-precision (32-bit) floating-point
// element from 'a' to all elements of 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := a[31:0]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTSS'. Intrinsic: '_mm512_broadcastss_ps'.
// Requires AVX512F.
func BroadcastssPs(a M128) M512 {
	return M512(broadcastssPs([4]float32(a)))
}

func broadcastssPs(a [4]float32) [16]float32


// MaskBroadcastssPs2: Broadcast the low single-precision (32-bit)
// floating-point element from 'a' to all elements of 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTSS'. Intrinsic: '_mm512_mask_broadcastss_ps'.
// Requires AVX512F.
func MaskBroadcastssPs2(src M512, k Mmask16, a M128) M512 {
	return M512(maskBroadcastssPs2([16]float32(src), uint16(k), [4]float32(a)))
}

func maskBroadcastssPs2(src [16]float32, k uint16, a [4]float32) [16]float32


// MaskzBroadcastssPs2: Broadcast the low single-precision (32-bit)
// floating-point element from 'a' to all elements of 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VBROADCASTSS'. Intrinsic: '_mm512_maskz_broadcastss_ps'.
// Requires AVX512F.
func MaskzBroadcastssPs2(k Mmask16, a M128) M512 {
	return M512(maskzBroadcastssPs2(uint16(k), [4]float32(a)))
}

func maskzBroadcastssPs2(k uint16, a [4]float32) [16]float32


// Castpd128Pd512: Cast vector of type __m128d to type __m512d; the upper 384
// bits of the result are undefined. 
// 	This intrinsic is only used for compilation and does not generate any
// instructions, thus it has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_castpd128_pd512'.
// Requires AVX512F.
func Castpd128Pd512(a M128d) M512d {
	return M512d(castpd128Pd512([2]float64(a)))
}

func castpd128Pd512(a [2]float64) [8]float64


// Castpd256Pd512: Cast vector of type __m256d to type __m512d; the upper 256
// bits of the result are undefined. 
// 	This intrinsic is only used for compilation and does not generate any
// instructions, thus it has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_castpd256_pd512'.
// Requires AVX512F.
func Castpd256Pd512(a M256d) M512d {
	return M512d(castpd256Pd512([4]float64(a)))
}

func castpd256Pd512(a [4]float64) [8]float64


// Castpd512Pd128: Cast vector of type __m512d to type __m128d. 
// 	This intrinsic is only used for compilation and does not generate any
// instructions, thus it has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_castpd512_pd128'.
// Requires AVX512F.
func Castpd512Pd128(a M512d) M128d {
	return M128d(castpd512Pd128([8]float64(a)))
}

func castpd512Pd128(a [8]float64) [2]float64


// Castpd512Pd256: Cast vector of type __m512d to type __m256d. 
// 	This intrinsic is only used for compilation and does not generate any
// instructions, thus it has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_castpd512_pd256'.
// Requires AVX512F.
func Castpd512Pd256(a M512d) M256d {
	return M256d(castpd512Pd256([8]float64(a)))
}

func castpd512Pd256(a [8]float64) [4]float64


// Castps128Ps512: Cast vector of type __m128 to type __m512; the upper 384
// bits of the result are undefined. 
// 	This intrinsic is only used for compilation and does not generate any
// instructions, thus it has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_castps128_ps512'.
// Requires AVX512F.
func Castps128Ps512(a M128) M512 {
	return M512(castps128Ps512([4]float32(a)))
}

func castps128Ps512(a [4]float32) [16]float32


// Castps256Ps512: Cast vector of type __m256 to type __m512; the upper 256
// bits of the result are undefined. 
// 	This intrinsic is only used for compilation and does not generate any
// instructions, thus it has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_castps256_ps512'.
// Requires AVX512F.
func Castps256Ps512(a M256) M512 {
	return M512(castps256Ps512([8]float32(a)))
}

func castps256Ps512(a [8]float32) [16]float32


// Castps512Ps128: Cast vector of type __m512 to type __m128. 
// 	This intrinsic is only used for compilation and does not generate any
// instructions, thus it has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_castps512_ps128'.
// Requires AVX512F.
func Castps512Ps128(a M512) M128 {
	return M128(castps512Ps128([16]float32(a)))
}

func castps512Ps128(a [16]float32) [4]float32


// Castps512Ps256: Cast vector of type __m512 to type __m256. 
// 	This intrinsic is only used for compilation and does not generate any
// instructions, thus it has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_castps512_ps256'.
// Requires AVX512F.
func Castps512Ps256(a M512) M256 {
	return M256(castps512Ps256([16]float32(a)))
}

func castps512Ps256(a [16]float32) [8]float32


// Castsi128Si512: Cast vector of type __m128i to type __m512i; the upper 384
// bits of the result are undefined. 
// 	This intrinsic is only used for compilation and does not generate any
// instructions, thus it has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_castsi128_si512'.
// Requires AVX512F.
func Castsi128Si512(a M128i) M512i {
	return M512i(castsi128Si512([16]byte(a)))
}

func castsi128Si512(a [16]byte) [64]byte


// Castsi256Si512: Cast vector of type __m256i to type __m512i; the upper 256
// bits of the result are undefined.
// 	 This intrinsic is only used for compilation and does not generate any
// instructions, thus it has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_castsi256_si512'.
// Requires AVX512F.
func Castsi256Si512(a M256i) M512i {
	return M512i(castsi256Si512([32]byte(a)))
}

func castsi256Si512(a [32]byte) [64]byte


// Castsi512Si128: Cast vector of type __m512i to type __m128i.
// 	 This intrinsic is only used for compilation and does not generate any
// instructions, thus it has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_castsi512_si128'.
// Requires AVX512F.
func Castsi512Si128(a M512i) M128i {
	return M128i(castsi512Si128([64]byte(a)))
}

func castsi512Si128(a [64]byte) [16]byte


// Castsi512Si256: Cast vector of type __m512i to type __m256i.
// 	 This intrinsic is only used for compilation and does not generate any
// instructions, thus it has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_castsi512_si256'.
// Requires AVX512F.
func Castsi512Si256(a M512i) M256i {
	return M256i(castsi512Si256([64]byte(a)))
}

func castsi512Si256(a [64]byte) [32]byte


// CbrtPd: Compute the cube root of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := CubeRoot(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_cbrt_pd'.
// Requires AVX512F.
func CbrtPd(a M512d) M512d {
	return M512d(cbrtPd([8]float64(a)))
}

func cbrtPd(a [8]float64) [8]float64


// MaskCbrtPd: Compute the cube root of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := CubeRoot(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_cbrt_pd'.
// Requires AVX512F.
func MaskCbrtPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskCbrtPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskCbrtPd(src [8]float64, k uint8, a [8]float64) [8]float64


// CbrtPs: Compute the cube root of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := CubeRoot(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_cbrt_ps'.
// Requires AVX512F.
func CbrtPs(a M512) M512 {
	return M512(cbrtPs([16]float32(a)))
}

func cbrtPs(a [16]float32) [16]float32


// MaskCbrtPs: Compute the cube root of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := CubeRoot(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_cbrt_ps'.
// Requires AVX512F.
func MaskCbrtPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskCbrtPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskCbrtPs(src [16]float32, k uint16, a [16]float32) [16]float32


// CdfnormPd: Compute the cumulative distribution function of packed
// double-precision (64-bit) floating-point elements in 'a' using the normal
// distribution, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := CDFNormal(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_cdfnorm_pd'.
// Requires AVX512F.
func CdfnormPd(a M512d) M512d {
	return M512d(cdfnormPd([8]float64(a)))
}

func cdfnormPd(a [8]float64) [8]float64


// MaskCdfnormPd: Compute the cumulative distribution function of packed
// double-precision (64-bit) floating-point elements in 'a' using the normal
// distribution, and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := CDFNormal(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_cdfnorm_pd'.
// Requires AVX512F.
func MaskCdfnormPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskCdfnormPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskCdfnormPd(src [8]float64, k uint8, a [8]float64) [8]float64


// CdfnormPs: Compute the cumulative distribution function of packed
// single-precision (32-bit) floating-point elements in 'a' using the normal
// distribution, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := CDFNormal(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_cdfnorm_ps'.
// Requires AVX512F.
func CdfnormPs(a M512) M512 {
	return M512(cdfnormPs([16]float32(a)))
}

func cdfnormPs(a [16]float32) [16]float32


// MaskCdfnormPs: Compute the cumulative distribution function of packed
// single-precision (32-bit) floating-point elements in 'a' using the normal
// distribution, and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := CDFNormal(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_cdfnorm_ps'.
// Requires AVX512F.
func MaskCdfnormPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskCdfnormPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskCdfnormPs(src [16]float32, k uint16, a [16]float32) [16]float32


// CdfnorminvPd: Compute the inverse cumulative distribution function of packed
// double-precision (64-bit) floating-point elements in 'a' using the normal
// distribution, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := InverseCDFNormal(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_cdfnorminv_pd'.
// Requires AVX512F.
func CdfnorminvPd(a M512d) M512d {
	return M512d(cdfnorminvPd([8]float64(a)))
}

func cdfnorminvPd(a [8]float64) [8]float64


// MaskCdfnorminvPd: Compute the inverse cumulative distribution function of
// packed double-precision (64-bit) floating-point elements in 'a' using the
// normal distribution, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := InverseCDFNormal(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_cdfnorminv_pd'.
// Requires AVX512F.
func MaskCdfnorminvPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskCdfnorminvPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskCdfnorminvPd(src [8]float64, k uint8, a [8]float64) [8]float64


// CdfnorminvPs: Compute the inverse cumulative distribution function of packed
// single-precision (32-bit) floating-point elements in 'a' using the normal
// distribution, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := InverseCDFNormal(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_cdfnorminv_ps'.
// Requires AVX512F.
func CdfnorminvPs(a M512) M512 {
	return M512(cdfnorminvPs([16]float32(a)))
}

func cdfnorminvPs(a [16]float32) [16]float32


// MaskCdfnorminvPs: Compute the inverse cumulative distribution function of
// packed single-precision (32-bit) floating-point elements in 'a' using the
// normal distribution, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := InverseCDFNormal(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_cdfnorminv_ps'.
// Requires AVX512F.
func MaskCdfnorminvPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskCdfnorminvPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskCdfnorminvPs(src [16]float32, k uint16, a [16]float32) [16]float32


// CeilPd: Round the packed double-precision (64-bit) floating-point elements
// in 'a' up to an integer value, and store the results as packed
// double-precision floating-point elements in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := CEIL(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_ceil_pd'.
// Requires AVX512F.
func CeilPd(a M512d) M512d {
	return M512d(ceilPd([8]float64(a)))
}

func ceilPd(a [8]float64) [8]float64


// MaskCeilPd: Round the packed double-precision (64-bit) floating-point
// elements in 'a' up to an integer value, and store the results as packed
// double-precision floating-point elements in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := CEIL(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_ceil_pd'.
// Requires AVX512F.
func MaskCeilPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskCeilPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskCeilPd(src [8]float64, k uint8, a [8]float64) [8]float64


// CeilPs: Round the packed single-precision (32-bit) floating-point elements
// in 'a' up to an integer value, and store the results as packed
// single-precision floating-point elements in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := CEIL(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_ceil_ps'.
// Requires AVX512F.
func CeilPs(a M512) M512 {
	return M512(ceilPs([16]float32(a)))
}

func ceilPs(a [16]float32) [16]float32


// MaskCeilPs: Round the packed single-precision (32-bit) floating-point
// elements in 'a' up to an integer value, and store the results as packed
// single-precision floating-point elements in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := CEIL(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_ceil_ps'.
// Requires AVX512F.
func MaskCeilPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskCeilPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskCeilPs(src [16]float32, k uint16, a [16]float32) [16]float32


// CmpEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' based on the
// comparison operand specified by 'imm8', and store the results in mask vector
// 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm_cmp_epi32_mask'.
// Requires AVX512F.
func CmpEpi32Mask(a M128i, b M128i, imm8 uint8) Mmask8 {
	return Mmask8(cmpEpi32Mask([16]byte(a), [16]byte(b), imm8))
}

func cmpEpi32Mask(a [16]byte, b [16]byte, imm8 uint8) uint8


// MaskCmpEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' based on the
// comparison operand specified by 'imm8', and store the results in mask vector
// 'k1' using zeromask 'k' (elements are zeroed out when the corresponding mask
// bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm_mask_cmp_epi32_mask'.
// Requires AVX512F.
func MaskCmpEpi32Mask(k1 Mmask8, a M128i, b M128i, imm8 uint8) Mmask8 {
	return Mmask8(maskCmpEpi32Mask(uint8(k1), [16]byte(a), [16]byte(b), imm8))
}

func maskCmpEpi32Mask(k1 uint8, a [16]byte, b [16]byte, imm8 uint8) uint8


// CmpEpi32Mask1: Compare packed 32-bit integers in 'a' and 'b' based on the
// comparison operand specified by 'imm8', and store the results in mask vector
// 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_cmp_epi32_mask'.
// Requires AVX512F.
func CmpEpi32Mask1(a M256i, b M256i, imm8 uint8) Mmask8 {
	return Mmask8(cmpEpi32Mask1([32]byte(a), [32]byte(b), imm8))
}

func cmpEpi32Mask1(a [32]byte, b [32]byte, imm8 uint8) uint8


// MaskCmpEpi32Mask1: Compare packed 32-bit integers in 'a' and 'b' based on
// the comparison operand specified by 'imm8', and store the results in mask
// vector 'k1' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_mask_cmp_epi32_mask'.
// Requires AVX512F.
func MaskCmpEpi32Mask1(k1 Mmask8, a M256i, b M256i, imm8 uint8) Mmask8 {
	return Mmask8(maskCmpEpi32Mask1(uint8(k1), [32]byte(a), [32]byte(b), imm8))
}

func maskCmpEpi32Mask1(k1 uint8, a [32]byte, b [32]byte, imm8 uint8) uint8


// CmpEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' based on the
// comparison operand specified by 'imm8', and store the results in mask vector
// 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm_cmp_epi64_mask'.
// Requires AVX512F.
func CmpEpi64Mask(a M128i, b M128i, imm8 uint8) Mmask8 {
	return Mmask8(cmpEpi64Mask([16]byte(a), [16]byte(b), imm8))
}

func cmpEpi64Mask(a [16]byte, b [16]byte, imm8 uint8) uint8


// MaskCmpEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' based on the
// comparison operand specified by 'imm8', and store the results in mask vector
// 'k1' using zeromask 'k' (elements are zeroed out when the corresponding mask
// bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm_mask_cmp_epi64_mask'.
// Requires AVX512F.
func MaskCmpEpi64Mask(k1 Mmask8, a M128i, b M128i, imm8 uint8) Mmask8 {
	return Mmask8(maskCmpEpi64Mask(uint8(k1), [16]byte(a), [16]byte(b), imm8))
}

func maskCmpEpi64Mask(k1 uint8, a [16]byte, b [16]byte, imm8 uint8) uint8


// CmpEpi64Mask1: Compare packed 64-bit integers in 'a' and 'b' based on the
// comparison operand specified by 'imm8', and store the results in mask vector
// 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_cmp_epi64_mask'.
// Requires AVX512F.
func CmpEpi64Mask1(a M256i, b M256i, imm8 uint8) Mmask8 {
	return Mmask8(cmpEpi64Mask1([32]byte(a), [32]byte(b), imm8))
}

func cmpEpi64Mask1(a [32]byte, b [32]byte, imm8 uint8) uint8


// MaskCmpEpi64Mask1: Compare packed 64-bit integers in 'a' and 'b' based on
// the comparison operand specified by 'imm8', and store the results in mask
// vector 'k1' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_mask_cmp_epi64_mask'.
// Requires AVX512F.
func MaskCmpEpi64Mask1(k1 Mmask8, a M256i, b M256i, imm8 uint8) Mmask8 {
	return Mmask8(maskCmpEpi64Mask1(uint8(k1), [32]byte(a), [32]byte(b), imm8))
}

func maskCmpEpi64Mask1(k1 uint8, a [32]byte, b [32]byte, imm8 uint8) uint8


// CmpEpi64Mask2: Compare packed 64-bit integers in 'a' and 'b' based on the
// comparison operand specified by 'imm8', and store the results in mask vector
// 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm512_cmp_epi64_mask'.
// Requires AVX512F.
func CmpEpi64Mask2(a M512i, b M512i, imm8 uint8) Mmask8 {
	return Mmask8(cmpEpi64Mask2([64]byte(a), [64]byte(b), imm8))
}

func cmpEpi64Mask2(a [64]byte, b [64]byte, imm8 uint8) uint8


// MaskCmpEpi64Mask2: Compare packed 64-bit integers in 'a' and 'b' based on
// the comparison operand specified by 'imm8', and store the results in mask
// vector 'k1' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm512_mask_cmp_epi64_mask'.
// Requires AVX512F.
func MaskCmpEpi64Mask2(k1 Mmask8, a M512i, b M512i, imm8 uint8) Mmask8 {
	return Mmask8(maskCmpEpi64Mask2(uint8(k1), [64]byte(a), [64]byte(b), imm8))
}

func maskCmpEpi64Mask2(k1 uint8, a [64]byte, b [64]byte, imm8 uint8) uint8


// CmpEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b' based
// on the comparison operand specified by 'imm8', and store the results in mask
// vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm_cmp_epu32_mask'.
// Requires AVX512F.
func CmpEpu32Mask(a M128i, b M128i, imm8 uint8) Mmask8 {
	return Mmask8(cmpEpu32Mask([16]byte(a), [16]byte(b), imm8))
}

func cmpEpu32Mask(a [16]byte, b [16]byte, imm8 uint8) uint8


// MaskCmpEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// based on the comparison operand specified by 'imm8', and store the results
// in mask vector 'k1' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm_mask_cmp_epu32_mask'.
// Requires AVX512F.
func MaskCmpEpu32Mask(k1 Mmask8, a M128i, b M128i, imm8 uint8) Mmask8 {
	return Mmask8(maskCmpEpu32Mask(uint8(k1), [16]byte(a), [16]byte(b), imm8))
}

func maskCmpEpu32Mask(k1 uint8, a [16]byte, b [16]byte, imm8 uint8) uint8


// CmpEpu32Mask1: Compare packed unsigned 32-bit integers in 'a' and 'b' based
// on the comparison operand specified by 'imm8', and store the results in mask
// vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_cmp_epu32_mask'.
// Requires AVX512F.
func CmpEpu32Mask1(a M256i, b M256i, imm8 uint8) Mmask8 {
	return Mmask8(cmpEpu32Mask1([32]byte(a), [32]byte(b), imm8))
}

func cmpEpu32Mask1(a [32]byte, b [32]byte, imm8 uint8) uint8


// MaskCmpEpu32Mask1: Compare packed unsigned 32-bit integers in 'a' and 'b'
// based on the comparison operand specified by 'imm8', and store the results
// in mask vector 'k1' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_mask_cmp_epu32_mask'.
// Requires AVX512F.
func MaskCmpEpu32Mask1(k1 Mmask8, a M256i, b M256i, imm8 uint8) Mmask8 {
	return Mmask8(maskCmpEpu32Mask1(uint8(k1), [32]byte(a), [32]byte(b), imm8))
}

func maskCmpEpu32Mask1(k1 uint8, a [32]byte, b [32]byte, imm8 uint8) uint8


// CmpEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b' based
// on the comparison operand specified by 'imm8', and store the results in mask
// vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm_cmp_epu64_mask'.
// Requires AVX512F.
func CmpEpu64Mask(a M128i, b M128i, imm8 uint8) Mmask8 {
	return Mmask8(cmpEpu64Mask([16]byte(a), [16]byte(b), imm8))
}

func cmpEpu64Mask(a [16]byte, b [16]byte, imm8 uint8) uint8


// MaskCmpEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// based on the comparison operand specified by 'imm8', and store the results
// in mask vector 'k1' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm_mask_cmp_epu64_mask'.
// Requires AVX512F.
func MaskCmpEpu64Mask(k1 Mmask8, a M128i, b M128i, imm8 uint8) Mmask8 {
	return Mmask8(maskCmpEpu64Mask(uint8(k1), [16]byte(a), [16]byte(b), imm8))
}

func maskCmpEpu64Mask(k1 uint8, a [16]byte, b [16]byte, imm8 uint8) uint8


// CmpEpu64Mask1: Compare packed unsigned 64-bit integers in 'a' and 'b' based
// on the comparison operand specified by 'imm8', and store the results in mask
// vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_cmp_epu64_mask'.
// Requires AVX512F.
func CmpEpu64Mask1(a M256i, b M256i, imm8 uint8) Mmask8 {
	return Mmask8(cmpEpu64Mask1([32]byte(a), [32]byte(b), imm8))
}

func cmpEpu64Mask1(a [32]byte, b [32]byte, imm8 uint8) uint8


// MaskCmpEpu64Mask1: Compare packed unsigned 64-bit integers in 'a' and 'b'
// based on the comparison operand specified by 'imm8', and store the results
// in mask vector 'k1' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_mask_cmp_epu64_mask'.
// Requires AVX512F.
func MaskCmpEpu64Mask1(k1 Mmask8, a M256i, b M256i, imm8 uint8) Mmask8 {
	return Mmask8(maskCmpEpu64Mask1(uint8(k1), [32]byte(a), [32]byte(b), imm8))
}

func maskCmpEpu64Mask1(k1 uint8, a [32]byte, b [32]byte, imm8 uint8) uint8


// CmpEpu64Mask2: Compare packed unsigned 64-bit integers in 'a' and 'b' based
// on the comparison operand specified by 'imm8', and store the results in mask
// vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm512_cmp_epu64_mask'.
// Requires AVX512F.
func CmpEpu64Mask2(a M512i, b M512i, imm8 uint8) Mmask8 {
	return Mmask8(cmpEpu64Mask2([64]byte(a), [64]byte(b), imm8))
}

func cmpEpu64Mask2(a [64]byte, b [64]byte, imm8 uint8) uint8


// MaskCmpEpu64Mask2: Compare packed unsigned 64-bit integers in 'a' and 'b'
// based on the comparison operand specified by 'imm8', and store the results
// in mask vector 'k1' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm512_mask_cmp_epu64_mask'.
// Requires AVX512F.
func MaskCmpEpu64Mask2(k1 Mmask8, a M512i, b M512i, imm8 uint8) Mmask8 {
	return Mmask8(maskCmpEpu64Mask2(uint8(k1), [64]byte(a), [64]byte(b), imm8))
}

func maskCmpEpu64Mask2(k1 uint8, a [64]byte, b [64]byte, imm8 uint8) uint8


// CmpPdMask: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b' based on the comparison operand specified by 'imm8', and
// store the results in mask vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VCMPPD'. Intrinsic: '_mm_cmp_pd_mask'.
// Requires AVX512F.
func CmpPdMask(a M128d, b M128d, imm8 int) Mmask8 {
	return Mmask8(cmpPdMask([2]float64(a), [2]float64(b), imm8))
}

func cmpPdMask(a [2]float64, b [2]float64, imm8 int) uint8


// MaskCmpPdMask: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' based on the comparison operand specified by 'imm8',
// and store the results in mask vector 'k' using zeromask 'k1' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VCMPPD'. Intrinsic: '_mm_mask_cmp_pd_mask'.
// Requires AVX512F.
func MaskCmpPdMask(k1 Mmask8, a M128d, b M128d, imm8 int) Mmask8 {
	return Mmask8(maskCmpPdMask(uint8(k1), [2]float64(a), [2]float64(b), imm8))
}

func maskCmpPdMask(k1 uint8, a [2]float64, b [2]float64, imm8 int) uint8


// CmpPdMask1: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b' based on the comparison operand specified by 'imm8', and
// store the results in mask vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := (a[i+63:i] OP b[i+63:i]) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VCMPPD'. Intrinsic: '_mm256_cmp_pd_mask'.
// Requires AVX512F.
func CmpPdMask1(a M256d, b M256d, imm8 int) Mmask8 {
	return Mmask8(cmpPdMask1([4]float64(a), [4]float64(b), imm8))
}

func cmpPdMask1(a [4]float64, b [4]float64, imm8 int) uint8


// MaskCmpPdMask1: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' based on the comparison operand specified by 'imm8',
// and store the results in mask vector 'k' using zeromask 'k1' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VCMPPD'. Intrinsic: '_mm256_mask_cmp_pd_mask'.
// Requires AVX512F.
func MaskCmpPdMask1(k1 Mmask8, a M256d, b M256d, imm8 int) Mmask8 {
	return Mmask8(maskCmpPdMask1(uint8(k1), [4]float64(a), [4]float64(b), imm8))
}

func maskCmpPdMask1(k1 uint8, a [4]float64, b [4]float64, imm8 int) uint8


// CmpPsMask: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b' based on the comparison operand specified by 'imm8', and
// store the results in mask vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VCMPPS'. Intrinsic: '_mm_cmp_ps_mask'.
// Requires AVX512F.
func CmpPsMask(a M128, b M128, imm8 int) Mmask8 {
	return Mmask8(cmpPsMask([4]float32(a), [4]float32(b), imm8))
}

func cmpPsMask(a [4]float32, b [4]float32, imm8 int) uint8


// MaskCmpPsMask: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' based on the comparison operand specified by 'imm8',
// and store the results in mask vector 'k' using zeromask 'k1' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VCMPPS'. Intrinsic: '_mm_mask_cmp_ps_mask'.
// Requires AVX512F.
func MaskCmpPsMask(k1 Mmask8, a M128, b M128, imm8 int) Mmask8 {
	return Mmask8(maskCmpPsMask(uint8(k1), [4]float32(a), [4]float32(b), imm8))
}

func maskCmpPsMask(k1 uint8, a [4]float32, b [4]float32, imm8 int) uint8


// CmpPsMask1: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b' based on the comparison operand specified by 'imm8', and
// store the results in mask vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := (a[i+31:i] OP b[i+31:i]) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VCMPPS'. Intrinsic: '_mm256_cmp_ps_mask'.
// Requires AVX512F.
func CmpPsMask1(a M256, b M256, imm8 int) Mmask8 {
	return Mmask8(cmpPsMask1([8]float32(a), [8]float32(b), imm8))
}

func cmpPsMask1(a [8]float32, b [8]float32, imm8 int) uint8


// MaskCmpPsMask1: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' based on the comparison operand specified by 'imm8',
// and store the results in mask vector 'k' using zeromask 'k1' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VCMPPS'. Intrinsic: '_mm256_mask_cmp_ps_mask'.
// Requires AVX512F.
func MaskCmpPsMask1(k1 Mmask8, a M256, b M256, imm8 int) Mmask8 {
	return Mmask8(maskCmpPsMask1(uint8(k1), [8]float32(a), [8]float32(b), imm8))
}

func maskCmpPsMask1(k1 uint8, a [8]float32, b [8]float32, imm8 int) uint8


// CmpRoundSdMask: Compare the lower double-precision (64-bit) floating-point
// element in 'a' and 'b' based on the comparison operand specified by 'imm8',
// and store the result in mask vector 'k'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		
//		k[0] := ( a[63:0] OP b[63:0] ) ? 1 : 0
//		
//		k[MAX:1] := 0
//
// Instruction: 'VCMPSD'. Intrinsic: '_mm_cmp_round_sd_mask'.
// Requires AVX512F.
func CmpRoundSdMask(a M128d, b M128d, imm8 int, sae int) Mmask8 {
	return Mmask8(cmpRoundSdMask([2]float64(a), [2]float64(b), imm8, sae))
}

func cmpRoundSdMask(a [2]float64, b [2]float64, imm8 int, sae int) uint8


// MaskCmpRoundSdMask: Compare the lower double-precision (64-bit)
// floating-point element in 'a' and 'b' based on the comparison operand
// specified by 'imm8', and store the result in mask vector 'k' using zeromask
// 'k1' (the element is zeroed out when mask bit 0 is not set).
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		
//		IF k1[0]
//			k[0] := ( a[63:0] OP b[63:0] ) ? 1 : 0
//		ELSE
//			k[0] := 0
//		FI
//		k[MAX:1] := 0
//
// Instruction: 'VCMPSD'. Intrinsic: '_mm_mask_cmp_round_sd_mask'.
// Requires AVX512F.
func MaskCmpRoundSdMask(k1 Mmask8, a M128d, b M128d, imm8 int, sae int) Mmask8 {
	return Mmask8(maskCmpRoundSdMask(uint8(k1), [2]float64(a), [2]float64(b), imm8, sae))
}

func maskCmpRoundSdMask(k1 uint8, a [2]float64, b [2]float64, imm8 int, sae int) uint8


// CmpRoundSsMask: Compare the lower single-precision (32-bit) floating-point
// element in 'a' and 'b' based on the comparison operand specified by 'imm8',
// and store the result in mask vector 'k'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		
//		k[0] := ( a[31:0] OP b[31:0] ) ? 1 : 0
//		
//		k[MAX:1] := 0
//
// Instruction: 'VCMPSS'. Intrinsic: '_mm_cmp_round_ss_mask'.
// Requires AVX512F.
func CmpRoundSsMask(a M128, b M128, imm8 int, sae int) Mmask8 {
	return Mmask8(cmpRoundSsMask([4]float32(a), [4]float32(b), imm8, sae))
}

func cmpRoundSsMask(a [4]float32, b [4]float32, imm8 int, sae int) uint8


// MaskCmpRoundSsMask: Compare the lower single-precision (32-bit)
// floating-point element in 'a' and 'b' based on the comparison operand
// specified by 'imm8', and store the result in mask vector 'k' using zeromask
// 'k1' (the element is zeroed out when mask bit 0 is not set). 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		
//		IF k1[0]
//			k[0] := ( a[31:0] OP b[31:0] ) ? 1 : 0
//		ELSE
//			k[0] := 0
//		FI
//		k[MAX:1] := 0
//
// Instruction: 'VCMPSS'. Intrinsic: '_mm_mask_cmp_round_ss_mask'.
// Requires AVX512F.
func MaskCmpRoundSsMask(k1 Mmask8, a M128, b M128, imm8 int, sae int) Mmask8 {
	return Mmask8(maskCmpRoundSsMask(uint8(k1), [4]float32(a), [4]float32(b), imm8, sae))
}

func maskCmpRoundSsMask(k1 uint8, a [4]float32, b [4]float32, imm8 int, sae int) uint8


// CmpSdMask: Compare the lower double-precision (64-bit) floating-point
// element in 'a' and 'b' based on the comparison operand specified by 'imm8',
// and store the result in mask vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		
//		k[0] := ( a[63:0] OP b[63:0] ) ? 1 : 0
//		
//		k[MAX:1] := 0
//
// Instruction: 'VCMPSD'. Intrinsic: '_mm_cmp_sd_mask'.
// Requires AVX512F.
func CmpSdMask(a M128d, b M128d, imm8 int) Mmask8 {
	return Mmask8(cmpSdMask([2]float64(a), [2]float64(b), imm8))
}

func cmpSdMask(a [2]float64, b [2]float64, imm8 int) uint8


// MaskCmpSdMask: Compare the lower double-precision (64-bit) floating-point
// element in 'a' and 'b' based on the comparison operand specified by 'imm8',
// and store the result in mask vector 'k' using zeromask 'k1' (the element is
// zeroed out when mask bit 0 is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		
//		IF k1[0]
//			k[0] := ( a[63:0] OP b[63:0] ) ? 1 : 0
//		ELSE
//			k[0] := 0
//		FI
//		k[MAX:1] := 0
//
// Instruction: 'VCMPSD'. Intrinsic: '_mm_mask_cmp_sd_mask'.
// Requires AVX512F.
func MaskCmpSdMask(k1 Mmask8, a M128d, b M128d, imm8 int) Mmask8 {
	return Mmask8(maskCmpSdMask(uint8(k1), [2]float64(a), [2]float64(b), imm8))
}

func maskCmpSdMask(k1 uint8, a [2]float64, b [2]float64, imm8 int) uint8


// CmpSsMask: Compare the lower single-precision (32-bit) floating-point
// element in 'a' and 'b' based on the comparison operand specified by 'imm8',
// and store the result in mask vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		
//		k[0] := ( a[31:0] OP b[31:0] ) ? 1 : 0
//		
//		k[MAX:1] := 0
//
// Instruction: 'VCMPSS'. Intrinsic: '_mm_cmp_ss_mask'.
// Requires AVX512F.
func CmpSsMask(a M128, b M128, imm8 int) Mmask8 {
	return Mmask8(cmpSsMask([4]float32(a), [4]float32(b), imm8))
}

func cmpSsMask(a [4]float32, b [4]float32, imm8 int) uint8


// MaskCmpSsMask: Compare the lower single-precision (32-bit) floating-point
// element in 'a' and 'b' based on the comparison operand specified by 'imm8',
// and store the result in mask vector 'k' using zeromask 'k1' (the element is
// zeroed out when mask bit 0 is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		
//		IF k1[0]
//			k[0] := ( a[31:0] OP b[31:0] ) ? 1 : 0
//		ELSE
//			k[0] := 0
//		FI
//		k[MAX:1] := 0
//
// Instruction: 'VCMPSS'. Intrinsic: '_mm_mask_cmp_ss_mask'.
// Requires AVX512F.
func MaskCmpSsMask(k1 Mmask8, a M128, b M128, imm8 int) Mmask8 {
	return Mmask8(maskCmpSsMask(uint8(k1), [4]float32(a), [4]float32(b), imm8))
}

func maskCmpSsMask(k1 uint8, a [4]float32, b [4]float32, imm8 int) uint8


// CmpeqEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for equality,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm_cmpeq_epi32_mask'.
// Requires AVX512F.
func CmpeqEpi32Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpeqEpi32Mask([16]byte(a), [16]byte(b)))
}

func cmpeqEpi32Mask(a [16]byte, b [16]byte) uint8


// MaskCmpeqEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm_mask_cmpeq_epi32_mask'.
// Requires AVX512F.
func MaskCmpeqEpi32Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpeqEpi32Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpeqEpi32Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// CmpeqEpi32Mask1: Compare packed 32-bit integers in 'a' and 'b' for equality,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_cmpeq_epi32_mask'.
// Requires AVX512F.
func CmpeqEpi32Mask1(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpeqEpi32Mask1([32]byte(a), [32]byte(b)))
}

func cmpeqEpi32Mask1(a [32]byte, b [32]byte) uint8


// MaskCmpeqEpi32Mask1: Compare packed 32-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_mask_cmpeq_epi32_mask'.
// Requires AVX512F.
func MaskCmpeqEpi32Mask1(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpeqEpi32Mask1(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpeqEpi32Mask1(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpeqEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for equality,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm_cmpeq_epi64_mask'.
// Requires AVX512F.
func CmpeqEpi64Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpeqEpi64Mask([16]byte(a), [16]byte(b)))
}

func cmpeqEpi64Mask(a [16]byte, b [16]byte) uint8


// MaskCmpeqEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm_mask_cmpeq_epi64_mask'.
// Requires AVX512F.
func MaskCmpeqEpi64Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpeqEpi64Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpeqEpi64Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// CmpeqEpi64Mask1: Compare packed 64-bit integers in 'a' and 'b' for equality,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_cmpeq_epi64_mask'.
// Requires AVX512F.
func CmpeqEpi64Mask1(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpeqEpi64Mask1([32]byte(a), [32]byte(b)))
}

func cmpeqEpi64Mask1(a [32]byte, b [32]byte) uint8


// MaskCmpeqEpi64Mask1: Compare packed 64-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_mask_cmpeq_epi64_mask'.
// Requires AVX512F.
func MaskCmpeqEpi64Mask1(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpeqEpi64Mask1(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpeqEpi64Mask1(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpeqEpi64Mask2: Compare packed 64-bit integers in 'a' and 'b' for equality,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPEQQ'. Intrinsic: '_mm512_cmpeq_epi64_mask'.
// Requires AVX512F.
func CmpeqEpi64Mask2(a M512i, b M512i) Mmask8 {
	return Mmask8(cmpeqEpi64Mask2([64]byte(a), [64]byte(b)))
}

func cmpeqEpi64Mask2(a [64]byte, b [64]byte) uint8


// MaskCmpeqEpi64Mask2: Compare packed 64-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPEQQ'. Intrinsic: '_mm512_mask_cmpeq_epi64_mask'.
// Requires AVX512F.
func MaskCmpeqEpi64Mask2(k1 Mmask8, a M512i, b M512i) Mmask8 {
	return Mmask8(maskCmpeqEpi64Mask2(uint8(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpeqEpi64Mask2(k1 uint8, a [64]byte, b [64]byte) uint8


// CmpeqEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm_cmpeq_epu32_mask'.
// Requires AVX512F.
func CmpeqEpu32Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpeqEpu32Mask([16]byte(a), [16]byte(b)))
}

func cmpeqEpu32Mask(a [16]byte, b [16]byte) uint8


// MaskCmpeqEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for equality, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm_mask_cmpeq_epu32_mask'.
// Requires AVX512F.
func MaskCmpeqEpu32Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpeqEpu32Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpeqEpu32Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// CmpeqEpu32Mask1: Compare packed unsigned 32-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_cmpeq_epu32_mask'.
// Requires AVX512F.
func CmpeqEpu32Mask1(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpeqEpu32Mask1([32]byte(a), [32]byte(b)))
}

func cmpeqEpu32Mask1(a [32]byte, b [32]byte) uint8


// MaskCmpeqEpu32Mask1: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for equality, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_mask_cmpeq_epu32_mask'.
// Requires AVX512F.
func MaskCmpeqEpu32Mask1(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpeqEpu32Mask1(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpeqEpu32Mask1(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpeqEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm_cmpeq_epu64_mask'.
// Requires AVX512F.
func CmpeqEpu64Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpeqEpu64Mask([16]byte(a), [16]byte(b)))
}

func cmpeqEpu64Mask(a [16]byte, b [16]byte) uint8


// MaskCmpeqEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for equality, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm_mask_cmpeq_epu64_mask'.
// Requires AVX512F.
func MaskCmpeqEpu64Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpeqEpu64Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpeqEpu64Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// CmpeqEpu64Mask1: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_cmpeq_epu64_mask'.
// Requires AVX512F.
func CmpeqEpu64Mask1(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpeqEpu64Mask1([32]byte(a), [32]byte(b)))
}

func cmpeqEpu64Mask1(a [32]byte, b [32]byte) uint8


// MaskCmpeqEpu64Mask1: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for equality, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_mask_cmpeq_epu64_mask'.
// Requires AVX512F.
func MaskCmpeqEpu64Mask1(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpeqEpu64Mask1(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpeqEpu64Mask1(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpeqEpu64Mask2: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm512_cmpeq_epu64_mask'.
// Requires AVX512F.
func CmpeqEpu64Mask2(a M512i, b M512i) Mmask8 {
	return Mmask8(cmpeqEpu64Mask2([64]byte(a), [64]byte(b)))
}

func cmpeqEpu64Mask2(a [64]byte, b [64]byte) uint8


// MaskCmpeqEpu64Mask2: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for equality, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm512_mask_cmpeq_epu64_mask'.
// Requires AVX512F.
func MaskCmpeqEpu64Mask2(k1 Mmask8, a M512i, b M512i) Mmask8 {
	return Mmask8(maskCmpeqEpu64Mask2(uint8(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpeqEpu64Mask2(k1 uint8, a [64]byte, b [64]byte) uint8


// CmpgeEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ( a[i+31:i] >= b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm_cmpge_epi32_mask'.
// Requires AVX512F.
func CmpgeEpi32Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpgeEpi32Mask([16]byte(a), [16]byte(b)))
}

func cmpgeEpi32Mask(a [16]byte, b [16]byte) uint8


// MaskCmpgeEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] >= b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm_mask_cmpge_epi32_mask'.
// Requires AVX512F.
func MaskCmpgeEpi32Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpgeEpi32Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpgeEpi32Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// CmpgeEpi32Mask1: Compare packed 32-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] >= b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_cmpge_epi32_mask'.
// Requires AVX512F.
func CmpgeEpi32Mask1(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpgeEpi32Mask1([32]byte(a), [32]byte(b)))
}

func cmpgeEpi32Mask1(a [32]byte, b [32]byte) uint8


// MaskCmpgeEpi32Mask1: Compare packed 32-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] >= b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_mask_cmpge_epi32_mask'.
// Requires AVX512F.
func MaskCmpgeEpi32Mask1(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpgeEpi32Mask1(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpgeEpi32Mask1(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpgeEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ( a[i+63:i] >= b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm_cmpge_epi64_mask'.
// Requires AVX512F.
func CmpgeEpi64Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpgeEpi64Mask([16]byte(a), [16]byte(b)))
}

func cmpgeEpi64Mask(a [16]byte, b [16]byte) uint8


// MaskCmpgeEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] >= b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm_mask_cmpge_epi64_mask'.
// Requires AVX512F.
func MaskCmpgeEpi64Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpgeEpi64Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpgeEpi64Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// CmpgeEpi64Mask1: Compare packed 64-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] >= b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_cmpge_epi64_mask'.
// Requires AVX512F.
func CmpgeEpi64Mask1(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpgeEpi64Mask1([32]byte(a), [32]byte(b)))
}

func cmpgeEpi64Mask1(a [32]byte, b [32]byte) uint8


// MaskCmpgeEpi64Mask1: Compare packed 64-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] >= b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_mask_cmpge_epi64_mask'.
// Requires AVX512F.
func MaskCmpgeEpi64Mask1(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpgeEpi64Mask1(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpgeEpi64Mask1(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpgeEpi64Mask2: Compare packed 64-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ( a[i+63:i] >= b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm512_cmpge_epi64_mask'.
// Requires AVX512F.
func CmpgeEpi64Mask2(a M512i, b M512i) Mmask8 {
	return Mmask8(cmpgeEpi64Mask2([64]byte(a), [64]byte(b)))
}

func cmpgeEpi64Mask2(a [64]byte, b [64]byte) uint8


// MaskCmpgeEpi64Mask2: Compare packed 64-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] >= b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm512_mask_cmpge_epi64_mask'.
// Requires AVX512F.
func MaskCmpgeEpi64Mask2(k1 Mmask8, a M512i, b M512i) Mmask8 {
	return Mmask8(maskCmpgeEpi64Mask2(uint8(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpgeEpi64Mask2(k1 uint8, a [64]byte, b [64]byte) uint8


// CmpgeEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ( a[i+31:i] >= b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm_cmpge_epu32_mask'.
// Requires AVX512F.
func CmpgeEpu32Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpgeEpu32Mask([16]byte(a), [16]byte(b)))
}

func cmpgeEpu32Mask(a [16]byte, b [16]byte) uint8


// MaskCmpgeEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for greater-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] >= b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm_mask_cmpge_epu32_mask'.
// Requires AVX512F.
func MaskCmpgeEpu32Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpgeEpu32Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpgeEpu32Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// CmpgeEpu32Mask1: Compare packed unsigned 32-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] >= b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_cmpge_epu32_mask'.
// Requires AVX512F.
func CmpgeEpu32Mask1(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpgeEpu32Mask1([32]byte(a), [32]byte(b)))
}

func cmpgeEpu32Mask1(a [32]byte, b [32]byte) uint8


// MaskCmpgeEpu32Mask1: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for greater-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] >= b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_mask_cmpge_epu32_mask'.
// Requires AVX512F.
func MaskCmpgeEpu32Mask1(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpgeEpu32Mask1(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpgeEpu32Mask1(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpgeEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ( a[i+63:i] >= b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm_cmpge_epu64_mask'.
// Requires AVX512F.
func CmpgeEpu64Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpgeEpu64Mask([16]byte(a), [16]byte(b)))
}

func cmpgeEpu64Mask(a [16]byte, b [16]byte) uint8


// MaskCmpgeEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for greater-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] >= b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm_mask_cmpge_epu64_mask'.
// Requires AVX512F.
func MaskCmpgeEpu64Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpgeEpu64Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpgeEpu64Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// CmpgeEpu64Mask1: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] >= b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_cmpge_epu64_mask'.
// Requires AVX512F.
func CmpgeEpu64Mask1(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpgeEpu64Mask1([32]byte(a), [32]byte(b)))
}

func cmpgeEpu64Mask1(a [32]byte, b [32]byte) uint8


// MaskCmpgeEpu64Mask1: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for greater-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] >= b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_mask_cmpge_epu64_mask'.
// Requires AVX512F.
func MaskCmpgeEpu64Mask1(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpgeEpu64Mask1(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpgeEpu64Mask1(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpgeEpu64Mask2: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ( a[i+63:i] >= b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm512_cmpge_epu64_mask'.
// Requires AVX512F.
func CmpgeEpu64Mask2(a M512i, b M512i) Mmask8 {
	return Mmask8(cmpgeEpu64Mask2([64]byte(a), [64]byte(b)))
}

func cmpgeEpu64Mask2(a [64]byte, b [64]byte) uint8


// MaskCmpgeEpu64Mask2: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for greater-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] >= b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm512_mask_cmpge_epu64_mask'.
// Requires AVX512F.
func MaskCmpgeEpu64Mask2(k1 Mmask8, a M512i, b M512i) Mmask8 {
	return Mmask8(maskCmpgeEpu64Mask2(uint8(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpgeEpu64Mask2(k1 uint8, a [64]byte, b [64]byte) uint8


// CmpgtEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ( a[i+31:i] > b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm_cmpgt_epi32_mask'.
// Requires AVX512F.
func CmpgtEpi32Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpgtEpi32Mask([16]byte(a), [16]byte(b)))
}

func cmpgtEpi32Mask(a [16]byte, b [16]byte) uint8


// MaskCmpgtEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] > b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm_mask_cmpgt_epi32_mask'.
// Requires AVX512F.
func MaskCmpgtEpi32Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpgtEpi32Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpgtEpi32Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// CmpgtEpi32Mask1: Compare packed 32-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] > b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_cmpgt_epi32_mask'.
// Requires AVX512F.
func CmpgtEpi32Mask1(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpgtEpi32Mask1([32]byte(a), [32]byte(b)))
}

func cmpgtEpi32Mask1(a [32]byte, b [32]byte) uint8


// MaskCmpgtEpi32Mask1: Compare packed 32-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] > b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_mask_cmpgt_epi32_mask'.
// Requires AVX512F.
func MaskCmpgtEpi32Mask1(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpgtEpi32Mask1(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpgtEpi32Mask1(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpgtEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ( a[i+63:i] > b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm_cmpgt_epi64_mask'.
// Requires AVX512F.
func CmpgtEpi64Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpgtEpi64Mask([16]byte(a), [16]byte(b)))
}

func cmpgtEpi64Mask(a [16]byte, b [16]byte) uint8


// MaskCmpgtEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] > b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm_mask_cmpgt_epi64_mask'.
// Requires AVX512F.
func MaskCmpgtEpi64Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpgtEpi64Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpgtEpi64Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// CmpgtEpi64Mask1: Compare packed 64-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] > b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_cmpgt_epi64_mask'.
// Requires AVX512F.
func CmpgtEpi64Mask1(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpgtEpi64Mask1([32]byte(a), [32]byte(b)))
}

func cmpgtEpi64Mask1(a [32]byte, b [32]byte) uint8


// MaskCmpgtEpi64Mask1: Compare packed 64-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] > b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_mask_cmpgt_epi64_mask'.
// Requires AVX512F.
func MaskCmpgtEpi64Mask1(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpgtEpi64Mask1(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpgtEpi64Mask1(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpgtEpi64Mask2: Compare packed 64-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ( a[i+63:i] > b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPGTQ'. Intrinsic: '_mm512_cmpgt_epi64_mask'.
// Requires AVX512F.
func CmpgtEpi64Mask2(a M512i, b M512i) Mmask8 {
	return Mmask8(cmpgtEpi64Mask2([64]byte(a), [64]byte(b)))
}

func cmpgtEpi64Mask2(a [64]byte, b [64]byte) uint8


// MaskCmpgtEpi64Mask2: Compare packed 64-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] > b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPGTQ'. Intrinsic: '_mm512_mask_cmpgt_epi64_mask'.
// Requires AVX512F.
func MaskCmpgtEpi64Mask2(k1 Mmask8, a M512i, b M512i) Mmask8 {
	return Mmask8(maskCmpgtEpi64Mask2(uint8(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpgtEpi64Mask2(k1 uint8, a [64]byte, b [64]byte) uint8


// CmpgtEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ( a[i+31:i] > b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm_cmpgt_epu32_mask'.
// Requires AVX512F.
func CmpgtEpu32Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpgtEpu32Mask([16]byte(a), [16]byte(b)))
}

func cmpgtEpu32Mask(a [16]byte, b [16]byte) uint8


// MaskCmpgtEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for greater-than, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] > b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm_mask_cmpgt_epu32_mask'.
// Requires AVX512F.
func MaskCmpgtEpu32Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpgtEpu32Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpgtEpu32Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// CmpgtEpu32Mask1: Compare packed unsigned 32-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] > b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_cmpgt_epu32_mask'.
// Requires AVX512F.
func CmpgtEpu32Mask1(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpgtEpu32Mask1([32]byte(a), [32]byte(b)))
}

func cmpgtEpu32Mask1(a [32]byte, b [32]byte) uint8


// MaskCmpgtEpu32Mask1: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for greater-than, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] > b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_mask_cmpgt_epu32_mask'.
// Requires AVX512F.
func MaskCmpgtEpu32Mask1(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpgtEpu32Mask1(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpgtEpu32Mask1(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpgtEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ( a[i+63:i] > b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm_cmpgt_epu64_mask'.
// Requires AVX512F.
func CmpgtEpu64Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpgtEpu64Mask([16]byte(a), [16]byte(b)))
}

func cmpgtEpu64Mask(a [16]byte, b [16]byte) uint8


// MaskCmpgtEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for greater-than, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] > b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm_mask_cmpgt_epu64_mask'.
// Requires AVX512F.
func MaskCmpgtEpu64Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpgtEpu64Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpgtEpu64Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// CmpgtEpu64Mask1: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] > b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_cmpgt_epu64_mask'.
// Requires AVX512F.
func CmpgtEpu64Mask1(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpgtEpu64Mask1([32]byte(a), [32]byte(b)))
}

func cmpgtEpu64Mask1(a [32]byte, b [32]byte) uint8


// MaskCmpgtEpu64Mask1: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for greater-than, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] > b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_mask_cmpgt_epu64_mask'.
// Requires AVX512F.
func MaskCmpgtEpu64Mask1(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpgtEpu64Mask1(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpgtEpu64Mask1(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpgtEpu64Mask2: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ( a[i+63:i] > b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm512_cmpgt_epu64_mask'.
// Requires AVX512F.
func CmpgtEpu64Mask2(a M512i, b M512i) Mmask8 {
	return Mmask8(cmpgtEpu64Mask2([64]byte(a), [64]byte(b)))
}

func cmpgtEpu64Mask2(a [64]byte, b [64]byte) uint8


// MaskCmpgtEpu64Mask2: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for greater-than, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] > b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm512_mask_cmpgt_epu64_mask'.
// Requires AVX512F.
func MaskCmpgtEpu64Mask2(k1 Mmask8, a M512i, b M512i) Mmask8 {
	return Mmask8(maskCmpgtEpu64Mask2(uint8(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpgtEpu64Mask2(k1 uint8, a [64]byte, b [64]byte) uint8


// CmpleEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ( a[i+31:i] <= b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm_cmple_epi32_mask'.
// Requires AVX512F.
func CmpleEpi32Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpleEpi32Mask([16]byte(a), [16]byte(b)))
}

func cmpleEpi32Mask(a [16]byte, b [16]byte) uint8


// MaskCmpleEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] <= b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm_mask_cmple_epi32_mask'.
// Requires AVX512F.
func MaskCmpleEpi32Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpleEpi32Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpleEpi32Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// CmpleEpi32Mask1: Compare packed 32-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] <= b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_cmple_epi32_mask'.
// Requires AVX512F.
func CmpleEpi32Mask1(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpleEpi32Mask1([32]byte(a), [32]byte(b)))
}

func cmpleEpi32Mask1(a [32]byte, b [32]byte) uint8


// MaskCmpleEpi32Mask1: Compare packed 32-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] <= b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_mask_cmple_epi32_mask'.
// Requires AVX512F.
func MaskCmpleEpi32Mask1(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpleEpi32Mask1(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpleEpi32Mask1(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpleEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ( a[i+63:i] <= b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm_cmple_epi64_mask'.
// Requires AVX512F.
func CmpleEpi64Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpleEpi64Mask([16]byte(a), [16]byte(b)))
}

func cmpleEpi64Mask(a [16]byte, b [16]byte) uint8


// MaskCmpleEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] <= b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm_mask_cmple_epi64_mask'.
// Requires AVX512F.
func MaskCmpleEpi64Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpleEpi64Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpleEpi64Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// CmpleEpi64Mask1: Compare packed 64-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] <= b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_cmple_epi64_mask'.
// Requires AVX512F.
func CmpleEpi64Mask1(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpleEpi64Mask1([32]byte(a), [32]byte(b)))
}

func cmpleEpi64Mask1(a [32]byte, b [32]byte) uint8


// MaskCmpleEpi64Mask1: Compare packed 64-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] <= b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_mask_cmple_epi64_mask'.
// Requires AVX512F.
func MaskCmpleEpi64Mask1(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpleEpi64Mask1(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpleEpi64Mask1(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpleEpi64Mask2: Compare packed 64-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ( a[i+63:i] <= b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm512_cmple_epi64_mask'.
// Requires AVX512F.
func CmpleEpi64Mask2(a M512i, b M512i) Mmask8 {
	return Mmask8(cmpleEpi64Mask2([64]byte(a), [64]byte(b)))
}

func cmpleEpi64Mask2(a [64]byte, b [64]byte) uint8


// MaskCmpleEpi64Mask2: Compare packed 64-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] <= b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm512_mask_cmple_epi64_mask'.
// Requires AVX512F.
func MaskCmpleEpi64Mask2(k1 Mmask8, a M512i, b M512i) Mmask8 {
	return Mmask8(maskCmpleEpi64Mask2(uint8(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpleEpi64Mask2(k1 uint8, a [64]byte, b [64]byte) uint8


// CmpleEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ( a[i+31:i] <= b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm_cmple_epu32_mask'.
// Requires AVX512F.
func CmpleEpu32Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpleEpu32Mask([16]byte(a), [16]byte(b)))
}

func cmpleEpu32Mask(a [16]byte, b [16]byte) uint8


// MaskCmpleEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for less-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] <= b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm_mask_cmple_epu32_mask'.
// Requires AVX512F.
func MaskCmpleEpu32Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpleEpu32Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpleEpu32Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// CmpleEpu32Mask1: Compare packed unsigned 32-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] <= b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_cmple_epu32_mask'.
// Requires AVX512F.
func CmpleEpu32Mask1(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpleEpu32Mask1([32]byte(a), [32]byte(b)))
}

func cmpleEpu32Mask1(a [32]byte, b [32]byte) uint8


// MaskCmpleEpu32Mask1: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for less-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] <= b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_mask_cmple_epu32_mask'.
// Requires AVX512F.
func MaskCmpleEpu32Mask1(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpleEpu32Mask1(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpleEpu32Mask1(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpleEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ( a[i+63:i] <= b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm_cmple_epu64_mask'.
// Requires AVX512F.
func CmpleEpu64Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpleEpu64Mask([16]byte(a), [16]byte(b)))
}

func cmpleEpu64Mask(a [16]byte, b [16]byte) uint8


// MaskCmpleEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for less-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] <= b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm_mask_cmple_epu64_mask'.
// Requires AVX512F.
func MaskCmpleEpu64Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpleEpu64Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpleEpu64Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// CmpleEpu64Mask1: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] <= b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_cmple_epu64_mask'.
// Requires AVX512F.
func CmpleEpu64Mask1(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpleEpu64Mask1([32]byte(a), [32]byte(b)))
}

func cmpleEpu64Mask1(a [32]byte, b [32]byte) uint8


// MaskCmpleEpu64Mask1: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for less-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] <= b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_mask_cmple_epu64_mask'.
// Requires AVX512F.
func MaskCmpleEpu64Mask1(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpleEpu64Mask1(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpleEpu64Mask1(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpleEpu64Mask2: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ( a[i+63:i] <= b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm512_cmple_epu64_mask'.
// Requires AVX512F.
func CmpleEpu64Mask2(a M512i, b M512i) Mmask8 {
	return Mmask8(cmpleEpu64Mask2([64]byte(a), [64]byte(b)))
}

func cmpleEpu64Mask2(a [64]byte, b [64]byte) uint8


// MaskCmpleEpu64Mask2: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for less-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] <= b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm512_mask_cmple_epu64_mask'.
// Requires AVX512F.
func MaskCmpleEpu64Mask2(k1 Mmask8, a M512i, b M512i) Mmask8 {
	return Mmask8(maskCmpleEpu64Mask2(uint8(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpleEpu64Mask2(k1 uint8, a [64]byte, b [64]byte) uint8


// CmpltEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for less-than,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ( a[i+31:i] < b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm_cmplt_epi32_mask'.
// Requires AVX512F.
func CmpltEpi32Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpltEpi32Mask([16]byte(a), [16]byte(b)))
}

func cmpltEpi32Mask(a [16]byte, b [16]byte) uint8


// MaskCmpltEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] < b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm_mask_cmplt_epi32_mask'.
// Requires AVX512F.
func MaskCmpltEpi32Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpltEpi32Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpltEpi32Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// CmpltEpi32Mask1: Compare packed 32-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] < b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_cmplt_epi32_mask'.
// Requires AVX512F.
func CmpltEpi32Mask1(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpltEpi32Mask1([32]byte(a), [32]byte(b)))
}

func cmpltEpi32Mask1(a [32]byte, b [32]byte) uint8


// MaskCmpltEpi32Mask1: Compare packed 32-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] < b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_mask_cmplt_epi32_mask'.
// Requires AVX512F.
func MaskCmpltEpi32Mask1(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpltEpi32Mask1(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpltEpi32Mask1(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpltEpi32Mask2: Compare packed 32-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			k[j] := ( a[i+31:i] < b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm512_cmplt_epi32_mask'.
// Requires AVX512F.
func CmpltEpi32Mask2(a M512i, b M512i) Mmask16 {
	return Mmask16(cmpltEpi32Mask2([64]byte(a), [64]byte(b)))
}

func cmpltEpi32Mask2(a [64]byte, b [64]byte) uint16


// MaskCmpltEpi32Mask2: Compare packed 32-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] <= b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm512_mask_cmplt_epi32_mask'.
// Requires AVX512F.
func MaskCmpltEpi32Mask2(k1 Mmask16, a M512i, b M512i) Mmask16 {
	return Mmask16(maskCmpltEpi32Mask2(uint16(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpltEpi32Mask2(k1 uint16, a [64]byte, b [64]byte) uint16


// CmpltEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for less-than,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ( a[i+63:i] < b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm_cmplt_epi64_mask'.
// Requires AVX512F.
func CmpltEpi64Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpltEpi64Mask([16]byte(a), [16]byte(b)))
}

func cmpltEpi64Mask(a [16]byte, b [16]byte) uint8


// MaskCmpltEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] < b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm_mask_cmplt_epi64_mask'.
// Requires AVX512F.
func MaskCmpltEpi64Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpltEpi64Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpltEpi64Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// CmpltEpi64Mask1: Compare packed 64-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] < b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_cmplt_epi64_mask'.
// Requires AVX512F.
func CmpltEpi64Mask1(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpltEpi64Mask1([32]byte(a), [32]byte(b)))
}

func cmpltEpi64Mask1(a [32]byte, b [32]byte) uint8


// MaskCmpltEpi64Mask1: Compare packed 64-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] < b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_mask_cmplt_epi64_mask'.
// Requires AVX512F.
func MaskCmpltEpi64Mask1(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpltEpi64Mask1(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpltEpi64Mask1(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpltEpi64Mask2: Compare packed 64-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ( a[i+63:i] < b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm512_cmplt_epi64_mask'.
// Requires AVX512F.
func CmpltEpi64Mask2(a M512i, b M512i) Mmask8 {
	return Mmask8(cmpltEpi64Mask2([64]byte(a), [64]byte(b)))
}

func cmpltEpi64Mask2(a [64]byte, b [64]byte) uint8


// MaskCmpltEpi64Mask2: Compare packed 64-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] < b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm512_mask_cmplt_epi64_mask'.
// Requires AVX512F.
func MaskCmpltEpi64Mask2(k1 Mmask8, a M512i, b M512i) Mmask8 {
	return Mmask8(maskCmpltEpi64Mask2(uint8(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpltEpi64Mask2(k1 uint8, a [64]byte, b [64]byte) uint8


// CmpltEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ( a[i+31:i] < b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm_cmplt_epu32_mask'.
// Requires AVX512F.
func CmpltEpu32Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpltEpu32Mask([16]byte(a), [16]byte(b)))
}

func cmpltEpu32Mask(a [16]byte, b [16]byte) uint8


// MaskCmpltEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for less-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] < b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm_mask_cmplt_epu32_mask'.
// Requires AVX512F.
func MaskCmpltEpu32Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpltEpu32Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpltEpu32Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// CmpltEpu32Mask1: Compare packed unsigned 32-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] < b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_cmplt_epu32_mask'.
// Requires AVX512F.
func CmpltEpu32Mask1(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpltEpu32Mask1([32]byte(a), [32]byte(b)))
}

func cmpltEpu32Mask1(a [32]byte, b [32]byte) uint8


// MaskCmpltEpu32Mask1: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for less-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] < b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_mask_cmplt_epu32_mask'.
// Requires AVX512F.
func MaskCmpltEpu32Mask1(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpltEpu32Mask1(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpltEpu32Mask1(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpltEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ( a[i+63:i] < b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm_cmplt_epu64_mask'.
// Requires AVX512F.
func CmpltEpu64Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpltEpu64Mask([16]byte(a), [16]byte(b)))
}

func cmpltEpu64Mask(a [16]byte, b [16]byte) uint8


// MaskCmpltEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for less-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] < b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm_mask_cmplt_epu64_mask'.
// Requires AVX512F.
func MaskCmpltEpu64Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpltEpu64Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpltEpu64Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// CmpltEpu64Mask1: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] < b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_cmplt_epu64_mask'.
// Requires AVX512F.
func CmpltEpu64Mask1(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpltEpu64Mask1([32]byte(a), [32]byte(b)))
}

func cmpltEpu64Mask1(a [32]byte, b [32]byte) uint8


// MaskCmpltEpu64Mask1: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for less-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] < b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_mask_cmplt_epu64_mask'.
// Requires AVX512F.
func MaskCmpltEpu64Mask1(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpltEpu64Mask1(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpltEpu64Mask1(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpltEpu64Mask2: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ( a[i+63:i] < b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm512_cmplt_epu64_mask'.
// Requires AVX512F.
func CmpltEpu64Mask2(a M512i, b M512i) Mmask8 {
	return Mmask8(cmpltEpu64Mask2([64]byte(a), [64]byte(b)))
}

func cmpltEpu64Mask2(a [64]byte, b [64]byte) uint8


// MaskCmpltEpu64Mask2: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for less-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] < b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm512_mask_cmplt_epu64_mask'.
// Requires AVX512F.
func MaskCmpltEpu64Mask2(k1 Mmask8, a M512i, b M512i) Mmask8 {
	return Mmask8(maskCmpltEpu64Mask2(uint8(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpltEpu64Mask2(k1 uint8, a [64]byte, b [64]byte) uint8


// CmpneqEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm_cmpneq_epi32_mask'.
// Requires AVX512F.
func CmpneqEpi32Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpneqEpi32Mask([16]byte(a), [16]byte(b)))
}

func cmpneqEpi32Mask(a [16]byte, b [16]byte) uint8


// MaskCmpneqEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm_mask_cmpneq_epi32_mask'.
// Requires AVX512F.
func MaskCmpneqEpi32Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpneqEpi32Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpneqEpi32Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// CmpneqEpi32Mask1: Compare packed 32-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_cmpneq_epi32_mask'.
// Requires AVX512F.
func CmpneqEpi32Mask1(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpneqEpi32Mask1([32]byte(a), [32]byte(b)))
}

func cmpneqEpi32Mask1(a [32]byte, b [32]byte) uint8


// MaskCmpneqEpi32Mask1: Compare packed 32-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_mask_cmpneq_epi32_mask'.
// Requires AVX512F.
func MaskCmpneqEpi32Mask1(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpneqEpi32Mask1(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpneqEpi32Mask1(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpneqEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm_cmpneq_epi64_mask'.
// Requires AVX512F.
func CmpneqEpi64Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpneqEpi64Mask([16]byte(a), [16]byte(b)))
}

func cmpneqEpi64Mask(a [16]byte, b [16]byte) uint8


// MaskCmpneqEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm_mask_cmpneq_epi64_mask'.
// Requires AVX512F.
func MaskCmpneqEpi64Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpneqEpi64Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpneqEpi64Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// CmpneqEpi64Mask1: Compare packed 64-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_cmpneq_epi64_mask'.
// Requires AVX512F.
func CmpneqEpi64Mask1(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpneqEpi64Mask1([32]byte(a), [32]byte(b)))
}

func cmpneqEpi64Mask1(a [32]byte, b [32]byte) uint8


// MaskCmpneqEpi64Mask1: Compare packed 64-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_mask_cmpneq_epi64_mask'.
// Requires AVX512F.
func MaskCmpneqEpi64Mask1(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpneqEpi64Mask1(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpneqEpi64Mask1(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpneqEpi64Mask2: Compare packed 64-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm512_cmpneq_epi64_mask'.
// Requires AVX512F.
func CmpneqEpi64Mask2(a M512i, b M512i) Mmask8 {
	return Mmask8(cmpneqEpi64Mask2([64]byte(a), [64]byte(b)))
}

func cmpneqEpi64Mask2(a [64]byte, b [64]byte) uint8


// MaskCmpneqEpi64Mask2: Compare packed 64-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm512_mask_cmpneq_epi64_mask'.
// Requires AVX512F.
func MaskCmpneqEpi64Mask2(k1 Mmask8, a M512i, b M512i) Mmask8 {
	return Mmask8(maskCmpneqEpi64Mask2(uint8(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpneqEpi64Mask2(k1 uint8, a [64]byte, b [64]byte) uint8


// CmpneqEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm_cmpneq_epu32_mask'.
// Requires AVX512F.
func CmpneqEpu32Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpneqEpu32Mask([16]byte(a), [16]byte(b)))
}

func cmpneqEpu32Mask(a [16]byte, b [16]byte) uint8


// MaskCmpneqEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for not-equal, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm_mask_cmpneq_epu32_mask'.
// Requires AVX512F.
func MaskCmpneqEpu32Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpneqEpu32Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpneqEpu32Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// CmpneqEpu32Mask1: Compare packed unsigned 32-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_cmpneq_epu32_mask'.
// Requires AVX512F.
func CmpneqEpu32Mask1(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpneqEpu32Mask1([32]byte(a), [32]byte(b)))
}

func cmpneqEpu32Mask1(a [32]byte, b [32]byte) uint8


// MaskCmpneqEpu32Mask1: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for not-equal, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_mask_cmpneq_epu32_mask'.
// Requires AVX512F.
func MaskCmpneqEpu32Mask1(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpneqEpu32Mask1(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpneqEpu32Mask1(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpneqEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm_cmpneq_epu64_mask'.
// Requires AVX512F.
func CmpneqEpu64Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpneqEpu64Mask([16]byte(a), [16]byte(b)))
}

func cmpneqEpu64Mask(a [16]byte, b [16]byte) uint8


// MaskCmpneqEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for not-equal, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm_mask_cmpneq_epu64_mask'.
// Requires AVX512F.
func MaskCmpneqEpu64Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpneqEpu64Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpneqEpu64Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// CmpneqEpu64Mask1: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_cmpneq_epu64_mask'.
// Requires AVX512F.
func CmpneqEpu64Mask1(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpneqEpu64Mask1([32]byte(a), [32]byte(b)))
}

func cmpneqEpu64Mask1(a [32]byte, b [32]byte) uint8


// MaskCmpneqEpu64Mask1: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for not-equal, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_mask_cmpneq_epu64_mask'.
// Requires AVX512F.
func MaskCmpneqEpu64Mask1(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpneqEpu64Mask1(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpneqEpu64Mask1(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpneqEpu64Mask2: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm512_cmpneq_epu64_mask'.
// Requires AVX512F.
func CmpneqEpu64Mask2(a M512i, b M512i) Mmask8 {
	return Mmask8(cmpneqEpu64Mask2([64]byte(a), [64]byte(b)))
}

func cmpneqEpu64Mask2(a [64]byte, b [64]byte) uint8


// MaskCmpneqEpu64Mask2: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for not-equal, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm512_mask_cmpneq_epu64_mask'.
// Requires AVX512F.
func MaskCmpneqEpu64Mask2(k1 Mmask8, a M512i, b M512i) Mmask8 {
	return Mmask8(maskCmpneqEpu64Mask2(uint8(k1), [64]byte(a), [64]byte(b)))
}

func maskCmpneqEpu64Mask2(k1 uint8, a [64]byte, b [64]byte) uint8


// ComiRoundSd: Compare the lower double-precision (64-bit) floating-point
// element in 'a' and 'b' based on the comparison operand specified by 'imm8',
// and return the boolean result (0 or 1).
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		
//		RETURN ( a[63:0] OP b[63:0] ) ? 1 : 0
//
// Instruction: 'VCOMISD'. Intrinsic: '_mm_comi_round_sd'.
// Requires AVX512F.
func ComiRoundSd(a M128d, b M128d, imm8 int, sae int) int {
	return int(comiRoundSd([2]float64(a), [2]float64(b), imm8, sae))
}

func comiRoundSd(a [2]float64, b [2]float64, imm8 int, sae int) int


// ComiRoundSs: Compare the lower single-precision (32-bit) floating-point
// element in 'a' and 'b' based on the comparison operand specified by 'imm8',
// and return the boolean result (0 or 1).
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		
//		RETURN ( a[31:0] OP b[31:0] ) ? 1 : 0
//
// Instruction: 'VCOMISS'. Intrinsic: '_mm_comi_round_ss'.
// Requires AVX512F.
func ComiRoundSs(a M128, b M128, imm8 int, sae int) int {
	return int(comiRoundSs([4]float32(a), [4]float32(b), imm8, sae))
}

func comiRoundSs(a [4]float32, b [4]float32, imm8 int, sae int) int


// MaskCompressEpi32: Contiguously store the active 32-bit integers in 'a'
// (those with their respective bit set in writemask 'k') to 'dst', and pass
// through the remaining elements from 'src'. 
//
//		size := 32
//		m := 0
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[127:m] := src[127:m]
//		dst[MAX:128] := 0
//
// Instruction: 'VPCOMPRESSD'. Intrinsic: '_mm_mask_compress_epi32'.
// Requires AVX512F.
func MaskCompressEpi32(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCompressEpi32([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCompressEpi32(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCompressEpi32: Contiguously store the active 32-bit integers in 'a'
// (those with their respective bit set in zeromask 'k') to 'dst', and set the
// remaining elements to zero. 
//
//		size := 32
//		m := 0
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[127:m] := 0
//		dst[MAX:128] := 0
//
// Instruction: 'VPCOMPRESSD'. Intrinsic: '_mm_maskz_compress_epi32'.
// Requires AVX512F.
func MaskzCompressEpi32(k Mmask8, a M128i) M128i {
	return M128i(maskzCompressEpi32(uint8(k), [16]byte(a)))
}

func maskzCompressEpi32(k uint8, a [16]byte) [16]byte


// MaskCompressEpi321: Contiguously store the active 32-bit integers in 'a'
// (those with their respective bit set in writemask 'k') to 'dst', and pass
// through the remaining elements from 'src'. 
//
//		size := 32
//		m := 0
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[255:m] := src[255:m]
//		dst[MAX:256] := 0
//
// Instruction: 'VPCOMPRESSD'. Intrinsic: '_mm256_mask_compress_epi32'.
// Requires AVX512F.
func MaskCompressEpi321(src M256i, k Mmask8, a M256i) M256i {
	return M256i(maskCompressEpi321([32]byte(src), uint8(k), [32]byte(a)))
}

func maskCompressEpi321(src [32]byte, k uint8, a [32]byte) [32]byte


// MaskzCompressEpi321: Contiguously store the active 32-bit integers in 'a'
// (those with their respective bit set in zeromask 'k') to 'dst', and set the
// remaining elements to zero. 
//
//		size := 32
//		m := 0
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[255:m] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VPCOMPRESSD'. Intrinsic: '_mm256_maskz_compress_epi32'.
// Requires AVX512F.
func MaskzCompressEpi321(k Mmask8, a M256i) M256i {
	return M256i(maskzCompressEpi321(uint8(k), [32]byte(a)))
}

func maskzCompressEpi321(k uint8, a [32]byte) [32]byte


// MaskCompressEpi322: Contiguously store the active 32-bit integers in 'a'
// (those with their respective bit set in writemask 'k') to 'dst', and pass
// through the remaining elements from 'src'. 
//
//		size := 32
//		m := 0
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[511:m] := src[511:m]
//		dst[MAX:512] := 0
//
// Instruction: 'VPCOMPRESSD'. Intrinsic: '_mm512_mask_compress_epi32'.
// Requires AVX512F.
func MaskCompressEpi322(src M512i, k Mmask16, a M512i) M512i {
	return M512i(maskCompressEpi322([64]byte(src), uint16(k), [64]byte(a)))
}

func maskCompressEpi322(src [64]byte, k uint16, a [64]byte) [64]byte


// MaskzCompressEpi322: Contiguously store the active 32-bit integers in 'a'
// (those with their respective bit set in zeromask 'k') to 'dst', and set the
// remaining elements to zero. 
//
//		size := 32
//		m := 0
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[511:m] := 0
//		dst[MAX:512] := 0
//
// Instruction: 'VPCOMPRESSD'. Intrinsic: '_mm512_maskz_compress_epi32'.
// Requires AVX512F.
func MaskzCompressEpi322(k Mmask16, a M512i) M512i {
	return M512i(maskzCompressEpi322(uint16(k), [64]byte(a)))
}

func maskzCompressEpi322(k uint16, a [64]byte) [64]byte


// MaskCompressEpi64: Contiguously store the active 64-bit integers in 'a'
// (those with their respective bit set in writemask 'k') to 'dst', and pass
// through the remaining elements from 'src'. 
//
//		size := 64
//		m := 0
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[127:m] := src[127:m]
//		dst[MAX:128] := 0
//
// Instruction: 'VPCOMPRESSQ'. Intrinsic: '_mm_mask_compress_epi64'.
// Requires AVX512F.
func MaskCompressEpi64(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCompressEpi64([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCompressEpi64(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCompressEpi64: Contiguously store the active 64-bit integers in 'a'
// (those with their respective bit set in zeromask 'k') to 'dst', and set the
// remaining elements to zero. 
//
//		size := 64
//		m := 0
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[127:m] := 0
//		dst[MAX:128] := 0
//
// Instruction: 'VPCOMPRESSQ'. Intrinsic: '_mm_maskz_compress_epi64'.
// Requires AVX512F.
func MaskzCompressEpi64(k Mmask8, a M128i) M128i {
	return M128i(maskzCompressEpi64(uint8(k), [16]byte(a)))
}

func maskzCompressEpi64(k uint8, a [16]byte) [16]byte


// MaskCompressEpi641: Contiguously store the active 64-bit integers in 'a'
// (those with their respective bit set in writemask 'k') to 'dst', and pass
// through the remaining elements from 'src'. 
//
//		size := 64
//		m := 0
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[255:m] := src[255:m]
//		dst[MAX:256] := 0
//
// Instruction: 'VPCOMPRESSQ'. Intrinsic: '_mm256_mask_compress_epi64'.
// Requires AVX512F.
func MaskCompressEpi641(src M256i, k Mmask8, a M256i) M256i {
	return M256i(maskCompressEpi641([32]byte(src), uint8(k), [32]byte(a)))
}

func maskCompressEpi641(src [32]byte, k uint8, a [32]byte) [32]byte


// MaskzCompressEpi641: Contiguously store the active 64-bit integers in 'a'
// (those with their respective bit set in zeromask 'k') to 'dst', and set the
// remaining elements to zero. 
//
//		size := 64
//		m := 0
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[255:m] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VPCOMPRESSQ'. Intrinsic: '_mm256_maskz_compress_epi64'.
// Requires AVX512F.
func MaskzCompressEpi641(k Mmask8, a M256i) M256i {
	return M256i(maskzCompressEpi641(uint8(k), [32]byte(a)))
}

func maskzCompressEpi641(k uint8, a [32]byte) [32]byte


// MaskCompressEpi642: Contiguously store the active 64-bit integers in 'a'
// (those with their respective bit set in writemask 'k') to 'dst', and pass
// through the remaining elements from 'src'. 
//
//		size := 64
//		m := 0
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[511:m] := src[511:m]
//		dst[MAX:512] := 0
//
// Instruction: 'VPCOMPRESSQ'. Intrinsic: '_mm512_mask_compress_epi64'.
// Requires AVX512F.
func MaskCompressEpi642(src M512i, k Mmask8, a M512i) M512i {
	return M512i(maskCompressEpi642([64]byte(src), uint8(k), [64]byte(a)))
}

func maskCompressEpi642(src [64]byte, k uint8, a [64]byte) [64]byte


// MaskzCompressEpi642: Contiguously store the active 64-bit integers in 'a'
// (those with their respective bit set in zeromask 'k') to 'dst', and set the
// remaining elements to zero. 
//
//		size := 64
//		m := 0
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[511:m] := 0
//		dst[MAX:512] := 0
//
// Instruction: 'VPCOMPRESSQ'. Intrinsic: '_mm512_maskz_compress_epi64'.
// Requires AVX512F.
func MaskzCompressEpi642(k Mmask8, a M512i) M512i {
	return M512i(maskzCompressEpi642(uint8(k), [64]byte(a)))
}

func maskzCompressEpi642(k uint8, a [64]byte) [64]byte


// MaskCompressPd: Contiguously store the active double-precision (64-bit)
// floating-point elements in 'a' (those with their respective bit set in
// writemask 'k') to 'dst', and pass through the remaining elements from 'src'. 
//
//		size := 64
//		m := 0
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[127:m] := src[127:m]
//		dst[MAX:128] := 0
//
// Instruction: 'VCOMPRESSPD'. Intrinsic: '_mm_mask_compress_pd'.
// Requires AVX512F.
func MaskCompressPd(src M128d, k Mmask8, a M128d) M128d {
	return M128d(maskCompressPd([2]float64(src), uint8(k), [2]float64(a)))
}

func maskCompressPd(src [2]float64, k uint8, a [2]float64) [2]float64


// MaskzCompressPd: Contiguously store the active double-precision (64-bit)
// floating-point elements in 'a' (those with their respective bit set in
// zeromask 'k') to 'dst', and set the remaining elements to zero. 
//
//		size := 64
//		m := 0
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[127:m] := 0
//		dst[MAX:128] := 0
//
// Instruction: 'VCOMPRESSPD'. Intrinsic: '_mm_maskz_compress_pd'.
// Requires AVX512F.
func MaskzCompressPd(k Mmask8, a M128d) M128d {
	return M128d(maskzCompressPd(uint8(k), [2]float64(a)))
}

func maskzCompressPd(k uint8, a [2]float64) [2]float64


// MaskCompressPd1: Contiguously store the active double-precision (64-bit)
// floating-point elements in 'a' (those with their respective bit set in
// writemask 'k') to 'dst', and pass through the remaining elements from 'src'. 
//
//		size := 64
//		m := 0
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[255:m] := src[255:m]
//		dst[MAX:256] := 0
//
// Instruction: 'VCOMPRESSPD'. Intrinsic: '_mm256_mask_compress_pd'.
// Requires AVX512F.
func MaskCompressPd1(src M256d, k Mmask8, a M256d) M256d {
	return M256d(maskCompressPd1([4]float64(src), uint8(k), [4]float64(a)))
}

func maskCompressPd1(src [4]float64, k uint8, a [4]float64) [4]float64


// MaskzCompressPd1: Contiguously store the active double-precision (64-bit)
// floating-point elements in 'a' (those with their respective bit set in
// zeromask 'k') to 'dst', and set the remaining elements to zero. 
//
//		size := 64
//		m := 0
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[255:m] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VCOMPRESSPD'. Intrinsic: '_mm256_maskz_compress_pd'.
// Requires AVX512F.
func MaskzCompressPd1(k Mmask8, a M256d) M256d {
	return M256d(maskzCompressPd1(uint8(k), [4]float64(a)))
}

func maskzCompressPd1(k uint8, a [4]float64) [4]float64


// MaskCompressPd2: Contiguously store the active double-precision (64-bit)
// floating-point elements in 'a' (those with their respective bit set in
// writemask 'k') to 'dst', and pass through the remaining elements from 'src'. 
//
//		size := 64
//		m := 0
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[511:m] := src[511:m]
//		dst[MAX:512] := 0
//
// Instruction: 'VCOMPRESSPD'. Intrinsic: '_mm512_mask_compress_pd'.
// Requires AVX512F.
func MaskCompressPd2(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskCompressPd2([8]float64(src), uint8(k), [8]float64(a)))
}

func maskCompressPd2(src [8]float64, k uint8, a [8]float64) [8]float64


// MaskzCompressPd2: Contiguously store the active double-precision (64-bit)
// floating-point elements in 'a' (those with their respective bit set in
// zeromask 'k') to 'dst', and set the remaining elements to zero. 
//
//		size := 64
//		m := 0
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[511:m] := 0
//		dst[MAX:512] := 0
//
// Instruction: 'VCOMPRESSPD'. Intrinsic: '_mm512_maskz_compress_pd'.
// Requires AVX512F.
func MaskzCompressPd2(k Mmask8, a M512d) M512d {
	return M512d(maskzCompressPd2(uint8(k), [8]float64(a)))
}

func maskzCompressPd2(k uint8, a [8]float64) [8]float64


// MaskCompressPs: Contiguously store the active single-precision (32-bit)
// floating-point elements in 'a' (those with their respective bit set in
// writemask 'k') to 'dst', and pass through the remaining elements from 'src'. 
//
//		size := 32
//		m := 0
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[127:m] := src[127:m]
//		dst[MAX:128] := 0
//
// Instruction: 'VCOMPRESSPS'. Intrinsic: '_mm_mask_compress_ps'.
// Requires AVX512F.
func MaskCompressPs(src M128, k Mmask8, a M128) M128 {
	return M128(maskCompressPs([4]float32(src), uint8(k), [4]float32(a)))
}

func maskCompressPs(src [4]float32, k uint8, a [4]float32) [4]float32


// MaskzCompressPs: Contiguously store the active single-precision (32-bit)
// floating-point elements in 'a' (those with their respective bit set in
// zeromask 'k') to 'dst', and set the remaining elements to zero. 
//
//		size := 32
//		m := 0
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[127:m] := 0
//		dst[MAX:128] := 0
//
// Instruction: 'VCOMPRESSPS'. Intrinsic: '_mm_maskz_compress_ps'.
// Requires AVX512F.
func MaskzCompressPs(k Mmask8, a M128) M128 {
	return M128(maskzCompressPs(uint8(k), [4]float32(a)))
}

func maskzCompressPs(k uint8, a [4]float32) [4]float32


// MaskCompressPs1: Contiguously store the active single-precision (32-bit)
// floating-point elements in 'a' (those with their respective bit set in
// writemask 'k') to 'dst', and pass through the remaining elements from 'src'. 
//
//		size := 32
//		m := 0
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[255:m] := src[255:m]
//		dst[MAX:256] := 0
//
// Instruction: 'VCOMPRESSPS'. Intrinsic: '_mm256_mask_compress_ps'.
// Requires AVX512F.
func MaskCompressPs1(src M256, k Mmask8, a M256) M256 {
	return M256(maskCompressPs1([8]float32(src), uint8(k), [8]float32(a)))
}

func maskCompressPs1(src [8]float32, k uint8, a [8]float32) [8]float32


// MaskzCompressPs1: Contiguously store the active single-precision (32-bit)
// floating-point elements in 'a' (those with their respective bit set in
// zeromask 'k') to 'dst', and set the remaining elements to zero. 
//
//		size := 32
//		m := 0
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[255:m] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VCOMPRESSPS'. Intrinsic: '_mm256_maskz_compress_ps'.
// Requires AVX512F.
func MaskzCompressPs1(k Mmask8, a M256) M256 {
	return M256(maskzCompressPs1(uint8(k), [8]float32(a)))
}

func maskzCompressPs1(k uint8, a [8]float32) [8]float32


// MaskCompressPs2: Contiguously store the active single-precision (32-bit)
// floating-point elements in 'a' (those with their respective bit set in
// writemask 'k') to 'dst', and pass through the remaining elements from 'src'. 
//
//		size := 32
//		m := 0
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[511:m] := src[511:m]
//		dst[MAX:512] := 0
//
// Instruction: 'VCOMPRESSPS'. Intrinsic: '_mm512_mask_compress_ps'.
// Requires AVX512F.
func MaskCompressPs2(src M512, k Mmask16, a M512) M512 {
	return M512(maskCompressPs2([16]float32(src), uint16(k), [16]float32(a)))
}

func maskCompressPs2(src [16]float32, k uint16, a [16]float32) [16]float32


// MaskzCompressPs2: Contiguously store the active single-precision (32-bit)
// floating-point elements in 'a' (those with their respective bit set in
// zeromask 'k') to 'dst', and set the remaining elements to zero. 
//
//		size := 32
//		m := 0
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[511:m] := 0
//		dst[MAX:512] := 0
//
// Instruction: 'VCOMPRESSPS'. Intrinsic: '_mm512_maskz_compress_ps'.
// Requires AVX512F.
func MaskzCompressPs2(k Mmask16, a M512) M512 {
	return M512(maskzCompressPs2(uint16(k), [16]float32(a)))
}

func maskzCompressPs2(k uint16, a [16]float32) [16]float32


// MaskCompressstoreuEpi32: Contiguously store the active 32-bit integers in
// 'a' (those with their respective bit set in writemask 'k') to unaligned
// memory at 'base_addr'. 
//
//		size := 32
//		m := base_addr
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				MEM[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//
// Instruction: 'VPCOMPRESSD'. Intrinsic: '_mm_mask_compressstoreu_epi32'.
// Requires AVX512F.
func MaskCompressstoreuEpi32(base_addr uintptr, k Mmask8, a M128i)  {
	maskCompressstoreuEpi32(uintptr(base_addr), uint8(k), [16]byte(a))
}

func maskCompressstoreuEpi32(base_addr uintptr, k uint8, a [16]byte) 


// MaskCompressstoreuEpi321: Contiguously store the active 32-bit integers in
// 'a' (those with their respective bit set in writemask 'k') to unaligned
// memory at 'base_addr'. 
//
//		size := 32
//		m := base_addr
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				MEM[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//
// Instruction: 'VPCOMPRESSD'. Intrinsic: '_mm256_mask_compressstoreu_epi32'.
// Requires AVX512F.
func MaskCompressstoreuEpi321(base_addr uintptr, k Mmask8, a M256i)  {
	maskCompressstoreuEpi321(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCompressstoreuEpi321(base_addr uintptr, k uint8, a [32]byte) 


// MaskCompressstoreuEpi322: Contiguously store the active 32-bit integers in
// 'a' (those with their respective bit set in writemask 'k') to unaligned
// memory at 'base_addr'. 
//
//		size := 32
//		m := base_addr
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				MEM[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//
// Instruction: 'VPCOMPRESSD'. Intrinsic: '_mm512_mask_compressstoreu_epi32'.
// Requires AVX512F.
func MaskCompressstoreuEpi322(base_addr uintptr, k Mmask16, a M512i)  {
	maskCompressstoreuEpi322(uintptr(base_addr), uint16(k), [64]byte(a))
}

func maskCompressstoreuEpi322(base_addr uintptr, k uint16, a [64]byte) 


// MaskCompressstoreuEpi64: Contiguously store the active 64-bit integers in
// 'a' (those with their respective bit set in writemask 'k') to unaligned
// memory at 'base_addr'. 
//
//		size := 64
//		m := base_addr
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				MEM[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//
// Instruction: 'VPCOMPRESSQ'. Intrinsic: '_mm_mask_compressstoreu_epi64'.
// Requires AVX512F.
func MaskCompressstoreuEpi64(base_addr uintptr, k Mmask8, a M128i)  {
	maskCompressstoreuEpi64(uintptr(base_addr), uint8(k), [16]byte(a))
}

func maskCompressstoreuEpi64(base_addr uintptr, k uint8, a [16]byte) 


// MaskCompressstoreuEpi641: Contiguously store the active 64-bit integers in
// 'a' (those with their respective bit set in writemask 'k') to unaligned
// memory at 'base_addr'. 
//
//		size := 64
//		m := base_addr
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				MEM[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//
// Instruction: 'VPCOMPRESSQ'. Intrinsic: '_mm256_mask_compressstoreu_epi64'.
// Requires AVX512F.
func MaskCompressstoreuEpi641(base_addr uintptr, k Mmask8, a M256i)  {
	maskCompressstoreuEpi641(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCompressstoreuEpi641(base_addr uintptr, k uint8, a [32]byte) 


// MaskCompressstoreuEpi642: Contiguously store the active 64-bit integers in
// 'a' (those with their respective bit set in writemask 'k') to unaligned
// memory at 'base_addr'. 
//
//		size := 64
//		m := base_addr
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				MEM[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//
// Instruction: 'VPCOMPRESSQ'. Intrinsic: '_mm512_mask_compressstoreu_epi64'.
// Requires AVX512F.
func MaskCompressstoreuEpi642(base_addr uintptr, k Mmask8, a M512i)  {
	maskCompressstoreuEpi642(uintptr(base_addr), uint8(k), [64]byte(a))
}

func maskCompressstoreuEpi642(base_addr uintptr, k uint8, a [64]byte) 


// MaskCompressstoreuPd: Contiguously store the active double-precision
// (64-bit) floating-point elements in 'a' (those with their respective bit set
// in writemask 'k') to unaligned memory at 'base_addr'. 
//
//		size := 64
//		m := base_addr
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				MEM[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//
// Instruction: 'VCOMPRESSPD'. Intrinsic: '_mm_mask_compressstoreu_pd'.
// Requires AVX512F.
func MaskCompressstoreuPd(base_addr uintptr, k Mmask8, a M128d)  {
	maskCompressstoreuPd(uintptr(base_addr), uint8(k), [2]float64(a))
}

func maskCompressstoreuPd(base_addr uintptr, k uint8, a [2]float64) 


// MaskCompressstoreuPd1: Contiguously store the active double-precision
// (64-bit) floating-point elements in 'a' (those with their respective bit set
// in writemask 'k') to unaligned memory at 'base_addr'. 
//
//		size := 64
//		m := base_addr
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				MEM[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//
// Instruction: 'VCOMPRESSPD'. Intrinsic: '_mm256_mask_compressstoreu_pd'.
// Requires AVX512F.
func MaskCompressstoreuPd1(base_addr uintptr, k Mmask8, a M256d)  {
	maskCompressstoreuPd1(uintptr(base_addr), uint8(k), [4]float64(a))
}

func maskCompressstoreuPd1(base_addr uintptr, k uint8, a [4]float64) 


// MaskCompressstoreuPd2: Contiguously store the active double-precision
// (64-bit) floating-point elements in 'a' (those with their respective bit set
// in writemask 'k') to unaligned memory at 'base_addr'. 
//
//		size := 64
//		m := base_addr
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				MEM[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//
// Instruction: 'VCOMPRESSPD'. Intrinsic: '_mm512_mask_compressstoreu_pd'.
// Requires AVX512F.
func MaskCompressstoreuPd2(base_addr uintptr, k Mmask8, a M512d)  {
	maskCompressstoreuPd2(uintptr(base_addr), uint8(k), [8]float64(a))
}

func maskCompressstoreuPd2(base_addr uintptr, k uint8, a [8]float64) 


// MaskCompressstoreuPs: Contiguously store the active single-precision
// (32-bit) floating-point elements in 'a' (those with their respective bit set
// in writemask 'k') to unaligned memory at 'base_addr'. 
//
//		size := 32
//		m := base_addr
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				MEM[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//
// Instruction: 'VCOMPRESSPS'. Intrinsic: '_mm_mask_compressstoreu_ps'.
// Requires AVX512F.
func MaskCompressstoreuPs(base_addr uintptr, k Mmask8, a M128)  {
	maskCompressstoreuPs(uintptr(base_addr), uint8(k), [4]float32(a))
}

func maskCompressstoreuPs(base_addr uintptr, k uint8, a [4]float32) 


// MaskCompressstoreuPs1: Contiguously store the active single-precision
// (32-bit) floating-point elements in 'a' (those with their respective bit set
// in writemask 'k') to unaligned memory at 'base_addr'. 
//
//		size := 32
//		m := base_addr
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				MEM[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//
// Instruction: 'VCOMPRESSPS'. Intrinsic: '_mm256_mask_compressstoreu_ps'.
// Requires AVX512F.
func MaskCompressstoreuPs1(base_addr uintptr, k Mmask8, a M256)  {
	maskCompressstoreuPs1(uintptr(base_addr), uint8(k), [8]float32(a))
}

func maskCompressstoreuPs1(base_addr uintptr, k uint8, a [8]float32) 


// MaskCompressstoreuPs2: Contiguously store the active single-precision
// (32-bit) floating-point elements in 'a' (those with their respective bit set
// in writemask 'k') to unaligned memory at 'base_addr'. 
//
//		size := 32
//		m := base_addr
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				MEM[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//
// Instruction: 'VCOMPRESSPS'. Intrinsic: '_mm512_mask_compressstoreu_ps'.
// Requires AVX512F.
func MaskCompressstoreuPs2(base_addr uintptr, k Mmask16, a M512)  {
	maskCompressstoreuPs2(uintptr(base_addr), uint16(k), [16]float32(a))
}

func maskCompressstoreuPs2(base_addr uintptr, k uint16, a [16]float32) 


// CosPd: Compute the cosine of packed double-precision (64-bit) floating-point
// elements in 'a' expressed in radians, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := COS(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_cos_pd'.
// Requires AVX512F.
func CosPd(a M512d) M512d {
	return M512d(cosPd([8]float64(a)))
}

func cosPd(a [8]float64) [8]float64


// MaskCosPd: Compute the cosine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := COS(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_cos_pd'.
// Requires AVX512F.
func MaskCosPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskCosPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskCosPd(src [8]float64, k uint8, a [8]float64) [8]float64


// CosPs: Compute the cosine of packed single-precision (32-bit) floating-point
// elements in 'a' expressed in radians, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := COS(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_cos_ps'.
// Requires AVX512F.
func CosPs(a M512) M512 {
	return M512(cosPs([16]float32(a)))
}

func cosPs(a [16]float32) [16]float32


// MaskCosPs: Compute the cosine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := COS(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_cos_ps'.
// Requires AVX512F.
func MaskCosPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskCosPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskCosPs(src [16]float32, k uint16, a [16]float32) [16]float32


// CosdPd: Compute the cosine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in degrees, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := COSD(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_cosd_pd'.
// Requires AVX512F.
func CosdPd(a M512d) M512d {
	return M512d(cosdPd([8]float64(a)))
}

func cosdPd(a [8]float64) [8]float64


// MaskCosdPd: Compute the cosine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in degrees, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := COSD(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_cosd_pd'.
// Requires AVX512F.
func MaskCosdPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskCosdPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskCosdPd(src [8]float64, k uint8, a [8]float64) [8]float64


// CosdPs: Compute the cosine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in degrees, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := COSD(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_cosd_ps'.
// Requires AVX512F.
func CosdPs(a M512) M512 {
	return M512(cosdPs([16]float32(a)))
}

func cosdPs(a [16]float32) [16]float32


// MaskCosdPs: Compute the cosine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in degrees, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := COSD(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_cosd_ps'.
// Requires AVX512F.
func MaskCosdPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskCosdPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskCosdPs(src [16]float32, k uint16, a [16]float32) [16]float32


// CoshPd: Compute the hyperbolic cosine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := COSH(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_cosh_pd'.
// Requires AVX512F.
func CoshPd(a M512d) M512d {
	return M512d(coshPd([8]float64(a)))
}

func coshPd(a [8]float64) [8]float64


// MaskCoshPd: Compute the hyperbolic cosine of packed double-precision
// (64-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := COSH(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_cosh_pd'.
// Requires AVX512F.
func MaskCoshPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskCoshPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskCoshPd(src [8]float64, k uint8, a [8]float64) [8]float64


// CoshPs: Compute the hyperbolic cosine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := COSH(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_cosh_ps'.
// Requires AVX512F.
func CoshPs(a M512) M512 {
	return M512(coshPs([16]float32(a)))
}

func coshPs(a [16]float32) [16]float32


// MaskCoshPs: Compute the hyperbolic cosine of packed single-precision
// (32-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := COSH(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_cosh_ps'.
// Requires AVX512F.
func MaskCoshPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskCoshPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskCoshPs(src [16]float32, k uint16, a [16]float32) [16]float32


// CvtRoundepi32Ps: Convert packed 32-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTDQ2PS'. Intrinsic: '_mm512_cvt_roundepi32_ps'.
// Requires AVX512F.
func CvtRoundepi32Ps(a M512i, rounding int) M512 {
	return M512(cvtRoundepi32Ps([64]byte(a), rounding))
}

func cvtRoundepi32Ps(a [64]byte, rounding int) [16]float32


// MaskCvtRoundepi32Ps: Convert packed 32-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTDQ2PS'. Intrinsic: '_mm512_mask_cvt_roundepi32_ps'.
// Requires AVX512F.
func MaskCvtRoundepi32Ps(src M512, k Mmask16, a M512i, rounding int) M512 {
	return M512(maskCvtRoundepi32Ps([16]float32(src), uint16(k), [64]byte(a), rounding))
}

func maskCvtRoundepi32Ps(src [16]float32, k uint16, a [64]byte, rounding int) [16]float32


// MaskzCvtRoundepi32Ps: Convert packed 32-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTDQ2PS'. Intrinsic: '_mm512_maskz_cvt_roundepi32_ps'.
// Requires AVX512F.
func MaskzCvtRoundepi32Ps(k Mmask16, a M512i, rounding int) M512 {
	return M512(maskzCvtRoundepi32Ps(uint16(k), [64]byte(a), rounding))
}

func maskzCvtRoundepi32Ps(k uint16, a [64]byte, rounding int) [16]float32


// CvtRoundepu32Ps: Convert packed unsigned 32-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := ConvertUnsignedInt32_To_FP32(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTUDQ2PS'. Intrinsic: '_mm512_cvt_roundepu32_ps'.
// Requires AVX512F.
func CvtRoundepu32Ps(a M512i, rounding int) M512 {
	return M512(cvtRoundepu32Ps([64]byte(a), rounding))
}

func cvtRoundepu32Ps(a [64]byte, rounding int) [16]float32


// MaskCvtRoundepu32Ps: Convert packed unsigned 32-bit integers in 'a' to
// packed single-precision (32-bit) floating-point elements, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ConvertUnsignedInt32_To_FP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTUDQ2PS'. Intrinsic: '_mm512_mask_cvt_roundepu32_ps'.
// Requires AVX512F.
func MaskCvtRoundepu32Ps(src M512, k Mmask16, a M512i, rounding int) M512 {
	return M512(maskCvtRoundepu32Ps([16]float32(src), uint16(k), [64]byte(a), rounding))
}

func maskCvtRoundepu32Ps(src [16]float32, k uint16, a [64]byte, rounding int) [16]float32


// MaskzCvtRoundepu32Ps: Convert packed unsigned 32-bit integers in 'a' to
// packed single-precision (32-bit) floating-point elements, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := ConvertUnsignedInt32_To_FP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTUDQ2PS'. Intrinsic: '_mm512_maskz_cvt_roundepu32_ps'.
// Requires AVX512F.
func MaskzCvtRoundepu32Ps(k Mmask16, a M512i, rounding int) M512 {
	return M512(maskzCvtRoundepu32Ps(uint16(k), [64]byte(a), rounding))
}

func maskzCvtRoundepu32Ps(k uint16, a [64]byte, rounding int) [16]float32


// CvtRoundi32Ss: Convert the 32-bit integer 'b' to a single-precision (32-bit)
// floating-point element, store the result in the lower element of 'dst', and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_Int32_To_FP32(b[31:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTSI2SS'. Intrinsic: '_mm_cvt_roundi32_ss'.
// Requires AVX512F.
func CvtRoundi32Ss(a M128, b int, rounding int) M128 {
	return M128(cvtRoundi32Ss([4]float32(a), b, rounding))
}

func cvtRoundi32Ss(a [4]float32, b int, rounding int) [4]float32


// CvtRoundi64Sd: Convert the 64-bit integer 'b' to a double-precision (64-bit)
// floating-point element, store the result in the lower element of 'dst', and
// copy the upper element from 'a' to the upper element of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_Int64_To_FP64(b[63:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTSI2SD'. Intrinsic: '_mm_cvt_roundi64_sd'.
// Requires AVX512F.
func CvtRoundi64Sd(a M128d, b int64, rounding int) M128d {
	return M128d(cvtRoundi64Sd([2]float64(a), b, rounding))
}

func cvtRoundi64Sd(a [2]float64, b int64, rounding int) [2]float64


// CvtRoundi64Ss: Convert the 64-bit integer 'b' to a single-precision (32-bit)
// floating-point element, store the result in the lower element of 'dst', and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_Int64_To_FP32(b[63:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTSI2SS'. Intrinsic: '_mm_cvt_roundi64_ss'.
// Requires AVX512F.
func CvtRoundi64Ss(a M128, b int64, rounding int) M128 {
	return M128(cvtRoundi64Ss([4]float32(a), b, rounding))
}

func cvtRoundi64Ss(a [4]float32, b int64, rounding int) [4]float32


// CvtRoundpdEpi32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_Int32(a[k+63:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2DQ'. Intrinsic: '_mm512_cvt_roundpd_epi32'.
// Requires AVX512F.
func CvtRoundpdEpi32(a M512d, rounding int) M256i {
	return M256i(cvtRoundpdEpi32([8]float64(a), rounding))
}

func cvtRoundpdEpi32(a [8]float64, rounding int) [32]byte


// MaskCvtRoundpdEpi32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*32
//			l := j*64
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_Int32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2DQ'. Intrinsic: '_mm512_mask_cvt_roundpd_epi32'.
// Requires AVX512F.
func MaskCvtRoundpdEpi32(src M256i, k Mmask8, a M512d, rounding int) M256i {
	return M256i(maskCvtRoundpdEpi32([32]byte(src), uint8(k), [8]float64(a), rounding))
}

func maskCvtRoundpdEpi32(src [32]byte, k uint8, a [8]float64, rounding int) [32]byte


// MaskzCvtRoundpdEpi32: Convert packed double-precision (64-bit)
// floating-point elements in 'a' to packed 32-bit integers, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_Int32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2DQ'. Intrinsic: '_mm512_maskz_cvt_roundpd_epi32'.
// Requires AVX512F.
func MaskzCvtRoundpdEpi32(k Mmask8, a M512d, rounding int) M256i {
	return M256i(maskzCvtRoundpdEpi32(uint8(k), [8]float64(a), rounding))
}

func maskzCvtRoundpdEpi32(k uint8, a [8]float64, rounding int) [32]byte


// CvtRoundpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[k+63:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2UDQ'. Intrinsic: '_mm512_cvt_roundpd_epu32'.
// Requires AVX512F.
func CvtRoundpdEpu32(a M512d, rounding int) M256i {
	return M256i(cvtRoundpdEpu32([8]float64(a), rounding))
}

func cvtRoundpdEpu32(a [8]float64, rounding int) [32]byte


// MaskCvtRoundpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*32
//			l := j*64
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2UDQ'. Intrinsic: '_mm512_mask_cvt_roundpd_epu32'.
// Requires AVX512F.
func MaskCvtRoundpdEpu32(src M256i, k Mmask8, a M512d, rounding int) M256i {
	return M256i(maskCvtRoundpdEpu32([32]byte(src), uint8(k), [8]float64(a), rounding))
}

func maskCvtRoundpdEpu32(src [32]byte, k uint8, a [8]float64, rounding int) [32]byte


// MaskzCvtRoundpdEpu32: Convert packed double-precision (64-bit)
// floating-point elements in 'a' to packed unsigned 32-bit integers, and store
// the results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2UDQ'. Intrinsic: '_mm512_maskz_cvt_roundpd_epu32'.
// Requires AVX512F.
func MaskzCvtRoundpdEpu32(k Mmask8, a M512d, rounding int) M256i {
	return M256i(maskzCvtRoundpdEpu32(uint8(k), [8]float64(a), rounding))
}

func maskzCvtRoundpdEpu32(k uint8, a [8]float64, rounding int) [32]byte


// CvtRoundpdPs: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed single-precision (32-bit) floating-point elements,
// and store the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_FP32(a[k+63:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2PS'. Intrinsic: '_mm512_cvt_roundpd_ps'.
// Requires AVX512F.
func CvtRoundpdPs(a M512d, rounding int) M256 {
	return M256(cvtRoundpdPs([8]float64(a), rounding))
}

func cvtRoundpdPs(a [8]float64, rounding int) [8]float32


// MaskCvtRoundpdPs: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed single-precision (32-bit) floating-point elements,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*32
//			l := j*64
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_FP32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2PS'. Intrinsic: '_mm512_mask_cvt_roundpd_ps'.
// Requires AVX512F.
func MaskCvtRoundpdPs(src M256, k Mmask8, a M512d, rounding int) M256 {
	return M256(maskCvtRoundpdPs([8]float32(src), uint8(k), [8]float64(a), rounding))
}

func maskCvtRoundpdPs(src [8]float32, k uint8, a [8]float64, rounding int) [8]float32


// MaskzCvtRoundpdPs: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed single-precision (32-bit) floating-point elements,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*32
//			l := j*64
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_FP32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2PS'. Intrinsic: '_mm512_maskz_cvt_roundpd_ps'.
// Requires AVX512F.
func MaskzCvtRoundpdPs(k Mmask8, a M512d, rounding int) M256 {
	return M256(maskzCvtRoundpdPs(uint8(k), [8]float64(a), rounding))
}

func maskzCvtRoundpdPs(k uint8, a [8]float64, rounding int) [8]float32


// CvtRoundphPs: Convert packed half-precision (16-bit) floating-point elements
// in 'a' to packed single-precision (32-bit) floating-point elements, and
// store the results in 'dst'. 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 15
//			i := j*32
//			m := j*16
//			dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPH2PS'. Intrinsic: '_mm512_cvt_roundph_ps'.
// Requires AVX512F.
func CvtRoundphPs(a M256i, sae int) M512 {
	return M512(cvtRoundphPs([32]byte(a), sae))
}

func cvtRoundphPs(a [32]byte, sae int) [16]float32


// MaskCvtRoundphPs: Convert packed half-precision (16-bit) floating-point
// elements in 'a' to packed single-precision (32-bit) floating-point elements,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 15
//			i := j*32
//			m := j*16
//			IF k[j]
//				dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPH2PS'. Intrinsic: '_mm512_mask_cvt_roundph_ps'.
// Requires AVX512F.
func MaskCvtRoundphPs(src M512, k Mmask16, a M256i, sae int) M512 {
	return M512(maskCvtRoundphPs([16]float32(src), uint16(k), [32]byte(a), sae))
}

func maskCvtRoundphPs(src [16]float32, k uint16, a [32]byte, sae int) [16]float32


// MaskzCvtRoundphPs: Convert packed half-precision (16-bit) floating-point
// elements in 'a' to packed single-precision (32-bit) floating-point elements,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 15
//			i := j*32
//			m := j*16
//			IF k[j]
//				dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPH2PS'. Intrinsic: '_mm512_maskz_cvt_roundph_ps'.
// Requires AVX512F.
func MaskzCvtRoundphPs(k Mmask16, a M256i, sae int) M512 {
	return M512(maskzCvtRoundphPs(uint16(k), [32]byte(a), sae))
}

func maskzCvtRoundphPs(k uint16, a [32]byte, sae int) [16]float32


// CvtRoundpsEpi32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2DQ'. Intrinsic: '_mm512_cvt_roundps_epi32'.
// Requires AVX512F.
func CvtRoundpsEpi32(a M512, rounding int) M512i {
	return M512i(cvtRoundpsEpi32([16]float32(a), rounding))
}

func cvtRoundpsEpi32(a [16]float32, rounding int) [64]byte


// MaskCvtRoundpsEpi32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2DQ'. Intrinsic: '_mm512_mask_cvt_roundps_epi32'.
// Requires AVX512F.
func MaskCvtRoundpsEpi32(src M512i, k Mmask16, a M512, rounding int) M512i {
	return M512i(maskCvtRoundpsEpi32([64]byte(src), uint16(k), [16]float32(a), rounding))
}

func maskCvtRoundpsEpi32(src [64]byte, k uint16, a [16]float32, rounding int) [64]byte


// MaskzCvtRoundpsEpi32: Convert packed single-precision (32-bit)
// floating-point elements in 'a' to packed 32-bit integers, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2DQ'. Intrinsic: '_mm512_maskz_cvt_roundps_epi32'.
// Requires AVX512F.
func MaskzCvtRoundpsEpi32(k Mmask16, a M512, rounding int) M512i {
	return M512i(maskzCvtRoundpsEpi32(uint16(k), [16]float32(a), rounding))
}

func maskzCvtRoundpsEpi32(k uint16, a [16]float32, rounding int) [64]byte


// CvtRoundpsEpu32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2UDQ'. Intrinsic: '_mm512_cvt_roundps_epu32'.
// Requires AVX512F.
func CvtRoundpsEpu32(a M512, rounding int) M512i {
	return M512i(cvtRoundpsEpu32([16]float32(a), rounding))
}

func cvtRoundpsEpu32(a [16]float32, rounding int) [64]byte


// MaskCvtRoundpsEpu32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2UDQ'. Intrinsic: '_mm512_mask_cvt_roundps_epu32'.
// Requires AVX512F.
func MaskCvtRoundpsEpu32(src M512i, k Mmask16, a M512, rounding int) M512i {
	return M512i(maskCvtRoundpsEpu32([64]byte(src), uint16(k), [16]float32(a), rounding))
}

func maskCvtRoundpsEpu32(src [64]byte, k uint16, a [16]float32, rounding int) [64]byte


// MaskzCvtRoundpsEpu32: Convert packed single-precision (32-bit)
// floating-point elements in 'a' to packed unsigned 32-bit integers, and store
// the results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2UDQ'. Intrinsic: '_mm512_maskz_cvt_roundps_epu32'.
// Requires AVX512F.
func MaskzCvtRoundpsEpu32(k Mmask16, a M512, rounding int) M512i {
	return M512i(maskzCvtRoundpsEpu32(uint16(k), [16]float32(a), rounding))
}

func maskzCvtRoundpsEpu32(k uint16, a [16]float32, rounding int) [64]byte


// CvtRoundpsPd: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed double-precision (64-bit) floating-point elements,
// and store the results in 'dst'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 32*j
//			dst[i+63:i] := Convert_FP32_To_FP64(a[k+31:k])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2PD'. Intrinsic: '_mm512_cvt_roundps_pd'.
// Requires AVX512F.
func CvtRoundpsPd(a M256, sae int) M512d {
	return M512d(cvtRoundpsPd([8]float32(a), sae))
}

func cvtRoundpsPd(a [8]float32, sae int) [8]float64


// MaskCvtRoundpsPd: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed double-precision (64-bit) floating-point elements,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_FP64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2PD'. Intrinsic: '_mm512_mask_cvt_roundps_pd'.
// Requires AVX512F.
func MaskCvtRoundpsPd(src M512d, k Mmask8, a M256, sae int) M512d {
	return M512d(maskCvtRoundpsPd([8]float64(src), uint8(k), [8]float32(a), sae))
}

func maskCvtRoundpsPd(src [8]float64, k uint8, a [8]float32, sae int) [8]float64


// MaskzCvtRoundpsPd: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed double-precision (64-bit) floating-point elements,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set).
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_FP64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2PD'. Intrinsic: '_mm512_maskz_cvt_roundps_pd'.
// Requires AVX512F.
func MaskzCvtRoundpsPd(k Mmask8, a M256, sae int) M512d {
	return M512d(maskzCvtRoundpsPd(uint8(k), [8]float32(a), sae))
}

func maskzCvtRoundpsPd(k uint8, a [8]float32, sae int) [8]float64


// MaskCvtRoundpsPh: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed half-precision (16-bit) floating-point elements,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 3
//			i := 16*j
//			l := 32*j
//			IF k[j]
//				dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm_mask_cvt_roundps_ph'.
// Requires AVX512F.
func MaskCvtRoundpsPh(src M128i, k Mmask8, a M128, rounding int) M128i {
	return M128i(maskCvtRoundpsPh([16]byte(src), uint8(k), [4]float32(a), rounding))
}

func maskCvtRoundpsPh(src [16]byte, k uint8, a [4]float32, rounding int) [16]byte


// MaskzCvtRoundpsPh: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed half-precision (16-bit) floating-point elements,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 3
//			i := 16*j
//			l := 32*j
//			IF k[j]
//				dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm_maskz_cvt_roundps_ph'.
// Requires AVX512F.
func MaskzCvtRoundpsPh(k Mmask8, a M128, rounding int) M128i {
	return M128i(maskzCvtRoundpsPh(uint8(k), [4]float32(a), rounding))
}

func maskzCvtRoundpsPh(k uint8, a [4]float32, rounding int) [16]byte


// MaskCvtRoundpsPh1: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed half-precision (16-bit) floating-point elements,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := 16*j
//			l := 32*j
//			IF k[j]
//				dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm256_mask_cvt_roundps_ph'.
// Requires AVX512F.
func MaskCvtRoundpsPh1(src M128i, k Mmask8, a M256, rounding int) M128i {
	return M128i(maskCvtRoundpsPh1([16]byte(src), uint8(k), [8]float32(a), rounding))
}

func maskCvtRoundpsPh1(src [16]byte, k uint8, a [8]float32, rounding int) [16]byte


// MaskzCvtRoundpsPh1: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed half-precision (16-bit) floating-point elements,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := 16*j
//			l := 32*j
//			IF k[j]
//				dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm256_maskz_cvt_roundps_ph'.
// Requires AVX512F.
func MaskzCvtRoundpsPh1(k Mmask8, a M256, rounding int) M128i {
	return M128i(maskzCvtRoundpsPh1(uint8(k), [8]float32(a), rounding))
}

func maskzCvtRoundpsPh1(k uint8, a [8]float32, rounding int) [16]byte


// CvtRoundpsPh: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed half-precision (16-bit) floating-point elements,
// and store the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 16*j
//			l := 32*j
//			dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm512_cvt_roundps_ph'.
// Requires AVX512F.
func CvtRoundpsPh(a M512, rounding int) M256i {
	return M256i(cvtRoundpsPh([16]float32(a), rounding))
}

func cvtRoundpsPh(a [16]float32, rounding int) [32]byte


// MaskCvtRoundpsPh2: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed half-precision (16-bit) floating-point elements,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 16*j
//			l := 32*j
//			IF k[j]
//				dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm512_mask_cvt_roundps_ph'.
// Requires AVX512F.
func MaskCvtRoundpsPh2(src M256i, k Mmask16, a M512, rounding int) M256i {
	return M256i(maskCvtRoundpsPh2([32]byte(src), uint16(k), [16]float32(a), rounding))
}

func maskCvtRoundpsPh2(src [32]byte, k uint16, a [16]float32, rounding int) [32]byte


// MaskzCvtRoundpsPh2: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed half-precision (16-bit) floating-point elements,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 16*j
//			l := 32*j
//			IF k[j]
//				dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm512_maskz_cvt_roundps_ph'.
// Requires AVX512F.
func MaskzCvtRoundpsPh2(k Mmask16, a M512, rounding int) M256i {
	return M256i(maskzCvtRoundpsPh2(uint16(k), [16]float32(a), rounding))
}

func maskzCvtRoundpsPh2(k uint16, a [16]float32, rounding int) [32]byte


// CvtRoundsdI32: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to a 32-bit integer, and store the result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_FP64_To_Int32(a[63:0])
//
// Instruction: 'VCVTSD2SI'. Intrinsic: '_mm_cvt_roundsd_i32'.
// Requires AVX512F.
func CvtRoundsdI32(a M128d, rounding int) int {
	return int(cvtRoundsdI32([2]float64(a), rounding))
}

func cvtRoundsdI32(a [2]float64, rounding int) int


// CvtRoundsdI64: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to a 64-bit integer, and store the result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_FP64_To_Int64(a[63:0])
//
// Instruction: 'VCVTSD2SI'. Intrinsic: '_mm_cvt_roundsd_i64'.
// Requires AVX512F.
func CvtRoundsdI64(a M128d, rounding int) int64 {
	return int64(cvtRoundsdI64([2]float64(a), rounding))
}

func cvtRoundsdI64(a [2]float64, rounding int) int64


// CvtRoundsdSi32: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to a 32-bit integer, and store the result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_FP64_To_Int32(a[63:0])
//
// Instruction: 'VCVTSD2SI'. Intrinsic: '_mm_cvt_roundsd_si32'.
// Requires AVX512F.
func CvtRoundsdSi32(a M128d, rounding int) int {
	return int(cvtRoundsdSi32([2]float64(a), rounding))
}

func cvtRoundsdSi32(a [2]float64, rounding int) int


// CvtRoundsdSi64: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to a 64-bit integer, and store the result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_FP64_To_Int64(a[63:0])
//
// Instruction: 'VCVTSD2SI'. Intrinsic: '_mm_cvt_roundsd_si64'.
// Requires AVX512F.
func CvtRoundsdSi64(a M128d, rounding int) int64 {
	return int64(cvtRoundsdSi64([2]float64(a), rounding))
}

func cvtRoundsdSi64(a [2]float64, rounding int) int64


// CvtRoundsdSs: Convert the lower double-precision (64-bit) floating-point
// element in 'b' to a single-precision (32-bit) floating-point element, store
// the result in the lower element of 'dst', and copy the upper 3 packed
// elements from 'a' to the upper elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_FP64_To_FP32(b[63:0])
//		dst[127:32] := a[127:31]
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTSD2SS'. Intrinsic: '_mm_cvt_roundsd_ss'.
// Requires AVX512F.
func CvtRoundsdSs(a M128, b M128d, rounding int) M128 {
	return M128(cvtRoundsdSs([4]float32(a), [2]float64(b), rounding))
}

func cvtRoundsdSs(a [4]float32, b [2]float64, rounding int) [4]float32


// MaskCvtRoundsdSs: Convert the lower double-precision (64-bit) floating-point
// element in 'b' to a single-precision (32-bit) floating-point element, store
// the result in the lower element of 'dst' using writemask 'k' (the element is
// copied from 'src' when mask bit 0 is not set), and copy the upper element
// from 'a' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := Convert_FP64_To_FP32(b[63:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:31]
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTSD2SS'. Intrinsic: '_mm_mask_cvt_roundsd_ss'.
// Requires AVX512F.
func MaskCvtRoundsdSs(src M128, k Mmask8, a M128, b M128d, rounding int) M128 {
	return M128(maskCvtRoundsdSs([4]float32(src), uint8(k), [4]float32(a), [2]float64(b), rounding))
}

func maskCvtRoundsdSs(src [4]float32, k uint8, a [4]float32, b [2]float64, rounding int) [4]float32


// MaskzCvtRoundsdSs: Convert the lower double-precision (64-bit)
// floating-point element in 'b' to a single-precision (32-bit) floating-point
// element, store the result in the lower element of 'dst' using zeromask 'k'
// (the element is zeroed out when mask bit 0 is not set), and copy the upper 3
// packed elements from 'a' to the upper elements of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := Convert_FP64_To_FP32(b[63:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:31]
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTSD2SS'. Intrinsic: '_mm_maskz_cvt_roundsd_ss'.
// Requires AVX512F.
func MaskzCvtRoundsdSs(k Mmask8, a M128, b M128d, rounding int) M128 {
	return M128(maskzCvtRoundsdSs(uint8(k), [4]float32(a), [2]float64(b), rounding))
}

func maskzCvtRoundsdSs(k uint8, a [4]float32, b [2]float64, rounding int) [4]float32


// CvtRoundsdU32: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to an unsigned 32-bit integer, and store the result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_FP64_To_UnsignedInt32(a[63:0])
//
// Instruction: 'VCVTSD2USI'. Intrinsic: '_mm_cvt_roundsd_u32'.
// Requires AVX512F.
func CvtRoundsdU32(a M128d, rounding int) uint32 {
	return uint32(cvtRoundsdU32([2]float64(a), rounding))
}

func cvtRoundsdU32(a [2]float64, rounding int) uint32


// CvtRoundsdU64: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to an unsigned 64-bit integer, and store the result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_FP64_To_UnsignedInt64(a[63:0])
//
// Instruction: 'VCVTSD2USI'. Intrinsic: '_mm_cvt_roundsd_u64'.
// Requires AVX512F.
func CvtRoundsdU64(a M128d, rounding int) uint64 {
	return uint64(cvtRoundsdU64([2]float64(a), rounding))
}

func cvtRoundsdU64(a [2]float64, rounding int) uint64


// CvtRoundsi32Ss: Convert the 32-bit integer 'b' to a single-precision
// (32-bit) floating-point element, store the result in the lower element of
// 'dst', and copy the upper 3 packed elements from 'a' to the upper elements
// of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_Int32_To_FP32(b[31:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTSI2SS'. Intrinsic: '_mm_cvt_roundsi32_ss'.
// Requires AVX512F.
func CvtRoundsi32Ss(a M128, b int, rounding int) M128 {
	return M128(cvtRoundsi32Ss([4]float32(a), b, rounding))
}

func cvtRoundsi32Ss(a [4]float32, b int, rounding int) [4]float32


// CvtRoundsi64Sd: Convert the 64-bit integer 'b' to a double-precision
// (64-bit) floating-point element, store the result in the lower element of
// 'dst', and copy the upper element from 'a' to the upper element of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_Int64_To_FP64(b[63:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTSI2SD'. Intrinsic: '_mm_cvt_roundsi64_sd'.
// Requires AVX512F.
func CvtRoundsi64Sd(a M128d, b int64, rounding int) M128d {
	return M128d(cvtRoundsi64Sd([2]float64(a), b, rounding))
}

func cvtRoundsi64Sd(a [2]float64, b int64, rounding int) [2]float64


// CvtRoundsi64Ss: Convert the 64-bit integer 'b' to a single-precision
// (32-bit) floating-point element, store the result in the lower element of
// 'dst', and copy the upper 3 packed elements from 'a' to the upper elements
// of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_Int64_To_FP32(b[63:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTSI2SS'. Intrinsic: '_mm_cvt_roundsi64_ss'.
// Requires AVX512F.
func CvtRoundsi64Ss(a M128, b int64, rounding int) M128 {
	return M128(cvtRoundsi64Ss([4]float32(a), b, rounding))
}

func cvtRoundsi64Ss(a [4]float32, b int64, rounding int) [4]float32


// CvtRoundssI32: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to a 32-bit integer, and store the result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_FP32_To_Int32(a[31:0])
//
// Instruction: 'VCVTSS2SI'. Intrinsic: '_mm_cvt_roundss_i32'.
// Requires AVX512F.
func CvtRoundssI32(a M128, rounding int) int {
	return int(cvtRoundssI32([4]float32(a), rounding))
}

func cvtRoundssI32(a [4]float32, rounding int) int


// CvtRoundssI64: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to a 64-bit integer, and store the result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_FP32_To_Int64(a[31:0])
//
// Instruction: 'VCVTSS2SI'. Intrinsic: '_mm_cvt_roundss_i64'.
// Requires AVX512F.
func CvtRoundssI64(a M128, rounding int) int64 {
	return int64(cvtRoundssI64([4]float32(a), rounding))
}

func cvtRoundssI64(a [4]float32, rounding int) int64


// CvtRoundssSd: Convert the lower single-precision (32-bit) floating-point
// element in 'b' to a double-precision (64-bit) floating-point element, store
// the result in the lower element of 'dst', and copy the upper element from
// 'a' to the upper element of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_FP32_To_FP64(b[31:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTSS2SD'. Intrinsic: '_mm_cvt_roundss_sd'.
// Requires AVX512F.
func CvtRoundssSd(a M128d, b M128, rounding int) M128d {
	return M128d(cvtRoundssSd([2]float64(a), [4]float32(b), rounding))
}

func cvtRoundssSd(a [2]float64, b [4]float32, rounding int) [2]float64


// MaskCvtRoundssSd: Convert the lower single-precision (32-bit) floating-point
// element in 'b' to a double-precision (64-bit) floating-point element, store
// the result in the lower element of 'dst' using writemask 'k' (the element is
// copied from 'src' when mask bit 0 is not set), and copy the upper element
// from 'a' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := Convert_FP32_To_FP64(b[31:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTSS2SD'. Intrinsic: '_mm_mask_cvt_roundss_sd'.
// Requires AVX512F.
func MaskCvtRoundssSd(src M128d, k Mmask8, a M128d, b M128, rounding int) M128d {
	return M128d(maskCvtRoundssSd([2]float64(src), uint8(k), [2]float64(a), [4]float32(b), rounding))
}

func maskCvtRoundssSd(src [2]float64, k uint8, a [2]float64, b [4]float32, rounding int) [2]float64


// MaskzCvtRoundssSd: Convert the lower single-precision (32-bit)
// floating-point element in 'b' to a double-precision (64-bit) floating-point
// element, store the result in the lower element of 'dst' using zeromask 'k'
// (the element is zeroed out when mask bit 0 is not set), and copy the upper
// element from 'a' to the upper element of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := Convert_FP32_To_FP64(b[31:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTSS2SD'. Intrinsic: '_mm_maskz_cvt_roundss_sd'.
// Requires AVX512F.
func MaskzCvtRoundssSd(k Mmask8, a M128d, b M128, rounding int) M128d {
	return M128d(maskzCvtRoundssSd(uint8(k), [2]float64(a), [4]float32(b), rounding))
}

func maskzCvtRoundssSd(k uint8, a [2]float64, b [4]float32, rounding int) [2]float64


// CvtRoundssSi32: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to a 32-bit integer, and store the result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_FP32_To_Int32(a[31:0])
//
// Instruction: 'VCVTSS2SI'. Intrinsic: '_mm_cvt_roundss_si32'.
// Requires AVX512F.
func CvtRoundssSi32(a M128, rounding int) int {
	return int(cvtRoundssSi32([4]float32(a), rounding))
}

func cvtRoundssSi32(a [4]float32, rounding int) int


// CvtRoundssSi64: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to a 64-bit integer, and store the result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_FP32_To_Int64(a[31:0])
//
// Instruction: 'VCVTSS2SI'. Intrinsic: '_mm_cvt_roundss_si64'.
// Requires AVX512F.
func CvtRoundssSi64(a M128, rounding int) int64 {
	return int64(cvtRoundssSi64([4]float32(a), rounding))
}

func cvtRoundssSi64(a [4]float32, rounding int) int64


// CvtRoundssU32: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to an unsigned 32-bit integer, and store the result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_FP32_To_UnsignedInt32(a[31:0])
//
// Instruction: 'VCVTSS2USI'. Intrinsic: '_mm_cvt_roundss_u32'.
// Requires AVX512F.
func CvtRoundssU32(a M128, rounding int) uint32 {
	return uint32(cvtRoundssU32([4]float32(a), rounding))
}

func cvtRoundssU32(a [4]float32, rounding int) uint32


// CvtRoundssU64: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to an unsigned 64-bit integer, and store the result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_FP32_To_UnsignedInt64(a[31:0])
//
// Instruction: 'VCVTSS2USI'. Intrinsic: '_mm_cvt_roundss_u64'.
// Requires AVX512F.
func CvtRoundssU64(a M128, rounding int) uint64 {
	return uint64(cvtRoundssU64([4]float32(a), rounding))
}

func cvtRoundssU64(a [4]float32, rounding int) uint64


// CvtRoundu32Ss: Convert the unsigned 32-bit integer 'b' to a single-precision
// (32-bit) floating-point element, store the result in the lower element of
// 'dst', and copy the upper 3 packed elements from 'a' to the upper elements
// of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_UnsignedInt32_To_FP32(b[31:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTUSI2SS'. Intrinsic: '_mm_cvt_roundu32_ss'.
// Requires AVX512F.
func CvtRoundu32Ss(a M128, b uint32, rounding int) M128 {
	return M128(cvtRoundu32Ss([4]float32(a), b, rounding))
}

func cvtRoundu32Ss(a [4]float32, b uint32, rounding int) [4]float32


// CvtRoundu64Sd: Convert the unsigned 64-bit integer 'b' to a double-precision
// (64-bit) floating-point element, store the result in the lower element of
// 'dst', and copy the upper element from 'a' to the upper element of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_UnsignedInt64_To_FP64(b[63:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTUSI2SD'. Intrinsic: '_mm_cvt_roundu64_sd'.
// Requires AVX512F.
func CvtRoundu64Sd(a M128d, b uint64, rounding int) M128d {
	return M128d(cvtRoundu64Sd([2]float64(a), b, rounding))
}

func cvtRoundu64Sd(a [2]float64, b uint64, rounding int) [2]float64


// CvtRoundu64Ss: Convert the unsigned 64-bit integer 'b' to a single-precision
// (32-bit) floating-point element, store the result in the lower element of
// 'dst', and copy the upper 3 packed elements from 'a' to the upper elements
// of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_UnsignedInt64_To_FP32(b[63:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTUSI2SS'. Intrinsic: '_mm_cvt_roundu64_ss'.
// Requires AVX512F.
func CvtRoundu64Ss(a M128, b uint64, rounding int) M128 {
	return M128(cvtRoundu64Ss([4]float32(a), b, rounding))
}

func cvtRoundu64Ss(a [4]float32, b uint64, rounding int) [4]float32


// MaskCvtepi16Epi32: Sign extend packed 16-bit integers in 'a' to packed
// 32-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			l := j*16
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[l+15:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSXWD'. Intrinsic: '_mm_mask_cvtepi16_epi32'.
// Requires AVX512F.
func MaskCvtepi16Epi32(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtepi16Epi32([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi16Epi32(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepi16Epi32: Sign extend packed 16-bit integers in 'a' to packed
// 32-bit integers, and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[l+15:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSXWD'. Intrinsic: '_mm_maskz_cvtepi16_epi32'.
// Requires AVX512F.
func MaskzCvtepi16Epi32(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtepi16Epi32(uint8(k), [16]byte(a)))
}

func maskzCvtepi16Epi32(k uint8, a [16]byte) [16]byte


// MaskCvtepi16Epi321: Sign extend packed 16-bit integers in 'a' to packed
// 32-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			l := j*16
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[l+15:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXWD'. Intrinsic: '_mm256_mask_cvtepi16_epi32'.
// Requires AVX512F.
func MaskCvtepi16Epi321(src M256i, k Mmask8, a M128i) M256i {
	return M256i(maskCvtepi16Epi321([32]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi16Epi321(src [32]byte, k uint8, a [16]byte) [32]byte


// MaskzCvtepi16Epi321: Sign extend packed 16-bit integers in 'a' to packed
// 32-bit integers, and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[l+15:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXWD'. Intrinsic: '_mm256_maskz_cvtepi16_epi32'.
// Requires AVX512F.
func MaskzCvtepi16Epi321(k Mmask8, a M128i) M256i {
	return M256i(maskzCvtepi16Epi321(uint8(k), [16]byte(a)))
}

func maskzCvtepi16Epi321(k uint8, a [16]byte) [32]byte


// Cvtepi16Epi32: Sign extend packed 16-bit integers in 'a' to packed 32-bit
// integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			k := 16*j
//			dst[i+31:i] := SignExtend(a[k+15:k])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXWD'. Intrinsic: '_mm512_cvtepi16_epi32'.
// Requires AVX512F.
func Cvtepi16Epi32(a M256i) M512i {
	return M512i(cvtepi16Epi32([32]byte(a)))
}

func cvtepi16Epi32(a [32]byte) [64]byte


// MaskCvtepi16Epi322: Sign extend packed 16-bit integers in 'a' to packed
// 32-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			l := j*16
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[l+15:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXWD'. Intrinsic: '_mm512_mask_cvtepi16_epi32'.
// Requires AVX512F.
func MaskCvtepi16Epi322(src M512i, k Mmask16, a M256i) M512i {
	return M512i(maskCvtepi16Epi322([64]byte(src), uint16(k), [32]byte(a)))
}

func maskCvtepi16Epi322(src [64]byte, k uint16, a [32]byte) [64]byte


// MaskzCvtepi16Epi322: Sign extend packed 16-bit integers in 'a' to packed
// 32-bit integers, and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[l+15:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXWD'. Intrinsic: '_mm512_maskz_cvtepi16_epi32'.
// Requires AVX512F.
func MaskzCvtepi16Epi322(k Mmask16, a M256i) M512i {
	return M512i(maskzCvtepi16Epi322(uint16(k), [32]byte(a)))
}

func maskzCvtepi16Epi322(k uint16, a [32]byte) [64]byte


// MaskCvtepi16Epi64: Sign extend packed 16-bit integers in the low 4 bytes of
// 'a' to packed 64-bit integers, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+15:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSXWQ'. Intrinsic: '_mm_mask_cvtepi16_epi64'.
// Requires AVX512F.
func MaskCvtepi16Epi64(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtepi16Epi64([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi16Epi64(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepi16Epi64: Sign extend packed 16-bit integers in the low 4 bytes of
// 'a' to packed 64-bit integers, and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+15:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSXWQ'. Intrinsic: '_mm_maskz_cvtepi16_epi64'.
// Requires AVX512F.
func MaskzCvtepi16Epi64(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtepi16Epi64(uint8(k), [16]byte(a)))
}

func maskzCvtepi16Epi64(k uint8, a [16]byte) [16]byte


// MaskCvtepi16Epi641: Sign extend packed 16-bit integers in the low 8 bytes of
// 'a' to packed 64-bit integers, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+15:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXWQ'. Intrinsic: '_mm256_mask_cvtepi16_epi64'.
// Requires AVX512F.
func MaskCvtepi16Epi641(src M256i, k Mmask8, a M128i) M256i {
	return M256i(maskCvtepi16Epi641([32]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi16Epi641(src [32]byte, k uint8, a [16]byte) [32]byte


// MaskzCvtepi16Epi641: Sign extend packed 16-bit integers in the low 8 bytes
// of 'a' to packed 64-bit integers, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+15:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXWQ'. Intrinsic: '_mm256_maskz_cvtepi16_epi64'.
// Requires AVX512F.
func MaskzCvtepi16Epi641(k Mmask8, a M128i) M256i {
	return M256i(maskzCvtepi16Epi641(uint8(k), [16]byte(a)))
}

func maskzCvtepi16Epi641(k uint8, a [16]byte) [32]byte


// Cvtepi16Epi64: Sign extend packed 16-bit integers in 'a' to packed 64-bit
// integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 16*j
//			dst[i+63:i] := SignExtend(a[k+15:k])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXWQ'. Intrinsic: '_mm512_cvtepi16_epi64'.
// Requires AVX512F.
func Cvtepi16Epi64(a M128i) M512i {
	return M512i(cvtepi16Epi64([16]byte(a)))
}

func cvtepi16Epi64(a [16]byte) [64]byte


// MaskCvtepi16Epi642: Sign extend packed 16-bit integers in 'a' to packed
// 64-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+15:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXWQ'. Intrinsic: '_mm512_mask_cvtepi16_epi64'.
// Requires AVX512F.
func MaskCvtepi16Epi642(src M512i, k Mmask8, a M128i) M512i {
	return M512i(maskCvtepi16Epi642([64]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi16Epi642(src [64]byte, k uint8, a [16]byte) [64]byte


// MaskzCvtepi16Epi642: Sign extend packed 16-bit integers in 'a' to packed
// 64-bit integers, and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+15:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXWQ'. Intrinsic: '_mm512_maskz_cvtepi16_epi64'.
// Requires AVX512F.
func MaskzCvtepi16Epi642(k Mmask8, a M128i) M512i {
	return M512i(maskzCvtepi16Epi642(uint8(k), [16]byte(a)))
}

func maskzCvtepi16Epi642(k uint8, a [16]byte) [64]byte


// Cvtepi32Epi16: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			k := 16*j
//			dst[k+15:k] := Truncate_Int32_To_Int16(a[i+31:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVDW'. Intrinsic: '_mm_cvtepi32_epi16'.
// Requires AVX512F.
func Cvtepi32Epi16(a M128i) M128i {
	return M128i(cvtepi32Epi16([16]byte(a)))
}

func cvtepi32Epi16(a [16]byte) [16]byte


// MaskCvtepi32Epi16: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Truncate_Int32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVDW'. Intrinsic: '_mm_mask_cvtepi32_epi16'.
// Requires AVX512F.
func MaskCvtepi32Epi16(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtepi32Epi16([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi32Epi16(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepi32Epi16: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Truncate_Int32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVDW'. Intrinsic: '_mm_maskz_cvtepi32_epi16'.
// Requires AVX512F.
func MaskzCvtepi32Epi16(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtepi32Epi16(uint8(k), [16]byte(a)))
}

func maskzCvtepi32Epi16(k uint8, a [16]byte) [16]byte


// Cvtepi32Epi161: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 16*j
//			dst[k+15:k] := Truncate_Int32_To_Int16(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVDW'. Intrinsic: '_mm256_cvtepi32_epi16'.
// Requires AVX512F.
func Cvtepi32Epi161(a M256i) M128i {
	return M128i(cvtepi32Epi161([32]byte(a)))
}

func cvtepi32Epi161(a [32]byte) [16]byte


// MaskCvtepi32Epi161: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Truncate_Int32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVDW'. Intrinsic: '_mm256_mask_cvtepi32_epi16'.
// Requires AVX512F.
func MaskCvtepi32Epi161(src M128i, k Mmask8, a M256i) M128i {
	return M128i(maskCvtepi32Epi161([16]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtepi32Epi161(src [16]byte, k uint8, a [32]byte) [16]byte


// MaskzCvtepi32Epi161: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Truncate_Int32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVDW'. Intrinsic: '_mm256_maskz_cvtepi32_epi16'.
// Requires AVX512F.
func MaskzCvtepi32Epi161(k Mmask8, a M256i) M128i {
	return M128i(maskzCvtepi32Epi161(uint8(k), [32]byte(a)))
}

func maskzCvtepi32Epi161(k uint8, a [32]byte) [16]byte


// Cvtepi32Epi162: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			k := 16*j
//			dst[k+15:k] := Truncate_Int32_To_Int16(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVDW'. Intrinsic: '_mm512_cvtepi32_epi16'.
// Requires AVX512F.
func Cvtepi32Epi162(a M512i) M256i {
	return M256i(cvtepi32Epi162([64]byte(a)))
}

func cvtepi32Epi162(a [64]byte) [32]byte


// MaskCvtepi32Epi162: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Truncate_Int32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVDW'. Intrinsic: '_mm512_mask_cvtepi32_epi16'.
// Requires AVX512F.
func MaskCvtepi32Epi162(src M256i, k Mmask16, a M512i) M256i {
	return M256i(maskCvtepi32Epi162([32]byte(src), uint16(k), [64]byte(a)))
}

func maskCvtepi32Epi162(src [32]byte, k uint16, a [64]byte) [32]byte


// MaskzCvtepi32Epi162: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Truncate_Int32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVDW'. Intrinsic: '_mm512_maskz_cvtepi32_epi16'.
// Requires AVX512F.
func MaskzCvtepi32Epi162(k Mmask16, a M512i) M256i {
	return M256i(maskzCvtepi32Epi162(uint16(k), [64]byte(a)))
}

func maskzCvtepi32Epi162(k uint16, a [64]byte) [32]byte


// MaskCvtepi32Epi64: Sign extend packed 32-bit integers in 'a' to packed
// 64-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSXDQ'. Intrinsic: '_mm_mask_cvtepi32_epi64'.
// Requires AVX512F.
func MaskCvtepi32Epi64(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtepi32Epi64([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi32Epi64(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepi32Epi64: Sign extend packed 32-bit integers in 'a' to packed
// 64-bit integers, and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+31:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSXDQ'. Intrinsic: '_mm_maskz_cvtepi32_epi64'.
// Requires AVX512F.
func MaskzCvtepi32Epi64(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtepi32Epi64(uint8(k), [16]byte(a)))
}

func maskzCvtepi32Epi64(k uint8, a [16]byte) [16]byte


// MaskCvtepi32Epi641: Sign extend packed 32-bit integers in 'a' to packed
// 64-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXDQ'. Intrinsic: '_mm256_mask_cvtepi32_epi64'.
// Requires AVX512F.
func MaskCvtepi32Epi641(src M256i, k Mmask8, a M128i) M256i {
	return M256i(maskCvtepi32Epi641([32]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi32Epi641(src [32]byte, k uint8, a [16]byte) [32]byte


// MaskzCvtepi32Epi641: Sign extend packed 32-bit integers in 'a' to packed
// 64-bit integers, and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+31:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXDQ'. Intrinsic: '_mm256_maskz_cvtepi32_epi64'.
// Requires AVX512F.
func MaskzCvtepi32Epi641(k Mmask8, a M128i) M256i {
	return M256i(maskzCvtepi32Epi641(uint8(k), [16]byte(a)))
}

func maskzCvtepi32Epi641(k uint8, a [16]byte) [32]byte


// Cvtepi32Epi64: Sign extend packed 32-bit integers in 'a' to packed 64-bit
// integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 32*j
//			dst[i+63:i] := SignExtend(a[k+31:k])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXDQ'. Intrinsic: '_mm512_cvtepi32_epi64'.
// Requires AVX512F.
func Cvtepi32Epi64(a M256i) M512i {
	return M512i(cvtepi32Epi64([32]byte(a)))
}

func cvtepi32Epi64(a [32]byte) [64]byte


// MaskCvtepi32Epi642: Sign extend packed 32-bit integers in 'a' to packed
// 64-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXDQ'. Intrinsic: '_mm512_mask_cvtepi32_epi64'.
// Requires AVX512F.
func MaskCvtepi32Epi642(src M512i, k Mmask8, a M256i) M512i {
	return M512i(maskCvtepi32Epi642([64]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtepi32Epi642(src [64]byte, k uint8, a [32]byte) [64]byte


// MaskzCvtepi32Epi642: Sign extend packed 32-bit integers in 'a' to packed
// 64-bit integers, and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+31:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXDQ'. Intrinsic: '_mm512_maskz_cvtepi32_epi64'.
// Requires AVX512F.
func MaskzCvtepi32Epi642(k Mmask8, a M256i) M512i {
	return M512i(maskzCvtepi32Epi642(uint8(k), [32]byte(a)))
}

func maskzCvtepi32Epi642(k uint8, a [32]byte) [64]byte


// Cvtepi32Epi8: Convert packed 32-bit integers in 'a' to packed 8-bit integers
// with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			k := 8*j
//			dst[k+7:k] := Truncate_Int32_To_Int8(a[i+31:i])
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVDB'. Intrinsic: '_mm_cvtepi32_epi8'.
// Requires AVX512F.
func Cvtepi32Epi8(a M128i) M128i {
	return M128i(cvtepi32Epi8([16]byte(a)))
}

func cvtepi32Epi8(a [16]byte) [16]byte


// MaskCvtepi32Epi8: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVDB'. Intrinsic: '_mm_mask_cvtepi32_epi8'.
// Requires AVX512F.
func MaskCvtepi32Epi8(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtepi32Epi8([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi32Epi8(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepi32Epi8: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVDB'. Intrinsic: '_mm_maskz_cvtepi32_epi8'.
// Requires AVX512F.
func MaskzCvtepi32Epi8(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtepi32Epi8(uint8(k), [16]byte(a)))
}

func maskzCvtepi32Epi8(k uint8, a [16]byte) [16]byte


// Cvtepi32Epi81: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 8*j
//			dst[k+7:k] := Truncate_Int32_To_Int8(a[i+31:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVDB'. Intrinsic: '_mm256_cvtepi32_epi8'.
// Requires AVX512F.
func Cvtepi32Epi81(a M256i) M128i {
	return M128i(cvtepi32Epi81([32]byte(a)))
}

func cvtepi32Epi81(a [32]byte) [16]byte


// MaskCvtepi32Epi81: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVDB'. Intrinsic: '_mm256_mask_cvtepi32_epi8'.
// Requires AVX512F.
func MaskCvtepi32Epi81(src M128i, k Mmask8, a M256i) M128i {
	return M128i(maskCvtepi32Epi81([16]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtepi32Epi81(src [16]byte, k uint8, a [32]byte) [16]byte


// MaskzCvtepi32Epi81: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVDB'. Intrinsic: '_mm256_maskz_cvtepi32_epi8'.
// Requires AVX512F.
func MaskzCvtepi32Epi81(k Mmask8, a M256i) M128i {
	return M128i(maskzCvtepi32Epi81(uint8(k), [32]byte(a)))
}

func maskzCvtepi32Epi81(k uint8, a [32]byte) [16]byte


// Cvtepi32Epi82: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			k := 8*j
//			dst[k+7:k] := Truncate_Int32_To_Int8(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVDB'. Intrinsic: '_mm512_cvtepi32_epi8'.
// Requires AVX512F.
func Cvtepi32Epi82(a M512i) M128i {
	return M128i(cvtepi32Epi82([64]byte(a)))
}

func cvtepi32Epi82(a [64]byte) [16]byte


// MaskCvtepi32Epi82: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVDB'. Intrinsic: '_mm512_mask_cvtepi32_epi8'.
// Requires AVX512F.
func MaskCvtepi32Epi82(src M128i, k Mmask16, a M512i) M128i {
	return M128i(maskCvtepi32Epi82([16]byte(src), uint16(k), [64]byte(a)))
}

func maskCvtepi32Epi82(src [16]byte, k uint16, a [64]byte) [16]byte


// MaskzCvtepi32Epi82: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVDB'. Intrinsic: '_mm512_maskz_cvtepi32_epi8'.
// Requires AVX512F.
func MaskzCvtepi32Epi82(k Mmask16, a M512i) M128i {
	return M128i(maskzCvtepi32Epi82(uint16(k), [64]byte(a)))
}

func maskzCvtepi32Epi82(k uint16, a [64]byte) [16]byte


// MaskCvtepi32Pd: Convert packed 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*32
//			m := j*64
//			IF k[j]
//				dst[m+63:m] := Convert_Int32_To_FP64(a[i+31:i])
//			ELSE
//				dst[m+63:m] := src[m+63:m]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTDQ2PD'. Intrinsic: '_mm_mask_cvtepi32_pd'.
// Requires AVX512F.
func MaskCvtepi32Pd(src M128d, k Mmask8, a M128i) M128d {
	return M128d(maskCvtepi32Pd([2]float64(src), uint8(k), [16]byte(a)))
}

func maskCvtepi32Pd(src [2]float64, k uint8, a [16]byte) [2]float64


// MaskzCvtepi32Pd: Convert packed 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*32
//			m := j*64
//			IF k[j]
//				dst[m+63:m] := Convert_Int32_To_FP64(a[i+31:i])
//			ELSE
//				dst[m+63:m] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTDQ2PD'. Intrinsic: '_mm_maskz_cvtepi32_pd'.
// Requires AVX512F.
func MaskzCvtepi32Pd(k Mmask8, a M128i) M128d {
	return M128d(maskzCvtepi32Pd(uint8(k), [16]byte(a)))
}

func maskzCvtepi32Pd(k uint8, a [16]byte) [2]float64


// MaskCvtepi32Pd1: Convert packed 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			m := j*64
//			IF k[j]
//				dst[m+63:m] := Convert_Int32_To_FP64(a[i+31:i])
//			ELSE
//				dst[m+63:m] := src[m+63:m]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTDQ2PD'. Intrinsic: '_mm256_mask_cvtepi32_pd'.
// Requires AVX512F.
func MaskCvtepi32Pd1(src M256d, k Mmask8, a M128i) M256d {
	return M256d(maskCvtepi32Pd1([4]float64(src), uint8(k), [16]byte(a)))
}

func maskCvtepi32Pd1(src [4]float64, k uint8, a [16]byte) [4]float64


// MaskzCvtepi32Pd1: Convert packed 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			m := j*64
//			IF k[j]
//				dst[m+63:m] := Convert_Int32_To_FP64(a[i+31:i])
//			ELSE
//				dst[m+63:m] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTDQ2PD'. Intrinsic: '_mm256_maskz_cvtepi32_pd'.
// Requires AVX512F.
func MaskzCvtepi32Pd1(k Mmask8, a M128i) M256d {
	return M256d(maskzCvtepi32Pd1(uint8(k), [16]byte(a)))
}

func maskzCvtepi32Pd1(k uint8, a [16]byte) [4]float64


// Cvtepi32Pd: Convert packed 32-bit integers in 'a' to packed double-precision
// (64-bit) floating-point elements, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			m := j*64
//			dst[m+63:m] := Convert_Int32_To_FP64(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTDQ2PD'. Intrinsic: '_mm512_cvtepi32_pd'.
// Requires AVX512F.
func Cvtepi32Pd(a M256i) M512d {
	return M512d(cvtepi32Pd([32]byte(a)))
}

func cvtepi32Pd(a [32]byte) [8]float64


// MaskCvtepi32Pd2: Convert packed 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			m := j*64
//			IF k[j]
//				dst[m+63:m] := Convert_Int32_To_FP64(a[i+31:i])
//			ELSE
//				dst[m+63:m] := src[m+63:m]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTDQ2PD'. Intrinsic: '_mm512_mask_cvtepi32_pd'.
// Requires AVX512F.
func MaskCvtepi32Pd2(src M512d, k Mmask8, a M256i) M512d {
	return M512d(maskCvtepi32Pd2([8]float64(src), uint8(k), [32]byte(a)))
}

func maskCvtepi32Pd2(src [8]float64, k uint8, a [32]byte) [8]float64


// MaskzCvtepi32Pd2: Convert packed 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			m := j*64
//			IF k[j]
//				dst[m+63:m] := Convert_Int32_To_FP64(a[i+31:i])
//			ELSE
//				dst[m+63:m] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTDQ2PD'. Intrinsic: '_mm512_maskz_cvtepi32_pd'.
// Requires AVX512F.
func MaskzCvtepi32Pd2(k Mmask8, a M256i) M512d {
	return M512d(maskzCvtepi32Pd2(uint8(k), [32]byte(a)))
}

func maskzCvtepi32Pd2(k uint8, a [32]byte) [8]float64


// MaskCvtepi32Ps: Convert packed 32-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTDQ2PS'. Intrinsic: '_mm_mask_cvtepi32_ps'.
// Requires AVX512F.
func MaskCvtepi32Ps(src M128, k Mmask8, a M128i) M128 {
	return M128(maskCvtepi32Ps([4]float32(src), uint8(k), [16]byte(a)))
}

func maskCvtepi32Ps(src [4]float32, k uint8, a [16]byte) [4]float32


// MaskzCvtepi32Ps: Convert packed 32-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTDQ2PS'. Intrinsic: '_mm_maskz_cvtepi32_ps'.
// Requires AVX512F.
func MaskzCvtepi32Ps(k Mmask8, a M128i) M128 {
	return M128(maskzCvtepi32Ps(uint8(k), [16]byte(a)))
}

func maskzCvtepi32Ps(k uint8, a [16]byte) [4]float32


// MaskCvtepi32Ps1: Convert packed 32-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTDQ2PS'. Intrinsic: '_mm256_mask_cvtepi32_ps'.
// Requires AVX512F.
func MaskCvtepi32Ps1(src M256, k Mmask8, a M256i) M256 {
	return M256(maskCvtepi32Ps1([8]float32(src), uint8(k), [32]byte(a)))
}

func maskCvtepi32Ps1(src [8]float32, k uint8, a [32]byte) [8]float32


// MaskzCvtepi32Ps1: Convert packed 32-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTDQ2PS'. Intrinsic: '_mm256_maskz_cvtepi32_ps'.
// Requires AVX512F.
func MaskzCvtepi32Ps1(k Mmask8, a M256i) M256 {
	return M256(maskzCvtepi32Ps1(uint8(k), [32]byte(a)))
}

func maskzCvtepi32Ps1(k uint8, a [32]byte) [8]float32


// Cvtepi32Ps: Convert packed 32-bit integers in 'a' to packed single-precision
// (32-bit) floating-point elements, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTDQ2PS'. Intrinsic: '_mm512_cvtepi32_ps'.
// Requires AVX512F.
func Cvtepi32Ps(a M512i) M512 {
	return M512(cvtepi32Ps([64]byte(a)))
}

func cvtepi32Ps(a [64]byte) [16]float32


// MaskCvtepi32Ps2: Convert packed 32-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTDQ2PS'. Intrinsic: '_mm512_mask_cvtepi32_ps'.
// Requires AVX512F.
func MaskCvtepi32Ps2(src M512, k Mmask16, a M512i) M512 {
	return M512(maskCvtepi32Ps2([16]float32(src), uint16(k), [64]byte(a)))
}

func maskCvtepi32Ps2(src [16]float32, k uint16, a [64]byte) [16]float32


// MaskzCvtepi32Ps2: Convert packed 32-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTDQ2PS'. Intrinsic: '_mm512_maskz_cvtepi32_ps'.
// Requires AVX512F.
func MaskzCvtepi32Ps2(k Mmask16, a M512i) M512 {
	return M512(maskzCvtepi32Ps2(uint16(k), [64]byte(a)))
}

func maskzCvtepi32Ps2(k uint16, a [64]byte) [16]float32


// MaskCvtepi32StoreuEpi16: Convert packed 32-bit integers in 'a' to packed
// 16-bit integers with truncation, and store the active results (those with
// their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				MEM[base_addr+l+15:base_addr+l] := Truncate_Int32_To_Int16(a[i+31:i])
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVDW'. Intrinsic: '_mm_mask_cvtepi32_storeu_epi16'.
// Requires AVX512F.
func MaskCvtepi32StoreuEpi16(base_addr uintptr, k Mmask8, a M128i)  {
	maskCvtepi32StoreuEpi16(uintptr(base_addr), uint8(k), [16]byte(a))
}

func maskCvtepi32StoreuEpi16(base_addr uintptr, k uint8, a [16]byte) 


// MaskCvtepi32StoreuEpi161: Convert packed 32-bit integers in 'a' to packed
// 16-bit integers with truncation, and store the active results (those with
// their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				MEM[base_addr+l+15:base_addr+l] := Truncate_Int32_To_Int16(a[i+31:i])
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVDW'. Intrinsic: '_mm256_mask_cvtepi32_storeu_epi16'.
// Requires AVX512F.
func MaskCvtepi32StoreuEpi161(base_addr uintptr, k Mmask8, a M256i)  {
	maskCvtepi32StoreuEpi161(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCvtepi32StoreuEpi161(base_addr uintptr, k uint8, a [32]byte) 


// MaskCvtepi32StoreuEpi162: Convert packed 32-bit integers in 'a' to packed
// 16-bit integers with truncation, and store the active results (those with
// their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				MEM[base_addr+l+15:base_addr+l] := Truncate_Int32_To_Int16(a[i+31:i])
//			FI
//		ENDFOR
//
// Instruction: 'VPMOVDW'. Intrinsic: '_mm512_mask_cvtepi32_storeu_epi16'.
// Requires AVX512F.
func MaskCvtepi32StoreuEpi162(base_addr uintptr, k Mmask16, a M512i)  {
	maskCvtepi32StoreuEpi162(uintptr(base_addr), uint16(k), [64]byte(a))
}

func maskCvtepi32StoreuEpi162(base_addr uintptr, k uint16, a [64]byte) 


// MaskCvtepi32StoreuEpi8: Convert packed 32-bit integers in 'a' to packed
// 8-bit integers with truncation, and store the active results (those with
// their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Truncate_Int32_To_Int8(a[i+31:i])
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVDB'. Intrinsic: '_mm_mask_cvtepi32_storeu_epi8'.
// Requires AVX512F.
func MaskCvtepi32StoreuEpi8(base_addr uintptr, k Mmask8, a M128i)  {
	maskCvtepi32StoreuEpi8(uintptr(base_addr), uint8(k), [16]byte(a))
}

func maskCvtepi32StoreuEpi8(base_addr uintptr, k uint8, a [16]byte) 


// MaskCvtepi32StoreuEpi81: Convert packed 32-bit integers in 'a' to packed
// 8-bit integers with truncation, and store the active results (those with
// their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Truncate_Int32_To_Int8(a[i+31:i])
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVDB'. Intrinsic: '_mm256_mask_cvtepi32_storeu_epi8'.
// Requires AVX512F.
func MaskCvtepi32StoreuEpi81(base_addr uintptr, k Mmask8, a M256i)  {
	maskCvtepi32StoreuEpi81(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCvtepi32StoreuEpi81(base_addr uintptr, k uint8, a [32]byte) 


// MaskCvtepi32StoreuEpi82: Convert packed 32-bit integers in 'a' to packed
// 8-bit integers with truncation, and store the active results (those with
// their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Truncate_Int32_To_Int8(a[i+31:i])
//			FI
//		ENDFOR
//
// Instruction: 'VPMOVDB'. Intrinsic: '_mm512_mask_cvtepi32_storeu_epi8'.
// Requires AVX512F.
func MaskCvtepi32StoreuEpi82(base_addr uintptr, k Mmask16, a M512i)  {
	maskCvtepi32StoreuEpi82(uintptr(base_addr), uint16(k), [64]byte(a))
}

func maskCvtepi32StoreuEpi82(base_addr uintptr, k uint16, a [64]byte) 


// Cvtepi64Epi16: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			k := 16*j
//			dst[k+15:k] := Truncate_Int64_To_Int16(a[i+63:i])
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVQW'. Intrinsic: '_mm_cvtepi64_epi16'.
// Requires AVX512F.
func Cvtepi64Epi16(a M128i) M128i {
	return M128i(cvtepi64Epi16([16]byte(a)))
}

func cvtepi64Epi16(a [16]byte) [16]byte


// MaskCvtepi64Epi16: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Truncate_Int64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVQW'. Intrinsic: '_mm_mask_cvtepi64_epi16'.
// Requires AVX512F.
func MaskCvtepi64Epi16(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtepi64Epi16([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi64Epi16(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepi64Epi16: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Truncate_Int64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVQW'. Intrinsic: '_mm_maskz_cvtepi64_epi16'.
// Requires AVX512F.
func MaskzCvtepi64Epi16(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtepi64Epi16(uint8(k), [16]byte(a)))
}

func maskzCvtepi64Epi16(k uint8, a [16]byte) [16]byte


// Cvtepi64Epi161: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			k := 16*j
//			dst[k+15:k] := Truncate_Int64_To_Int16(a[i+63:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVQW'. Intrinsic: '_mm256_cvtepi64_epi16'.
// Requires AVX512F.
func Cvtepi64Epi161(a M256i) M128i {
	return M128i(cvtepi64Epi161([32]byte(a)))
}

func cvtepi64Epi161(a [32]byte) [16]byte


// MaskCvtepi64Epi161: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Truncate_Int64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVQW'. Intrinsic: '_mm256_mask_cvtepi64_epi16'.
// Requires AVX512F.
func MaskCvtepi64Epi161(src M128i, k Mmask8, a M256i) M128i {
	return M128i(maskCvtepi64Epi161([16]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtepi64Epi161(src [16]byte, k uint8, a [32]byte) [16]byte


// MaskzCvtepi64Epi161: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Truncate_Int64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVQW'. Intrinsic: '_mm256_maskz_cvtepi64_epi16'.
// Requires AVX512F.
func MaskzCvtepi64Epi161(k Mmask8, a M256i) M128i {
	return M128i(maskzCvtepi64Epi161(uint8(k), [32]byte(a)))
}

func maskzCvtepi64Epi161(k uint8, a [32]byte) [16]byte


// Cvtepi64Epi162: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 16*j
//			dst[k+15:k] := Truncate_Int64_To_Int16(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQW'. Intrinsic: '_mm512_cvtepi64_epi16'.
// Requires AVX512F.
func Cvtepi64Epi162(a M512i) M128i {
	return M128i(cvtepi64Epi162([64]byte(a)))
}

func cvtepi64Epi162(a [64]byte) [16]byte


// MaskCvtepi64Epi162: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Truncate_Int64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQW'. Intrinsic: '_mm512_mask_cvtepi64_epi16'.
// Requires AVX512F.
func MaskCvtepi64Epi162(src M128i, k Mmask8, a M512i) M128i {
	return M128i(maskCvtepi64Epi162([16]byte(src), uint8(k), [64]byte(a)))
}

func maskCvtepi64Epi162(src [16]byte, k uint8, a [64]byte) [16]byte


// MaskzCvtepi64Epi162: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Truncate_Int64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQW'. Intrinsic: '_mm512_maskz_cvtepi64_epi16'.
// Requires AVX512F.
func MaskzCvtepi64Epi162(k Mmask8, a M512i) M128i {
	return M128i(maskzCvtepi64Epi162(uint8(k), [64]byte(a)))
}

func maskzCvtepi64Epi162(k uint8, a [64]byte) [16]byte


// Cvtepi64Epi32: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			k := 32*j
//			dst[k+31:k] := Truncate_Int64_To_Int32(a[i+63:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVQD'. Intrinsic: '_mm_cvtepi64_epi32'.
// Requires AVX512F.
func Cvtepi64Epi32(a M128i) M128i {
	return M128i(cvtepi64Epi32([16]byte(a)))
}

func cvtepi64Epi32(a [16]byte) [16]byte


// MaskCvtepi64Epi32: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Truncate_Int64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := src[l+31:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVQD'. Intrinsic: '_mm_mask_cvtepi64_epi32'.
// Requires AVX512F.
func MaskCvtepi64Epi32(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtepi64Epi32([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi64Epi32(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepi64Epi32: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Truncate_Int64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVQD'. Intrinsic: '_mm_maskz_cvtepi64_epi32'.
// Requires AVX512F.
func MaskzCvtepi64Epi32(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtepi64Epi32(uint8(k), [16]byte(a)))
}

func maskzCvtepi64Epi32(k uint8, a [16]byte) [16]byte


// Cvtepi64Epi321: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			k := 32*j
//			dst[k+31:k] := Truncate_Int64_To_Int32(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQD'. Intrinsic: '_mm256_cvtepi64_epi32'.
// Requires AVX512F.
func Cvtepi64Epi321(a M256i) M128i {
	return M128i(cvtepi64Epi321([32]byte(a)))
}

func cvtepi64Epi321(a [32]byte) [16]byte


// MaskCvtepi64Epi321: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Truncate_Int64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := src[l+31:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQD'. Intrinsic: '_mm256_mask_cvtepi64_epi32'.
// Requires AVX512F.
func MaskCvtepi64Epi321(src M128i, k Mmask8, a M256i) M128i {
	return M128i(maskCvtepi64Epi321([16]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtepi64Epi321(src [16]byte, k uint8, a [32]byte) [16]byte


// MaskzCvtepi64Epi321: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Truncate_Int64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQD'. Intrinsic: '_mm256_maskz_cvtepi64_epi32'.
// Requires AVX512F.
func MaskzCvtepi64Epi321(k Mmask8, a M256i) M128i {
	return M128i(maskzCvtepi64Epi321(uint8(k), [32]byte(a)))
}

func maskzCvtepi64Epi321(k uint8, a [32]byte) [16]byte


// Cvtepi64Epi322: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 32*j
//			dst[k+31:k] := Truncate_Int64_To_Int32(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVQD'. Intrinsic: '_mm512_cvtepi64_epi32'.
// Requires AVX512F.
func Cvtepi64Epi322(a M512i) M256i {
	return M256i(cvtepi64Epi322([64]byte(a)))
}

func cvtepi64Epi322(a [64]byte) [32]byte


// MaskCvtepi64Epi322: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Truncate_Int64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := src[l+31:l]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVQD'. Intrinsic: '_mm512_mask_cvtepi64_epi32'.
// Requires AVX512F.
func MaskCvtepi64Epi322(src M256i, k Mmask8, a M512i) M256i {
	return M256i(maskCvtepi64Epi322([32]byte(src), uint8(k), [64]byte(a)))
}

func maskCvtepi64Epi322(src [32]byte, k uint8, a [64]byte) [32]byte


// MaskzCvtepi64Epi322: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Truncate_Int64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVQD'. Intrinsic: '_mm512_maskz_cvtepi64_epi32'.
// Requires AVX512F.
func MaskzCvtepi64Epi322(k Mmask8, a M512i) M256i {
	return M256i(maskzCvtepi64Epi322(uint8(k), [64]byte(a)))
}

func maskzCvtepi64Epi322(k uint8, a [64]byte) [32]byte


// Cvtepi64Epi8: Convert packed 64-bit integers in 'a' to packed 8-bit integers
// with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			k := 8*j
//			dst[k+7:k] := Truncate_Int64_To_Int8(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQB'. Intrinsic: '_mm_cvtepi64_epi8'.
// Requires AVX512F.
func Cvtepi64Epi8(a M128i) M128i {
	return M128i(cvtepi64Epi8([16]byte(a)))
}

func cvtepi64Epi8(a [16]byte) [16]byte


// MaskCvtepi64Epi8: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQB'. Intrinsic: '_mm_mask_cvtepi64_epi8'.
// Requires AVX512F.
func MaskCvtepi64Epi8(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtepi64Epi8([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi64Epi8(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepi64Epi8: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQB'. Intrinsic: '_mm_maskz_cvtepi64_epi8'.
// Requires AVX512F.
func MaskzCvtepi64Epi8(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtepi64Epi8(uint8(k), [16]byte(a)))
}

func maskzCvtepi64Epi8(k uint8, a [16]byte) [16]byte


// Cvtepi64Epi81: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			k := 8*j
//			dst[k+7:k] := Truncate_Int64_To_Int8(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQB'. Intrinsic: '_mm256_cvtepi64_epi8'.
// Requires AVX512F.
func Cvtepi64Epi81(a M256i) M128i {
	return M128i(cvtepi64Epi81([32]byte(a)))
}

func cvtepi64Epi81(a [32]byte) [16]byte


// MaskCvtepi64Epi81: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQB'. Intrinsic: '_mm256_mask_cvtepi64_epi8'.
// Requires AVX512F.
func MaskCvtepi64Epi81(src M128i, k Mmask8, a M256i) M128i {
	return M128i(maskCvtepi64Epi81([16]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtepi64Epi81(src [16]byte, k uint8, a [32]byte) [16]byte


// MaskzCvtepi64Epi81: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQB'. Intrinsic: '_mm256_maskz_cvtepi64_epi8'.
// Requires AVX512F.
func MaskzCvtepi64Epi81(k Mmask8, a M256i) M128i {
	return M128i(maskzCvtepi64Epi81(uint8(k), [32]byte(a)))
}

func maskzCvtepi64Epi81(k uint8, a [32]byte) [16]byte


// Cvtepi64Epi82: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 8*j
//			dst[k+7:k] := Truncate_Int64_To_Int8(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQB'. Intrinsic: '_mm512_cvtepi64_epi8'.
// Requires AVX512F.
func Cvtepi64Epi82(a M512i) M128i {
	return M128i(cvtepi64Epi82([64]byte(a)))
}

func cvtepi64Epi82(a [64]byte) [16]byte


// MaskCvtepi64Epi82: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQB'. Intrinsic: '_mm512_mask_cvtepi64_epi8'.
// Requires AVX512F.
func MaskCvtepi64Epi82(src M128i, k Mmask8, a M512i) M128i {
	return M128i(maskCvtepi64Epi82([16]byte(src), uint8(k), [64]byte(a)))
}

func maskCvtepi64Epi82(src [16]byte, k uint8, a [64]byte) [16]byte


// MaskzCvtepi64Epi82: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQB'. Intrinsic: '_mm512_maskz_cvtepi64_epi8'.
// Requires AVX512F.
func MaskzCvtepi64Epi82(k Mmask8, a M512i) M128i {
	return M128i(maskzCvtepi64Epi82(uint8(k), [64]byte(a)))
}

func maskzCvtepi64Epi82(k uint8, a [64]byte) [16]byte


// MaskCvtepi64StoreuEpi16: Convert packed 64-bit integers in 'a' to packed
// 16-bit integers with truncation, and store the active results (those with
// their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				MEM[base_addr+l+15:base_addr+l] := Truncate_Int64_To_Int16(a[i+63:i])
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVQW'. Intrinsic: '_mm_mask_cvtepi64_storeu_epi16'.
// Requires AVX512F.
func MaskCvtepi64StoreuEpi16(base_addr uintptr, k Mmask8, a M128i)  {
	maskCvtepi64StoreuEpi16(uintptr(base_addr), uint8(k), [16]byte(a))
}

func maskCvtepi64StoreuEpi16(base_addr uintptr, k uint8, a [16]byte) 


// MaskCvtepi64StoreuEpi161: Convert packed 64-bit integers in 'a' to packed
// 16-bit integers with truncation, and store the active results (those with
// their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				MEM[base_addr+l+15:base_addr+l] := Truncate_Int64_To_Int16(a[i+63:i])
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVQW'. Intrinsic: '_mm256_mask_cvtepi64_storeu_epi16'.
// Requires AVX512F.
func MaskCvtepi64StoreuEpi161(base_addr uintptr, k Mmask8, a M256i)  {
	maskCvtepi64StoreuEpi161(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCvtepi64StoreuEpi161(base_addr uintptr, k uint8, a [32]byte) 


// MaskCvtepi64StoreuEpi162: Convert packed 64-bit integers in 'a' to packed
// 16-bit integers with truncation, and store the active results (those with
// their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				MEM[base_addr+l+15:base_addr+l] := Truncate_Int64_To_Int16(a[i+63:i])
//			FI
//		ENDFOR
//
// Instruction: 'VPMOVQW'. Intrinsic: '_mm512_mask_cvtepi64_storeu_epi16'.
// Requires AVX512F.
func MaskCvtepi64StoreuEpi162(base_addr uintptr, k Mmask8, a M512i)  {
	maskCvtepi64StoreuEpi162(uintptr(base_addr), uint8(k), [64]byte(a))
}

func maskCvtepi64StoreuEpi162(base_addr uintptr, k uint8, a [64]byte) 


// MaskCvtepi64StoreuEpi32: Convert packed 64-bit integers in 'a' to packed
// 32-bit integers with truncation, and store the active results (those with
// their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				MEM[base_addr+l+31:base_addr+l] := Truncate_Int64_To_Int32(a[i+63:i])
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVQD'. Intrinsic: '_mm_mask_cvtepi64_storeu_epi32'.
// Requires AVX512F.
func MaskCvtepi64StoreuEpi32(base_addr uintptr, k Mmask8, a M128i)  {
	maskCvtepi64StoreuEpi32(uintptr(base_addr), uint8(k), [16]byte(a))
}

func maskCvtepi64StoreuEpi32(base_addr uintptr, k uint8, a [16]byte) 


// MaskCvtepi64StoreuEpi321: Convert packed 64-bit integers in 'a' to packed
// 32-bit integers with truncation, and store the active results (those with
// their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				MEM[base_addr+l+31:base_addr+l] := Truncate_Int64_To_Int32(a[i+63:i])
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQD'. Intrinsic: '_mm256_mask_cvtepi64_storeu_epi32'.
// Requires AVX512F.
func MaskCvtepi64StoreuEpi321(base_addr uintptr, k Mmask8, a M256i)  {
	maskCvtepi64StoreuEpi321(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCvtepi64StoreuEpi321(base_addr uintptr, k uint8, a [32]byte) 


// MaskCvtepi64StoreuEpi322: Convert packed 64-bit integers in 'a' to packed
// 32-bit integers with truncation, and store the active results (those with
// their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				MEM[base_addr+l+31:base_addr+l] := Truncate_Int64_To_Int32(a[i+63:i])
//			FI
//		ENDFOR
//
// Instruction: 'VPMOVQD'. Intrinsic: '_mm512_mask_cvtepi64_storeu_epi32'.
// Requires AVX512F.
func MaskCvtepi64StoreuEpi322(base_addr uintptr, k Mmask8, a M512i)  {
	maskCvtepi64StoreuEpi322(uintptr(base_addr), uint8(k), [64]byte(a))
}

func maskCvtepi64StoreuEpi322(base_addr uintptr, k uint8, a [64]byte) 


// MaskCvtepi64StoreuEpi8: Convert packed 64-bit integers in 'a' to packed
// 8-bit integers with truncation, and store the active results (those with
// their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Truncate_Int64_To_Int8(a[i+63:i])
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQB'. Intrinsic: '_mm_mask_cvtepi64_storeu_epi8'.
// Requires AVX512F.
func MaskCvtepi64StoreuEpi8(base_addr uintptr, k Mmask8, a M128i)  {
	maskCvtepi64StoreuEpi8(uintptr(base_addr), uint8(k), [16]byte(a))
}

func maskCvtepi64StoreuEpi8(base_addr uintptr, k uint8, a [16]byte) 


// MaskCvtepi64StoreuEpi81: Convert packed 64-bit integers in 'a' to packed
// 8-bit integers with truncation, and store the active results (those with
// their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Truncate_Int64_To_Int8(a[i+63:i])
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQB'. Intrinsic: '_mm256_mask_cvtepi64_storeu_epi8'.
// Requires AVX512F.
func MaskCvtepi64StoreuEpi81(base_addr uintptr, k Mmask8, a M256i)  {
	maskCvtepi64StoreuEpi81(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCvtepi64StoreuEpi81(base_addr uintptr, k uint8, a [32]byte) 


// MaskCvtepi64StoreuEpi82: Convert packed 64-bit integers in 'a' to packed
// 8-bit integers with truncation, and store the active results (those with
// their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Truncate_Int64_To_Int8(a[i+63:i])
//			FI
//		ENDFOR
//
// Instruction: 'VPMOVQB'. Intrinsic: '_mm512_mask_cvtepi64_storeu_epi8'.
// Requires AVX512F.
func MaskCvtepi64StoreuEpi82(base_addr uintptr, k Mmask8, a M512i)  {
	maskCvtepi64StoreuEpi82(uintptr(base_addr), uint8(k), [64]byte(a))
}

func maskCvtepi64StoreuEpi82(base_addr uintptr, k uint8, a [64]byte) 


// MaskCvtepi8Epi32: Sign extend packed 8-bit integers in the low 4 bytes of
// 'a' to packed 32-bit integers, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[l+7:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSXBD'. Intrinsic: '_mm_mask_cvtepi8_epi32'.
// Requires AVX512F.
func MaskCvtepi8Epi32(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtepi8Epi32([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi8Epi32(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepi8Epi32: Sign extend packed 8-bit integers in the low 4 bytes of
// 'a' to packed 32-bit integers, and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[l+7:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSXBD'. Intrinsic: '_mm_maskz_cvtepi8_epi32'.
// Requires AVX512F.
func MaskzCvtepi8Epi32(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtepi8Epi32(uint8(k), [16]byte(a)))
}

func maskzCvtepi8Epi32(k uint8, a [16]byte) [16]byte


// MaskCvtepi8Epi321: Sign extend packed 8-bit integers in the low 8 bytes of
// 'a' to packed 32-bit integers, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[l+7:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXBD'. Intrinsic: '_mm256_mask_cvtepi8_epi32'.
// Requires AVX512F.
func MaskCvtepi8Epi321(src M256i, k Mmask8, a M128i) M256i {
	return M256i(maskCvtepi8Epi321([32]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi8Epi321(src [32]byte, k uint8, a [16]byte) [32]byte


// MaskzCvtepi8Epi321: Sign extend packed 8-bit integers in the low 8 bytes of
// 'a' to packed 32-bit integers, and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[l+7:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXBD'. Intrinsic: '_mm256_maskz_cvtepi8_epi32'.
// Requires AVX512F.
func MaskzCvtepi8Epi321(k Mmask8, a M128i) M256i {
	return M256i(maskzCvtepi8Epi321(uint8(k), [16]byte(a)))
}

func maskzCvtepi8Epi321(k uint8, a [16]byte) [32]byte


// Cvtepi8Epi32: Sign extend packed 8-bit integers in 'a' to packed 32-bit
// integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			k := 8*j
//			dst[i+31:i] := SignExtend(a[k+7:k])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXBD'. Intrinsic: '_mm512_cvtepi8_epi32'.
// Requires AVX512F.
func Cvtepi8Epi32(a M128i) M512i {
	return M512i(cvtepi8Epi32([16]byte(a)))
}

func cvtepi8Epi32(a [16]byte) [64]byte


// MaskCvtepi8Epi322: Sign extend packed 8-bit integers in 'a' to packed 32-bit
// integers, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[l+7:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXBD'. Intrinsic: '_mm512_mask_cvtepi8_epi32'.
// Requires AVX512F.
func MaskCvtepi8Epi322(src M512i, k Mmask16, a M128i) M512i {
	return M512i(maskCvtepi8Epi322([64]byte(src), uint16(k), [16]byte(a)))
}

func maskCvtepi8Epi322(src [64]byte, k uint16, a [16]byte) [64]byte


// MaskzCvtepi8Epi322: Sign extend packed 8-bit integers in 'a' to packed
// 32-bit integers, and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[l+7:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXBD'. Intrinsic: '_mm512_maskz_cvtepi8_epi32'.
// Requires AVX512F.
func MaskzCvtepi8Epi322(k Mmask16, a M128i) M512i {
	return M512i(maskzCvtepi8Epi322(uint16(k), [16]byte(a)))
}

func maskzCvtepi8Epi322(k uint16, a [16]byte) [64]byte


// MaskCvtepi8Epi64: Sign extend packed 8-bit integers in the low 2 bytes of
// 'a' to packed 64-bit integers, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+7:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSXBQ'. Intrinsic: '_mm_mask_cvtepi8_epi64'.
// Requires AVX512F.
func MaskCvtepi8Epi64(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtepi8Epi64([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi8Epi64(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepi8Epi64: Sign extend packed 8-bit integers in the low 2 bytes of
// 'a' to packed 64-bit integers, and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+7:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSXBQ'. Intrinsic: '_mm_maskz_cvtepi8_epi64'.
// Requires AVX512F.
func MaskzCvtepi8Epi64(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtepi8Epi64(uint8(k), [16]byte(a)))
}

func maskzCvtepi8Epi64(k uint8, a [16]byte) [16]byte


// MaskCvtepi8Epi641: Sign extend packed 8-bit integers in the low 4 bytes of
// 'a' to packed 64-bit integers, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+7:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXBQ'. Intrinsic: '_mm256_mask_cvtepi8_epi64'.
// Requires AVX512F.
func MaskCvtepi8Epi641(src M256i, k Mmask8, a M128i) M256i {
	return M256i(maskCvtepi8Epi641([32]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi8Epi641(src [32]byte, k uint8, a [16]byte) [32]byte


// MaskzCvtepi8Epi641: Sign extend packed 8-bit integers in the low 4 bytes of
// 'a' to packed 64-bit integers, and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+7:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXBQ'. Intrinsic: '_mm256_maskz_cvtepi8_epi64'.
// Requires AVX512F.
func MaskzCvtepi8Epi641(k Mmask8, a M128i) M256i {
	return M256i(maskzCvtepi8Epi641(uint8(k), [16]byte(a)))
}

func maskzCvtepi8Epi641(k uint8, a [16]byte) [32]byte


// Cvtepi8Epi64: Sign extend packed 8-bit integers in the low 8 bytes of 'a' to
// packed 64-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 8*j
//			dst[i+63:i] := SignExtend(a[k+7:k])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXBQ'. Intrinsic: '_mm512_cvtepi8_epi64'.
// Requires AVX512F.
func Cvtepi8Epi64(a M128i) M512i {
	return M512i(cvtepi8Epi64([16]byte(a)))
}

func cvtepi8Epi64(a [16]byte) [64]byte


// MaskCvtepi8Epi642: Sign extend packed 8-bit integers in the low 8 bytes of
// 'a' to packed 64-bit integers, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+7:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXBQ'. Intrinsic: '_mm512_mask_cvtepi8_epi64'.
// Requires AVX512F.
func MaskCvtepi8Epi642(src M512i, k Mmask8, a M128i) M512i {
	return M512i(maskCvtepi8Epi642([64]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi8Epi642(src [64]byte, k uint8, a [16]byte) [64]byte


// MaskzCvtepi8Epi642: Sign extend packed 8-bit integers in the low 8 bytes of
// 'a' to packed 64-bit integers, and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+7:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVSXBQ'. Intrinsic: '_mm512_maskz_cvtepi8_epi64'.
// Requires AVX512F.
func MaskzCvtepi8Epi642(k Mmask8, a M128i) M512i {
	return M512i(maskzCvtepi8Epi642(uint8(k), [16]byte(a)))
}

func maskzCvtepi8Epi642(k uint8, a [16]byte) [64]byte


// MaskCvtepu16Epi32: Zero extend packed unsigned 16-bit integers in 'a' to
// packed 32-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[l+15:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVZXWD'. Intrinsic: '_mm_mask_cvtepu16_epi32'.
// Requires AVX512F.
func MaskCvtepu16Epi32(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtepu16Epi32([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepu16Epi32(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepu16Epi32: Zero extend packed unsigned 16-bit integers in 'a' to
// packed 32-bit integers, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[l+15:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVZXWD'. Intrinsic: '_mm_maskz_cvtepu16_epi32'.
// Requires AVX512F.
func MaskzCvtepu16Epi32(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtepu16Epi32(uint8(k), [16]byte(a)))
}

func maskzCvtepu16Epi32(k uint8, a [16]byte) [16]byte


// MaskCvtepu16Epi321: Zero extend packed unsigned 16-bit integers in 'a' to
// packed 32-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[l+15:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXWD'. Intrinsic: '_mm256_mask_cvtepu16_epi32'.
// Requires AVX512F.
func MaskCvtepu16Epi321(src M256i, k Mmask8, a M128i) M256i {
	return M256i(maskCvtepu16Epi321([32]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepu16Epi321(src [32]byte, k uint8, a [16]byte) [32]byte


// MaskzCvtepu16Epi321: Zero extend packed unsigned 16-bit integers in 'a' to
// packed 32-bit integers, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[l+15:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXWD'. Intrinsic: '_mm256_maskz_cvtepu16_epi32'.
// Requires AVX512F.
func MaskzCvtepu16Epi321(k Mmask8, a M128i) M256i {
	return M256i(maskzCvtepu16Epi321(uint8(k), [16]byte(a)))
}

func maskzCvtepu16Epi321(k uint8, a [16]byte) [32]byte


// Cvtepu16Epi32: Zero extend packed unsigned 16-bit integers in 'a' to packed
// 32-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			k := 16*j
//			dst[i+31:i] := ZeroExtend(a[k+15:k])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXWD'. Intrinsic: '_mm512_cvtepu16_epi32'.
// Requires AVX512F.
func Cvtepu16Epi32(a M256i) M512i {
	return M512i(cvtepu16Epi32([32]byte(a)))
}

func cvtepu16Epi32(a [32]byte) [64]byte


// MaskCvtepu16Epi322: Zero extend packed unsigned 16-bit integers in 'a' to
// packed 32-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[l+15:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXWD'. Intrinsic: '_mm512_mask_cvtepu16_epi32'.
// Requires AVX512F.
func MaskCvtepu16Epi322(src M512i, k Mmask16, a M256i) M512i {
	return M512i(maskCvtepu16Epi322([64]byte(src), uint16(k), [32]byte(a)))
}

func maskCvtepu16Epi322(src [64]byte, k uint16, a [32]byte) [64]byte


// MaskzCvtepu16Epi322: Zero extend packed unsigned 16-bit integers in 'a' to
// packed 32-bit integers, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[l+15:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXWD'. Intrinsic: '_mm512_maskz_cvtepu16_epi32'.
// Requires AVX512F.
func MaskzCvtepu16Epi322(k Mmask16, a M256i) M512i {
	return M512i(maskzCvtepu16Epi322(uint16(k), [32]byte(a)))
}

func maskzCvtepu16Epi322(k uint16, a [32]byte) [64]byte


// MaskCvtepu16Epi64: Zero extend packed unsigned 16-bit integers in the low 4
// bytes of 'a' to packed 64-bit integers, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+15:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVZXWQ'. Intrinsic: '_mm_mask_cvtepu16_epi64'.
// Requires AVX512F.
func MaskCvtepu16Epi64(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtepu16Epi64([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepu16Epi64(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepu16Epi64: Zero extend packed unsigned 16-bit integers in the low 4
// bytes of 'a' to packed 64-bit integers, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+15:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVZXWQ'. Intrinsic: '_mm_maskz_cvtepu16_epi64'.
// Requires AVX512F.
func MaskzCvtepu16Epi64(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtepu16Epi64(uint8(k), [16]byte(a)))
}

func maskzCvtepu16Epi64(k uint8, a [16]byte) [16]byte


// MaskCvtepu16Epi641: Zero extend packed unsigned 16-bit integers in the low 8
// bytes of 'a' to packed 64-bit integers, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+15:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXWQ'. Intrinsic: '_mm256_mask_cvtepu16_epi64'.
// Requires AVX512F.
func MaskCvtepu16Epi641(src M256i, k Mmask8, a M128i) M256i {
	return M256i(maskCvtepu16Epi641([32]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepu16Epi641(src [32]byte, k uint8, a [16]byte) [32]byte


// MaskzCvtepu16Epi641: Zero extend packed unsigned 16-bit integers in the low
// 8 bytes of 'a' to packed 64-bit integers, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+15:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXWQ'. Intrinsic: '_mm256_maskz_cvtepu16_epi64'.
// Requires AVX512F.
func MaskzCvtepu16Epi641(k Mmask8, a M128i) M256i {
	return M256i(maskzCvtepu16Epi641(uint8(k), [16]byte(a)))
}

func maskzCvtepu16Epi641(k uint8, a [16]byte) [32]byte


// Cvtepu16Epi64: Zero extend packed unsigned 16-bit integers in 'a' to packed
// 64-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 16*j
//			dst[i+63:i] := ZeroExtend(a[k+15:k])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXWQ'. Intrinsic: '_mm512_cvtepu16_epi64'.
// Requires AVX512F.
func Cvtepu16Epi64(a M128i) M512i {
	return M512i(cvtepu16Epi64([16]byte(a)))
}

func cvtepu16Epi64(a [16]byte) [64]byte


// MaskCvtepu16Epi642: Zero extend packed unsigned 16-bit integers in 'a' to
// packed 64-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+15:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXWQ'. Intrinsic: '_mm512_mask_cvtepu16_epi64'.
// Requires AVX512F.
func MaskCvtepu16Epi642(src M512i, k Mmask8, a M128i) M512i {
	return M512i(maskCvtepu16Epi642([64]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepu16Epi642(src [64]byte, k uint8, a [16]byte) [64]byte


// MaskzCvtepu16Epi642: Zero extend packed unsigned 16-bit integers in 'a' to
// packed 64-bit integers, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+15:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXWQ'. Intrinsic: '_mm512_maskz_cvtepu16_epi64'.
// Requires AVX512F.
func MaskzCvtepu16Epi642(k Mmask8, a M128i) M512i {
	return M512i(maskzCvtepu16Epi642(uint8(k), [16]byte(a)))
}

func maskzCvtepu16Epi642(k uint8, a [16]byte) [64]byte


// MaskCvtepu32Epi64: Zero extend packed unsigned 32-bit integers in 'a' to
// packed 64-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVZXDQ'. Intrinsic: '_mm_mask_cvtepu32_epi64'.
// Requires AVX512F.
func MaskCvtepu32Epi64(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtepu32Epi64([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepu32Epi64(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepu32Epi64: Zero extend packed unsigned 32-bit integers in 'a' to
// packed 64-bit integers, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+31:l])
//			ELSE 
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVZXDQ'. Intrinsic: '_mm_maskz_cvtepu32_epi64'.
// Requires AVX512F.
func MaskzCvtepu32Epi64(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtepu32Epi64(uint8(k), [16]byte(a)))
}

func maskzCvtepu32Epi64(k uint8, a [16]byte) [16]byte


// MaskCvtepu32Epi641: Zero extend packed unsigned 32-bit integers in 'a' to
// packed 64-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXDQ'. Intrinsic: '_mm256_mask_cvtepu32_epi64'.
// Requires AVX512F.
func MaskCvtepu32Epi641(src M256i, k Mmask8, a M128i) M256i {
	return M256i(maskCvtepu32Epi641([32]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepu32Epi641(src [32]byte, k uint8, a [16]byte) [32]byte


// MaskzCvtepu32Epi641: Zero extend packed unsigned 32-bit integers in 'a' to
// packed 64-bit integers, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+31:l])
//			ELSE 
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXDQ'. Intrinsic: '_mm256_maskz_cvtepu32_epi64'.
// Requires AVX512F.
func MaskzCvtepu32Epi641(k Mmask8, a M128i) M256i {
	return M256i(maskzCvtepu32Epi641(uint8(k), [16]byte(a)))
}

func maskzCvtepu32Epi641(k uint8, a [16]byte) [32]byte


// Cvtepu32Epi64: Zero extend packed unsigned 32-bit integers in 'a' to packed
// 64-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 32*j
//			dst[i+63:i] := ZeroExtend(a[k+31:k])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXDQ'. Intrinsic: '_mm512_cvtepu32_epi64'.
// Requires AVX512F.
func Cvtepu32Epi64(a M256i) M512i {
	return M512i(cvtepu32Epi64([32]byte(a)))
}

func cvtepu32Epi64(a [32]byte) [64]byte


// MaskCvtepu32Epi642: Zero extend packed unsigned 32-bit integers in 'a' to
// packed 64-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXDQ'. Intrinsic: '_mm512_mask_cvtepu32_epi64'.
// Requires AVX512F.
func MaskCvtepu32Epi642(src M512i, k Mmask8, a M256i) M512i {
	return M512i(maskCvtepu32Epi642([64]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtepu32Epi642(src [64]byte, k uint8, a [32]byte) [64]byte


// MaskzCvtepu32Epi642: Zero extend packed unsigned 32-bit integers in 'a' to
// packed 64-bit integers, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+31:l])
//			ELSE 
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXDQ'. Intrinsic: '_mm512_maskz_cvtepu32_epi64'.
// Requires AVX512F.
func MaskzCvtepu32Epi642(k Mmask8, a M256i) M512i {
	return M512i(maskzCvtepu32Epi642(uint8(k), [32]byte(a)))
}

func maskzCvtepu32Epi642(k uint8, a [32]byte) [64]byte


// Cvtepu32Pd: Convert packed unsigned 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			l := j*32
//			dst[i+63:i] := ConvertUnsignedIntegerTo_FP64(a[l+31:l])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTUDQ2PD'. Intrinsic: '_mm_cvtepu32_pd'.
// Requires AVX512F.
func Cvtepu32Pd(a M128i) M128d {
	return M128d(cvtepu32Pd([16]byte(a)))
}

func cvtepu32Pd(a [16]byte) [2]float64


// MaskCvtepu32Pd: Convert packed unsigned 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := ConvertUnsignedIntegerTo_FP64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTUDQ2PD'. Intrinsic: '_mm_mask_cvtepu32_pd'.
// Requires AVX512F.
func MaskCvtepu32Pd(src M128d, k Mmask8, a M128i) M128d {
	return M128d(maskCvtepu32Pd([2]float64(src), uint8(k), [16]byte(a)))
}

func maskCvtepu32Pd(src [2]float64, k uint8, a [16]byte) [2]float64


// MaskzCvtepu32Pd: Convert packed unsigned 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := ConvertUnsignedIntegerTo_FP64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTUDQ2PD'. Intrinsic: '_mm_maskz_cvtepu32_pd'.
// Requires AVX512F.
func MaskzCvtepu32Pd(k Mmask8, a M128i) M128d {
	return M128d(maskzCvtepu32Pd(uint8(k), [16]byte(a)))
}

func maskzCvtepu32Pd(k uint8, a [16]byte) [2]float64


// Cvtepu32Pd1: Convert packed unsigned 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			dst[i+63:i] := ConvertUnsignedIntegerTo_FP64(a[l+31:l])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTUDQ2PD'. Intrinsic: '_mm256_cvtepu32_pd'.
// Requires AVX512F.
func Cvtepu32Pd1(a M128i) M256d {
	return M256d(cvtepu32Pd1([16]byte(a)))
}

func cvtepu32Pd1(a [16]byte) [4]float64


// MaskCvtepu32Pd1: Convert packed unsigned 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := ConvertUnsignedIntegerTo_FP64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTUDQ2PD'. Intrinsic: '_mm256_mask_cvtepu32_pd'.
// Requires AVX512F.
func MaskCvtepu32Pd1(src M256d, k Mmask8, a M128i) M256d {
	return M256d(maskCvtepu32Pd1([4]float64(src), uint8(k), [16]byte(a)))
}

func maskCvtepu32Pd1(src [4]float64, k uint8, a [16]byte) [4]float64


// MaskzCvtepu32Pd1: Convert packed unsigned 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := ConvertUnsignedIntegerTo_FP64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTUDQ2PD'. Intrinsic: '_mm256_maskz_cvtepu32_pd'.
// Requires AVX512F.
func MaskzCvtepu32Pd1(k Mmask8, a M128i) M256d {
	return M256d(maskzCvtepu32Pd1(uint8(k), [16]byte(a)))
}

func maskzCvtepu32Pd1(k uint8, a [16]byte) [4]float64


// Cvtepu32Pd2: Convert packed unsigned 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			dst[i+63:i] := ConvertUnsignedIntegerTo_FP64(a[l+31:l])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTUDQ2PD'. Intrinsic: '_mm512_cvtepu32_pd'.
// Requires AVX512F.
func Cvtepu32Pd2(a M256i) M512d {
	return M512d(cvtepu32Pd2([32]byte(a)))
}

func cvtepu32Pd2(a [32]byte) [8]float64


// MaskCvtepu32Pd2: Convert packed unsigned 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := ConvertUnsignedIntegerTo_FP64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTUDQ2PD'. Intrinsic: '_mm512_mask_cvtepu32_pd'.
// Requires AVX512F.
func MaskCvtepu32Pd2(src M512d, k Mmask8, a M256i) M512d {
	return M512d(maskCvtepu32Pd2([8]float64(src), uint8(k), [32]byte(a)))
}

func maskCvtepu32Pd2(src [8]float64, k uint8, a [32]byte) [8]float64


// MaskzCvtepu32Pd2: Convert packed unsigned 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := ConvertUnsignedIntegerTo_FP64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTUDQ2PD'. Intrinsic: '_mm512_maskz_cvtepu32_pd'.
// Requires AVX512F.
func MaskzCvtepu32Pd2(k Mmask8, a M256i) M512d {
	return M512d(maskzCvtepu32Pd2(uint8(k), [32]byte(a)))
}

func maskzCvtepu32Pd2(k uint8, a [32]byte) [8]float64


// Cvtepu32Ps: Convert packed unsigned 32-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := ConvertUnsignedInt32_To_FP32(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTUDQ2PS'. Intrinsic: '_mm512_cvtepu32_ps'.
// Requires AVX512F.
func Cvtepu32Ps(a M512i) M512 {
	return M512(cvtepu32Ps([64]byte(a)))
}

func cvtepu32Ps(a [64]byte) [16]float32


// MaskCvtepu32Ps: Convert packed unsigned 32-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ConvertUnsignedInt32_To_FP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTUDQ2PS'. Intrinsic: '_mm512_mask_cvtepu32_ps'.
// Requires AVX512F.
func MaskCvtepu32Ps(src M512, k Mmask16, a M512i) M512 {
	return M512(maskCvtepu32Ps([16]float32(src), uint16(k), [64]byte(a)))
}

func maskCvtepu32Ps(src [16]float32, k uint16, a [64]byte) [16]float32


// MaskzCvtepu32Ps: Convert packed unsigned 32-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := ConvertUnsignedInt32_To_FP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTUDQ2PS'. Intrinsic: '_mm512_maskz_cvtepu32_ps'.
// Requires AVX512F.
func MaskzCvtepu32Ps(k Mmask16, a M512i) M512 {
	return M512(maskzCvtepu32Ps(uint16(k), [64]byte(a)))
}

func maskzCvtepu32Ps(k uint16, a [64]byte) [16]float32


// MaskCvtepu8Epi32: Zero extend packed unsigned 8-bit integers in the low 4
// bytes of 'a' to packed 32-bit integers, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[l+7:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVZXBD'. Intrinsic: '_mm_mask_cvtepu8_epi32'.
// Requires AVX512F.
func MaskCvtepu8Epi32(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtepu8Epi32([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepu8Epi32(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepu8Epi32: Zero extend packed unsigned 8-bit integers in th elow 4
// bytes of 'a' to packed 32-bit integers, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[l+7:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVZXBD'. Intrinsic: '_mm_maskz_cvtepu8_epi32'.
// Requires AVX512F.
func MaskzCvtepu8Epi32(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtepu8Epi32(uint8(k), [16]byte(a)))
}

func maskzCvtepu8Epi32(k uint8, a [16]byte) [16]byte


// MaskCvtepu8Epi321: Zero extend packed unsigned 8-bit integers in the low 8
// bytes of 'a' to packed 32-bit integers, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[l+7:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXBD'. Intrinsic: '_mm256_mask_cvtepu8_epi32'.
// Requires AVX512F.
func MaskCvtepu8Epi321(src M256i, k Mmask8, a M128i) M256i {
	return M256i(maskCvtepu8Epi321([32]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepu8Epi321(src [32]byte, k uint8, a [16]byte) [32]byte


// MaskzCvtepu8Epi321: Zero extend packed unsigned 8-bit integers in the low 8
// bytes of 'a' to packed 32-bit integers, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[l+7:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXBD'. Intrinsic: '_mm256_maskz_cvtepu8_epi32'.
// Requires AVX512F.
func MaskzCvtepu8Epi321(k Mmask8, a M128i) M256i {
	return M256i(maskzCvtepu8Epi321(uint8(k), [16]byte(a)))
}

func maskzCvtepu8Epi321(k uint8, a [16]byte) [32]byte


// Cvtepu8Epi32: Zero extend packed unsigned 8-bit integers in 'a' to packed
// 32-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			k := 8*j
//			dst[i+31:i] := ZeroExtend(a[k+7:k])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXBD'. Intrinsic: '_mm512_cvtepu8_epi32'.
// Requires AVX512F.
func Cvtepu8Epi32(a M128i) M512i {
	return M512i(cvtepu8Epi32([16]byte(a)))
}

func cvtepu8Epi32(a [16]byte) [64]byte


// MaskCvtepu8Epi322: Zero extend packed unsigned 8-bit integers in 'a' to
// packed 32-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[l+7:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXBD'. Intrinsic: '_mm512_mask_cvtepu8_epi32'.
// Requires AVX512F.
func MaskCvtepu8Epi322(src M512i, k Mmask16, a M128i) M512i {
	return M512i(maskCvtepu8Epi322([64]byte(src), uint16(k), [16]byte(a)))
}

func maskCvtepu8Epi322(src [64]byte, k uint16, a [16]byte) [64]byte


// MaskzCvtepu8Epi322: Zero extend packed unsigned 8-bit integers in 'a' to
// packed 32-bit integers, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[l+7:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXBD'. Intrinsic: '_mm512_maskz_cvtepu8_epi32'.
// Requires AVX512F.
func MaskzCvtepu8Epi322(k Mmask16, a M128i) M512i {
	return M512i(maskzCvtepu8Epi322(uint16(k), [16]byte(a)))
}

func maskzCvtepu8Epi322(k uint16, a [16]byte) [64]byte


// MaskCvtepu8Epi64: Zero extend packed unsigned 8-bit integers in the low 2
// bytes of 'a' to packed 64-bit integers, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+7:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVZXBQ'. Intrinsic: '_mm_mask_cvtepu8_epi64'.
// Requires AVX512F.
func MaskCvtepu8Epi64(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtepu8Epi64([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepu8Epi64(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepu8Epi64: Zero extend packed unsigned 8-bit integers in the low 2
// bytes of 'a' to packed 64-bit integers, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+7:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVZXBQ'. Intrinsic: '_mm_maskz_cvtepu8_epi64'.
// Requires AVX512F.
func MaskzCvtepu8Epi64(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtepu8Epi64(uint8(k), [16]byte(a)))
}

func maskzCvtepu8Epi64(k uint8, a [16]byte) [16]byte


// MaskCvtepu8Epi641: Zero extend packed unsigned 8-bit integers in the low 4
// bytes of 'a' to packed 64-bit integers, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+7:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXBQ'. Intrinsic: '_mm256_mask_cvtepu8_epi64'.
// Requires AVX512F.
func MaskCvtepu8Epi641(src M256i, k Mmask8, a M128i) M256i {
	return M256i(maskCvtepu8Epi641([32]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepu8Epi641(src [32]byte, k uint8, a [16]byte) [32]byte


// MaskzCvtepu8Epi641: Zero extend packed unsigned 8-bit integers in the low 4
// bytes of 'a' to packed 64-bit integers, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+7:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXBQ'. Intrinsic: '_mm256_maskz_cvtepu8_epi64'.
// Requires AVX512F.
func MaskzCvtepu8Epi641(k Mmask8, a M128i) M256i {
	return M256i(maskzCvtepu8Epi641(uint8(k), [16]byte(a)))
}

func maskzCvtepu8Epi641(k uint8, a [16]byte) [32]byte


// Cvtepu8Epi64: Zero extend packed unsigned 8-bit integers in the low 8 byte
// sof 'a' to packed 64-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 8*j
//			dst[i+63:i] := ZeroExtend(a[k+7:k])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXBQ'. Intrinsic: '_mm512_cvtepu8_epi64'.
// Requires AVX512F.
func Cvtepu8Epi64(a M128i) M512i {
	return M512i(cvtepu8Epi64([16]byte(a)))
}

func cvtepu8Epi64(a [16]byte) [64]byte


// MaskCvtepu8Epi642: Zero extend packed unsigned 8-bit integers in the low 8
// bytes of 'a' to packed 64-bit integers, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+7:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXBQ'. Intrinsic: '_mm512_mask_cvtepu8_epi64'.
// Requires AVX512F.
func MaskCvtepu8Epi642(src M512i, k Mmask8, a M128i) M512i {
	return M512i(maskCvtepu8Epi642([64]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepu8Epi642(src [64]byte, k uint8, a [16]byte) [64]byte


// MaskzCvtepu8Epi642: Zero extend packed unsigned 8-bit integers in the low 8
// bytes of 'a' to packed 64-bit integers, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+7:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMOVZXBQ'. Intrinsic: '_mm512_maskz_cvtepu8_epi64'.
// Requires AVX512F.
func MaskzCvtepu8Epi642(k Mmask8, a M128i) M512i {
	return M512i(maskzCvtepu8Epi642(uint8(k), [16]byte(a)))
}

func maskzCvtepu8Epi642(k uint8, a [16]byte) [64]byte


// Cvti32Sd: Convert the 32-bit integer 'b' to a double-precision (64-bit)
// floating-point element, store the result in the lower element of 'dst', and
// copy the upper element from 'a' to the upper element of 'dst'. 
//
//		dst[63:0] := Convert_Int32_To_FP64(b[31:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTSI2SD'. Intrinsic: '_mm_cvti32_sd'.
// Requires AVX512F.
func Cvti32Sd(a M128d, b int) M128d {
	return M128d(cvti32Sd([2]float64(a), b))
}

func cvti32Sd(a [2]float64, b int) [2]float64


// Cvti32Ss: Convert the 32-bit integer 'b' to a single-precision (32-bit)
// floating-point element, store the result in the lower element of 'dst', and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'. 
//
//		dst[31:0] := Convert_Int32_To_FP32(b[31:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTSI2SS'. Intrinsic: '_mm_cvti32_ss'.
// Requires AVX512F.
func Cvti32Ss(a M128, b int) M128 {
	return M128(cvti32Ss([4]float32(a), b))
}

func cvti32Ss(a [4]float32, b int) [4]float32


// Cvti64Sd: Convert the 64-bit integer 'b' to a double-precision (64-bit)
// floating-point element, store the result in the lower element of 'dst', and
// copy the upper element from 'a' to the upper element of 'dst'. 
//
//		dst[63:0] := Convert_Int64_To_FP64(b[63:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTSI2SD'. Intrinsic: '_mm_cvti64_sd'.
// Requires AVX512F.
func Cvti64Sd(a M128d, b int64) M128d {
	return M128d(cvti64Sd([2]float64(a), b))
}

func cvti64Sd(a [2]float64, b int64) [2]float64


// Cvti64Ss: Convert the 64-bit integer 'b' to a single-precision (32-bit)
// floating-point element, store the result in the lower element of 'dst', and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'. 
//
//		dst[31:0] := Convert_Int64_To_FP32(b[63:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTSI2SS'. Intrinsic: '_mm_cvti64_ss'.
// Requires AVX512F.
func Cvti64Ss(a M128, b int64) M128 {
	return M128(cvti64Ss([4]float32(a), b))
}

func cvti64Ss(a [4]float32, b int64) [4]float32


// MaskCvtpdEpi32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*32
//			l := j*64
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_Int32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTPD2DQ'. Intrinsic: '_mm_mask_cvtpd_epi32'.
// Requires AVX512F.
func MaskCvtpdEpi32(src M128i, k Mmask8, a M128d) M128i {
	return M128i(maskCvtpdEpi32([16]byte(src), uint8(k), [2]float64(a)))
}

func maskCvtpdEpi32(src [16]byte, k uint8, a [2]float64) [16]byte


// MaskzCvtpdEpi32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 1
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_Int32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTPD2DQ'. Intrinsic: '_mm_maskz_cvtpd_epi32'.
// Requires AVX512F.
func MaskzCvtpdEpi32(k Mmask8, a M128d) M128i {
	return M128i(maskzCvtpdEpi32(uint8(k), [2]float64(a)))
}

func maskzCvtpdEpi32(k uint8, a [2]float64) [16]byte


// MaskCvtpdEpi321: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			l := j*64
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_Int32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPD2DQ'. Intrinsic: '_mm256_mask_cvtpd_epi32'.
// Requires AVX512F.
func MaskCvtpdEpi321(src M128i, k Mmask8, a M256d) M128i {
	return M128i(maskCvtpdEpi321([16]byte(src), uint8(k), [4]float64(a)))
}

func maskCvtpdEpi321(src [16]byte, k uint8, a [4]float64) [16]byte


// MaskzCvtpdEpi321: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_Int32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPD2DQ'. Intrinsic: '_mm256_maskz_cvtpd_epi32'.
// Requires AVX512F.
func MaskzCvtpdEpi321(k Mmask8, a M256d) M128i {
	return M128i(maskzCvtpdEpi321(uint8(k), [4]float64(a)))
}

func maskzCvtpdEpi321(k uint8, a [4]float64) [16]byte


// CvtpdEpi32: Convert packed double-precision (64-bit) floating-point elements
// in 'a' to packed 32-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_Int32(a[k+63:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2DQ'. Intrinsic: '_mm512_cvtpd_epi32'.
// Requires AVX512F.
func CvtpdEpi32(a M512d) M256i {
	return M256i(cvtpdEpi32([8]float64(a)))
}

func cvtpdEpi32(a [8]float64) [32]byte


// MaskCvtpdEpi322: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			l := j*64
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_Int32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2DQ'. Intrinsic: '_mm512_mask_cvtpd_epi32'.
// Requires AVX512F.
func MaskCvtpdEpi322(src M256i, k Mmask8, a M512d) M256i {
	return M256i(maskCvtpdEpi322([32]byte(src), uint8(k), [8]float64(a)))
}

func maskCvtpdEpi322(src [32]byte, k uint8, a [8]float64) [32]byte


// MaskzCvtpdEpi322: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_Int32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2DQ'. Intrinsic: '_mm512_maskz_cvtpd_epi32'.
// Requires AVX512F.
func MaskzCvtpdEpi322(k Mmask8, a M512d) M256i {
	return M256i(maskzCvtpdEpi322(uint8(k), [8]float64(a)))
}

func maskzCvtpdEpi322(k uint8, a [8]float64) [32]byte


// CvtpdEpu32: Convert packed double-precision (64-bit) floating-point elements
// in 'a' to packed unsigned 32-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[k+63:k])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTPD2UDQ'. Intrinsic: '_mm_cvtpd_epu32'.
// Requires AVX512F.
func CvtpdEpu32(a M128d) M128i {
	return M128i(cvtpdEpu32([2]float64(a)))
}

func cvtpdEpu32(a [2]float64) [16]byte


// MaskCvtpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*32
//			l := j*64
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTPD2UDQ'. Intrinsic: '_mm_mask_cvtpd_epu32'.
// Requires AVX512F.
func MaskCvtpdEpu32(src M128i, k Mmask8, a M128d) M128i {
	return M128i(maskCvtpdEpu32([16]byte(src), uint8(k), [2]float64(a)))
}

func maskCvtpdEpu32(src [16]byte, k uint8, a [2]float64) [16]byte


// MaskzCvtpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTPD2UDQ'. Intrinsic: '_mm_maskz_cvtpd_epu32'.
// Requires AVX512F.
func MaskzCvtpdEpu32(k Mmask8, a M128d) M128i {
	return M128i(maskzCvtpdEpu32(uint8(k), [2]float64(a)))
}

func maskzCvtpdEpu32(k uint8, a [2]float64) [16]byte


// CvtpdEpu321: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[k+63:k])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPD2UDQ'. Intrinsic: '_mm256_cvtpd_epu32'.
// Requires AVX512F.
func CvtpdEpu321(a M256d) M128i {
	return M128i(cvtpdEpu321([4]float64(a)))
}

func cvtpdEpu321(a [4]float64) [16]byte


// MaskCvtpdEpu321: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			l := j*64
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPD2UDQ'. Intrinsic: '_mm256_mask_cvtpd_epu32'.
// Requires AVX512F.
func MaskCvtpdEpu321(src M128i, k Mmask8, a M256d) M128i {
	return M128i(maskCvtpdEpu321([16]byte(src), uint8(k), [4]float64(a)))
}

func maskCvtpdEpu321(src [16]byte, k uint8, a [4]float64) [16]byte


// MaskzCvtpdEpu321: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPD2UDQ'. Intrinsic: '_mm256_maskz_cvtpd_epu32'.
// Requires AVX512F.
func MaskzCvtpdEpu321(k Mmask8, a M256d) M128i {
	return M128i(maskzCvtpdEpu321(uint8(k), [4]float64(a)))
}

func maskzCvtpdEpu321(k uint8, a [4]float64) [16]byte


// CvtpdEpu322: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[k+63:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2UDQ'. Intrinsic: '_mm512_cvtpd_epu32'.
// Requires AVX512F.
func CvtpdEpu322(a M512d) M256i {
	return M256i(cvtpdEpu322([8]float64(a)))
}

func cvtpdEpu322(a [8]float64) [32]byte


// MaskCvtpdEpu322: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			l := j*64
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2UDQ'. Intrinsic: '_mm512_mask_cvtpd_epu32'.
// Requires AVX512F.
func MaskCvtpdEpu322(src M256i, k Mmask8, a M512d) M256i {
	return M256i(maskCvtpdEpu322([32]byte(src), uint8(k), [8]float64(a)))
}

func maskCvtpdEpu322(src [32]byte, k uint8, a [8]float64) [32]byte


// MaskzCvtpdEpu322: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2UDQ'. Intrinsic: '_mm512_maskz_cvtpd_epu32'.
// Requires AVX512F.
func MaskzCvtpdEpu322(k Mmask8, a M512d) M256i {
	return M256i(maskzCvtpdEpu322(uint8(k), [8]float64(a)))
}

func maskzCvtpdEpu322(k uint8, a [8]float64) [32]byte


// MaskCvtpdPs: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed single-precision (32-bit) floating-point elements,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_FP32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTPD2PS'. Intrinsic: '_mm_mask_cvtpd_ps'.
// Requires AVX512F.
func MaskCvtpdPs(src M128, k Mmask8, a M128d) M128 {
	return M128(maskCvtpdPs([4]float32(src), uint8(k), [2]float64(a)))
}

func maskCvtpdPs(src [4]float32, k uint8, a [2]float64) [4]float32


// MaskzCvtpdPs: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed single-precision (32-bit) floating-point elements,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*32
//			l := j*64
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_FP32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTPD2PS'. Intrinsic: '_mm_maskz_cvtpd_ps'.
// Requires AVX512F.
func MaskzCvtpdPs(k Mmask8, a M128d) M128 {
	return M128(maskzCvtpdPs(uint8(k), [2]float64(a)))
}

func maskzCvtpdPs(k uint8, a [2]float64) [4]float32


// MaskCvtpdPs1: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed single-precision (32-bit) floating-point elements,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_FP32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPD2PS'. Intrinsic: '_mm256_mask_cvtpd_ps'.
// Requires AVX512F.
func MaskCvtpdPs1(src M128, k Mmask8, a M256d) M128 {
	return M128(maskCvtpdPs1([4]float32(src), uint8(k), [4]float64(a)))
}

func maskCvtpdPs1(src [4]float32, k uint8, a [4]float64) [4]float32


// MaskzCvtpdPs1: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed single-precision (32-bit) floating-point elements,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			l := j*64
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_FP32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPD2PS'. Intrinsic: '_mm256_maskz_cvtpd_ps'.
// Requires AVX512F.
func MaskzCvtpdPs1(k Mmask8, a M256d) M128 {
	return M128(maskzCvtpdPs1(uint8(k), [4]float64(a)))
}

func maskzCvtpdPs1(k uint8, a [4]float64) [4]float32


// CvtpdPs: Convert packed double-precision (64-bit) floating-point elements in
// 'a' to packed single-precision (32-bit) floating-point elements, and store
// the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_FP32(a[k+63:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2PS'. Intrinsic: '_mm512_cvtpd_ps'.
// Requires AVX512F.
func CvtpdPs(a M512d) M256 {
	return M256(cvtpdPs([8]float64(a)))
}

func cvtpdPs(a [8]float64) [8]float32


// MaskCvtpdPs2: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed single-precision (32-bit) floating-point elements,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_FP32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2PS'. Intrinsic: '_mm512_mask_cvtpd_ps'.
// Requires AVX512F.
func MaskCvtpdPs2(src M256, k Mmask8, a M512d) M256 {
	return M256(maskCvtpdPs2([8]float32(src), uint8(k), [8]float64(a)))
}

func maskCvtpdPs2(src [8]float32, k uint8, a [8]float64) [8]float32


// MaskzCvtpdPs2: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed single-precision (32-bit) floating-point elements,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			l := j*64
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_FP32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2PS'. Intrinsic: '_mm512_maskz_cvtpd_ps'.
// Requires AVX512F.
func MaskzCvtpdPs2(k Mmask8, a M512d) M256 {
	return M256(maskzCvtpdPs2(uint8(k), [8]float64(a)))
}

func maskzCvtpdPs2(k uint8, a [8]float64) [8]float32


// MaskCvtphPs: Convert packed half-precision (16-bit) floating-point elements
// in 'a' to packed single-precision (32-bit) floating-point elements, and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			m := j*16
//			IF k[j]
//				dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPH2PS'. Intrinsic: '_mm_mask_cvtph_ps'.
// Requires AVX512F.
func MaskCvtphPs(src M128, k Mmask8, a M128i) M128 {
	return M128(maskCvtphPs([4]float32(src), uint8(k), [16]byte(a)))
}

func maskCvtphPs(src [4]float32, k uint8, a [16]byte) [4]float32


// MaskzCvtphPs: Convert packed half-precision (16-bit) floating-point elements
// in 'a' to packed single-precision (32-bit) floating-point elements, and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			m := j*16
//			IF k[j]
//				dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPH2PS'. Intrinsic: '_mm_maskz_cvtph_ps'.
// Requires AVX512F.
func MaskzCvtphPs(k Mmask8, a M128i) M128 {
	return M128(maskzCvtphPs(uint8(k), [16]byte(a)))
}

func maskzCvtphPs(k uint8, a [16]byte) [4]float32


// MaskCvtphPs1: Convert packed half-precision (16-bit) floating-point elements
// in 'a' to packed single-precision (32-bit) floating-point elements, and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			m := j*16
//			IF k[j]
//				dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPH2PS'. Intrinsic: '_mm256_mask_cvtph_ps'.
// Requires AVX512F.
func MaskCvtphPs1(src M256, k Mmask8, a M128i) M256 {
	return M256(maskCvtphPs1([8]float32(src), uint8(k), [16]byte(a)))
}

func maskCvtphPs1(src [8]float32, k uint8, a [16]byte) [8]float32


// MaskzCvtphPs1: Convert packed half-precision (16-bit) floating-point
// elements in 'a' to packed single-precision (32-bit) floating-point elements,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			m := j*16
//			IF k[j]
//				dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPH2PS'. Intrinsic: '_mm256_maskz_cvtph_ps'.
// Requires AVX512F.
func MaskzCvtphPs1(k Mmask8, a M128i) M256 {
	return M256(maskzCvtphPs1(uint8(k), [16]byte(a)))
}

func maskzCvtphPs1(k uint8, a [16]byte) [8]float32


// CvtphPs: Convert packed half-precision (16-bit) floating-point elements in
// 'a' to packed single-precision (32-bit) floating-point elements, and store
// the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			m := j*16
//			dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPH2PS'. Intrinsic: '_mm512_cvtph_ps'.
// Requires AVX512F.
func CvtphPs(a M256i) M512 {
	return M512(cvtphPs([32]byte(a)))
}

func cvtphPs(a [32]byte) [16]float32


// MaskCvtphPs2: Convert packed half-precision (16-bit) floating-point elements
// in 'a' to packed single-precision (32-bit) floating-point elements, and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			m := j*16
//			IF k[j]
//				dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPH2PS'. Intrinsic: '_mm512_mask_cvtph_ps'.
// Requires AVX512F.
func MaskCvtphPs2(src M512, k Mmask16, a M256i) M512 {
	return M512(maskCvtphPs2([16]float32(src), uint16(k), [32]byte(a)))
}

func maskCvtphPs2(src [16]float32, k uint16, a [32]byte) [16]float32


// MaskzCvtphPs2: Convert packed half-precision (16-bit) floating-point
// elements in 'a' to packed single-precision (32-bit) floating-point elements,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			m := j*16
//			IF k[j]
//				dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPH2PS'. Intrinsic: '_mm512_maskz_cvtph_ps'.
// Requires AVX512F.
func MaskzCvtphPs2(k Mmask16, a M256i) M512 {
	return M512(maskzCvtphPs2(uint16(k), [32]byte(a)))
}

func maskzCvtphPs2(k uint16, a [32]byte) [16]float32


// MaskCvtpsEpi32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPS2DQ'. Intrinsic: '_mm_mask_cvtps_epi32'.
// Requires AVX512F.
func MaskCvtpsEpi32(src M128i, k Mmask8, a M128) M128i {
	return M128i(maskCvtpsEpi32([16]byte(src), uint8(k), [4]float32(a)))
}

func maskCvtpsEpi32(src [16]byte, k uint8, a [4]float32) [16]byte


// MaskzCvtpsEpi32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPS2DQ'. Intrinsic: '_mm_maskz_cvtps_epi32'.
// Requires AVX512F.
func MaskzCvtpsEpi32(k Mmask8, a M128) M128i {
	return M128i(maskzCvtpsEpi32(uint8(k), [4]float32(a)))
}

func maskzCvtpsEpi32(k uint8, a [4]float32) [16]byte


// MaskCvtpsEpi321: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2DQ'. Intrinsic: '_mm256_mask_cvtps_epi32'.
// Requires AVX512F.
func MaskCvtpsEpi321(src M256i, k Mmask8, a M256) M256i {
	return M256i(maskCvtpsEpi321([32]byte(src), uint8(k), [8]float32(a)))
}

func maskCvtpsEpi321(src [32]byte, k uint8, a [8]float32) [32]byte


// MaskzCvtpsEpi321: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2DQ'. Intrinsic: '_mm256_maskz_cvtps_epi32'.
// Requires AVX512F.
func MaskzCvtpsEpi321(k Mmask8, a M256) M256i {
	return M256i(maskzCvtpsEpi321(uint8(k), [8]float32(a)))
}

func maskzCvtpsEpi321(k uint8, a [8]float32) [32]byte


// CvtpsEpi32: Convert packed single-precision (32-bit) floating-point elements
// in 'a' to packed 32-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2DQ'. Intrinsic: '_mm512_cvtps_epi32'.
// Requires AVX512F.
func CvtpsEpi32(a M512) M512i {
	return M512i(cvtpsEpi32([16]float32(a)))
}

func cvtpsEpi32(a [16]float32) [64]byte


// MaskCvtpsEpi322: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2DQ'. Intrinsic: '_mm512_mask_cvtps_epi32'.
// Requires AVX512F.
func MaskCvtpsEpi322(src M512i, k Mmask16, a M512) M512i {
	return M512i(maskCvtpsEpi322([64]byte(src), uint16(k), [16]float32(a)))
}

func maskCvtpsEpi322(src [64]byte, k uint16, a [16]float32) [64]byte


// MaskzCvtpsEpi322: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2DQ'. Intrinsic: '_mm512_maskz_cvtps_epi32'.
// Requires AVX512F.
func MaskzCvtpsEpi322(k Mmask16, a M512) M512i {
	return M512i(maskzCvtpsEpi322(uint16(k), [16]float32(a)))
}

func maskzCvtpsEpi322(k uint16, a [16]float32) [64]byte


// CvtpsEpu32: Convert packed single-precision (32-bit) floating-point elements
// in 'a' to packed unsigned 32-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPS2UDQ'. Intrinsic: '_mm_cvtps_epu32'.
// Requires AVX512F.
func CvtpsEpu32(a M128) M128i {
	return M128i(cvtpsEpu32([4]float32(a)))
}

func cvtpsEpu32(a [4]float32) [16]byte


// MaskCvtpsEpu32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPS2UDQ'. Intrinsic: '_mm_mask_cvtps_epu32'.
// Requires AVX512F.
func MaskCvtpsEpu32(src M128i, k Mmask8, a M128) M128i {
	return M128i(maskCvtpsEpu32([16]byte(src), uint8(k), [4]float32(a)))
}

func maskCvtpsEpu32(src [16]byte, k uint8, a [4]float32) [16]byte


// MaskzCvtpsEpu32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPS2UDQ'. Intrinsic: '_mm_maskz_cvtps_epu32'.
// Requires AVX512F.
func MaskzCvtpsEpu32(k Mmask8, a M128) M128i {
	return M128i(maskzCvtpsEpu32(uint8(k), [4]float32(a)))
}

func maskzCvtpsEpu32(k uint8, a [4]float32) [16]byte


// CvtpsEpu321: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2UDQ'. Intrinsic: '_mm256_cvtps_epu32'.
// Requires AVX512F.
func CvtpsEpu321(a M256) M256i {
	return M256i(cvtpsEpu321([8]float32(a)))
}

func cvtpsEpu321(a [8]float32) [32]byte


// MaskCvtpsEpu321: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2UDQ'. Intrinsic: '_mm256_mask_cvtps_epu32'.
// Requires AVX512F.
func MaskCvtpsEpu321(src M256i, k Mmask8, a M256) M256i {
	return M256i(maskCvtpsEpu321([32]byte(src), uint8(k), [8]float32(a)))
}

func maskCvtpsEpu321(src [32]byte, k uint8, a [8]float32) [32]byte


// MaskzCvtpsEpu321: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2UDQ'. Intrinsic: '_mm256_maskz_cvtps_epu32'.
// Requires AVX512F.
func MaskzCvtpsEpu321(k Mmask8, a M256) M256i {
	return M256i(maskzCvtpsEpu321(uint8(k), [8]float32(a)))
}

func maskzCvtpsEpu321(k uint8, a [8]float32) [32]byte


// CvtpsEpu322: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2UDQ'. Intrinsic: '_mm512_cvtps_epu32'.
// Requires AVX512F.
func CvtpsEpu322(a M512) M512i {
	return M512i(cvtpsEpu322([16]float32(a)))
}

func cvtpsEpu322(a [16]float32) [64]byte


// MaskCvtpsEpu322: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2UDQ'. Intrinsic: '_mm512_mask_cvtps_epu32'.
// Requires AVX512F.
func MaskCvtpsEpu322(src M512i, k Mmask16, a M512) M512i {
	return M512i(maskCvtpsEpu322([64]byte(src), uint16(k), [16]float32(a)))
}

func maskCvtpsEpu322(src [64]byte, k uint16, a [16]float32) [64]byte


// MaskzCvtpsEpu322: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2UDQ'. Intrinsic: '_mm512_maskz_cvtps_epu32'.
// Requires AVX512F.
func MaskzCvtpsEpu322(k Mmask16, a M512) M512i {
	return M512i(maskzCvtpsEpu322(uint16(k), [16]float32(a)))
}

func maskzCvtpsEpu322(k uint16, a [16]float32) [64]byte


// CvtpsPd: Convert packed single-precision (32-bit) floating-point elements in
// 'a' to packed double-precision (64-bit) floating-point elements, and store
// the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 32*j
//			dst[i+63:i] := Convert_FP32_To_FP64(a[k+31:k])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2PD'. Intrinsic: '_mm512_cvtps_pd'.
// Requires AVX512F.
func CvtpsPd(a M256) M512d {
	return M512d(cvtpsPd([8]float32(a)))
}

func cvtpsPd(a [8]float32) [8]float64


// MaskCvtpsPd: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed double-precision (64-bit) floating-point elements,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_FP64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2PD'. Intrinsic: '_mm512_mask_cvtps_pd'.
// Requires AVX512F.
func MaskCvtpsPd(src M512d, k Mmask8, a M256) M512d {
	return M512d(maskCvtpsPd([8]float64(src), uint8(k), [8]float32(a)))
}

func maskCvtpsPd(src [8]float64, k uint8, a [8]float32) [8]float64


// MaskzCvtpsPd: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed double-precision (64-bit) floating-point elements,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_FP64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTPS2PD'. Intrinsic: '_mm512_maskz_cvtps_pd'.
// Requires AVX512F.
func MaskzCvtpsPd(k Mmask8, a M256) M512d {
	return M512d(maskzCvtpsPd(uint8(k), [8]float32(a)))
}

func maskzCvtpsPd(k uint8, a [8]float32) [8]float64


// MaskCvtpsPh: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed half-precision (16-bit) floating-point elements,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 3
//			i := 16*j
//			l := 32*j
//			IF k[j]
//				dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm_mask_cvtps_ph'.
// Requires AVX512F.
func MaskCvtpsPh(src M128i, k Mmask8, a M128, rounding int) M128i {
	return M128i(maskCvtpsPh([16]byte(src), uint8(k), [4]float32(a), rounding))
}

func maskCvtpsPh(src [16]byte, k uint8, a [4]float32, rounding int) [16]byte


// MaskzCvtpsPh: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed half-precision (16-bit) floating-point elements,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 3
//			i := 16*j
//			l := 32*j
//			IF k[j]
//				dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm_maskz_cvtps_ph'.
// Requires AVX512F.
func MaskzCvtpsPh(k Mmask8, a M128, rounding int) M128i {
	return M128i(maskzCvtpsPh(uint8(k), [4]float32(a), rounding))
}

func maskzCvtpsPh(k uint8, a [4]float32, rounding int) [16]byte


// MaskCvtpsPh1: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed half-precision (16-bit) floating-point elements,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := 16*j
//			l := 32*j
//			IF k[j]
//				dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm256_mask_cvtps_ph'.
// Requires AVX512F.
func MaskCvtpsPh1(src M128i, k Mmask8, a M256, rounding int) M128i {
	return M128i(maskCvtpsPh1([16]byte(src), uint8(k), [8]float32(a), rounding))
}

func maskCvtpsPh1(src [16]byte, k uint8, a [8]float32, rounding int) [16]byte


// MaskzCvtpsPh1: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed half-precision (16-bit) floating-point elements,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := 16*j
//			l := 32*j
//			IF k[j]
//				dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm256_maskz_cvtps_ph'.
// Requires AVX512F.
func MaskzCvtpsPh1(k Mmask8, a M256, rounding int) M128i {
	return M128i(maskzCvtpsPh1(uint8(k), [8]float32(a), rounding))
}

func maskzCvtpsPh1(k uint8, a [8]float32, rounding int) [16]byte


// CvtpsPh: Convert packed single-precision (32-bit) floating-point elements in
// 'a' to packed half-precision (16-bit) floating-point elements, and store the
// results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 16*j
//			l := 32*j
//			dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm512_cvtps_ph'.
// Requires AVX512F.
func CvtpsPh(a M512, rounding int) M256i {
	return M256i(cvtpsPh([16]float32(a), rounding))
}

func cvtpsPh(a [16]float32, rounding int) [32]byte


// MaskCvtpsPh2: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed half-precision (16-bit) floating-point elements,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 16*j
//			l := 32*j
//			IF k[j]
//				dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm512_mask_cvtps_ph'.
// Requires AVX512F.
func MaskCvtpsPh2(src M256i, k Mmask16, a M512, rounding int) M256i {
	return M256i(maskCvtpsPh2([32]byte(src), uint16(k), [16]float32(a), rounding))
}

func maskCvtpsPh2(src [32]byte, k uint16, a [16]float32, rounding int) [32]byte


// MaskzCvtpsPh2: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed half-precision (16-bit) floating-point elements,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 16*j
//			l := 32*j
//			IF k[j]
//				dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm512_maskz_cvtps_ph'.
// Requires AVX512F.
func MaskzCvtpsPh2(k Mmask16, a M512, rounding int) M256i {
	return M256i(maskzCvtpsPh2(uint16(k), [16]float32(a), rounding))
}

func maskzCvtpsPh2(k uint16, a [16]float32, rounding int) [32]byte


// CvtsdI32: Convert the lower double-precision (64-bit) floating-point element
// in 'a' to a 32-bit integer, and store the result in 'dst'. 
//
//		dst[31:0] := Convert_FP64_To_Int32(a[63:0])
//
// Instruction: 'VCVTSD2SI'. Intrinsic: '_mm_cvtsd_i32'.
// Requires AVX512F.
func CvtsdI32(a M128d) int {
	return int(cvtsdI32([2]float64(a)))
}

func cvtsdI32(a [2]float64) int


// CvtsdI64: Convert the lower double-precision (64-bit) floating-point element
// in 'a' to a 64-bit integer, and store the result in 'dst'. 
//
//		dst[63:0] := Convert_FP64_To_Int64(a[63:0])
//
// Instruction: 'VCVTSD2SI'. Intrinsic: '_mm_cvtsd_i64'.
// Requires AVX512F.
func CvtsdI64(a M128d) int64 {
	return int64(cvtsdI64([2]float64(a)))
}

func cvtsdI64(a [2]float64) int64


// MaskCvtsdSs: Convert the lower double-precision (64-bit) floating-point
// element in 'b' to a single-precision (32-bit) floating-point element, store
// the result in the lower element of 'dst' using writemask 'k' (the element is
// copied from 'src' when mask bit 0 is not set), and copy the upper element
// from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := Convert_FP64_To_FP32(b[63:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:31]
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTSD2SS'. Intrinsic: '_mm_mask_cvtsd_ss'.
// Requires AVX512F.
func MaskCvtsdSs(src M128, k Mmask8, a M128, b M128d) M128 {
	return M128(maskCvtsdSs([4]float32(src), uint8(k), [4]float32(a), [2]float64(b)))
}

func maskCvtsdSs(src [4]float32, k uint8, a [4]float32, b [2]float64) [4]float32


// MaskzCvtsdSs: Convert the lower double-precision (64-bit) floating-point
// element in 'b' to a single-precision (32-bit) floating-point element, store
// the result in the lower element of 'dst' using zeromask 'k' (the element is
// zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements
// from 'a' to the upper elements of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := Convert_FP64_To_FP32(b[63:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:31]
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTSD2SS'. Intrinsic: '_mm_maskz_cvtsd_ss'.
// Requires AVX512F.
func MaskzCvtsdSs(k Mmask8, a M128, b M128d) M128 {
	return M128(maskzCvtsdSs(uint8(k), [4]float32(a), [2]float64(b)))
}

func maskzCvtsdSs(k uint8, a [4]float32, b [2]float64) [4]float32


// CvtsdU32: Convert the lower double-precision (64-bit) floating-point element
// in 'a' to an unsigned 32-bit integer, and store the result in 'dst'. 
//
//		dst[31:0] := Convert_FP64_To_UnsignedInt32(a[63:0])
//
// Instruction: 'VCVTSD2USI'. Intrinsic: '_mm_cvtsd_u32'.
// Requires AVX512F.
func CvtsdU32(a M128d) uint32 {
	return uint32(cvtsdU32([2]float64(a)))
}

func cvtsdU32(a [2]float64) uint32


// CvtsdU64: Convert the lower double-precision (64-bit) floating-point element
// in 'a' to an unsigned 64-bit integer, and store the result in 'dst'. 
//
//		dst[63:0] := Convert_FP64_To_UnsignedInt64(a[63:0])
//
// Instruction: 'VCVTSD2USI'. Intrinsic: '_mm_cvtsd_u64'.
// Requires AVX512F.
func CvtsdU64(a M128d) uint64 {
	return uint64(cvtsdU64([2]float64(a)))
}

func cvtsdU64(a [2]float64) uint64


// Cvtsepi32Epi16: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			k := 16*j
//			dst[k+15:k] := Saturate_Int32_To_Int16(a[i+31:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSDW'. Intrinsic: '_mm_cvtsepi32_epi16'.
// Requires AVX512F.
func Cvtsepi32Epi16(a M128i) M128i {
	return M128i(cvtsepi32Epi16([16]byte(a)))
}

func cvtsepi32Epi16(a [16]byte) [16]byte


// MaskCvtsepi32Epi16: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_Int32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSDW'. Intrinsic: '_mm_mask_cvtsepi32_epi16'.
// Requires AVX512F.
func MaskCvtsepi32Epi16(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtsepi32Epi16([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtsepi32Epi16(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtsepi32Epi16: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_Int32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSDW'. Intrinsic: '_mm_maskz_cvtsepi32_epi16'.
// Requires AVX512F.
func MaskzCvtsepi32Epi16(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtsepi32Epi16(uint8(k), [16]byte(a)))
}

func maskzCvtsepi32Epi16(k uint8, a [16]byte) [16]byte


// Cvtsepi32Epi161: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 16*j
//			dst[k+15:k] := Saturate_Int32_To_Int16(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSDW'. Intrinsic: '_mm256_cvtsepi32_epi16'.
// Requires AVX512F.
func Cvtsepi32Epi161(a M256i) M128i {
	return M128i(cvtsepi32Epi161([32]byte(a)))
}

func cvtsepi32Epi161(a [32]byte) [16]byte


// MaskCvtsepi32Epi161: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_Int32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSDW'. Intrinsic: '_mm256_mask_cvtsepi32_epi16'.
// Requires AVX512F.
func MaskCvtsepi32Epi161(src M128i, k Mmask8, a M256i) M128i {
	return M128i(maskCvtsepi32Epi161([16]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtsepi32Epi161(src [16]byte, k uint8, a [32]byte) [16]byte


// MaskzCvtsepi32Epi161: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_Int32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSDW'. Intrinsic: '_mm256_maskz_cvtsepi32_epi16'.
// Requires AVX512F.
func MaskzCvtsepi32Epi161(k Mmask8, a M256i) M128i {
	return M128i(maskzCvtsepi32Epi161(uint8(k), [32]byte(a)))
}

func maskzCvtsepi32Epi161(k uint8, a [32]byte) [16]byte


// Cvtsepi32Epi162: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			k := 16*j
//			dst[k+15:k] := Saturate_Int32_To_Int16(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSDW'. Intrinsic: '_mm512_cvtsepi32_epi16'.
// Requires AVX512F.
func Cvtsepi32Epi162(a M512i) M256i {
	return M256i(cvtsepi32Epi162([64]byte(a)))
}

func cvtsepi32Epi162(a [64]byte) [32]byte


// MaskCvtsepi32Epi162: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_Int32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSDW'. Intrinsic: '_mm512_mask_cvtsepi32_epi16'.
// Requires AVX512F.
func MaskCvtsepi32Epi162(src M256i, k Mmask16, a M512i) M256i {
	return M256i(maskCvtsepi32Epi162([32]byte(src), uint16(k), [64]byte(a)))
}

func maskCvtsepi32Epi162(src [32]byte, k uint16, a [64]byte) [32]byte


// MaskzCvtsepi32Epi162: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_Int32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSDW'. Intrinsic: '_mm512_maskz_cvtsepi32_epi16'.
// Requires AVX512F.
func MaskzCvtsepi32Epi162(k Mmask16, a M512i) M256i {
	return M256i(maskzCvtsepi32Epi162(uint16(k), [64]byte(a)))
}

func maskzCvtsepi32Epi162(k uint16, a [64]byte) [32]byte


// Cvtsepi32Epi8: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			k := 8*j
//			dst[k+7:k] := Saturate_Int32_To_Int8(a[i+31:i])
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVSDB'. Intrinsic: '_mm_cvtsepi32_epi8'.
// Requires AVX512F.
func Cvtsepi32Epi8(a M128i) M128i {
	return M128i(cvtsepi32Epi8([16]byte(a)))
}

func cvtsepi32Epi8(a [16]byte) [16]byte


// MaskCvtsepi32Epi8: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVSDB'. Intrinsic: '_mm_mask_cvtsepi32_epi8'.
// Requires AVX512F.
func MaskCvtsepi32Epi8(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtsepi32Epi8([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtsepi32Epi8(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtsepi32Epi8: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVSDB'. Intrinsic: '_mm_maskz_cvtsepi32_epi8'.
// Requires AVX512F.
func MaskzCvtsepi32Epi8(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtsepi32Epi8(uint8(k), [16]byte(a)))
}

func maskzCvtsepi32Epi8(k uint8, a [16]byte) [16]byte


// Cvtsepi32Epi81: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 8*j
//			dst[k+7:k] := Saturate_Int32_To_Int8(a[i+31:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSDB'. Intrinsic: '_mm256_cvtsepi32_epi8'.
// Requires AVX512F.
func Cvtsepi32Epi81(a M256i) M128i {
	return M128i(cvtsepi32Epi81([32]byte(a)))
}

func cvtsepi32Epi81(a [32]byte) [16]byte


// MaskCvtsepi32Epi81: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSDB'. Intrinsic: '_mm256_mask_cvtsepi32_epi8'.
// Requires AVX512F.
func MaskCvtsepi32Epi81(src M128i, k Mmask8, a M256i) M128i {
	return M128i(maskCvtsepi32Epi81([16]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtsepi32Epi81(src [16]byte, k uint8, a [32]byte) [16]byte


// MaskzCvtsepi32Epi81: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSDB'. Intrinsic: '_mm256_maskz_cvtsepi32_epi8'.
// Requires AVX512F.
func MaskzCvtsepi32Epi81(k Mmask8, a M256i) M128i {
	return M128i(maskzCvtsepi32Epi81(uint8(k), [32]byte(a)))
}

func maskzCvtsepi32Epi81(k uint8, a [32]byte) [16]byte


// Cvtsepi32Epi82: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			k := 8*j
//			dst[k+7:k] := Saturate_Int32_To_Int8(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSDB'. Intrinsic: '_mm512_cvtsepi32_epi8'.
// Requires AVX512F.
func Cvtsepi32Epi82(a M512i) M128i {
	return M128i(cvtsepi32Epi82([64]byte(a)))
}

func cvtsepi32Epi82(a [64]byte) [16]byte


// MaskCvtsepi32Epi82: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSDB'. Intrinsic: '_mm512_mask_cvtsepi32_epi8'.
// Requires AVX512F.
func MaskCvtsepi32Epi82(src M128i, k Mmask16, a M512i) M128i {
	return M128i(maskCvtsepi32Epi82([16]byte(src), uint16(k), [64]byte(a)))
}

func maskCvtsepi32Epi82(src [16]byte, k uint16, a [64]byte) [16]byte


// MaskzCvtsepi32Epi82: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSDB'. Intrinsic: '_mm512_maskz_cvtsepi32_epi8'.
// Requires AVX512F.
func MaskzCvtsepi32Epi82(k Mmask16, a M512i) M128i {
	return M128i(maskzCvtsepi32Epi82(uint16(k), [64]byte(a)))
}

func maskzCvtsepi32Epi82(k uint16, a [64]byte) [16]byte


// MaskCvtsepi32StoreuEpi16: Convert packed 32-bit integers in 'a' to packed
// 16-bit integers with signed saturation, and store the active results (those
// with their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				MEM[base_addr+l+15:base_addr+l] := Saturate_Int32_To_Int16(a[i+31:i])
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSDW'. Intrinsic: '_mm_mask_cvtsepi32_storeu_epi16'.
// Requires AVX512F.
func MaskCvtsepi32StoreuEpi16(base_addr uintptr, k Mmask8, a M128i)  {
	maskCvtsepi32StoreuEpi16(uintptr(base_addr), uint8(k), [16]byte(a))
}

func maskCvtsepi32StoreuEpi16(base_addr uintptr, k uint8, a [16]byte) 


// MaskCvtsepi32StoreuEpi161: Convert packed 32-bit integers in 'a' to packed
// 16-bit integers with signed saturation, and store the active results (those
// with their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				MEM[base_addr+l+15:base_addr+l] := Saturate_Int32_To_Int16(a[i+31:i])
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSDW'. Intrinsic: '_mm256_mask_cvtsepi32_storeu_epi16'.
// Requires AVX512F.
func MaskCvtsepi32StoreuEpi161(base_addr uintptr, k Mmask8, a M256i)  {
	maskCvtsepi32StoreuEpi161(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCvtsepi32StoreuEpi161(base_addr uintptr, k uint8, a [32]byte) 


// MaskCvtsepi32StoreuEpi162: Convert packed 32-bit integers in 'a' to packed
// 16-bit integers with signed saturation, and store the active results (those
// with their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				MEM[base_addr+l+15:base_addr+l] := Saturate_Int32_To_Int16(a[i+31:i])
//			FI
//		ENDFOR
//
// Instruction: 'VPMOVSDW'. Intrinsic: '_mm512_mask_cvtsepi32_storeu_epi16'.
// Requires AVX512F.
func MaskCvtsepi32StoreuEpi162(base_addr uintptr, k Mmask16, a M512i)  {
	maskCvtsepi32StoreuEpi162(uintptr(base_addr), uint16(k), [64]byte(a))
}

func maskCvtsepi32StoreuEpi162(base_addr uintptr, k uint16, a [64]byte) 


// MaskCvtsepi32StoreuEpi8: Convert packed 32-bit integers in 'a' to packed
// 8-bit integers with signed saturation, and store the active results (those
// with their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Saturate_Int32_To_Int8(a[i+31:i])
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVSDB'. Intrinsic: '_mm_mask_cvtsepi32_storeu_epi8'.
// Requires AVX512F.
func MaskCvtsepi32StoreuEpi8(base_addr uintptr, k Mmask8, a M128i)  {
	maskCvtsepi32StoreuEpi8(uintptr(base_addr), uint8(k), [16]byte(a))
}

func maskCvtsepi32StoreuEpi8(base_addr uintptr, k uint8, a [16]byte) 


// MaskCvtsepi32StoreuEpi81: Convert packed 32-bit integers in 'a' to packed
// 8-bit integers with signed saturation, and store the active results (those
// with their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Saturate_Int32_To_Int8(a[i+31:i])
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSDB'. Intrinsic: '_mm256_mask_cvtsepi32_storeu_epi8'.
// Requires AVX512F.
func MaskCvtsepi32StoreuEpi81(base_addr uintptr, k Mmask8, a M256i)  {
	maskCvtsepi32StoreuEpi81(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCvtsepi32StoreuEpi81(base_addr uintptr, k uint8, a [32]byte) 


// MaskCvtsepi32StoreuEpi82: Convert packed 32-bit integers in 'a' to packed
// 8-bit integers with signed saturation, and store the active results (those
// with their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Saturate_Int32_To_Int8(a[i+31:i])
//			FI
//		ENDFOR
//
// Instruction: 'VPMOVSDB'. Intrinsic: '_mm512_mask_cvtsepi32_storeu_epi8'.
// Requires AVX512F.
func MaskCvtsepi32StoreuEpi82(base_addr uintptr, k Mmask16, a M512i)  {
	maskCvtsepi32StoreuEpi82(uintptr(base_addr), uint16(k), [64]byte(a))
}

func maskCvtsepi32StoreuEpi82(base_addr uintptr, k uint16, a [64]byte) 


// Cvtsepi64Epi16: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			k := 16*j
//			dst[k+15:k] := Saturate_Int64_To_Int16(a[i+63:i])
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVSQW'. Intrinsic: '_mm_cvtsepi64_epi16'.
// Requires AVX512F.
func Cvtsepi64Epi16(a M128i) M128i {
	return M128i(cvtsepi64Epi16([16]byte(a)))
}

func cvtsepi64Epi16(a [16]byte) [16]byte


// MaskCvtsepi64Epi16: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_Int64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVSQW'. Intrinsic: '_mm_mask_cvtsepi64_epi16'.
// Requires AVX512F.
func MaskCvtsepi64Epi16(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtsepi64Epi16([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtsepi64Epi16(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtsepi64Epi16: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_Int64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVSQW'. Intrinsic: '_mm_maskz_cvtsepi64_epi16'.
// Requires AVX512F.
func MaskzCvtsepi64Epi16(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtsepi64Epi16(uint8(k), [16]byte(a)))
}

func maskzCvtsepi64Epi16(k uint8, a [16]byte) [16]byte


// Cvtsepi64Epi161: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			k := 16*j
//			dst[k+15:k] := Saturate_Int64_To_Int16(a[i+63:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSQW'. Intrinsic: '_mm256_cvtsepi64_epi16'.
// Requires AVX512F.
func Cvtsepi64Epi161(a M256i) M128i {
	return M128i(cvtsepi64Epi161([32]byte(a)))
}

func cvtsepi64Epi161(a [32]byte) [16]byte


// MaskCvtsepi64Epi161: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_Int64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSQW'. Intrinsic: '_mm256_mask_cvtsepi64_epi16'.
// Requires AVX512F.
func MaskCvtsepi64Epi161(src M128i, k Mmask8, a M256i) M128i {
	return M128i(maskCvtsepi64Epi161([16]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtsepi64Epi161(src [16]byte, k uint8, a [32]byte) [16]byte


// MaskzCvtsepi64Epi161: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_Int64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSQW'. Intrinsic: '_mm256_maskz_cvtsepi64_epi16'.
// Requires AVX512F.
func MaskzCvtsepi64Epi161(k Mmask8, a M256i) M128i {
	return M128i(maskzCvtsepi64Epi161(uint8(k), [32]byte(a)))
}

func maskzCvtsepi64Epi161(k uint8, a [32]byte) [16]byte


// Cvtsepi64Epi162: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 16*j
//			dst[k+15:k] := Saturate_Int64_To_Int16(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSQW'. Intrinsic: '_mm512_cvtsepi64_epi16'.
// Requires AVX512F.
func Cvtsepi64Epi162(a M512i) M128i {
	return M128i(cvtsepi64Epi162([64]byte(a)))
}

func cvtsepi64Epi162(a [64]byte) [16]byte


// MaskCvtsepi64Epi162: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_Int64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSQW'. Intrinsic: '_mm512_mask_cvtsepi64_epi16'.
// Requires AVX512F.
func MaskCvtsepi64Epi162(src M128i, k Mmask8, a M512i) M128i {
	return M128i(maskCvtsepi64Epi162([16]byte(src), uint8(k), [64]byte(a)))
}

func maskCvtsepi64Epi162(src [16]byte, k uint8, a [64]byte) [16]byte


// MaskzCvtsepi64Epi162: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_Int64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSQW'. Intrinsic: '_mm512_maskz_cvtsepi64_epi16'.
// Requires AVX512F.
func MaskzCvtsepi64Epi162(k Mmask8, a M512i) M128i {
	return M128i(maskzCvtsepi64Epi162(uint8(k), [64]byte(a)))
}

func maskzCvtsepi64Epi162(k uint8, a [64]byte) [16]byte


// Cvtsepi64Epi32: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			k := 32*j
//			dst[k+31:k] := Saturate_Int64_To_Int32(a[i+63:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSQD'. Intrinsic: '_mm_cvtsepi64_epi32'.
// Requires AVX512F.
func Cvtsepi64Epi32(a M128i) M128i {
	return M128i(cvtsepi64Epi32([16]byte(a)))
}

func cvtsepi64Epi32(a [16]byte) [16]byte


// MaskCvtsepi64Epi32: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Saturate_Int64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := src[l+31:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSQD'. Intrinsic: '_mm_mask_cvtsepi64_epi32'.
// Requires AVX512F.
func MaskCvtsepi64Epi32(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtsepi64Epi32([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtsepi64Epi32(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtsepi64Epi32: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Saturate_Int64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSQD'. Intrinsic: '_mm_maskz_cvtsepi64_epi32'.
// Requires AVX512F.
func MaskzCvtsepi64Epi32(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtsepi64Epi32(uint8(k), [16]byte(a)))
}

func maskzCvtsepi64Epi32(k uint8, a [16]byte) [16]byte


// Cvtsepi64Epi321: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			k := 32*j
//			dst[k+31:k] := Saturate_Int64_To_Int32(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSQD'. Intrinsic: '_mm256_cvtsepi64_epi32'.
// Requires AVX512F.
func Cvtsepi64Epi321(a M256i) M128i {
	return M128i(cvtsepi64Epi321([32]byte(a)))
}

func cvtsepi64Epi321(a [32]byte) [16]byte


// MaskCvtsepi64Epi321: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Saturate_Int64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := src[l+31:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSQD'. Intrinsic: '_mm256_mask_cvtsepi64_epi32'.
// Requires AVX512F.
func MaskCvtsepi64Epi321(src M128i, k Mmask8, a M256i) M128i {
	return M128i(maskCvtsepi64Epi321([16]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtsepi64Epi321(src [16]byte, k uint8, a [32]byte) [16]byte


// MaskzCvtsepi64Epi321: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Saturate_Int64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSQD'. Intrinsic: '_mm256_maskz_cvtsepi64_epi32'.
// Requires AVX512F.
func MaskzCvtsepi64Epi321(k Mmask8, a M256i) M128i {
	return M128i(maskzCvtsepi64Epi321(uint8(k), [32]byte(a)))
}

func maskzCvtsepi64Epi321(k uint8, a [32]byte) [16]byte


// Cvtsepi64Epi322: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 32*j
//			dst[k+31:k] := Saturate_Int64_To_Int32(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSQD'. Intrinsic: '_mm512_cvtsepi64_epi32'.
// Requires AVX512F.
func Cvtsepi64Epi322(a M512i) M256i {
	return M256i(cvtsepi64Epi322([64]byte(a)))
}

func cvtsepi64Epi322(a [64]byte) [32]byte


// MaskCvtsepi64Epi322: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Saturate_Int64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := src[l+31:l]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSQD'. Intrinsic: '_mm512_mask_cvtsepi64_epi32'.
// Requires AVX512F.
func MaskCvtsepi64Epi322(src M256i, k Mmask8, a M512i) M256i {
	return M256i(maskCvtsepi64Epi322([32]byte(src), uint8(k), [64]byte(a)))
}

func maskCvtsepi64Epi322(src [32]byte, k uint8, a [64]byte) [32]byte


// MaskzCvtsepi64Epi322: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Saturate_Int64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSQD'. Intrinsic: '_mm512_maskz_cvtsepi64_epi32'.
// Requires AVX512F.
func MaskzCvtsepi64Epi322(k Mmask8, a M512i) M256i {
	return M256i(maskzCvtsepi64Epi322(uint8(k), [64]byte(a)))
}

func maskzCvtsepi64Epi322(k uint8, a [64]byte) [32]byte


// Cvtsepi64Epi8: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			k := 8*j
//			dst[k+7:k] := Saturate_Int64_To_Int8(a[i+63:i])
//		ENDFOR
//		dst[MAX:16] := 0
//
// Instruction: 'VPMOVSQB'. Intrinsic: '_mm_cvtsepi64_epi8'.
// Requires AVX512F.
func Cvtsepi64Epi8(a M128i) M128i {
	return M128i(cvtsepi64Epi8([16]byte(a)))
}

func cvtsepi64Epi8(a [16]byte) [16]byte


// MaskCvtsepi64Epi8: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:16] := 0
//
// Instruction: 'VPMOVSQB'. Intrinsic: '_mm_mask_cvtsepi64_epi8'.
// Requires AVX512F.
func MaskCvtsepi64Epi8(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtsepi64Epi8([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtsepi64Epi8(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtsepi64Epi8: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:16] := 0
//
// Instruction: 'VPMOVSQB'. Intrinsic: '_mm_maskz_cvtsepi64_epi8'.
// Requires AVX512F.
func MaskzCvtsepi64Epi8(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtsepi64Epi8(uint8(k), [16]byte(a)))
}

func maskzCvtsepi64Epi8(k uint8, a [16]byte) [16]byte


// Cvtsepi64Epi81: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			k := 8*j
//			dst[k+7:k] := Saturate_Int64_To_Int8(a[i+63:i])
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVSQB'. Intrinsic: '_mm256_cvtsepi64_epi8'.
// Requires AVX512F.
func Cvtsepi64Epi81(a M256i) M128i {
	return M128i(cvtsepi64Epi81([32]byte(a)))
}

func cvtsepi64Epi81(a [32]byte) [16]byte


// MaskCvtsepi64Epi81: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVSQB'. Intrinsic: '_mm256_mask_cvtsepi64_epi8'.
// Requires AVX512F.
func MaskCvtsepi64Epi81(src M128i, k Mmask8, a M256i) M128i {
	return M128i(maskCvtsepi64Epi81([16]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtsepi64Epi81(src [16]byte, k uint8, a [32]byte) [16]byte


// MaskzCvtsepi64Epi81: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVSQB'. Intrinsic: '_mm256_maskz_cvtsepi64_epi8'.
// Requires AVX512F.
func MaskzCvtsepi64Epi81(k Mmask8, a M256i) M128i {
	return M128i(maskzCvtsepi64Epi81(uint8(k), [32]byte(a)))
}

func maskzCvtsepi64Epi81(k uint8, a [32]byte) [16]byte


// Cvtsepi64Epi82: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 8*j
//			dst[k+7:k] := Saturate_Int64_To_Int8(a[i+63:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSQB'. Intrinsic: '_mm512_cvtsepi64_epi8'.
// Requires AVX512F.
func Cvtsepi64Epi82(a M512i) M128i {
	return M128i(cvtsepi64Epi82([64]byte(a)))
}

func cvtsepi64Epi82(a [64]byte) [16]byte


// MaskCvtsepi64Epi82: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSQB'. Intrinsic: '_mm512_mask_cvtsepi64_epi8'.
// Requires AVX512F.
func MaskCvtsepi64Epi82(src M128i, k Mmask8, a M512i) M128i {
	return M128i(maskCvtsepi64Epi82([16]byte(src), uint8(k), [64]byte(a)))
}

func maskCvtsepi64Epi82(src [16]byte, k uint8, a [64]byte) [16]byte


// MaskzCvtsepi64Epi82: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSQB'. Intrinsic: '_mm512_maskz_cvtsepi64_epi8'.
// Requires AVX512F.
func MaskzCvtsepi64Epi82(k Mmask8, a M512i) M128i {
	return M128i(maskzCvtsepi64Epi82(uint8(k), [64]byte(a)))
}

func maskzCvtsepi64Epi82(k uint8, a [64]byte) [16]byte


// MaskCvtsepi64StoreuEpi16: Convert packed 64-bit integers in 'a' to packed
// 16-bit integers with signed saturation, and store the active results (those
// with their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				MEM[base_addr+l+15:base_addr+l] := Saturate_Int64_To_Int16(a[i+63:i])
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVSQW'. Intrinsic: '_mm_mask_cvtsepi64_storeu_epi16'.
// Requires AVX512F.
func MaskCvtsepi64StoreuEpi16(base_addr uintptr, k Mmask8, a M128i)  {
	maskCvtsepi64StoreuEpi16(uintptr(base_addr), uint8(k), [16]byte(a))
}

func maskCvtsepi64StoreuEpi16(base_addr uintptr, k uint8, a [16]byte) 


// MaskCvtsepi64StoreuEpi161: Convert packed 64-bit integers in 'a' to packed
// 16-bit integers with signed saturation, and store the active results (those
// with their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				MEM[base_addr+l+15:base_addr+l] := Saturate_Int64_To_Int16(a[i+63:i])
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSQW'. Intrinsic: '_mm256_mask_cvtsepi64_storeu_epi16'.
// Requires AVX512F.
func MaskCvtsepi64StoreuEpi161(base_addr uintptr, k Mmask8, a M256i)  {
	maskCvtsepi64StoreuEpi161(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCvtsepi64StoreuEpi161(base_addr uintptr, k uint8, a [32]byte) 


// MaskCvtsepi64StoreuEpi162: Convert packed 64-bit integers in 'a' to packed
// 16-bit integers with signed saturation, and store the active results (those
// with their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				MEM[base_addr+l+15:base_addr+l] := Saturate_Int64_To_Int16(a[i+63:i])
//			FI
//		ENDFOR
//
// Instruction: 'VPMOVSQW'. Intrinsic: '_mm512_mask_cvtsepi64_storeu_epi16'.
// Requires AVX512F.
func MaskCvtsepi64StoreuEpi162(base_addr uintptr, k Mmask8, a M512i)  {
	maskCvtsepi64StoreuEpi162(uintptr(base_addr), uint8(k), [64]byte(a))
}

func maskCvtsepi64StoreuEpi162(base_addr uintptr, k uint8, a [64]byte) 


// MaskCvtsepi64StoreuEpi32: Convert packed 64-bit integers in 'a' to packed
// 32-bit integers with signed saturation, and store the active results (those
// with their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				MEM[base_addr+l+31:base_addr+l] := Saturate_Int64_To_Int32(a[i+63:i])
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSQD'. Intrinsic: '_mm_mask_cvtsepi64_storeu_epi32'.
// Requires AVX512F.
func MaskCvtsepi64StoreuEpi32(base_addr uintptr, k Mmask8, a M128i)  {
	maskCvtsepi64StoreuEpi32(uintptr(base_addr), uint8(k), [16]byte(a))
}

func maskCvtsepi64StoreuEpi32(base_addr uintptr, k uint8, a [16]byte) 


// MaskCvtsepi64StoreuEpi321: Convert packed 64-bit integers in 'a' to packed
// 32-bit integers with signed saturation, and store the active results (those
// with their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				MEM[base_addr+l+31:base_addr+l] := Saturate_Int64_To_Int32(a[i+63:i])
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSQD'. Intrinsic: '_mm256_mask_cvtsepi64_storeu_epi32'.
// Requires AVX512F.
func MaskCvtsepi64StoreuEpi321(base_addr uintptr, k Mmask8, a M256i)  {
	maskCvtsepi64StoreuEpi321(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCvtsepi64StoreuEpi321(base_addr uintptr, k uint8, a [32]byte) 


// MaskCvtsepi64StoreuEpi322: Convert packed 64-bit integers in 'a' to packed
// 32-bit integers with signed saturation, and store the active results (those
// with their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				MEM[base_addr+l+31:base_addr+l] := Saturate_Int64_To_Int32(a[i+63:i])
//			FI
//		ENDFOR
//
// Instruction: 'VPMOVSQD'. Intrinsic: '_mm512_mask_cvtsepi64_storeu_epi32'.
// Requires AVX512F.
func MaskCvtsepi64StoreuEpi322(base_addr uintptr, k Mmask8, a M512i)  {
	maskCvtsepi64StoreuEpi322(uintptr(base_addr), uint8(k), [64]byte(a))
}

func maskCvtsepi64StoreuEpi322(base_addr uintptr, k uint8, a [64]byte) 


// MaskCvtsepi64StoreuEpi8: Convert packed 64-bit integers in 'a' to packed
// 8-bit integers with signed saturation, and store the active results (those
// with their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Saturate_Int64_To_Int8(a[i+63:i])
//			FI
//		ENDFOR
//		dst[MAX:16] := 0
//
// Instruction: 'VPMOVSQB'. Intrinsic: '_mm_mask_cvtsepi64_storeu_epi8'.
// Requires AVX512F.
func MaskCvtsepi64StoreuEpi8(base_addr uintptr, k Mmask8, a M128i)  {
	maskCvtsepi64StoreuEpi8(uintptr(base_addr), uint8(k), [16]byte(a))
}

func maskCvtsepi64StoreuEpi8(base_addr uintptr, k uint8, a [16]byte) 


// MaskCvtsepi64StoreuEpi81: Convert packed 64-bit integers in 'a' to packed
// 8-bit integers with signed saturation, and store the active results (those
// with their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Saturate_Int64_To_Int8(a[i+63:i])
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVSQB'. Intrinsic: '_mm256_mask_cvtsepi64_storeu_epi8'.
// Requires AVX512F.
func MaskCvtsepi64StoreuEpi81(base_addr uintptr, k Mmask8, a M256i)  {
	maskCvtsepi64StoreuEpi81(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCvtsepi64StoreuEpi81(base_addr uintptr, k uint8, a [32]byte) 


// MaskCvtsepi64StoreuEpi82: Convert packed 64-bit integers in 'a' to packed
// 8-bit integers with signed saturation, and store the active results (those
// with their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Saturate_Int64_To_Int8(a[i+63:i])
//			FI
//		ENDFOR
//
// Instruction: 'VPMOVSQB'. Intrinsic: '_mm512_mask_cvtsepi64_storeu_epi8'.
// Requires AVX512F.
func MaskCvtsepi64StoreuEpi82(base_addr uintptr, k Mmask8, a M512i)  {
	maskCvtsepi64StoreuEpi82(uintptr(base_addr), uint8(k), [64]byte(a))
}

func maskCvtsepi64StoreuEpi82(base_addr uintptr, k uint8, a [64]byte) 


// CvtssI32: Convert the lower single-precision (32-bit) floating-point element
// in 'a' to a 32-bit integer, and store the result in 'dst'. 
//
//		dst[31:0] := Convert_FP32_To_Int32(a[31:0])
//
// Instruction: 'VCVTSS2SI'. Intrinsic: '_mm_cvtss_i32'.
// Requires AVX512F.
func CvtssI32(a M128) int {
	return int(cvtssI32([4]float32(a)))
}

func cvtssI32(a [4]float32) int


// CvtssI64: Convert the lower single-precision (32-bit) floating-point element
// in 'a' to a 64-bit integer, and store the result in 'dst'. 
//
//		dst[63:0] := Convert_FP32_To_Int64(a[31:0])
//
// Instruction: 'VCVTSS2SI'. Intrinsic: '_mm_cvtss_i64'.
// Requires AVX512F.
func CvtssI64(a M128) int64 {
	return int64(cvtssI64([4]float32(a)))
}

func cvtssI64(a [4]float32) int64


// MaskCvtssSd: Convert the lower single-precision (32-bit) floating-point
// element in 'b' to a double-precision (64-bit) floating-point element, store
// the result in the lower element of 'dst' using writemask 'k' (the element is
// copied from 'src' when mask bit 0 is not set), and copy the upper element
// from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := Convert_FP32_To_FP64(b[31:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTSS2SD'. Intrinsic: '_mm_mask_cvtss_sd'.
// Requires AVX512F.
func MaskCvtssSd(src M128d, k Mmask8, a M128d, b M128) M128d {
	return M128d(maskCvtssSd([2]float64(src), uint8(k), [2]float64(a), [4]float32(b)))
}

func maskCvtssSd(src [2]float64, k uint8, a [2]float64, b [4]float32) [2]float64


// MaskzCvtssSd: Convert the lower single-precision (32-bit) floating-point
// element in 'b' to a double-precision (64-bit) floating-point element, store
// the result in the lower element of 'dst' using zeromask 'k' (the element is
// zeroed out when mask bit 0 is not set), and copy the upper element from 'a'
// to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := Convert_FP32_To_FP64(b[31:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTSS2SD'. Intrinsic: '_mm_maskz_cvtss_sd'.
// Requires AVX512F.
func MaskzCvtssSd(k Mmask8, a M128d, b M128) M128d {
	return M128d(maskzCvtssSd(uint8(k), [2]float64(a), [4]float32(b)))
}

func maskzCvtssSd(k uint8, a [2]float64, b [4]float32) [2]float64


// CvtssU32: Convert the lower single-precision (32-bit) floating-point element
// in 'a' to an unsigned 32-bit integer, and store the result in 'dst'. 
//
//		dst[31:0] := Convert_FP32_To_UnsignedInt32(a[31:0])
//
// Instruction: 'VCVTSS2USI'. Intrinsic: '_mm_cvtss_u32'.
// Requires AVX512F.
func CvtssU32(a M128) uint32 {
	return uint32(cvtssU32([4]float32(a)))
}

func cvtssU32(a [4]float32) uint32


// CvtssU64: Convert the lower single-precision (32-bit) floating-point element
// in 'a' to an unsigned 64-bit integer, and store the result in 'dst'. 
//
//		dst[63:0] := Convert_FP32_To_UnsignedInt64(a[31:0])
//
// Instruction: 'VCVTSS2USI'. Intrinsic: '_mm_cvtss_u64'.
// Requires AVX512F.
func CvtssU64(a M128) uint64 {
	return uint64(cvtssU64([4]float32(a)))
}

func cvtssU64(a [4]float32) uint64


// CvttRoundpdEpi32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst'. 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := 32*i
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_IntegerTruncate(a[k+63:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2DQ'. Intrinsic: '_mm512_cvtt_roundpd_epi32'.
// Requires AVX512F.
func CvttRoundpdEpi32(a M512d, sae int) M256i {
	return M256i(cvttRoundpdEpi32([8]float64(a), sae))
}

func cvttRoundpdEpi32(a [8]float64, sae int) [32]byte


// MaskCvttRoundpdEpi32: Convert packed double-precision (64-bit)
// floating-point elements in 'a' to packed 32-bit integers with truncation,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set).  Pass __MM_FROUND_NO_EXC
// to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := 32*i
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_IntegerTruncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2DQ'. Intrinsic: '_mm512_mask_cvtt_roundpd_epi32'.
// Requires AVX512F.
func MaskCvttRoundpdEpi32(src M256i, k Mmask8, a M512d, sae int) M256i {
	return M256i(maskCvttRoundpdEpi32([32]byte(src), uint8(k), [8]float64(a), sae))
}

func maskCvttRoundpdEpi32(src [32]byte, k uint8, a [8]float64, sae int) [32]byte


// MaskzCvttRoundpdEpi32: Convert packed double-precision (64-bit)
// floating-point elements in 'a' to packed 32-bit integers with truncation,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := 32*i
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_IntegerTruncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2DQ'. Intrinsic: '_mm512_maskz_cvtt_roundpd_epi32'.
// Requires AVX512F.
func MaskzCvttRoundpdEpi32(k Mmask8, a M512d, sae int) M256i {
	return M256i(maskzCvttRoundpdEpi32(uint8(k), [8]float64(a), sae))
}

func maskzCvttRoundpdEpi32(k uint8, a [8]float64, sae int) [32]byte


// CvttRoundpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst'. 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := 32*i
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_UnsignedIntegerTruncate(a[k+63:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2UDQ'. Intrinsic: '_mm512_cvtt_roundpd_epu32'.
// Requires AVX512F.
func CvttRoundpdEpu32(a M512d, sae int) M256i {
	return M256i(cvttRoundpdEpu32([8]float64(a), sae))
}

func cvttRoundpdEpu32(a [8]float64, sae int) [32]byte


// MaskCvttRoundpdEpu32: Convert packed double-precision (64-bit)
// floating-point elements in 'a' to packed unsigned 32-bit integers with
// truncation, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set).  
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := 32*i
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedIntegerTruncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2UDQ'. Intrinsic: '_mm512_mask_cvtt_roundpd_epu32'.
// Requires AVX512F.
func MaskCvttRoundpdEpu32(src M256i, k Mmask8, a M512d, sae int) M256i {
	return M256i(maskCvttRoundpdEpu32([32]byte(src), uint8(k), [8]float64(a), sae))
}

func maskCvttRoundpdEpu32(src [32]byte, k uint8, a [8]float64, sae int) [32]byte


// MaskzCvttRoundpdEpu32: Convert packed double-precision (64-bit)
// floating-point elements in 'a' to packed unsigned 32-bit integers with
// truncation, and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := 32*i
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedIntegerTruncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2UDQ'. Intrinsic: '_mm512_maskz_cvtt_roundpd_epu32'.
// Requires AVX512F.
func MaskzCvttRoundpdEpu32(k Mmask8, a M512d, sae int) M256i {
	return M256i(maskzCvttRoundpdEpu32(uint8(k), [8]float64(a), sae))
}

func maskzCvttRoundpdEpu32(k uint8, a [8]float64, sae int) [32]byte


// CvttRoundpsEpi32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst'. 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 15
//			i := 32*i
//			dst[i+31:i] := Convert_FP32_To_IntegerTruncate(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2DQ'. Intrinsic: '_mm512_cvtt_roundps_epi32'.
// Requires AVX512F.
func CvttRoundpsEpi32(a M512, sae int) M512i {
	return M512i(cvttRoundpsEpi32([16]float32(a), sae))
}

func cvttRoundpsEpi32(a [16]float32, sae int) [64]byte


// MaskCvttRoundpsEpi32: Convert packed single-precision (32-bit)
// floating-point elements in 'a' to packed 32-bit integers with truncation,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 15
//			i := 32*i
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_IntegerTruncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2DQ'. Intrinsic: '_mm512_mask_cvtt_roundps_epi32'.
// Requires AVX512F.
func MaskCvttRoundpsEpi32(src M512i, k Mmask16, a M512, sae int) M512i {
	return M512i(maskCvttRoundpsEpi32([64]byte(src), uint16(k), [16]float32(a), sae))
}

func maskCvttRoundpsEpi32(src [64]byte, k uint16, a [16]float32, sae int) [64]byte


// MaskzCvttRoundpsEpi32: Convert packed single-precision (32-bit)
// floating-point elements in 'a' to packed 32-bit integers with truncation,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 15
//			i := 32*i
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_IntegerTruncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2DQ'. Intrinsic: '_mm512_maskz_cvtt_roundps_epi32'.
// Requires AVX512F.
func MaskzCvttRoundpsEpi32(k Mmask16, a M512, sae int) M512i {
	return M512i(maskzCvttRoundpsEpi32(uint16(k), [16]float32(a), sae))
}

func maskzCvttRoundpsEpi32(k uint16, a [16]float32, sae int) [64]byte


// CvttRoundpsEpu32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst'. 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 15
//			i := 32*i
//			dst[i+31:i] := Convert_FP32_To_UnsignedIntegerTruncate(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2UDQ'. Intrinsic: '_mm512_cvtt_roundps_epu32'.
// Requires AVX512F.
func CvttRoundpsEpu32(a M512, sae int) M512i {
	return M512i(cvttRoundpsEpu32([16]float32(a), sae))
}

func cvttRoundpsEpu32(a [16]float32, sae int) [64]byte


// MaskCvttRoundpsEpu32: Convert packed single-precision (32-bit)
// floating-point elements in 'a' to packed unsigned 32-bit integers with
// truncation, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set).  
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 15
//			i := 32*i
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_UnsignedIntegerTruncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2UDQ'. Intrinsic: '_mm512_mask_cvtt_roundps_epu32'.
// Requires AVX512F.
func MaskCvttRoundpsEpu32(src M512i, k Mmask16, a M512, sae int) M512i {
	return M512i(maskCvttRoundpsEpu32([64]byte(src), uint16(k), [16]float32(a), sae))
}

func maskCvttRoundpsEpu32(src [64]byte, k uint16, a [16]float32, sae int) [64]byte


// MaskzCvttRoundpsEpu32: Convert packed single-precision (32-bit)
// floating-point elements in 'a' to packed unsigned 32-bit integers with
// truncation, and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 15
//			i := 32*i
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_UnsignedIntegerTruncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2UDQ'. Intrinsic: '_mm512_maskz_cvtt_roundps_epu32'.
// Requires AVX512F.
func MaskzCvttRoundpsEpu32(k Mmask16, a M512, sae int) M512i {
	return M512i(maskzCvttRoundpsEpu32(uint16(k), [16]float32(a), sae))
}

func maskzCvttRoundpsEpu32(k uint16, a [16]float32, sae int) [64]byte


// CvttRoundsdI32: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to a 32-bit integer with truncation, and store the result in
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_FP64_To_Int32_Truncate(a[63:0])
//
// Instruction: 'VCVTTSD2SI'. Intrinsic: '_mm_cvtt_roundsd_i32'.
// Requires AVX512F.
func CvttRoundsdI32(a M128d, rounding int) int {
	return int(cvttRoundsdI32([2]float64(a), rounding))
}

func cvttRoundsdI32(a [2]float64, rounding int) int


// CvttRoundsdI64: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to a 64-bit integer with truncation, and store the result in
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_FP64_To_Int64_Truncate(a[63:0])
//
// Instruction: 'VCVTTSD2SI'. Intrinsic: '_mm_cvtt_roundsd_i64'.
// Requires AVX512F.
func CvttRoundsdI64(a M128d, rounding int) int64 {
	return int64(cvttRoundsdI64([2]float64(a), rounding))
}

func cvttRoundsdI64(a [2]float64, rounding int) int64


// CvttRoundsdSi32: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to a 32-bit integer with truncation, and store the result in
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_FP64_To_Int32_Truncate(a[63:0])
//
// Instruction: 'VCVTTSD2SI'. Intrinsic: '_mm_cvtt_roundsd_si32'.
// Requires AVX512F.
func CvttRoundsdSi32(a M128d, rounding int) int {
	return int(cvttRoundsdSi32([2]float64(a), rounding))
}

func cvttRoundsdSi32(a [2]float64, rounding int) int


// CvttRoundsdSi64: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to a 64-bit integer with truncation, and store the result in
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_FP64_To_Int64_Truncate(a[63:0])
//
// Instruction: 'VCVTTSD2SI'. Intrinsic: '_mm_cvtt_roundsd_si64'.
// Requires AVX512F.
func CvttRoundsdSi64(a M128d, rounding int) int64 {
	return int64(cvttRoundsdSi64([2]float64(a), rounding))
}

func cvttRoundsdSi64(a [2]float64, rounding int) int64


// CvttRoundsdU32: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to an unsigned 32-bit integer with truncation, and store the
// result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_FP64_To_UnsignedInt32_Truncate(a[63:0])
//
// Instruction: 'VCVTTSD2USI'. Intrinsic: '_mm_cvtt_roundsd_u32'.
// Requires AVX512F.
func CvttRoundsdU32(a M128d, rounding int) uint32 {
	return uint32(cvttRoundsdU32([2]float64(a), rounding))
}

func cvttRoundsdU32(a [2]float64, rounding int) uint32


// CvttRoundsdU64: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to an unsigned 64-bit integer with truncation, and store the
// result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_FP64_To_UnsignedInt64_Truncate(a[63:0])
//
// Instruction: 'VCVTTSD2USI'. Intrinsic: '_mm_cvtt_roundsd_u64'.
// Requires AVX512F.
func CvttRoundsdU64(a M128d, rounding int) uint64 {
	return uint64(cvttRoundsdU64([2]float64(a), rounding))
}

func cvttRoundsdU64(a [2]float64, rounding int) uint64


// CvttRoundssI32: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to a 32-bit integer with truncation, and store the result in
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_FP32_To_Int32_Truncate(a[31:0])
//
// Instruction: 'VCVTTSS2SI'. Intrinsic: '_mm_cvtt_roundss_i32'.
// Requires AVX512F.
func CvttRoundssI32(a M128, rounding int) int {
	return int(cvttRoundssI32([4]float32(a), rounding))
}

func cvttRoundssI32(a [4]float32, rounding int) int


// CvttRoundssI64: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to a 64-bit integer with truncation, and store the result in
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_FP32_To_Int64_Truncate(a[31:0])
//
// Instruction: 'VCVTTSS2SI'. Intrinsic: '_mm_cvtt_roundss_i64'.
// Requires AVX512F.
func CvttRoundssI64(a M128, rounding int) int64 {
	return int64(cvttRoundssI64([4]float32(a), rounding))
}

func cvttRoundssI64(a [4]float32, rounding int) int64


// CvttRoundssSi32: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to a 32-bit integer with truncation, and store the result in
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_FP32_To_Int32_Truncate(a[31:0])
//
// Instruction: 'VCVTTSS2SI'. Intrinsic: '_mm_cvtt_roundss_si32'.
// Requires AVX512F.
func CvttRoundssSi32(a M128, rounding int) int {
	return int(cvttRoundssSi32([4]float32(a), rounding))
}

func cvttRoundssSi32(a [4]float32, rounding int) int


// CvttRoundssSi64: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to a 64-bit integer with truncation, and store the result in
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_FP32_To_Int64_Truncate(a[31:0])
//
// Instruction: 'VCVTTSS2SI'. Intrinsic: '_mm_cvtt_roundss_si64'.
// Requires AVX512F.
func CvttRoundssSi64(a M128, rounding int) int64 {
	return int64(cvttRoundssSi64([4]float32(a), rounding))
}

func cvttRoundssSi64(a [4]float32, rounding int) int64


// CvttRoundssU32: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to an unsigned 32-bit integer with truncation, and store the
// result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_FP32_To_UnsignedInt32_Truncate(a[31:0])
//
// Instruction: 'VCVTTSS2USI'. Intrinsic: '_mm_cvtt_roundss_u32'.
// Requires AVX512F.
func CvttRoundssU32(a M128, rounding int) uint32 {
	return uint32(cvttRoundssU32([4]float32(a), rounding))
}

func cvttRoundssU32(a [4]float32, rounding int) uint32


// CvttRoundssU64: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to an unsigned 64-bit integer with truncation, and store the
// result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_FP32_To_UnsignedInt64_Truncate(a[31:0])
//
// Instruction: 'VCVTTSS2USI'. Intrinsic: '_mm_cvtt_roundss_u64'.
// Requires AVX512F.
func CvttRoundssU64(a M128, rounding int) uint64 {
	return uint64(cvttRoundssU64([4]float32(a), rounding))
}

func cvttRoundssU64(a [4]float32, rounding int) uint64


// MaskCvttpdEpi32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_Int32_Truncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTTPD2DQ'. Intrinsic: '_mm_mask_cvttpd_epi32'.
// Requires AVX512F.
func MaskCvttpdEpi32(src M128i, k Mmask8, a M128d) M128i {
	return M128i(maskCvttpdEpi32([16]byte(src), uint8(k), [2]float64(a)))
}

func maskCvttpdEpi32(src [16]byte, k uint8, a [2]float64) [16]byte


// MaskzCvttpdEpi32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_Int32_Truncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTTPD2DQ'. Intrinsic: '_mm_maskz_cvttpd_epi32'.
// Requires AVX512F.
func MaskzCvttpdEpi32(k Mmask8, a M128d) M128i {
	return M128i(maskzCvttpdEpi32(uint8(k), [2]float64(a)))
}

func maskzCvttpdEpi32(k uint8, a [2]float64) [16]byte


// MaskCvttpdEpi321: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_Int32_Truncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPD2DQ'. Intrinsic: '_mm256_mask_cvttpd_epi32'.
// Requires AVX512F.
func MaskCvttpdEpi321(src M128i, k Mmask8, a M256d) M128i {
	return M128i(maskCvttpdEpi321([16]byte(src), uint8(k), [4]float64(a)))
}

func maskCvttpdEpi321(src [16]byte, k uint8, a [4]float64) [16]byte


// MaskzCvttpdEpi321: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_Int32_Truncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPD2DQ'. Intrinsic: '_mm256_maskz_cvttpd_epi32'.
// Requires AVX512F.
func MaskzCvttpdEpi321(k Mmask8, a M256d) M128i {
	return M128i(maskzCvttpdEpi321(uint8(k), [4]float64(a)))
}

func maskzCvttpdEpi321(k uint8, a [4]float64) [16]byte


// CvttpdEpi32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_Int32_Truncate(a[k+63:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2DQ'. Intrinsic: '_mm512_cvttpd_epi32'.
// Requires AVX512F.
func CvttpdEpi32(a M512d) M256i {
	return M256i(cvttpdEpi32([8]float64(a)))
}

func cvttpdEpi32(a [8]float64) [32]byte


// MaskCvttpdEpi322: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_Int32_Truncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2DQ'. Intrinsic: '_mm512_mask_cvttpd_epi32'.
// Requires AVX512F.
func MaskCvttpdEpi322(src M256i, k Mmask8, a M512d) M256i {
	return M256i(maskCvttpdEpi322([32]byte(src), uint8(k), [8]float64(a)))
}

func maskCvttpdEpi322(src [32]byte, k uint8, a [8]float64) [32]byte


// MaskzCvttpdEpi322: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_Int32_Truncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2DQ'. Intrinsic: '_mm512_maskz_cvttpd_epi32'.
// Requires AVX512F.
func MaskzCvttpdEpi322(k Mmask8, a M512d) M256i {
	return M256i(maskzCvttpdEpi322(uint8(k), [8]float64(a)))
}

func maskzCvttpdEpi322(k uint8, a [8]float64) [32]byte


// CvttpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[k+63:k])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTTPD2UDQ'. Intrinsic: '_mm_cvttpd_epu32'.
// Requires AVX512F.
func CvttpdEpu32(a M128d) M128i {
	return M128i(cvttpdEpu32([2]float64(a)))
}

func cvttpdEpu32(a [2]float64) [16]byte


// MaskCvttpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTTPD2UDQ'. Intrinsic: '_mm_mask_cvttpd_epu32'.
// Requires AVX512F.
func MaskCvttpdEpu32(src M128i, k Mmask8, a M128d) M128i {
	return M128i(maskCvttpdEpu32([16]byte(src), uint8(k), [2]float64(a)))
}

func maskCvttpdEpu32(src [16]byte, k uint8, a [2]float64) [16]byte


// MaskzCvttpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTTPD2UDQ'. Intrinsic: '_mm_maskz_cvttpd_epu32'.
// Requires AVX512F.
func MaskzCvttpdEpu32(k Mmask8, a M128d) M128i {
	return M128i(maskzCvttpdEpu32(uint8(k), [2]float64(a)))
}

func maskzCvttpdEpu32(k uint8, a [2]float64) [16]byte


// CvttpdEpu321: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[k+63:k])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPD2UDQ'. Intrinsic: '_mm256_cvttpd_epu32'.
// Requires AVX512F.
func CvttpdEpu321(a M256d) M128i {
	return M128i(cvttpdEpu321([4]float64(a)))
}

func cvttpdEpu321(a [4]float64) [16]byte


// MaskCvttpdEpu321: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPD2UDQ'. Intrinsic: '_mm256_mask_cvttpd_epu32'.
// Requires AVX512F.
func MaskCvttpdEpu321(src M128i, k Mmask8, a M256d) M128i {
	return M128i(maskCvttpdEpu321([16]byte(src), uint8(k), [4]float64(a)))
}

func maskCvttpdEpu321(src [16]byte, k uint8, a [4]float64) [16]byte


// MaskzCvttpdEpu321: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPD2UDQ'. Intrinsic: '_mm256_maskz_cvttpd_epu32'.
// Requires AVX512F.
func MaskzCvttpdEpu321(k Mmask8, a M256d) M128i {
	return M128i(maskzCvttpdEpu321(uint8(k), [4]float64(a)))
}

func maskzCvttpdEpu321(k uint8, a [4]float64) [16]byte


// CvttpdEpu322: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[k+63:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2UDQ'. Intrinsic: '_mm512_cvttpd_epu32'.
// Requires AVX512F.
func CvttpdEpu322(a M512d) M256i {
	return M256i(cvttpdEpu322([8]float64(a)))
}

func cvttpdEpu322(a [8]float64) [32]byte


// MaskCvttpdEpu322: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2UDQ'. Intrinsic: '_mm512_mask_cvttpd_epu32'.
// Requires AVX512F.
func MaskCvttpdEpu322(src M256i, k Mmask8, a M512d) M256i {
	return M256i(maskCvttpdEpu322([32]byte(src), uint8(k), [8]float64(a)))
}

func maskCvttpdEpu322(src [32]byte, k uint8, a [8]float64) [32]byte


// MaskzCvttpdEpu322: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2UDQ'. Intrinsic: '_mm512_maskz_cvttpd_epu32'.
// Requires AVX512F.
func MaskzCvttpdEpu322(k Mmask8, a M512d) M256i {
	return M256i(maskzCvttpdEpu322(uint8(k), [8]float64(a)))
}

func maskzCvttpdEpu322(k uint8, a [8]float64) [32]byte


// MaskCvttpsEpi32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_Int32_Truncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPS2DQ'. Intrinsic: '_mm_mask_cvttps_epi32'.
// Requires AVX512F.
func MaskCvttpsEpi32(src M128i, k Mmask8, a M128) M128i {
	return M128i(maskCvttpsEpi32([16]byte(src), uint8(k), [4]float32(a)))
}

func maskCvttpsEpi32(src [16]byte, k uint8, a [4]float32) [16]byte


// MaskzCvttpsEpi32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*i
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_IntegerTruncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPS2DQ'. Intrinsic: '_mm_maskz_cvttps_epi32'.
// Requires AVX512F.
func MaskzCvttpsEpi32(k Mmask8, a M128) M128i {
	return M128i(maskzCvttpsEpi32(uint8(k), [4]float32(a)))
}

func maskzCvttpsEpi32(k uint8, a [4]float32) [16]byte


// MaskCvttpsEpi321: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_Int32_Truncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPS2DQ'. Intrinsic: '_mm256_mask_cvttps_epi32'.
// Requires AVX512F.
func MaskCvttpsEpi321(src M256i, k Mmask8, a M256) M256i {
	return M256i(maskCvttpsEpi321([32]byte(src), uint8(k), [8]float32(a)))
}

func maskCvttpsEpi321(src [32]byte, k uint8, a [8]float32) [32]byte


// MaskzCvttpsEpi321: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*i
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_IntegerTruncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPS2DQ'. Intrinsic: '_mm256_maskz_cvttps_epi32'.
// Requires AVX512F.
func MaskzCvttpsEpi321(k Mmask8, a M256) M256i {
	return M256i(maskzCvttpsEpi321(uint8(k), [8]float32(a)))
}

func maskzCvttpsEpi321(k uint8, a [8]float32) [32]byte


// CvttpsEpi32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := Convert_FP32_To_Int32_Truncate(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2DQ'. Intrinsic: '_mm512_cvttps_epi32'.
// Requires AVX512F.
func CvttpsEpi32(a M512) M512i {
	return M512i(cvttpsEpi32([16]float32(a)))
}

func cvttpsEpi32(a [16]float32) [64]byte


// MaskCvttpsEpi322: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_Int32_Truncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2DQ'. Intrinsic: '_mm512_mask_cvttps_epi32'.
// Requires AVX512F.
func MaskCvttpsEpi322(src M512i, k Mmask16, a M512) M512i {
	return M512i(maskCvttpsEpi322([64]byte(src), uint16(k), [16]float32(a)))
}

func maskCvttpsEpi322(src [64]byte, k uint16, a [16]float32) [64]byte


// MaskzCvttpsEpi322: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_Int32_Truncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2DQ'. Intrinsic: '_mm512_maskz_cvttps_epi32'.
// Requires AVX512F.
func MaskzCvttpsEpi322(k Mmask16, a M512) M512i {
	return M512i(maskzCvttpsEpi322(uint16(k), [16]float32(a)))
}

func maskzCvttpsEpi322(k uint16, a [16]float32) [64]byte


// CvttpsEpu32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			dst[i+31:i] := Convert_FP32_To_UnsignedInt32_Truncate(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPS2UDQ'. Intrinsic: '_mm_cvttps_epu32'.
// Requires AVX512F.
func CvttpsEpu32(a M128) M128i {
	return M128i(cvttpsEpu32([4]float32(a)))
}

func cvttpsEpu32(a [4]float32) [16]byte


// MaskCvttpsEpu32: Convert packed double-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPS2UDQ'. Intrinsic: '_mm_mask_cvttps_epu32'.
// Requires AVX512F.
func MaskCvttpsEpu32(src M128i, k Mmask8, a M128) M128i {
	return M128i(maskCvttpsEpu32([16]byte(src), uint8(k), [4]float32(a)))
}

func maskCvttpsEpu32(src [16]byte, k uint8, a [4]float32) [16]byte


// MaskzCvttpsEpu32: Convert packed double-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPS2UDQ'. Intrinsic: '_mm_maskz_cvttps_epu32'.
// Requires AVX512F.
func MaskzCvttpsEpu32(k Mmask8, a M128) M128i {
	return M128i(maskzCvttpsEpu32(uint8(k), [4]float32(a)))
}

func maskzCvttpsEpu32(k uint8, a [4]float32) [16]byte


// CvttpsEpu321: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			dst[i+31:i] := Convert_FP32_To_UnsignedInt32_Truncate(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPS2UDQ'. Intrinsic: '_mm256_cvttps_epu32'.
// Requires AVX512F.
func CvttpsEpu321(a M256) M256i {
	return M256i(cvttpsEpu321([8]float32(a)))
}

func cvttpsEpu321(a [8]float32) [32]byte


// MaskCvttpsEpu321: Convert packed double-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPS2UDQ'. Intrinsic: '_mm256_mask_cvttps_epu32'.
// Requires AVX512F.
func MaskCvttpsEpu321(src M256i, k Mmask8, a M256) M256i {
	return M256i(maskCvttpsEpu321([32]byte(src), uint8(k), [8]float32(a)))
}

func maskCvttpsEpu321(src [32]byte, k uint8, a [8]float32) [32]byte


// MaskzCvttpsEpu321: Convert packed double-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPS2UDQ'. Intrinsic: '_mm256_maskz_cvttps_epu32'.
// Requires AVX512F.
func MaskzCvttpsEpu321(k Mmask8, a M256) M256i {
	return M256i(maskzCvttpsEpu321(uint8(k), [8]float32(a)))
}

func maskzCvttpsEpu321(k uint8, a [8]float32) [32]byte


// CvttpsEpu322: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := Convert_FP32_To_UnsignedInt32_Truncate(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2UDQ'. Intrinsic: '_mm512_cvttps_epu32'.
// Requires AVX512F.
func CvttpsEpu322(a M512) M512i {
	return M512i(cvttpsEpu322([16]float32(a)))
}

func cvttpsEpu322(a [16]float32) [64]byte


// MaskCvttpsEpu322: Convert packed double-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2UDQ'. Intrinsic: '_mm512_mask_cvttps_epu32'.
// Requires AVX512F.
func MaskCvttpsEpu322(src M512i, k Mmask16, a M512) M512i {
	return M512i(maskCvttpsEpu322([64]byte(src), uint16(k), [16]float32(a)))
}

func maskCvttpsEpu322(src [64]byte, k uint16, a [16]float32) [64]byte


// MaskzCvttpsEpu322: Convert packed double-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VCVTTPS2UDQ'. Intrinsic: '_mm512_maskz_cvttps_epu32'.
// Requires AVX512F.
func MaskzCvttpsEpu322(k Mmask16, a M512) M512i {
	return M512i(maskzCvttpsEpu322(uint16(k), [16]float32(a)))
}

func maskzCvttpsEpu322(k uint16, a [16]float32) [64]byte


// CvttsdI32: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to a 32-bit integer with truncation, and store the result in
// 'dst'. 
//
//		dst[31:0] := Convert_FP64_To_Int32_Truncate(a[63:0])
//
// Instruction: 'VCVTTSD2SI'. Intrinsic: '_mm_cvttsd_i32'.
// Requires AVX512F.
func CvttsdI32(a M128d) int {
	return int(cvttsdI32([2]float64(a)))
}

func cvttsdI32(a [2]float64) int


// CvttsdI64: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to a 64-bit integer with truncation, and store the result in
// 'dst'. 
//
//		dst[63:0] := Convert_FP64_To_Int64_Truncate(a[63:0])
//
// Instruction: 'VCVTTSD2SI'. Intrinsic: '_mm_cvttsd_i64'.
// Requires AVX512F.
func CvttsdI64(a M128d) int64 {
	return int64(cvttsdI64([2]float64(a)))
}

func cvttsdI64(a [2]float64) int64


// CvttsdU32: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to an unsigned 32-bit integer with truncation, and store the
// result in 'dst'. 
//
//		dst[31:0] := Convert_FP64_To_UnsignedInt32_Truncate(a[63:0])
//
// Instruction: 'VCVTTSD2USI'. Intrinsic: '_mm_cvttsd_u32'.
// Requires AVX512F.
func CvttsdU32(a M128d) uint32 {
	return uint32(cvttsdU32([2]float64(a)))
}

func cvttsdU32(a [2]float64) uint32


// CvttsdU64: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to an unsigned 64-bit integer with truncation, and store the
// result in 'dst'. 
//
//		dst[63:0] := Convert_FP64_To_UnsignedInt64_Truncate(a[63:0])
//
// Instruction: 'VCVTTSD2USI'. Intrinsic: '_mm_cvttsd_u64'.
// Requires AVX512F.
func CvttsdU64(a M128d) uint64 {
	return uint64(cvttsdU64([2]float64(a)))
}

func cvttsdU64(a [2]float64) uint64


// CvttssI32: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to a 32-bit integer with truncation, and store the result in
// 'dst'. 
//
//		dst[31:0] := Convert_FP32_To_Int32_Truncate(a[31:0])
//
// Instruction: 'VCVTTSS2SI'. Intrinsic: '_mm_cvttss_i32'.
// Requires AVX512F.
func CvttssI32(a M128) int {
	return int(cvttssI32([4]float32(a)))
}

func cvttssI32(a [4]float32) int


// CvttssI64: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to a 64-bit integer with truncation, and store the result in
// 'dst'. 
//
//		dst[63:0] := Convert_FP32_To_Int64_Truncate(a[31:0])
//
// Instruction: 'VCVTTSS2SI'. Intrinsic: '_mm_cvttss_i64'.
// Requires AVX512F.
func CvttssI64(a M128) int64 {
	return int64(cvttssI64([4]float32(a)))
}

func cvttssI64(a [4]float32) int64


// CvttssU32: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to an unsigned 32-bit integer with truncation, and store the
// result in 'dst'. 
//
//		dst[31:0] := Convert_FP32_To_UnsignedInt32_Truncate(a[31:0])
//
// Instruction: 'VCVTTSS2USI'. Intrinsic: '_mm_cvttss_u32'.
// Requires AVX512F.
func CvttssU32(a M128) uint32 {
	return uint32(cvttssU32([4]float32(a)))
}

func cvttssU32(a [4]float32) uint32


// CvttssU64: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to an unsigned 64-bit integer with truncation, and store the
// result in 'dst'. 
//
//		dst[63:0] := Convert_FP32_To_UnsignedInt64_Truncate(a[31:0])
//
// Instruction: 'VCVTTSS2USI'. Intrinsic: '_mm_cvttss_u64'.
// Requires AVX512F.
func CvttssU64(a M128) uint64 {
	return uint64(cvttssU64([4]float32(a)))
}

func cvttssU64(a [4]float32) uint64


// Cvtu32Sd: Convert the unsigned 32-bit integer 'b' to a double-precision
// (64-bit) floating-point element, store the result in the lower element of
// 'dst', and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		dst[63:0] := Convert_UnsignedInt32_To_FP64(b[31:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTUSI2SD'. Intrinsic: '_mm_cvtu32_sd'.
// Requires AVX512F.
func Cvtu32Sd(a M128d, b uint32) M128d {
	return M128d(cvtu32Sd([2]float64(a), b))
}

func cvtu32Sd(a [2]float64, b uint32) [2]float64


// Cvtu32Ss: Convert the unsigned 32-bit integer 'b' to a single-precision
// (32-bit) floating-point element, store the result in the lower element of
// 'dst', and copy the upper 3 packed elements from 'a' to the upper elements
// of 'dst'. 
//
//		dst[31:0] := Convert_UnsignedInt32_To_FP32(b[31:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTUSI2SS'. Intrinsic: '_mm_cvtu32_ss'.
// Requires AVX512F.
func Cvtu32Ss(a M128, b uint32) M128 {
	return M128(cvtu32Ss([4]float32(a), b))
}

func cvtu32Ss(a [4]float32, b uint32) [4]float32


// Cvtu64Sd: Convert the unsigned 64-bit integer 'b' to a double-precision
// (64-bit) floating-point element, store the result in the lower element of
// 'dst', and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		dst[63:0] := Convert_UnsignedInt64_To_FP64(b[63:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTUSI2SD'. Intrinsic: '_mm_cvtu64_sd'.
// Requires AVX512F.
func Cvtu64Sd(a M128d, b uint64) M128d {
	return M128d(cvtu64Sd([2]float64(a), b))
}

func cvtu64Sd(a [2]float64, b uint64) [2]float64


// Cvtu64Ss: Convert the unsigned 64-bit integer 'b' to a single-precision
// (32-bit) floating-point element, store the result in the lower element of
// 'dst', and copy the upper 3 packed elements from 'a' to the upper elements
// of 'dst'. 
//
//		dst[31:0] := Convert_UnsignedInt64_To_FP32(b[63:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTUSI2SS'. Intrinsic: '_mm_cvtu64_ss'.
// Requires AVX512F.
func Cvtu64Ss(a M128, b uint64) M128 {
	return M128(cvtu64Ss([4]float32(a), b))
}

func cvtu64Ss(a [4]float32, b uint64) [4]float32


// Cvtusepi32Epi16: Convert packed unsigned 32-bit integers in 'a' to packed
// unsigned 16-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			k := 16*j
//			dst[k+15:k] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSDW'. Intrinsic: '_mm_cvtusepi32_epi16'.
// Requires AVX512F.
func Cvtusepi32Epi16(a M128i) M128i {
	return M128i(cvtusepi32Epi16([16]byte(a)))
}

func cvtusepi32Epi16(a [16]byte) [16]byte


// MaskCvtusepi32Epi16: Convert packed unsigned 32-bit integers in 'a' to
// packed unsigned 16-bit integers with unsigned saturation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSDW'. Intrinsic: '_mm_mask_cvtusepi32_epi16'.
// Requires AVX512F.
func MaskCvtusepi32Epi16(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtusepi32Epi16([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtusepi32Epi16(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtusepi32Epi16: Convert packed unsigned 32-bit integers in 'a' to
// packed unsigned 16-bit integers with unsigned saturation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSDW'. Intrinsic: '_mm_maskz_cvtusepi32_epi16'.
// Requires AVX512F.
func MaskzCvtusepi32Epi16(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtusepi32Epi16(uint8(k), [16]byte(a)))
}

func maskzCvtusepi32Epi16(k uint8, a [16]byte) [16]byte


// Cvtusepi32Epi161: Convert packed unsigned 32-bit integers in 'a' to packed
// unsigned 16-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 16*j
//			dst[k+15:k] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSDW'. Intrinsic: '_mm256_cvtusepi32_epi16'.
// Requires AVX512F.
func Cvtusepi32Epi161(a M256i) M128i {
	return M128i(cvtusepi32Epi161([32]byte(a)))
}

func cvtusepi32Epi161(a [32]byte) [16]byte


// MaskCvtusepi32Epi161: Convert packed unsigned 32-bit integers in 'a' to
// packed unsigned 16-bit integers with unsigned saturation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSDW'. Intrinsic: '_mm256_mask_cvtusepi32_epi16'.
// Requires AVX512F.
func MaskCvtusepi32Epi161(src M128i, k Mmask8, a M256i) M128i {
	return M128i(maskCvtusepi32Epi161([16]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtusepi32Epi161(src [16]byte, k uint8, a [32]byte) [16]byte


// MaskzCvtusepi32Epi161: Convert packed unsigned 32-bit integers in 'a' to
// packed unsigned 16-bit integers with unsigned saturation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSDW'. Intrinsic: '_mm256_maskz_cvtusepi32_epi16'.
// Requires AVX512F.
func MaskzCvtusepi32Epi161(k Mmask8, a M256i) M128i {
	return M128i(maskzCvtusepi32Epi161(uint8(k), [32]byte(a)))
}

func maskzCvtusepi32Epi161(k uint8, a [32]byte) [16]byte


// Cvtusepi32Epi162: Convert packed unsigned 32-bit integers in 'a' to packed
// unsigned 16-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			k := 16*j
//			dst[k+15:k] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVUSDW'. Intrinsic: '_mm512_cvtusepi32_epi16'.
// Requires AVX512F.
func Cvtusepi32Epi162(a M512i) M256i {
	return M256i(cvtusepi32Epi162([64]byte(a)))
}

func cvtusepi32Epi162(a [64]byte) [32]byte


// MaskCvtusepi32Epi162: Convert packed unsigned 32-bit integers in 'a' to
// packed unsigned 16-bit integers with unsigned saturation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVUSDW'. Intrinsic: '_mm512_mask_cvtusepi32_epi16'.
// Requires AVX512F.
func MaskCvtusepi32Epi162(src M256i, k Mmask16, a M512i) M256i {
	return M256i(maskCvtusepi32Epi162([32]byte(src), uint16(k), [64]byte(a)))
}

func maskCvtusepi32Epi162(src [32]byte, k uint16, a [64]byte) [32]byte


// MaskzCvtusepi32Epi162: Convert packed unsigned 32-bit integers in 'a' to
// packed unsigned 16-bit integers with unsigned saturation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVUSDW'. Intrinsic: '_mm512_maskz_cvtusepi32_epi16'.
// Requires AVX512F.
func MaskzCvtusepi32Epi162(k Mmask16, a M512i) M256i {
	return M256i(maskzCvtusepi32Epi162(uint16(k), [64]byte(a)))
}

func maskzCvtusepi32Epi162(k uint16, a [64]byte) [32]byte


// Cvtusepi32Epi8: Convert packed unsigned 32-bit integers in 'a' to packed
// unsigned 8-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			k := 8*j
//			dst[k+7:k] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVUSDB'. Intrinsic: '_mm_cvtusepi32_epi8'.
// Requires AVX512F.
func Cvtusepi32Epi8(a M128i) M128i {
	return M128i(cvtusepi32Epi8([16]byte(a)))
}

func cvtusepi32Epi8(a [16]byte) [16]byte


// MaskCvtusepi32Epi8: Convert packed unsigned 32-bit integers in 'a' to packed
// unsigned 8-bit integers with unsigned saturation, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVUSDB'. Intrinsic: '_mm_mask_cvtusepi32_epi8'.
// Requires AVX512F.
func MaskCvtusepi32Epi8(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtusepi32Epi8([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtusepi32Epi8(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtusepi32Epi8: Convert packed unsigned 32-bit integers in 'a' to
// packed unsigned 8-bit integers with unsigned saturation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVUSDB'. Intrinsic: '_mm_maskz_cvtusepi32_epi8'.
// Requires AVX512F.
func MaskzCvtusepi32Epi8(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtusepi32Epi8(uint8(k), [16]byte(a)))
}

func maskzCvtusepi32Epi8(k uint8, a [16]byte) [16]byte


// Cvtusepi32Epi81: Convert packed unsigned 32-bit integers in 'a' to packed
// unsigned 8-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 8*j
//			dst[k+7:k] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSDB'. Intrinsic: '_mm256_cvtusepi32_epi8'.
// Requires AVX512F.
func Cvtusepi32Epi81(a M256i) M128i {
	return M128i(cvtusepi32Epi81([32]byte(a)))
}

func cvtusepi32Epi81(a [32]byte) [16]byte


// MaskCvtusepi32Epi81: Convert packed unsigned 32-bit integers in 'a' to
// packed unsigned 8-bit integers with unsigned saturation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSDB'. Intrinsic: '_mm256_mask_cvtusepi32_epi8'.
// Requires AVX512F.
func MaskCvtusepi32Epi81(src M128i, k Mmask8, a M256i) M128i {
	return M128i(maskCvtusepi32Epi81([16]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtusepi32Epi81(src [16]byte, k uint8, a [32]byte) [16]byte


// MaskzCvtusepi32Epi81: Convert packed unsigned 32-bit integers in 'a' to
// packed unsigned 8-bit integers with unsigned saturation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSDB'. Intrinsic: '_mm256_maskz_cvtusepi32_epi8'.
// Requires AVX512F.
func MaskzCvtusepi32Epi81(k Mmask8, a M256i) M128i {
	return M128i(maskzCvtusepi32Epi81(uint8(k), [32]byte(a)))
}

func maskzCvtusepi32Epi81(k uint8, a [32]byte) [16]byte


// Cvtusepi32Epi82: Convert packed unsigned 32-bit integers in 'a' to packed
// unsigned 8-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			k := 8*j
//			dst[k+7:k] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSDB'. Intrinsic: '_mm512_cvtusepi32_epi8'.
// Requires AVX512F.
func Cvtusepi32Epi82(a M512i) M128i {
	return M128i(cvtusepi32Epi82([64]byte(a)))
}

func cvtusepi32Epi82(a [64]byte) [16]byte


// MaskCvtusepi32Epi82: Convert packed unsigned 32-bit integers in 'a' to
// packed unsigned 8-bit integers with unsigned saturation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSDB'. Intrinsic: '_mm512_mask_cvtusepi32_epi8'.
// Requires AVX512F.
func MaskCvtusepi32Epi82(src M128i, k Mmask16, a M512i) M128i {
	return M128i(maskCvtusepi32Epi82([16]byte(src), uint16(k), [64]byte(a)))
}

func maskCvtusepi32Epi82(src [16]byte, k uint16, a [64]byte) [16]byte


// MaskzCvtusepi32Epi82: Convert packed unsigned 32-bit integers in 'a' to
// packed unsigned 8-bit integers with unsigned saturation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSDB'. Intrinsic: '_mm512_maskz_cvtusepi32_epi8'.
// Requires AVX512F.
func MaskzCvtusepi32Epi82(k Mmask16, a M512i) M128i {
	return M128i(maskzCvtusepi32Epi82(uint16(k), [64]byte(a)))
}

func maskzCvtusepi32Epi82(k uint16, a [64]byte) [16]byte


// MaskCvtusepi32StoreuEpi16: Convert packed unsigned 32-bit integers in 'a' to
// packed unsigned 16-bit integers with unsigned saturation, and store the
// active results (those with their respective bit set in writemask 'k') to
// unaligned memory at 'base_addr'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				MEM[base_addr+l+15:base_addr+l] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSDW'. Intrinsic: '_mm_mask_cvtusepi32_storeu_epi16'.
// Requires AVX512F.
func MaskCvtusepi32StoreuEpi16(base_addr uintptr, k Mmask8, a M128i)  {
	maskCvtusepi32StoreuEpi16(uintptr(base_addr), uint8(k), [16]byte(a))
}

func maskCvtusepi32StoreuEpi16(base_addr uintptr, k uint8, a [16]byte) 


// MaskCvtusepi32StoreuEpi161: Convert packed unsigned 32-bit integers in 'a'
// to packed unsigned 16-bit integers with unsigned saturation, and store the
// active results (those with their respective bit set in writemask 'k') to
// unaligned memory at 'base_addr'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				MEM[base_addr+l+15:base_addr+l] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSDW'. Intrinsic: '_mm256_mask_cvtusepi32_storeu_epi16'.
// Requires AVX512F.
func MaskCvtusepi32StoreuEpi161(base_addr uintptr, k Mmask8, a M256i)  {
	maskCvtusepi32StoreuEpi161(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCvtusepi32StoreuEpi161(base_addr uintptr, k uint8, a [32]byte) 


// MaskCvtusepi32StoreuEpi162: Convert packed unsigned 32-bit integers in 'a'
// to packed 16-bit integers with unsigned saturation, and store the active
// results (those with their respective bit set in writemask 'k') to unaligned
// memory at 'base_addr'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				MEM[base_addr+l+15:base_addr+l] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
//			FI
//		ENDFOR
//
// Instruction: 'VPMOVUSDW'. Intrinsic: '_mm512_mask_cvtusepi32_storeu_epi16'.
// Requires AVX512F.
func MaskCvtusepi32StoreuEpi162(base_addr uintptr, k Mmask16, a M512i)  {
	maskCvtusepi32StoreuEpi162(uintptr(base_addr), uint16(k), [64]byte(a))
}

func maskCvtusepi32StoreuEpi162(base_addr uintptr, k uint16, a [64]byte) 


// MaskCvtusepi32StoreuEpi8: Convert packed unsigned 32-bit integers in 'a' to
// packed unsigned 8-bit integers with unsigned saturation, and store the
// active results (those with their respective bit set in writemask 'k') to
// unaligned memory at 'base_addr'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVUSDB'. Intrinsic: '_mm_mask_cvtusepi32_storeu_epi8'.
// Requires AVX512F.
func MaskCvtusepi32StoreuEpi8(base_addr uintptr, k Mmask8, a M128i)  {
	maskCvtusepi32StoreuEpi8(uintptr(base_addr), uint8(k), [16]byte(a))
}

func maskCvtusepi32StoreuEpi8(base_addr uintptr, k uint8, a [16]byte) 


// MaskCvtusepi32StoreuEpi81: Convert packed unsigned 32-bit integers in 'a' to
// packed unsigned 8-bit integers with unsigned saturation, and store the
// active results (those with their respective bit set in writemask 'k') to
// unaligned memory at 'base_addr'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSDB'. Intrinsic: '_mm256_mask_cvtusepi32_storeu_epi8'.
// Requires AVX512F.
func MaskCvtusepi32StoreuEpi81(base_addr uintptr, k Mmask8, a M256i)  {
	maskCvtusepi32StoreuEpi81(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCvtusepi32StoreuEpi81(base_addr uintptr, k uint8, a [32]byte) 


// MaskCvtusepi32StoreuEpi82: Convert packed unsigned 32-bit integers in 'a' to
// packed 8-bit integers with unsigned saturation, and store the active results
// (those with their respective bit set in writemask 'k') to unaligned memory
// at 'base_addr'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
//			FI
//		ENDFOR
//
// Instruction: 'VPMOVUSDB'. Intrinsic: '_mm512_mask_cvtusepi32_storeu_epi8'.
// Requires AVX512F.
func MaskCvtusepi32StoreuEpi82(base_addr uintptr, k Mmask16, a M512i)  {
	maskCvtusepi32StoreuEpi82(uintptr(base_addr), uint16(k), [64]byte(a))
}

func maskCvtusepi32StoreuEpi82(base_addr uintptr, k uint16, a [64]byte) 


// Cvtusepi64Epi16: Convert packed unsigned 64-bit integers in 'a' to packed
// unsigned 16-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			k := 16*j
//			dst[k+15:k] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVUSQW'. Intrinsic: '_mm_cvtusepi64_epi16'.
// Requires AVX512F.
func Cvtusepi64Epi16(a M128i) M128i {
	return M128i(cvtusepi64Epi16([16]byte(a)))
}

func cvtusepi64Epi16(a [16]byte) [16]byte


// MaskCvtusepi64Epi16: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 16-bit integers with unsigned saturation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVUSQW'. Intrinsic: '_mm_mask_cvtusepi64_epi16'.
// Requires AVX512F.
func MaskCvtusepi64Epi16(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtusepi64Epi16([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtusepi64Epi16(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtusepi64Epi16: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 16-bit integers with unsigned saturation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVUSQW'. Intrinsic: '_mm_maskz_cvtusepi64_epi16'.
// Requires AVX512F.
func MaskzCvtusepi64Epi16(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtusepi64Epi16(uint8(k), [16]byte(a)))
}

func maskzCvtusepi64Epi16(k uint8, a [16]byte) [16]byte


// Cvtusepi64Epi161: Convert packed unsigned 64-bit integers in 'a' to packed
// unsigned 16-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			k := 16*j
//			dst[k+15:k] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSQW'. Intrinsic: '_mm256_cvtusepi64_epi16'.
// Requires AVX512F.
func Cvtusepi64Epi161(a M256i) M128i {
	return M128i(cvtusepi64Epi161([32]byte(a)))
}

func cvtusepi64Epi161(a [32]byte) [16]byte


// MaskCvtusepi64Epi161: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 16-bit integers with unsigned saturation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSQW'. Intrinsic: '_mm256_mask_cvtusepi64_epi16'.
// Requires AVX512F.
func MaskCvtusepi64Epi161(src M128i, k Mmask8, a M256i) M128i {
	return M128i(maskCvtusepi64Epi161([16]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtusepi64Epi161(src [16]byte, k uint8, a [32]byte) [16]byte


// MaskzCvtusepi64Epi161: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 16-bit integers with unsigned saturation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSQW'. Intrinsic: '_mm256_maskz_cvtusepi64_epi16'.
// Requires AVX512F.
func MaskzCvtusepi64Epi161(k Mmask8, a M256i) M128i {
	return M128i(maskzCvtusepi64Epi161(uint8(k), [32]byte(a)))
}

func maskzCvtusepi64Epi161(k uint8, a [32]byte) [16]byte


// Cvtusepi64Epi162: Convert packed unsigned 64-bit integers in 'a' to packed
// unsigned 16-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 16*j
//			dst[k+15:k] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSQW'. Intrinsic: '_mm512_cvtusepi64_epi16'.
// Requires AVX512F.
func Cvtusepi64Epi162(a M512i) M128i {
	return M128i(cvtusepi64Epi162([64]byte(a)))
}

func cvtusepi64Epi162(a [64]byte) [16]byte


// MaskCvtusepi64Epi162: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 16-bit integers with unsigned saturation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSQW'. Intrinsic: '_mm512_mask_cvtusepi64_epi16'.
// Requires AVX512F.
func MaskCvtusepi64Epi162(src M128i, k Mmask8, a M512i) M128i {
	return M128i(maskCvtusepi64Epi162([16]byte(src), uint8(k), [64]byte(a)))
}

func maskCvtusepi64Epi162(src [16]byte, k uint8, a [64]byte) [16]byte


// MaskzCvtusepi64Epi162: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 16-bit integers with unsigned saturation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSQW'. Intrinsic: '_mm512_maskz_cvtusepi64_epi16'.
// Requires AVX512F.
func MaskzCvtusepi64Epi162(k Mmask8, a M512i) M128i {
	return M128i(maskzCvtusepi64Epi162(uint8(k), [64]byte(a)))
}

func maskzCvtusepi64Epi162(k uint8, a [64]byte) [16]byte


// Cvtusepi64Epi32: Convert packed unsigned 64-bit integers in 'a' to packed
// unsigned 32-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			k := 32*j
//			dst[k+31:k] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSQD'. Intrinsic: '_mm_cvtusepi64_epi32'.
// Requires AVX512F.
func Cvtusepi64Epi32(a M128i) M128i {
	return M128i(cvtusepi64Epi32([16]byte(a)))
}

func cvtusepi64Epi32(a [16]byte) [16]byte


// MaskCvtusepi64Epi32: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 32-bit integers with unsigned saturation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := src[l+31:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSQD'. Intrinsic: '_mm_mask_cvtusepi64_epi32'.
// Requires AVX512F.
func MaskCvtusepi64Epi32(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtusepi64Epi32([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtusepi64Epi32(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtusepi64Epi32: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 32-bit integers with unsigned saturation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSQD'. Intrinsic: '_mm_maskz_cvtusepi64_epi32'.
// Requires AVX512F.
func MaskzCvtusepi64Epi32(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtusepi64Epi32(uint8(k), [16]byte(a)))
}

func maskzCvtusepi64Epi32(k uint8, a [16]byte) [16]byte


// Cvtusepi64Epi321: Convert packed unsigned 64-bit integers in 'a' to packed
// unsigned 32-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			k := 32*j
//			dst[k+31:k] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSQD'. Intrinsic: '_mm256_cvtusepi64_epi32'.
// Requires AVX512F.
func Cvtusepi64Epi321(a M256i) M128i {
	return M128i(cvtusepi64Epi321([32]byte(a)))
}

func cvtusepi64Epi321(a [32]byte) [16]byte


// MaskCvtusepi64Epi321: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 32-bit integers with unsigned saturation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := src[l+31:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSQD'. Intrinsic: '_mm256_mask_cvtusepi64_epi32'.
// Requires AVX512F.
func MaskCvtusepi64Epi321(src M128i, k Mmask8, a M256i) M128i {
	return M128i(maskCvtusepi64Epi321([16]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtusepi64Epi321(src [16]byte, k uint8, a [32]byte) [16]byte


// MaskzCvtusepi64Epi321: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 32-bit integers with unsigned saturation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSQD'. Intrinsic: '_mm256_maskz_cvtusepi64_epi32'.
// Requires AVX512F.
func MaskzCvtusepi64Epi321(k Mmask8, a M256i) M128i {
	return M128i(maskzCvtusepi64Epi321(uint8(k), [32]byte(a)))
}

func maskzCvtusepi64Epi321(k uint8, a [32]byte) [16]byte


// Cvtusepi64Epi322: Convert packed unsigned 64-bit integers in 'a' to packed
// unsigned 32-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 32*j
//			dst[k+31:k] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVUSQD'. Intrinsic: '_mm512_cvtusepi64_epi32'.
// Requires AVX512F.
func Cvtusepi64Epi322(a M512i) M256i {
	return M256i(cvtusepi64Epi322([64]byte(a)))
}

func cvtusepi64Epi322(a [64]byte) [32]byte


// MaskCvtusepi64Epi322: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 32-bit integers with unsigned saturation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := src[l+31:l]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVUSQD'. Intrinsic: '_mm512_mask_cvtusepi64_epi32'.
// Requires AVX512F.
func MaskCvtusepi64Epi322(src M256i, k Mmask8, a M512i) M256i {
	return M256i(maskCvtusepi64Epi322([32]byte(src), uint8(k), [64]byte(a)))
}

func maskCvtusepi64Epi322(src [32]byte, k uint8, a [64]byte) [32]byte


// MaskzCvtusepi64Epi322: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 32-bit integers with unsigned saturation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVUSQD'. Intrinsic: '_mm512_maskz_cvtusepi64_epi32'.
// Requires AVX512F.
func MaskzCvtusepi64Epi322(k Mmask8, a M512i) M256i {
	return M256i(maskzCvtusepi64Epi322(uint8(k), [64]byte(a)))
}

func maskzCvtusepi64Epi322(k uint8, a [64]byte) [32]byte


// Cvtusepi64Epi8: Convert packed unsigned 64-bit integers in 'a' to packed
// unsigned 8-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			k := 8*j
//			dst[k+7:k] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
//		ENDFOR
//		dst[MAX:16] := 0
//
// Instruction: 'VPMOVUSQB'. Intrinsic: '_mm_cvtusepi64_epi8'.
// Requires AVX512F.
func Cvtusepi64Epi8(a M128i) M128i {
	return M128i(cvtusepi64Epi8([16]byte(a)))
}

func cvtusepi64Epi8(a [16]byte) [16]byte


// MaskCvtusepi64Epi8: Convert packed unsigned 64-bit integers in 'a' to packed
// unsigned 8-bit integers with unsigned saturation, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:16] := 0
//
// Instruction: 'VPMOVUSQB'. Intrinsic: '_mm_mask_cvtusepi64_epi8'.
// Requires AVX512F.
func MaskCvtusepi64Epi8(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtusepi64Epi8([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtusepi64Epi8(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtusepi64Epi8: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 8-bit integers with unsigned saturation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:16] := 0
//
// Instruction: 'VPMOVUSQB'. Intrinsic: '_mm_maskz_cvtusepi64_epi8'.
// Requires AVX512F.
func MaskzCvtusepi64Epi8(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtusepi64Epi8(uint8(k), [16]byte(a)))
}

func maskzCvtusepi64Epi8(k uint8, a [16]byte) [16]byte


// Cvtusepi64Epi81: Convert packed unsigned 64-bit integers in 'a' to packed
// unsigned 8-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			k := 8*j
//			dst[k+7:k] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVUSQB'. Intrinsic: '_mm256_cvtusepi64_epi8'.
// Requires AVX512F.
func Cvtusepi64Epi81(a M256i) M128i {
	return M128i(cvtusepi64Epi81([32]byte(a)))
}

func cvtusepi64Epi81(a [32]byte) [16]byte


// MaskCvtusepi64Epi81: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 8-bit integers with unsigned saturation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVUSQB'. Intrinsic: '_mm256_mask_cvtusepi64_epi8'.
// Requires AVX512F.
func MaskCvtusepi64Epi81(src M128i, k Mmask8, a M256i) M128i {
	return M128i(maskCvtusepi64Epi81([16]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtusepi64Epi81(src [16]byte, k uint8, a [32]byte) [16]byte


// MaskzCvtusepi64Epi81: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 8-bit integers with unsigned saturation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVUSQB'. Intrinsic: '_mm256_maskz_cvtusepi64_epi8'.
// Requires AVX512F.
func MaskzCvtusepi64Epi81(k Mmask8, a M256i) M128i {
	return M128i(maskzCvtusepi64Epi81(uint8(k), [32]byte(a)))
}

func maskzCvtusepi64Epi81(k uint8, a [32]byte) [16]byte


// Cvtusepi64Epi82: Convert packed unsigned 64-bit integers in 'a' to packed
// unsigned 8-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			k := 8*j
//			dst[k+7:k] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSQB'. Intrinsic: '_mm512_cvtusepi64_epi8'.
// Requires AVX512F.
func Cvtusepi64Epi82(a M512i) M128i {
	return M128i(cvtusepi64Epi82([64]byte(a)))
}

func cvtusepi64Epi82(a [64]byte) [16]byte


// MaskCvtusepi64Epi82: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 8-bit integers with unsigned saturation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSQB'. Intrinsic: '_mm512_mask_cvtusepi64_epi8'.
// Requires AVX512F.
func MaskCvtusepi64Epi82(src M128i, k Mmask8, a M512i) M128i {
	return M128i(maskCvtusepi64Epi82([16]byte(src), uint8(k), [64]byte(a)))
}

func maskCvtusepi64Epi82(src [16]byte, k uint8, a [64]byte) [16]byte


// MaskzCvtusepi64Epi82: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 8-bit integers with unsigned saturation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSQB'. Intrinsic: '_mm512_maskz_cvtusepi64_epi8'.
// Requires AVX512F.
func MaskzCvtusepi64Epi82(k Mmask8, a M512i) M128i {
	return M128i(maskzCvtusepi64Epi82(uint8(k), [64]byte(a)))
}

func maskzCvtusepi64Epi82(k uint8, a [64]byte) [16]byte


// MaskCvtusepi64StoreuEpi16: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 16-bit integers with unsigned saturation, and store the
// active results (those with their respective bit set in writemask 'k') to
// unaligned memory at 'base_addr'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				MEM[base_addr+l+15:base_addr+l] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVUSQW'. Intrinsic: '_mm_mask_cvtusepi64_storeu_epi16'.
// Requires AVX512F.
func MaskCvtusepi64StoreuEpi16(base_addr uintptr, k Mmask8, a M128i)  {
	maskCvtusepi64StoreuEpi16(uintptr(base_addr), uint8(k), [16]byte(a))
}

func maskCvtusepi64StoreuEpi16(base_addr uintptr, k uint8, a [16]byte) 


// MaskCvtusepi64StoreuEpi161: Convert packed unsigned 64-bit integers in 'a'
// to packed unsigned 16-bit integers with unsigned saturation, and store the
// active results (those with their respective bit set in writemask 'k') to
// unaligned memory at 'base_addr'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				MEM[base_addr+l+15:base_addr+l] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSQW'. Intrinsic: '_mm256_mask_cvtusepi64_storeu_epi16'.
// Requires AVX512F.
func MaskCvtusepi64StoreuEpi161(base_addr uintptr, k Mmask8, a M256i)  {
	maskCvtusepi64StoreuEpi161(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCvtusepi64StoreuEpi161(base_addr uintptr, k uint8, a [32]byte) 


// MaskCvtusepi64StoreuEpi162: Convert packed unsigned 64-bit integers in 'a'
// to packed 16-bit integers with unsigned saturation, and store the active
// results (those with their respective bit set in writemask 'k') to unaligned
// memory at 'base_addr'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				MEM[base_addr+l+15:base_addr+l] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
//			FI
//		ENDFOR
//
// Instruction: 'VPMOVUSQW'. Intrinsic: '_mm512_mask_cvtusepi64_storeu_epi16'.
// Requires AVX512F.
func MaskCvtusepi64StoreuEpi162(base_addr uintptr, k Mmask8, a M512i)  {
	maskCvtusepi64StoreuEpi162(uintptr(base_addr), uint8(k), [64]byte(a))
}

func maskCvtusepi64StoreuEpi162(base_addr uintptr, k uint8, a [64]byte) 


// MaskCvtusepi64StoreuEpi32: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 32-bit integers with unsigned saturation, and store the
// active results (those with their respective bit set in writemask 'k') to
// unaligned memory at 'base_addr'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				MEM[base_addr+l+31:base_addr+l] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSQD'. Intrinsic: '_mm_mask_cvtusepi64_storeu_epi32'.
// Requires AVX512F.
func MaskCvtusepi64StoreuEpi32(base_addr uintptr, k Mmask8, a M128i)  {
	maskCvtusepi64StoreuEpi32(uintptr(base_addr), uint8(k), [16]byte(a))
}

func maskCvtusepi64StoreuEpi32(base_addr uintptr, k uint8, a [16]byte) 


// MaskCvtusepi64StoreuEpi321: Convert packed unsigned 64-bit integers in 'a'
// to packed unsigned 32-bit integers with unsigned saturation, and store the
// active results (those with their respective bit set in writemask 'k') to
// unaligned memory at 'base_addr'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				MEM[base_addr+l+31:base_addr+l] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSQD'. Intrinsic: '_mm256_mask_cvtusepi64_storeu_epi32'.
// Requires AVX512F.
func MaskCvtusepi64StoreuEpi321(base_addr uintptr, k Mmask8, a M256i)  {
	maskCvtusepi64StoreuEpi321(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCvtusepi64StoreuEpi321(base_addr uintptr, k uint8, a [32]byte) 


// MaskCvtusepi64StoreuEpi322: Convert packed unsigned 64-bit integers in 'a'
// to packed 32-bit integers with unsigned saturation, and store the active
// results (those with their respective bit set in writemask 'k') to unaligned
// memory at 'base_addr'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				MEM[base_addr+l+31:base_addr+l] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
//			FI
//		ENDFOR
//
// Instruction: 'VPMOVUSQD'. Intrinsic: '_mm512_mask_cvtusepi64_storeu_epi32'.
// Requires AVX512F.
func MaskCvtusepi64StoreuEpi322(base_addr uintptr, k Mmask8, a M512i)  {
	maskCvtusepi64StoreuEpi322(uintptr(base_addr), uint8(k), [64]byte(a))
}

func maskCvtusepi64StoreuEpi322(base_addr uintptr, k uint8, a [64]byte) 


// MaskCvtusepi64StoreuEpi8: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 8-bit integers with unsigned saturation, and store the
// active results (those with their respective bit set in writemask 'k') to
// unaligned memory at 'base_addr'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
//			FI
//		ENDFOR
//		dst[MAX:16] := 0
//
// Instruction: 'VPMOVUSQB'. Intrinsic: '_mm_mask_cvtusepi64_storeu_epi8'.
// Requires AVX512F.
func MaskCvtusepi64StoreuEpi8(base_addr uintptr, k Mmask8, a M128i)  {
	maskCvtusepi64StoreuEpi8(uintptr(base_addr), uint8(k), [16]byte(a))
}

func maskCvtusepi64StoreuEpi8(base_addr uintptr, k uint8, a [16]byte) 


// MaskCvtusepi64StoreuEpi81: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 8-bit integers with unsigned saturation, and store the
// active results (those with their respective bit set in writemask 'k') to
// unaligned memory at 'base_addr'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVUSQB'. Intrinsic: '_mm256_mask_cvtusepi64_storeu_epi8'.
// Requires AVX512F.
func MaskCvtusepi64StoreuEpi81(base_addr uintptr, k Mmask8, a M256i)  {
	maskCvtusepi64StoreuEpi81(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCvtusepi64StoreuEpi81(base_addr uintptr, k uint8, a [32]byte) 


// MaskCvtusepi64StoreuEpi82: Convert packed unsigned 64-bit integers in 'a' to
// packed 8-bit integers with unsigned saturation, and store the active results
// (those with their respective bit set in writemask 'k') to unaligned memory
// at 'base_addr'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
//			FI
//		ENDFOR
//
// Instruction: 'VPMOVUSQB'. Intrinsic: '_mm512_mask_cvtusepi64_storeu_epi8'.
// Requires AVX512F.
func MaskCvtusepi64StoreuEpi82(base_addr uintptr, k Mmask8, a M512i)  {
	maskCvtusepi64StoreuEpi82(uintptr(base_addr), uint8(k), [64]byte(a))
}

func maskCvtusepi64StoreuEpi82(base_addr uintptr, k uint8, a [64]byte) 


// DivEpi16: Divide packed 16-bit integers in 'a' by packed elements in 'b',
// and store the truncated results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := 16*j
//			dst[i+15:i] := TRUNCATE(a[i+15:i] / b[i+15:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_div_epi16'.
// Requires AVX512F.
func DivEpi16(a M512i, b M512i) M512i {
	return M512i(divEpi16([64]byte(a), [64]byte(b)))
}

func divEpi16(a [64]byte, b [64]byte) [64]byte


// DivEpi32: Divide packed 32-bit integers in 'a' by packed elements in 'b',
// and store the truncated results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := TRUNCATE(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_div_epi32'.
// Requires AVX512F.
func DivEpi32(a M512i, b M512i) M512i {
	return M512i(divEpi32([64]byte(a), [64]byte(b)))
}

func divEpi32(a [64]byte, b [64]byte) [64]byte


// MaskDivEpi32: Divide packed 32-bit integers in 'a' by packed elements in
// 'b', and store the truncated results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := TRUNCATE(a[i+31:i] / b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_div_epi32'.
// Requires AVX512F.
func MaskDivEpi32(src M512i, k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskDivEpi32([64]byte(src), uint16(k), [64]byte(a), [64]byte(b)))
}

func maskDivEpi32(src [64]byte, k uint16, a [64]byte, b [64]byte) [64]byte


// DivEpi64: Divide packed 64-bit integers in 'a' by packed elements in 'b',
// and store the truncated results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			dst[i+63:i] := TRUNCATE(a[i+63:i] / b[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_div_epi64'.
// Requires AVX512F.
func DivEpi64(a M512i, b M512i) M512i {
	return M512i(divEpi64([64]byte(a), [64]byte(b)))
}

func divEpi64(a [64]byte, b [64]byte) [64]byte


// DivEpi8: Divide packed 8-bit integers in 'a' by packed elements in 'b', and
// store the truncated results in 'dst'. 
//
//		FOR j := 0 to 63
//			i := 8*j
//			dst[i+7:i] := TRUNCATE(a[i+7:i] / b[i+7:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_div_epi8'.
// Requires AVX512F.
func DivEpi8(a M512i, b M512i) M512i {
	return M512i(divEpi8([64]byte(a), [64]byte(b)))
}

func divEpi8(a [64]byte, b [64]byte) [64]byte


// DivEpu16: Divide packed unsigned 16-bit integers in 'a' by packed elements
// in 'b', and store the truncated results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := 16*j
//			dst[i+15:i] := TRUNCATE(a[i+15:i] / b[i+15:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_div_epu16'.
// Requires AVX512F.
func DivEpu16(a M512i, b M512i) M512i {
	return M512i(divEpu16([64]byte(a), [64]byte(b)))
}

func divEpu16(a [64]byte, b [64]byte) [64]byte


// DivEpu32: Divide packed unsigned 32-bit integers in 'a' by packed elements
// in 'b', and store the truncated results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := TRUNCATE(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_div_epu32'.
// Requires AVX512F.
func DivEpu32(a M512i, b M512i) M512i {
	return M512i(divEpu32([64]byte(a), [64]byte(b)))
}

func divEpu32(a [64]byte, b [64]byte) [64]byte


// MaskDivEpu32: Divide packed unsigned 32-bit integers in 'a' by packed
// elements in 'b', and store the truncated results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := TRUNCATE(a[i+31:i] / b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_div_epu32'.
// Requires AVX512F.
func MaskDivEpu32(src M512i, k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskDivEpu32([64]byte(src), uint16(k), [64]byte(a), [64]byte(b)))
}

func maskDivEpu32(src [64]byte, k uint16, a [64]byte, b [64]byte) [64]byte


// DivEpu64: Divide packed unsigned 64-bit integers in 'a' by packed elements
// in 'b', and store the truncated results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			dst[i+63:i] := TRUNCATE(a[i+63:i] / b[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_div_epu64'.
// Requires AVX512F.
func DivEpu64(a M512i, b M512i) M512i {
	return M512i(divEpu64([64]byte(a), [64]byte(b)))
}

func divEpu64(a [64]byte, b [64]byte) [64]byte


// DivEpu8: Divide packed unsigned 8-bit integers in 'a' by packed elements in
// 'b', and store the truncated results in 'dst'. 
//
//		FOR j := 0 to 63
//			i := 8*j
//			dst[i+7:i] := TRUNCATE(a[i+7:i] / b[i+7:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_div_epu8'.
// Requires AVX512F.
func DivEpu8(a M512i, b M512i) M512i {
	return M512i(divEpu8([64]byte(a), [64]byte(b)))
}

func divEpu8(a [64]byte, b [64]byte) [64]byte


// MaskDivPd: Divide packed double-precision (64-bit) floating-point elements
// in 'a' by packed elements in 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] / b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VDIVPD'. Intrinsic: '_mm_mask_div_pd'.
// Requires AVX512F.
func MaskDivPd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskDivPd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskDivPd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzDivPd: Divide packed double-precision (64-bit) floating-point elements
// in 'a' by packed elements in 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] / b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VDIVPD'. Intrinsic: '_mm_maskz_div_pd'.
// Requires AVX512F.
func MaskzDivPd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzDivPd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzDivPd(k uint8, a [2]float64, b [2]float64) [2]float64


// MaskDivPd1: Divide packed double-precision (64-bit) floating-point elements
// in 'a' by packed elements in 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] / b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VDIVPD'. Intrinsic: '_mm256_mask_div_pd'.
// Requires AVX512F.
func MaskDivPd1(src M256d, k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskDivPd1([4]float64(src), uint8(k), [4]float64(a), [4]float64(b)))
}

func maskDivPd1(src [4]float64, k uint8, a [4]float64, b [4]float64) [4]float64


// MaskzDivPd1: Divide packed double-precision (64-bit) floating-point elements
// in 'a' by packed elements in 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] / b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VDIVPD'. Intrinsic: '_mm256_maskz_div_pd'.
// Requires AVX512F.
func MaskzDivPd1(k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskzDivPd1(uint8(k), [4]float64(a), [4]float64(b)))
}

func maskzDivPd1(k uint8, a [4]float64, b [4]float64) [4]float64


// DivPd: Divide packed double-precision (64-bit) floating-point elements in
// 'a' by packed elements in 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			dst[i+63:i] := a[i+63:i] / b[i+63:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VDIVPD'. Intrinsic: '_mm512_div_pd'.
// Requires AVX512F.
func DivPd(a M512d, b M512d) M512d {
	return M512d(divPd([8]float64(a), [8]float64(b)))
}

func divPd(a [8]float64, b [8]float64) [8]float64


// MaskDivPd2: Divide packed double-precision (64-bit) floating-point elements
// in 'a' by packed elements in 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] / b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VDIVPD'. Intrinsic: '_mm512_mask_div_pd'.
// Requires AVX512F.
func MaskDivPd2(src M512d, k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskDivPd2([8]float64(src), uint8(k), [8]float64(a), [8]float64(b)))
}

func maskDivPd2(src [8]float64, k uint8, a [8]float64, b [8]float64) [8]float64


// MaskzDivPd2: Divide packed double-precision (64-bit) floating-point elements
// in 'a' by packed elements in 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := 64*j
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] / b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VDIVPD'. Intrinsic: '_mm512_maskz_div_pd'.
// Requires AVX512F.
func MaskzDivPd2(k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskzDivPd2(uint8(k), [8]float64(a), [8]float64(b)))
}

func maskzDivPd2(k uint8, a [8]float64, b [8]float64) [8]float64


// MaskDivPs: Divide packed single-precision (32-bit) floating-point elements
// in 'a' by packed elements in 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] / b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VDIVPS'. Intrinsic: '_mm_mask_div_ps'.
// Requires AVX512F.
func MaskDivPs(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskDivPs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskDivPs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzDivPs: Divide packed single-precision (32-bit) floating-point elements
// in 'a' by packed elements in 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] / b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VDIVPS'. Intrinsic: '_mm_maskz_div_ps'.
// Requires AVX512F.
func MaskzDivPs(k Mmask8, a M128, b M128) M128 {
	return M128(maskzDivPs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzDivPs(k uint8, a [4]float32, b [4]float32) [4]float32


// MaskDivPs1: Divide packed single-precision (32-bit) floating-point elements
// in 'a' by packed elements in 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] / b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VDIVPS'. Intrinsic: '_mm256_mask_div_ps'.
// Requires AVX512F.
func MaskDivPs1(src M256, k Mmask8, a M256, b M256) M256 {
	return M256(maskDivPs1([8]float32(src), uint8(k), [8]float32(a), [8]float32(b)))
}

func maskDivPs1(src [8]float32, k uint8, a [8]float32, b [8]float32) [8]float32


// MaskzDivPs1: Divide packed single-precision (32-bit) floating-point elements
// in 'a' by packed elements in 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] / b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VDIVPS'. Intrinsic: '_mm256_maskz_div_ps'.
// Requires AVX512F.
func MaskzDivPs1(k Mmask8, a M256, b M256) M256 {
	return M256(maskzDivPs1(uint8(k), [8]float32(a), [8]float32(b)))
}

func maskzDivPs1(k uint8, a [8]float32, b [8]float32) [8]float32


// DivPs: Divide packed single-precision (32-bit) floating-point elements in
// 'a' by packed elements in 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := a[i+31:i] / b[i+31:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VDIVPS'. Intrinsic: '_mm512_div_ps'.
// Requires AVX512F.
func DivPs(a M512, b M512) M512 {
	return M512(divPs([16]float32(a), [16]float32(b)))
}

func divPs(a [16]float32, b [16]float32) [16]float32


// MaskDivPs2: Divide packed single-precision (32-bit) floating-point elements
// in 'a' by packed elements in 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] / b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VDIVPS'. Intrinsic: '_mm512_mask_div_ps'.
// Requires AVX512F.
func MaskDivPs2(src M512, k Mmask16, a M512, b M512) M512 {
	return M512(maskDivPs2([16]float32(src), uint16(k), [16]float32(a), [16]float32(b)))
}

func maskDivPs2(src [16]float32, k uint16, a [16]float32, b [16]float32) [16]float32


// MaskzDivPs2: Divide packed single-precision (32-bit) floating-point elements
// in 'a' by packed elements in 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] / b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VDIVPS'. Intrinsic: '_mm512_maskz_div_ps'.
// Requires AVX512F.
func MaskzDivPs2(k Mmask16, a M512, b M512) M512 {
	return M512(maskzDivPs2(uint16(k), [16]float32(a), [16]float32(b)))
}

func maskzDivPs2(k uint16, a [16]float32, b [16]float32) [16]float32


// DivRoundPd: Divide packed double-precision (64-bit) floating-point elements
// in 'a' by packed elements in 'b', =and store the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := 64*j
//			dst[i+63:i] := a[i+63:i] / b[i+63:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VDIVPD'. Intrinsic: '_mm512_div_round_pd'.
// Requires AVX512F.
func DivRoundPd(a M512d, b M512d, rounding int) M512d {
	return M512d(divRoundPd([8]float64(a), [8]float64(b), rounding))
}

func divRoundPd(a [8]float64, b [8]float64, rounding int) [8]float64


// MaskDivRoundPd: Divide packed double-precision (64-bit) floating-point
// elements in 'a' by packed elements in 'b', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := 64*j
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] / b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VDIVPD'. Intrinsic: '_mm512_mask_div_round_pd'.
// Requires AVX512F.
func MaskDivRoundPd(src M512d, k Mmask8, a M512d, b M512d, rounding int) M512d {
	return M512d(maskDivRoundPd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b), rounding))
}

func maskDivRoundPd(src [8]float64, k uint8, a [8]float64, b [8]float64, rounding int) [8]float64


// MaskzDivRoundPd: Divide packed double-precision (64-bit) floating-point
// elements in 'a' by packed elements in 'b', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := 64*j
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] / b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VDIVPD'. Intrinsic: '_mm512_maskz_div_round_pd'.
// Requires AVX512F.
func MaskzDivRoundPd(k Mmask8, a M512d, b M512d, rounding int) M512d {
	return M512d(maskzDivRoundPd(uint8(k), [8]float64(a), [8]float64(b), rounding))
}

func maskzDivRoundPd(k uint8, a [8]float64, b [8]float64, rounding int) [8]float64


// DivRoundPs: Divide packed single-precision (32-bit) floating-point elements
// in 'a' by packed elements in 'b', and store the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := a[i+31:i] / b[i+31:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VDIVPS'. Intrinsic: '_mm512_div_round_ps'.
// Requires AVX512F.
func DivRoundPs(a M512, b M512, rounding int) M512 {
	return M512(divRoundPs([16]float32(a), [16]float32(b), rounding))
}

func divRoundPs(a [16]float32, b [16]float32, rounding int) [16]float32


// MaskDivRoundPs: Divide packed single-precision (32-bit) floating-point
// elements in 'a' by packed elements in 'b', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] / b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VDIVPS'. Intrinsic: '_mm512_mask_div_round_ps'.
// Requires AVX512F.
func MaskDivRoundPs(src M512, k Mmask16, a M512, b M512, rounding int) M512 {
	return M512(maskDivRoundPs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b), rounding))
}

func maskDivRoundPs(src [16]float32, k uint16, a [16]float32, b [16]float32, rounding int) [16]float32


// MaskzDivRoundPs: Divide packed single-precision (32-bit) floating-point
// elements in 'a' by packed elements in 'b', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] / b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VDIVPS'. Intrinsic: '_mm512_maskz_div_round_ps'.
// Requires AVX512F.
func MaskzDivRoundPs(k Mmask16, a M512, b M512, rounding int) M512 {
	return M512(maskzDivRoundPs(uint16(k), [16]float32(a), [16]float32(b), rounding))
}

func maskzDivRoundPs(k uint16, a [16]float32, b [16]float32, rounding int) [16]float32


// DivRoundSd: Divide the lower double-precision (64-bit) floating-point
// element in 'a' by the lower double-precision (64-bit) floating-point element
// in 'b', store the result in the lower element of 'dst', and copy the upper
// element from 'a' to the upper element of 'dst'.
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := a[63:0] / b[63:0]
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VDIVSD'. Intrinsic: '_mm_div_round_sd'.
// Requires AVX512F.
func DivRoundSd(a M128d, b M128d, rounding int) M128d {
	return M128d(divRoundSd([2]float64(a), [2]float64(b), rounding))
}

func divRoundSd(a [2]float64, b [2]float64, rounding int) [2]float64


// MaskDivRoundSd: Divide the lower double-precision (64-bit) floating-point
// element in 'a' by the lower double-precision (64-bit) floating-point element
// in 'b', store the result in the lower element of 'dst' using writemask 'k'
// (the element is copied from 'src' when mask bit 0 is not set), and copy the
// upper element from 'a' to the upper element of 'dst'. 
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := a[63:0] / b[63:0]
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VDIVSD'. Intrinsic: '_mm_mask_div_round_sd'.
// Requires AVX512F.
func MaskDivRoundSd(src M128d, k Mmask8, a M128d, b M128d, rounding int) M128d {
	return M128d(maskDivRoundSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), rounding))
}

func maskDivRoundSd(src [2]float64, k uint8, a [2]float64, b [2]float64, rounding int) [2]float64


// MaskzDivRoundSd: Divide the lower double-precision (64-bit) floating-point
// element in 'a' by the lower double-precision (64-bit) floating-point element
// in 'b', store the result in the lower element of 'dst' using zeromask 'k'
// (the element is zeroed out when mask bit 0 is not set), and copy the upper
// element from 'a' to the upper element of 'dst'.
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := a[63:0] / b[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VDIVSD'. Intrinsic: '_mm_maskz_div_round_sd'.
// Requires AVX512F.
func MaskzDivRoundSd(k Mmask8, a M128d, b M128d, rounding int) M128d {
	return M128d(maskzDivRoundSd(uint8(k), [2]float64(a), [2]float64(b), rounding))
}

func maskzDivRoundSd(k uint8, a [2]float64, b [2]float64, rounding int) [2]float64


// DivRoundSs: Divide the lower single-precision (32-bit) floating-point
// element in 'a' by the lower single-precision (32-bit) floating-point element
// in 'b', store the result in the lower element of 'dst', and copy the upper 3
// packed elements from 'a' to the upper elements of 'dst'.
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := a[31:0] / b[31:0]
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VDIVSS'. Intrinsic: '_mm_div_round_ss'.
// Requires AVX512F.
func DivRoundSs(a M128, b M128, rounding int) M128 {
	return M128(divRoundSs([4]float32(a), [4]float32(b), rounding))
}

func divRoundSs(a [4]float32, b [4]float32, rounding int) [4]float32


// MaskDivRoundSs: Divide the lower single-precision (32-bit) floating-point
// element in 'a' by the lower single-precision (32-bit) floating-point element
// in 'b', store the result in the lower element of 'dst' using writemask 'k'
// (the element is copied from 'src' when mask bit 0 is not set), and copy the
// upper 3 packed elements from 'a' to the upper elements of 'dst'. 
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := a[31:0] / b[31:0]
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VDIVSS'. Intrinsic: '_mm_mask_div_round_ss'.
// Requires AVX512F.
func MaskDivRoundSs(src M128, k Mmask8, a M128, b M128, rounding int) M128 {
	return M128(maskDivRoundSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), rounding))
}

func maskDivRoundSs(src [4]float32, k uint8, a [4]float32, b [4]float32, rounding int) [4]float32


// MaskzDivRoundSs: Divide the lower single-precision (32-bit) floating-point
// element in 'a' by the lower single-precision (32-bit) floating-point element
// in 'b', store the result in the lower element of 'dst' using zeromask 'k'
// (the element is zeroed out when mask bit 0 is not set), and copy the upper 3
// packed elements from 'a' to the upper elements of 'dst'.
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := a[31:0] / b[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VDIVSS'. Intrinsic: '_mm_maskz_div_round_ss'.
// Requires AVX512F.
func MaskzDivRoundSs(k Mmask8, a M128, b M128, rounding int) M128 {
	return M128(maskzDivRoundSs(uint8(k), [4]float32(a), [4]float32(b), rounding))
}

func maskzDivRoundSs(k uint8, a [4]float32, b [4]float32, rounding int) [4]float32


// MaskDivSd: Divide the lower double-precision (64-bit) floating-point element
// in 'a' by the lower double-precision (64-bit) floating-point element in 'b',
// store the result in the lower element of 'dst' using writemask 'k' (the
// element is copied from 'src' when mask bit 0 is not set), and copy the upper
// element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := a[63:0] / b[63:0]
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VDIVSD'. Intrinsic: '_mm_mask_div_sd'.
// Requires AVX512F.
func MaskDivSd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskDivSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskDivSd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzDivSd: Divide the lower double-precision (64-bit) floating-point
// element in 'a' by the lower double-precision (64-bit) floating-point element
// in 'b', store the result in the lower element of 'dst' using zeromask 'k'
// (the element is zeroed out when mask bit 0 is not set), and copy the upper
// element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := a[63:0] / b[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VDIVSD'. Intrinsic: '_mm_maskz_div_sd'.
// Requires AVX512F.
func MaskzDivSd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzDivSd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzDivSd(k uint8, a [2]float64, b [2]float64) [2]float64


// MaskDivSs: Divide the lower single-precision (32-bit) floating-point element
// in 'a' by the lower single-precision (32-bit) floating-point element in 'b',
// store the result in the lower element of 'dst' using writemask 'k' (the
// element is copied from 'src' when mask bit 0 is not set), and copy the upper
// 3 packed elements from 'a' to the upper elements of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := a[31:0] / b[31:0]
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VDIVSS'. Intrinsic: '_mm_mask_div_ss'.
// Requires AVX512F.
func MaskDivSs(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskDivSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskDivSs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzDivSs: Divide the lower single-precision (32-bit) floating-point
// element in 'a' by the lower single-precision (32-bit) floating-point element
// in 'b', store the result in the lower element of 'dst' using zeromask 'k'
// (the element is zeroed out when mask bit 0 is not set), and copy the upper 3
// packed elements from 'a' to the upper elements of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := a[31:0] / b[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VDIVSS'. Intrinsic: '_mm_maskz_div_ss'.
// Requires AVX512F.
func MaskzDivSs(k Mmask8, a M128, b M128) M128 {
	return M128(maskzDivSs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzDivSs(k uint8, a [4]float32, b [4]float32) [4]float32


// ErfPd: Compute the error function of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ERF(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_erf_pd'.
// Requires AVX512F.
func ErfPd(a M512d) M512d {
	return M512d(erfPd([8]float64(a)))
}

func erfPd(a [8]float64) [8]float64


// MaskErfPd: Compute the error function of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ERF(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_erf_pd'.
// Requires AVX512F.
func MaskErfPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskErfPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskErfPd(src [8]float64, k uint8, a [8]float64) [8]float64


// ErfPs: Compute the error function of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ERF(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_erf_ps'.
// Requires AVX512F.
func ErfPs(a M512) M512 {
	return M512(erfPs([16]float32(a)))
}

func erfPs(a [16]float32) [16]float32


// MaskErfPs: Compute the error function of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ERF(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_erf_ps'.
// Requires AVX512F.
func MaskErfPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskErfPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskErfPs(src [16]float32, k uint16, a [16]float32) [16]float32


// ErfcPd: Compute the complementary error function of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := 1.0 - ERF(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_erfc_pd'.
// Requires AVX512F.
func ErfcPd(a M512d) M512d {
	return M512d(erfcPd([8]float64(a)))
}

func erfcPd(a [8]float64) [8]float64


// MaskErfcPd: Compute the complementary error function of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := 1.0 - ERF(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_erfc_pd'.
// Requires AVX512F.
func MaskErfcPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskErfcPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskErfcPd(src [8]float64, k uint8, a [8]float64) [8]float64


// ErfcPs: Compute the complementary error function of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := 1.0 - ERF(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_erfc_ps'.
// Requires AVX512F.
func ErfcPs(a M512) M512 {
	return M512(erfcPs([16]float32(a)))
}

func erfcPs(a [16]float32) [16]float32


// MaskErfcPs: Compute the complementary error function of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := 1.0 - ERF(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_erfc_ps'.
// Requires AVX512F.
func MaskErfcPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskErfcPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskErfcPs(src [16]float32, k uint16, a [16]float32) [16]float32


// ErfcinvPd: Compute the inverse complementary error function of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := 1.0 / (1.0 - ERF(a[i+63:i]))
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_erfcinv_pd'.
// Requires AVX512F.
func ErfcinvPd(a M512d) M512d {
	return M512d(erfcinvPd([8]float64(a)))
}

func erfcinvPd(a [8]float64) [8]float64


// MaskErfcinvPd: Compute the inverse complementary error function of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := 1.0 / (1.0 - ERF(a[i+63:i]))
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_erfcinv_pd'.
// Requires AVX512F.
func MaskErfcinvPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskErfcinvPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskErfcinvPd(src [8]float64, k uint8, a [8]float64) [8]float64


// ErfcinvPs: Compute the inverse complementary error function of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := 1.0 / (1.0 - ERF(a[i+31:i]))
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_erfcinv_ps'.
// Requires AVX512F.
func ErfcinvPs(a M512) M512 {
	return M512(erfcinvPs([16]float32(a)))
}

func erfcinvPs(a [16]float32) [16]float32


// MaskErfcinvPs: Compute the inverse complementary error function of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := 1.0 / (1.0 - ERF(a[i+31:i]))
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_erfcinv_ps'.
// Requires AVX512F.
func MaskErfcinvPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskErfcinvPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskErfcinvPs(src [16]float32, k uint16, a [16]float32) [16]float32


// ErfinvPd: Compute the inverse error function of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := 1.0 / ERF(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_erfinv_pd'.
// Requires AVX512F.
func ErfinvPd(a M512d) M512d {
	return M512d(erfinvPd([8]float64(a)))
}

func erfinvPd(a [8]float64) [8]float64


// MaskErfinvPd: Compute the inverse error function of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := 1.0 / ERF(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_erfinv_pd'.
// Requires AVX512F.
func MaskErfinvPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskErfinvPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskErfinvPd(src [8]float64, k uint8, a [8]float64) [8]float64


// ErfinvPs: Compute the inverse error function of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := 1.0 / ERF(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_erfinv_ps'.
// Requires AVX512F.
func ErfinvPs(a M512) M512 {
	return M512(erfinvPs([16]float32(a)))
}

func erfinvPs(a [16]float32) [16]float32


// MaskErfinvPs: Compute the inverse error function of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := 1.0 / ERF(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_erfinv_ps'.
// Requires AVX512F.
func MaskErfinvPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskErfinvPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskErfinvPs(src [16]float32, k uint16, a [16]float32) [16]float32


// ExpPd: Compute the exponential value of 'e' raised to the power of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := e^(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_exp_pd'.
// Requires AVX512F.
func ExpPd(a M512d) M512d {
	return M512d(expPd([8]float64(a)))
}

func expPd(a [8]float64) [8]float64


// MaskExpPd: Compute the exponential value of 'e' raised to the power of
// packed double-precision (64-bit) floating-point elements in 'a', and store
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := e^(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_exp_pd'.
// Requires AVX512F.
func MaskExpPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskExpPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskExpPd(src [8]float64, k uint8, a [8]float64) [8]float64


// ExpPs: Compute the exponential value of 'e' raised to the power of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := e^(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_exp_ps'.
// Requires AVX512F.
func ExpPs(a M512) M512 {
	return M512(expPs([16]float32(a)))
}

func expPs(a [16]float32) [16]float32


// MaskExpPs: Compute the exponential value of 'e' raised to the power of
// packed single-precision (32-bit) floating-point elements in 'a', and store
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := e^(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_exp_ps'.
// Requires AVX512F.
func MaskExpPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskExpPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskExpPs(src [16]float32, k uint16, a [16]float32) [16]float32


// Exp10Pd: Compute the exponential value of 10 raised to the power of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := 10^(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_exp10_pd'.
// Requires AVX512F.
func Exp10Pd(a M512d) M512d {
	return M512d(exp10Pd([8]float64(a)))
}

func exp10Pd(a [8]float64) [8]float64


// MaskExp10Pd: Compute the exponential value of 10 raised to the power of
// packed double-precision (64-bit) floating-point elements in 'a', and store
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := 10^(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_exp10_pd'.
// Requires AVX512F.
func MaskExp10Pd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskExp10Pd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskExp10Pd(src [8]float64, k uint8, a [8]float64) [8]float64


// Exp10Ps: Compute the exponential value of 10 raised to the power of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := 10^(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_exp10_ps'.
// Requires AVX512F.
func Exp10Ps(a M512) M512 {
	return M512(exp10Ps([16]float32(a)))
}

func exp10Ps(a [16]float32) [16]float32


// MaskExp10Ps: Compute the exponential value of 10 raised to the power of
// packed single-precision (32-bit) floating-point elements in 'a', and store
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := 10^(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_exp10_ps'.
// Requires AVX512F.
func MaskExp10Ps(src M512, k Mmask16, a M512) M512 {
	return M512(maskExp10Ps([16]float32(src), uint16(k), [16]float32(a)))
}

func maskExp10Ps(src [16]float32, k uint16, a [16]float32) [16]float32


// Exp2Pd: Compute the exponential value of 2 raised to the power of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := 2^(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_exp2_pd'.
// Requires AVX512F.
func Exp2Pd(a M512d) M512d {
	return M512d(exp2Pd([8]float64(a)))
}

func exp2Pd(a [8]float64) [8]float64


// MaskExp2Pd: Compute the exponential value of 2 raised to the power of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := 2^(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_exp2_pd'.
// Requires AVX512F.
func MaskExp2Pd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskExp2Pd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskExp2Pd(src [8]float64, k uint8, a [8]float64) [8]float64


// Exp2Ps: Compute the exponential value of 2 raised to the power of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := 2^(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_exp2_ps'.
// Requires AVX512F.
func Exp2Ps(a M512) M512 {
	return M512(exp2Ps([16]float32(a)))
}

func exp2Ps(a [16]float32) [16]float32


// MaskExp2Ps: Compute the exponential value of 2 raised to the power of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := 2^(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_exp2_ps'.
// Requires AVX512F.
func MaskExp2Ps(src M512, k Mmask16, a M512) M512 {
	return M512(maskExp2Ps([16]float32(src), uint16(k), [16]float32(a)))
}

func maskExp2Ps(src [16]float32, k uint16, a [16]float32) [16]float32


// MaskExpandEpi32: Load contiguous active 32-bit integers from 'a' (those with
// their respective bit set in mask 'k'), and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[m+31:m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPEXPANDD'. Intrinsic: '_mm_mask_expand_epi32'.
// Requires AVX512F.
func MaskExpandEpi32(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskExpandEpi32([16]byte(src), uint8(k), [16]byte(a)))
}

func maskExpandEpi32(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzExpandEpi32: Load contiguous active 32-bit integers from 'a' (those
// with their respective bit set in mask 'k'), and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[m+31:m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPEXPANDD'. Intrinsic: '_mm_maskz_expand_epi32'.
// Requires AVX512F.
func MaskzExpandEpi32(k Mmask8, a M128i) M128i {
	return M128i(maskzExpandEpi32(uint8(k), [16]byte(a)))
}

func maskzExpandEpi32(k uint8, a [16]byte) [16]byte


// MaskExpandEpi321: Load contiguous active 32-bit integers from 'a' (those
// with their respective bit set in mask 'k'), and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[m+31:m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPEXPANDD'. Intrinsic: '_mm256_mask_expand_epi32'.
// Requires AVX512F.
func MaskExpandEpi321(src M256i, k Mmask8, a M256i) M256i {
	return M256i(maskExpandEpi321([32]byte(src), uint8(k), [32]byte(a)))
}

func maskExpandEpi321(src [32]byte, k uint8, a [32]byte) [32]byte


// MaskzExpandEpi321: Load contiguous active 32-bit integers from 'a' (those
// with their respective bit set in mask 'k'), and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[m+31:m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPEXPANDD'. Intrinsic: '_mm256_maskz_expand_epi32'.
// Requires AVX512F.
func MaskzExpandEpi321(k Mmask8, a M256i) M256i {
	return M256i(maskzExpandEpi321(uint8(k), [32]byte(a)))
}

func maskzExpandEpi321(k uint8, a [32]byte) [32]byte


// MaskExpandEpi322: Load contiguous active 32-bit integers from 'a' (those
// with their respective bit set in mask 'k'), and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[m+31:m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPEXPANDD'. Intrinsic: '_mm512_mask_expand_epi32'.
// Requires AVX512F.
func MaskExpandEpi322(src M512i, k Mmask16, a M512i) M512i {
	return M512i(maskExpandEpi322([64]byte(src), uint16(k), [64]byte(a)))
}

func maskExpandEpi322(src [64]byte, k uint16, a [64]byte) [64]byte


// MaskzExpandEpi322: Load contiguous active 32-bit integers from 'a' (those
// with their respective bit set in mask 'k'), and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		m := 0
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[m+31:m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPEXPANDD'. Intrinsic: '_mm512_maskz_expand_epi32'.
// Requires AVX512F.
func MaskzExpandEpi322(k Mmask16, a M512i) M512i {
	return M512i(maskzExpandEpi322(uint16(k), [64]byte(a)))
}

func maskzExpandEpi322(k uint16, a [64]byte) [64]byte


// MaskExpandEpi64: Load contiguous active 64-bit integers from 'a' (those with
// their respective bit set in mask 'k'), and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		m := 0
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[m+63:m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPEXPANDQ'. Intrinsic: '_mm_mask_expand_epi64'.
// Requires AVX512F.
func MaskExpandEpi64(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskExpandEpi64([16]byte(src), uint8(k), [16]byte(a)))
}

func maskExpandEpi64(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzExpandEpi64: Load contiguous active 64-bit integers from 'a' (those
// with their respective bit set in mask 'k'), and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		m := 0
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[m+63:m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPEXPANDQ'. Intrinsic: '_mm_maskz_expand_epi64'.
// Requires AVX512F.
func MaskzExpandEpi64(k Mmask8, a M128i) M128i {
	return M128i(maskzExpandEpi64(uint8(k), [16]byte(a)))
}

func maskzExpandEpi64(k uint8, a [16]byte) [16]byte


// MaskExpandEpi641: Load contiguous active 64-bit integers from 'a' (those
// with their respective bit set in mask 'k'), and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[m+63:m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPEXPANDQ'. Intrinsic: '_mm256_mask_expand_epi64'.
// Requires AVX512F.
func MaskExpandEpi641(src M256i, k Mmask8, a M256i) M256i {
	return M256i(maskExpandEpi641([32]byte(src), uint8(k), [32]byte(a)))
}

func maskExpandEpi641(src [32]byte, k uint8, a [32]byte) [32]byte


// MaskzExpandEpi641: Load contiguous active 64-bit integers from 'a' (those
// with their respective bit set in mask 'k'), and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[m+63:m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPEXPANDQ'. Intrinsic: '_mm256_maskz_expand_epi64'.
// Requires AVX512F.
func MaskzExpandEpi641(k Mmask8, a M256i) M256i {
	return M256i(maskzExpandEpi641(uint8(k), [32]byte(a)))
}

func maskzExpandEpi641(k uint8, a [32]byte) [32]byte


// MaskExpandEpi642: Load contiguous active 64-bit integers from 'a' (those
// with their respective bit set in mask 'k'), and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[m+63:m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPEXPANDQ'. Intrinsic: '_mm512_mask_expand_epi64'.
// Requires AVX512F.
func MaskExpandEpi642(src M512i, k Mmask8, a M512i) M512i {
	return M512i(maskExpandEpi642([64]byte(src), uint8(k), [64]byte(a)))
}

func maskExpandEpi642(src [64]byte, k uint8, a [64]byte) [64]byte


// MaskzExpandEpi642: Load contiguous active 64-bit integers from 'a' (those
// with their respective bit set in mask 'k'), and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[m+63:m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPEXPANDQ'. Intrinsic: '_mm512_maskz_expand_epi64'.
// Requires AVX512F.
func MaskzExpandEpi642(k Mmask8, a M512i) M512i {
	return M512i(maskzExpandEpi642(uint8(k), [64]byte(a)))
}

func maskzExpandEpi642(k uint8, a [64]byte) [64]byte


// MaskExpandPd: Load contiguous active double-precision (64-bit)
// floating-point elements from 'a' (those with their respective bit set in
// mask 'k'), and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[m+63:m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXPANDPD'. Intrinsic: '_mm_mask_expand_pd'.
// Requires AVX512F.
func MaskExpandPd(src M128d, k Mmask8, a M128d) M128d {
	return M128d(maskExpandPd([2]float64(src), uint8(k), [2]float64(a)))
}

func maskExpandPd(src [2]float64, k uint8, a [2]float64) [2]float64


// MaskzExpandPd: Load contiguous active double-precision (64-bit)
// floating-point elements from 'a' (those with their respective bit set in
// mask 'k'), and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[m+63:m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXPANDPD'. Intrinsic: '_mm_maskz_expand_pd'.
// Requires AVX512F.
func MaskzExpandPd(k Mmask8, a M128d) M128d {
	return M128d(maskzExpandPd(uint8(k), [2]float64(a)))
}

func maskzExpandPd(k uint8, a [2]float64) [2]float64


// MaskExpandPd1: Load contiguous active double-precision (64-bit)
// floating-point elements from 'a' (those with their respective bit set in
// mask 'k'), and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[m+63:m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VEXPANDPD'. Intrinsic: '_mm256_mask_expand_pd'.
// Requires AVX512F.
func MaskExpandPd1(src M256d, k Mmask8, a M256d) M256d {
	return M256d(maskExpandPd1([4]float64(src), uint8(k), [4]float64(a)))
}

func maskExpandPd1(src [4]float64, k uint8, a [4]float64) [4]float64


// MaskzExpandPd1: Load contiguous active double-precision (64-bit)
// floating-point elements from 'a' (those with their respective bit set in
// mask 'k'), and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[m+63:m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VEXPANDPD'. Intrinsic: '_mm256_maskz_expand_pd'.
// Requires AVX512F.
func MaskzExpandPd1(k Mmask8, a M256d) M256d {
	return M256d(maskzExpandPd1(uint8(k), [4]float64(a)))
}

func maskzExpandPd1(k uint8, a [4]float64) [4]float64


// MaskExpandPd2: Load contiguous active double-precision (64-bit)
// floating-point elements from 'a' (those with their respective bit set in
// mask 'k'), and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[m+63:m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VEXPANDPD'. Intrinsic: '_mm512_mask_expand_pd'.
// Requires AVX512F.
func MaskExpandPd2(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskExpandPd2([8]float64(src), uint8(k), [8]float64(a)))
}

func maskExpandPd2(src [8]float64, k uint8, a [8]float64) [8]float64


// MaskzExpandPd2: Load contiguous active double-precision (64-bit)
// floating-point elements from 'a' (those with their respective bit set in
// mask 'k'), and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[m+63:m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VEXPANDPD'. Intrinsic: '_mm512_maskz_expand_pd'.
// Requires AVX512F.
func MaskzExpandPd2(k Mmask8, a M512d) M512d {
	return M512d(maskzExpandPd2(uint8(k), [8]float64(a)))
}

func maskzExpandPd2(k uint8, a [8]float64) [8]float64


// MaskExpandPs: Load contiguous active single-precision (32-bit)
// floating-point elements from 'a' (those with their respective bit set in
// mask 'k'), and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[m+31:m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXPANDPS'. Intrinsic: '_mm_mask_expand_ps'.
// Requires AVX512F.
func MaskExpandPs(src M128, k Mmask8, a M128) M128 {
	return M128(maskExpandPs([4]float32(src), uint8(k), [4]float32(a)))
}

func maskExpandPs(src [4]float32, k uint8, a [4]float32) [4]float32


// MaskzExpandPs: Load contiguous active single-precision (32-bit)
// floating-point elements from 'a' (those with their respective bit set in
// mask 'k'), and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[m+31:m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXPANDPS'. Intrinsic: '_mm_maskz_expand_ps'.
// Requires AVX512F.
func MaskzExpandPs(k Mmask8, a M128) M128 {
	return M128(maskzExpandPs(uint8(k), [4]float32(a)))
}

func maskzExpandPs(k uint8, a [4]float32) [4]float32


// MaskExpandPs1: Load contiguous active single-precision (32-bit)
// floating-point elements from 'a' (those with their respective bit set in
// mask 'k'), and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[m+31:m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VEXPANDPS'. Intrinsic: '_mm256_mask_expand_ps'.
// Requires AVX512F.
func MaskExpandPs1(src M256, k Mmask8, a M256) M256 {
	return M256(maskExpandPs1([8]float32(src), uint8(k), [8]float32(a)))
}

func maskExpandPs1(src [8]float32, k uint8, a [8]float32) [8]float32


// MaskzExpandPs1: Load contiguous active single-precision (32-bit)
// floating-point elements from 'a' (those with their respective bit set in
// mask 'k'), and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[m+31:m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VEXPANDPS'. Intrinsic: '_mm256_maskz_expand_ps'.
// Requires AVX512F.
func MaskzExpandPs1(k Mmask8, a M256) M256 {
	return M256(maskzExpandPs1(uint8(k), [8]float32(a)))
}

func maskzExpandPs1(k uint8, a [8]float32) [8]float32


// MaskExpandPs2: Load contiguous active single-precision (32-bit)
// floating-point elements from 'a' (those with their respective bit set in
// mask 'k'), and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[m+31:m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VEXPANDPS'. Intrinsic: '_mm512_mask_expand_ps'.
// Requires AVX512F.
func MaskExpandPs2(src M512, k Mmask16, a M512) M512 {
	return M512(maskExpandPs2([16]float32(src), uint16(k), [16]float32(a)))
}

func maskExpandPs2(src [16]float32, k uint16, a [16]float32) [16]float32


// MaskzExpandPs2: Load contiguous active single-precision (32-bit)
// floating-point elements from 'a' (those with their respective bit set in
// mask 'k'), and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[m+31:m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VEXPANDPS'. Intrinsic: '_mm512_maskz_expand_ps'.
// Requires AVX512F.
func MaskzExpandPs2(k Mmask16, a M512) M512 {
	return M512(maskzExpandPs2(uint16(k), [16]float32(a)))
}

func maskzExpandPs2(k uint16, a [16]float32) [16]float32


// MaskExpandloaduEpi32: Load contiguous active 32-bit integers from unaligned
// memory at 'mem_addr' (those with their respective bit set in mask 'k'), and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPEXPANDD'. Intrinsic: '_mm_mask_expandloadu_epi32'.
// Requires AVX512F.
func MaskExpandloaduEpi32(src M128i, k Mmask8, mem_addr uintptr) M128i {
	return M128i(maskExpandloaduEpi32([16]byte(src), uint8(k), uintptr(mem_addr)))
}

func maskExpandloaduEpi32(src [16]byte, k uint8, mem_addr uintptr) [16]byte


// MaskzExpandloaduEpi32: Load contiguous active 32-bit integers from unaligned
// memory at 'mem_addr' (those with their respective bit set in mask 'k'), and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPEXPANDD'. Intrinsic: '_mm_maskz_expandloadu_epi32'.
// Requires AVX512F.
func MaskzExpandloaduEpi32(k Mmask8, mem_addr uintptr) M128i {
	return M128i(maskzExpandloaduEpi32(uint8(k), uintptr(mem_addr)))
}

func maskzExpandloaduEpi32(k uint8, mem_addr uintptr) [16]byte


// MaskExpandloaduEpi321: Load contiguous active 32-bit integers from unaligned
// memory at 'mem_addr' (those with their respective bit set in mask 'k'), and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPEXPANDD'. Intrinsic: '_mm256_mask_expandloadu_epi32'.
// Requires AVX512F.
func MaskExpandloaduEpi321(src M256i, k Mmask8, mem_addr uintptr) M256i {
	return M256i(maskExpandloaduEpi321([32]byte(src), uint8(k), uintptr(mem_addr)))
}

func maskExpandloaduEpi321(src [32]byte, k uint8, mem_addr uintptr) [32]byte


// MaskzExpandloaduEpi321: Load contiguous active 32-bit integers from
// unaligned memory at 'mem_addr' (those with their respective bit set in mask
// 'k'), and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPEXPANDD'. Intrinsic: '_mm256_maskz_expandloadu_epi32'.
// Requires AVX512F.
func MaskzExpandloaduEpi321(k Mmask8, mem_addr uintptr) M256i {
	return M256i(maskzExpandloaduEpi321(uint8(k), uintptr(mem_addr)))
}

func maskzExpandloaduEpi321(k uint8, mem_addr uintptr) [32]byte


// MaskExpandloaduEpi322: Load contiguous active 32-bit integers from unaligned
// memory at 'mem_addr' (those with their respective bit set in mask 'k'), and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPEXPANDD'. Intrinsic: '_mm512_mask_expandloadu_epi32'.
// Requires AVX512F.
func MaskExpandloaduEpi322(src M512i, k Mmask16, mem_addr uintptr) M512i {
	return M512i(maskExpandloaduEpi322([64]byte(src), uint16(k), uintptr(mem_addr)))
}

func maskExpandloaduEpi322(src [64]byte, k uint16, mem_addr uintptr) [64]byte


// MaskzExpandloaduEpi322: Load contiguous active 32-bit integers from
// unaligned memory at 'mem_addr' (those with their respective bit set in mask
// 'k'), and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPEXPANDD'. Intrinsic: '_mm512_maskz_expandloadu_epi32'.
// Requires AVX512F.
func MaskzExpandloaduEpi322(k Mmask16, mem_addr uintptr) M512i {
	return M512i(maskzExpandloaduEpi322(uint16(k), uintptr(mem_addr)))
}

func maskzExpandloaduEpi322(k uint16, mem_addr uintptr) [64]byte


// MaskExpandloaduEpi64: Load contiguous active 64-bit integers from unaligned
// memory at 'mem_addr' (those with their respective bit set in mask 'k'), and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPEXPANDQ'. Intrinsic: '_mm_mask_expandloadu_epi64'.
// Requires AVX512F.
func MaskExpandloaduEpi64(src M128i, k Mmask8, mem_addr uintptr) M128i {
	return M128i(maskExpandloaduEpi64([16]byte(src), uint8(k), uintptr(mem_addr)))
}

func maskExpandloaduEpi64(src [16]byte, k uint8, mem_addr uintptr) [16]byte


// MaskzExpandloaduEpi64: Load contiguous active 64-bit integers from unaligned
// memory at 'mem_addr' (those with their respective bit set in mask 'k'), and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPEXPANDQ'. Intrinsic: '_mm_maskz_expandloadu_epi64'.
// Requires AVX512F.
func MaskzExpandloaduEpi64(k Mmask8, mem_addr uintptr) M128i {
	return M128i(maskzExpandloaduEpi64(uint8(k), uintptr(mem_addr)))
}

func maskzExpandloaduEpi64(k uint8, mem_addr uintptr) [16]byte


// MaskExpandloaduEpi641: Load contiguous active 64-bit integers from unaligned
// memory at 'mem_addr' (those with their respective bit set in mask 'k'), and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPEXPANDQ'. Intrinsic: '_mm256_mask_expandloadu_epi64'.
// Requires AVX512F.
func MaskExpandloaduEpi641(src M256i, k Mmask8, mem_addr uintptr) M256i {
	return M256i(maskExpandloaduEpi641([32]byte(src), uint8(k), uintptr(mem_addr)))
}

func maskExpandloaduEpi641(src [32]byte, k uint8, mem_addr uintptr) [32]byte


// MaskzExpandloaduEpi641: Load contiguous active 64-bit integers from
// unaligned memory at 'mem_addr' (those with their respective bit set in mask
// 'k'), and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPEXPANDQ'. Intrinsic: '_mm256_maskz_expandloadu_epi64'.
// Requires AVX512F.
func MaskzExpandloaduEpi641(k Mmask8, mem_addr uintptr) M256i {
	return M256i(maskzExpandloaduEpi641(uint8(k), uintptr(mem_addr)))
}

func maskzExpandloaduEpi641(k uint8, mem_addr uintptr) [32]byte


// MaskExpandloaduEpi642: Load contiguous active 64-bit integers from unaligned
// memory at 'mem_addr' (those with their respective bit set in mask 'k'), and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPEXPANDQ'. Intrinsic: '_mm512_mask_expandloadu_epi64'.
// Requires AVX512F.
func MaskExpandloaduEpi642(src M512i, k Mmask8, mem_addr uintptr) M512i {
	return M512i(maskExpandloaduEpi642([64]byte(src), uint8(k), uintptr(mem_addr)))
}

func maskExpandloaduEpi642(src [64]byte, k uint8, mem_addr uintptr) [64]byte


// MaskzExpandloaduEpi642: Load contiguous active 64-bit integers from
// unaligned memory at 'mem_addr' (those with their respective bit set in mask
// 'k'), and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPEXPANDQ'. Intrinsic: '_mm512_maskz_expandloadu_epi64'.
// Requires AVX512F.
func MaskzExpandloaduEpi642(k Mmask8, mem_addr uintptr) M512i {
	return M512i(maskzExpandloaduEpi642(uint8(k), uintptr(mem_addr)))
}

func maskzExpandloaduEpi642(k uint8, mem_addr uintptr) [64]byte


// MaskExpandloaduPd: Load contiguous active double-precision (64-bit)
// floating-point elements from unaligned memory at 'mem_addr' (those with
// their respective bit set in mask 'k'), and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		m := 0
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXPANDPD'. Intrinsic: '_mm_mask_expandloadu_pd'.
// Requires AVX512F.
func MaskExpandloaduPd(src M128d, k Mmask8, mem_addr uintptr) M128d {
	return M128d(maskExpandloaduPd([2]float64(src), uint8(k), uintptr(mem_addr)))
}

func maskExpandloaduPd(src [2]float64, k uint8, mem_addr uintptr) [2]float64


// MaskzExpandloaduPd: Load contiguous active double-precision (64-bit)
// floating-point elements from unaligned memory at 'mem_addr' (those with
// their respective bit set in mask 'k'), and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		m := 0
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXPANDPD'. Intrinsic: '_mm_maskz_expandloadu_pd'.
// Requires AVX512F.
func MaskzExpandloaduPd(k Mmask8, mem_addr uintptr) M128d {
	return M128d(maskzExpandloaduPd(uint8(k), uintptr(mem_addr)))
}

func maskzExpandloaduPd(k uint8, mem_addr uintptr) [2]float64


// MaskExpandloaduPd1: Load contiguous active double-precision (64-bit)
// floating-point elements from unaligned memory at 'mem_addr' (those with
// their respective bit set in mask 'k'), and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VEXPANDPD'. Intrinsic: '_mm256_mask_expandloadu_pd'.
// Requires AVX512F.
func MaskExpandloaduPd1(src M256d, k Mmask8, mem_addr uintptr) M256d {
	return M256d(maskExpandloaduPd1([4]float64(src), uint8(k), uintptr(mem_addr)))
}

func maskExpandloaduPd1(src [4]float64, k uint8, mem_addr uintptr) [4]float64


// MaskzExpandloaduPd1: Load contiguous active double-precision (64-bit)
// floating-point elements from unaligned memory at 'mem_addr' (those with
// their respective bit set in mask 'k'), and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VEXPANDPD'. Intrinsic: '_mm256_maskz_expandloadu_pd'.
// Requires AVX512F.
func MaskzExpandloaduPd1(k Mmask8, mem_addr uintptr) M256d {
	return M256d(maskzExpandloaduPd1(uint8(k), uintptr(mem_addr)))
}

func maskzExpandloaduPd1(k uint8, mem_addr uintptr) [4]float64


// MaskExpandloaduPd2: Load contiguous active double-precision (64-bit)
// floating-point elements from unaligned memory at 'mem_addr' (those with
// their respective bit set in mask 'k'), and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VEXPANDPD'. Intrinsic: '_mm512_mask_expandloadu_pd'.
// Requires AVX512F.
func MaskExpandloaduPd2(src M512d, k Mmask8, mem_addr uintptr) M512d {
	return M512d(maskExpandloaduPd2([8]float64(src), uint8(k), uintptr(mem_addr)))
}

func maskExpandloaduPd2(src [8]float64, k uint8, mem_addr uintptr) [8]float64


// MaskzExpandloaduPd2: Load contiguous active double-precision (64-bit)
// floating-point elements from unaligned memory at 'mem_addr' (those with
// their respective bit set in mask 'k'), and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VEXPANDPD'. Intrinsic: '_mm512_maskz_expandloadu_pd'.
// Requires AVX512F.
func MaskzExpandloaduPd2(k Mmask8, mem_addr uintptr) M512d {
	return M512d(maskzExpandloaduPd2(uint8(k), uintptr(mem_addr)))
}

func maskzExpandloaduPd2(k uint8, mem_addr uintptr) [8]float64


// MaskExpandloaduPs: Load contiguous active single-precision (32-bit)
// floating-point elements from unaligned memory at 'mem_addr' (those with
// their respective bit set in mask 'k'), and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXPANDPS'. Intrinsic: '_mm_mask_expandloadu_ps'.
// Requires AVX512F.
func MaskExpandloaduPs(src M128, k Mmask8, mem_addr uintptr) M128 {
	return M128(maskExpandloaduPs([4]float32(src), uint8(k), uintptr(mem_addr)))
}

func maskExpandloaduPs(src [4]float32, k uint8, mem_addr uintptr) [4]float32


// MaskzExpandloaduPs: Load contiguous active single-precision (32-bit)
// floating-point elements from unaligned memory at 'mem_addr' (those with
// their respective bit set in mask 'k'), and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXPANDPS'. Intrinsic: '_mm_maskz_expandloadu_ps'.
// Requires AVX512F.
func MaskzExpandloaduPs(k Mmask8, mem_addr uintptr) M128 {
	return M128(maskzExpandloaduPs(uint8(k), uintptr(mem_addr)))
}

func maskzExpandloaduPs(k uint8, mem_addr uintptr) [4]float32


// MaskExpandloaduPs1: Load contiguous active single-precision (32-bit)
// floating-point elements from unaligned memory at 'mem_addr' (those with
// their respective bit set in mask 'k'), and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VEXPANDPS'. Intrinsic: '_mm256_mask_expandloadu_ps'.
// Requires AVX512F.
func MaskExpandloaduPs1(src M256, k Mmask8, mem_addr uintptr) M256 {
	return M256(maskExpandloaduPs1([8]float32(src), uint8(k), uintptr(mem_addr)))
}

func maskExpandloaduPs1(src [8]float32, k uint8, mem_addr uintptr) [8]float32


// MaskzExpandloaduPs1: Load contiguous active single-precision (32-bit)
// floating-point elements from unaligned memory at 'mem_addr' (those with
// their respective bit set in mask 'k'), and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VEXPANDPS'. Intrinsic: '_mm256_maskz_expandloadu_ps'.
// Requires AVX512F.
func MaskzExpandloaduPs1(k Mmask8, mem_addr uintptr) M256 {
	return M256(maskzExpandloaduPs1(uint8(k), uintptr(mem_addr)))
}

func maskzExpandloaduPs1(k uint8, mem_addr uintptr) [8]float32


// MaskExpandloaduPs2: Load contiguous active single-precision (32-bit)
// floating-point elements from unaligned memory at 'mem_addr' (those with
// their respective bit set in mask 'k'), and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		m := 0
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VEXPANDPS'. Intrinsic: '_mm512_mask_expandloadu_ps'.
// Requires AVX512F.
func MaskExpandloaduPs2(src M512, k Mmask16, mem_addr uintptr) M512 {
	return M512(maskExpandloaduPs2([16]float32(src), uint16(k), uintptr(mem_addr)))
}

func maskExpandloaduPs2(src [16]float32, k uint16, mem_addr uintptr) [16]float32


// MaskzExpandloaduPs2: Load contiguous active single-precision (32-bit)
// floating-point elements from unaligned memory at 'mem_addr' (those with
// their respective bit set in mask 'k'), and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		m := 0
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VEXPANDPS'. Intrinsic: '_mm512_maskz_expandloadu_ps'.
// Requires AVX512F.
func MaskzExpandloaduPs2(k Mmask16, mem_addr uintptr) M512 {
	return M512(maskzExpandloaduPs2(uint16(k), uintptr(mem_addr)))
}

func maskzExpandloaduPs2(k uint16, mem_addr uintptr) [16]float32


// Expm1Pd: Compute the exponential value of 'e' raised to the power of packed
// double-precision (64-bit) floating-point elements in 'a', subtract one from
// each element, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := e^(a[i+63:i]) - 1.0
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_expm1_pd'.
// Requires AVX512F.
func Expm1Pd(a M512d) M512d {
	return M512d(expm1Pd([8]float64(a)))
}

func expm1Pd(a [8]float64) [8]float64


// MaskExpm1Pd: Compute the exponential value of 'e' raised to the power of
// packed double-precision (64-bit) floating-point elements in 'a', subtract
// one from each element, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := e^(a[i+63:i]) - 1.0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_expm1_pd'.
// Requires AVX512F.
func MaskExpm1Pd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskExpm1Pd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskExpm1Pd(src [8]float64, k uint8, a [8]float64) [8]float64


// Expm1Ps: Compute the exponential value of 'e' raised to the power of packed
// single-precision (32-bit) floating-point elements in 'a', subtract one from
// each element, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := e^(a[i+31:i]) - 1.0
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_expm1_ps'.
// Requires AVX512F.
func Expm1Ps(a M512) M512 {
	return M512(expm1Ps([16]float32(a)))
}

func expm1Ps(a [16]float32) [16]float32


// MaskExpm1Ps: Compute the exponential value of 'e' raised to the power of
// packed single-precision (32-bit) floating-point elements in 'a', subtract
// one from each element, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := e^(a[i+31:i]) - 1.0
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_expm1_ps'.
// Requires AVX512F.
func MaskExpm1Ps(src M512, k Mmask16, a M512) M512 {
	return M512(maskExpm1Ps([16]float32(src), uint16(k), [16]float32(a)))
}

func maskExpm1Ps(src [16]float32, k uint16, a [16]float32) [16]float32


// Extractf32x4Ps: Extract 128 bits (composed of 4 packed single-precision
// (32-bit) floating-point elements) from 'a', selected with 'imm8', and store
// the result in 'dst'. 
//
//		CASE imm8[7:0] of
//		0: dst[127:0] := a[127:0]
//		1: dst[127:0] := a[255:128]
//		ESAC
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTF32X4'. Intrinsic: '_mm256_extractf32x4_ps'.
// Requires AVX512F.
func Extractf32x4Ps(a M256, imm8 int) M128 {
	return M128(extractf32x4Ps([8]float32(a), imm8))
}

func extractf32x4Ps(a [8]float32, imm8 int) [4]float32


// MaskExtractf32x4Ps: Extract 128 bits (composed of 4 packed single-precision
// (32-bit) floating-point elements) from 'a', selected with 'imm8', and store
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		CASE imm8[7:0] of
//		0: tmp[127:0] := a[127:0]
//		1: tmp[127:0] := a[255:128]
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTF32X4'. Intrinsic: '_mm256_mask_extractf32x4_ps'.
// Requires AVX512F.
func MaskExtractf32x4Ps(src M128, k Mmask8, a M256, imm8 int) M128 {
	return M128(maskExtractf32x4Ps([4]float32(src), uint8(k), [8]float32(a), imm8))
}

func maskExtractf32x4Ps(src [4]float32, k uint8, a [8]float32, imm8 int) [4]float32


// MaskzExtractf32x4Ps: Extract 128 bits (composed of 4 packed single-precision
// (32-bit) floating-point elements) from 'a', selected with 'imm8', and store
// the results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		CASE imm8[7:0] of
//		0: tmp[127:0] := a[127:0]
//		1: tmp[127:0] := a[255:128]
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTF32X4'. Intrinsic: '_mm256_maskz_extractf32x4_ps'.
// Requires AVX512F.
func MaskzExtractf32x4Ps(k Mmask8, a M256, imm8 int) M128 {
	return M128(maskzExtractf32x4Ps(uint8(k), [8]float32(a), imm8))
}

func maskzExtractf32x4Ps(k uint8, a [8]float32, imm8 int) [4]float32


// Extractf32x4Ps1: Extract 128 bits (composed of 4 packed single-precision
// (32-bit) floating-point elements) from 'a', selected with 'imm8', and store
// the result in 'dst'. 
//
//		CASE imm8[7:0] of
//		0: dst[127:0] := a[127:0]
//		1: dst[127:0] := a[255:128]
//		2: dst[127:0] := a[383:256]
//		3: dst[127:0] := a[511:384]
//		ESAC
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTF32X4'. Intrinsic: '_mm512_extractf32x4_ps'.
// Requires AVX512F.
func Extractf32x4Ps1(a M512, imm8 int) M128 {
	return M128(extractf32x4Ps1([16]float32(a), imm8))
}

func extractf32x4Ps1(a [16]float32, imm8 int) [4]float32


// MaskExtractf32x4Ps1: Extract 128 bits (composed of 4 packed single-precision
// (32-bit) floating-point elements) from 'a', selected with 'imm8', and store
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		CASE imm8[7:0] of
//		0: tmp[127:0] := a[127:0]
//		1: tmp[127:0] := a[255:128]
//		2: tmp[127:0] := a[383:256]
//		3: tmp[127:0] := a[511:384]
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTF32X4'. Intrinsic: '_mm512_mask_extractf32x4_ps'.
// Requires AVX512F.
func MaskExtractf32x4Ps1(src M128, k Mmask8, a M512, imm8 int) M128 {
	return M128(maskExtractf32x4Ps1([4]float32(src), uint8(k), [16]float32(a), imm8))
}

func maskExtractf32x4Ps1(src [4]float32, k uint8, a [16]float32, imm8 int) [4]float32


// MaskzExtractf32x4Ps1: Extract 128 bits (composed of 4 packed
// single-precision (32-bit) floating-point elements) from 'a', selected with
// 'imm8', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		CASE imm8[7:0] of
//		0: tmp[127:0] := a[127:0]
//		1: tmp[127:0] := a[255:128]
//		2: tmp[127:0] := a[383:256]
//		3: tmp[127:0] := a[511:384]
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTF32X4'. Intrinsic: '_mm512_maskz_extractf32x4_ps'.
// Requires AVX512F.
func MaskzExtractf32x4Ps1(k Mmask8, a M512, imm8 int) M128 {
	return M128(maskzExtractf32x4Ps1(uint8(k), [16]float32(a), imm8))
}

func maskzExtractf32x4Ps1(k uint8, a [16]float32, imm8 int) [4]float32


// Extractf64x4Pd: Extract 256 bits (composed of 4 packed double-precision
// (64-bit) floating-point elements) from 'a', selected with 'imm8', and store
// the result in 'dst'. 
//
//		CASE imm8[7:0] of
//		0: dst[255:0] := a[255:0]
//		1: dst[255:0] := a[511:256]
//		ESAC
//		dst[MAX:256] := 0
//
// Instruction: 'VEXTRACTF64X4'. Intrinsic: '_mm512_extractf64x4_pd'.
// Requires AVX512F.
func Extractf64x4Pd(a M512d, imm8 int) M256d {
	return M256d(extractf64x4Pd([8]float64(a), imm8))
}

func extractf64x4Pd(a [8]float64, imm8 int) [4]float64


// MaskExtractf64x4Pd: Extract 256 bits (composed of 4 packed double-precision
// (64-bit) floating-point elements) from 'a', selected with 'imm8', and store
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		CASE imm8[7:0] of
//		0: tmp[255:0] := a[255:0]
//		1: tmp[255:0] := a[511:256]
//		ESAC
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VEXTRACTF64X4'. Intrinsic: '_mm512_mask_extractf64x4_pd'.
// Requires AVX512F.
func MaskExtractf64x4Pd(src M256d, k Mmask8, a M512d, imm8 int) M256d {
	return M256d(maskExtractf64x4Pd([4]float64(src), uint8(k), [8]float64(a), imm8))
}

func maskExtractf64x4Pd(src [4]float64, k uint8, a [8]float64, imm8 int) [4]float64


// MaskzExtractf64x4Pd: Extract 256 bits (composed of 4 packed double-precision
// (64-bit) floating-point elements) from 'a', selected with 'imm8', and store
// the results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		CASE imm8[7:0] of
//		0: tmp[255:0] := a[255:0]
//		1: tmp[255:0] := a[511:256]
//		ESAC
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VEXTRACTF64X4'. Intrinsic: '_mm512_maskz_extractf64x4_pd'.
// Requires AVX512F.
func MaskzExtractf64x4Pd(k Mmask8, a M512d, imm8 int) M256d {
	return M256d(maskzExtractf64x4Pd(uint8(k), [8]float64(a), imm8))
}

func maskzExtractf64x4Pd(k uint8, a [8]float64, imm8 int) [4]float64


// Extracti32x4Epi32: Extract 128 bits (composed of 4 packed 32-bit integers)
// from 'a', selected with 'imm8', and store the result in 'dst'. 
//
//		CASE imm8[7:0] of
//		0: dst[127:0] := a[127:0]
//		1: dst[127:0] := a[255:128]
//		ESAC
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTI32X4'. Intrinsic: '_mm256_extracti32x4_epi32'.
// Requires AVX512F.
func Extracti32x4Epi32(a M256i, imm8 int) M128i {
	return M128i(extracti32x4Epi32([32]byte(a), imm8))
}

func extracti32x4Epi32(a [32]byte, imm8 int) [16]byte


// MaskExtracti32x4Epi32: Extract 128 bits (composed of 4 packed 32-bit
// integers) from 'a', selected with 'imm8', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		CASE imm8[7:0] of
//		0: dst[127:0] := a[127:0]
//		1: dst[127:0] := a[255:128]
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTI32X4'. Intrinsic: '_mm256_mask_extracti32x4_epi32'.
// Requires AVX512F.
func MaskExtracti32x4Epi32(src M128i, k Mmask8, a M256i, imm8 int) M128i {
	return M128i(maskExtracti32x4Epi32([16]byte(src), uint8(k), [32]byte(a), imm8))
}

func maskExtracti32x4Epi32(src [16]byte, k uint8, a [32]byte, imm8 int) [16]byte


// MaskzExtracti32x4Epi32: Extract 128 bits (composed of 4 packed 32-bit
// integers) from 'a', selected with 'imm8', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		CASE imm8[7:0] of
//		0: dst[127:0] := a[127:0]
//		1: dst[127:0] := a[255:128]
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTI32X4'. Intrinsic: '_mm256_maskz_extracti32x4_epi32'.
// Requires AVX512F.
func MaskzExtracti32x4Epi32(k Mmask8, a M256i, imm8 int) M128i {
	return M128i(maskzExtracti32x4Epi32(uint8(k), [32]byte(a), imm8))
}

func maskzExtracti32x4Epi32(k uint8, a [32]byte, imm8 int) [16]byte


// Extracti32x4Epi321: Extract 128 bits (composed of 4 packed 32-bit integers)
// from 'a', selected with 'imm8', and store the result in 'dst'. 
//
//		CASE imm8[7:0] of
//		0: dst[127:0] := a[127:0]
//		1: dst[127:0] := a[255:128]
//		2: dst[127:0] := a[383:256]
//		3: dst[127:0] := a[511:384]
//		ESAC
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTI32X4'. Intrinsic: '_mm512_extracti32x4_epi32'.
// Requires AVX512F.
func Extracti32x4Epi321(a M512i, imm8 int) M128i {
	return M128i(extracti32x4Epi321([64]byte(a), imm8))
}

func extracti32x4Epi321(a [64]byte, imm8 int) [16]byte


// MaskExtracti32x4Epi321: Extract 128 bits (composed of 4 packed 32-bit
// integers) from 'a', selected with 'imm8', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		CASE imm8[7:0] of
//		0: dst[127:0] := a[127:0]
//		1: dst[127:0] := a[255:128]
//		2: dst[127:0] := a[383:256]
//		3: dst[127:0] := a[511:384]
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTI32X4'. Intrinsic: '_mm512_mask_extracti32x4_epi32'.
// Requires AVX512F.
func MaskExtracti32x4Epi321(src M128i, k Mmask8, a M512i, imm8 int) M128i {
	return M128i(maskExtracti32x4Epi321([16]byte(src), uint8(k), [64]byte(a), imm8))
}

func maskExtracti32x4Epi321(src [16]byte, k uint8, a [64]byte, imm8 int) [16]byte


// MaskzExtracti32x4Epi321: Extract 128 bits (composed of 4 packed 32-bit
// integers) from 'a', selected with 'imm8', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		CASE imm8[7:0] of
//		0: dst[127:0] := a[127:0]
//		1: dst[127:0] := a[255:128]
//		2: dst[127:0] := a[383:256]
//		3: dst[127:0] := a[511:384]
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTI32X4'. Intrinsic: '_mm512_maskz_extracti32x4_epi32'.
// Requires AVX512F.
func MaskzExtracti32x4Epi321(k Mmask8, a M512i, imm8 int) M128i {
	return M128i(maskzExtracti32x4Epi321(uint8(k), [64]byte(a), imm8))
}

func maskzExtracti32x4Epi321(k uint8, a [64]byte, imm8 int) [16]byte


// Extracti64x4Epi64: Extract 256 bits (composed of 4 packed 64-bit integers)
// from 'a', selected with 'imm8', and store the result in 'dst'. 
//
//		CASE imm8[7:0] of
//		0: dst[255:0] := a[255:0]
//		1: dst[255:0] := a[511:256]
//		ESAC
//		dst[MAX:256] := 0
//
// Instruction: 'VEXTRACTI64X4'. Intrinsic: '_mm512_extracti64x4_epi64'.
// Requires AVX512F.
func Extracti64x4Epi64(a M512i, imm8 int) M256i {
	return M256i(extracti64x4Epi64([64]byte(a), imm8))
}

func extracti64x4Epi64(a [64]byte, imm8 int) [32]byte


// MaskExtracti64x4Epi64: Extract 256 bits (composed of 4 packed 64-bit
// integers) from 'a', selected with 'imm8', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		CASE imm8[7:0] of
//		0: dst[255:0] := a[255:0]
//		1: dst[255:0] := a[511:256]
//		ESAC
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VEXTRACTI64X4'. Intrinsic: '_mm512_mask_extracti64x4_epi64'.
// Requires AVX512F.
func MaskExtracti64x4Epi64(src M256i, k Mmask8, a M512i, imm8 int) M256i {
	return M256i(maskExtracti64x4Epi64([32]byte(src), uint8(k), [64]byte(a), imm8))
}

func maskExtracti64x4Epi64(src [32]byte, k uint8, a [64]byte, imm8 int) [32]byte


// MaskzExtracti64x4Epi64: Extract 256 bits (composed of 4 packed 64-bit
// integers) from 'a', selected with 'imm8', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		CASE imm8[7:0] of
//		0: dst[255:0] := a[255:0]
//		1: dst[255:0] := a[511:256]
//		ESAC
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VEXTRACTI64X4'. Intrinsic: '_mm512_maskz_extracti64x4_epi64'.
// Requires AVX512F.
func MaskzExtracti64x4Epi64(k Mmask8, a M512i, imm8 int) M256i {
	return M256i(maskzExtracti64x4Epi64(uint8(k), [64]byte(a), imm8))
}

func maskzExtracti64x4Epi64(k uint8, a [64]byte, imm8 int) [32]byte


// FixupimmPd: Fix up packed double-precision (64-bit) floating-point elements
// in 'a' and 'b' using packed 64-bit integers in 'c', and store the results in
// 'dst'. 'imm8' is used to set the required flags reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMPD'. Intrinsic: '_mm_fixupimm_pd'.
// Requires AVX512F.
func FixupimmPd(a M128d, b M128d, c M128i, imm8 int) M128d {
	return M128d(fixupimmPd([2]float64(a), [2]float64(b), [16]byte(c), imm8))
}

func fixupimmPd(a [2]float64, b [2]float64, c [16]byte, imm8 int) [2]float64


// MaskFixupimmPd: Fix up packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' using packed 64-bit integers in 'c', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMPD'. Intrinsic: '_mm_mask_fixupimm_pd'.
// Requires AVX512F.
func MaskFixupimmPd(a M128d, k Mmask8, b M128d, c M128i, imm8 int) M128d {
	return M128d(maskFixupimmPd([2]float64(a), uint8(k), [2]float64(b), [16]byte(c), imm8))
}

func maskFixupimmPd(a [2]float64, k uint8, b [2]float64, c [16]byte, imm8 int) [2]float64


// MaskzFixupimmPd: Fix up packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' using packed 64-bit integers in 'c', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMPD'. Intrinsic: '_mm_maskz_fixupimm_pd'.
// Requires AVX512F.
func MaskzFixupimmPd(k Mmask8, a M128d, b M128d, c M128i, imm8 int) M128d {
	return M128d(maskzFixupimmPd(uint8(k), [2]float64(a), [2]float64(b), [16]byte(c), imm8))
}

func maskzFixupimmPd(k uint8, a [2]float64, b [2]float64, c [16]byte, imm8 int) [2]float64


// FixupimmPd1: Fix up packed double-precision (64-bit) floating-point elements
// in 'a' and 'b' using packed 64-bit integers in 'c', and store the results in
// 'dst'. 'imm8' is used to set the required flags reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFIXUPIMMPD'. Intrinsic: '_mm256_fixupimm_pd'.
// Requires AVX512F.
func FixupimmPd1(a M256d, b M256d, c M256i, imm8 int) M256d {
	return M256d(fixupimmPd1([4]float64(a), [4]float64(b), [32]byte(c), imm8))
}

func fixupimmPd1(a [4]float64, b [4]float64, c [32]byte, imm8 int) [4]float64


// MaskFixupimmPd1: Fix up packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' using packed 64-bit integers in 'c', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFIXUPIMMPD'. Intrinsic: '_mm256_mask_fixupimm_pd'.
// Requires AVX512F.
func MaskFixupimmPd1(a M256d, k Mmask8, b M256d, c M256i, imm8 int) M256d {
	return M256d(maskFixupimmPd1([4]float64(a), uint8(k), [4]float64(b), [32]byte(c), imm8))
}

func maskFixupimmPd1(a [4]float64, k uint8, b [4]float64, c [32]byte, imm8 int) [4]float64


// MaskzFixupimmPd1: Fix up packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' using packed 64-bit integers in 'c', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFIXUPIMMPD'. Intrinsic: '_mm256_maskz_fixupimm_pd'.
// Requires AVX512F.
func MaskzFixupimmPd1(k Mmask8, a M256d, b M256d, c M256i, imm8 int) M256d {
	return M256d(maskzFixupimmPd1(uint8(k), [4]float64(a), [4]float64(b), [32]byte(c), imm8))
}

func maskzFixupimmPd1(k uint8, a [4]float64, b [4]float64, c [32]byte, imm8 int) [4]float64


// FixupimmPd2: Fix up packed double-precision (64-bit) floating-point elements
// in 'a' and 'b' using packed 64-bit integers in 'c', and store the results in
// 'dst'. 'imm8' is used to set the required flags reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFIXUPIMMPD'. Intrinsic: '_mm512_fixupimm_pd'.
// Requires AVX512F.
func FixupimmPd2(a M512d, b M512d, c M512i, imm8 int) M512d {
	return M512d(fixupimmPd2([8]float64(a), [8]float64(b), [64]byte(c), imm8))
}

func fixupimmPd2(a [8]float64, b [8]float64, c [64]byte, imm8 int) [8]float64


// MaskFixupimmPd2: Fix up packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' using packed 64-bit integers in 'c', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFIXUPIMMPD'. Intrinsic: '_mm512_mask_fixupimm_pd'.
// Requires AVX512F.
func MaskFixupimmPd2(a M512d, k Mmask8, b M512d, c M512i, imm8 int) M512d {
	return M512d(maskFixupimmPd2([8]float64(a), uint8(k), [8]float64(b), [64]byte(c), imm8))
}

func maskFixupimmPd2(a [8]float64, k uint8, b [8]float64, c [64]byte, imm8 int) [8]float64


// MaskzFixupimmPd2: Fix up packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' using packed 64-bit integers in 'c', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFIXUPIMMPD'. Intrinsic: '_mm512_maskz_fixupimm_pd'.
// Requires AVX512F.
func MaskzFixupimmPd2(k Mmask8, a M512d, b M512d, c M512i, imm8 int) M512d {
	return M512d(maskzFixupimmPd2(uint8(k), [8]float64(a), [8]float64(b), [64]byte(c), imm8))
}

func maskzFixupimmPd2(k uint8, a [8]float64, b [8]float64, c [64]byte, imm8 int) [8]float64


// FixupimmPs: Fix up packed single-precision (32-bit) floating-point elements
// in 'a' and 'b' using packed 32-bit integers in 'c', and store the results in
// 'dst'. 'imm8' is used to set the required flags reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMPS'. Intrinsic: '_mm_fixupimm_ps'.
// Requires AVX512F.
func FixupimmPs(a M128, b M128, c M128i, imm8 int) M128 {
	return M128(fixupimmPs([4]float32(a), [4]float32(b), [16]byte(c), imm8))
}

func fixupimmPs(a [4]float32, b [4]float32, c [16]byte, imm8 int) [4]float32


// MaskFixupimmPs: Fix up packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' using packed 32-bit integers in 'c', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMPS'. Intrinsic: '_mm_mask_fixupimm_ps'.
// Requires AVX512F.
func MaskFixupimmPs(a M128, k Mmask8, b M128, c M128i, imm8 int) M128 {
	return M128(maskFixupimmPs([4]float32(a), uint8(k), [4]float32(b), [16]byte(c), imm8))
}

func maskFixupimmPs(a [4]float32, k uint8, b [4]float32, c [16]byte, imm8 int) [4]float32


// MaskzFixupimmPs: Fix up packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' using packed 32-bit integers in 'c', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMPS'. Intrinsic: '_mm_maskz_fixupimm_ps'.
// Requires AVX512F.
func MaskzFixupimmPs(k Mmask8, a M128, b M128, c M128i, imm8 int) M128 {
	return M128(maskzFixupimmPs(uint8(k), [4]float32(a), [4]float32(b), [16]byte(c), imm8))
}

func maskzFixupimmPs(k uint8, a [4]float32, b [4]float32, c [16]byte, imm8 int) [4]float32


// FixupimmPs1: Fix up packed single-precision (32-bit) floating-point elements
// in 'a' and 'b' using packed 32-bit integers in 'c', and store the results in
// 'dst'. 'imm8' is used to set the required flags reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFIXUPIMMPS'. Intrinsic: '_mm256_fixupimm_ps'.
// Requires AVX512F.
func FixupimmPs1(a M256, b M256, c M256i, imm8 int) M256 {
	return M256(fixupimmPs1([8]float32(a), [8]float32(b), [32]byte(c), imm8))
}

func fixupimmPs1(a [8]float32, b [8]float32, c [32]byte, imm8 int) [8]float32


// MaskFixupimmPs1: Fix up packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' using packed 32-bit integers in 'c', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFIXUPIMMPS'. Intrinsic: '_mm256_mask_fixupimm_ps'.
// Requires AVX512F.
func MaskFixupimmPs1(a M256, k Mmask8, b M256, c M256i, imm8 int) M256 {
	return M256(maskFixupimmPs1([8]float32(a), uint8(k), [8]float32(b), [32]byte(c), imm8))
}

func maskFixupimmPs1(a [8]float32, k uint8, b [8]float32, c [32]byte, imm8 int) [8]float32


// MaskzFixupimmPs1: Fix up packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' using packed 32-bit integers in 'c', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFIXUPIMMPS'. Intrinsic: '_mm256_maskz_fixupimm_ps'.
// Requires AVX512F.
func MaskzFixupimmPs1(k Mmask8, a M256, b M256, c M256i, imm8 int) M256 {
	return M256(maskzFixupimmPs1(uint8(k), [8]float32(a), [8]float32(b), [32]byte(c), imm8))
}

func maskzFixupimmPs1(k uint8, a [8]float32, b [8]float32, c [32]byte, imm8 int) [8]float32


// FixupimmPs2: Fix up packed single-precision (32-bit) floating-point elements
// in 'a' and 'b' using packed 32-bit integers in 'c', and store the results in
// 'dst'. 'imm8' is used to set the required flags reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFIXUPIMMPS'. Intrinsic: '_mm512_fixupimm_ps'.
// Requires AVX512F.
func FixupimmPs2(a M512, b M512, c M512i, imm8 int) M512 {
	return M512(fixupimmPs2([16]float32(a), [16]float32(b), [64]byte(c), imm8))
}

func fixupimmPs2(a [16]float32, b [16]float32, c [64]byte, imm8 int) [16]float32


// MaskFixupimmPs2: Fix up packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' using packed 32-bit integers in 'c', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFIXUPIMMPS'. Intrinsic: '_mm512_mask_fixupimm_ps'.
// Requires AVX512F.
func MaskFixupimmPs2(a M512, k Mmask16, b M512, c M512i, imm8 int) M512 {
	return M512(maskFixupimmPs2([16]float32(a), uint16(k), [16]float32(b), [64]byte(c), imm8))
}

func maskFixupimmPs2(a [16]float32, k uint16, b [16]float32, c [64]byte, imm8 int) [16]float32


// MaskzFixupimmPs2: Fix up packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' using packed 32-bit integers in 'c', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFIXUPIMMPS'. Intrinsic: '_mm512_maskz_fixupimm_ps'.
// Requires AVX512F.
func MaskzFixupimmPs2(k Mmask16, a M512, b M512, c M512i, imm8 int) M512 {
	return M512(maskzFixupimmPs2(uint16(k), [16]float32(a), [16]float32(b), [64]byte(c), imm8))
}

func maskzFixupimmPs2(k uint16, a [16]float32, b [16]float32, c [64]byte, imm8 int) [16]float32


// FixupimmRoundPd: Fix up packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' using packed 64-bit integers in 'c', and store the
// results in 'dst'. 'imm8' is used to set the required flags reporting.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFIXUPIMMPD'. Intrinsic: '_mm512_fixupimm_round_pd'.
// Requires AVX512F.
func FixupimmRoundPd(a M512d, b M512d, c M512i, imm8 int, rounding int) M512d {
	return M512d(fixupimmRoundPd([8]float64(a), [8]float64(b), [64]byte(c), imm8, rounding))
}

func fixupimmRoundPd(a [8]float64, b [8]float64, c [64]byte, imm8 int, rounding int) [8]float64


// MaskFixupimmRoundPd: Fix up packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' using packed 64-bit integers in 'c', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFIXUPIMMPD'. Intrinsic: '_mm512_mask_fixupimm_round_pd'.
// Requires AVX512F.
func MaskFixupimmRoundPd(a M512d, k Mmask8, b M512d, c M512i, imm8 int, rounding int) M512d {
	return M512d(maskFixupimmRoundPd([8]float64(a), uint8(k), [8]float64(b), [64]byte(c), imm8, rounding))
}

func maskFixupimmRoundPd(a [8]float64, k uint8, b [8]float64, c [64]byte, imm8 int, rounding int) [8]float64


// MaskzFixupimmRoundPd: Fix up packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' using packed 64-bit integers in 'c', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFIXUPIMMPD'. Intrinsic: '_mm512_maskz_fixupimm_round_pd'.
// Requires AVX512F.
func MaskzFixupimmRoundPd(k Mmask8, a M512d, b M512d, c M512i, imm8 int, rounding int) M512d {
	return M512d(maskzFixupimmRoundPd(uint8(k), [8]float64(a), [8]float64(b), [64]byte(c), imm8, rounding))
}

func maskzFixupimmRoundPd(k uint8, a [8]float64, b [8]float64, c [64]byte, imm8 int, rounding int) [8]float64


// FixupimmRoundPs: Fix up packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' using packed 32-bit integers in 'c', and store the
// results in 'dst'. 'imm8' is used to set the required flags reporting.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFIXUPIMMPS'. Intrinsic: '_mm512_fixupimm_round_ps'.
// Requires AVX512F.
func FixupimmRoundPs(a M512, b M512, c M512i, imm8 int, rounding int) M512 {
	return M512(fixupimmRoundPs([16]float32(a), [16]float32(b), [64]byte(c), imm8, rounding))
}

func fixupimmRoundPs(a [16]float32, b [16]float32, c [64]byte, imm8 int, rounding int) [16]float32


// MaskFixupimmRoundPs: Fix up packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' using packed 32-bit integers in 'c', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFIXUPIMMPS'. Intrinsic: '_mm512_mask_fixupimm_round_ps'.
// Requires AVX512F.
func MaskFixupimmRoundPs(a M512, k Mmask16, b M512, c M512i, imm8 int, rounding int) M512 {
	return M512(maskFixupimmRoundPs([16]float32(a), uint16(k), [16]float32(b), [64]byte(c), imm8, rounding))
}

func maskFixupimmRoundPs(a [16]float32, k uint16, b [16]float32, c [64]byte, imm8 int, rounding int) [16]float32


// MaskzFixupimmRoundPs: Fix up packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' using packed 32-bit integers in 'c', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFIXUPIMMPS'. Intrinsic: '_mm512_maskz_fixupimm_round_ps'.
// Requires AVX512F.
func MaskzFixupimmRoundPs(k Mmask16, a M512, b M512, c M512i, imm8 int, rounding int) M512 {
	return M512(maskzFixupimmRoundPs(uint16(k), [16]float32(a), [16]float32(b), [64]byte(c), imm8, rounding))
}

func maskzFixupimmRoundPs(k uint16, a [16]float32, b [16]float32, c [64]byte, imm8 int, rounding int) [16]float32


// FixupimmRoundSd: Fix up the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b' using the lower 64-bit integer in 'c', store the
// result in the lower element of 'dst', and copy the upper element from 'a' to
// the upper element of 'dst'. 'imm8' is used to set the required flags
// reporting.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		dst[63:0] := FIXUPIMMPD(a[63:0], b[63:0], c[63:0], imm8[7:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMSD'. Intrinsic: '_mm_fixupimm_round_sd'.
// Requires AVX512F.
func FixupimmRoundSd(a M128d, b M128d, c M128i, imm8 int, rounding int) M128d {
	return M128d(fixupimmRoundSd([2]float64(a), [2]float64(b), [16]byte(c), imm8, rounding))
}

func fixupimmRoundSd(a [2]float64, b [2]float64, c [16]byte, imm8 int, rounding int) [2]float64


// MaskFixupimmRoundSd: Fix up the lower double-precision (64-bit)
// floating-point elements in 'a' and 'b' using the lower 64-bit integer in
// 'c', store the result in the lower element of 'dst' using writemask 'k' (the
// element is copied from 'a' when mask bit 0 is not set), and copy the upper
// element from 'a' to the upper element of 'dst'. 'imm8' is used to set the
// required flags reporting.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		IF k[0]
//			dst[63:0] := FIXUPIMMPD(a[63:0], b[63:0], c[63:0], imm8[7:0])
//		ELSE
//			dst[63:0] := a[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMSD'. Intrinsic: '_mm_mask_fixupimm_round_sd'.
// Requires AVX512F.
func MaskFixupimmRoundSd(a M128d, k Mmask8, b M128d, c M128i, imm8 int, rounding int) M128d {
	return M128d(maskFixupimmRoundSd([2]float64(a), uint8(k), [2]float64(b), [16]byte(c), imm8, rounding))
}

func maskFixupimmRoundSd(a [2]float64, k uint8, b [2]float64, c [16]byte, imm8 int, rounding int) [2]float64


// MaskzFixupimmRoundSd: Fix up the lower double-precision (64-bit)
// floating-point elements in 'a' and 'b' using the lower 64-bit integer in
// 'c', store the result in the lower element of 'dst' using zeromask 'k' (the
// element is zeroed out when mask bit 0 is not set), and copy the upper
// element from 'a' to the upper element of 'dst'. 'imm8' is used to set the
// required flags reporting.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		IF k[0]
//			dst[63:0] := FIXUPIMMPD(a[63:0], b[63:0], c[63:0], imm8[7:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMSD'. Intrinsic: '_mm_maskz_fixupimm_round_sd'.
// Requires AVX512F.
func MaskzFixupimmRoundSd(k Mmask8, a M128d, b M128d, c M128i, imm8 int, rounding int) M128d {
	return M128d(maskzFixupimmRoundSd(uint8(k), [2]float64(a), [2]float64(b), [16]byte(c), imm8, rounding))
}

func maskzFixupimmRoundSd(k uint8, a [2]float64, b [2]float64, c [16]byte, imm8 int, rounding int) [2]float64


// FixupimmRoundSs: Fix up the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b' using the lower 32-bit integer in 'c', store the
// result in the lower element of 'dst', and copy the upper 3 packed elements
// from 'a' to the upper elements of 'dst'. 'imm8' is used to set the required
// flags reporting.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		dst[31:0] := FIXUPIMMPD(a[31:0], b[31:0], c[31:0], imm8[7:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMSS'. Intrinsic: '_mm_fixupimm_round_ss'.
// Requires AVX512F.
func FixupimmRoundSs(a M128, b M128, c M128i, imm8 int, rounding int) M128 {
	return M128(fixupimmRoundSs([4]float32(a), [4]float32(b), [16]byte(c), imm8, rounding))
}

func fixupimmRoundSs(a [4]float32, b [4]float32, c [16]byte, imm8 int, rounding int) [4]float32


// MaskFixupimmRoundSs: Fix up the lower single-precision (32-bit)
// floating-point elements in 'a' and 'b' using the lower 32-bit integer in
// 'c', store the result in the lower element of 'dst' using writemask 'k' (the
// element is copied from 'a' when mask bit 0 is not set), and copy the upper 3
// packed elements from 'a' to the upper elements of 'dst'. 'imm8' is used to
// set the required flags reporting.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		IF k[0]
//			dst[31:0] := FIXUPIMMPD(a[31:0], b[31:0], c[31:0], imm8[7:0])
//		ELSE
//			dst[31:0] := a[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMSS'. Intrinsic: '_mm_mask_fixupimm_round_ss'.
// Requires AVX512F.
func MaskFixupimmRoundSs(a M128, k Mmask8, b M128, c M128i, imm8 int, rounding int) M128 {
	return M128(maskFixupimmRoundSs([4]float32(a), uint8(k), [4]float32(b), [16]byte(c), imm8, rounding))
}

func maskFixupimmRoundSs(a [4]float32, k uint8, b [4]float32, c [16]byte, imm8 int, rounding int) [4]float32


// MaskzFixupimmRoundSs: Fix up the lower single-precision (32-bit)
// floating-point elements in 'a' and 'b' using the lower 32-bit integer in
// 'c', store the result in the lower element of 'dst' using zeromask 'k' (the
// element is zeroed out when mask bit 0 is not set), and copy the upper 3
// packed elements from 'a' to the upper elements of 'dst'. 'imm8' is used to
// set the required flags reporting.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		IF k[0]
//			dst[31:0] := FIXUPIMMPD(a[31:0], b[31:0], c[31:0], imm8[7:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMSS'. Intrinsic: '_mm_maskz_fixupimm_round_ss'.
// Requires AVX512F.
func MaskzFixupimmRoundSs(k Mmask8, a M128, b M128, c M128i, imm8 int, rounding int) M128 {
	return M128(maskzFixupimmRoundSs(uint8(k), [4]float32(a), [4]float32(b), [16]byte(c), imm8, rounding))
}

func maskzFixupimmRoundSs(k uint8, a [4]float32, b [4]float32, c [16]byte, imm8 int, rounding int) [4]float32


// FixupimmSd: Fix up the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b' using the lower 64-bit integer in 'c', store the
// result in the lower element of 'dst', and copy the upper element from 'a' to
// the upper element of 'dst'. 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		dst[63:0] := FIXUPIMMPD(a[63:0], b[63:0], c[63:0], imm8[7:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMSD'. Intrinsic: '_mm_fixupimm_sd'.
// Requires AVX512F.
func FixupimmSd(a M128d, b M128d, c M128i, imm8 int) M128d {
	return M128d(fixupimmSd([2]float64(a), [2]float64(b), [16]byte(c), imm8))
}

func fixupimmSd(a [2]float64, b [2]float64, c [16]byte, imm8 int) [2]float64


// MaskFixupimmSd: Fix up the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b' using the lower 64-bit integer in 'c', store the
// result in the lower element of 'dst' using writemask 'k' (the element is
// copied from 'a' when mask bit 0 is not set), and copy the upper element from
// 'a' to the upper element of 'dst'. 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		IF k[0]
//			dst[63:0] := FIXUPIMMPD(a[63:0], b[63:0], c[63:0], imm8[7:0])
//		ELSE
//			dst[63:0] := a[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMSD'. Intrinsic: '_mm_mask_fixupimm_sd'.
// Requires AVX512F.
func MaskFixupimmSd(a M128d, k Mmask8, b M128d, c M128i, imm8 int) M128d {
	return M128d(maskFixupimmSd([2]float64(a), uint8(k), [2]float64(b), [16]byte(c), imm8))
}

func maskFixupimmSd(a [2]float64, k uint8, b [2]float64, c [16]byte, imm8 int) [2]float64


// MaskzFixupimmSd: Fix up the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b' using the lower 64-bit integer in 'c', store the
// result in the lower element of 'dst' using zeromask 'k' (the element is
// zeroed out when mask bit 0 is not set), and copy the upper element from 'a'
// to the upper element of 'dst'. 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		IF k[0]
//			dst[63:0] := FIXUPIMMPD(a[63:0], b[63:0], c[63:0], imm8[7:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMSD'. Intrinsic: '_mm_maskz_fixupimm_sd'.
// Requires AVX512F.
func MaskzFixupimmSd(k Mmask8, a M128d, b M128d, c M128i, imm8 int) M128d {
	return M128d(maskzFixupimmSd(uint8(k), [2]float64(a), [2]float64(b), [16]byte(c), imm8))
}

func maskzFixupimmSd(k uint8, a [2]float64, b [2]float64, c [16]byte, imm8 int) [2]float64


// FixupimmSs: Fix up the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b' using the lower 32-bit integer in 'c', store the
// result in the lower element of 'dst', and copy the upper 3 packed elements
// from 'a' to the upper elements of 'dst'. 'imm8' is used to set the required
// flags reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		dst[31:0] := FIXUPIMMPD(a[31:0], b[31:0], c[31:0], imm8[7:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMSS'. Intrinsic: '_mm_fixupimm_ss'.
// Requires AVX512F.
func FixupimmSs(a M128, b M128, c M128i, imm8 int) M128 {
	return M128(fixupimmSs([4]float32(a), [4]float32(b), [16]byte(c), imm8))
}

func fixupimmSs(a [4]float32, b [4]float32, c [16]byte, imm8 int) [4]float32


// MaskFixupimmSs: Fix up the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b' using the lower 32-bit integer in 'c', store the
// result in the lower element of 'dst' using writemask 'k' (the element is
// copied from 'a' when mask bit 0 is not set), and copy the upper 3 packed
// elements from 'a' to the upper elements of 'dst'. 'imm8' is used to set the
// required flags reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		IF k[0]
//			dst[31:0] := FIXUPIMMPD(a[31:0], b[31:0], c[31:0], imm8[7:0])
//		ELSE
//			dst[31:0] := a[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMSS'. Intrinsic: '_mm_mask_fixupimm_ss'.
// Requires AVX512F.
func MaskFixupimmSs(a M128, k Mmask8, b M128, c M128i, imm8 int) M128 {
	return M128(maskFixupimmSs([4]float32(a), uint8(k), [4]float32(b), [16]byte(c), imm8))
}

func maskFixupimmSs(a [4]float32, k uint8, b [4]float32, c [16]byte, imm8 int) [4]float32


// MaskzFixupimmSs: Fix up the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b' using the lower 32-bit integer in 'c', store the
// result in the lower element of 'dst' using zeromask 'k' (the element is
// zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements
// from 'a' to the upper elements of 'dst'. 'imm8' is used to set the required
// flags reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		IF k[0]
//			dst[31:0] := FIXUPIMMPD(a[31:0], b[31:0], c[31:0], imm8[7:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMSS'. Intrinsic: '_mm_maskz_fixupimm_ss'.
// Requires AVX512F.
func MaskzFixupimmSs(k Mmask8, a M128, b M128, c M128i, imm8 int) M128 {
	return M128(maskzFixupimmSs(uint8(k), [4]float32(a), [4]float32(b), [16]byte(c), imm8))
}

func maskzFixupimmSs(k uint8, a [4]float32, b [4]float32, c [16]byte, imm8 int) [4]float32


// FloorPd: Round the packed double-precision (64-bit) floating-point elements
// in 'a' down to an integer value, and store the results as packed
// double-precision floating-point elements in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := FLOOR(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_floor_pd'.
// Requires AVX512F.
func FloorPd(a M512d) M512d {
	return M512d(floorPd([8]float64(a)))
}

func floorPd(a [8]float64) [8]float64


// MaskFloorPd: Round the packed double-precision (64-bit) floating-point
// elements in 'a' down to an integer value, and store the results as packed
// double-precision floating-point elements in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := FLOOR(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_floor_pd'.
// Requires AVX512F.
func MaskFloorPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskFloorPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskFloorPd(src [8]float64, k uint8, a [8]float64) [8]float64


// FloorPs: Round the packed single-precision (32-bit) floating-point elements
// in 'a' down to an integer value, and store the results as packed
// single-precision floating-point elements in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := FLOOR(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_floor_ps'.
// Requires AVX512F.
func FloorPs(a M512) M512 {
	return M512(floorPs([16]float32(a)))
}

func floorPs(a [16]float32) [16]float32


// MaskFloorPs: Round the packed single-precision (32-bit) floating-point
// elements in 'a' down to an integer value, and store the results as packed
// single-precision floating-point elements in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := FLOOR(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_floor_ps'.
// Requires AVX512F.
func MaskFloorPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskFloorPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskFloorPs(src [16]float32, k uint16, a [16]float32) [16]float32


// MaskFmaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132PD, VFMADD213PD, VFMADD231PD'. Intrinsic: '_mm_mask_fmadd_pd'.
// Requires AVX512F.
func MaskFmaddPd(a M128d, k Mmask8, b M128d, c M128d) M128d {
	return M128d(maskFmaddPd([2]float64(a), uint8(k), [2]float64(b), [2]float64(c)))
}

func maskFmaddPd(a [2]float64, k uint8, b [2]float64, c [2]float64) [2]float64


// Mask3FmaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132PD, VFMADD213PD, VFMADD231PD'. Intrinsic: '_mm_mask3_fmadd_pd'.
// Requires AVX512F.
func Mask3FmaddPd(a M128d, b M128d, c M128d, k Mmask8) M128d {
	return M128d(mask3FmaddPd([2]float64(a), [2]float64(b), [2]float64(c), uint8(k)))
}

func mask3FmaddPd(a [2]float64, b [2]float64, c [2]float64, k uint8) [2]float64


// MaskzFmaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132PD, VFMADD213PD, VFMADD231PD'. Intrinsic: '_mm_maskz_fmadd_pd'.
// Requires AVX512F.
func MaskzFmaddPd(k Mmask8, a M128d, b M128d, c M128d) M128d {
	return M128d(maskzFmaddPd(uint8(k), [2]float64(a), [2]float64(b), [2]float64(c)))
}

func maskzFmaddPd(k uint8, a [2]float64, b [2]float64, c [2]float64) [2]float64


// MaskFmaddPd1: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADD132PD, VFMADD213PD, VFMADD231PD'. Intrinsic: '_mm256_mask_fmadd_pd'.
// Requires AVX512F.
func MaskFmaddPd1(a M256d, k Mmask8, b M256d, c M256d) M256d {
	return M256d(maskFmaddPd1([4]float64(a), uint8(k), [4]float64(b), [4]float64(c)))
}

func maskFmaddPd1(a [4]float64, k uint8, b [4]float64, c [4]float64) [4]float64


// Mask3FmaddPd1: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADD132PD, VFMADD213PD, VFMADD231PD'. Intrinsic: '_mm256_mask3_fmadd_pd'.
// Requires AVX512F.
func Mask3FmaddPd1(a M256d, b M256d, c M256d, k Mmask8) M256d {
	return M256d(mask3FmaddPd1([4]float64(a), [4]float64(b), [4]float64(c), uint8(k)))
}

func mask3FmaddPd1(a [4]float64, b [4]float64, c [4]float64, k uint8) [4]float64


// MaskzFmaddPd1: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADD132PD, VFMADD213PD, VFMADD231PD'. Intrinsic: '_mm256_maskz_fmadd_pd'.
// Requires AVX512F.
func MaskzFmaddPd1(k Mmask8, a M256d, b M256d, c M256d) M256d {
	return M256d(maskzFmaddPd1(uint8(k), [4]float64(a), [4]float64(b), [4]float64(c)))
}

func maskzFmaddPd1(k uint8, a [4]float64, b [4]float64, c [4]float64) [4]float64


// MaskzFmaddPd2: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADD132PD, VFMADD213PD, VFMADD231PD'. Intrinsic: '_mm512_maskz_fmadd_pd'.
// Requires AVX512F.
func MaskzFmaddPd2(k Mmask8, a M512d, b M512d, c M512d) M512d {
	return M512d(maskzFmaddPd2(uint8(k), [8]float64(a), [8]float64(b), [8]float64(c)))
}

func maskzFmaddPd2(k uint8, a [8]float64, b [8]float64, c [8]float64) [8]float64


// MaskFmaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132PS, VFMADD213PS, VFMADD231PS'. Intrinsic: '_mm_mask_fmadd_ps'.
// Requires AVX512F.
func MaskFmaddPs(a M128, k Mmask8, b M128, c M128) M128 {
	return M128(maskFmaddPs([4]float32(a), uint8(k), [4]float32(b), [4]float32(c)))
}

func maskFmaddPs(a [4]float32, k uint8, b [4]float32, c [4]float32) [4]float32


// Mask3FmaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132PS, VFMADD213PS, VFMADD231PS'. Intrinsic: '_mm_mask3_fmadd_ps'.
// Requires AVX512F.
func Mask3FmaddPs(a M128, b M128, c M128, k Mmask8) M128 {
	return M128(mask3FmaddPs([4]float32(a), [4]float32(b), [4]float32(c), uint8(k)))
}

func mask3FmaddPs(a [4]float32, b [4]float32, c [4]float32, k uint8) [4]float32


// MaskzFmaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132PS, VFMADD213PS, VFMADD231PS'. Intrinsic: '_mm_maskz_fmadd_ps'.
// Requires AVX512F.
func MaskzFmaddPs(k Mmask8, a M128, b M128, c M128) M128 {
	return M128(maskzFmaddPs(uint8(k), [4]float32(a), [4]float32(b), [4]float32(c)))
}

func maskzFmaddPs(k uint8, a [4]float32, b [4]float32, c [4]float32) [4]float32


// MaskFmaddPs1: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADD132PS, VFMADD213PS, VFMADD231PS'. Intrinsic: '_mm256_mask_fmadd_ps'.
// Requires AVX512F.
func MaskFmaddPs1(a M256, k Mmask8, b M256, c M256) M256 {
	return M256(maskFmaddPs1([8]float32(a), uint8(k), [8]float32(b), [8]float32(c)))
}

func maskFmaddPs1(a [8]float32, k uint8, b [8]float32, c [8]float32) [8]float32


// Mask3FmaddPs1: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADD132PS, VFMADD213PS, VFMADD231PS'. Intrinsic: '_mm256_mask3_fmadd_ps'.
// Requires AVX512F.
func Mask3FmaddPs1(a M256, b M256, c M256, k Mmask8) M256 {
	return M256(mask3FmaddPs1([8]float32(a), [8]float32(b), [8]float32(c), uint8(k)))
}

func mask3FmaddPs1(a [8]float32, b [8]float32, c [8]float32, k uint8) [8]float32


// MaskzFmaddPs1: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADD132PS, VFMADD213PS, VFMADD231PS'. Intrinsic: '_mm256_maskz_fmadd_ps'.
// Requires AVX512F.
func MaskzFmaddPs1(k Mmask8, a M256, b M256, c M256) M256 {
	return M256(maskzFmaddPs1(uint8(k), [8]float32(a), [8]float32(b), [8]float32(c)))
}

func maskzFmaddPs1(k uint8, a [8]float32, b [8]float32, c [8]float32) [8]float32


// MaskzFmaddPs2: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADD132PS, VFMADD213PS, VFMADD231PS'. Intrinsic: '_mm512_maskz_fmadd_ps'.
// Requires AVX512F.
func MaskzFmaddPs2(k Mmask16, a M512, b M512, c M512) M512 {
	return M512(maskzFmaddPs2(uint16(k), [16]float32(a), [16]float32(b), [16]float32(c)))
}

func maskzFmaddPs2(k uint16, a [16]float32, b [16]float32, c [16]float32) [16]float32


// MaskzFmaddRoundPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADD132PD, VFMADD213PD, VFMADD231PD'. Intrinsic: '_mm512_maskz_fmadd_round_pd'.
// Requires AVX512F.
func MaskzFmaddRoundPd(k Mmask8, a M512d, b M512d, c M512d, rounding int) M512d {
	return M512d(maskzFmaddRoundPd(uint8(k), [8]float64(a), [8]float64(b), [8]float64(c), rounding))
}

func maskzFmaddRoundPd(k uint8, a [8]float64, b [8]float64, c [8]float64, rounding int) [8]float64


// MaskzFmaddRoundPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'a' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				a[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADD132PS, VFMADD213PS, VFMADD231PS'. Intrinsic: '_mm512_maskz_fmadd_round_ps'.
// Requires AVX512F.
func MaskzFmaddRoundPs(k Mmask16, a M512, b M512, c M512, rounding int) M512 {
	return M512(maskzFmaddRoundPs(uint16(k), [16]float32(a), [16]float32(b), [16]float32(c), rounding))
}

func maskzFmaddRoundPs(k uint16, a [16]float32, b [16]float32, c [16]float32, rounding int) [16]float32


// MaskFmaddRoundSd: Multiply the lower double-precision (64-bit)
// floating-point elements in 'a' and 'b', and add the intermediate result to
// the lower element in 'c'. Store the result in the lower element of 'dst'
// using writemask 'k' (the element is copied from 'a' when mask bit 0 is not
// set), and copy the upper element from 'a' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := (a[63:0] * b[63:0]) + c[63:0]
//		ELSE
//			dst[63:0] := a[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132SD, VFMADD213SD, VFMADD231SD'. Intrinsic: '_mm_mask_fmadd_round_sd'.
// Requires AVX512F.
func MaskFmaddRoundSd(a M128d, k Mmask8, b M128d, c M128d, rounding int) M128d {
	return M128d(maskFmaddRoundSd([2]float64(a), uint8(k), [2]float64(b), [2]float64(c), rounding))
}

func maskFmaddRoundSd(a [2]float64, k uint8, b [2]float64, c [2]float64, rounding int) [2]float64


// Mask3FmaddRoundSd: Multiply the lower double-precision (64-bit)
// floating-point elements in 'a' and 'b', and add the intermediate result to
// the lower element in 'c'. Store the result in the lower element of 'dst'
// using writemask 'k' (the element is copied from 'c' when mask bit 0 is not
// set), and copy the upper element from 'a' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := (a[63:0] * b[63:0]) + c[63:0]
//		ELSE
//			dst[63:0] := c[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132SD, VFMADD213SD, VFMADD231SD'. Intrinsic: '_mm_mask3_fmadd_round_sd'.
// Requires AVX512F.
func Mask3FmaddRoundSd(a M128d, b M128d, c M128d, k Mmask8, rounding int) M128d {
	return M128d(mask3FmaddRoundSd([2]float64(a), [2]float64(b), [2]float64(c), uint8(k), rounding))
}

func mask3FmaddRoundSd(a [2]float64, b [2]float64, c [2]float64, k uint8, rounding int) [2]float64


// MaskzFmaddRoundSd: Multiply the lower double-precision (64-bit)
// floating-point elements in 'a' and 'b', and add the intermediate result to
// the lower element in 'c'. Store the result in the lower element of 'dst'
// using zeromask 'k' (the element is zeroed out when mask bit 0 is not set),
// and copy the upper element from 'a' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := (a[63:0] * b[63:0]) + c[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132SD, VFMADD213SD, VFMADD231SD'. Intrinsic: '_mm_maskz_fmadd_round_sd'.
// Requires AVX512F.
func MaskzFmaddRoundSd(k Mmask8, a M128d, b M128d, c M128d, rounding int) M128d {
	return M128d(maskzFmaddRoundSd(uint8(k), [2]float64(a), [2]float64(b), [2]float64(c), rounding))
}

func maskzFmaddRoundSd(k uint8, a [2]float64, b [2]float64, c [2]float64, rounding int) [2]float64


// MaskFmaddRoundSs: Multiply the lower single-precision (32-bit)
// floating-point elements in 'a' and 'b', and add the intermediate result to
// the lower element in 'c'. Store the result in the lower element of 'dst'
// using writemask 'k' (the element is copied from 'a' when mask bit 0 is not
// set), and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := (a[31:0] * b[31:0]) + c[31:0]
//		ELSE
//			dst[31:0] := a[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132SS, VFMADD213SS, VFMADD231SS'. Intrinsic: '_mm_mask_fmadd_round_ss'.
// Requires AVX512F.
func MaskFmaddRoundSs(a M128, k Mmask8, b M128, c M128, rounding int) M128 {
	return M128(maskFmaddRoundSs([4]float32(a), uint8(k), [4]float32(b), [4]float32(c), rounding))
}

func maskFmaddRoundSs(a [4]float32, k uint8, b [4]float32, c [4]float32, rounding int) [4]float32


// Mask3FmaddRoundSs: Multiply the lower single-precision (32-bit)
// floating-point elements in 'a' and 'b', and add the intermediate result to
// the lower element in 'c'. Store the result in the lower element of 'dst'
// using writemask 'k' (the element is copied from 'c' when mask bit 0 is not
// set), and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := (a[31:0] * b[31:0]) + c[31:0]
//		ELSE
//			dst[31:0] := c[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132SS, VFMADD213SS, VFMADD231SS'. Intrinsic: '_mm_mask3_fmadd_round_ss'.
// Requires AVX512F.
func Mask3FmaddRoundSs(a M128, b M128, c M128, k Mmask8, rounding int) M128 {
	return M128(mask3FmaddRoundSs([4]float32(a), [4]float32(b), [4]float32(c), uint8(k), rounding))
}

func mask3FmaddRoundSs(a [4]float32, b [4]float32, c [4]float32, k uint8, rounding int) [4]float32


// MaskzFmaddRoundSs: Multiply the lower single-precision (32-bit)
// floating-point elements in 'a' and 'b', and add the intermediate result to
// the lower element in 'c'. Store the result in the lower element of 'dst'
// using zeromask 'k' (the element is zeroed out when mask bit 0 is not set),
// and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := (a[31:0] * b[31:0]) + c[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132SS, VFMADD213SS, VFMADD231SS'. Intrinsic: '_mm_maskz_fmadd_round_ss'.
// Requires AVX512F.
func MaskzFmaddRoundSs(k Mmask8, a M128, b M128, c M128, rounding int) M128 {
	return M128(maskzFmaddRoundSs(uint8(k), [4]float32(a), [4]float32(b), [4]float32(c), rounding))
}

func maskzFmaddRoundSs(k uint8, a [4]float32, b [4]float32, c [4]float32, rounding int) [4]float32


// MaskFmaddSd: Multiply the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', and add the intermediate result to the lower
// element in 'c'. Store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'a' when mask bit 0 is not set),
// and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := (a[63:0] * b[63:0]) + c[63:0]
//		ELSE
//			dst[63:0] := a[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132SD, VFMADD213SD, VFMADD231SD'. Intrinsic: '_mm_mask_fmadd_sd'.
// Requires AVX512F.
func MaskFmaddSd(a M128d, k Mmask8, b M128d, c M128d) M128d {
	return M128d(maskFmaddSd([2]float64(a), uint8(k), [2]float64(b), [2]float64(c)))
}

func maskFmaddSd(a [2]float64, k uint8, b [2]float64, c [2]float64) [2]float64


// Mask3FmaddSd: Multiply the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', and add the intermediate result to the lower
// element in 'c'. Store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'c' when mask bit 0 is not set),
// and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := (a[63:0] * b[63:0]) + c[63:0]
//		ELSE
//			dst[63:0] := c[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132SD, VFMADD213SD, VFMADD231SD'. Intrinsic: '_mm_mask3_fmadd_sd'.
// Requires AVX512F.
func Mask3FmaddSd(a M128d, b M128d, c M128d, k Mmask8) M128d {
	return M128d(mask3FmaddSd([2]float64(a), [2]float64(b), [2]float64(c), uint8(k)))
}

func mask3FmaddSd(a [2]float64, b [2]float64, c [2]float64, k uint8) [2]float64


// MaskzFmaddSd: Multiply the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', and add the intermediate result to the lower
// element in 'c'. Store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := (a[63:0] * b[63:0]) + c[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132SD, VFMADD213SD, VFMADD231SD'. Intrinsic: '_mm_maskz_fmadd_sd'.
// Requires AVX512F.
func MaskzFmaddSd(k Mmask8, a M128d, b M128d, c M128d) M128d {
	return M128d(maskzFmaddSd(uint8(k), [2]float64(a), [2]float64(b), [2]float64(c)))
}

func maskzFmaddSd(k uint8, a [2]float64, b [2]float64, c [2]float64) [2]float64


// MaskFmaddSs: Multiply the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', and add the intermediate result to the lower
// element in 'c'. Store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'a' when mask bit 0 is not set),
// and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'. 
//
//		IF k[0]
//			dst[31:0] := (a[31:0] * b[31:0]) + c[31:0]
//		ELSE
//			dst[31:0] := a[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132SS, VFMADD213SS, VFMADD231SS'. Intrinsic: '_mm_mask_fmadd_ss'.
// Requires AVX512F.
func MaskFmaddSs(a M128, k Mmask8, b M128, c M128) M128 {
	return M128(maskFmaddSs([4]float32(a), uint8(k), [4]float32(b), [4]float32(c)))
}

func maskFmaddSs(a [4]float32, k uint8, b [4]float32, c [4]float32) [4]float32


// Mask3FmaddSs: Multiply the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', and add the intermediate result to the lower
// element in 'c'. Store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'c' when mask bit 0 is not set),
// and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'. 
//
//		IF k[0]
//			dst[31:0] := (a[31:0] * b[31:0]) + c[31:0]
//		ELSE
//			dst[31:0] := c[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132SS, VFMADD213SS, VFMADD231SS'. Intrinsic: '_mm_mask3_fmadd_ss'.
// Requires AVX512F.
func Mask3FmaddSs(a M128, b M128, c M128, k Mmask8) M128 {
	return M128(mask3FmaddSs([4]float32(a), [4]float32(b), [4]float32(c), uint8(k)))
}

func mask3FmaddSs(a [4]float32, b [4]float32, c [4]float32, k uint8) [4]float32


// MaskzFmaddSs: Multiply the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', and add the intermediate result to the lower
// element in 'c'. Store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := (a[31:0] * b[31:0]) + c[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132SS, VFMADD213SS, VFMADD231SS'. Intrinsic: '_mm_maskz_fmadd_ss'.
// Requires AVX512F.
func MaskzFmaddSs(k Mmask8, a M128, b M128, c M128) M128 {
	return M128(maskzFmaddSs(uint8(k), [4]float32(a), [4]float32(b), [4]float32(c)))
}

func maskzFmaddSs(k uint8, a [4]float32, b [4]float32, c [4]float32) [4]float32


// MaskFmaddsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm_mask_fmaddsub_pd'.
// Requires AVX512F.
func MaskFmaddsubPd(a M128d, k Mmask8, b M128d, c M128d) M128d {
	return M128d(maskFmaddsubPd([2]float64(a), uint8(k), [2]float64(b), [2]float64(c)))
}

func maskFmaddsubPd(a [2]float64, k uint8, b [2]float64, c [2]float64) [2]float64


// Mask3FmaddsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'c' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm_mask3_fmaddsub_pd'.
// Requires AVX512F.
func Mask3FmaddsubPd(a M128d, b M128d, c M128d, k Mmask8) M128d {
	return M128d(mask3FmaddsubPd([2]float64(a), [2]float64(b), [2]float64(c), uint8(k)))
}

func mask3FmaddsubPd(a [2]float64, b [2]float64, c [2]float64, k uint8) [2]float64


// MaskzFmaddsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm_maskz_fmaddsub_pd'.
// Requires AVX512F.
func MaskzFmaddsubPd(k Mmask8, a M128d, b M128d, c M128d) M128d {
	return M128d(maskzFmaddsubPd(uint8(k), [2]float64(a), [2]float64(b), [2]float64(c)))
}

func maskzFmaddsubPd(k uint8, a [2]float64, b [2]float64, c [2]float64) [2]float64


// MaskFmaddsubPd1: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm256_mask_fmaddsub_pd'.
// Requires AVX512F.
func MaskFmaddsubPd1(a M256d, k Mmask8, b M256d, c M256d) M256d {
	return M256d(maskFmaddsubPd1([4]float64(a), uint8(k), [4]float64(b), [4]float64(c)))
}

func maskFmaddsubPd1(a [4]float64, k uint8, b [4]float64, c [4]float64) [4]float64


// Mask3FmaddsubPd1: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'c' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm256_mask3_fmaddsub_pd'.
// Requires AVX512F.
func Mask3FmaddsubPd1(a M256d, b M256d, c M256d, k Mmask8) M256d {
	return M256d(mask3FmaddsubPd1([4]float64(a), [4]float64(b), [4]float64(c), uint8(k)))
}

func mask3FmaddsubPd1(a [4]float64, b [4]float64, c [4]float64, k uint8) [4]float64


// MaskzFmaddsubPd1: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm256_maskz_fmaddsub_pd'.
// Requires AVX512F.
func MaskzFmaddsubPd1(k Mmask8, a M256d, b M256d, c M256d) M256d {
	return M256d(maskzFmaddsubPd1(uint8(k), [4]float64(a), [4]float64(b), [4]float64(c)))
}

func maskzFmaddsubPd1(k uint8, a [4]float64, b [4]float64, c [4]float64) [4]float64


// FmaddsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF (j is even) 
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm512_fmaddsub_pd'.
// Requires AVX512F.
func FmaddsubPd(a M512d, b M512d, c M512d) M512d {
	return M512d(fmaddsubPd([8]float64(a), [8]float64(b), [8]float64(c)))
}

func fmaddsubPd(a [8]float64, b [8]float64, c [8]float64) [8]float64


// MaskFmaddsubPd2: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm512_mask_fmaddsub_pd'.
// Requires AVX512F.
func MaskFmaddsubPd2(a M512d, k Mmask8, b M512d, c M512d) M512d {
	return M512d(maskFmaddsubPd2([8]float64(a), uint8(k), [8]float64(b), [8]float64(c)))
}

func maskFmaddsubPd2(a [8]float64, k uint8, b [8]float64, c [8]float64) [8]float64


// Mask3FmaddsubPd2: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'c' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm512_mask3_fmaddsub_pd'.
// Requires AVX512F.
func Mask3FmaddsubPd2(a M512d, b M512d, c M512d, k Mmask8) M512d {
	return M512d(mask3FmaddsubPd2([8]float64(a), [8]float64(b), [8]float64(c), uint8(k)))
}

func mask3FmaddsubPd2(a [8]float64, b [8]float64, c [8]float64, k uint8) [8]float64


// MaskzFmaddsubPd2: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm512_maskz_fmaddsub_pd'.
// Requires AVX512F.
func MaskzFmaddsubPd2(k Mmask8, a M512d, b M512d, c M512d) M512d {
	return M512d(maskzFmaddsubPd2(uint8(k), [8]float64(a), [8]float64(b), [8]float64(c)))
}

func maskzFmaddsubPd2(k uint8, a [8]float64, b [8]float64, c [8]float64) [8]float64


// MaskFmaddsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm_mask_fmaddsub_ps'.
// Requires AVX512F.
func MaskFmaddsubPs(a M128, k Mmask8, b M128, c M128) M128 {
	return M128(maskFmaddsubPs([4]float32(a), uint8(k), [4]float32(b), [4]float32(c)))
}

func maskFmaddsubPs(a [4]float32, k uint8, b [4]float32, c [4]float32) [4]float32


// Mask3FmaddsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'c' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm_mask3_fmaddsub_ps'.
// Requires AVX512F.
func Mask3FmaddsubPs(a M128, b M128, c M128, k Mmask8) M128 {
	return M128(mask3FmaddsubPs([4]float32(a), [4]float32(b), [4]float32(c), uint8(k)))
}

func mask3FmaddsubPs(a [4]float32, b [4]float32, c [4]float32, k uint8) [4]float32


// MaskzFmaddsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm_maskz_fmaddsub_ps'.
// Requires AVX512F.
func MaskzFmaddsubPs(k Mmask8, a M128, b M128, c M128) M128 {
	return M128(maskzFmaddsubPs(uint8(k), [4]float32(a), [4]float32(b), [4]float32(c)))
}

func maskzFmaddsubPs(k uint8, a [4]float32, b [4]float32, c [4]float32) [4]float32


// MaskFmaddsubPs1: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm256_mask_fmaddsub_ps'.
// Requires AVX512F.
func MaskFmaddsubPs1(a M256, k Mmask8, b M256, c M256) M256 {
	return M256(maskFmaddsubPs1([8]float32(a), uint8(k), [8]float32(b), [8]float32(c)))
}

func maskFmaddsubPs1(a [8]float32, k uint8, b [8]float32, c [8]float32) [8]float32


// Mask3FmaddsubPs1: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'c' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm256_mask3_fmaddsub_ps'.
// Requires AVX512F.
func Mask3FmaddsubPs1(a M256, b M256, c M256, k Mmask8) M256 {
	return M256(mask3FmaddsubPs1([8]float32(a), [8]float32(b), [8]float32(c), uint8(k)))
}

func mask3FmaddsubPs1(a [8]float32, b [8]float32, c [8]float32, k uint8) [8]float32


// MaskzFmaddsubPs1: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm256_maskz_fmaddsub_ps'.
// Requires AVX512F.
func MaskzFmaddsubPs1(k Mmask8, a M256, b M256, c M256) M256 {
	return M256(maskzFmaddsubPs1(uint8(k), [8]float32(a), [8]float32(b), [8]float32(c)))
}

func maskzFmaddsubPs1(k uint8, a [8]float32, b [8]float32, c [8]float32) [8]float32


// FmaddsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF (j is even) 
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm512_fmaddsub_ps'.
// Requires AVX512F.
func FmaddsubPs(a M512, b M512, c M512) M512 {
	return M512(fmaddsubPs([16]float32(a), [16]float32(b), [16]float32(c)))
}

func fmaddsubPs(a [16]float32, b [16]float32, c [16]float32) [16]float32


// MaskFmaddsubPs2: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm512_mask_fmaddsub_ps'.
// Requires AVX512F.
func MaskFmaddsubPs2(a M512, k Mmask16, b M512, c M512) M512 {
	return M512(maskFmaddsubPs2([16]float32(a), uint16(k), [16]float32(b), [16]float32(c)))
}

func maskFmaddsubPs2(a [16]float32, k uint16, b [16]float32, c [16]float32) [16]float32


// Mask3FmaddsubPs2: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'c' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm512_mask3_fmaddsub_ps'.
// Requires AVX512F.
func Mask3FmaddsubPs2(a M512, b M512, c M512, k Mmask16) M512 {
	return M512(mask3FmaddsubPs2([16]float32(a), [16]float32(b), [16]float32(c), uint16(k)))
}

func mask3FmaddsubPs2(a [16]float32, b [16]float32, c [16]float32, k uint16) [16]float32


// MaskzFmaddsubPs2: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm512_maskz_fmaddsub_ps'.
// Requires AVX512F.
func MaskzFmaddsubPs2(k Mmask16, a M512, b M512, c M512) M512 {
	return M512(maskzFmaddsubPs2(uint16(k), [16]float32(a), [16]float32(b), [16]float32(c)))
}

func maskzFmaddsubPs2(k uint16, a [16]float32, b [16]float32, c [16]float32) [16]float32


// FmaddsubRoundPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF (j is even) 
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm512_fmaddsub_round_pd'.
// Requires AVX512F.
func FmaddsubRoundPd(a M512d, b M512d, c M512d, rounding int) M512d {
	return M512d(fmaddsubRoundPd([8]float64(a), [8]float64(b), [8]float64(c), rounding))
}

func fmaddsubRoundPd(a [8]float64, b [8]float64, c [8]float64, rounding int) [8]float64


// MaskFmaddsubRoundPd: Multiply packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', alternatively add and subtract
// packed elements in 'c' to/from the intermediate result, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). Rounding is done according to the
// 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm512_mask_fmaddsub_round_pd'.
// Requires AVX512F.
func MaskFmaddsubRoundPd(a M512d, k Mmask8, b M512d, c M512d, rounding int) M512d {
	return M512d(maskFmaddsubRoundPd([8]float64(a), uint8(k), [8]float64(b), [8]float64(c), rounding))
}

func maskFmaddsubRoundPd(a [8]float64, k uint8, b [8]float64, c [8]float64, rounding int) [8]float64


// Mask3FmaddsubRoundPd: Multiply packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', alternatively add and subtract
// packed elements in 'c' to/from the intermediate result, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'c' when the
// corresponding mask bit is not set).  Rounding is done according to the
// 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				FI
//			ELSE 
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm512_mask3_fmaddsub_round_pd'.
// Requires AVX512F.
func Mask3FmaddsubRoundPd(a M512d, b M512d, c M512d, k Mmask8, rounding int) M512d {
	return M512d(mask3FmaddsubRoundPd([8]float64(a), [8]float64(b), [8]float64(c), uint8(k), rounding))
}

func mask3FmaddsubRoundPd(a [8]float64, b [8]float64, c [8]float64, k uint8, rounding int) [8]float64


// MaskzFmaddsubRoundPd: Multiply packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', alternatively add and subtract
// packed elements in 'c' to/from the intermediate result, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm512_maskz_fmaddsub_round_pd'.
// Requires AVX512F.
func MaskzFmaddsubRoundPd(k Mmask8, a M512d, b M512d, c M512d, rounding int) M512d {
	return M512d(maskzFmaddsubRoundPd(uint8(k), [8]float64(a), [8]float64(b), [8]float64(c), rounding))
}

func maskzFmaddsubRoundPd(k uint8, a [8]float64, b [8]float64, c [8]float64, rounding int) [8]float64


// FmaddsubRoundPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF (j is even) 
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm512_fmaddsub_round_ps'.
// Requires AVX512F.
func FmaddsubRoundPs(a M512, b M512, c M512, rounding int) M512 {
	return M512(fmaddsubRoundPs([16]float32(a), [16]float32(b), [16]float32(c), rounding))
}

func fmaddsubRoundPs(a [16]float32, b [16]float32, c [16]float32, rounding int) [16]float32


// MaskFmaddsubRoundPs: Multiply packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', alternatively add and subtract
// packed elements in 'c' to/from the intermediate result, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). Rounding is done according to the
// 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm512_mask_fmaddsub_round_ps'.
// Requires AVX512F.
func MaskFmaddsubRoundPs(a M512, k Mmask16, b M512, c M512, rounding int) M512 {
	return M512(maskFmaddsubRoundPs([16]float32(a), uint16(k), [16]float32(b), [16]float32(c), rounding))
}

func maskFmaddsubRoundPs(a [16]float32, k uint16, b [16]float32, c [16]float32, rounding int) [16]float32


// Mask3FmaddsubRoundPs: Multiply packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', alternatively add and subtract
// packed elements in 'c' to/from the intermediate result, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'c' when the
// corresponding mask bit is not set).  Rounding is done according to the
// 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm512_mask3_fmaddsub_round_ps'.
// Requires AVX512F.
func Mask3FmaddsubRoundPs(a M512, b M512, c M512, k Mmask16, rounding int) M512 {
	return M512(mask3FmaddsubRoundPs([16]float32(a), [16]float32(b), [16]float32(c), uint16(k), rounding))
}

func mask3FmaddsubRoundPs(a [16]float32, b [16]float32, c [16]float32, k uint16, rounding int) [16]float32


// MaskzFmaddsubRoundPs: Multiply packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', alternatively add and subtract
// packed elements in 'c' to/from the intermediate result, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm512_maskz_fmaddsub_round_ps'.
// Requires AVX512F.
func MaskzFmaddsubRoundPs(k Mmask16, a M512, b M512, c M512, rounding int) M512 {
	return M512(maskzFmaddsubRoundPs(uint16(k), [16]float32(a), [16]float32(b), [16]float32(c), rounding))
}

func maskzFmaddsubRoundPs(k uint16, a [16]float32, b [16]float32, c [16]float32, rounding int) [16]float32


// MaskFmsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132PD, VFMSUB213PD, VFMSUB231PD'. Intrinsic: '_mm_mask_fmsub_pd'.
// Requires AVX512F.
func MaskFmsubPd(a M128d, k Mmask8, b M128d, c M128d) M128d {
	return M128d(maskFmsubPd([2]float64(a), uint8(k), [2]float64(b), [2]float64(c)))
}

func maskFmsubPd(a [2]float64, k uint8, b [2]float64, c [2]float64) [2]float64


// Mask3FmsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132PD, VFMSUB213PD, VFMSUB231PD'. Intrinsic: '_mm_mask3_fmsub_pd'.
// Requires AVX512F.
func Mask3FmsubPd(a M128d, b M128d, c M128d, k Mmask8) M128d {
	return M128d(mask3FmsubPd([2]float64(a), [2]float64(b), [2]float64(c), uint8(k)))
}

func mask3FmsubPd(a [2]float64, b [2]float64, c [2]float64, k uint8) [2]float64


// MaskzFmsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132PD, VFMSUB213PD, VFMSUB231PD'. Intrinsic: '_mm_maskz_fmsub_pd'.
// Requires AVX512F.
func MaskzFmsubPd(k Mmask8, a M128d, b M128d, c M128d) M128d {
	return M128d(maskzFmsubPd(uint8(k), [2]float64(a), [2]float64(b), [2]float64(c)))
}

func maskzFmsubPd(k uint8, a [2]float64, b [2]float64, c [2]float64) [2]float64


// MaskFmsubPd1: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUB132PD, VFMSUB213PD, VFMSUB231PD'. Intrinsic: '_mm256_mask_fmsub_pd'.
// Requires AVX512F.
func MaskFmsubPd1(a M256d, k Mmask8, b M256d, c M256d) M256d {
	return M256d(maskFmsubPd1([4]float64(a), uint8(k), [4]float64(b), [4]float64(c)))
}

func maskFmsubPd1(a [4]float64, k uint8, b [4]float64, c [4]float64) [4]float64


// Mask3FmsubPd1: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUB132PD, VFMSUB213PD, VFMSUB231PD'. Intrinsic: '_mm256_mask3_fmsub_pd'.
// Requires AVX512F.
func Mask3FmsubPd1(a M256d, b M256d, c M256d, k Mmask8) M256d {
	return M256d(mask3FmsubPd1([4]float64(a), [4]float64(b), [4]float64(c), uint8(k)))
}

func mask3FmsubPd1(a [4]float64, b [4]float64, c [4]float64, k uint8) [4]float64


// MaskzFmsubPd1: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUB132PD, VFMSUB213PD, VFMSUB231PD'. Intrinsic: '_mm256_maskz_fmsub_pd'.
// Requires AVX512F.
func MaskzFmsubPd1(k Mmask8, a M256d, b M256d, c M256d) M256d {
	return M256d(maskzFmsubPd1(uint8(k), [4]float64(a), [4]float64(b), [4]float64(c)))
}

func maskzFmsubPd1(k uint8, a [4]float64, b [4]float64, c [4]float64) [4]float64


// MaskzFmsubPd2: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUB132PD, VFMSUB213PD, VFMSUB231PD'. Intrinsic: '_mm512_maskz_fmsub_pd'.
// Requires AVX512F.
func MaskzFmsubPd2(k Mmask8, a M512d, b M512d, c M512d) M512d {
	return M512d(maskzFmsubPd2(uint8(k), [8]float64(a), [8]float64(b), [8]float64(c)))
}

func maskzFmsubPd2(k uint8, a [8]float64, b [8]float64, c [8]float64) [8]float64


// MaskFmsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132PS, VFMSUB213PS, VFMSUB231PS'. Intrinsic: '_mm_mask_fmsub_ps'.
// Requires AVX512F.
func MaskFmsubPs(a M128, k Mmask8, b M128, c M128) M128 {
	return M128(maskFmsubPs([4]float32(a), uint8(k), [4]float32(b), [4]float32(c)))
}

func maskFmsubPs(a [4]float32, k uint8, b [4]float32, c [4]float32) [4]float32


// Mask3FmsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132PS, VFMSUB213PS, VFMSUB231PS'. Intrinsic: '_mm_mask3_fmsub_ps'.
// Requires AVX512F.
func Mask3FmsubPs(a M128, b M128, c M128, k Mmask8) M128 {
	return M128(mask3FmsubPs([4]float32(a), [4]float32(b), [4]float32(c), uint8(k)))
}

func mask3FmsubPs(a [4]float32, b [4]float32, c [4]float32, k uint8) [4]float32


// MaskzFmsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132PS, VFMSUB213PS, VFMSUB231PS'. Intrinsic: '_mm_maskz_fmsub_ps'.
// Requires AVX512F.
func MaskzFmsubPs(k Mmask8, a M128, b M128, c M128) M128 {
	return M128(maskzFmsubPs(uint8(k), [4]float32(a), [4]float32(b), [4]float32(c)))
}

func maskzFmsubPs(k uint8, a [4]float32, b [4]float32, c [4]float32) [4]float32


// MaskFmsubPs1: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUB132PS, VFMSUB213PS, VFMSUB231PS'. Intrinsic: '_mm256_mask_fmsub_ps'.
// Requires AVX512F.
func MaskFmsubPs1(a M256, k Mmask8, b M256, c M256) M256 {
	return M256(maskFmsubPs1([8]float32(a), uint8(k), [8]float32(b), [8]float32(c)))
}

func maskFmsubPs1(a [8]float32, k uint8, b [8]float32, c [8]float32) [8]float32


// Mask3FmsubPs1: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUB132PS, VFMSUB213PS, VFMSUB231PS'. Intrinsic: '_mm256_mask3_fmsub_ps'.
// Requires AVX512F.
func Mask3FmsubPs1(a M256, b M256, c M256, k Mmask8) M256 {
	return M256(mask3FmsubPs1([8]float32(a), [8]float32(b), [8]float32(c), uint8(k)))
}

func mask3FmsubPs1(a [8]float32, b [8]float32, c [8]float32, k uint8) [8]float32


// MaskzFmsubPs1: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUB132PS, VFMSUB213PS, VFMSUB231PS'. Intrinsic: '_mm256_maskz_fmsub_ps'.
// Requires AVX512F.
func MaskzFmsubPs1(k Mmask8, a M256, b M256, c M256) M256 {
	return M256(maskzFmsubPs1(uint8(k), [8]float32(a), [8]float32(b), [8]float32(c)))
}

func maskzFmsubPs1(k uint8, a [8]float32, b [8]float32, c [8]float32) [8]float32


// MaskzFmsubPs2: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUB132PS, VFMSUB213PS, VFMSUB231PS'. Intrinsic: '_mm512_maskz_fmsub_ps'.
// Requires AVX512F.
func MaskzFmsubPs2(k Mmask16, a M512, b M512, c M512) M512 {
	return M512(maskzFmsubPs2(uint16(k), [16]float32(a), [16]float32(b), [16]float32(c)))
}

func maskzFmsubPs2(k uint16, a [16]float32, b [16]float32, c [16]float32) [16]float32


// MaskzFmsubRoundPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUB132PD, VFMSUB213PD, VFMSUB231PD'. Intrinsic: '_mm512_maskz_fmsub_round_pd'.
// Requires AVX512F.
func MaskzFmsubRoundPd(k Mmask8, a M512d, b M512d, c M512d, rounding int) M512d {
	return M512d(maskzFmsubRoundPd(uint8(k), [8]float64(a), [8]float64(b), [8]float64(c), rounding))
}

func maskzFmsubRoundPd(k uint8, a [8]float64, b [8]float64, c [8]float64, rounding int) [8]float64


// MaskzFmsubRoundPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUB132PS, VFMSUB213PS, VFMSUB231PS'. Intrinsic: '_mm512_maskz_fmsub_round_ps'.
// Requires AVX512F.
func MaskzFmsubRoundPs(k Mmask16, a M512, b M512, c M512, rounding int) M512 {
	return M512(maskzFmsubRoundPs(uint16(k), [16]float32(a), [16]float32(b), [16]float32(c), rounding))
}

func maskzFmsubRoundPs(k uint16, a [16]float32, b [16]float32, c [16]float32, rounding int) [16]float32


// MaskFmsubRoundSd: Multiply the lower double-precision (64-bit)
// floating-point elements in 'a' and 'b', and subtract the lower element in
// 'c' from the intermediate result. Store the result in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'a' when mask bit 0 is
// not set), and copy the upper element from 'a' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := (a[63:0] * b[63:0]) - c[63:0]
//		ELSE
//			dst[63:0] := a[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132SD, VFMSUB213SD, VFMSUB231SD'. Intrinsic: '_mm_mask_fmsub_round_sd'.
// Requires AVX512F.
func MaskFmsubRoundSd(a M128d, k Mmask8, b M128d, c M128d, rounding int) M128d {
	return M128d(maskFmsubRoundSd([2]float64(a), uint8(k), [2]float64(b), [2]float64(c), rounding))
}

func maskFmsubRoundSd(a [2]float64, k uint8, b [2]float64, c [2]float64, rounding int) [2]float64


// Mask3FmsubRoundSd: Multiply the lower double-precision (64-bit)
// floating-point elements in 'a' and 'b', and subtract the lower element in
// 'c' from the intermediate result. Store the result in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'c' when mask bit 0 is
// not set), and copy the upper element from 'a' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := (a[63:0] * b[63:0]) - c[63:0]
//		ELSE
//			dst[63:0] := c[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132SD, VFMSUB213SD, VFMSUB231SD'. Intrinsic: '_mm_mask3_fmsub_round_sd'.
// Requires AVX512F.
func Mask3FmsubRoundSd(a M128d, b M128d, c M128d, k Mmask8, rounding int) M128d {
	return M128d(mask3FmsubRoundSd([2]float64(a), [2]float64(b), [2]float64(c), uint8(k), rounding))
}

func mask3FmsubRoundSd(a [2]float64, b [2]float64, c [2]float64, k uint8, rounding int) [2]float64


// MaskzFmsubRoundSd: Multiply the lower double-precision (64-bit)
// floating-point elements in 'a' and 'b', and subtract the lower element in
// 'c' from the intermediate result. Store the result in the lower element of
// 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is not
// set), and copy the upper element from 'a' to the upper element of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := (a[63:0] * b[63:0]) - c[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132SD, VFMSUB213SD, VFMSUB231SD'. Intrinsic: '_mm_maskz_fmsub_round_sd'.
// Requires AVX512F.
func MaskzFmsubRoundSd(k Mmask8, a M128d, b M128d, c M128d, rounding int) M128d {
	return M128d(maskzFmsubRoundSd(uint8(k), [2]float64(a), [2]float64(b), [2]float64(c), rounding))
}

func maskzFmsubRoundSd(k uint8, a [2]float64, b [2]float64, c [2]float64, rounding int) [2]float64


// MaskFmsubRoundSs: Multiply the lower single-precision (32-bit)
// floating-point elements in 'a' and 'b', and subtract the lower element in
// 'c' from the intermediate result. Store the result in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'a' when mask bit 0 is
// not set), and copy the upper 3 packed elements from 'a' to the upper
// elements of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := (a[31:0] * b[31:0]) - c[31:0]
//		ELSE
//			dst[31:0] := a[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132SS, VFMSUB213SS, VFMSUB231SS'. Intrinsic: '_mm_mask_fmsub_round_ss'.
// Requires AVX512F.
func MaskFmsubRoundSs(a M128, k Mmask8, b M128, c M128, rounding int) M128 {
	return M128(maskFmsubRoundSs([4]float32(a), uint8(k), [4]float32(b), [4]float32(c), rounding))
}

func maskFmsubRoundSs(a [4]float32, k uint8, b [4]float32, c [4]float32, rounding int) [4]float32


// Mask3FmsubRoundSs: Multiply the lower single-precision (32-bit)
// floating-point elements in 'a' and 'b', and subtract the lower element in
// 'c' from the intermediate result. Store the result in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'c' when mask bit 0 is
// not set), and copy the upper 3 packed elements from 'a' to the upper
// elements of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := (a[31:0] * b[31:0]) - c[31:0]
//		ELSE
//			dst[31:0] := c[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132SS, VFMSUB213SS, VFMSUB231SS'. Intrinsic: '_mm_mask3_fmsub_round_ss'.
// Requires AVX512F.
func Mask3FmsubRoundSs(a M128, b M128, c M128, k Mmask8, rounding int) M128 {
	return M128(mask3FmsubRoundSs([4]float32(a), [4]float32(b), [4]float32(c), uint8(k), rounding))
}

func mask3FmsubRoundSs(a [4]float32, b [4]float32, c [4]float32, k uint8, rounding int) [4]float32


// MaskzFmsubRoundSs: Multiply the lower single-precision (32-bit)
// floating-point elements in 'a' and 'b', and subtract the lower element in
// 'c' from the intermediate result. Store the result in the lower element of
// 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is not
// set), and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := (a[31:0] * b[31:0]) - c[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132SS, VFMSUB213SS, VFMSUB231SS'. Intrinsic: '_mm_maskz_fmsub_round_ss'.
// Requires AVX512F.
func MaskzFmsubRoundSs(k Mmask8, a M128, b M128, c M128, rounding int) M128 {
	return M128(maskzFmsubRoundSs(uint8(k), [4]float32(a), [4]float32(b), [4]float32(c), rounding))
}

func maskzFmsubRoundSs(k uint8, a [4]float32, b [4]float32, c [4]float32, rounding int) [4]float32


// MaskFmsubSd: Multiply the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', and subtract the lower element in 'c' from the
// intermediate result. Store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'a' when mask bit 0 is not set),
// and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := (a[63:0] * b[63:0]) - c[63:0]
//		ELSE
//			dst[63:0] := a[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132SD, VFMSUB213SD, VFMSUB231SD'. Intrinsic: '_mm_mask_fmsub_sd'.
// Requires AVX512F.
func MaskFmsubSd(a M128d, k Mmask8, b M128d, c M128d) M128d {
	return M128d(maskFmsubSd([2]float64(a), uint8(k), [2]float64(b), [2]float64(c)))
}

func maskFmsubSd(a [2]float64, k uint8, b [2]float64, c [2]float64) [2]float64


// Mask3FmsubSd: Multiply the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', and subtract the lower element in 'c' from the
// intermediate result. Store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'c' when mask bit 0 is not set),
// and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := (a[63:0] * b[63:0]) - c[63:0]
//		ELSE
//			dst[63:0] := c[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132SD, VFMSUB213SD, VFMSUB231SD'. Intrinsic: '_mm_mask3_fmsub_sd'.
// Requires AVX512F.
func Mask3FmsubSd(a M128d, b M128d, c M128d, k Mmask8) M128d {
	return M128d(mask3FmsubSd([2]float64(a), [2]float64(b), [2]float64(c), uint8(k)))
}

func mask3FmsubSd(a [2]float64, b [2]float64, c [2]float64, k uint8) [2]float64


// MaskzFmsubSd: Multiply the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', and subtract the lower element in 'c' from the
// intermediate result. Store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := (a[63:0] * b[63:0]) - c[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132SD, VFMSUB213SD, VFMSUB231SD'. Intrinsic: '_mm_maskz_fmsub_sd'.
// Requires AVX512F.
func MaskzFmsubSd(k Mmask8, a M128d, b M128d, c M128d) M128d {
	return M128d(maskzFmsubSd(uint8(k), [2]float64(a), [2]float64(b), [2]float64(c)))
}

func maskzFmsubSd(k uint8, a [2]float64, b [2]float64, c [2]float64) [2]float64


// MaskFmsubSs: Multiply the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', and subtract the lower element in 'c' from the
// intermediate result. Store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'a' when mask bit 0 is not set),
// and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'. 
//
//		IF k[0]
//			dst[31:0] := (a[31:0] * b[31:0]) - c[31:0]
//		ELSE
//			dst[31:0] := a[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132SS, VFMSUB213SS, VFMSUB231SS'. Intrinsic: '_mm_mask_fmsub_ss'.
// Requires AVX512F.
func MaskFmsubSs(a M128, k Mmask8, b M128, c M128) M128 {
	return M128(maskFmsubSs([4]float32(a), uint8(k), [4]float32(b), [4]float32(c)))
}

func maskFmsubSs(a [4]float32, k uint8, b [4]float32, c [4]float32) [4]float32


// Mask3FmsubSs: Multiply the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', and subtract the lower element in 'c' from the
// intermediate result. Store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'c' when mask bit 0 is not set),
// and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'. 
//
//		IF k[0]
//			dst[31:0] := (a[31:0] * b[31:0]) - c[31:0]
//		ELSE
//			dst[31:0] := c[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132SS, VFMSUB213SS, VFMSUB231SS'. Intrinsic: '_mm_mask3_fmsub_ss'.
// Requires AVX512F.
func Mask3FmsubSs(a M128, b M128, c M128, k Mmask8) M128 {
	return M128(mask3FmsubSs([4]float32(a), [4]float32(b), [4]float32(c), uint8(k)))
}

func mask3FmsubSs(a [4]float32, b [4]float32, c [4]float32, k uint8) [4]float32


// MaskzFmsubSs: Multiply the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', and subtract the lower element in 'c' from the
// intermediate result. Store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := (a[31:0] * b[31:0]) - c[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132SS, VFMSUB213SS, VFMSUB231SS'. Intrinsic: '_mm_maskz_fmsub_ss'.
// Requires AVX512F.
func MaskzFmsubSs(k Mmask8, a M128, b M128, c M128) M128 {
	return M128(maskzFmsubSs(uint8(k), [4]float32(a), [4]float32(b), [4]float32(c)))
}

func maskzFmsubSs(k uint8, a [4]float32, b [4]float32, c [4]float32) [4]float32


// MaskFmsubaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 1 
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm_mask_fmsubadd_pd'.
// Requires AVX512F.
func MaskFmsubaddPd(a M128d, k Mmask8, b M128d, c M128d) M128d {
	return M128d(maskFmsubaddPd([2]float64(a), uint8(k), [2]float64(b), [2]float64(c)))
}

func maskFmsubaddPd(a [2]float64, k uint8, b [2]float64, c [2]float64) [2]float64


// Mask3FmsubaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'c' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm_mask3_fmsubadd_pd'.
// Requires AVX512F.
func Mask3FmsubaddPd(a M128d, b M128d, c M128d, k Mmask8) M128d {
	return M128d(mask3FmsubaddPd([2]float64(a), [2]float64(b), [2]float64(c), uint8(k)))
}

func mask3FmsubaddPd(a [2]float64, b [2]float64, c [2]float64, k uint8) [2]float64


// MaskzFmsubaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm_maskz_fmsubadd_pd'.
// Requires AVX512F.
func MaskzFmsubaddPd(k Mmask8, a M128d, b M128d, c M128d) M128d {
	return M128d(maskzFmsubaddPd(uint8(k), [2]float64(a), [2]float64(b), [2]float64(c)))
}

func maskzFmsubaddPd(k uint8, a [2]float64, b [2]float64, c [2]float64) [2]float64


// MaskFmsubaddPd1: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm256_mask_fmsubadd_pd'.
// Requires AVX512F.
func MaskFmsubaddPd1(a M256d, k Mmask8, b M256d, c M256d) M256d {
	return M256d(maskFmsubaddPd1([4]float64(a), uint8(k), [4]float64(b), [4]float64(c)))
}

func maskFmsubaddPd1(a [4]float64, k uint8, b [4]float64, c [4]float64) [4]float64


// Mask3FmsubaddPd1: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'c' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm256_mask3_fmsubadd_pd'.
// Requires AVX512F.
func Mask3FmsubaddPd1(a M256d, b M256d, c M256d, k Mmask8) M256d {
	return M256d(mask3FmsubaddPd1([4]float64(a), [4]float64(b), [4]float64(c), uint8(k)))
}

func mask3FmsubaddPd1(a [4]float64, b [4]float64, c [4]float64, k uint8) [4]float64


// MaskzFmsubaddPd1: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm256_maskz_fmsubadd_pd'.
// Requires AVX512F.
func MaskzFmsubaddPd1(k Mmask8, a M256d, b M256d, c M256d) M256d {
	return M256d(maskzFmsubaddPd1(uint8(k), [4]float64(a), [4]float64(b), [4]float64(c)))
}

func maskzFmsubaddPd1(k uint8, a [4]float64, b [4]float64, c [4]float64) [4]float64


// FmsubaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF (j is even) 
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm512_fmsubadd_pd'.
// Requires AVX512F.
func FmsubaddPd(a M512d, b M512d, c M512d) M512d {
	return M512d(fmsubaddPd([8]float64(a), [8]float64(b), [8]float64(c)))
}

func fmsubaddPd(a [8]float64, b [8]float64, c [8]float64) [8]float64


// MaskFmsubaddPd2: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm512_mask_fmsubadd_pd'.
// Requires AVX512F.
func MaskFmsubaddPd2(a M512d, k Mmask8, b M512d, c M512d) M512d {
	return M512d(maskFmsubaddPd2([8]float64(a), uint8(k), [8]float64(b), [8]float64(c)))
}

func maskFmsubaddPd2(a [8]float64, k uint8, b [8]float64, c [8]float64) [8]float64


// Mask3FmsubaddPd2: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'c' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm512_mask3_fmsubadd_pd'.
// Requires AVX512F.
func Mask3FmsubaddPd2(a M512d, b M512d, c M512d, k Mmask8) M512d {
	return M512d(mask3FmsubaddPd2([8]float64(a), [8]float64(b), [8]float64(c), uint8(k)))
}

func mask3FmsubaddPd2(a [8]float64, b [8]float64, c [8]float64, k uint8) [8]float64


// MaskzFmsubaddPd2: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm512_maskz_fmsubadd_pd'.
// Requires AVX512F.
func MaskzFmsubaddPd2(k Mmask8, a M512d, b M512d, c M512d) M512d {
	return M512d(maskzFmsubaddPd2(uint8(k), [8]float64(a), [8]float64(b), [8]float64(c)))
}

func maskzFmsubaddPd2(k uint8, a [8]float64, b [8]float64, c [8]float64) [8]float64


// MaskFmsubaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm_mask_fmsubadd_ps'.
// Requires AVX512F.
func MaskFmsubaddPs(a M128, k Mmask8, b M128, c M128) M128 {
	return M128(maskFmsubaddPs([4]float32(a), uint8(k), [4]float32(b), [4]float32(c)))
}

func maskFmsubaddPs(a [4]float32, k uint8, b [4]float32, c [4]float32) [4]float32


// Mask3FmsubaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'c' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm_mask3_fmsubadd_ps'.
// Requires AVX512F.
func Mask3FmsubaddPs(a M128, b M128, c M128, k Mmask8) M128 {
	return M128(mask3FmsubaddPs([4]float32(a), [4]float32(b), [4]float32(c), uint8(k)))
}

func mask3FmsubaddPs(a [4]float32, b [4]float32, c [4]float32, k uint8) [4]float32


// MaskzFmsubaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm_maskz_fmsubadd_ps'.
// Requires AVX512F.
func MaskzFmsubaddPs(k Mmask8, a M128, b M128, c M128) M128 {
	return M128(maskzFmsubaddPs(uint8(k), [4]float32(a), [4]float32(b), [4]float32(c)))
}

func maskzFmsubaddPs(k uint8, a [4]float32, b [4]float32, c [4]float32) [4]float32


// MaskFmsubaddPs1: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm256_mask_fmsubadd_ps'.
// Requires AVX512F.
func MaskFmsubaddPs1(a M256, k Mmask8, b M256, c M256) M256 {
	return M256(maskFmsubaddPs1([8]float32(a), uint8(k), [8]float32(b), [8]float32(c)))
}

func maskFmsubaddPs1(a [8]float32, k uint8, b [8]float32, c [8]float32) [8]float32


// Mask3FmsubaddPs1: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'c' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm256_mask3_fmsubadd_ps'.
// Requires AVX512F.
func Mask3FmsubaddPs1(a M256, b M256, c M256, k Mmask8) M256 {
	return M256(mask3FmsubaddPs1([8]float32(a), [8]float32(b), [8]float32(c), uint8(k)))
}

func mask3FmsubaddPs1(a [8]float32, b [8]float32, c [8]float32, k uint8) [8]float32


// MaskzFmsubaddPs1: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm256_maskz_fmsubadd_ps'.
// Requires AVX512F.
func MaskzFmsubaddPs1(k Mmask8, a M256, b M256, c M256) M256 {
	return M256(maskzFmsubaddPs1(uint8(k), [8]float32(a), [8]float32(b), [8]float32(c)))
}

func maskzFmsubaddPs1(k uint8, a [8]float32, b [8]float32, c [8]float32) [8]float32


// FmsubaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF (j is even) 
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm512_fmsubadd_ps'.
// Requires AVX512F.
func FmsubaddPs(a M512, b M512, c M512) M512 {
	return M512(fmsubaddPs([16]float32(a), [16]float32(b), [16]float32(c)))
}

func fmsubaddPs(a [16]float32, b [16]float32, c [16]float32) [16]float32


// MaskFmsubaddPs2: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm512_mask_fmsubadd_ps'.
// Requires AVX512F.
func MaskFmsubaddPs2(a M512, k Mmask16, b M512, c M512) M512 {
	return M512(maskFmsubaddPs2([16]float32(a), uint16(k), [16]float32(b), [16]float32(c)))
}

func maskFmsubaddPs2(a [16]float32, k uint16, b [16]float32, c [16]float32) [16]float32


// Mask3FmsubaddPs2: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'c' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm512_mask3_fmsubadd_ps'.
// Requires AVX512F.
func Mask3FmsubaddPs2(a M512, b M512, c M512, k Mmask16) M512 {
	return M512(mask3FmsubaddPs2([16]float32(a), [16]float32(b), [16]float32(c), uint16(k)))
}

func mask3FmsubaddPs2(a [16]float32, b [16]float32, c [16]float32, k uint16) [16]float32


// MaskzFmsubaddPs2: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm512_maskz_fmsubadd_ps'.
// Requires AVX512F.
func MaskzFmsubaddPs2(k Mmask16, a M512, b M512, c M512) M512 {
	return M512(maskzFmsubaddPs2(uint16(k), [16]float32(a), [16]float32(b), [16]float32(c)))
}

func maskzFmsubaddPs2(k uint16, a [16]float32, b [16]float32, c [16]float32) [16]float32


// FmsubaddRoundPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF (j is even) 
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm512_fmsubadd_round_pd'.
// Requires AVX512F.
func FmsubaddRoundPd(a M512d, b M512d, c M512d, rounding int) M512d {
	return M512d(fmsubaddRoundPd([8]float64(a), [8]float64(b), [8]float64(c), rounding))
}

func fmsubaddRoundPd(a [8]float64, b [8]float64, c [8]float64, rounding int) [8]float64


// MaskFmsubaddRoundPd: Multiply packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', alternatively subtract and add
// packed elements in 'c' from/to the intermediate result, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). Rounding is done according to the
// 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm512_mask_fmsubadd_round_pd'.
// Requires AVX512F.
func MaskFmsubaddRoundPd(a M512d, k Mmask8, b M512d, c M512d, rounding int) M512d {
	return M512d(maskFmsubaddRoundPd([8]float64(a), uint8(k), [8]float64(b), [8]float64(c), rounding))
}

func maskFmsubaddRoundPd(a [8]float64, k uint8, b [8]float64, c [8]float64, rounding int) [8]float64


// Mask3FmsubaddRoundPd: Multiply packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', alternatively subtract and add
// packed elements in 'c' from/to the intermediate result, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'c' when the
// corresponding mask bit is not set).  Rounding is done according to the
// 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm512_mask3_fmsubadd_round_pd'.
// Requires AVX512F.
func Mask3FmsubaddRoundPd(a M512d, b M512d, c M512d, k Mmask8, rounding int) M512d {
	return M512d(mask3FmsubaddRoundPd([8]float64(a), [8]float64(b), [8]float64(c), uint8(k), rounding))
}

func mask3FmsubaddRoundPd(a [8]float64, b [8]float64, c [8]float64, k uint8, rounding int) [8]float64


// MaskzFmsubaddRoundPd: Multiply packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', alternatively subtract and add
// packed elements in 'c' from/to the intermediate result, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm512_maskz_fmsubadd_round_pd'.
// Requires AVX512F.
func MaskzFmsubaddRoundPd(k Mmask8, a M512d, b M512d, c M512d, rounding int) M512d {
	return M512d(maskzFmsubaddRoundPd(uint8(k), [8]float64(a), [8]float64(b), [8]float64(c), rounding))
}

func maskzFmsubaddRoundPd(k uint8, a [8]float64, b [8]float64, c [8]float64, rounding int) [8]float64


// FmsubaddRoundPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF (j is even) 
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm512_fmsubadd_round_ps'.
// Requires AVX512F.
func FmsubaddRoundPs(a M512, b M512, c M512, rounding int) M512 {
	return M512(fmsubaddRoundPs([16]float32(a), [16]float32(b), [16]float32(c), rounding))
}

func fmsubaddRoundPs(a [16]float32, b [16]float32, c [16]float32, rounding int) [16]float32


// MaskFmsubaddRoundPs: Multiply packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', alternatively subtract and add
// packed elements in 'c' from/to the intermediate result, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm512_mask_fmsubadd_round_ps'.
// Requires AVX512F.
func MaskFmsubaddRoundPs(a M512, k Mmask16, b M512, c M512, rounding int) M512 {
	return M512(maskFmsubaddRoundPs([16]float32(a), uint16(k), [16]float32(b), [16]float32(c), rounding))
}

func maskFmsubaddRoundPs(a [16]float32, k uint16, b [16]float32, c [16]float32, rounding int) [16]float32


// Mask3FmsubaddRoundPs: Multiply packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', alternatively subtract and add
// packed elements in 'c' from/to the intermediate result, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'c' when the
// corresponding mask bit is not set).  Rounding is done according to the
// 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm512_mask3_fmsubadd_round_ps'.
// Requires AVX512F.
func Mask3FmsubaddRoundPs(a M512, b M512, c M512, k Mmask16, rounding int) M512 {
	return M512(mask3FmsubaddRoundPs([16]float32(a), [16]float32(b), [16]float32(c), uint16(k), rounding))
}

func mask3FmsubaddRoundPs(a [16]float32, b [16]float32, c [16]float32, k uint16, rounding int) [16]float32


// MaskzFmsubaddRoundPs: Multiply packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', alternatively subtract and add
// packed elements in 'c' from/to the intermediate result, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm512_maskz_fmsubadd_round_ps'.
// Requires AVX512F.
func MaskzFmsubaddRoundPs(k Mmask16, a M512, b M512, c M512, rounding int) M512 {
	return M512(maskzFmsubaddRoundPs(uint16(k), [16]float32(a), [16]float32(b), [16]float32(c), rounding))
}

func maskzFmsubaddRoundPs(k uint16, a [16]float32, b [16]float32, c [16]float32, rounding int) [16]float32


// MaskFnmaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR	
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132PD, VFNMADD213PD, VFNMADD231PD'. Intrinsic: '_mm_mask_fnmadd_pd'.
// Requires AVX512F.
func MaskFnmaddPd(a M128d, k Mmask8, b M128d, c M128d) M128d {
	return M128d(maskFnmaddPd([2]float64(a), uint8(k), [2]float64(b), [2]float64(c)))
}

func maskFnmaddPd(a [2]float64, k uint8, b [2]float64, c [2]float64) [2]float64


// Mask3FnmaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR	
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132PD, VFNMADD213PD, VFNMADD231PD'. Intrinsic: '_mm_mask3_fnmadd_pd'.
// Requires AVX512F.
func Mask3FnmaddPd(a M128d, b M128d, c M128d, k Mmask8) M128d {
	return M128d(mask3FnmaddPd([2]float64(a), [2]float64(b), [2]float64(c), uint8(k)))
}

func mask3FnmaddPd(a [2]float64, b [2]float64, c [2]float64, k uint8) [2]float64


// MaskzFnmaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132PD, VFNMADD213PD, VFNMADD231PD'. Intrinsic: '_mm_maskz_fnmadd_pd'.
// Requires AVX512F.
func MaskzFnmaddPd(k Mmask8, a M128d, b M128d, c M128d) M128d {
	return M128d(maskzFnmaddPd(uint8(k), [2]float64(a), [2]float64(b), [2]float64(c)))
}

func maskzFnmaddPd(k uint8, a [2]float64, b [2]float64, c [2]float64) [2]float64


// MaskFnmaddPd1: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMADD132PD, VFNMADD213PD, VFNMADD231PD'. Intrinsic: '_mm256_mask_fnmadd_pd'.
// Requires AVX512F.
func MaskFnmaddPd1(a M256d, k Mmask8, b M256d, c M256d) M256d {
	return M256d(maskFnmaddPd1([4]float64(a), uint8(k), [4]float64(b), [4]float64(c)))
}

func maskFnmaddPd1(a [4]float64, k uint8, b [4]float64, c [4]float64) [4]float64


// Mask3FnmaddPd1: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMADD132PD, VFNMADD213PD, VFNMADD231PD'. Intrinsic: '_mm256_mask3_fnmadd_pd'.
// Requires AVX512F.
func Mask3FnmaddPd1(a M256d, b M256d, c M256d, k Mmask8) M256d {
	return M256d(mask3FnmaddPd1([4]float64(a), [4]float64(b), [4]float64(c), uint8(k)))
}

func mask3FnmaddPd1(a [4]float64, b [4]float64, c [4]float64, k uint8) [4]float64


// MaskzFnmaddPd1: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMADD132PD, VFNMADD213PD, VFNMADD231PD'. Intrinsic: '_mm256_maskz_fnmadd_pd'.
// Requires AVX512F.
func MaskzFnmaddPd1(k Mmask8, a M256d, b M256d, c M256d) M256d {
	return M256d(maskzFnmaddPd1(uint8(k), [4]float64(a), [4]float64(b), [4]float64(c)))
}

func maskzFnmaddPd1(k uint8, a [4]float64, b [4]float64, c [4]float64) [4]float64


// MaskzFnmaddPd2: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMADD132PD, VFNMADD213PD, VFNMADD231PD'. Intrinsic: '_mm512_maskz_fnmadd_pd'.
// Requires AVX512F.
func MaskzFnmaddPd2(k Mmask8, a M512d, b M512d, c M512d) M512d {
	return M512d(maskzFnmaddPd2(uint8(k), [8]float64(a), [8]float64(b), [8]float64(c)))
}

func maskzFnmaddPd2(k uint8, a [8]float64, b [8]float64, c [8]float64) [8]float64


// MaskFnmaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR	
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132PS, VFNMADD213PS, VFNMADD231PS'. Intrinsic: '_mm_mask_fnmadd_ps'.
// Requires AVX512F.
func MaskFnmaddPs(a M128, k Mmask8, b M128, c M128) M128 {
	return M128(maskFnmaddPs([4]float32(a), uint8(k), [4]float32(b), [4]float32(c)))
}

func maskFnmaddPs(a [4]float32, k uint8, b [4]float32, c [4]float32) [4]float32


// Mask3FnmaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR	
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132PS, VFNMADD213PS, VFNMADD231PS'. Intrinsic: '_mm_mask3_fnmadd_ps'.
// Requires AVX512F.
func Mask3FnmaddPs(a M128, b M128, c M128, k Mmask8) M128 {
	return M128(mask3FnmaddPs([4]float32(a), [4]float32(b), [4]float32(c), uint8(k)))
}

func mask3FnmaddPs(a [4]float32, b [4]float32, c [4]float32, k uint8) [4]float32


// MaskzFnmaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132PS, VFNMADD213PS, VFNMADD231PS'. Intrinsic: '_mm_maskz_fnmadd_ps'.
// Requires AVX512F.
func MaskzFnmaddPs(k Mmask8, a M128, b M128, c M128) M128 {
	return M128(maskzFnmaddPs(uint8(k), [4]float32(a), [4]float32(b), [4]float32(c)))
}

func maskzFnmaddPs(k uint8, a [4]float32, b [4]float32, c [4]float32) [4]float32


// MaskFnmaddPs1: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMADD132PS, VFNMADD213PS, VFNMADD231PS'. Intrinsic: '_mm256_mask_fnmadd_ps'.
// Requires AVX512F.
func MaskFnmaddPs1(a M256, k Mmask8, b M256, c M256) M256 {
	return M256(maskFnmaddPs1([8]float32(a), uint8(k), [8]float32(b), [8]float32(c)))
}

func maskFnmaddPs1(a [8]float32, k uint8, b [8]float32, c [8]float32) [8]float32


// Mask3FnmaddPs1: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMADD132PS, VFNMADD213PS, VFNMADD231PS'. Intrinsic: '_mm256_mask3_fnmadd_ps'.
// Requires AVX512F.
func Mask3FnmaddPs1(a M256, b M256, c M256, k Mmask8) M256 {
	return M256(mask3FnmaddPs1([8]float32(a), [8]float32(b), [8]float32(c), uint8(k)))
}

func mask3FnmaddPs1(a [8]float32, b [8]float32, c [8]float32, k uint8) [8]float32


// MaskzFnmaddPs1: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMADD132PS, VFNMADD213PS, VFNMADD231PS'. Intrinsic: '_mm256_maskz_fnmadd_ps'.
// Requires AVX512F.
func MaskzFnmaddPs1(k Mmask8, a M256, b M256, c M256) M256 {
	return M256(maskzFnmaddPs1(uint8(k), [8]float32(a), [8]float32(b), [8]float32(c)))
}

func maskzFnmaddPs1(k uint8, a [8]float32, b [8]float32, c [8]float32) [8]float32


// MaskzFnmaddPs2: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMADD132PS, VFNMADD213PS, VFNMADD231PS'. Intrinsic: '_mm512_maskz_fnmadd_ps'.
// Requires AVX512F.
func MaskzFnmaddPs2(k Mmask16, a M512, b M512, c M512) M512 {
	return M512(maskzFnmaddPs2(uint16(k), [16]float32(a), [16]float32(b), [16]float32(c)))
}

func maskzFnmaddPs2(k uint16, a [16]float32, b [16]float32, c [16]float32) [16]float32


// MaskzFnmaddRoundPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). Rounding is done
// according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMADD132PD, VFNMADD213PD, VFNMADD231PD'. Intrinsic: '_mm512_maskz_fnmadd_round_pd'.
// Requires AVX512F.
func MaskzFnmaddRoundPd(k Mmask8, a M512d, b M512d, c M512d, rounding int) M512d {
	return M512d(maskzFnmaddRoundPd(uint8(k), [8]float64(a), [8]float64(b), [8]float64(c), rounding))
}

func maskzFnmaddRoundPd(k uint8, a [8]float64, b [8]float64, c [8]float64, rounding int) [8]float64


// MaskzFnmaddRoundPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). Rounding is done
// according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMADD132PS, VFNMADD213PS, VFNMADD231PS'. Intrinsic: '_mm512_maskz_fnmadd_round_ps'.
// Requires AVX512F.
func MaskzFnmaddRoundPs(k Mmask16, a M512, b M512, c M512, rounding int) M512 {
	return M512(maskzFnmaddRoundPs(uint16(k), [16]float32(a), [16]float32(b), [16]float32(c), rounding))
}

func maskzFnmaddRoundPs(k uint16, a [16]float32, b [16]float32, c [16]float32, rounding int) [16]float32


// MaskFnmaddRoundSd: Multiply the lower double-precision (64-bit)
// floating-point elements in 'a' and 'b', and add the negated intermediate
// result to the lower element in 'c'. Store the result in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'a' when mask bit 0 is
// not set), and copy the upper element from 'a' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := -(a[63:0] * b[63:0]) + c[63:0]
//		ELSE
//			dst[63:0] := a[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132SD, VFNMADD213SD, VFNMADD231SD'. Intrinsic: '_mm_mask_fnmadd_round_sd'.
// Requires AVX512F.
func MaskFnmaddRoundSd(a M128d, k Mmask8, b M128d, c M128d, rounding int) M128d {
	return M128d(maskFnmaddRoundSd([2]float64(a), uint8(k), [2]float64(b), [2]float64(c), rounding))
}

func maskFnmaddRoundSd(a [2]float64, k uint8, b [2]float64, c [2]float64, rounding int) [2]float64


// Mask3FnmaddRoundSd: Multiply the lower double-precision (64-bit)
// floating-point elements in 'a' and 'b', and add the negated intermediate
// result to the lower element in 'c'. Store the result in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'c' when mask bit 0 is
// not set), and copy the upper element from 'a' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := -(a[63:0] * b[63:0]) + c[63:0]
//		ELSE
//			dst[63:0] := c[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132SD, VFNMADD213SD, VFNMADD231SD'. Intrinsic: '_mm_mask3_fnmadd_round_sd'.
// Requires AVX512F.
func Mask3FnmaddRoundSd(a M128d, b M128d, c M128d, k Mmask8, rounding int) M128d {
	return M128d(mask3FnmaddRoundSd([2]float64(a), [2]float64(b), [2]float64(c), uint8(k), rounding))
}

func mask3FnmaddRoundSd(a [2]float64, b [2]float64, c [2]float64, k uint8, rounding int) [2]float64


// MaskzFnmaddRoundSd: Multiply the lower double-precision (64-bit)
// floating-point elements in 'a' and 'b', and add the negated intermediate
// result to the lower element in 'c'. Store the result in the lower element of
// 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is not
// set), and copy the upper element from 'a' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := -(a[63:0] * b[63:0]) + c[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132SD, VFNMADD213SD, VFNMADD231SD'. Intrinsic: '_mm_maskz_fnmadd_round_sd'.
// Requires AVX512F.
func MaskzFnmaddRoundSd(k Mmask8, a M128d, b M128d, c M128d, rounding int) M128d {
	return M128d(maskzFnmaddRoundSd(uint8(k), [2]float64(a), [2]float64(b), [2]float64(c), rounding))
}

func maskzFnmaddRoundSd(k uint8, a [2]float64, b [2]float64, c [2]float64, rounding int) [2]float64


// MaskFnmaddRoundSs: Multiply the lower single-precision (32-bit)
// floating-point elements in 'a' and 'b', and add the negated intermediate
// result to the lower element in 'c'. Store the result in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'a' when mask bit 0 is
// not set), and copy the upper 3 packed elements from 'a' to the upper
// elements of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := -(a[31:0] * b[31:0]) + c[31:0]
//		ELSE
//			dst[31:0] := a[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132SS, VFNMADD213SS, VFNMADD231SS'. Intrinsic: '_mm_mask_fnmadd_round_ss'.
// Requires AVX512F.
func MaskFnmaddRoundSs(a M128, k Mmask8, b M128, c M128, rounding int) M128 {
	return M128(maskFnmaddRoundSs([4]float32(a), uint8(k), [4]float32(b), [4]float32(c), rounding))
}

func maskFnmaddRoundSs(a [4]float32, k uint8, b [4]float32, c [4]float32, rounding int) [4]float32


// Mask3FnmaddRoundSs: Multiply the lower single-precision (32-bit)
// floating-point elements in 'a' and 'b', and add the negated intermediate
// result to the lower element in 'c'. Store the result in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'c' when mask bit 0 is
// not set), and copy the upper 3 packed elements from 'a' to the upper
// elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := -(a[31:0] * b[31:0]) + c[31:0]
//		ELSE
//			dst[31:0] := c[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132SS, VFNMADD213SS, VFNMADD231SS'. Intrinsic: '_mm_mask3_fnmadd_round_ss'.
// Requires AVX512F.
func Mask3FnmaddRoundSs(a M128, b M128, c M128, k Mmask8, rounding int) M128 {
	return M128(mask3FnmaddRoundSs([4]float32(a), [4]float32(b), [4]float32(c), uint8(k), rounding))
}

func mask3FnmaddRoundSs(a [4]float32, b [4]float32, c [4]float32, k uint8, rounding int) [4]float32


// MaskzFnmaddRoundSs: Multiply the lower single-precision (32-bit)
// floating-point elements in 'a' and 'b', and add the negated intermediate
// result to the lower element in 'c'. Store the result in the lower element of
// 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is not
// set), and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := -(a[31:0] * b[31:0]) + c[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132SS, VFNMADD213SS, VFNMADD231SS'. Intrinsic: '_mm_maskz_fnmadd_round_ss'.
// Requires AVX512F.
func MaskzFnmaddRoundSs(k Mmask8, a M128, b M128, c M128, rounding int) M128 {
	return M128(maskzFnmaddRoundSs(uint8(k), [4]float32(a), [4]float32(b), [4]float32(c), rounding))
}

func maskzFnmaddRoundSs(k uint8, a [4]float32, b [4]float32, c [4]float32, rounding int) [4]float32


// MaskFnmaddSd: Multiply the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', and add the negated intermediate result to the
// lower element in 'c'. Store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'a' when mask bit 0 is not set),
// and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := -(a[63:0] * b[63:0]) + c[63:0]
//		ELSE
//			dst[63:0] := a[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132SD, VFNMADD213SD, VFNMADD231SD'. Intrinsic: '_mm_mask_fnmadd_sd'.
// Requires AVX512F.
func MaskFnmaddSd(a M128d, k Mmask8, b M128d, c M128d) M128d {
	return M128d(maskFnmaddSd([2]float64(a), uint8(k), [2]float64(b), [2]float64(c)))
}

func maskFnmaddSd(a [2]float64, k uint8, b [2]float64, c [2]float64) [2]float64


// Mask3FnmaddSd: Multiply the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', and add the negated intermediate result to the
// lower element in 'c'. Store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'c' when mask bit 0 is not set),
// and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := -(a[63:0] * b[63:0]) + c[63:0]
//		ELSE
//			dst[63:0] := c[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132SD, VFNMADD213SD, VFNMADD231SD'. Intrinsic: '_mm_mask3_fnmadd_sd'.
// Requires AVX512F.
func Mask3FnmaddSd(a M128d, b M128d, c M128d, k Mmask8) M128d {
	return M128d(mask3FnmaddSd([2]float64(a), [2]float64(b), [2]float64(c), uint8(k)))
}

func mask3FnmaddSd(a [2]float64, b [2]float64, c [2]float64, k uint8) [2]float64


// MaskzFnmaddSd: Multiply the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', and add the negated intermediate result to the
// lower element in 'c'. Store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := -(a[63:0] * b[63:0]) + c[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD213SD, VFNMADD231SD, VFNMADD132SD'. Intrinsic: '_mm_maskz_fnmadd_sd'.
// Requires AVX512F.
func MaskzFnmaddSd(k Mmask8, a M128d, b M128d, c M128d) M128d {
	return M128d(maskzFnmaddSd(uint8(k), [2]float64(a), [2]float64(b), [2]float64(c)))
}

func maskzFnmaddSd(k uint8, a [2]float64, b [2]float64, c [2]float64) [2]float64


// MaskFnmaddSs: Multiply the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', and add the negated intermediate result to the
// lower element in 'c'. Store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'a' when mask bit 0 is not set),
// and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'. 
//
//		IF k[0]
//			dst[31:0] := -(a[31:0] * b[31:0]) + c[31:0]
//		ELSE
//			dst[31:0] := a[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132SS, VFNMADD213SS, VFNMADD231SS'. Intrinsic: '_mm_mask_fnmadd_ss'.
// Requires AVX512F.
func MaskFnmaddSs(a M128, k Mmask8, b M128, c M128) M128 {
	return M128(maskFnmaddSs([4]float32(a), uint8(k), [4]float32(b), [4]float32(c)))
}

func maskFnmaddSs(a [4]float32, k uint8, b [4]float32, c [4]float32) [4]float32


// Mask3FnmaddSs: Multiply the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', and add the negated intermediate result to the
// lower element in 'c'. Store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'c' when mask bit 0 is not set),
// and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'. 
//
//		IF k[0]
//			dst[31:0] := -(a[31:0] * b[31:0]) + c[31:0]
//		ELSE
//			dst[31:0] := c[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132SS, VFNMADD213SS, VFNMADD231SS'. Intrinsic: '_mm_mask3_fnmadd_ss'.
// Requires AVX512F.
func Mask3FnmaddSs(a M128, b M128, c M128, k Mmask8) M128 {
	return M128(mask3FnmaddSs([4]float32(a), [4]float32(b), [4]float32(c), uint8(k)))
}

func mask3FnmaddSs(a [4]float32, b [4]float32, c [4]float32, k uint8) [4]float32


// MaskzFnmaddSs: Multiply the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', and add the negated intermediate result to the
// lower element in 'c'. Store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := -(a[31:0] * b[31:0]) + c[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132SS, VFNMADD213SS, VFNMADD231SS'. Intrinsic: '_mm_maskz_fnmadd_ss'.
// Requires AVX512F.
func MaskzFnmaddSs(k Mmask8, a M128, b M128, c M128) M128 {
	return M128(maskzFnmaddSs(uint8(k), [4]float32(a), [4]float32(b), [4]float32(c)))
}

func maskzFnmaddSs(k uint8, a [4]float32, b [4]float32, c [4]float32) [4]float32


// MaskFnmsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR	
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD'. Intrinsic: '_mm_mask_fnmsub_pd'.
// Requires AVX512F.
func MaskFnmsubPd(a M128d, k Mmask8, b M128d, c M128d) M128d {
	return M128d(maskFnmsubPd([2]float64(a), uint8(k), [2]float64(b), [2]float64(c)))
}

func maskFnmsubPd(a [2]float64, k uint8, b [2]float64, c [2]float64) [2]float64


// Mask3FnmsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR	
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD'. Intrinsic: '_mm_mask3_fnmsub_pd'.
// Requires AVX512F.
func Mask3FnmsubPd(a M128d, b M128d, c M128d, k Mmask8) M128d {
	return M128d(mask3FnmsubPd([2]float64(a), [2]float64(b), [2]float64(c), uint8(k)))
}

func mask3FnmsubPd(a [2]float64, b [2]float64, c [2]float64, k uint8) [2]float64


// MaskzFnmsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD'. Intrinsic: '_mm_maskz_fnmsub_pd'.
// Requires AVX512F.
func MaskzFnmsubPd(k Mmask8, a M128d, b M128d, c M128d) M128d {
	return M128d(maskzFnmsubPd(uint8(k), [2]float64(a), [2]float64(b), [2]float64(c)))
}

func maskzFnmsubPd(k uint8, a [2]float64, b [2]float64, c [2]float64) [2]float64


// MaskFnmsubPd1: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD'. Intrinsic: '_mm256_mask_fnmsub_pd'.
// Requires AVX512F.
func MaskFnmsubPd1(a M256d, k Mmask8, b M256d, c M256d) M256d {
	return M256d(maskFnmsubPd1([4]float64(a), uint8(k), [4]float64(b), [4]float64(c)))
}

func maskFnmsubPd1(a [4]float64, k uint8, b [4]float64, c [4]float64) [4]float64


// Mask3FnmsubPd1: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD'. Intrinsic: '_mm256_mask3_fnmsub_pd'.
// Requires AVX512F.
func Mask3FnmsubPd1(a M256d, b M256d, c M256d, k Mmask8) M256d {
	return M256d(mask3FnmsubPd1([4]float64(a), [4]float64(b), [4]float64(c), uint8(k)))
}

func mask3FnmsubPd1(a [4]float64, b [4]float64, c [4]float64, k uint8) [4]float64


// MaskzFnmsubPd1: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD'. Intrinsic: '_mm256_maskz_fnmsub_pd'.
// Requires AVX512F.
func MaskzFnmsubPd1(k Mmask8, a M256d, b M256d, c M256d) M256d {
	return M256d(maskzFnmsubPd1(uint8(k), [4]float64(a), [4]float64(b), [4]float64(c)))
}

func maskzFnmsubPd1(k uint8, a [4]float64, b [4]float64, c [4]float64) [4]float64


// MaskzFnmsubPd2: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD'. Intrinsic: '_mm512_maskz_fnmsub_pd'.
// Requires AVX512F.
func MaskzFnmsubPd2(k Mmask8, a M512d, b M512d, c M512d) M512d {
	return M512d(maskzFnmsubPd2(uint8(k), [8]float64(a), [8]float64(b), [8]float64(c)))
}

func maskzFnmsubPd2(k uint8, a [8]float64, b [8]float64, c [8]float64) [8]float64


// MaskFnmsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR	
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS'. Intrinsic: '_mm_mask_fnmsub_ps'.
// Requires AVX512F.
func MaskFnmsubPs(a M128, k Mmask8, b M128, c M128) M128 {
	return M128(maskFnmsubPs([4]float32(a), uint8(k), [4]float32(b), [4]float32(c)))
}

func maskFnmsubPs(a [4]float32, k uint8, b [4]float32, c [4]float32) [4]float32


// Mask3FnmsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR	
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS'. Intrinsic: '_mm_mask3_fnmsub_ps'.
// Requires AVX512F.
func Mask3FnmsubPs(a M128, b M128, c M128, k Mmask8) M128 {
	return M128(mask3FnmsubPs([4]float32(a), [4]float32(b), [4]float32(c), uint8(k)))
}

func mask3FnmsubPs(a [4]float32, b [4]float32, c [4]float32, k uint8) [4]float32


// MaskzFnmsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS'. Intrinsic: '_mm_maskz_fnmsub_ps'.
// Requires AVX512F.
func MaskzFnmsubPs(k Mmask8, a M128, b M128, c M128) M128 {
	return M128(maskzFnmsubPs(uint8(k), [4]float32(a), [4]float32(b), [4]float32(c)))
}

func maskzFnmsubPs(k uint8, a [4]float32, b [4]float32, c [4]float32) [4]float32


// MaskFnmsubPs1: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS'. Intrinsic: '_mm256_mask_fnmsub_ps'.
// Requires AVX512F.
func MaskFnmsubPs1(a M256, k Mmask8, b M256, c M256) M256 {
	return M256(maskFnmsubPs1([8]float32(a), uint8(k), [8]float32(b), [8]float32(c)))
}

func maskFnmsubPs1(a [8]float32, k uint8, b [8]float32, c [8]float32) [8]float32


// Mask3FnmsubPs1: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS'. Intrinsic: '_mm256_mask3_fnmsub_ps'.
// Requires AVX512F.
func Mask3FnmsubPs1(a M256, b M256, c M256, k Mmask8) M256 {
	return M256(mask3FnmsubPs1([8]float32(a), [8]float32(b), [8]float32(c), uint8(k)))
}

func mask3FnmsubPs1(a [8]float32, b [8]float32, c [8]float32, k uint8) [8]float32


// MaskzFnmsubPs1: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS'. Intrinsic: '_mm256_maskz_fnmsub_ps'.
// Requires AVX512F.
func MaskzFnmsubPs1(k Mmask8, a M256, b M256, c M256) M256 {
	return M256(maskzFnmsubPs1(uint8(k), [8]float32(a), [8]float32(b), [8]float32(c)))
}

func maskzFnmsubPs1(k uint8, a [8]float32, b [8]float32, c [8]float32) [8]float32


// MaskzFnmsubPs2: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS'. Intrinsic: '_mm512_maskz_fnmsub_ps'.
// Requires AVX512F.
func MaskzFnmsubPs2(k Mmask16, a M512, b M512, c M512) M512 {
	return M512(maskzFnmsubPs2(uint16(k), [16]float32(a), [16]float32(b), [16]float32(c)))
}

func maskzFnmsubPs2(k uint16, a [16]float32, b [16]float32, c [16]float32) [16]float32


// MaskzFnmsubRoundPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set).
// Rounding is done according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD'. Intrinsic: '_mm512_maskz_fnmsub_round_pd'.
// Requires AVX512F.
func MaskzFnmsubRoundPd(k Mmask8, a M512d, b M512d, c M512d, rounding int) M512d {
	return M512d(maskzFnmsubRoundPd(uint8(k), [8]float64(a), [8]float64(b), [8]float64(c), rounding))
}

func maskzFnmsubRoundPd(k uint8, a [8]float64, b [8]float64, c [8]float64, rounding int) [8]float64


// MaskzFnmsubRoundPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS'. Intrinsic: '_mm512_maskz_fnmsub_round_ps'.
// Requires AVX512F.
func MaskzFnmsubRoundPs(k Mmask16, a M512, b M512, c M512, rounding int) M512 {
	return M512(maskzFnmsubRoundPs(uint16(k), [16]float32(a), [16]float32(b), [16]float32(c), rounding))
}

func maskzFnmsubRoundPs(k uint16, a [16]float32, b [16]float32, c [16]float32, rounding int) [16]float32


// MaskFnmsubRoundSd: Multiply the lower double-precision (64-bit)
// floating-point elements in 'a' and 'b', and subtract the lower element in
// 'c' from the negated intermediate result. Store the result in the lower
// element of 'dst' using writemask 'k' (the element is copied from 'c' when
// mask bit 0 is not set), and copy the upper element from 'a' to the upper
// element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := -(a[63:0] * b[63:0]) - c[63:0]
//		ELSE
//			dst[63:0] := a[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132SD, VFNMSUB213SD, VFNMSUB231SD'. Intrinsic: '_mm_mask_fnmsub_round_sd'.
// Requires AVX512F.
func MaskFnmsubRoundSd(a M128d, k Mmask8, b M128d, c M128d, rounding int) M128d {
	return M128d(maskFnmsubRoundSd([2]float64(a), uint8(k), [2]float64(b), [2]float64(c), rounding))
}

func maskFnmsubRoundSd(a [2]float64, k uint8, b [2]float64, c [2]float64, rounding int) [2]float64


// Mask3FnmsubRoundSd: Multiply the lower double-precision (64-bit)
// floating-point elements in 'a' and 'b', and subtract the lower element in
// 'c' from the negated intermediate result. Store the result in the lower
// element of 'dst' using writemask 'k' (the element is copied from 'c' when
// mask bit 0 is not set), and copy the upper element from 'a' to the upper
// element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := -(a[63:0] * b[63:0]) - c[63:0]
//		ELSE
//			dst[63:0] := c[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132SD, VFNMSUB213SD, VFNMSUB231SD'. Intrinsic: '_mm_mask3_fnmsub_round_sd'.
// Requires AVX512F.
func Mask3FnmsubRoundSd(a M128d, b M128d, c M128d, k Mmask8, rounding int) M128d {
	return M128d(mask3FnmsubRoundSd([2]float64(a), [2]float64(b), [2]float64(c), uint8(k), rounding))
}

func mask3FnmsubRoundSd(a [2]float64, b [2]float64, c [2]float64, k uint8, rounding int) [2]float64


// MaskzFnmsubRoundSd: Multiply the lower double-precision (64-bit)
// floating-point elements in 'a' and 'b', and subtract the lower element in
// 'c' from the negated intermediate result. Store the result in 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper element from 'a' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := -(a[63:0] * b[63:0]) - c[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132SD, VFNMSUB213SD, VFNMSUB231SD'. Intrinsic: '_mm_maskz_fnmsub_round_sd'.
// Requires AVX512F.
func MaskzFnmsubRoundSd(k Mmask8, a M128d, b M128d, c M128d, rounding int) M128d {
	return M128d(maskzFnmsubRoundSd(uint8(k), [2]float64(a), [2]float64(b), [2]float64(c), rounding))
}

func maskzFnmsubRoundSd(k uint8, a [2]float64, b [2]float64, c [2]float64, rounding int) [2]float64


// MaskFnmsubRoundSs: Multiply the lower single-precision (32-bit)
// floating-point elements in 'a' and 'b', and subtract the lower element in
// 'c' from the negated intermediate result. Store the result in the lower
// element of 'dst' using writemask 'k' (the element is copied from 'c' when
// mask bit 0 is not set), and copy the upper 3 packed elements from 'a' to the
// upper elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := -(a[31:0] * b[31:0]) - c[31:0]
//		ELSE
//			dst[31:0] := a[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132SS, VFNMSUB213SS, VFNMSUB231SS'. Intrinsic: '_mm_mask_fnmsub_round_ss'.
// Requires AVX512F.
func MaskFnmsubRoundSs(a M128, k Mmask8, b M128, c M128, rounding int) M128 {
	return M128(maskFnmsubRoundSs([4]float32(a), uint8(k), [4]float32(b), [4]float32(c), rounding))
}

func maskFnmsubRoundSs(a [4]float32, k uint8, b [4]float32, c [4]float32, rounding int) [4]float32


// Mask3FnmsubRoundSs: Multiply the lower single-precision (32-bit)
// floating-point elements in 'a' and 'b', subtract the lower element in 'c'
// from the negated intermediate result, store the result in the lower element
// of 'dst', and copy the upper element from 'a' to the upper element of 'dst'
// using writemask 'k' (elements are copied from 'c' when the corresponding
// mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := -(a[31:0] * b[31:0]) - c[31:0]
//		ELSE
//			dst[31:0] := c[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132SS, VFNMSUB213SS, VFNMSUB231SS'. Intrinsic: '_mm_mask3_fnmsub_round_ss'.
// Requires AVX512F.
func Mask3FnmsubRoundSs(a M128, b M128, c M128, k Mmask8, rounding int) M128 {
	return M128(mask3FnmsubRoundSs([4]float32(a), [4]float32(b), [4]float32(c), uint8(k), rounding))
}

func mask3FnmsubRoundSs(a [4]float32, b [4]float32, c [4]float32, k uint8, rounding int) [4]float32


// MaskzFnmsubRoundSs: Multiply the lower single-precision (32-bit)
// floating-point elements in 'a' and 'b', and subtract the lower element in
// 'c' from the negated intermediate result. Store the result in the lower
// element of 'dst' using zeromask 'k' (the element is zeroed out when mask bit
// 0 is not set), and copy the upper 3 packed elements from 'a' to the upper
// elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := -(a[31:0] * b[31:0]) - c[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132SS, VFNMSUB213SS, VFNMSUB231SS'. Intrinsic: '_mm_maskz_fnmsub_round_ss'.
// Requires AVX512F.
func MaskzFnmsubRoundSs(k Mmask8, a M128, b M128, c M128, rounding int) M128 {
	return M128(maskzFnmsubRoundSs(uint8(k), [4]float32(a), [4]float32(b), [4]float32(c), rounding))
}

func maskzFnmsubRoundSs(k uint8, a [4]float32, b [4]float32, c [4]float32, rounding int) [4]float32


// MaskFnmsubSd: Multiply the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', and subtract the lower element in 'c' from the
// negated intermediate result. Store the result in the lower element of 'dst'
// using writemask 'k' (the element is copied from 'c' when mask bit 0 is not
// set), and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := -(a[63:0] * b[63:0]) - c[63:0]
//		ELSE
//			dst[63:0] := a[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132SD, VFNMSUB213SD, VFNMSUB231SD'. Intrinsic: '_mm_mask_fnmsub_sd'.
// Requires AVX512F.
func MaskFnmsubSd(a M128d, k Mmask8, b M128d, c M128d) M128d {
	return M128d(maskFnmsubSd([2]float64(a), uint8(k), [2]float64(b), [2]float64(c)))
}

func maskFnmsubSd(a [2]float64, k uint8, b [2]float64, c [2]float64) [2]float64


// Mask3FnmsubSd: Multiply the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', and subtract the lower element in 'c' from the
// negated intermediate result. Store the result in the lower element of 'dst'
// using writemask 'k' (the element is copied from 'c' when mask bit 0 is not
// set), and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := -(a[63:0] * b[63:0]) - c[63:0]
//		ELSE
//			dst[63:0] := c[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132SD, VFNMSUB213SD, VFNMSUB231SD'. Intrinsic: '_mm_mask3_fnmsub_sd'.
// Requires AVX512F.
func Mask3FnmsubSd(a M128d, b M128d, c M128d, k Mmask8) M128d {
	return M128d(mask3FnmsubSd([2]float64(a), [2]float64(b), [2]float64(c), uint8(k)))
}

func mask3FnmsubSd(a [2]float64, b [2]float64, c [2]float64, k uint8) [2]float64


// MaskzFnmsubSd: Multiply the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', and subtract the lower element in 'c' from the
// negated intermediate result. Store the result in 'dst' using zeromask 'k'
// (the element is zeroed out when mask bit 0 is not set), and copy the upper
// element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := -(a[63:0] * b[63:0]) - c[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132SD, VFNMSUB213SD, VFNMSUB231SD'. Intrinsic: '_mm_maskz_fnmsub_sd'.
// Requires AVX512F.
func MaskzFnmsubSd(k Mmask8, a M128d, b M128d, c M128d) M128d {
	return M128d(maskzFnmsubSd(uint8(k), [2]float64(a), [2]float64(b), [2]float64(c)))
}

func maskzFnmsubSd(k uint8, a [2]float64, b [2]float64, c [2]float64) [2]float64


// MaskFnmsubSs: Multiply the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', and subtract the lower element in 'c' from the
// negated intermediate result. Store the result in the lower element of 'dst'
// using writemask 'k' (the element is copied from 'c' when mask bit 0 is not
// set), and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'. 
//
//		IF k[0]
//			dst[31:0] := -(a[31:0] * b[31:0]) - c[31:0]
//		ELSE
//			dst[31:0] := a[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132SS, VFNMSUB213SS, VFNMSUB231SS'. Intrinsic: '_mm_mask_fnmsub_ss'.
// Requires AVX512F.
func MaskFnmsubSs(a M128, k Mmask8, b M128, c M128) M128 {
	return M128(maskFnmsubSs([4]float32(a), uint8(k), [4]float32(b), [4]float32(c)))
}

func maskFnmsubSs(a [4]float32, k uint8, b [4]float32, c [4]float32) [4]float32


// Mask3FnmsubSs: Multiply the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', and subtract the lower element in 'c' from the
// negated intermediate result. Store the result in the lower element of 'dst',
// and copy the upper element from 'a' to the upper element of 'dst' using
// writemask 'k' (elements are copied from 'c' when the corresponding mask bit
// is not set). 
//
//		IF k[0]
//			dst[31:0] := -(a[31:0] * b[31:0]) - c[31:0]
//		ELSE
//			dst[31:0] := c[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132SS, VFNMSUB213SS, VFNMSUB231SS'. Intrinsic: '_mm_mask3_fnmsub_ss'.
// Requires AVX512F.
func Mask3FnmsubSs(a M128, b M128, c M128, k Mmask8) M128 {
	return M128(mask3FnmsubSs([4]float32(a), [4]float32(b), [4]float32(c), uint8(k)))
}

func mask3FnmsubSs(a [4]float32, b [4]float32, c [4]float32, k uint8) [4]float32


// MaskzFnmsubSs: Multiply the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', and subtract the lower element in 'c' from the
// negated intermediate result. Store the result in the lower element of 'dst'
// using zeromask 'k' (the element is zeroed out when mask bit 0 is not set),
// and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'. 
//
//		IF k[0]
//			dst[31:0] := -(a[31:0] * b[31:0]) - c[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132SS, VFNMSUB213SS, VFNMSUB231SS'. Intrinsic: '_mm_maskz_fnmsub_ss'.
// Requires AVX512F.
func MaskzFnmsubSs(k Mmask8, a M128, b M128, c M128) M128 {
	return M128(maskzFnmsubSs(uint8(k), [4]float32(a), [4]float32(b), [4]float32(c)))
}

func maskzFnmsubSs(k uint8, a [4]float32, b [4]float32, c [4]float32) [4]float32


// GetexpPd: Convert the exponent of each packed double-precision (64-bit)
// floating-point element in 'a' to a double-precision (64-bit) floating-point
// number representing the integer exponent, and store the results in 'dst'.
// This intrinsic essentially calculates 'floor(log2(x))' for each element. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := ConvertExpFP64(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPPD'. Intrinsic: '_mm_getexp_pd'.
// Requires AVX512F.
func GetexpPd(a M128d) M128d {
	return M128d(getexpPd([2]float64(a)))
}

func getexpPd(a [2]float64) [2]float64


// MaskGetexpPd: Convert the exponent of each packed double-precision (64-bit)
// floating-point element in 'a' to a double-precision (64-bit) floating-point
// number representing the integer exponent, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). This intrinsic essentially calculates 'floor(log2(x))'
// for each element. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ConvertExpFP64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPPD'. Intrinsic: '_mm_mask_getexp_pd'.
// Requires AVX512F.
func MaskGetexpPd(src M128d, k Mmask8, a M128d) M128d {
	return M128d(maskGetexpPd([2]float64(src), uint8(k), [2]float64(a)))
}

func maskGetexpPd(src [2]float64, k uint8, a [2]float64) [2]float64


// MaskzGetexpPd: Convert the exponent of each packed double-precision (64-bit)
// floating-point element in 'a' to a double-precision (64-bit) floating-point
// number representing the integer exponent, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). This intrinsic essentially calculates 'floor(log2(x))' for each
// element. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ConvertExpFP64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPPD'. Intrinsic: '_mm_maskz_getexp_pd'.
// Requires AVX512F.
func MaskzGetexpPd(k Mmask8, a M128d) M128d {
	return M128d(maskzGetexpPd(uint8(k), [2]float64(a)))
}

func maskzGetexpPd(k uint8, a [2]float64) [2]float64


// GetexpPd1: Convert the exponent of each packed double-precision (64-bit)
// floating-point element in 'a' to a double-precision (64-bit) floating-point
// number representing the integer exponent, and store the results in 'dst'.
// This intrinsic essentially calculates 'floor(log2(x))' for each element. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ConvertExpFP64(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETEXPPD'. Intrinsic: '_mm256_getexp_pd'.
// Requires AVX512F.
func GetexpPd1(a M256d) M256d {
	return M256d(getexpPd1([4]float64(a)))
}

func getexpPd1(a [4]float64) [4]float64


// MaskGetexpPd1: Convert the exponent of each packed double-precision (64-bit)
// floating-point element in 'a' to a double-precision (64-bit) floating-point
// number representing the integer exponent, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). This intrinsic essentially calculates 'floor(log2(x))'
// for each element. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ConvertExpFP64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETEXPPD'. Intrinsic: '_mm256_mask_getexp_pd'.
// Requires AVX512F.
func MaskGetexpPd1(src M256d, k Mmask8, a M256d) M256d {
	return M256d(maskGetexpPd1([4]float64(src), uint8(k), [4]float64(a)))
}

func maskGetexpPd1(src [4]float64, k uint8, a [4]float64) [4]float64


// MaskzGetexpPd1: Convert the exponent of each packed double-precision
// (64-bit) floating-point element in 'a' to a double-precision (64-bit)
// floating-point number representing the integer exponent, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). This intrinsic essentially calculates
// 'floor(log2(x))' for each element. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ConvertExpFP64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETEXPPD'. Intrinsic: '_mm256_maskz_getexp_pd'.
// Requires AVX512F.
func MaskzGetexpPd1(k Mmask8, a M256d) M256d {
	return M256d(maskzGetexpPd1(uint8(k), [4]float64(a)))
}

func maskzGetexpPd1(k uint8, a [4]float64) [4]float64


// MaskzGetexpPd2: Convert the exponent of each packed double-precision
// (64-bit) floating-point element in 'a' to a double-precision (64-bit)
// floating-point number representing the integer exponent, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). This intrinsic essentially calculates
// 'floor(log2(x))' for each element. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ConvertExpFP64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGETEXPPD'. Intrinsic: '_mm512_maskz_getexp_pd'.
// Requires AVX512F.
func MaskzGetexpPd2(k Mmask8, a M512d) M512d {
	return M512d(maskzGetexpPd2(uint8(k), [8]float64(a)))
}

func maskzGetexpPd2(k uint8, a [8]float64) [8]float64


// GetexpPs: Convert the exponent of each packed single-precision (32-bit)
// floating-point element in 'a' to a single-precision (32-bit) floating-point
// number representing the integer exponent, and store the results in 'dst'.
// This intrinsic essentially calculates 'floor(log2(x))' for each element. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := ConvertExpFP32(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPPS'. Intrinsic: '_mm_getexp_ps'.
// Requires AVX512F.
func GetexpPs(a M128) M128 {
	return M128(getexpPs([4]float32(a)))
}

func getexpPs(a [4]float32) [4]float32


// MaskGetexpPs: Convert the exponent of each packed single-precision (32-bit)
// floating-point element in 'a' to a single-precision (32-bit) floating-point
// number representing the integer exponent, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). This intrinsic essentially calculates 'floor(log2(x))'
// for each element. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ConvertExpFP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPPS'. Intrinsic: '_mm_mask_getexp_ps'.
// Requires AVX512F.
func MaskGetexpPs(src M128, k Mmask8, a M128) M128 {
	return M128(maskGetexpPs([4]float32(src), uint8(k), [4]float32(a)))
}

func maskGetexpPs(src [4]float32, k uint8, a [4]float32) [4]float32


// MaskzGetexpPs: Convert the exponent of each packed single-precision (32-bit)
// floating-point element in 'a' to a single-precision (32-bit) floating-point
// number representing the integer exponent, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). This intrinsic essentially calculates 'floor(log2(x))' for each
// element. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ConvertExpFP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPPS'. Intrinsic: '_mm_maskz_getexp_ps'.
// Requires AVX512F.
func MaskzGetexpPs(k Mmask8, a M128) M128 {
	return M128(maskzGetexpPs(uint8(k), [4]float32(a)))
}

func maskzGetexpPs(k uint8, a [4]float32) [4]float32


// GetexpPs1: Convert the exponent of each packed single-precision (32-bit)
// floating-point element in 'a' to a single-precision (32-bit) floating-point
// number representing the integer exponent, and store the results in 'dst'.
// This intrinsic essentially calculates 'floor(log2(x))' for each element. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ConvertExpFP32(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETEXPPS'. Intrinsic: '_mm256_getexp_ps'.
// Requires AVX512F.
func GetexpPs1(a M256) M256 {
	return M256(getexpPs1([8]float32(a)))
}

func getexpPs1(a [8]float32) [8]float32


// MaskGetexpPs1: Convert the exponent of each packed single-precision (32-bit)
// floating-point element in 'a' to a single-precision (32-bit) floating-point
// number representing the integer exponent, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). This intrinsic essentially calculates 'floor(log2(x))'
// for each element. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ConvertExpFP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETEXPPS'. Intrinsic: '_mm256_mask_getexp_ps'.
// Requires AVX512F.
func MaskGetexpPs1(src M256, k Mmask8, a M256) M256 {
	return M256(maskGetexpPs1([8]float32(src), uint8(k), [8]float32(a)))
}

func maskGetexpPs1(src [8]float32, k uint8, a [8]float32) [8]float32


// MaskzGetexpPs1: Convert the exponent of each packed single-precision
// (32-bit) floating-point element in 'a' to a single-precision (32-bit)
// floating-point number representing the integer exponent, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). This intrinsic essentially calculates
// 'floor(log2(x))' for each element. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ConvertExpFP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETEXPPS'. Intrinsic: '_mm256_maskz_getexp_ps'.
// Requires AVX512F.
func MaskzGetexpPs1(k Mmask8, a M256) M256 {
	return M256(maskzGetexpPs1(uint8(k), [8]float32(a)))
}

func maskzGetexpPs1(k uint8, a [8]float32) [8]float32


// MaskzGetexpPs2: Convert the exponent of each packed single-precision
// (32-bit) floating-point element in 'a' to a single-precision (32-bit)
// floating-point number representing the integer exponent, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). This intrinsic essentially calculates
// 'floor(log2(x))' for each element. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ConvertExpFP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGETEXPPS'. Intrinsic: '_mm512_maskz_getexp_ps'.
// Requires AVX512F.
func MaskzGetexpPs2(k Mmask16, a M512) M512 {
	return M512(maskzGetexpPs2(uint16(k), [16]float32(a)))
}

func maskzGetexpPs2(k uint16, a [16]float32) [16]float32


// MaskzGetexpRoundPd: Convert the exponent of each packed double-precision
// (64-bit) floating-point element in 'a' to a double-precision (64-bit)
// floating-point number representing the integer exponent, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). This intrinsic essentially calculates
// 'floor(log2(x))' for each element.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ConvertExpFP64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGETEXPPD'. Intrinsic: '_mm512_maskz_getexp_round_pd'.
// Requires AVX512F.
func MaskzGetexpRoundPd(k Mmask8, a M512d, rounding int) M512d {
	return M512d(maskzGetexpRoundPd(uint8(k), [8]float64(a), rounding))
}

func maskzGetexpRoundPd(k uint8, a [8]float64, rounding int) [8]float64


// MaskzGetexpRoundPs: Convert the exponent of each packed single-precision
// (32-bit) floating-point element in 'a' to a single-precision (32-bit)
// floating-point number representing the integer exponent, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). This intrinsic essentially calculates
// 'floor(log2(x))' for each element.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ConvertExpFP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGETEXPPS'. Intrinsic: '_mm512_maskz_getexp_round_ps'.
// Requires AVX512F.
func MaskzGetexpRoundPs(k Mmask16, a M512, rounding int) M512 {
	return M512(maskzGetexpRoundPs(uint16(k), [16]float32(a), rounding))
}

func maskzGetexpRoundPs(k uint16, a [16]float32, rounding int) [16]float32


// GetexpRoundSd: Convert the exponent of the lower double-precision (64-bit)
// floating-point element in 'b' to a double-precision (64-bit) floating-point
// number representing the integer exponent, store the result in the lower
// element of 'dst', and copy the upper element from 'a' to the upper element
// of 'dst'. This intrinsic essentially calculates 'floor(log2(x))' for the
// lower element.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := ConvertExpFP64(b[63:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPSD'. Intrinsic: '_mm_getexp_round_sd'.
// Requires AVX512F.
func GetexpRoundSd(a M128d, b M128d, rounding int) M128d {
	return M128d(getexpRoundSd([2]float64(a), [2]float64(b), rounding))
}

func getexpRoundSd(a [2]float64, b [2]float64, rounding int) [2]float64


// MaskGetexpRoundSd: Convert the exponent of the lower double-precision
// (64-bit) floating-point element in 'b' to a double-precision (64-bit)
// floating-point number representing the integer exponent, store the result in
// the lower element of 'dst' using writemask 'k' (the element is copied from
// 'src' when mask bit 0 is not set), and copy the upper element from 'a' to
// the upper element of 'dst'. This intrinsic essentially calculates
// 'floor(log2(x))' for the lower element.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := ConvertExpFP64(b[63:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPSD'. Intrinsic: '_mm_mask_getexp_round_sd'.
// Requires AVX512F.
func MaskGetexpRoundSd(src M128d, k Mmask8, a M128d, b M128d, rounding int) M128d {
	return M128d(maskGetexpRoundSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), rounding))
}

func maskGetexpRoundSd(src [2]float64, k uint8, a [2]float64, b [2]float64, rounding int) [2]float64


// MaskzGetexpRoundSd: Convert the exponent of the lower double-precision
// (64-bit) floating-point element in 'b' to a double-precision (64-bit)
// floating-point number representing the integer exponent, store the result in
// the lower element of 'dst' using zeromask 'k' (the element is zeroed out
// when mask bit 0 is not set), and copy the upper element from 'a' to the
// upper element of 'dst'. This intrinsic essentially calculates
// 'floor(log2(x))' for the lower element.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := ConvertExpFP64(b[63:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPSD'. Intrinsic: '_mm_maskz_getexp_round_sd'.
// Requires AVX512F.
func MaskzGetexpRoundSd(k Mmask8, a M128d, b M128d, rounding int) M128d {
	return M128d(maskzGetexpRoundSd(uint8(k), [2]float64(a), [2]float64(b), rounding))
}

func maskzGetexpRoundSd(k uint8, a [2]float64, b [2]float64, rounding int) [2]float64


// GetexpRoundSs: Convert the exponent of the lower single-precision (32-bit)
// floating-point element in 'b' to a single-precision (32-bit) floating-point
// number representing the integer exponent, store the result in the lower
// element of 'dst', and copy the upper 3 packed elements from 'a' to the upper
// elements of 'dst'. This intrinsic essentially calculates 'floor(log2(x))'
// for the lower element.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := ConvertExpFP32(b[31:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPSS'. Intrinsic: '_mm_getexp_round_ss'.
// Requires AVX512F.
func GetexpRoundSs(a M128, b M128, rounding int) M128 {
	return M128(getexpRoundSs([4]float32(a), [4]float32(b), rounding))
}

func getexpRoundSs(a [4]float32, b [4]float32, rounding int) [4]float32


// MaskGetexpRoundSs: Convert the exponent of the lower single-precision
// (32-bit) floating-point element in 'b' to a single-precision (32-bit)
// floating-point number representing the integer exponent, store the result in
// the lower element of 'dst' using writemask 'k' (the element is copied from
// 'src' when mask bit 0 is not set), and copy the upper 3 packed elements from
// 'a' to the upper elements of 'dst'. This intrinsic essentially calculates
// 'floor(log2(x))' for the lower element.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := ConvertExpFP32(b[31:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPSS'. Intrinsic: '_mm_mask_getexp_round_ss'.
// Requires AVX512F.
func MaskGetexpRoundSs(src M128, k Mmask8, a M128, b M128, rounding int) M128 {
	return M128(maskGetexpRoundSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), rounding))
}

func maskGetexpRoundSs(src [4]float32, k uint8, a [4]float32, b [4]float32, rounding int) [4]float32


// MaskzGetexpRoundSs: Convert the exponent of the lower single-precision
// (32-bit) floating-point element in 'b' to a single-precision (32-bit)
// floating-point number representing the integer exponent, store the result in
// the lower element of 'dst' using zeromask 'k' (the element is zeroed out
// when mask bit 0 is not set), and copy the upper 3 packed elements from 'a'
// to the upper elements of 'dst'. This intrinsic essentially calculates
// 'floor(log2(x))' for the lower element.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := ConvertExpFP32(b[31:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPSS'. Intrinsic: '_mm_maskz_getexp_round_ss'.
// Requires AVX512F.
func MaskzGetexpRoundSs(k Mmask8, a M128, b M128, rounding int) M128 {
	return M128(maskzGetexpRoundSs(uint8(k), [4]float32(a), [4]float32(b), rounding))
}

func maskzGetexpRoundSs(k uint8, a [4]float32, b [4]float32, rounding int) [4]float32


// GetexpSd: Convert the exponent of the lower double-precision (64-bit)
// floating-point element in 'b' to a double-precision (64-bit) floating-point
// number representing the integer exponent, store the result in the lower
// element of 'dst', and copy the upper element from 'a' to the upper element
// of 'dst'. This intrinsic essentially calculates 'floor(log2(x))' for the
// lower element. 
//
//		dst[63:0] := ConvertExpFP64(b[63:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPSD'. Intrinsic: '_mm_getexp_sd'.
// Requires AVX512F.
func GetexpSd(a M128d, b M128d) M128d {
	return M128d(getexpSd([2]float64(a), [2]float64(b)))
}

func getexpSd(a [2]float64, b [2]float64) [2]float64


// MaskGetexpSd: Convert the exponent of the lower double-precision (64-bit)
// floating-point element in 'b' to a double-precision (64-bit) floating-point
// number representing the integer exponent, store the result in the lower
// element of 'dst' using writemask 'k' (the element is copied from 'src' when
// mask bit 0 is not set), and copy the upper element from 'a' to the upper
// element of 'dst'. This intrinsic essentially calculates 'floor(log2(x))' for
// the lower element. 
//
//		IF k[0]
//			dst[63:0] := ConvertExpFP64(b[63:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPSD'. Intrinsic: '_mm_mask_getexp_sd'.
// Requires AVX512F.
func MaskGetexpSd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskGetexpSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskGetexpSd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzGetexpSd: Convert the exponent of the lower double-precision (64-bit)
// floating-point element in 'b' to a double-precision (64-bit) floating-point
// number representing the integer exponent, store the result in the lower
// element of 'dst' using zeromask 'k' (the element is zeroed out when mask bit
// 0 is not set), and copy the upper element from 'a' to the upper element of
// 'dst'. This intrinsic essentially calculates 'floor(log2(x))' for the lower
// element. 
//
//		IF k[0]
//			dst[63:0] := ConvertExpFP64(b[63:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPSD'. Intrinsic: '_mm_maskz_getexp_sd'.
// Requires AVX512F.
func MaskzGetexpSd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzGetexpSd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzGetexpSd(k uint8, a [2]float64, b [2]float64) [2]float64


// GetexpSs: Convert the exponent of the lower single-precision (32-bit)
// floating-point element in 'b' to a single-precision (32-bit) floating-point
// number representing the integer exponent, store the result in the lower
// element of 'dst', and copy the upper 3 packed elements from 'a' to the upper
// elements of 'dst'. This intrinsic essentially calculates 'floor(log2(x))'
// for the lower element. 
//
//		dst[31:0] := ConvertExpFP32(b[31:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPSS'. Intrinsic: '_mm_getexp_ss'.
// Requires AVX512F.
func GetexpSs(a M128, b M128) M128 {
	return M128(getexpSs([4]float32(a), [4]float32(b)))
}

func getexpSs(a [4]float32, b [4]float32) [4]float32


// MaskGetexpSs: Convert the exponent of the lower single-precision (32-bit)
// floating-point element in 'b' to a single-precision (32-bit) floating-point
// number representing the integer exponent, store the result in the lower
// element of 'dst' using writemask 'k' (the element is copied from 'src' when
// mask bit 0 is not set), and copy the upper 3 packed elements from 'a' to the
// upper elements of 'dst'. This intrinsic essentially calculates
// 'floor(log2(x))' for the lower element. 
//
//		IF k[0]
//			dst[31:0] := ConvertExpFP32(b[31:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPSS'. Intrinsic: '_mm_mask_getexp_ss'.
// Requires AVX512F.
func MaskGetexpSs(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskGetexpSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskGetexpSs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzGetexpSs: Convert the exponent of the lower single-precision (32-bit)
// floating-point element in 'b' to a single-precision (32-bit) floating-point
// number representing the integer exponent, store the result in the lower
// element of 'dst' using zeromask 'k' (the element is zeroed out when mask bit
// 0 is not set), and copy the upper 3 packed elements from 'a' to the upper
// elements of 'dst'. This intrinsic essentially calculates 'floor(log2(x))'
// for the lower element. 
//
//		IF k[0]
//			dst[31:0] := ConvertExpFP32(b[31:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPSS'. Intrinsic: '_mm_maskz_getexp_ss'.
// Requires AVX512F.
func MaskzGetexpSs(k Mmask8, a M128, b M128) M128 {
	return M128(maskzGetexpSs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzGetexpSs(k uint8, a [4]float32, b [4]float32) [4]float32


// GetmantPd: Normalize the mantissas of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. This
// intrinsic essentially calculates '(2^k)*|x.significand|', where 'k'
// depends on the interval range defined by 'interv' and the sign depends on
// 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTPD'. Intrinsic: '_mm_getmant_pd'.
// Requires AVX512F.
func GetmantPd(a M128d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M128d {
	return M128d(getmantPd([2]float64(a), interv, sc))
}

func getmantPd(a [2]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [2]float64


// MaskGetmantPd: Normalize the mantissas of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). This intrinsic essentially calculates
// '(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTPD'. Intrinsic: '_mm_mask_getmant_pd'.
// Requires AVX512F.
func MaskGetmantPd(src M128d, k Mmask8, a M128d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M128d {
	return M128d(maskGetmantPd([2]float64(src), uint8(k), [2]float64(a), interv, sc))
}

func maskGetmantPd(src [2]float64, k uint8, a [2]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [2]float64


// MaskzGetmantPd: Normalize the mantissas of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). This intrinsic essentially calculates '(2^k)*|x.significand|', where
// 'k' depends on the interval range defined by 'interv' and the sign depends
// on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTPD'. Intrinsic: '_mm_maskz_getmant_pd'.
// Requires AVX512F.
func MaskzGetmantPd(k Mmask8, a M128d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M128d {
	return M128d(maskzGetmantPd(uint8(k), [2]float64(a), interv, sc))
}

func maskzGetmantPd(k uint8, a [2]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [2]float64


// GetmantPd1: Normalize the mantissas of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. This
// intrinsic essentially calculates '(2^k)*|x.significand|', where 'k'
// depends on the interval range defined by 'interv' and the sign depends on
// 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETMANTPD'. Intrinsic: '_mm256_getmant_pd'.
// Requires AVX512F.
func GetmantPd1(a M256d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M256d {
	return M256d(getmantPd1([4]float64(a), interv, sc))
}

func getmantPd1(a [4]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [4]float64


// MaskGetmantPd1: Normalize the mantissas of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). This intrinsic essentially calculates
// '(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETMANTPD'. Intrinsic: '_mm256_mask_getmant_pd'.
// Requires AVX512F.
func MaskGetmantPd1(src M256d, k Mmask8, a M256d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M256d {
	return M256d(maskGetmantPd1([4]float64(src), uint8(k), [4]float64(a), interv, sc))
}

func maskGetmantPd1(src [4]float64, k uint8, a [4]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [4]float64


// MaskzGetmantPd1: Normalize the mantissas of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). This intrinsic essentially calculates '(2^k)*|x.significand|', where
// 'k' depends on the interval range defined by 'interv' and the sign depends
// on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETMANTPD'. Intrinsic: '_mm256_maskz_getmant_pd'.
// Requires AVX512F.
func MaskzGetmantPd1(k Mmask8, a M256d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M256d {
	return M256d(maskzGetmantPd1(uint8(k), [4]float64(a), interv, sc))
}

func maskzGetmantPd1(k uint8, a [4]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [4]float64


// MaskzGetmantPd2: Normalize the mantissas of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). This intrinsic essentially calculates '(2^k)*|x.significand|', where
// 'k' depends on the interval range defined by 'interv' and the sign depends
// on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGETMANTPD'. Intrinsic: '_mm512_maskz_getmant_pd'.
// Requires AVX512F.
func MaskzGetmantPd2(k Mmask8, a M512d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M512d {
	return M512d(maskzGetmantPd2(uint8(k), [8]float64(a), interv, sc))
}

func maskzGetmantPd2(k uint8, a [8]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [8]float64


// GetmantPs: Normalize the mantissas of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. This
// intrinsic essentially calculates '(2^k)*|x.significand|', where 'k'
// depends on the interval range defined by 'interv' and the sign depends on
// 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTPS'. Intrinsic: '_mm_getmant_ps'.
// Requires AVX512F.
func GetmantPs(a M128, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M128 {
	return M128(getmantPs([4]float32(a), interv, sc))
}

func getmantPs(a [4]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [4]float32


// MaskGetmantPs: Normalize the mantissas of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). This intrinsic essentially calculates
// '(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTPS'. Intrinsic: '_mm_mask_getmant_ps'.
// Requires AVX512F.
func MaskGetmantPs(src M128, k Mmask8, a M128, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M128 {
	return M128(maskGetmantPs([4]float32(src), uint8(k), [4]float32(a), interv, sc))
}

func maskGetmantPs(src [4]float32, k uint8, a [4]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [4]float32


// MaskzGetmantPs: Normalize the mantissas of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). This intrinsic essentially calculates '(2^k)*|x.significand|', where
// 'k' depends on the interval range defined by 'interv' and the sign depends
// on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTPS'. Intrinsic: '_mm_maskz_getmant_ps'.
// Requires AVX512F.
func MaskzGetmantPs(k Mmask8, a M128, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M128 {
	return M128(maskzGetmantPs(uint8(k), [4]float32(a), interv, sc))
}

func maskzGetmantPs(k uint8, a [4]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [4]float32


// GetmantPs1: Normalize the mantissas of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. This
// intrinsic essentially calculates '(2^k)*|x.significand|', where 'k'
// depends on the interval range defined by 'interv' and the sign depends on
// 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETMANTPS'. Intrinsic: '_mm256_getmant_ps'.
// Requires AVX512F.
func GetmantPs1(a M256, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M256 {
	return M256(getmantPs1([8]float32(a), interv, sc))
}

func getmantPs1(a [8]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [8]float32


// MaskGetmantPs1: Normalize the mantissas of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). This intrinsic essentially calculates
// '(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETMANTPS'. Intrinsic: '_mm256_mask_getmant_ps'.
// Requires AVX512F.
func MaskGetmantPs1(src M256, k Mmask8, a M256, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M256 {
	return M256(maskGetmantPs1([8]float32(src), uint8(k), [8]float32(a), interv, sc))
}

func maskGetmantPs1(src [8]float32, k uint8, a [8]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [8]float32


// MaskzGetmantPs1: Normalize the mantissas of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). This intrinsic essentially calculates '(2^k)*|x.significand|', where
// 'k' depends on the interval range defined by 'interv' and the sign depends
// on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETMANTPS'. Intrinsic: '_mm256_maskz_getmant_ps'.
// Requires AVX512F.
func MaskzGetmantPs1(k Mmask8, a M256, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M256 {
	return M256(maskzGetmantPs1(uint8(k), [8]float32(a), interv, sc))
}

func maskzGetmantPs1(k uint8, a [8]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [8]float32


// MaskzGetmantPs2: Normalize the mantissas of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). This intrinsic essentially calculates '(2^k)*|x.significand|', where
// 'k' depends on the interval range defined by 'interv' and the sign depends
// on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGETMANTPS'. Intrinsic: '_mm512_maskz_getmant_ps'.
// Requires AVX512F.
func MaskzGetmantPs2(k Mmask16, a M512, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M512 {
	return M512(maskzGetmantPs2(uint16(k), [16]float32(a), interv, sc))
}

func maskzGetmantPs2(k uint16, a [16]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [16]float32


// MaskzGetmantRoundPd: Normalize the mantissas of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). This intrinsic essentially calculates
// '(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1Rounding is done according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGETMANTPD'. Intrinsic: '_mm512_maskz_getmant_round_pd'.
// Requires AVX512F.
func MaskzGetmantRoundPd(k Mmask8, a M512d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) M512d {
	return M512d(maskzGetmantRoundPd(uint8(k), [8]float64(a), interv, sc, rounding))
}

func maskzGetmantRoundPd(k uint8, a [8]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) [8]float64


// MaskzGetmantRoundPs: Normalize the mantissas of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). This intrinsic essentially calculates
// '(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1Rounding is done according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGETMANTPS'. Intrinsic: '_mm512_maskz_getmant_round_ps'.
// Requires AVX512F.
func MaskzGetmantRoundPs(k Mmask16, a M512, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) M512 {
	return M512(maskzGetmantRoundPs(uint16(k), [16]float32(a), interv, sc, rounding))
}

func maskzGetmantRoundPs(k uint16, a [16]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) [16]float32


// GetmantRoundSd: Normalize the mantissas of the lower double-precision
// (64-bit) floating-point element in 'a', store the result in the lower
// element of 'dst', and copy the upper element from 'b' to the upper element
// of 'dst'. This intrinsic essentially calculates '(2^k)*|x.significand|',
// where 'k' depends on the interval range defined by 'interv' and the sign
// depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1Rounding is done according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := GetNormalizedMantissa(a[63:0], sc, interv)
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTSD'. Intrinsic: '_mm_getmant_round_sd'.
// Requires AVX512F.
func GetmantRoundSd(a M128d, b M128d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) M128d {
	return M128d(getmantRoundSd([2]float64(a), [2]float64(b), interv, sc, rounding))
}

func getmantRoundSd(a [2]float64, b [2]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) [2]float64


// MaskGetmantRoundSd: Normalize the mantissas of the lower double-precision
// (64-bit) floating-point element in 'a', store the result in the lower
// element of 'dst' using writemask 'k' (the element is copied from 'src' when
// mask bit 0 is not set), and copy the upper element from 'b' to the upper
// element of 'dst'. This intrinsic essentially calculates
// '(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1Rounding is done according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := GetNormalizedMantissa(a[63:0], sc, interv)
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTSD'. Intrinsic: '_mm_mask_getmant_round_sd'.
// Requires AVX512F.
func MaskGetmantRoundSd(src M128d, k Mmask8, a M128d, b M128d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) M128d {
	return M128d(maskGetmantRoundSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), interv, sc, rounding))
}

func maskGetmantRoundSd(src [2]float64, k uint8, a [2]float64, b [2]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) [2]float64


// MaskzGetmantRoundSd: Normalize the mantissas of the lower double-precision
// (64-bit) floating-point element in 'a', store the result in the lower
// element of 'dst' using zeromask 'k' (the element is zeroed out when mask bit
// 0 is not set), and copy the upper element from 'b' to the upper element of
// 'dst'. This intrinsic essentially calculates '(2^k)*|x.significand|',
// where 'k' depends on the interval range defined by 'interv' and the sign
// depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1Rounding is done according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := GetNormalizedMantissa(a[63:0], sc, interv)
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTSD'. Intrinsic: '_mm_maskz_getmant_round_sd'.
// Requires AVX512F.
func MaskzGetmantRoundSd(k Mmask8, a M128d, b M128d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) M128d {
	return M128d(maskzGetmantRoundSd(uint8(k), [2]float64(a), [2]float64(b), interv, sc, rounding))
}

func maskzGetmantRoundSd(k uint8, a [2]float64, b [2]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) [2]float64


// GetmantRoundSs: Normalize the mantissas of the lower single-precision
// (32-bit) floating-point element in 'a', store the result in the lower
// element of 'dst', and copy the upper 3 packed elements from 'b' to the upper
// elements of 'dst'. This intrinsic essentially calculates
// '(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1Rounding is done according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := GetNormalizedMantissa(a[31:0], sc, interv)
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTSS'. Intrinsic: '_mm_getmant_round_ss'.
// Requires AVX512F.
func GetmantRoundSs(a M128, b M128, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) M128 {
	return M128(getmantRoundSs([4]float32(a), [4]float32(b), interv, sc, rounding))
}

func getmantRoundSs(a [4]float32, b [4]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) [4]float32


// MaskGetmantRoundSs: Normalize the mantissas of the lower single-precision
// (32-bit) floating-point element in 'a', store the result in the lower
// element of 'dst' using writemask 'k' (the element is copied from 'src' when
// mask bit 0 is not set), and copy the upper 3 packed elements from 'b' to the
// upper elements of 'dst'. This intrinsic essentially calculates
// '(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1Rounding is done according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := GetNormalizedMantissa(a[31:0], sc, interv)
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTSS'. Intrinsic: '_mm_mask_getmant_round_ss'.
// Requires AVX512F.
func MaskGetmantRoundSs(src M128, k Mmask8, a M128, b M128, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) M128 {
	return M128(maskGetmantRoundSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), interv, sc, rounding))
}

func maskGetmantRoundSs(src [4]float32, k uint8, a [4]float32, b [4]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) [4]float32


// MaskzGetmantRoundSs: Normalize the mantissas of the lower single-precision
// (32-bit) floating-point element in 'a', store the result in the lower
// element of 'dst' using zeromask 'k' (the element is zeroed out when mask bit
// 0 is not set), and copy the upper 3 packed elements from 'b' to the upper
// elements of 'dst'. This intrinsic essentially calculates
// '(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1Rounding is done according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := GetNormalizedMantissa(a[31:0], sc, interv)
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTSS'. Intrinsic: '_mm_maskz_getmant_round_ss'.
// Requires AVX512F.
func MaskzGetmantRoundSs(k Mmask8, a M128, b M128, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) M128 {
	return M128(maskzGetmantRoundSs(uint8(k), [4]float32(a), [4]float32(b), interv, sc, rounding))
}

func maskzGetmantRoundSs(k uint8, a [4]float32, b [4]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) [4]float32


// GetmantSd: Normalize the mantissas of the lower double-precision (64-bit)
// floating-point element in 'a', store the result in the lower element of
// 'dst', and copy the upper element from 'b' to the upper element of 'dst'.
// This intrinsic essentially calculates '(2^k)*|x.significand|', where 'k'
// depends on the interval range defined by 'interv' and the sign depends on
// 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		dst[63:0] := GetNormalizedMantissa(a[63:0], sc, interv)
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTSD'. Intrinsic: '_mm_getmant_sd'.
// Requires AVX512F.
func GetmantSd(a M128d, b M128d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M128d {
	return M128d(getmantSd([2]float64(a), [2]float64(b), interv, sc))
}

func getmantSd(a [2]float64, b [2]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [2]float64


// MaskGetmantSd: Normalize the mantissas of the lower double-precision
// (64-bit) floating-point element in 'a', store the result in the lower
// element of 'dst' using writemask 'k' (the element is copied from 'src' when
// mask bit 0 is not set), and copy the upper element from 'b' to the upper
// element of 'dst'. This intrinsic essentially calculates
// '(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		IF k[0]
//			dst[63:0] := GetNormalizedMantissa(a[63:0], sc, interv)
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTSD'. Intrinsic: '_mm_mask_getmant_sd'.
// Requires AVX512F.
func MaskGetmantSd(src M128d, k Mmask8, a M128d, b M128d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M128d {
	return M128d(maskGetmantSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), interv, sc))
}

func maskGetmantSd(src [2]float64, k uint8, a [2]float64, b [2]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [2]float64


// MaskzGetmantSd: Normalize the mantissas of the lower double-precision
// (64-bit) floating-point element in 'a', store the result in the lower
// element of 'dst' using zeromask 'k' (the element is zeroed out when mask bit
// 0 is not set), and copy the upper element from 'b' to the upper element of
// 'dst'. This intrinsic essentially calculates '(2^k)*|x.significand|',
// where 'k' depends on the interval range defined by 'interv' and the sign
// depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		IF k[0]
//			dst[63:0] := GetNormalizedMantissa(a[63:0], sc, interv)
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTSD'. Intrinsic: '_mm_maskz_getmant_sd'.
// Requires AVX512F.
func MaskzGetmantSd(k Mmask8, a M128d, b M128d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M128d {
	return M128d(maskzGetmantSd(uint8(k), [2]float64(a), [2]float64(b), interv, sc))
}

func maskzGetmantSd(k uint8, a [2]float64, b [2]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [2]float64


// GetmantSs: Normalize the mantissas of the lower single-precision (32-bit)
// floating-point element in 'a', store the result in the lower element of
// 'dst', and copy the upper 3 packed elements from 'b' to the upper elements
// of 'dst'. This intrinsic essentially calculates '(2^k)*|x.significand|',
// where 'k' depends on the interval range defined by 'interv' and the sign
// depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		dst[31:0] := GetNormalizedMantissa(a[31:0], sc, interv)
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTSS'. Intrinsic: '_mm_getmant_ss'.
// Requires AVX512F.
func GetmantSs(a M128, b M128, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M128 {
	return M128(getmantSs([4]float32(a), [4]float32(b), interv, sc))
}

func getmantSs(a [4]float32, b [4]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [4]float32


// MaskGetmantSs: Normalize the mantissas of the lower single-precision
// (32-bit) floating-point element in 'a', store the result in the lower
// element of 'dst' using writemask 'k' (the element is copied from 'src' when
// mask bit 0 is not set), and copy the upper 3 packed elements from 'b' to the
// upper elements of 'dst'. This intrinsic essentially calculates
// '(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		IF k[0]
//			dst[31:0] := GetNormalizedMantissa(a[31:0], sc, interv)
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTSS'. Intrinsic: '_mm_mask_getmant_ss'.
// Requires AVX512F.
func MaskGetmantSs(src M128, k Mmask8, a M128, b M128, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M128 {
	return M128(maskGetmantSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), interv, sc))
}

func maskGetmantSs(src [4]float32, k uint8, a [4]float32, b [4]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [4]float32


// MaskzGetmantSs: Normalize the mantissas of the lower single-precision
// (32-bit) floating-point element in 'a', store the result in the lower
// element of 'dst' using zeromask 'k' (the element is zeroed out when mask bit
// 0 is not set), and copy the upper 3 packed elements from 'b' to the upper
// elements of 'dst'. This intrinsic essentially calculates
// '(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		IF k[0]
//			dst[31:0] := GetNormalizedMantissa(a[31:0], sc, interv)
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTSS'. Intrinsic: '_mm_maskz_getmant_ss'.
// Requires AVX512F.
func MaskzGetmantSs(k Mmask8, a M128, b M128, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M128 {
	return M128(maskzGetmantSs(uint8(k), [4]float32(a), [4]float32(b), interv, sc))
}

func maskzGetmantSs(k uint8, a [4]float32, b [4]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [4]float32


// HypotPd: Compute the length of the hypotenous of a right triangle, with the
// lengths of the other two sides of the triangle stored as packed
// double-precision (64-bit) floating-point elements in 'a' and 'b', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := SQRT(a[i+63:i]^2 + b[i+63:i]^2)
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_hypot_pd'.
// Requires AVX512F.
func HypotPd(a M512d, b M512d) M512d {
	return M512d(hypotPd([8]float64(a), [8]float64(b)))
}

func hypotPd(a [8]float64, b [8]float64) [8]float64


// MaskHypotPd: Compute the length of the hypotenous of a right triangle, with
// the lengths of the other two sides of the triangle stored as packed
// double-precision (64-bit) floating-point elements in 'a' and 'b', and store
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SQRT(a[i+63:i]^2 + b[i+63:i]^2)
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_hypot_pd'.
// Requires AVX512F.
func MaskHypotPd(src M512d, k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskHypotPd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b)))
}

func maskHypotPd(src [8]float64, k uint8, a [8]float64, b [8]float64) [8]float64


// HypotPs: Compute the length of the hypotenous of a right triangle, with the
// lengths of the other two sides of the triangle stored as packed
// single-precision (32-bit) floating-point elements in 'a' and 'b', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := SQRT(a[i+31:i]^2 + b[i+31:i]^2)
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_hypot_ps'.
// Requires AVX512F.
func HypotPs(a M512, b M512) M512 {
	return M512(hypotPs([16]float32(a), [16]float32(b)))
}

func hypotPs(a [16]float32, b [16]float32) [16]float32


// MaskHypotPs: Compute the length of the hypotenous of a right triangle, with
// the lengths of the other two sides of the triangle stored as packed
// single-precision (32-bit) floating-point elements in 'a' and 'b', and store
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SQRT(a[i+31:i]^2 + b[i+31:i]^2)
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_hypot_ps'.
// Requires AVX512F.
func MaskHypotPs(src M512, k Mmask16, a M512, b M512) M512 {
	return M512(maskHypotPs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b)))
}

func maskHypotPs(src [16]float32, k uint16, a [16]float32, b [16]float32) [16]float32


// MmaskI32gatherEpi32: Gather 32-bit integers from memory using 32-bit
// indices. 32-bit elements are loaded from addresses starting at 'base_addr'
// and offset by each 32-bit element in 'vindex' (each index is scaled by the
// factor in 'scale'). Gathered elements are merged into 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+31:i])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//		dst[MAX:128] := 0
//
// Instruction: 'VPGATHERDD'. Intrinsic: '_mm_mmask_i32gather_epi32'.
// Requires AVX512F.
func MmaskI32gatherEpi32(src M128i, k Mmask8, vindex M128i, base_addr uintptr, scale int) M128i {
	return M128i(mmaskI32gatherEpi32([16]byte(src), uint8(k), [16]byte(vindex), uintptr(base_addr), scale))
}

func mmaskI32gatherEpi32(src [16]byte, k uint8, vindex [16]byte, base_addr uintptr, scale int) [16]byte


// MmaskI32gatherEpi321: Gather 32-bit integers from memory using 32-bit
// indices. 32-bit elements are loaded from addresses starting at 'base_addr'
// and offset by each 32-bit element in 'vindex' (each index is scaled by the
// factor in 'scale'). Gathered elements are merged into 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+31:i])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VPGATHERDD'. Intrinsic: '_mm256_mmask_i32gather_epi32'.
// Requires AVX512F.
func MmaskI32gatherEpi321(src M256i, k Mmask8, vindex M256i, base_addr uintptr, scale int) M256i {
	return M256i(mmaskI32gatherEpi321([32]byte(src), uint8(k), [32]byte(vindex), uintptr(base_addr), scale))
}

func mmaskI32gatherEpi321(src [32]byte, k uint8, vindex [32]byte, base_addr uintptr, scale int) [32]byte


// MmaskI32gatherEpi64: Gather 64-bit integers from memory using 32-bit
// indices. 64-bit elements are loaded from addresses starting at 'base_addr'
// and offset by each 32-bit element in 'vindex' (each index is scaled by the
// factor in 'scale'). Gathered elements are merged into 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 1
//			i := j*64
//			m := j*32
//			IF k[j]
//				dst[i+63:i] := MEM[base_addr + SignExtend(vindex[m+31:m])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//		dst[MAX:128] := 0
//
// Instruction: 'VPGATHERDQ'. Intrinsic: '_mm_mmask_i32gather_epi64'.
// Requires AVX512F.
func MmaskI32gatherEpi64(src M128i, k Mmask8, vindex M128i, base_addr uintptr, scale int) M128i {
	return M128i(mmaskI32gatherEpi64([16]byte(src), uint8(k), [16]byte(vindex), uintptr(base_addr), scale))
}

func mmaskI32gatherEpi64(src [16]byte, k uint8, vindex [16]byte, base_addr uintptr, scale int) [16]byte


// MmaskI32gatherEpi641: Gather 64-bit integers from memory using 32-bit
// indices. 64-bit elements are loaded from addresses starting at 'base_addr'
// and offset by each 32-bit element in 'vindex' (each index is scaled by the
// factor in 'scale'). Gathered elements are merged into 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			m := j*32
//			IF k[j]
//				dst[i+63:i] := MEM[base_addr + SignExtend(vindex[m+31:m])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VPGATHERDQ'. Intrinsic: '_mm256_mmask_i32gather_epi64'.
// Requires AVX512F.
func MmaskI32gatherEpi641(src M256i, k Mmask8, vindex M128i, base_addr uintptr, scale int) M256i {
	return M256i(mmaskI32gatherEpi641([32]byte(src), uint8(k), [16]byte(vindex), uintptr(base_addr), scale))
}

func mmaskI32gatherEpi641(src [32]byte, k uint8, vindex [16]byte, base_addr uintptr, scale int) [32]byte


// I32gatherEpi64: Gather 64-bit integers from memory using 32-bit indices.
// 64-bit elements are loaded from addresses starting at 'base_addr' and offset
// by each 32-bit element in 'vindex' (each index is scaled by the factor in
// 'scale'). Gathered elements are merged into 'dst'. 'scale' should be 1, 2, 4
// or 8. 
//
//		FOR j := 0 to 7
//			i := j*64
//			m := j*32
//			dst[i+63:i] := MEM[base_addr + SignExtend(vindex[m+31:m])*scale]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPGATHERDQ'. Intrinsic: '_mm512_i32gather_epi64'.
// Requires AVX512F.
func I32gatherEpi64(vindex M256i, base_addr uintptr, scale int) M512i {
	return M512i(i32gatherEpi64([32]byte(vindex), uintptr(base_addr), scale))
}

func i32gatherEpi64(vindex [32]byte, base_addr uintptr, scale int) [64]byte


// MaskI32gatherEpi64: Gather 64-bit integers from memory using 32-bit indices.
// 64-bit elements are loaded from addresses starting at 'base_addr' and offset
// by each 32-bit element in 'vindex' (each index is scaled by the factor in
// 'scale'). Gathered elements are merged into 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*64
//			m := j*32
//			IF k[j]
//				dst[i+63:i] := MEM[base_addr + SignExtend(vindex[m+31:m])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//		dst[MAX:512] := 0
//
// Instruction: 'VPGATHERDQ'. Intrinsic: '_mm512_mask_i32gather_epi64'.
// Requires AVX512F.
func MaskI32gatherEpi64(src M512i, k Mmask8, vindex M256i, base_addr uintptr, scale int) M512i {
	return M512i(maskI32gatherEpi64([64]byte(src), uint8(k), [32]byte(vindex), uintptr(base_addr), scale))
}

func maskI32gatherEpi64(src [64]byte, k uint8, vindex [32]byte, base_addr uintptr, scale int) [64]byte


// MmaskI32gatherPd: Gather double-precision (64-bit) floating-point elements
// from memory using 32-bit indices. 64-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 32-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 1
//			i := j*64
//			m := j*32
//			IF k[j]
//				dst[i+63:i] := MEM[base_addr + SignExtend(vindex[m+31:m])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//		dst[MAX:128] := 0
//
// Instruction: 'VGATHERDPD'. Intrinsic: '_mm_mmask_i32gather_pd'.
// Requires AVX512F.
func MmaskI32gatherPd(src M128d, k Mmask8, vindex M128i, base_addr uintptr, scale int) M128d {
	return M128d(mmaskI32gatherPd([2]float64(src), uint8(k), [16]byte(vindex), uintptr(base_addr), scale))
}

func mmaskI32gatherPd(src [2]float64, k uint8, vindex [16]byte, base_addr uintptr, scale int) [2]float64


// MmaskI32gatherPd1: Gather double-precision (64-bit) floating-point elements
// from memory using 32-bit indices. 64-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 32-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			m := j*32
//			IF k[j]
//				dst[i+63:i] := MEM[base_addr + SignExtend(vindex[m+31:m])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VGATHERDPD'. Intrinsic: '_mm256_mmask_i32gather_pd'.
// Requires AVX512F.
func MmaskI32gatherPd1(src M256d, k Mmask8, vindex M128i, base_addr uintptr, scale int) M256d {
	return M256d(mmaskI32gatherPd1([4]float64(src), uint8(k), [16]byte(vindex), uintptr(base_addr), scale))
}

func mmaskI32gatherPd1(src [4]float64, k uint8, vindex [16]byte, base_addr uintptr, scale int) [4]float64


// I32gatherPd: Gather double-precision (64-bit) floating-point elements from
// memory using 32-bit indices. 64-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 32-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst'. 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*64
//			m := j*32
//			dst[i+63:i] := MEM[base_addr + SignExtend(vindex[m+31:m])*scale]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGATHERDPD'. Intrinsic: '_mm512_i32gather_pd'.
// Requires AVX512F.
func I32gatherPd(vindex M256i, base_addr uintptr, scale int) M512d {
	return M512d(i32gatherPd([32]byte(vindex), uintptr(base_addr), scale))
}

func i32gatherPd(vindex [32]byte, base_addr uintptr, scale int) [8]float64


// MaskI32gatherPd: Gather double-precision (64-bit) floating-point elements
// from memory using 32-bit indices. 64-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 32-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*64
//			m := j*32
//			IF k[j]
//				dst[i+63:i] := MEM[base_addr + SignExtend(vindex[m+31:m])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//		dst[MAX:512] := 0
//
// Instruction: 'VGATHERDPD'. Intrinsic: '_mm512_mask_i32gather_pd'.
// Requires AVX512F.
func MaskI32gatherPd(src M512d, k Mmask8, vindex M256i, base_addr uintptr, scale int) M512d {
	return M512d(maskI32gatherPd([8]float64(src), uint8(k), [32]byte(vindex), uintptr(base_addr), scale))
}

func maskI32gatherPd(src [8]float64, k uint8, vindex [32]byte, base_addr uintptr, scale int) [8]float64


// MmaskI32gatherPs: Gather single-precision (32-bit) floating-point elements
// from memory using 32-bit indices. 32-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 32-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+31:i])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//		dst[MAX:128] := 0
//
// Instruction: 'VGATHERDPS'. Intrinsic: '_mm_mmask_i32gather_ps'.
// Requires AVX512F.
func MmaskI32gatherPs(src M128, k Mmask8, vindex M128i, base_addr uintptr, scale int) M128 {
	return M128(mmaskI32gatherPs([4]float32(src), uint8(k), [16]byte(vindex), uintptr(base_addr), scale))
}

func mmaskI32gatherPs(src [4]float32, k uint8, vindex [16]byte, base_addr uintptr, scale int) [4]float32


// MmaskI32gatherPs1: Gather single-precision (32-bit) floating-point elements
// from memory using 32-bit indices. 32-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 32-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+31:i])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VGATHERDPS'. Intrinsic: '_mm256_mmask_i32gather_ps'.
// Requires AVX512F.
func MmaskI32gatherPs1(src M256, k Mmask8, vindex M256i, base_addr uintptr, scale int) M256 {
	return M256(mmaskI32gatherPs1([8]float32(src), uint8(k), [32]byte(vindex), uintptr(base_addr), scale))
}

func mmaskI32gatherPs1(src [8]float32, k uint8, vindex [32]byte, base_addr uintptr, scale int) [8]float32


// I32scatterEpi32: Scatter 32-bit integers from 'a' into memory using 32-bit
// indices. 32-bit elements are stored at addresses starting at 'base_addr' and
// offset by each 32-bit element in 'vindex' (each index is scaled by the
// factor in 'scale'). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*32
//			MEM[base_addr + SignExtend(vindex[i+31:i])*scale] := a[i+31:i]
//		ENDFOR
//
// Instruction: 'VPSCATTERDD'. Intrinsic: '_mm_i32scatter_epi32'.
// Requires AVX512F.
func I32scatterEpi32(base_addr uintptr, vindex M128i, a M128i, scale int)  {
	i32scatterEpi32(uintptr(base_addr), [16]byte(vindex), [16]byte(a), scale)
}

func i32scatterEpi32(base_addr uintptr, vindex [16]byte, a [16]byte, scale int) 


// MaskI32scatterEpi32: Scatter 32-bit integers from 'a' into memory using
// 32-bit indices. 32-bit elements are stored at addresses starting at
// 'base_addr' and offset by each 32-bit element in 'vindex' (each index is
// scaled by the factor in 'scale') subject to mask 'k' (elements are not
// stored when the corresponding mask bit is not set). 'scale' should be 1, 2,
// 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[i+31:i])*scale] := a[i+31:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPSCATTERDD'. Intrinsic: '_mm_mask_i32scatter_epi32'.
// Requires AVX512F.
func MaskI32scatterEpi32(base_addr uintptr, k Mmask8, vindex M128i, a M128i, scale int)  {
	maskI32scatterEpi32(uintptr(base_addr), uint8(k), [16]byte(vindex), [16]byte(a), scale)
}

func maskI32scatterEpi32(base_addr uintptr, k uint8, vindex [16]byte, a [16]byte, scale int) 


// I32scatterEpi321: Scatter 32-bit integers from 'a' into memory using 32-bit
// indices. 32-bit elements are stored at addresses starting at 'base_addr' and
// offset by each 32-bit element in 'vindex' (each index is scaled by the
// factor in 'scale'). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*32
//			MEM[base_addr + SignExtend(vindex[i+31:i])*scale] := a[i+31:i]
//		ENDFOR
//
// Instruction: 'VPSCATTERDD'. Intrinsic: '_mm256_i32scatter_epi32'.
// Requires AVX512F.
func I32scatterEpi321(base_addr uintptr, vindex M256i, a M256i, scale int)  {
	i32scatterEpi321(uintptr(base_addr), [32]byte(vindex), [32]byte(a), scale)
}

func i32scatterEpi321(base_addr uintptr, vindex [32]byte, a [32]byte, scale int) 


// MaskI32scatterEpi321: Scatter 32-bit integers from 'a' into memory using
// 32-bit indices. 32-bit elements are stored at addresses starting at
// 'base_addr' and offset by each 32-bit element in 'vindex' (each index is
// scaled by the factor in 'scale') subject to mask 'k' (elements are not
// stored when the corresponding mask bit is not set). 'scale' should be 1, 2,
// 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[i+31:i])*scale] := a[i+31:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPSCATTERDD'. Intrinsic: '_mm256_mask_i32scatter_epi32'.
// Requires AVX512F.
func MaskI32scatterEpi321(base_addr uintptr, k Mmask8, vindex M256i, a M256i, scale int)  {
	maskI32scatterEpi321(uintptr(base_addr), uint8(k), [32]byte(vindex), [32]byte(a), scale)
}

func maskI32scatterEpi321(base_addr uintptr, k uint8, vindex [32]byte, a [32]byte, scale int) 


// I32scatterEpi64: Scatter 64-bit integers from 'a' into memory using 32-bit
// indices. 64-bit elements are stored at addresses starting at 'base_addr' and
// offset by each 32-bit element in 'vindex' (each index is scaled by the
// factor in 'scale'). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 1
//			i := j*64
//			l := j*32
//			MEM[base_addr + SignExtend(vindex[l+31:l])*scale] := a[i+63:i]
//		ENDFOR
//
// Instruction: 'VPSCATTERDQ'. Intrinsic: '_mm_i32scatter_epi64'.
// Requires AVX512F.
func I32scatterEpi64(base_addr uintptr, vindex M128i, a M128i, scale int)  {
	i32scatterEpi64(uintptr(base_addr), [16]byte(vindex), [16]byte(a), scale)
}

func i32scatterEpi64(base_addr uintptr, vindex [16]byte, a [16]byte, scale int) 


// MaskI32scatterEpi64: Scatter 64-bit integers from 'a' into memory using
// 32-bit indices. 64-bit elements are stored at addresses starting at
// 'base_addr' and offset by each 32-bit element in 'vindex' (each index is
// scaled by the factor in 'scale') subject to mask 'k' (elements are not
// stored when the corresponding mask bit is not set). 'scale' should be 1, 2,
// 4 or 8. 
//
//		FOR j := 0 to 1
//			i := j*64
//			l := j*32
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[l+31:l])*scale] := a[i+63:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPSCATTERDQ'. Intrinsic: '_mm_mask_i32scatter_epi64'.
// Requires AVX512F.
func MaskI32scatterEpi64(base_addr uintptr, k Mmask8, vindex M128i, a M128i, scale int)  {
	maskI32scatterEpi64(uintptr(base_addr), uint8(k), [16]byte(vindex), [16]byte(a), scale)
}

func maskI32scatterEpi64(base_addr uintptr, k uint8, vindex [16]byte, a [16]byte, scale int) 


// I32scatterEpi641: Scatter 64-bit integers from 'a' into memory using 32-bit
// indices. 64-bit elements are stored at addresses starting at 'base_addr' and
// offset by each 32-bit element in 'vindex' (each index is scaled by the
// factor in 'scale'). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			MEM[base_addr + SignExtend(vindex[l+31:l])*scale] := a[i+63:i]
//		ENDFOR
//
// Instruction: 'VPSCATTERDQ'. Intrinsic: '_mm256_i32scatter_epi64'.
// Requires AVX512F.
func I32scatterEpi641(base_addr uintptr, vindex M128i, a M256i, scale int)  {
	i32scatterEpi641(uintptr(base_addr), [16]byte(vindex), [32]byte(a), scale)
}

func i32scatterEpi641(base_addr uintptr, vindex [16]byte, a [32]byte, scale int) 


// MaskI32scatterEpi641: Scatter 64-bit integers from 'a' into memory using
// 32-bit indices. 64-bit elements are stored at addresses starting at
// 'base_addr' and offset by each 32-bit element in 'vindex' (each index is
// scaled by the factor in 'scale') subject to mask 'k' (elements are not
// stored when the corresponding mask bit is not set). 'scale' should be 1, 2,
// 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[l+31:l])*scale] := a[i+63:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPSCATTERDQ'. Intrinsic: '_mm256_mask_i32scatter_epi64'.
// Requires AVX512F.
func MaskI32scatterEpi641(base_addr uintptr, k Mmask8, vindex M128i, a M256i, scale int)  {
	maskI32scatterEpi641(uintptr(base_addr), uint8(k), [16]byte(vindex), [32]byte(a), scale)
}

func maskI32scatterEpi641(base_addr uintptr, k uint8, vindex [16]byte, a [32]byte, scale int) 


// I32scatterEpi642: Scatter 64-bit integers from 'a' into memory using 32-bit
// indices. 64-bit elements are stored at addresses starting at 'base_addr' and
// offset by each 32-bit element in 'vindex' (each index is scaled by the
// factor in 'scale'). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			MEM[base_addr + SignExtend(vindex[l+31:l])*scale] := a[i+63:i]
//		ENDFOR
//
// Instruction: 'VPSCATTERDQ'. Intrinsic: '_mm512_i32scatter_epi64'.
// Requires AVX512F.
func I32scatterEpi642(base_addr uintptr, vindex M256i, a M512i, scale int)  {
	i32scatterEpi642(uintptr(base_addr), [32]byte(vindex), [64]byte(a), scale)
}

func i32scatterEpi642(base_addr uintptr, vindex [32]byte, a [64]byte, scale int) 


// MaskI32scatterEpi642: Scatter 64-bit integers from 'a' into memory using
// 32-bit indices. 64-bit elements are stored at addresses starting at
// 'base_addr' and offset by each 32-bit element in 'vindex' (each index is
// scaled by the factor in 'scale') subject to mask 'k' (elements are not
// stored when the corresponding mask bit is not set). 'scale' should be 1, 2,
// 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[l+31:l])*scale] := a[i+63:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPSCATTERDQ'. Intrinsic: '_mm512_mask_i32scatter_epi64'.
// Requires AVX512F.
func MaskI32scatterEpi642(base_addr uintptr, k Mmask8, vindex M256i, a M512i, scale int)  {
	maskI32scatterEpi642(uintptr(base_addr), uint8(k), [32]byte(vindex), [64]byte(a), scale)
}

func maskI32scatterEpi642(base_addr uintptr, k uint8, vindex [32]byte, a [64]byte, scale int) 


// I32scatterPd: Scatter double-precision (64-bit) floating-point elements from
// 'a' into memory using 32-bit indices. 64-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 32-bit element in
// 'vindex' (each index is scaled by the factor in 'scale'). 'scale' should be
// 1, 2, 4 or 8. 
//
//		FOR j := 0 to 1
//			i := j*64
//			l := j*32
//			MEM[base_addr + SignExtend(vindex[l+31:l])*scale] := a[i+63:i]
//		ENDFOR
//
// Instruction: 'VSCATTERDPD'. Intrinsic: '_mm_i32scatter_pd'.
// Requires AVX512F.
func I32scatterPd(base_addr uintptr, vindex M128i, a M128d, scale int)  {
	i32scatterPd(uintptr(base_addr), [16]byte(vindex), [2]float64(a), scale)
}

func i32scatterPd(base_addr uintptr, vindex [16]byte, a [2]float64, scale int) 


// MaskI32scatterPd: Scatter double-precision (64-bit) floating-point elements
// from 'a' into memory using 32-bit indices. 64-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 32-bit element in
// 'vindex' (each index is scaled by the factor in 'scale') subject to mask 'k'
// (elements are not stored when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 1
//			i := j*64
//			l := j*32
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[l+31:l])*scale] := a[i+63:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VSCATTERDPD'. Intrinsic: '_mm_mask_i32scatter_pd'.
// Requires AVX512F.
func MaskI32scatterPd(base_addr uintptr, k Mmask8, vindex M128i, a M128d, scale int)  {
	maskI32scatterPd(uintptr(base_addr), uint8(k), [16]byte(vindex), [2]float64(a), scale)
}

func maskI32scatterPd(base_addr uintptr, k uint8, vindex [16]byte, a [2]float64, scale int) 


// I32scatterPd1: Scatter double-precision (64-bit) floating-point elements
// from 'a' into memory using 32-bit indices. 64-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 32-bit element in
// 'vindex' (each index is scaled by the factor in 'scale'). 'scale' should be
// 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			MEM[base_addr + SignExtend(vindex[l+31:l])*scale] := a[i+63:i]
//		ENDFOR
//
// Instruction: 'VSCATTERDPD'. Intrinsic: '_mm256_i32scatter_pd'.
// Requires AVX512F.
func I32scatterPd1(base_addr uintptr, vindex M128i, a M256d, scale int)  {
	i32scatterPd1(uintptr(base_addr), [16]byte(vindex), [4]float64(a), scale)
}

func i32scatterPd1(base_addr uintptr, vindex [16]byte, a [4]float64, scale int) 


// MaskI32scatterPd1: Scatter double-precision (64-bit) floating-point elements
// from 'a' into memory using 32-bit indices. 64-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 32-bit element in
// 'vindex' (each index is scaled by the factor in 'scale') subject to mask 'k'
// (elements are not stored when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[l+31:l])*scale] := a[i+63:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VSCATTERDPD'. Intrinsic: '_mm256_mask_i32scatter_pd'.
// Requires AVX512F.
func MaskI32scatterPd1(base_addr uintptr, k Mmask8, vindex M128i, a M256d, scale int)  {
	maskI32scatterPd1(uintptr(base_addr), uint8(k), [16]byte(vindex), [4]float64(a), scale)
}

func maskI32scatterPd1(base_addr uintptr, k uint8, vindex [16]byte, a [4]float64, scale int) 


// I32scatterPd2: Scatter double-precision (64-bit) floating-point elements
// from 'a' into memory using 32-bit indices. 64-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 32-bit element in
// 'vindex' (each index is scaled by the factor in 'scale'). 'scale' should be
// 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			MEM[base_addr + SignExtend(vindex[l+31:l])*scale] := a[i+63:i]
//		ENDFOR
//
// Instruction: 'VSCATTERDPD'. Intrinsic: '_mm512_i32scatter_pd'.
// Requires AVX512F.
func I32scatterPd2(base_addr uintptr, vindex M256i, a M512d, scale int)  {
	i32scatterPd2(uintptr(base_addr), [32]byte(vindex), [8]float64(a), scale)
}

func i32scatterPd2(base_addr uintptr, vindex [32]byte, a [8]float64, scale int) 


// MaskI32scatterPd2: Scatter double-precision (64-bit) floating-point elements
// from 'a' into memory using 32-bit indices. 64-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 32-bit element in
// 'vindex' (each index is scaled by the factor in 'scale') subject to mask 'k'
// (elements are not stored when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*64
//			l := j*32
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[l+31:l])*scale] := a[i+63:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VSCATTERDPD'. Intrinsic: '_mm512_mask_i32scatter_pd'.
// Requires AVX512F.
func MaskI32scatterPd2(base_addr uintptr, k Mmask8, vindex M256i, a M512d, scale int)  {
	maskI32scatterPd2(uintptr(base_addr), uint8(k), [32]byte(vindex), [8]float64(a), scale)
}

func maskI32scatterPd2(base_addr uintptr, k uint8, vindex [32]byte, a [8]float64, scale int) 


// I32scatterPs: Scatter single-precision (32-bit) floating-point elements from
// 'a' into memory using 32-bit indices. 32-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 32-bit element in
// 'vindex' (each index is scaled by the factor in 'scale'). 'scale' should be
// 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*32
//			MEM[base_addr + SignExtend(vindex[i+31:i])*scale] := a[i+31:i]
//		ENDFOR
//
// Instruction: 'VSCATTERDPS'. Intrinsic: '_mm_i32scatter_ps'.
// Requires AVX512F.
func I32scatterPs(base_addr uintptr, vindex M128i, a M128, scale int)  {
	i32scatterPs(uintptr(base_addr), [16]byte(vindex), [4]float32(a), scale)
}

func i32scatterPs(base_addr uintptr, vindex [16]byte, a [4]float32, scale int) 


// MaskI32scatterPs: Scatter single-precision (32-bit) floating-point elements
// from 'a' into memory using 32-bit indices. 32-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 32-bit element in
// 'vindex' (each index is scaled by the factor in 'scale') subject to mask 'k'
// (elements are not stored when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[i+31:i])*scale] := a[i+31:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VSCATTERDPS'. Intrinsic: '_mm_mask_i32scatter_ps'.
// Requires AVX512F.
func MaskI32scatterPs(base_addr uintptr, k Mmask8, vindex M128i, a M128, scale int)  {
	maskI32scatterPs(uintptr(base_addr), uint8(k), [16]byte(vindex), [4]float32(a), scale)
}

func maskI32scatterPs(base_addr uintptr, k uint8, vindex [16]byte, a [4]float32, scale int) 


// I32scatterPs1: Scatter single-precision (32-bit) floating-point elements
// from 'a' into memory using 32-bit indices. 32-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 32-bit element in
// 'vindex' (each index is scaled by the factor in 'scale'). 'scale' should be
// 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*32
//			MEM[base_addr + SignExtend(vindex[i+31:i])*scale] := a[i+31:i]
//		ENDFOR
//
// Instruction: 'VSCATTERDPS'. Intrinsic: '_mm256_i32scatter_ps'.
// Requires AVX512F.
func I32scatterPs1(base_addr uintptr, vindex M256i, a M256, scale int)  {
	i32scatterPs1(uintptr(base_addr), [32]byte(vindex), [8]float32(a), scale)
}

func i32scatterPs1(base_addr uintptr, vindex [32]byte, a [8]float32, scale int) 


// MaskI32scatterPs1: Scatter single-precision (32-bit) floating-point elements
// from 'a' into memory using 32-bit indices. 32-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 32-bit element in
// 'vindex' (each index is scaled by the factor in 'scale') subject to mask 'k'
// (elements are not stored when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[i+31:i])*scale] := a[i+31:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VSCATTERDPS'. Intrinsic: '_mm256_mask_i32scatter_ps'.
// Requires AVX512F.
func MaskI32scatterPs1(base_addr uintptr, k Mmask8, vindex M256i, a M256, scale int)  {
	maskI32scatterPs1(uintptr(base_addr), uint8(k), [32]byte(vindex), [8]float32(a), scale)
}

func maskI32scatterPs1(base_addr uintptr, k uint8, vindex [32]byte, a [8]float32, scale int) 


// MmaskI64gatherEpi32: Gather 32-bit integers from memory using 64-bit
// indices. 32-bit elements are loaded from addresses starting at 'base_addr'
// and offset by each 64-bit element in 'vindex' (each index is scaled by the
// factor in 'scale'). Gathered elements are merged into 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 1
//			i := j*32
//			m := j*64
//			IF k[j]
//				dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//		dst[MAX:64] := 0
//
// Instruction: 'VPGATHERQD'. Intrinsic: '_mm_mmask_i64gather_epi32'.
// Requires AVX512F.
func MmaskI64gatherEpi32(src M128i, k Mmask8, vindex M128i, base_addr uintptr, scale int) M128i {
	return M128i(mmaskI64gatherEpi32([16]byte(src), uint8(k), [16]byte(vindex), uintptr(base_addr), scale))
}

func mmaskI64gatherEpi32(src [16]byte, k uint8, vindex [16]byte, base_addr uintptr, scale int) [16]byte


// MmaskI64gatherEpi321: Gather 32-bit integers from memory using 64-bit
// indices. 32-bit elements are loaded from addresses starting at 'base_addr'
// and offset by each 64-bit element in 'vindex' (each index is scaled by the
// factor in 'scale'). Gathered elements are merged into 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*32
//			m := j*64
//			IF k[j]
//				dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//		dst[MAX:128] := 0
//
// Instruction: 'VPGATHERQD'. Intrinsic: '_mm256_mmask_i64gather_epi32'.
// Requires AVX512F.
func MmaskI64gatherEpi321(src M128i, k Mmask8, vindex M256i, base_addr uintptr, scale int) M128i {
	return M128i(mmaskI64gatherEpi321([16]byte(src), uint8(k), [32]byte(vindex), uintptr(base_addr), scale))
}

func mmaskI64gatherEpi321(src [16]byte, k uint8, vindex [32]byte, base_addr uintptr, scale int) [16]byte


// I64gatherEpi32: Gather 32-bit integers from memory using 64-bit indices.
// 32-bit elements are loaded from addresses starting at 'base_addr' and offset
// by each 64-bit element in 'vindex' (each index is scaled by the factor in
// 'scale'). Gathered elements are merged into 'dst'. 'scale' should be 1, 2, 4
// or 8. 
//
//		FOR j := 0 to 7
//			i := j*32
//			m := j*64
//			dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPGATHERQD'. Intrinsic: '_mm512_i64gather_epi32'.
// Requires AVX512F.
func I64gatherEpi32(vindex M512i, base_addr uintptr, scale int) M256i {
	return M256i(i64gatherEpi32([64]byte(vindex), uintptr(base_addr), scale))
}

func i64gatherEpi32(vindex [64]byte, base_addr uintptr, scale int) [32]byte


// MaskI64gatherEpi32: Gather 32-bit integers from memory using 64-bit indices.
// 32-bit elements are loaded from addresses starting at 'base_addr' and offset
// by each 64-bit element in 'vindex' (each index is scaled by the factor in
// 'scale'). Gathered elements are merged into 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*32
//			m := j*64
//			IF k[j]
//				dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VPGATHERQD'. Intrinsic: '_mm512_mask_i64gather_epi32'.
// Requires AVX512F.
func MaskI64gatherEpi32(src M256i, k Mmask8, vindex M512i, base_addr uintptr, scale int) M256i {
	return M256i(maskI64gatherEpi32([32]byte(src), uint8(k), [64]byte(vindex), uintptr(base_addr), scale))
}

func maskI64gatherEpi32(src [32]byte, k uint8, vindex [64]byte, base_addr uintptr, scale int) [32]byte


// MmaskI64gatherEpi64: Gather 64-bit integers from memory using 64-bit
// indices. 64-bit elements are loaded from addresses starting at 'base_addr'
// and offset by each 64-bit element in 'vindex' (each index is scaled by the
// factor in 'scale'). Gathered elements are merged into 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//		dst[MAX:128] := 0
//
// Instruction: 'VPGATHERQQ'. Intrinsic: '_mm_mmask_i64gather_epi64'.
// Requires AVX512F.
func MmaskI64gatherEpi64(src M128i, k Mmask8, vindex M128i, base_addr uintptr, scale int) M128i {
	return M128i(mmaskI64gatherEpi64([16]byte(src), uint8(k), [16]byte(vindex), uintptr(base_addr), scale))
}

func mmaskI64gatherEpi64(src [16]byte, k uint8, vindex [16]byte, base_addr uintptr, scale int) [16]byte


// MmaskI64gatherEpi641: Gather 64-bit integers from memory using 64-bit
// indices. 64-bit elements are loaded from addresses starting at 'base_addr'
// and offset by each 64-bit element in 'vindex' (each index is scaled by the
// factor in 'scale'). Gathered elements are merged into 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VPGATHERQQ'. Intrinsic: '_mm256_mmask_i64gather_epi64'.
// Requires AVX512F.
func MmaskI64gatherEpi641(src M256i, k Mmask8, vindex M256i, base_addr uintptr, scale int) M256i {
	return M256i(mmaskI64gatherEpi641([32]byte(src), uint8(k), [32]byte(vindex), uintptr(base_addr), scale))
}

func mmaskI64gatherEpi641(src [32]byte, k uint8, vindex [32]byte, base_addr uintptr, scale int) [32]byte


// I64gatherEpi64: Gather 64-bit integers from memory using 64-bit indices.
// 64-bit elements are loaded from addresses starting at 'base_addr' and offset
// by each 64-bit element in 'vindex' (each index is scaled by the factor in
// 'scale'). Gathered elements are merged into 'dst'. 'scale' should be 1, 2, 4
// or 8. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPGATHERQQ'. Intrinsic: '_mm512_i64gather_epi64'.
// Requires AVX512F.
func I64gatherEpi64(vindex M512i, base_addr uintptr, scale int) M512i {
	return M512i(i64gatherEpi64([64]byte(vindex), uintptr(base_addr), scale))
}

func i64gatherEpi64(vindex [64]byte, base_addr uintptr, scale int) [64]byte


// MaskI64gatherEpi64: Gather 64-bit integers from memory using 64-bit indices.
// 64-bit elements are loaded from addresses starting at 'base_addr' and offset
// by each 64-bit element in 'vindex' (each index is scaled by the factor in
// 'scale'). Gathered elements are merged into 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//		dst[MAX:512] := 0
//
// Instruction: 'VPGATHERQQ'. Intrinsic: '_mm512_mask_i64gather_epi64'.
// Requires AVX512F.
func MaskI64gatherEpi64(src M512i, k Mmask8, vindex M512i, base_addr uintptr, scale int) M512i {
	return M512i(maskI64gatherEpi64([64]byte(src), uint8(k), [64]byte(vindex), uintptr(base_addr), scale))
}

func maskI64gatherEpi64(src [64]byte, k uint8, vindex [64]byte, base_addr uintptr, scale int) [64]byte


// MmaskI64gatherPd: Gather double-precision (64-bit) floating-point elements
// from memory using 64-bit indices. 64-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 64-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//		dst[MAX:128] := 0
//
// Instruction: 'VGATHERQPD'. Intrinsic: '_mm_mmask_i64gather_pd'.
// Requires AVX512F.
func MmaskI64gatherPd(src M128d, k Mmask8, vindex M128i, base_addr uintptr, scale int) M128d {
	return M128d(mmaskI64gatherPd([2]float64(src), uint8(k), [16]byte(vindex), uintptr(base_addr), scale))
}

func mmaskI64gatherPd(src [2]float64, k uint8, vindex [16]byte, base_addr uintptr, scale int) [2]float64


// MmaskI64gatherPd1: Gather double-precision (64-bit) floating-point elements
// from memory using 64-bit indices. 64-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 64-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VGATHERQPD'. Intrinsic: '_mm256_mmask_i64gather_pd'.
// Requires AVX512F.
func MmaskI64gatherPd1(src M256d, k Mmask8, vindex M256i, base_addr uintptr, scale int) M256d {
	return M256d(mmaskI64gatherPd1([4]float64(src), uint8(k), [32]byte(vindex), uintptr(base_addr), scale))
}

func mmaskI64gatherPd1(src [4]float64, k uint8, vindex [32]byte, base_addr uintptr, scale int) [4]float64


// I64gatherPd: Gather double-precision (64-bit) floating-point elements from
// memory using 64-bit indices. 64-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 64-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst'. 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VGATHERQPD'. Intrinsic: '_mm512_i64gather_pd'.
// Requires AVX512F.
func I64gatherPd(vindex M512i, base_addr uintptr, scale int) M512d {
	return M512d(i64gatherPd([64]byte(vindex), uintptr(base_addr), scale))
}

func i64gatherPd(vindex [64]byte, base_addr uintptr, scale int) [8]float64


// MaskI64gatherPd: Gather double-precision (64-bit) floating-point elements
// from memory using 64-bit indices. 64-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 64-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//		dst[MAX:512] := 0
//
// Instruction: 'VGATHERQPD'. Intrinsic: '_mm512_mask_i64gather_pd'.
// Requires AVX512F.
func MaskI64gatherPd(src M512d, k Mmask8, vindex M512i, base_addr uintptr, scale int) M512d {
	return M512d(maskI64gatherPd([8]float64(src), uint8(k), [64]byte(vindex), uintptr(base_addr), scale))
}

func maskI64gatherPd(src [8]float64, k uint8, vindex [64]byte, base_addr uintptr, scale int) [8]float64


// MmaskI64gatherPs: Gather single-precision (32-bit) floating-point elements
// from memory using 64-bit indices. 32-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 64-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 1
//			i := j*32
//			m := j*64
//			IF k[j]
//				dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//		dst[MAX:64] := 0
//
// Instruction: 'VGATHERQPS'. Intrinsic: '_mm_mmask_i64gather_ps'.
// Requires AVX512F.
func MmaskI64gatherPs(src M128, k Mmask8, vindex M128i, base_addr uintptr, scale int) M128 {
	return M128(mmaskI64gatherPs([4]float32(src), uint8(k), [16]byte(vindex), uintptr(base_addr), scale))
}

func mmaskI64gatherPs(src [4]float32, k uint8, vindex [16]byte, base_addr uintptr, scale int) [4]float32


// MmaskI64gatherPs1: Gather single-precision (32-bit) floating-point elements
// from memory using 64-bit indices. 32-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 64-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*32
//			m := j*64
//			IF k[j]
//				dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//		dst[MAX:128] := 0
//
// Instruction: 'VGATHERQPS'. Intrinsic: '_mm256_mmask_i64gather_ps'.
// Requires AVX512F.
func MmaskI64gatherPs1(src M128, k Mmask8, vindex M256i, base_addr uintptr, scale int) M128 {
	return M128(mmaskI64gatherPs1([4]float32(src), uint8(k), [32]byte(vindex), uintptr(base_addr), scale))
}

func mmaskI64gatherPs1(src [4]float32, k uint8, vindex [32]byte, base_addr uintptr, scale int) [4]float32


// I64gatherPs: Gather single-precision (32-bit) floating-point elements from
// memory using 64-bit indices. 32-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 64-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst'. 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*32
//			m := j*64
//			dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGATHERQPS'. Intrinsic: '_mm512_i64gather_ps'.
// Requires AVX512F.
func I64gatherPs(vindex M512i, base_addr uintptr, scale int) M256 {
	return M256(i64gatherPs([64]byte(vindex), uintptr(base_addr), scale))
}

func i64gatherPs(vindex [64]byte, base_addr uintptr, scale int) [8]float32


// MaskI64gatherPs: Gather single-precision (32-bit) floating-point elements
// from memory using 64-bit indices. 32-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 64-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*32
//			m := j*64
//			IF k[j]
//				dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VGATHERQPS'. Intrinsic: '_mm512_mask_i64gather_ps'.
// Requires AVX512F.
func MaskI64gatherPs(src M256, k Mmask8, vindex M512i, base_addr uintptr, scale int) M256 {
	return M256(maskI64gatherPs([8]float32(src), uint8(k), [64]byte(vindex), uintptr(base_addr), scale))
}

func maskI64gatherPs(src [8]float32, k uint8, vindex [64]byte, base_addr uintptr, scale int) [8]float32


// I64scatterEpi32: Scatter 32-bit integers from 'a' into memory using 64-bit
// indices. 32-bit elements are stored at addresses starting at 'base_addr' and
// offset by each 64-bit element in 'vindex' (each index is scaled by the
// factor in 'scale'). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 1
//			i := j*32
//			l := j*64
//			MEM[base_addr + SignExtend(vindex[l+63:l])*scale] := a[i+31:i]
//		ENDFOR
//
// Instruction: 'VPSCATTERQD'. Intrinsic: '_mm_i64scatter_epi32'.
// Requires AVX512F.
func I64scatterEpi32(base_addr uintptr, vindex M128i, a M128i, scale int)  {
	i64scatterEpi32(uintptr(base_addr), [16]byte(vindex), [16]byte(a), scale)
}

func i64scatterEpi32(base_addr uintptr, vindex [16]byte, a [16]byte, scale int) 


// MaskI64scatterEpi32: Scatter 32-bit integers from 'a' into memory using
// 64-bit indices. 32-bit elements are stored at addresses starting at
// 'base_addr' and offset by each 64-bit element in 'vindex' (each index is
// scaled by the factor in 'scale') subject to mask 'k' (elements are not
// stored when the corresponding mask bit is not set). 'scale' should be 1, 2,
// 4 or 8. 
//
//		FOR j := 0 to 1
//			i := j*32
//			l := j*64
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[l+63:l])*scale] := a[i+31:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPSCATTERQD'. Intrinsic: '_mm_mask_i64scatter_epi32'.
// Requires AVX512F.
func MaskI64scatterEpi32(base_addr uintptr, k Mmask8, vindex M128i, a M128i, scale int)  {
	maskI64scatterEpi32(uintptr(base_addr), uint8(k), [16]byte(vindex), [16]byte(a), scale)
}

func maskI64scatterEpi32(base_addr uintptr, k uint8, vindex [16]byte, a [16]byte, scale int) 


// I64scatterEpi321: Scatter 32-bit integers from 'a' into memory using 64-bit
// indices. 32-bit elements are stored at addresses starting at 'base_addr' and
// offset by each 64-bit element in 'vindex' (each index is scaled by the
// factor in 'scale'). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*32
//			l := j*64
//			MEM[base_addr + SignExtend(vindex[l+63:l])*scale] := a[i+31:i]
//		ENDFOR
//
// Instruction: 'VPSCATTERQD'. Intrinsic: '_mm256_i64scatter_epi32'.
// Requires AVX512F.
func I64scatterEpi321(base_addr uintptr, vindex M256i, a M128i, scale int)  {
	i64scatterEpi321(uintptr(base_addr), [32]byte(vindex), [16]byte(a), scale)
}

func i64scatterEpi321(base_addr uintptr, vindex [32]byte, a [16]byte, scale int) 


// MaskI64scatterEpi321: Scatter 32-bit integers from 'a' into memory using
// 64-bit indices. 32-bit elements are stored at addresses starting at
// 'base_addr' and offset by each 64-bit element in 'vindex' (each index is
// scaled by the factor in 'scale') subject to mask 'k' (elements are not
// stored when the corresponding mask bit is not set). 'scale' should be 1, 2,
// 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*32
//			l := j*64
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[l+63:l])*scale] := a[i+31:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPSCATTERQD'. Intrinsic: '_mm256_mask_i64scatter_epi32'.
// Requires AVX512F.
func MaskI64scatterEpi321(base_addr uintptr, k Mmask8, vindex M256i, a M128i, scale int)  {
	maskI64scatterEpi321(uintptr(base_addr), uint8(k), [32]byte(vindex), [16]byte(a), scale)
}

func maskI64scatterEpi321(base_addr uintptr, k uint8, vindex [32]byte, a [16]byte, scale int) 


// I64scatterEpi322: Scatter 32-bit integers from 'a' into memory using 64-bit
// indices. 32-bit elements are stored at addresses starting at 'base_addr' and
// offset by each 64-bit element in 'vindex' (each index is scaled by the
// factor in 'scale'). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*32
//			l := j*64
//			MEM[base_addr + SignExtend(vindex[l+63:l])*scale] := a[i+31:i]
//		ENDFOR
//
// Instruction: 'VPSCATTERQD'. Intrinsic: '_mm512_i64scatter_epi32'.
// Requires AVX512F.
func I64scatterEpi322(base_addr uintptr, vindex M512i, a M256i, scale int)  {
	i64scatterEpi322(uintptr(base_addr), [64]byte(vindex), [32]byte(a), scale)
}

func i64scatterEpi322(base_addr uintptr, vindex [64]byte, a [32]byte, scale int) 


// MaskI64scatterEpi322: Scatter 32-bit integers from 'a' into memory using
// 64-bit indices. 32-bit elements are stored at addresses starting at
// 'base_addr' and offset by each 64-bit element in 'vindex' (each index is
// scaled by the factor in 'scale') subject to mask 'k' (elements are not
// stored when the corresponding mask bit is not set). 'scale' should be 1, 2,
// 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*32
//			l := j*64
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[l+63:l])*scale] := a[i+31:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPSCATTERQD'. Intrinsic: '_mm512_mask_i64scatter_epi32'.
// Requires AVX512F.
func MaskI64scatterEpi322(base_addr uintptr, k Mmask8, vindex M512i, a M256i, scale int)  {
	maskI64scatterEpi322(uintptr(base_addr), uint8(k), [64]byte(vindex), [32]byte(a), scale)
}

func maskI64scatterEpi322(base_addr uintptr, k uint8, vindex [64]byte, a [32]byte, scale int) 


// I64scatterEpi64: Scatter 64-bit integers from 'a' into memory using 64-bit
// indices. 64-bit elements are stored at addresses starting at 'base_addr' and
// offset by each 64-bit element in 'vindex' (each index is scaled by the
// factor in 'scale'). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 1
//			i := j*64
//			MEM[base_addr + SignExtend(vindex[i+63:i])*scale] := a[i+63:i]
//		ENDFOR
//
// Instruction: 'VPSCATTERQQ'. Intrinsic: '_mm_i64scatter_epi64'.
// Requires AVX512F.
func I64scatterEpi64(base_addr uintptr, vindex M128i, a M128i, scale int)  {
	i64scatterEpi64(uintptr(base_addr), [16]byte(vindex), [16]byte(a), scale)
}

func i64scatterEpi64(base_addr uintptr, vindex [16]byte, a [16]byte, scale int) 


// MaskI64scatterEpi64: Scatter 64-bit integers from 'a' into memory using
// 64-bit indices. 64-bit elements are stored at addresses starting at
// 'base_addr' and offset by each 64-bit element in 'vindex' (each index is
// scaled by the factor in 'scale') subject to mask 'k' (elements are not
// stored when the corresponding mask bit is not set). 'scale' should be 1, 2,
// 4 or 8. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[i+63:i])*scale] := a[i+63:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPSCATTERQQ'. Intrinsic: '_mm_mask_i64scatter_epi64'.
// Requires AVX512F.
func MaskI64scatterEpi64(base_addr uintptr, k Mmask8, vindex M128i, a M128i, scale int)  {
	maskI64scatterEpi64(uintptr(base_addr), uint8(k), [16]byte(vindex), [16]byte(a), scale)
}

func maskI64scatterEpi64(base_addr uintptr, k uint8, vindex [16]byte, a [16]byte, scale int) 


// I64scatterEpi641: Scatter 64-bit integers from 'a' into memory using 64-bit
// indices. 64-bit elements are stored at addresses starting at 'base_addr' and
// offset by each 64-bit element in 'vindex' (each index is scaled by the
// factor in 'scale'). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			MEM[base_addr + SignExtend(vindex[i+63:i])*scale] := a[i+63:i]
//		ENDFOR
//
// Instruction: 'VPSCATTERQQ'. Intrinsic: '_mm256_i64scatter_epi64'.
// Requires AVX512F.
func I64scatterEpi641(base_addr uintptr, vindex M256i, a M256i, scale int)  {
	i64scatterEpi641(uintptr(base_addr), [32]byte(vindex), [32]byte(a), scale)
}

func i64scatterEpi641(base_addr uintptr, vindex [32]byte, a [32]byte, scale int) 


// MaskI64scatterEpi641: Scatter 64-bit integers from 'a' into memory using
// 64-bit indices. 64-bit elements are stored at addresses starting at
// 'base_addr' and offset by each 64-bit element in 'vindex' (each index is
// scaled by the factor in 'scale') subject to mask 'k' (elements are not
// stored when the corresponding mask bit is not set). 'scale' should be 1, 2,
// 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[i+63:i])*scale] := a[i+63:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPSCATTERQQ'. Intrinsic: '_mm256_mask_i64scatter_epi64'.
// Requires AVX512F.
func MaskI64scatterEpi641(base_addr uintptr, k Mmask8, vindex M256i, a M256i, scale int)  {
	maskI64scatterEpi641(uintptr(base_addr), uint8(k), [32]byte(vindex), [32]byte(a), scale)
}

func maskI64scatterEpi641(base_addr uintptr, k uint8, vindex [32]byte, a [32]byte, scale int) 


// I64scatterEpi642: Scatter 64-bit integers from 'a' into memory using 64-bit
// indices. 64-bit elements are stored at addresses starting at 'base_addr' and
// offset by each 64-bit element in 'vindex' (each index is scaled by the
// factor in 'scale'). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*64
//			MEM[base_addr + SignExtend(vindex[i+63:i])*scale] := a[i+63:i]
//		ENDFOR
//
// Instruction: 'VPSCATTERQQ'. Intrinsic: '_mm512_i64scatter_epi64'.
// Requires AVX512F.
func I64scatterEpi642(base_addr uintptr, vindex M512i, a M512i, scale int)  {
	i64scatterEpi642(uintptr(base_addr), [64]byte(vindex), [64]byte(a), scale)
}

func i64scatterEpi642(base_addr uintptr, vindex [64]byte, a [64]byte, scale int) 


// MaskI64scatterEpi642: Scatter 64-bit integers from 'a' into memory using
// 64-bit indices. 64-bit elements are stored at addresses starting at
// 'base_addr' and offset by each 64-bit element in 'vindex' (each index is
// scaled by the factor in 'scale') subject to mask 'k' (elements are not
// stored when the corresponding mask bit is not set). 'scale' should be 1, 2,
// 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[i+63:i])*scale] := a[i+63:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPSCATTERQQ'. Intrinsic: '_mm512_mask_i64scatter_epi64'.
// Requires AVX512F.
func MaskI64scatterEpi642(base_addr uintptr, k Mmask8, vindex M512i, a M512i, scale int)  {
	maskI64scatterEpi642(uintptr(base_addr), uint8(k), [64]byte(vindex), [64]byte(a), scale)
}

func maskI64scatterEpi642(base_addr uintptr, k uint8, vindex [64]byte, a [64]byte, scale int) 


// I64scatterPd: Scatter double-precision (64-bit) floating-point elements from
// 'a' into memory using 64-bit indices. 64-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 64-bit element in
// 'vindex' (each index is scaled by the factor in 'scale'). 'scale' should be
// 1, 2, 4 or 8. 
//
//		FOR j := 0 to 1
//			i := j*64
//			MEM[base_addr + SignExtend(vindex[i+63:i])*scale] := a[i+63:i]
//		ENDFOR
//
// Instruction: 'VSCATTERQPD'. Intrinsic: '_mm_i64scatter_pd'.
// Requires AVX512F.
func I64scatterPd(base_addr uintptr, vindex M128i, a M128d, scale int)  {
	i64scatterPd(uintptr(base_addr), [16]byte(vindex), [2]float64(a), scale)
}

func i64scatterPd(base_addr uintptr, vindex [16]byte, a [2]float64, scale int) 


// MaskI64scatterPd: Scatter double-precision (64-bit) floating-point elements
// from 'a' into memory using 64-bit indices. 64-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 64-bit element in
// 'vindex' (each index is scaled by the factor in 'scale') subject to mask 'k'
// (elements are not stored when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[i+63:i])*scale] := a[i+63:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VSCATTERQPD'. Intrinsic: '_mm_mask_i64scatter_pd'.
// Requires AVX512F.
func MaskI64scatterPd(base_addr uintptr, k Mmask8, vindex M128i, a M128d, scale int)  {
	maskI64scatterPd(uintptr(base_addr), uint8(k), [16]byte(vindex), [2]float64(a), scale)
}

func maskI64scatterPd(base_addr uintptr, k uint8, vindex [16]byte, a [2]float64, scale int) 


// I64scatterPd1: Scatter double-precision (64-bit) floating-point elements
// from 'a' into memory using 64-bit indices. 64-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 64-bit element in
// 'vindex' (each index is scaled by the factor in 'scale'). 'scale' should be
// 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			MEM[base_addr + SignExtend(vindex[i+63:i])*scale] := a[i+63:i]
//		ENDFOR
//
// Instruction: 'VSCATTERQPD'. Intrinsic: '_mm256_i64scatter_pd'.
// Requires AVX512F.
func I64scatterPd1(base_addr uintptr, vindex M256i, a M256d, scale int)  {
	i64scatterPd1(uintptr(base_addr), [32]byte(vindex), [4]float64(a), scale)
}

func i64scatterPd1(base_addr uintptr, vindex [32]byte, a [4]float64, scale int) 


// MaskI64scatterPd1: Scatter double-precision (64-bit) floating-point elements
// from 'a' into memory using 64-bit indices. 64-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 64-bit element in
// 'vindex' (each index is scaled by the factor in 'scale') subject to mask 'k'
// (elements are not stored when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[i+63:i])*scale] := a[i+63:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VSCATTERQPD'. Intrinsic: '_mm256_mask_i64scatter_pd'.
// Requires AVX512F.
func MaskI64scatterPd1(base_addr uintptr, k Mmask8, vindex M256i, a M256d, scale int)  {
	maskI64scatterPd1(uintptr(base_addr), uint8(k), [32]byte(vindex), [4]float64(a), scale)
}

func maskI64scatterPd1(base_addr uintptr, k uint8, vindex [32]byte, a [4]float64, scale int) 


// I64scatterPd2: Scatter double-precision (64-bit) floating-point elements
// from 'a' into memory using 64-bit indices. 64-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 64-bit element in
// 'vindex' (each index is scaled by the factor in 'scale'). 'scale' should be
// 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*64
//			MEM[base_addr + SignExtend(vindex[i+63:i])*scale] := a[i+63:i]
//		ENDFOR
//
// Instruction: 'VSCATTERQPD'. Intrinsic: '_mm512_i64scatter_pd'.
// Requires AVX512F.
func I64scatterPd2(base_addr uintptr, vindex M512i, a M512d, scale int)  {
	i64scatterPd2(uintptr(base_addr), [64]byte(vindex), [8]float64(a), scale)
}

func i64scatterPd2(base_addr uintptr, vindex [64]byte, a [8]float64, scale int) 


// MaskI64scatterPd2: Scatter double-precision (64-bit) floating-point elements
// from 'a' into memory using 64-bit indices. 64-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 64-bit element in
// 'vindex' (each index is scaled by the factor in 'scale') subject to mask 'k'
// (elements are not stored when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[i+63:i])*scale] := a[i+63:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VSCATTERQPD'. Intrinsic: '_mm512_mask_i64scatter_pd'.
// Requires AVX512F.
func MaskI64scatterPd2(base_addr uintptr, k Mmask8, vindex M512i, a M512d, scale int)  {
	maskI64scatterPd2(uintptr(base_addr), uint8(k), [64]byte(vindex), [8]float64(a), scale)
}

func maskI64scatterPd2(base_addr uintptr, k uint8, vindex [64]byte, a [8]float64, scale int) 


// I64scatterPs: Scatter single-precision (32-bit) floating-point elements from
// 'a' into memory using 64-bit indices. 32-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 64-bit element in
// 'vindex' (each index is scaled by the factor in 'scale') subject to mask 'k'
// (elements are not stored when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 1
//			i := j*32
//			l := j*64
//			MEM[base_addr + SignExtend(vindex[l+63:l])*scale] := a[i+31:i]
//		ENDFOR
//
// Instruction: 'VSCATTERQPS'. Intrinsic: '_mm_i64scatter_ps'.
// Requires AVX512F.
func I64scatterPs(base_addr uintptr, vindex M128i, a M128, scale int)  {
	i64scatterPs(uintptr(base_addr), [16]byte(vindex), [4]float32(a), scale)
}

func i64scatterPs(base_addr uintptr, vindex [16]byte, a [4]float32, scale int) 


// MaskI64scatterPs: Scatter single-precision (32-bit) floating-point elements
// from 'a' into memory using 64-bit indices. 32-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 64-bit element in
// 'vindex' (each index is scaled by the factor in 'scale') subject to mask 'k'
// (elements are not stored when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 1
//			i := j*32
//			l := j*64
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[l+63:l])*scale] := a[i+31:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VSCATTERQPS'. Intrinsic: '_mm_mask_i64scatter_ps'.
// Requires AVX512F.
func MaskI64scatterPs(base_addr uintptr, k Mmask8, vindex M128i, a M128, scale int)  {
	maskI64scatterPs(uintptr(base_addr), uint8(k), [16]byte(vindex), [4]float32(a), scale)
}

func maskI64scatterPs(base_addr uintptr, k uint8, vindex [16]byte, a [4]float32, scale int) 


// I64scatterPs1: Scatter single-precision (32-bit) floating-point elements
// from 'a' into memory using 64-bit indices. 32-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 64-bit element in
// 'vindex' (each index is scaled by the factor in 'scale') subject to mask 'k'
// (elements are not stored when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*32
//			l := j*64
//			MEM[base_addr + SignExtend(vindex[l+63:l])*scale] := a[i+31:i]
//		ENDFOR
//
// Instruction: 'VSCATTERQPS'. Intrinsic: '_mm256_i64scatter_ps'.
// Requires AVX512F.
func I64scatterPs1(base_addr uintptr, vindex M256i, a M128, scale int)  {
	i64scatterPs1(uintptr(base_addr), [32]byte(vindex), [4]float32(a), scale)
}

func i64scatterPs1(base_addr uintptr, vindex [32]byte, a [4]float32, scale int) 


// MaskI64scatterPs1: Scatter single-precision (32-bit) floating-point elements
// from 'a' into memory using 64-bit indices. 32-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 64-bit element in
// 'vindex' (each index is scaled by the factor in 'scale') subject to mask 'k'
// (elements are not stored when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*32
//			l := j*64
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[l+63:l])*scale] := a[i+31:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VSCATTERQPS'. Intrinsic: '_mm256_mask_i64scatter_ps'.
// Requires AVX512F.
func MaskI64scatterPs1(base_addr uintptr, k Mmask8, vindex M256i, a M128, scale int)  {
	maskI64scatterPs1(uintptr(base_addr), uint8(k), [32]byte(vindex), [4]float32(a), scale)
}

func maskI64scatterPs1(base_addr uintptr, k uint8, vindex [32]byte, a [4]float32, scale int) 


// I64scatterPs2: Scatter single-precision (32-bit) floating-point elements
// from 'a' into memory using 64-bit indices. 32-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 64-bit element in
// 'vindex' (each index is scaled by the factor in 'scale') subject to mask 'k'
// (elements are not stored when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*32
//			l := j*64
//			MEM[base_addr + SignExtend(vindex[l+63:l])*scale] := a[i+31:i]
//		ENDFOR
//
// Instruction: 'VSCATTERQPS'. Intrinsic: '_mm512_i64scatter_ps'.
// Requires AVX512F.
func I64scatterPs2(base_addr uintptr, vindex M512i, a M256, scale int)  {
	i64scatterPs2(uintptr(base_addr), [64]byte(vindex), [8]float32(a), scale)
}

func i64scatterPs2(base_addr uintptr, vindex [64]byte, a [8]float32, scale int) 


// MaskI64scatterPs2: Scatter single-precision (32-bit) floating-point elements
// from 'a' into memory using 64-bit indices. 32-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 64-bit element in
// 'vindex' (each index is scaled by the factor in 'scale') subject to mask 'k'
// (elements are not stored when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*32
//			l := j*64
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[l+63:l])*scale] := a[i+31:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VSCATTERQPS'. Intrinsic: '_mm512_mask_i64scatter_ps'.
// Requires AVX512F.
func MaskI64scatterPs2(base_addr uintptr, k Mmask8, vindex M512i, a M256, scale int)  {
	maskI64scatterPs2(uintptr(base_addr), uint8(k), [64]byte(vindex), [8]float32(a), scale)
}

func maskI64scatterPs2(base_addr uintptr, k uint8, vindex [64]byte, a [8]float32, scale int) 


// Insertf32x4: Copy 'a' to 'dst', then insert 128 bits (composed of 4 packed
// single-precision (32-bit) floating-point elements) from 'b' into 'dst' at
// the location specified by 'imm8'. 
//
//		dst[255:0] := a[255:0]
//		CASE (imm8[1:0]) of
//		0: dst[127:0] := b[127:0]
//		1: dst[255:128] := b[127:0]
//		ESAC
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTF32X4'. Intrinsic: '_mm256_insertf32x4'.
// Requires AVX512F.
func Insertf32x4(a M256, b M128, imm8 int) M256 {
	return M256(insertf32x4([8]float32(a), [4]float32(b), imm8))
}

func insertf32x4(a [8]float32, b [4]float32, imm8 int) [8]float32


// MaskInsertf32x4: Copy 'a' to 'tmp', then insert 128 bits (composed of 4
// packed single-precision (32-bit) floating-point elements) from 'b' into
// 'tmp' at the location specified by 'imm8'.  Store 'tmp' to 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp[255:0] := a[255:0]
//		CASE (imm8[1:0]) of
//		0: tmp[127:0] := b[127:0]
//		1: tmp[255:128] := b[127:0]
//		ESAC
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTF32X4'. Intrinsic: '_mm256_mask_insertf32x4'.
// Requires AVX512F.
func MaskInsertf32x4(src M256, k Mmask8, a M256, b M128, imm8 int) M256 {
	return M256(maskInsertf32x4([8]float32(src), uint8(k), [8]float32(a), [4]float32(b), imm8))
}

func maskInsertf32x4(src [8]float32, k uint8, a [8]float32, b [4]float32, imm8 int) [8]float32


// MaskzInsertf32x4: Copy 'a' to 'tmp', then insert 128 bits (composed of 4
// packed single-precision (32-bit) floating-point elements) from 'b' into
// 'tmp' at the location specified by 'imm8'.  Store 'tmp' to 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp[255:0] := a[255:0]
//		CASE (imm8[1:0]) of
//		0: tmp[127:0] := b[127:0]
//		1: tmp[255:128] := b[127:0]
//		ESAC
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTF32X4'. Intrinsic: '_mm256_maskz_insertf32x4'.
// Requires AVX512F.
func MaskzInsertf32x4(k Mmask8, a M256, b M128, imm8 int) M256 {
	return M256(maskzInsertf32x4(uint8(k), [8]float32(a), [4]float32(b), imm8))
}

func maskzInsertf32x4(k uint8, a [8]float32, b [4]float32, imm8 int) [8]float32


// Insertf32x41: Copy 'a' to 'dst', then insert 128 bits (composed of 4 packed
// single-precision (32-bit) floating-point elements) from 'b' into 'dst' at
// the location specified by 'imm8'. 
//
//		dst[511:0] := a[511:0]
//		CASE (imm8[1:0]) of
//		0: dst[127:0] := b[127:0]
//		1: dst[255:128] := b[127:0]
//		2: dst[383:256] := b[127:0]
//		3: dst[511:384] := b[127:0]
//		ESAC
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTF32X4'. Intrinsic: '_mm512_insertf32x4'.
// Requires AVX512F.
func Insertf32x41(a M512, b M128, imm8 int) M512 {
	return M512(insertf32x41([16]float32(a), [4]float32(b), imm8))
}

func insertf32x41(a [16]float32, b [4]float32, imm8 int) [16]float32


// MaskInsertf32x41: Copy 'a' to 'tmp', then insert 128 bits (composed of 4
// packed single-precision (32-bit) floating-point elements) from 'b' into
// 'tmp' at the location specified by 'imm8'.  Store 'tmp' to 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp[511:0] := a[511:0]
//		CASE (imm8[1:0]) of
//		0: tmp[127:0] := b[127:0]
//		1: tmp[255:128] := b[127:0]
//		2: tmp[383:256] := b[127:0]
//		3: tmp[511:384] := b[127:0]
//		ESAC
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTF32X4'. Intrinsic: '_mm512_mask_insertf32x4'.
// Requires AVX512F.
func MaskInsertf32x41(src M512, k Mmask16, a M512, b M128, imm8 int) M512 {
	return M512(maskInsertf32x41([16]float32(src), uint16(k), [16]float32(a), [4]float32(b), imm8))
}

func maskInsertf32x41(src [16]float32, k uint16, a [16]float32, b [4]float32, imm8 int) [16]float32


// MaskzInsertf32x41: Copy 'a' to 'tmp', then insert 128 bits (composed of 4
// packed single-precision (32-bit) floating-point elements) from 'b' into
// 'tmp' at the location specified by 'imm8'.  Store 'tmp' to 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp[511:0] := a[511:0]
//		CASE (imm8[1:0]) of
//		0: tmp[127:0] := b[127:0]
//		1: tmp[255:128] := b[127:0]
//		2: tmp[383:256] := b[127:0]
//		3: tmp[511:384] := b[127:0]
//		ESAC
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTF32X4'. Intrinsic: '_mm512_maskz_insertf32x4'.
// Requires AVX512F.
func MaskzInsertf32x41(k Mmask16, a M512, b M128, imm8 int) M512 {
	return M512(maskzInsertf32x41(uint16(k), [16]float32(a), [4]float32(b), imm8))
}

func maskzInsertf32x41(k uint16, a [16]float32, b [4]float32, imm8 int) [16]float32


// Insertf64x4: Copy 'a' to 'dst', then insert 256 bits (composed of 4 packed
// double-precision (64-bit) floating-point elements) from 'b' into 'dst' at
// the location specified by 'imm8'. 
//
//		dst[511:0] := a[511:0]
//		CASE (imm8[0]) of
//		0: dst[255:0] := b[255:0]
//		1: dst[511:256] := b[255:0]
//		ESAC
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTF64X4'. Intrinsic: '_mm512_insertf64x4'.
// Requires AVX512F.
func Insertf64x4(a M512d, b M256d, imm8 int) M512d {
	return M512d(insertf64x4([8]float64(a), [4]float64(b), imm8))
}

func insertf64x4(a [8]float64, b [4]float64, imm8 int) [8]float64


// MaskInsertf64x4: Copy 'a' to 'tmp', then insert 256 bits (composed of 4
// packed double-precision (64-bit) floating-point elements) from 'b' into
// 'tmp' at the location specified by 'imm8'.  Store 'tmp' to 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp[511:0] := a[511:0]
//		CASE (imm8[0]) of
//		0: tmp[255:0] := b[255:0]
//		1: tmp[511:256] := b[255:0]
//		ESAC
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTF64X4'. Intrinsic: '_mm512_mask_insertf64x4'.
// Requires AVX512F.
func MaskInsertf64x4(src M512d, k Mmask8, a M512d, b M256d, imm8 int) M512d {
	return M512d(maskInsertf64x4([8]float64(src), uint8(k), [8]float64(a), [4]float64(b), imm8))
}

func maskInsertf64x4(src [8]float64, k uint8, a [8]float64, b [4]float64, imm8 int) [8]float64


// MaskzInsertf64x4: Copy 'a' to 'tmp', then insert 256 bits (composed of 4
// packed double-precision (64-bit) floating-point elements) from 'b' into
// 'tmp' at the location specified by 'imm8'.  Store 'tmp' to 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp[511:0] := a[511:0]
//		CASE (imm8[0]) of
//		0: tmp[255:0] := b[255:0]
//		1: tmp[511:256] := b[255:0]
//		ESAC
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTF64X4'. Intrinsic: '_mm512_maskz_insertf64x4'.
// Requires AVX512F.
func MaskzInsertf64x4(k Mmask8, a M512d, b M256d, imm8 int) M512d {
	return M512d(maskzInsertf64x4(uint8(k), [8]float64(a), [4]float64(b), imm8))
}

func maskzInsertf64x4(k uint8, a [8]float64, b [4]float64, imm8 int) [8]float64


// Inserti32x4: Copy 'a' to 'dst', then insert 128 bits (composed of 4 packed
// 32-bit integers) from 'b' into 'dst' at the location specified by 'imm8'. 
//
//		dst[255:0] := a[255:0]
//		CASE (imm8[1:0]) of
//		0: dst[127:0] := b[127:0]
//		1: dst[255:128] := b[127:0]
//		ESAC
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTI32X4'. Intrinsic: '_mm256_inserti32x4'.
// Requires AVX512F.
func Inserti32x4(a M256i, b M128i, imm8 int) M256i {
	return M256i(inserti32x4([32]byte(a), [16]byte(b), imm8))
}

func inserti32x4(a [32]byte, b [16]byte, imm8 int) [32]byte


// MaskInserti32x4: Copy 'a' to 'tmp', then insert 128 bits (composed of 4
// packed 32-bit integers) from 'b' into 'tmp' at the location specified by
// 'imm8'.  Store 'tmp' to 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		tmp[255:0] := a[255:0]
//		CASE (imm8[1:0]) of
//		0: tmp[127:0] := b[127:0]
//		1: tmp[255:128] := b[127:0]
//		ESAC
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTI32X4'. Intrinsic: '_mm256_mask_inserti32x4'.
// Requires AVX512F.
func MaskInserti32x4(src M256i, k Mmask8, a M256i, b M128i, imm8 int) M256i {
	return M256i(maskInserti32x4([32]byte(src), uint8(k), [32]byte(a), [16]byte(b), imm8))
}

func maskInserti32x4(src [32]byte, k uint8, a [32]byte, b [16]byte, imm8 int) [32]byte


// MaskzInserti32x4: Copy 'a' to 'tmp', then insert 128 bits (composed of 4
// packed 32-bit integers) from 'b' into 'tmp' at the location specified by
// 'imm8'.  Store 'tmp' to 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		tmp[255:0] := a[255:0]
//		CASE (imm8[1:0]) of
//		0: tmp[127:0] := b[127:0]
//		1: tmp[255:128] := b[127:0]
//		ESAC
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTI32X4'. Intrinsic: '_mm256_maskz_inserti32x4'.
// Requires AVX512F.
func MaskzInserti32x4(k Mmask8, a M256i, b M128i, imm8 int) M256i {
	return M256i(maskzInserti32x4(uint8(k), [32]byte(a), [16]byte(b), imm8))
}

func maskzInserti32x4(k uint8, a [32]byte, b [16]byte, imm8 int) [32]byte


// Inserti32x41: Copy 'a' to 'dst', then insert 128 bits (composed of 4 packed
// 32-bit integers) from 'b' into 'dst' at the location specified by 'imm8'. 
//
//		dst[511:0] := a[511:0]
//		CASE (imm8[1:0]) of
//		0: dst[127:0] := b[127:0]
//		1: dst[255:128] := b[127:0]
//		2: dst[383:256] := b[127:0]
//		3: dst[511:384] := b[127:0]
//		ESAC
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTI32X4'. Intrinsic: '_mm512_inserti32x4'.
// Requires AVX512F.
func Inserti32x41(a M512i, b M128i, imm8 int) M512i {
	return M512i(inserti32x41([64]byte(a), [16]byte(b), imm8))
}

func inserti32x41(a [64]byte, b [16]byte, imm8 int) [64]byte


// MaskInserti32x41: Copy 'a' to 'tmp', then insert 128 bits (composed of 4
// packed 32-bit integers) from 'b' into 'tmp' at the location specified by
// 'imm8'.  Store 'tmp' to 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		tmp[511:0] := a[511:0]
//		CASE (imm8[1:0]) of
//		0: tmp[127:0] := b[127:0]
//		1: tmp[255:128] := b[127:0]
//		2: tmp[383:256] := b[127:0]
//		3: tmp[511:384] := b[127:0]
//		ESAC
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTI32X4'. Intrinsic: '_mm512_mask_inserti32x4'.
// Requires AVX512F.
func MaskInserti32x41(src M512i, k Mmask16, a M512i, b M128i, imm8 int) M512i {
	return M512i(maskInserti32x41([64]byte(src), uint16(k), [64]byte(a), [16]byte(b), imm8))
}

func maskInserti32x41(src [64]byte, k uint16, a [64]byte, b [16]byte, imm8 int) [64]byte


// MaskzInserti32x41: Copy 'a' to 'tmp', then insert 128 bits (composed of 4
// packed 32-bit integers) from 'b' into 'tmp' at the location specified by
// 'imm8'.  Store 'tmp' to 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		tmp[511:0] := a[511:0]
//		CASE (imm8[1:0]) of
//		0: tmp[127:0] := b[127:0]
//		1: tmp[255:128] := b[127:0]
//		2: tmp[383:256] := b[127:0]
//		3: tmp[511:384] := b[127:0]
//		ESAC
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTI32X4'. Intrinsic: '_mm512_maskz_inserti32x4'.
// Requires AVX512F.
func MaskzInserti32x41(k Mmask16, a M512i, b M128i, imm8 int) M512i {
	return M512i(maskzInserti32x41(uint16(k), [64]byte(a), [16]byte(b), imm8))
}

func maskzInserti32x41(k uint16, a [64]byte, b [16]byte, imm8 int) [64]byte


// Inserti64x4: Copy 'a' to 'dst', then insert 256 bits (composed of 4 packed
// 64-bit integers) from 'b' into 'dst' at the location specified by 'imm8'. 
//
//		dst[511:0] := a[511:0]
//		CASE (imm8[7:0]) OF
//		0: dst[255:0] := b[255:0]
//		1: dst[511:256] := b[255:0]
//		ESAC
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTI64X4'. Intrinsic: '_mm512_inserti64x4'.
// Requires AVX512F.
func Inserti64x4(a M512i, b M256i, imm8 int) M512i {
	return M512i(inserti64x4([64]byte(a), [32]byte(b), imm8))
}

func inserti64x4(a [64]byte, b [32]byte, imm8 int) [64]byte


// MaskInserti64x4: Copy 'a' to 'tmp', then insert 256 bits (composed of 4
// packed 64-bit integers) from 'b' into 'tmp' at the location specified by
// 'imm8'.  Store 'tmp' to 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		tmp[511:0] := a[511:0]
//		CASE (imm8[0]) of
//		0: tmp[255:0] := b[255:0]
//		1: tmp[511:256] := b[255:0]
//		ESAC
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTI64X4'. Intrinsic: '_mm512_mask_inserti64x4'.
// Requires AVX512F.
func MaskInserti64x4(src M512i, k Mmask8, a M512i, b M256i, imm8 int) M512i {
	return M512i(maskInserti64x4([64]byte(src), uint8(k), [64]byte(a), [32]byte(b), imm8))
}

func maskInserti64x4(src [64]byte, k uint8, a [64]byte, b [32]byte, imm8 int) [64]byte


// MaskzInserti64x4: Copy 'a' to 'tmp', then insert 256 bits (composed of 4
// packed 64-bit integers) from 'b' into 'tmp' at the location specified by
// 'imm8'.  Store 'tmp' to 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		tmp[511:0] := a[511:0]
//		CASE (imm8[0]) of
//		0: tmp[255:0] := b[255:0]
//		1: tmp[511:256] := b[255:0]
//		ESAC
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VINSERTI64X4'. Intrinsic: '_mm512_maskz_inserti64x4'.
// Requires AVX512F.
func MaskzInserti64x4(k Mmask8, a M512i, b M256i, imm8 int) M512i {
	return M512i(maskzInserti64x4(uint8(k), [64]byte(a), [32]byte(b), imm8))
}

func maskzInserti64x4(k uint8, a [64]byte, b [32]byte, imm8 int) [64]byte


// InvsqrtPd: Compute the inverse square root of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := InvSQRT(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_invsqrt_pd'.
// Requires AVX512F.
func InvsqrtPd(a M512d) M512d {
	return M512d(invsqrtPd([8]float64(a)))
}

func invsqrtPd(a [8]float64) [8]float64


// MaskInvsqrtPd: Compute the inverse square root of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := InvSQRT(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_invsqrt_pd'.
// Requires AVX512F.
func MaskInvsqrtPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskInvsqrtPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskInvsqrtPd(src [8]float64, k uint8, a [8]float64) [8]float64


// InvsqrtPs: Compute the inverse square root of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := InvSQRT(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_invsqrt_ps'.
// Requires AVX512F.
func InvsqrtPs(a M512) M512 {
	return M512(invsqrtPs([16]float32(a)))
}

func invsqrtPs(a [16]float32) [16]float32


// MaskInvsqrtPs: Compute the inverse square root of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := InvSQRT(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_invsqrt_ps'.
// Requires AVX512F.
func MaskInvsqrtPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskInvsqrtPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskInvsqrtPs(src [16]float32, k uint16, a [16]float32) [16]float32


// Kand: Compute the bitwise AND of 16-bit masks 'a' and 'b', and store the
// result in 'k'. 
//
//		k[15:0] := a[15:0] AND b[15:0]
//		k[MAX:16] := 0
//
// Instruction: 'KANDW'. Intrinsic: '_mm512_kand'.
// Requires AVX512F.
func Kand(a Mmask16, b Mmask16) Mmask16 {
	return Mmask16(kand(uint16(a), uint16(b)))
}

func kand(a uint16, b uint16) uint16


// Kandn: Compute the bitwise AND NOT of 16-bit masks 'a' and 'b', and store
// the result in 'k'. 
//
//		k[15:0] := (NOT a[15:0]) AND b[15:0]
//		k[MAX:16] := 0
//
// Instruction: 'KANDNW'. Intrinsic: '_mm512_kandn'.
// Requires AVX512F.
func Kandn(a Mmask16, b Mmask16) Mmask16 {
	return Mmask16(kandn(uint16(a), uint16(b)))
}

func kandn(a uint16, b uint16) uint16


// Kmov: Copy 16-bit mask 'a' to 'k'. 
//
//		k[15:0] := a[15:0]
//		k[MAX:16] := 0
//
// Instruction: 'KMOVW'. Intrinsic: '_mm512_kmov'.
// Requires AVX512F.
func Kmov(a Mmask16) Mmask16 {
	return Mmask16(kmov(uint16(a)))
}

func kmov(a uint16) uint16


// Knot: Compute the bitwise NOT of 16-bit mask 'a', and store the result in
// 'k'. 
//
//		k[15:0] := NOT a[15:0]
//		k[MAX:16] := 0
//
// Instruction: 'KNOTW'. Intrinsic: '_mm512_knot'.
// Requires AVX512F.
func Knot(a Mmask16) Mmask16 {
	return Mmask16(knot(uint16(a)))
}

func knot(a uint16) uint16


// Kor: Compute the bitwise OR of 16-bit masks 'a' and 'b', and store the
// result in 'k'. 
//
//		k[15:0] := a[15:0] OR b[15:0]
//		k[MAX:16] := 0
//
// Instruction: 'KORW'. Intrinsic: '_mm512_kor'.
// Requires AVX512F.
func Kor(a Mmask16, b Mmask16) Mmask16 {
	return Mmask16(kor(uint16(a), uint16(b)))
}

func kor(a uint16, b uint16) uint16


// Kortestc: Performs bitwise OR between 'k1' and 'k2', storing the result in
// 'dst'. CF flag is set if 'dst' consists of all 1's. 
//
//		dst[15:0] := k1[15:0] | k2[15:0]
//		IF PopCount(dst[15:0]) = 16
//			SetCF()
//		FI
//
// Instruction: 'KORTESTW'. Intrinsic: '_mm512_kortestc'.
// Requires AVX512F.
func Kortestc(k1 Mmask16, k2 Mmask16) int {
	return int(kortestc(uint16(k1), uint16(k2)))
}

func kortestc(k1 uint16, k2 uint16) int


// Kortestz: Performs bitwise OR between 'k1' and 'k2', storing the result in
// 'dst'. ZF flag is set if 'dst' is 0. 
//
//		dst[15:0] := k1[15:0] | k2[15:0]
//		IF dst = 0
//			SetZF()
//		FI
//
// Instruction: 'KORTESTW'. Intrinsic: '_mm512_kortestz'.
// Requires AVX512F.
func Kortestz(k1 Mmask16, k2 Mmask16) int {
	return int(kortestz(uint16(k1), uint16(k2)))
}

func kortestz(k1 uint16, k2 uint16) int


// Kunpackb: Unpack and interleave 8 bits from masks 'a' and 'b', and store the
// 16-bit result in 'k'. 
//
//		k[7:0] := b[7:0]
//		k[15:8] := a[7:0]
//		k[MAX:16] := 0
//
// Instruction: 'KUNPCKBW'. Intrinsic: '_mm512_kunpackb'.
// Requires AVX512F.
func Kunpackb(a Mmask16, b Mmask16) Mmask16 {
	return Mmask16(kunpackb(uint16(a), uint16(b)))
}

func kunpackb(a uint16, b uint16) uint16


// Kxnor: Compute the bitwise XNOR of 16-bit masks 'a' and 'b', and store the
// result in 'k'. 
//
//		k[15:0] := NOT (a[15:0] XOR b[15:0])
//		k[MAX:16] := 0
//
// Instruction: 'KXNORW'. Intrinsic: '_mm512_kxnor'.
// Requires AVX512F.
func Kxnor(a Mmask16, b Mmask16) Mmask16 {
	return Mmask16(kxnor(uint16(a), uint16(b)))
}

func kxnor(a uint16, b uint16) uint16


// Kxor: Compute the bitwise XOR of 16-bit masks 'a' and 'b', and store the
// result in 'k'. 
//
//		k[15:0] := a[15:0] XOR b[15:0]
//		k[MAX:16] := 0
//
// Instruction: 'KXORW'. Intrinsic: '_mm512_kxor'.
// Requires AVX512F.
func Kxor(a Mmask16, b Mmask16) Mmask16 {
	return Mmask16(kxor(uint16(a), uint16(b)))
}

func kxor(a uint16, b uint16) uint16


// MaskLoadEpi32: Load packed 32-bit integers from memory into 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
// 	'mem_addr' must be aligned on a 16-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVDQA32'. Intrinsic: '_mm_mask_load_epi32'.
// Requires AVX512F.
func MaskLoadEpi32(src M128i, k Mmask8, mem_addr uintptr) M128i {
	return M128i(maskLoadEpi32([16]byte(src), uint8(k), uintptr(mem_addr)))
}

func maskLoadEpi32(src [16]byte, k uint8, mem_addr uintptr) [16]byte


// MaskzLoadEpi32: Load packed 32-bit integers from memory into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
// 	'mem_addr' must be aligned on a 16-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVDQA32'. Intrinsic: '_mm_maskz_load_epi32'.
// Requires AVX512F.
func MaskzLoadEpi32(k Mmask8, mem_addr uintptr) M128i {
	return M128i(maskzLoadEpi32(uint8(k), uintptr(mem_addr)))
}

func maskzLoadEpi32(k uint8, mem_addr uintptr) [16]byte


// MaskLoadEpi321: Load packed 32-bit integers from memory into 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
// 	'mem_addr' must be aligned on a 32-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQA32'. Intrinsic: '_mm256_mask_load_epi32'.
// Requires AVX512F.
func MaskLoadEpi321(src M256i, k Mmask8, mem_addr uintptr) M256i {
	return M256i(maskLoadEpi321([32]byte(src), uint8(k), uintptr(mem_addr)))
}

func maskLoadEpi321(src [32]byte, k uint8, mem_addr uintptr) [32]byte


// MaskzLoadEpi321: Load packed 32-bit integers from memory into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
// 	'mem_addr' must be aligned on a 32-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQA32'. Intrinsic: '_mm256_maskz_load_epi32'.
// Requires AVX512F.
func MaskzLoadEpi321(k Mmask8, mem_addr uintptr) M256i {
	return M256i(maskzLoadEpi321(uint8(k), uintptr(mem_addr)))
}

func maskzLoadEpi321(k uint8, mem_addr uintptr) [32]byte


// MaskzLoadEpi322: Load packed 32-bit integers from memory into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
// 	'mem_addr' must be aligned on a 64-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDQA32'. Intrinsic: '_mm512_maskz_load_epi32'.
// Requires AVX512F.
func MaskzLoadEpi322(k Mmask16, mem_addr uintptr) M512i {
	return M512i(maskzLoadEpi322(uint16(k), uintptr(mem_addr)))
}

func maskzLoadEpi322(k uint16, mem_addr uintptr) [64]byte


// MaskLoadEpi64: Load packed 64-bit integers from memory into 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
// 	'mem_addr' must be aligned on a 16-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVDQA64'. Intrinsic: '_mm_mask_load_epi64'.
// Requires AVX512F.
func MaskLoadEpi64(src M128i, k Mmask8, mem_addr uintptr) M128i {
	return M128i(maskLoadEpi64([16]byte(src), uint8(k), uintptr(mem_addr)))
}

func maskLoadEpi64(src [16]byte, k uint8, mem_addr uintptr) [16]byte


// MaskzLoadEpi64: Load packed 64-bit integers from memory into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
// 	'mem_addr' must be aligned on a 16-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVDQA64'. Intrinsic: '_mm_maskz_load_epi64'.
// Requires AVX512F.
func MaskzLoadEpi64(k Mmask8, mem_addr uintptr) M128i {
	return M128i(maskzLoadEpi64(uint8(k), uintptr(mem_addr)))
}

func maskzLoadEpi64(k uint8, mem_addr uintptr) [16]byte


// MaskLoadEpi641: Load packed 64-bit integers from memory into 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
// 	'mem_addr' must be aligned on a 32-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQA64'. Intrinsic: '_mm256_mask_load_epi64'.
// Requires AVX512F.
func MaskLoadEpi641(src M256i, k Mmask8, mem_addr uintptr) M256i {
	return M256i(maskLoadEpi641([32]byte(src), uint8(k), uintptr(mem_addr)))
}

func maskLoadEpi641(src [32]byte, k uint8, mem_addr uintptr) [32]byte


// MaskzLoadEpi641: Load packed 64-bit integers from memory into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
// 	'mem_addr' must be aligned on a 32-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQA64'. Intrinsic: '_mm256_maskz_load_epi64'.
// Requires AVX512F.
func MaskzLoadEpi641(k Mmask8, mem_addr uintptr) M256i {
	return M256i(maskzLoadEpi641(uint8(k), uintptr(mem_addr)))
}

func maskzLoadEpi641(k uint8, mem_addr uintptr) [32]byte


// MaskzLoadEpi642: Load packed 64-bit integers from memory into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
// 	'mem_addr' must be aligned on a 64-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDQA64'. Intrinsic: '_mm512_maskz_load_epi64'.
// Requires AVX512F.
func MaskzLoadEpi642(k Mmask8, mem_addr uintptr) M512i {
	return M512i(maskzLoadEpi642(uint8(k), uintptr(mem_addr)))
}

func maskzLoadEpi642(k uint8, mem_addr uintptr) [64]byte


// MaskLoadPd: Load packed double-precision (64-bit) floating-point elements
// from memory into 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 'mem_addr' must be aligned on a
// 16-byte boundary or a general-protection exception may be generated. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVAPD'. Intrinsic: '_mm_mask_load_pd'.
// Requires AVX512F.
func MaskLoadPd(src M128d, k Mmask8, mem_addr uintptr) M128d {
	return M128d(maskLoadPd([2]float64(src), uint8(k), uintptr(mem_addr)))
}

func maskLoadPd(src [2]float64, k uint8, mem_addr uintptr) [2]float64


// MaskzLoadPd: Load packed double-precision (64-bit) floating-point elements
// from memory into 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 'mem_addr' must be aligned on a 16-byte
// boundary or a general-protection exception may be generated. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVAPD'. Intrinsic: '_mm_maskz_load_pd'.
// Requires AVX512F.
func MaskzLoadPd(k Mmask8, mem_addr uintptr) M128d {
	return M128d(maskzLoadPd(uint8(k), uintptr(mem_addr)))
}

func maskzLoadPd(k uint8, mem_addr uintptr) [2]float64


// MaskLoadPd1: Load packed double-precision (64-bit) floating-point elements
// from memory into 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 'mem_addr' must be aligned on a
// 32-byte boundary or a general-protection exception may be generated. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVAPD'. Intrinsic: '_mm256_mask_load_pd'.
// Requires AVX512F.
func MaskLoadPd1(src M256d, k Mmask8, mem_addr uintptr) M256d {
	return M256d(maskLoadPd1([4]float64(src), uint8(k), uintptr(mem_addr)))
}

func maskLoadPd1(src [4]float64, k uint8, mem_addr uintptr) [4]float64


// MaskzLoadPd1: Load packed double-precision (64-bit) floating-point elements
// from memory into 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 'mem_addr' must be aligned on a 32-byte
// boundary or a general-protection exception may be generated. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVAPD'. Intrinsic: '_mm256_maskz_load_pd'.
// Requires AVX512F.
func MaskzLoadPd1(k Mmask8, mem_addr uintptr) M256d {
	return M256d(maskzLoadPd1(uint8(k), uintptr(mem_addr)))
}

func maskzLoadPd1(k uint8, mem_addr uintptr) [4]float64


// MaskzLoadPd2: Load packed double-precision (64-bit) floating-point elements
// from memory into 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 'mem_addr' must be aligned on a 64-byte
// boundary or a general-protection exception may be generated. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVAPD'. Intrinsic: '_mm512_maskz_load_pd'.
// Requires AVX512F.
func MaskzLoadPd2(k Mmask8, mem_addr uintptr) M512d {
	return M512d(maskzLoadPd2(uint8(k), uintptr(mem_addr)))
}

func maskzLoadPd2(k uint8, mem_addr uintptr) [8]float64


// MaskLoadPs: Load packed single-precision (32-bit) floating-point elements
// from memory into 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 'mem_addr' must be aligned on a
// 16-byte boundary or a general-protection exception may be generated. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVAPS'. Intrinsic: '_mm_mask_load_ps'.
// Requires AVX512F.
func MaskLoadPs(src M128, k Mmask8, mem_addr uintptr) M128 {
	return M128(maskLoadPs([4]float32(src), uint8(k), uintptr(mem_addr)))
}

func maskLoadPs(src [4]float32, k uint8, mem_addr uintptr) [4]float32


// MaskzLoadPs: Load packed single-precision (32-bit) floating-point elements
// from memory into 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 'mem_addr' must be aligned on a 16-byte
// boundary or a general-protection exception may be generated. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVAPS'. Intrinsic: '_mm_maskz_load_ps'.
// Requires AVX512F.
func MaskzLoadPs(k Mmask8, mem_addr uintptr) M128 {
	return M128(maskzLoadPs(uint8(k), uintptr(mem_addr)))
}

func maskzLoadPs(k uint8, mem_addr uintptr) [4]float32


// MaskLoadPs1: Load packed single-precision (32-bit) floating-point elements
// from memory into 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 'mem_addr' must be aligned on a
// 32-byte boundary or a general-protection exception may be generated. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVAPS'. Intrinsic: '_mm256_mask_load_ps'.
// Requires AVX512F.
func MaskLoadPs1(src M256, k Mmask8, mem_addr uintptr) M256 {
	return M256(maskLoadPs1([8]float32(src), uint8(k), uintptr(mem_addr)))
}

func maskLoadPs1(src [8]float32, k uint8, mem_addr uintptr) [8]float32


// MaskzLoadPs1: Load packed single-precision (32-bit) floating-point elements
// from memory into 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 'mem_addr' must be aligned on a 32-byte
// boundary or a general-protection exception may be generated. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVAPS'. Intrinsic: '_mm256_maskz_load_ps'.
// Requires AVX512F.
func MaskzLoadPs1(k Mmask8, mem_addr uintptr) M256 {
	return M256(maskzLoadPs1(uint8(k), uintptr(mem_addr)))
}

func maskzLoadPs1(k uint8, mem_addr uintptr) [8]float32


// MaskzLoadPs2: Load packed single-precision (32-bit) floating-point elements
// from memory into 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 'mem_addr' must be aligned on a 64-byte
// boundary or a general-protection exception may be generated. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVAPS'. Intrinsic: '_mm512_maskz_load_ps'.
// Requires AVX512F.
func MaskzLoadPs2(k Mmask16, mem_addr uintptr) M512 {
	return M512(maskzLoadPs2(uint16(k), uintptr(mem_addr)))
}

func maskzLoadPs2(k uint16, mem_addr uintptr) [16]float32


// MaskLoadSd: Load a double-precision (64-bit) floating-point element from
// memory into the lower element of 'dst' using writemask 'k' (the element is
// copied from 'src' when mask bit 0 is not set), and set the upper element of
// 'dst' to zero. 'mem_addr' must be aligned on a 16-byte boundary or a
// general-protection exception may be generated. 
//
//		IF k[0]
//			dst[63:0] := MEM[mem_addr+63:mem_addr]
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[MAX:64] := 0
//
// Instruction: 'VMOVSD'. Intrinsic: '_mm_mask_load_sd'.
// Requires AVX512F.
func MaskLoadSd(src M128d, k Mmask8, mem_addr float64) M128d {
	return M128d(maskLoadSd([2]float64(src), uint8(k), mem_addr))
}

func maskLoadSd(src [2]float64, k uint8, mem_addr float64) [2]float64


// MaskzLoadSd: Load a double-precision (64-bit) floating-point element from
// memory into the lower element of 'dst' using zeromask 'k' (the element is
// zeroed out when mask bit 0 is not set), and set the upper element of 'dst'
// to zero. 'mem_addr' must be aligned on a 16-byte boundary or a
// general-protection exception may be generated. 
//
//		IF k[0]
//			dst[63:0] := MEM[mem_addr+63:mem_addr]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[MAX:64] := 0
//
// Instruction: 'VMOVSD'. Intrinsic: '_mm_maskz_load_sd'.
// Requires AVX512F.
func MaskzLoadSd(k Mmask8, mem_addr float64) M128d {
	return M128d(maskzLoadSd(uint8(k), mem_addr))
}

func maskzLoadSd(k uint8, mem_addr float64) [2]float64


// MaskLoadSs: Load a single-precision (32-bit) floating-point element from
// memory into the lower element of 'dst' using writemask 'k' (the element is
// copied from 'src' when mask bit 0 is not set), and set the upper elements of
// 'dst' to zero. 'mem_addr' must be aligned on a 16-byte boundary or a
// general-protection exception may be generated. 
//
//		IF k[0]
//			dst[31:0] := MEM[mem_addr+31:mem_addr]
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[MAX:32] := 0
//
// Instruction: 'VMOVSS'. Intrinsic: '_mm_mask_load_ss'.
// Requires AVX512F.
func MaskLoadSs(src M128, k Mmask8, mem_addr float32) M128 {
	return M128(maskLoadSs([4]float32(src), uint8(k), mem_addr))
}

func maskLoadSs(src [4]float32, k uint8, mem_addr float32) [4]float32


// MaskzLoadSs: Load a single-precision (32-bit) floating-point element from
// memory into the lower element of 'dst' using zeromask 'k' (the element is
// zeroed out when mask bit 0 is not set), and set the upper elements of 'dst'
// to zero. 'mem_addr' must be aligned on a 16-byte boundary or a
// general-protection exception may be generated. 
//
//		IF k[0]
//			dst[31:0] := MEM[mem_addr+31:mem_addr]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[MAX:32] := 0
//
// Instruction: 'VMOVSS'. Intrinsic: '_mm_maskz_load_ss'.
// Requires AVX512F.
func MaskzLoadSs(k Mmask8, mem_addr float32) M128 {
	return M128(maskzLoadSs(uint8(k), mem_addr))
}

func maskzLoadSs(k uint8, mem_addr float32) [4]float32


// MaskLoaduEpi32: Load packed 32-bit integers from memory into 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVDQU32'. Intrinsic: '_mm_mask_loadu_epi32'.
// Requires AVX512F.
func MaskLoaduEpi32(src M128i, k Mmask8, mem_addr uintptr) M128i {
	return M128i(maskLoaduEpi32([16]byte(src), uint8(k), uintptr(mem_addr)))
}

func maskLoaduEpi32(src [16]byte, k uint8, mem_addr uintptr) [16]byte


// MaskzLoaduEpi32: Load packed 32-bit integers from memory into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVDQU32'. Intrinsic: '_mm_maskz_loadu_epi32'.
// Requires AVX512F.
func MaskzLoaduEpi32(k Mmask8, mem_addr uintptr) M128i {
	return M128i(maskzLoaduEpi32(uint8(k), uintptr(mem_addr)))
}

func maskzLoaduEpi32(k uint8, mem_addr uintptr) [16]byte


// MaskLoaduEpi321: Load packed 32-bit integers from memory into 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQU32'. Intrinsic: '_mm256_mask_loadu_epi32'.
// Requires AVX512F.
func MaskLoaduEpi321(src M256i, k Mmask8, mem_addr uintptr) M256i {
	return M256i(maskLoaduEpi321([32]byte(src), uint8(k), uintptr(mem_addr)))
}

func maskLoaduEpi321(src [32]byte, k uint8, mem_addr uintptr) [32]byte


// MaskzLoaduEpi321: Load packed 32-bit integers from memory into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQU32'. Intrinsic: '_mm256_maskz_loadu_epi32'.
// Requires AVX512F.
func MaskzLoaduEpi321(k Mmask8, mem_addr uintptr) M256i {
	return M256i(maskzLoaduEpi321(uint8(k), uintptr(mem_addr)))
}

func maskzLoaduEpi321(k uint8, mem_addr uintptr) [32]byte


// MaskLoaduEpi322: Load packed 32-bit integers from memory into 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDQU32'. Intrinsic: '_mm512_mask_loadu_epi32'.
// Requires AVX512F.
func MaskLoaduEpi322(src M512i, k Mmask16, mem_addr uintptr) M512i {
	return M512i(maskLoaduEpi322([64]byte(src), uint16(k), uintptr(mem_addr)))
}

func maskLoaduEpi322(src [64]byte, k uint16, mem_addr uintptr) [64]byte


// MaskzLoaduEpi322: Load packed 32-bit integers from memory into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDQU32'. Intrinsic: '_mm512_maskz_loadu_epi32'.
// Requires AVX512F.
func MaskzLoaduEpi322(k Mmask16, mem_addr uintptr) M512i {
	return M512i(maskzLoaduEpi322(uint16(k), uintptr(mem_addr)))
}

func maskzLoaduEpi322(k uint16, mem_addr uintptr) [64]byte


// MaskLoaduEpi64: Load packed 64-bit integers from memory into 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVDQU64'. Intrinsic: '_mm_mask_loadu_epi64'.
// Requires AVX512F.
func MaskLoaduEpi64(src M128i, k Mmask8, mem_addr uintptr) M128i {
	return M128i(maskLoaduEpi64([16]byte(src), uint8(k), uintptr(mem_addr)))
}

func maskLoaduEpi64(src [16]byte, k uint8, mem_addr uintptr) [16]byte


// MaskzLoaduEpi64: Load packed 64-bit integers from memory into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVDQU64'. Intrinsic: '_mm_maskz_loadu_epi64'.
// Requires AVX512F.
func MaskzLoaduEpi64(k Mmask8, mem_addr uintptr) M128i {
	return M128i(maskzLoaduEpi64(uint8(k), uintptr(mem_addr)))
}

func maskzLoaduEpi64(k uint8, mem_addr uintptr) [16]byte


// MaskLoaduEpi641: Load packed 64-bit integers from memory into 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQU64'. Intrinsic: '_mm256_mask_loadu_epi64'.
// Requires AVX512F.
func MaskLoaduEpi641(src M256i, k Mmask8, mem_addr uintptr) M256i {
	return M256i(maskLoaduEpi641([32]byte(src), uint8(k), uintptr(mem_addr)))
}

func maskLoaduEpi641(src [32]byte, k uint8, mem_addr uintptr) [32]byte


// MaskzLoaduEpi641: Load packed 64-bit integers from memory into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQU64'. Intrinsic: '_mm256_maskz_loadu_epi64'.
// Requires AVX512F.
func MaskzLoaduEpi641(k Mmask8, mem_addr uintptr) M256i {
	return M256i(maskzLoaduEpi641(uint8(k), uintptr(mem_addr)))
}

func maskzLoaduEpi641(k uint8, mem_addr uintptr) [32]byte


// MaskLoaduEpi642: Load packed 64-bit integers from memory into 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDQU64'. Intrinsic: '_mm512_mask_loadu_epi64'.
// Requires AVX512F.
func MaskLoaduEpi642(src M512i, k Mmask8, mem_addr uintptr) M512i {
	return M512i(maskLoaduEpi642([64]byte(src), uint8(k), uintptr(mem_addr)))
}

func maskLoaduEpi642(src [64]byte, k uint8, mem_addr uintptr) [64]byte


// MaskzLoaduEpi642: Load packed 64-bit integers from memory into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDQU64'. Intrinsic: '_mm512_maskz_loadu_epi64'.
// Requires AVX512F.
func MaskzLoaduEpi642(k Mmask8, mem_addr uintptr) M512i {
	return M512i(maskzLoaduEpi642(uint8(k), uintptr(mem_addr)))
}

func maskzLoaduEpi642(k uint8, mem_addr uintptr) [64]byte


// MaskLoaduPd: Load packed double-precision (64-bit) floating-point elements
// from memoy into 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVUPD'. Intrinsic: '_mm_mask_loadu_pd'.
// Requires AVX512F.
func MaskLoaduPd(src M128d, k Mmask8, mem_addr uintptr) M128d {
	return M128d(maskLoaduPd([2]float64(src), uint8(k), uintptr(mem_addr)))
}

func maskLoaduPd(src [2]float64, k uint8, mem_addr uintptr) [2]float64


// MaskzLoaduPd: Load packed double-precision (64-bit) floating-point elements
// from memoy into 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVUPD'. Intrinsic: '_mm_maskz_loadu_pd'.
// Requires AVX512F.
func MaskzLoaduPd(k Mmask8, mem_addr uintptr) M128d {
	return M128d(maskzLoaduPd(uint8(k), uintptr(mem_addr)))
}

func maskzLoaduPd(k uint8, mem_addr uintptr) [2]float64


// MaskLoaduPd1: Load packed double-precision (64-bit) floating-point elements
// from memoy into 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVUPD'. Intrinsic: '_mm256_mask_loadu_pd'.
// Requires AVX512F.
func MaskLoaduPd1(src M256d, k Mmask8, mem_addr uintptr) M256d {
	return M256d(maskLoaduPd1([4]float64(src), uint8(k), uintptr(mem_addr)))
}

func maskLoaduPd1(src [4]float64, k uint8, mem_addr uintptr) [4]float64


// MaskzLoaduPd1: Load packed double-precision (64-bit) floating-point elements
// from memoy into 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVUPD'. Intrinsic: '_mm256_maskz_loadu_pd'.
// Requires AVX512F.
func MaskzLoaduPd1(k Mmask8, mem_addr uintptr) M256d {
	return M256d(maskzLoaduPd1(uint8(k), uintptr(mem_addr)))
}

func maskzLoaduPd1(k uint8, mem_addr uintptr) [4]float64


// LoaduPd: Load 512-bits (composed of 8 packed double-precision (64-bit)
// floating-point elements) from memory into 'dst'. 
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		dst[511:0] := MEM[mem_addr+511:mem_addr]
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVUPD'. Intrinsic: '_mm512_loadu_pd'.
// Requires AVX512F.
func LoaduPd(mem_addr uintptr) M512d {
	return M512d(loaduPd(uintptr(mem_addr)))
}

func loaduPd(mem_addr uintptr) [8]float64


// MaskLoaduPd2: Load packed double-precision (64-bit) floating-point elements
// from memoy into 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVUPD'. Intrinsic: '_mm512_mask_loadu_pd'.
// Requires AVX512F.
func MaskLoaduPd2(src M512d, k Mmask8, mem_addr uintptr) M512d {
	return M512d(maskLoaduPd2([8]float64(src), uint8(k), uintptr(mem_addr)))
}

func maskLoaduPd2(src [8]float64, k uint8, mem_addr uintptr) [8]float64


// MaskzLoaduPd2: Load packed double-precision (64-bit) floating-point elements
// from memoy into 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVUPD'. Intrinsic: '_mm512_maskz_loadu_pd'.
// Requires AVX512F.
func MaskzLoaduPd2(k Mmask8, mem_addr uintptr) M512d {
	return M512d(maskzLoaduPd2(uint8(k), uintptr(mem_addr)))
}

func maskzLoaduPd2(k uint8, mem_addr uintptr) [8]float64


// MaskLoaduPs: Load packed single-precision (32-bit) floating-point elements
// from memory into 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVUPS'. Intrinsic: '_mm_mask_loadu_ps'.
// Requires AVX512F.
func MaskLoaduPs(src M128, k Mmask8, mem_addr uintptr) M128 {
	return M128(maskLoaduPs([4]float32(src), uint8(k), uintptr(mem_addr)))
}

func maskLoaduPs(src [4]float32, k uint8, mem_addr uintptr) [4]float32


// MaskzLoaduPs: Load packed single-precision (32-bit) floating-point elements
// from memory into 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVUPS'. Intrinsic: '_mm_maskz_loadu_ps'.
// Requires AVX512F.
func MaskzLoaduPs(k Mmask8, mem_addr uintptr) M128 {
	return M128(maskzLoaduPs(uint8(k), uintptr(mem_addr)))
}

func maskzLoaduPs(k uint8, mem_addr uintptr) [4]float32


// MaskLoaduPs1: Load packed single-precision (32-bit) floating-point elements
// from memory into 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVUPS'. Intrinsic: '_mm256_mask_loadu_ps'.
// Requires AVX512F.
func MaskLoaduPs1(src M256, k Mmask8, mem_addr uintptr) M256 {
	return M256(maskLoaduPs1([8]float32(src), uint8(k), uintptr(mem_addr)))
}

func maskLoaduPs1(src [8]float32, k uint8, mem_addr uintptr) [8]float32


// MaskzLoaduPs1: Load packed single-precision (32-bit) floating-point elements
// from memory into 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVUPS'. Intrinsic: '_mm256_maskz_loadu_ps'.
// Requires AVX512F.
func MaskzLoaduPs1(k Mmask8, mem_addr uintptr) M256 {
	return M256(maskzLoaduPs1(uint8(k), uintptr(mem_addr)))
}

func maskzLoaduPs1(k uint8, mem_addr uintptr) [8]float32


// LoaduPs: Load 512-bits (composed of 16 packed single-precision (32-bit)
// floating-point elements) from memory into 'dst'. 
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		dst[511:0] := MEM[mem_addr+511:mem_addr]
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVUPS'. Intrinsic: '_mm512_loadu_ps'.
// Requires AVX512F.
func LoaduPs(mem_addr uintptr) M512 {
	return M512(loaduPs(uintptr(mem_addr)))
}

func loaduPs(mem_addr uintptr) [16]float32


// MaskLoaduPs2: Load packed single-precision (32-bit) floating-point elements
// from memory into 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVUPS'. Intrinsic: '_mm512_mask_loadu_ps'.
// Requires AVX512F.
func MaskLoaduPs2(src M512, k Mmask16, mem_addr uintptr) M512 {
	return M512(maskLoaduPs2([16]float32(src), uint16(k), uintptr(mem_addr)))
}

func maskLoaduPs2(src [16]float32, k uint16, mem_addr uintptr) [16]float32


// MaskzLoaduPs2: Load packed single-precision (32-bit) floating-point elements
// from memory into 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVUPS'. Intrinsic: '_mm512_maskz_loadu_ps'.
// Requires AVX512F.
func MaskzLoaduPs2(k Mmask16, mem_addr uintptr) M512 {
	return M512(maskzLoaduPs2(uint16(k), uintptr(mem_addr)))
}

func maskzLoaduPs2(k uint16, mem_addr uintptr) [16]float32


// LoaduSi512: Load 512-bits of integer data from memory into 'dst'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		dst[511:0] := MEM[mem_addr+511:mem_addr]
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDQU32'. Intrinsic: '_mm512_loadu_si512'.
// Requires AVX512F.
func LoaduSi512(mem_addr uintptr) M512i {
	return M512i(loaduSi512(uintptr(mem_addr)))
}

func loaduSi512(mem_addr uintptr) [64]byte


// LogPd: Compute the natural logarithm of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ln(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_log_pd'.
// Requires AVX512F.
func LogPd(a M512d) M512d {
	return M512d(logPd([8]float64(a)))
}

func logPd(a [8]float64) [8]float64


// MaskLogPd: Compute the natural logarithm of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ln(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_log_pd'.
// Requires AVX512F.
func MaskLogPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskLogPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskLogPd(src [8]float64, k uint8, a [8]float64) [8]float64


// LogPs: Compute the natural logarithm of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ln(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_log_ps'.
// Requires AVX512F.
func LogPs(a M512) M512 {
	return M512(logPs([16]float32(a)))
}

func logPs(a [16]float32) [16]float32


// MaskLogPs: Compute the natural logarithm of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ln(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_log_ps'.
// Requires AVX512F.
func MaskLogPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskLogPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskLogPs(src [16]float32, k uint16, a [16]float32) [16]float32


// Log10Pd: Compute the base-10 logarithm of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := log10(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_log10_pd'.
// Requires AVX512F.
func Log10Pd(a M512d) M512d {
	return M512d(log10Pd([8]float64(a)))
}

func log10Pd(a [8]float64) [8]float64


// MaskLog10Pd: Compute the base-10 logarithm of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := log10(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_log10_pd'.
// Requires AVX512F.
func MaskLog10Pd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskLog10Pd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskLog10Pd(src [8]float64, k uint8, a [8]float64) [8]float64


// Log10Ps: Compute the base-10 logarithm of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := log10(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_log10_ps'.
// Requires AVX512F.
func Log10Ps(a M512) M512 {
	return M512(log10Ps([16]float32(a)))
}

func log10Ps(a [16]float32) [16]float32


// MaskLog10Ps: Compute the base-10 logarithm of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := log10(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_log10_ps'.
// Requires AVX512F.
func MaskLog10Ps(src M512, k Mmask16, a M512) M512 {
	return M512(maskLog10Ps([16]float32(src), uint16(k), [16]float32(a)))
}

func maskLog10Ps(src [16]float32, k uint16, a [16]float32) [16]float32


// Log1pPd: Compute the natural logarithm of one plus packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ln(1.0 + a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_log1p_pd'.
// Requires AVX512F.
func Log1pPd(a M512d) M512d {
	return M512d(log1pPd([8]float64(a)))
}

func log1pPd(a [8]float64) [8]float64


// MaskLog1pPd: Compute the natural logarithm of one plus packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ln(1.0 + a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_log1p_pd'.
// Requires AVX512F.
func MaskLog1pPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskLog1pPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskLog1pPd(src [8]float64, k uint8, a [8]float64) [8]float64


// Log1pPs: Compute the natural logarithm of one plus packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ln(1.0 + a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_log1p_ps'.
// Requires AVX512F.
func Log1pPs(a M512) M512 {
	return M512(log1pPs([16]float32(a)))
}

func log1pPs(a [16]float32) [16]float32


// MaskLog1pPs: Compute the natural logarithm of one plus packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ln(1.0 + a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_log1p_ps'.
// Requires AVX512F.
func MaskLog1pPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskLog1pPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskLog1pPs(src [16]float32, k uint16, a [16]float32) [16]float32


// Log2Pd: Compute the base-2 logarithm of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := log2(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_log2_pd'.
// Requires AVX512F.
func Log2Pd(a M512d) M512d {
	return M512d(log2Pd([8]float64(a)))
}

func log2Pd(a [8]float64) [8]float64


// MaskLog2Pd: Compute the base-2 logarithm of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := log2(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_log2_pd'.
// Requires AVX512F.
func MaskLog2Pd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskLog2Pd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskLog2Pd(src [8]float64, k uint8, a [8]float64) [8]float64


// LogbPd: Convert the exponent of each packed double-precision (64-bit)
// floating-point element in 'a' to a double-precision floating-point number
// representing the integer exponent, and store the results in 'dst'. This
// intrinsic essentially calculates 'floor(log2(x))' for each element. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ConvertExpFP64(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_logb_pd'.
// Requires AVX512F.
func LogbPd(a M512d) M512d {
	return M512d(logbPd([8]float64(a)))
}

func logbPd(a [8]float64) [8]float64


// MaskLogbPd: Convert the exponent of each packed double-precision (64-bit)
// floating-point element in 'a' to a double-precision floating-point number
// representing the integer exponent, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). This intrinsic essentially calculates 'floor(log2(x))' for
// each element. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ConvertExpFP64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_logb_pd'.
// Requires AVX512F.
func MaskLogbPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskLogbPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskLogbPd(src [8]float64, k uint8, a [8]float64) [8]float64


// LogbPs: Convert the exponent of each packed single-precision (32-bit)
// floating-point element in 'a' to a single-precision floating-point number
// representing the integer exponent, and store the results in 'dst'. This
// intrinsic essentially calculates 'floor(log2(x))' for each element. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := ConvertExpFP32(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_logb_ps'.
// Requires AVX512F.
func LogbPs(a M512) M512 {
	return M512(logbPs([16]float32(a)))
}

func logbPs(a [16]float32) [16]float32


// MaskLogbPs: Convert the exponent of each packed single-precision (32-bit)
// floating-point element in 'a' to a single-precision floating-point number
// representing the integer exponent, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). This intrinsic essentially calculates 'floor(log2(x))' for
// each element. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ConvertExpFP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_logb_ps'.
// Requires AVX512F.
func MaskLogbPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskLogbPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskLogbPs(src [16]float32, k uint16, a [16]float32) [16]float32


// MaskMaxEpi32: Compare packed 32-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] > b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMAXSD'. Intrinsic: '_mm_mask_max_epi32'.
// Requires AVX512F.
func MaskMaxEpi32(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskMaxEpi32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskMaxEpi32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzMaxEpi32: Compare packed 32-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] > b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMAXSD'. Intrinsic: '_mm_maskz_max_epi32'.
// Requires AVX512F.
func MaskzMaxEpi32(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzMaxEpi32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzMaxEpi32(k uint8, a [16]byte, b [16]byte) [16]byte


// MaskMaxEpi321: Compare packed 32-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] > b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXSD'. Intrinsic: '_mm256_mask_max_epi32'.
// Requires AVX512F.
func MaskMaxEpi321(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskMaxEpi321([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskMaxEpi321(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzMaxEpi321: Compare packed 32-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] > b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXSD'. Intrinsic: '_mm256_maskz_max_epi32'.
// Requires AVX512F.
func MaskzMaxEpi321(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzMaxEpi321(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzMaxEpi321(k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzMaxEpi322: Compare packed 32-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] > b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0 
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMAXSD'. Intrinsic: '_mm512_maskz_max_epi32'.
// Requires AVX512F.
func MaskzMaxEpi322(k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskzMaxEpi322(uint16(k), [64]byte(a), [64]byte(b)))
}

func maskzMaxEpi322(k uint16, a [64]byte, b [64]byte) [64]byte


// MaskMaxEpi64: Compare packed 64-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] > b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMAXSQ'. Intrinsic: '_mm_mask_max_epi64'.
// Requires AVX512F.
func MaskMaxEpi64(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskMaxEpi64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskMaxEpi64(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzMaxEpi64: Compare packed 64-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] > b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMAXSQ'. Intrinsic: '_mm_maskz_max_epi64'.
// Requires AVX512F.
func MaskzMaxEpi64(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzMaxEpi64(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzMaxEpi64(k uint8, a [16]byte, b [16]byte) [16]byte


// MaxEpi64: Compare packed 64-bit integers in 'a' and 'b', and store packed
// maximum values in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF a[i+63:i] > b[i+63:i]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := b[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMAXSQ'. Intrinsic: '_mm_max_epi64'.
// Requires AVX512F.
func MaxEpi64(a M128i, b M128i) M128i {
	return M128i(maxEpi64([16]byte(a), [16]byte(b)))
}

func maxEpi64(a [16]byte, b [16]byte) [16]byte


// MaskMaxEpi641: Compare packed 64-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] > b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXSQ'. Intrinsic: '_mm256_mask_max_epi64'.
// Requires AVX512F.
func MaskMaxEpi641(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskMaxEpi641([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskMaxEpi641(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzMaxEpi641: Compare packed 64-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] > b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXSQ'. Intrinsic: '_mm256_maskz_max_epi64'.
// Requires AVX512F.
func MaskzMaxEpi641(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzMaxEpi641(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzMaxEpi641(k uint8, a [32]byte, b [32]byte) [32]byte


// MaxEpi641: Compare packed 64-bit integers in 'a' and 'b', and store packed
// maximum values in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF a[i+63:i] > b[i+63:i]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := b[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXSQ'. Intrinsic: '_mm256_max_epi64'.
// Requires AVX512F.
func MaxEpi641(a M256i, b M256i) M256i {
	return M256i(maxEpi641([32]byte(a), [32]byte(b)))
}

func maxEpi641(a [32]byte, b [32]byte) [32]byte


// MaskMaxEpi642: Compare packed 64-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] > b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMAXSQ'. Intrinsic: '_mm512_mask_max_epi64'.
// Requires AVX512F.
func MaskMaxEpi642(src M512i, k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskMaxEpi642([64]byte(src), uint8(k), [64]byte(a), [64]byte(b)))
}

func maskMaxEpi642(src [64]byte, k uint8, a [64]byte, b [64]byte) [64]byte


// MaskzMaxEpi642: Compare packed 64-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] > b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMAXSQ'. Intrinsic: '_mm512_maskz_max_epi64'.
// Requires AVX512F.
func MaskzMaxEpi642(k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskzMaxEpi642(uint8(k), [64]byte(a), [64]byte(b)))
}

func maskzMaxEpi642(k uint8, a [64]byte, b [64]byte) [64]byte


// MaxEpi642: Compare packed 64-bit integers in 'a' and 'b', and store packed
// maximum values in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF a[i+63:i] > b[i+63:i]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := b[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMAXSQ'. Intrinsic: '_mm512_max_epi64'.
// Requires AVX512F.
func MaxEpi642(a M512i, b M512i) M512i {
	return M512i(maxEpi642([64]byte(a), [64]byte(b)))
}

func maxEpi642(a [64]byte, b [64]byte) [64]byte


// MaskMaxEpu32: Compare packed unsigned 32-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] > b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMAXUD'. Intrinsic: '_mm_mask_max_epu32'.
// Requires AVX512F.
func MaskMaxEpu32(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskMaxEpu32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskMaxEpu32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzMaxEpu32: Compare packed unsigned 32-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] > b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMAXUD'. Intrinsic: '_mm_maskz_max_epu32'.
// Requires AVX512F.
func MaskzMaxEpu32(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzMaxEpu32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzMaxEpu32(k uint8, a [16]byte, b [16]byte) [16]byte


// MaskMaxEpu321: Compare packed unsigned 32-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] > b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXUD'. Intrinsic: '_mm256_mask_max_epu32'.
// Requires AVX512F.
func MaskMaxEpu321(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskMaxEpu321([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskMaxEpu321(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzMaxEpu321: Compare packed unsigned 32-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] > b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXUD'. Intrinsic: '_mm256_maskz_max_epu32'.
// Requires AVX512F.
func MaskzMaxEpu321(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzMaxEpu321(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzMaxEpu321(k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzMaxEpu322: Compare packed unsigned 32-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] > b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMAXUD'. Intrinsic: '_mm512_maskz_max_epu32'.
// Requires AVX512F.
func MaskzMaxEpu322(k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskzMaxEpu322(uint16(k), [64]byte(a), [64]byte(b)))
}

func maskzMaxEpu322(k uint16, a [64]byte, b [64]byte) [64]byte


// MaskMaxEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] > b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMAXUQ'. Intrinsic: '_mm_mask_max_epu64'.
// Requires AVX512F.
func MaskMaxEpu64(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskMaxEpu64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskMaxEpu64(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzMaxEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] > b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMAXUQ'. Intrinsic: '_mm_maskz_max_epu64'.
// Requires AVX512F.
func MaskzMaxEpu64(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzMaxEpu64(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzMaxEpu64(k uint8, a [16]byte, b [16]byte) [16]byte


// MaxEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF a[i+63:i] > b[i+63:i]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := b[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMAXUQ'. Intrinsic: '_mm_max_epu64'.
// Requires AVX512F.
func MaxEpu64(a M128i, b M128i) M128i {
	return M128i(maxEpu64([16]byte(a), [16]byte(b)))
}

func maxEpu64(a [16]byte, b [16]byte) [16]byte


// MaskMaxEpu641: Compare packed unsigned 64-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] > b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXUQ'. Intrinsic: '_mm256_mask_max_epu64'.
// Requires AVX512F.
func MaskMaxEpu641(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskMaxEpu641([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskMaxEpu641(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzMaxEpu641: Compare packed unsigned 64-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] > b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXUQ'. Intrinsic: '_mm256_maskz_max_epu64'.
// Requires AVX512F.
func MaskzMaxEpu641(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzMaxEpu641(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzMaxEpu641(k uint8, a [32]byte, b [32]byte) [32]byte


// MaxEpu641: Compare packed unsigned 64-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF a[i+63:i] > b[i+63:i]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := b[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXUQ'. Intrinsic: '_mm256_max_epu64'.
// Requires AVX512F.
func MaxEpu641(a M256i, b M256i) M256i {
	return M256i(maxEpu641([32]byte(a), [32]byte(b)))
}

func maxEpu641(a [32]byte, b [32]byte) [32]byte


// MaskMaxEpu642: Compare packed unsigned 64-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] > b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMAXUQ'. Intrinsic: '_mm512_mask_max_epu64'.
// Requires AVX512F.
func MaskMaxEpu642(src M512i, k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskMaxEpu642([64]byte(src), uint8(k), [64]byte(a), [64]byte(b)))
}

func maskMaxEpu642(src [64]byte, k uint8, a [64]byte, b [64]byte) [64]byte


// MaskzMaxEpu642: Compare packed unsigned 64-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] > b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMAXUQ'. Intrinsic: '_mm512_maskz_max_epu64'.
// Requires AVX512F.
func MaskzMaxEpu642(k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskzMaxEpu642(uint8(k), [64]byte(a), [64]byte(b)))
}

func maskzMaxEpu642(k uint8, a [64]byte, b [64]byte) [64]byte


// MaxEpu642: Compare packed unsigned 64-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF a[i+63:i] > b[i+63:i]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := b[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMAXUQ'. Intrinsic: '_mm512_max_epu64'.
// Requires AVX512F.
func MaxEpu642(a M512i, b M512i) M512i {
	return M512i(maxEpu642([64]byte(a), [64]byte(b)))
}

func maxEpu642(a [64]byte, b [64]byte) [64]byte


// MaskMaxPd: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', and store packed maximum values in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMAXPD'. Intrinsic: '_mm_mask_max_pd'.
// Requires AVX512F.
func MaskMaxPd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskMaxPd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskMaxPd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzMaxPd: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', and store packed maximum values in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMAXPD'. Intrinsic: '_mm_maskz_max_pd'.
// Requires AVX512F.
func MaskzMaxPd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzMaxPd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzMaxPd(k uint8, a [2]float64, b [2]float64) [2]float64


// MaskMaxPd1: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', and store packed maximum values in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMAXPD'. Intrinsic: '_mm256_mask_max_pd'.
// Requires AVX512F.
func MaskMaxPd1(src M256d, k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskMaxPd1([4]float64(src), uint8(k), [4]float64(a), [4]float64(b)))
}

func maskMaxPd1(src [4]float64, k uint8, a [4]float64, b [4]float64) [4]float64


// MaskzMaxPd1: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store packed maximum values in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMAXPD'. Intrinsic: '_mm256_maskz_max_pd'.
// Requires AVX512F.
func MaskzMaxPd1(k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskzMaxPd1(uint8(k), [4]float64(a), [4]float64(b)))
}

func maskzMaxPd1(k uint8, a [4]float64, b [4]float64) [4]float64


// MaskMaxPd2: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', and store packed maximum values in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMAXPD'. Intrinsic: '_mm512_mask_max_pd'.
// Requires AVX512F.
func MaskMaxPd2(src M512d, k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskMaxPd2([8]float64(src), uint8(k), [8]float64(a), [8]float64(b)))
}

func maskMaxPd2(src [8]float64, k uint8, a [8]float64, b [8]float64) [8]float64


// MaskzMaxPd2: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store packed maximum values in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMAXPD'. Intrinsic: '_mm512_maskz_max_pd'.
// Requires AVX512F.
func MaskzMaxPd2(k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskzMaxPd2(uint8(k), [8]float64(a), [8]float64(b)))
}

func maskzMaxPd2(k uint8, a [8]float64, b [8]float64) [8]float64


// MaxPd: Compare packed double-precision (64-bit) floating-point elements in
// 'a' and 'b', and store packed maximum values in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMAXPD'. Intrinsic: '_mm512_max_pd'.
// Requires AVX512F.
func MaxPd(a M512d, b M512d) M512d {
	return M512d(maxPd([8]float64(a), [8]float64(b)))
}

func maxPd(a [8]float64, b [8]float64) [8]float64


// MaskMaxPs: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', and store packed maximum values in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMAXPS'. Intrinsic: '_mm_mask_max_ps'.
// Requires AVX512F.
func MaskMaxPs(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskMaxPs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskMaxPs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzMaxPs: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', and store packed maximum values in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMAXPS'. Intrinsic: '_mm_maskz_max_ps'.
// Requires AVX512F.
func MaskzMaxPs(k Mmask8, a M128, b M128) M128 {
	return M128(maskzMaxPs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzMaxPs(k uint8, a [4]float32, b [4]float32) [4]float32


// MaskMaxPs1: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', and store packed maximum values in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMAXPS'. Intrinsic: '_mm256_mask_max_ps'.
// Requires AVX512F.
func MaskMaxPs1(src M256, k Mmask8, a M256, b M256) M256 {
	return M256(maskMaxPs1([8]float32(src), uint8(k), [8]float32(a), [8]float32(b)))
}

func maskMaxPs1(src [8]float32, k uint8, a [8]float32, b [8]float32) [8]float32


// MaskzMaxPs1: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store packed maximum values in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMAXPS'. Intrinsic: '_mm256_maskz_max_ps'.
// Requires AVX512F.
func MaskzMaxPs1(k Mmask8, a M256, b M256) M256 {
	return M256(maskzMaxPs1(uint8(k), [8]float32(a), [8]float32(b)))
}

func maskzMaxPs1(k uint8, a [8]float32, b [8]float32) [8]float32


// MaskMaxPs2: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', and store packed maximum values in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMAXPS'. Intrinsic: '_mm512_mask_max_ps'.
// Requires AVX512F.
func MaskMaxPs2(src M512, k Mmask16, a M512, b M512) M512 {
	return M512(maskMaxPs2([16]float32(src), uint16(k), [16]float32(a), [16]float32(b)))
}

func maskMaxPs2(src [16]float32, k uint16, a [16]float32, b [16]float32) [16]float32


// MaskzMaxPs2: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store packed maximum values in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMAXPS'. Intrinsic: '_mm512_maskz_max_ps'.
// Requires AVX512F.
func MaskzMaxPs2(k Mmask16, a M512, b M512) M512 {
	return M512(maskzMaxPs2(uint16(k), [16]float32(a), [16]float32(b)))
}

func maskzMaxPs2(k uint16, a [16]float32, b [16]float32) [16]float32


// MaxPs: Compare packed single-precision (32-bit) floating-point elements in
// 'a' and 'b', and store packed maximum values in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMAXPS'. Intrinsic: '_mm512_max_ps'.
// Requires AVX512F.
func MaxPs(a M512, b M512) M512 {
	return M512(maxPs([16]float32(a), [16]float32(b)))
}

func maxPs(a [16]float32, b [16]float32) [16]float32


// MaskMaxRoundPd: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store packed maximum values in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set).  
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMAXPD'. Intrinsic: '_mm512_mask_max_round_pd'.
// Requires AVX512F.
func MaskMaxRoundPd(src M512d, k Mmask8, a M512d, b M512d, sae int) M512d {
	return M512d(maskMaxRoundPd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b), sae))
}

func maskMaxRoundPd(src [8]float64, k uint8, a [8]float64, b [8]float64, sae int) [8]float64


// MaskzMaxRoundPd: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store packed maximum values in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMAXPD'. Intrinsic: '_mm512_maskz_max_round_pd'.
// Requires AVX512F.
func MaskzMaxRoundPd(k Mmask8, a M512d, b M512d, sae int) M512d {
	return M512d(maskzMaxRoundPd(uint8(k), [8]float64(a), [8]float64(b), sae))
}

func maskzMaxRoundPd(k uint8, a [8]float64, b [8]float64, sae int) [8]float64


// MaxRoundPd: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', and store packed maximum values in 'dst'. 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMAXPD'. Intrinsic: '_mm512_max_round_pd'.
// Requires AVX512F.
func MaxRoundPd(a M512d, b M512d, sae int) M512d {
	return M512d(maxRoundPd([8]float64(a), [8]float64(b), sae))
}

func maxRoundPd(a [8]float64, b [8]float64, sae int) [8]float64


// MaskMaxRoundPs: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store packed maximum values in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set).  
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMAXPS'. Intrinsic: '_mm512_mask_max_round_ps'.
// Requires AVX512F.
func MaskMaxRoundPs(src M512, k Mmask16, a M512, b M512, sae int) M512 {
	return M512(maskMaxRoundPs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b), sae))
}

func maskMaxRoundPs(src [16]float32, k uint16, a [16]float32, b [16]float32, sae int) [16]float32


// MaskzMaxRoundPs: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store packed maximum values in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMAXPS'. Intrinsic: '_mm512_maskz_max_round_ps'.
// Requires AVX512F.
func MaskzMaxRoundPs(k Mmask16, a M512, b M512, sae int) M512 {
	return M512(maskzMaxRoundPs(uint16(k), [16]float32(a), [16]float32(b), sae))
}

func maskzMaxRoundPs(k uint16, a [16]float32, b [16]float32, sae int) [16]float32


// MaxRoundPs: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', and store packed maximum values in 'dst'. 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMAXPS'. Intrinsic: '_mm512_max_round_ps'.
// Requires AVX512F.
func MaxRoundPs(a M512, b M512, sae int) M512 {
	return M512(maxRoundPs([16]float32(a), [16]float32(b), sae))
}

func maxRoundPs(a [16]float32, b [16]float32, sae int) [16]float32


// MaskMaxRoundSd: Compare the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', store the maximum value in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'src' when mask bit 0
// is not set), and copy the upper element from 'a' to the upper element of
// 'dst'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		IF k[0]
//			dst[63:0] := MAX(a[63:0], b[63:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMAXSD'. Intrinsic: '_mm_mask_max_round_sd'.
// Requires AVX512F.
func MaskMaxRoundSd(src M128d, k Mmask8, a M128d, b M128d, sae int) M128d {
	return M128d(maskMaxRoundSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), sae))
}

func maskMaxRoundSd(src [2]float64, k uint8, a [2]float64, b [2]float64, sae int) [2]float64


// MaskzMaxRoundSd: Compare the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', store the maximum value in the lower element of
// 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is not
// set), and copy the upper element from 'a' to the upper element of 'dst'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		IF k[0]
//			dst[63:0] := MAX(a[63:0], b[63:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMAXSD'. Intrinsic: '_mm_maskz_max_round_sd'.
// Requires AVX512F.
func MaskzMaxRoundSd(k Mmask8, a M128d, b M128d, sae int) M128d {
	return M128d(maskzMaxRoundSd(uint8(k), [2]float64(a), [2]float64(b), sae))
}

func maskzMaxRoundSd(k uint8, a [2]float64, b [2]float64, sae int) [2]float64


// MaxRoundSd: Compare the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', store the maximum value in the lower element of
// 'dst', and copy the upper element from 'a' to the upper element of 'dst'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		dst[63:0] := MAX(a[63:0], b[63:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMAXSD'. Intrinsic: '_mm_max_round_sd'.
// Requires AVX512F.
func MaxRoundSd(a M128d, b M128d, sae int) M128d {
	return M128d(maxRoundSd([2]float64(a), [2]float64(b), sae))
}

func maxRoundSd(a [2]float64, b [2]float64, sae int) [2]float64


// MaskMaxRoundSs: Compare the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', store the maximum value in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'src' when mask bit 0
// is not set), and copy the upper element from 'a' to the upper element of
// 'dst'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		IF k[0]
//			dst[31:0] := MAX(a[31:0], b[31:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMAXSS'. Intrinsic: '_mm_mask_max_round_ss'.
// Requires AVX512F.
func MaskMaxRoundSs(src M128, k Mmask8, a M128, b M128, sae int) M128 {
	return M128(maskMaxRoundSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), sae))
}

func maskMaxRoundSs(src [4]float32, k uint8, a [4]float32, b [4]float32, sae int) [4]float32


// MaskzMaxRoundSs: Compare the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', store the maximum value in the lower element of
// 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is not
// set), and copy the upper element from 'a' to the upper element of 'dst'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		IF k[0]
//			dst[31:0] := MAX(a[31:0], b[31:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMAXSS'. Intrinsic: '_mm_maskz_max_round_ss'.
// Requires AVX512F.
func MaskzMaxRoundSs(k Mmask8, a M128, b M128, sae int) M128 {
	return M128(maskzMaxRoundSs(uint8(k), [4]float32(a), [4]float32(b), sae))
}

func maskzMaxRoundSs(k uint8, a [4]float32, b [4]float32, sae int) [4]float32


// MaxRoundSs: Compare the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', store the maximum value in the lower element of
// 'dst', and copy the upper element from 'a' to the upper element of 'dst'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		dst[31:0] := MAX(a[31:0], b[31:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMAXSS'. Intrinsic: '_mm_max_round_ss'.
// Requires AVX512F.
func MaxRoundSs(a M128, b M128, sae int) M128 {
	return M128(maxRoundSs([4]float32(a), [4]float32(b), sae))
}

func maxRoundSs(a [4]float32, b [4]float32, sae int) [4]float32


// MaskMaxSd: Compare the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', store the maximum value in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'src' when mask bit 0
// is not set), and copy the upper element from 'a' to the upper element of
// 'dst'. 
//
//		IF k[0]
//			dst[63:0] := MAX(a[63:0], b[63:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMAXSD'. Intrinsic: '_mm_mask_max_sd'.
// Requires AVX512F.
func MaskMaxSd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskMaxSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskMaxSd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzMaxSd: Compare the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', store the maximum value in the lower element of
// 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is not
// set), and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := MAX(a[63:0], b[63:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMAXSD'. Intrinsic: '_mm_maskz_max_sd'.
// Requires AVX512F.
func MaskzMaxSd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzMaxSd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzMaxSd(k uint8, a [2]float64, b [2]float64) [2]float64


// MaskMaxSs: Compare the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', store the maximum value in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'src' when mask bit 0
// is not set), and copy the upper element from 'a' to the upper element of
// 'dst'. 
//
//		IF k[0]
//			dst[31:0] := MAX(a[31:0], b[31:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMAXSS'. Intrinsic: '_mm_mask_max_ss'.
// Requires AVX512F.
func MaskMaxSs(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskMaxSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskMaxSs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzMaxSs: Compare the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', store the maximum value in the lower element of
// 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is not
// set), and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := MAX(a[31:0], b[31:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMAXSS'. Intrinsic: '_mm_maskz_max_ss'.
// Requires AVX512F.
func MaskzMaxSs(k Mmask8, a M128, b M128) M128 {
	return M128(maskzMaxSs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzMaxSs(k uint8, a [4]float32, b [4]float32) [4]float32


// MaskMinEpi32: Compare packed 32-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] < b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMINSD'. Intrinsic: '_mm_mask_min_epi32'.
// Requires AVX512F.
func MaskMinEpi32(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskMinEpi32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskMinEpi32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzMinEpi32: Compare packed 32-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] < b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMINSD'. Intrinsic: '_mm_maskz_min_epi32'.
// Requires AVX512F.
func MaskzMinEpi32(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzMinEpi32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzMinEpi32(k uint8, a [16]byte, b [16]byte) [16]byte


// MaskMinEpi321: Compare packed 32-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] < b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINSD'. Intrinsic: '_mm256_mask_min_epi32'.
// Requires AVX512F.
func MaskMinEpi321(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskMinEpi321([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskMinEpi321(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzMinEpi321: Compare packed 32-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] < b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINSD'. Intrinsic: '_mm256_maskz_min_epi32'.
// Requires AVX512F.
func MaskzMinEpi321(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzMinEpi321(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzMinEpi321(k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzMinEpi322: Compare packed 32-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] < b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMINSD'. Intrinsic: '_mm512_maskz_min_epi32'.
// Requires AVX512F.
func MaskzMinEpi322(k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskzMinEpi322(uint16(k), [64]byte(a), [64]byte(b)))
}

func maskzMinEpi322(k uint16, a [64]byte, b [64]byte) [64]byte


// MaskMinEpi64: Compare packed 64-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] < b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMINSQ'. Intrinsic: '_mm_mask_min_epi64'.
// Requires AVX512F.
func MaskMinEpi64(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskMinEpi64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskMinEpi64(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzMinEpi64: Compare packed 64-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] < b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMINSQ'. Intrinsic: '_mm_maskz_min_epi64'.
// Requires AVX512F.
func MaskzMinEpi64(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzMinEpi64(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzMinEpi64(k uint8, a [16]byte, b [16]byte) [16]byte


// MinEpi64: Compare packed 64-bit integers in 'a' and 'b', and store packed
// minimum values in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF a[i+63:i] < b[i+63:i]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := b[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMINSQ'. Intrinsic: '_mm_min_epi64'.
// Requires AVX512F.
func MinEpi64(a M128i, b M128i) M128i {
	return M128i(minEpi64([16]byte(a), [16]byte(b)))
}

func minEpi64(a [16]byte, b [16]byte) [16]byte


// MaskMinEpi641: Compare packed 64-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] < b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINSQ'. Intrinsic: '_mm256_mask_min_epi64'.
// Requires AVX512F.
func MaskMinEpi641(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskMinEpi641([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskMinEpi641(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzMinEpi641: Compare packed 64-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] < b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINSQ'. Intrinsic: '_mm256_maskz_min_epi64'.
// Requires AVX512F.
func MaskzMinEpi641(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzMinEpi641(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzMinEpi641(k uint8, a [32]byte, b [32]byte) [32]byte


// MinEpi641: Compare packed 64-bit integers in 'a' and 'b', and store packed
// minimum values in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF a[i+63:i] < b[i+63:i]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := b[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINSQ'. Intrinsic: '_mm256_min_epi64'.
// Requires AVX512F.
func MinEpi641(a M256i, b M256i) M256i {
	return M256i(minEpi641([32]byte(a), [32]byte(b)))
}

func minEpi641(a [32]byte, b [32]byte) [32]byte


// MaskMinEpi642: Compare packed 64-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] < b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMINSQ'. Intrinsic: '_mm512_mask_min_epi64'.
// Requires AVX512F.
func MaskMinEpi642(src M512i, k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskMinEpi642([64]byte(src), uint8(k), [64]byte(a), [64]byte(b)))
}

func maskMinEpi642(src [64]byte, k uint8, a [64]byte, b [64]byte) [64]byte


// MaskzMinEpi642: Compare packed 64-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] < b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMINSQ'. Intrinsic: '_mm512_maskz_min_epi64'.
// Requires AVX512F.
func MaskzMinEpi642(k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskzMinEpi642(uint8(k), [64]byte(a), [64]byte(b)))
}

func maskzMinEpi642(k uint8, a [64]byte, b [64]byte) [64]byte


// MinEpi642: Compare packed 64-bit integers in 'a' and 'b', and store packed
// minimum values in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF a[i+63:i] < b[i+63:i]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := b[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMINSQ'. Intrinsic: '_mm512_min_epi64'.
// Requires AVX512F.
func MinEpi642(a M512i, b M512i) M512i {
	return M512i(minEpi642([64]byte(a), [64]byte(b)))
}

func minEpi642(a [64]byte, b [64]byte) [64]byte


// MaskMinEpu32: Compare packed unsigned 32-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] < b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMINUD'. Intrinsic: '_mm_mask_min_epu32'.
// Requires AVX512F.
func MaskMinEpu32(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskMinEpu32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskMinEpu32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzMinEpu32: Compare packed unsigned 32-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] < b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMINUD'. Intrinsic: '_mm_maskz_min_epu32'.
// Requires AVX512F.
func MaskzMinEpu32(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzMinEpu32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzMinEpu32(k uint8, a [16]byte, b [16]byte) [16]byte


// MaskMinEpu321: Compare packed unsigned 32-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] < b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINUD'. Intrinsic: '_mm256_mask_min_epu32'.
// Requires AVX512F.
func MaskMinEpu321(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskMinEpu321([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskMinEpu321(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzMinEpu321: Compare packed unsigned 32-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] < b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINUD'. Intrinsic: '_mm256_maskz_min_epu32'.
// Requires AVX512F.
func MaskzMinEpu321(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzMinEpu321(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzMinEpu321(k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzMinEpu322: Compare packed unsigned 32-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] < b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMINUD'. Intrinsic: '_mm512_maskz_min_epu32'.
// Requires AVX512F.
func MaskzMinEpu322(k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskzMinEpu322(uint16(k), [64]byte(a), [64]byte(b)))
}

func maskzMinEpu322(k uint16, a [64]byte, b [64]byte) [64]byte


// MaskMinEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] < b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMINUQ'. Intrinsic: '_mm_mask_min_epu64'.
// Requires AVX512F.
func MaskMinEpu64(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskMinEpu64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskMinEpu64(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzMinEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] < b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMINUQ'. Intrinsic: '_mm_maskz_min_epu64'.
// Requires AVX512F.
func MaskzMinEpu64(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzMinEpu64(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzMinEpu64(k uint8, a [16]byte, b [16]byte) [16]byte


// MinEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF a[i+63:i] < b[i+63:i]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := b[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMINUQ'. Intrinsic: '_mm_min_epu64'.
// Requires AVX512F.
func MinEpu64(a M128i, b M128i) M128i {
	return M128i(minEpu64([16]byte(a), [16]byte(b)))
}

func minEpu64(a [16]byte, b [16]byte) [16]byte


// MaskMinEpu641: Compare packed unsigned 64-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] < b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINUQ'. Intrinsic: '_mm256_mask_min_epu64'.
// Requires AVX512F.
func MaskMinEpu641(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskMinEpu641([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskMinEpu641(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzMinEpu641: Compare packed unsigned 64-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] < b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINUQ'. Intrinsic: '_mm256_maskz_min_epu64'.
// Requires AVX512F.
func MaskzMinEpu641(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzMinEpu641(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzMinEpu641(k uint8, a [32]byte, b [32]byte) [32]byte


// MinEpu641: Compare packed unsigned 64-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF a[i+63:i] < b[i+63:i]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := b[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINUQ'. Intrinsic: '_mm256_min_epu64'.
// Requires AVX512F.
func MinEpu641(a M256i, b M256i) M256i {
	return M256i(minEpu641([32]byte(a), [32]byte(b)))
}

func minEpu641(a [32]byte, b [32]byte) [32]byte


// MaskMinEpu642: Compare packed unsigned 64-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] < b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMINUQ'. Intrinsic: '_mm512_mask_min_epu64'.
// Requires AVX512F.
func MaskMinEpu642(src M512i, k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskMinEpu642([64]byte(src), uint8(k), [64]byte(a), [64]byte(b)))
}

func maskMinEpu642(src [64]byte, k uint8, a [64]byte, b [64]byte) [64]byte


// MaskzMinEpu642: Compare packed unsigned 64-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] < b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMINUQ'. Intrinsic: '_mm512_maskz_min_epu64'.
// Requires AVX512F.
func MaskzMinEpu642(k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskzMinEpu642(uint8(k), [64]byte(a), [64]byte(b)))
}

func maskzMinEpu642(k uint8, a [64]byte, b [64]byte) [64]byte


// MinEpu642: Compare packed unsigned 64-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF a[i+63:i] < b[i+63:i]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := b[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMINUQ'. Intrinsic: '_mm512_min_epu64'.
// Requires AVX512F.
func MinEpu642(a M512i, b M512i) M512i {
	return M512i(minEpu642([64]byte(a), [64]byte(b)))
}

func minEpu642(a [64]byte, b [64]byte) [64]byte


// MaskMinPd: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', and store packed minimum values in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMINPD'. Intrinsic: '_mm_mask_min_pd'.
// Requires AVX512F.
func MaskMinPd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskMinPd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskMinPd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzMinPd: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', and store packed minimum values in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMINPD'. Intrinsic: '_mm_maskz_min_pd'.
// Requires AVX512F.
func MaskzMinPd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzMinPd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzMinPd(k uint8, a [2]float64, b [2]float64) [2]float64


// MaskMinPd1: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', and store packed minimum values in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMINPD'. Intrinsic: '_mm256_mask_min_pd'.
// Requires AVX512F.
func MaskMinPd1(src M256d, k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskMinPd1([4]float64(src), uint8(k), [4]float64(a), [4]float64(b)))
}

func maskMinPd1(src [4]float64, k uint8, a [4]float64, b [4]float64) [4]float64


// MaskzMinPd1: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store packed minimum values in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMINPD'. Intrinsic: '_mm256_maskz_min_pd'.
// Requires AVX512F.
func MaskzMinPd1(k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskzMinPd1(uint8(k), [4]float64(a), [4]float64(b)))
}

func maskzMinPd1(k uint8, a [4]float64, b [4]float64) [4]float64


// MaskMinPd2: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', and store packed minimum values in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMINPD'. Intrinsic: '_mm512_mask_min_pd'.
// Requires AVX512F.
func MaskMinPd2(src M512d, k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskMinPd2([8]float64(src), uint8(k), [8]float64(a), [8]float64(b)))
}

func maskMinPd2(src [8]float64, k uint8, a [8]float64, b [8]float64) [8]float64


// MaskzMinPd2: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store packed minimum values in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMINPD'. Intrinsic: '_mm512_maskz_min_pd'.
// Requires AVX512F.
func MaskzMinPd2(k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskzMinPd2(uint8(k), [8]float64(a), [8]float64(b)))
}

func maskzMinPd2(k uint8, a [8]float64, b [8]float64) [8]float64


// MinPd: Compare packed double-precision (64-bit) floating-point elements in
// 'a' and 'b', and store packed minimum values in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMINPD'. Intrinsic: '_mm512_min_pd'.
// Requires AVX512F.
func MinPd(a M512d, b M512d) M512d {
	return M512d(minPd([8]float64(a), [8]float64(b)))
}

func minPd(a [8]float64, b [8]float64) [8]float64


// MaskMinPs: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', and store packed minimum values in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMINPS'. Intrinsic: '_mm_mask_min_ps'.
// Requires AVX512F.
func MaskMinPs(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskMinPs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskMinPs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzMinPs: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', and store packed minimum values in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMINPS'. Intrinsic: '_mm_maskz_min_ps'.
// Requires AVX512F.
func MaskzMinPs(k Mmask8, a M128, b M128) M128 {
	return M128(maskzMinPs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzMinPs(k uint8, a [4]float32, b [4]float32) [4]float32


// MaskMinPs1: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', and store packed minimum values in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMINPS'. Intrinsic: '_mm256_mask_min_ps'.
// Requires AVX512F.
func MaskMinPs1(src M256, k Mmask8, a M256, b M256) M256 {
	return M256(maskMinPs1([8]float32(src), uint8(k), [8]float32(a), [8]float32(b)))
}

func maskMinPs1(src [8]float32, k uint8, a [8]float32, b [8]float32) [8]float32


// MaskzMinPs1: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store packed minimum values in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMINPS'. Intrinsic: '_mm256_maskz_min_ps'.
// Requires AVX512F.
func MaskzMinPs1(k Mmask8, a M256, b M256) M256 {
	return M256(maskzMinPs1(uint8(k), [8]float32(a), [8]float32(b)))
}

func maskzMinPs1(k uint8, a [8]float32, b [8]float32) [8]float32


// MaskMinPs2: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', and store packed minimum values in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMINPS'. Intrinsic: '_mm512_mask_min_ps'.
// Requires AVX512F.
func MaskMinPs2(src M512, k Mmask16, a M512, b M512) M512 {
	return M512(maskMinPs2([16]float32(src), uint16(k), [16]float32(a), [16]float32(b)))
}

func maskMinPs2(src [16]float32, k uint16, a [16]float32, b [16]float32) [16]float32


// MaskzMinPs2: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store packed minimum values in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMINPS'. Intrinsic: '_mm512_maskz_min_ps'.
// Requires AVX512F.
func MaskzMinPs2(k Mmask16, a M512, b M512) M512 {
	return M512(maskzMinPs2(uint16(k), [16]float32(a), [16]float32(b)))
}

func maskzMinPs2(k uint16, a [16]float32, b [16]float32) [16]float32


// MinPs: Compare packed single-precision (32-bit) floating-point elements in
// 'a' and 'b', and store packed minimum values in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMINPS'. Intrinsic: '_mm512_min_ps'.
// Requires AVX512F.
func MinPs(a M512, b M512) M512 {
	return M512(minPs([16]float32(a), [16]float32(b)))
}

func minPs(a [16]float32, b [16]float32) [16]float32


// MaskMinRoundPd: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store packed minimum values in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set).  
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMINPD'. Intrinsic: '_mm512_mask_min_round_pd'.
// Requires AVX512F.
func MaskMinRoundPd(src M512d, k Mmask8, a M512d, b M512d, sae int) M512d {
	return M512d(maskMinRoundPd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b), sae))
}

func maskMinRoundPd(src [8]float64, k uint8, a [8]float64, b [8]float64, sae int) [8]float64


// MaskzMinRoundPd: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store packed minimum values in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMINPD'. Intrinsic: '_mm512_maskz_min_round_pd'.
// Requires AVX512F.
func MaskzMinRoundPd(k Mmask8, a M512d, b M512d, sae int) M512d {
	return M512d(maskzMinRoundPd(uint8(k), [8]float64(a), [8]float64(b), sae))
}

func maskzMinRoundPd(k uint8, a [8]float64, b [8]float64, sae int) [8]float64


// MinRoundPd: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', and store packed minimum values in 'dst'. 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMINPD'. Intrinsic: '_mm512_min_round_pd'.
// Requires AVX512F.
func MinRoundPd(a M512d, b M512d, sae int) M512d {
	return M512d(minRoundPd([8]float64(a), [8]float64(b), sae))
}

func minRoundPd(a [8]float64, b [8]float64, sae int) [8]float64


// MaskMinRoundPs: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store packed minimum values in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set).  
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMINPS'. Intrinsic: '_mm512_mask_min_round_ps'.
// Requires AVX512F.
func MaskMinRoundPs(src M512, k Mmask16, a M512, b M512, sae int) M512 {
	return M512(maskMinRoundPs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b), sae))
}

func maskMinRoundPs(src [16]float32, k uint16, a [16]float32, b [16]float32, sae int) [16]float32


// MaskzMinRoundPs: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store packed minimum values in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMINPS'. Intrinsic: '_mm512_maskz_min_round_ps'.
// Requires AVX512F.
func MaskzMinRoundPs(k Mmask16, a M512, b M512, sae int) M512 {
	return M512(maskzMinRoundPs(uint16(k), [16]float32(a), [16]float32(b), sae))
}

func maskzMinRoundPs(k uint16, a [16]float32, b [16]float32, sae int) [16]float32


// MinRoundPs: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', and store packed minimum values in 'dst'. 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMINPS'. Intrinsic: '_mm512_min_round_ps'.
// Requires AVX512F.
func MinRoundPs(a M512, b M512, sae int) M512 {
	return M512(minRoundPs([16]float32(a), [16]float32(b), sae))
}

func minRoundPs(a [16]float32, b [16]float32, sae int) [16]float32


// MaskMinRoundSd: Compare the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', store the minimum value in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'src' when mask bit 0
// is not set), and copy the upper element from 'a' to the upper element of
// 'dst'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		IF k[0]
//			dst[63:0] := MIN(a[63:0], b[63:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMINSD'. Intrinsic: '_mm_mask_min_round_sd'.
// Requires AVX512F.
func MaskMinRoundSd(src M128d, k Mmask8, a M128d, b M128d, sae int) M128d {
	return M128d(maskMinRoundSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), sae))
}

func maskMinRoundSd(src [2]float64, k uint8, a [2]float64, b [2]float64, sae int) [2]float64


// MaskzMinRoundSd: Compare the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', store the minimum value in the lower element of
// 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is not
// set), and copy the upper element from 'a' to the upper element of 'dst'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		IF k[0]
//			dst[63:0] := MIN(a[63:0], b[63:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMINSD'. Intrinsic: '_mm_maskz_min_round_sd'.
// Requires AVX512F.
func MaskzMinRoundSd(k Mmask8, a M128d, b M128d, sae int) M128d {
	return M128d(maskzMinRoundSd(uint8(k), [2]float64(a), [2]float64(b), sae))
}

func maskzMinRoundSd(k uint8, a [2]float64, b [2]float64, sae int) [2]float64


// MinRoundSd: Compare the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', store the minimum value in the lower element of
// 'dst' , and copy the upper element from 'a' to the upper element of 'dst'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		dst[63:0] := MIN(a[63:0], b[63:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMINSD'. Intrinsic: '_mm_min_round_sd'.
// Requires AVX512F.
func MinRoundSd(a M128d, b M128d, sae int) M128d {
	return M128d(minRoundSd([2]float64(a), [2]float64(b), sae))
}

func minRoundSd(a [2]float64, b [2]float64, sae int) [2]float64


// MaskMinRoundSs: Compare the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', store the minimum value in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'src' when mask bit 0
// is not set), and copy the upper element from 'a' to the upper element of
// 'dst'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		IF k[0]
//			dst[31:0] := MIN(a[31:0], b[31:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMINSS'. Intrinsic: '_mm_mask_min_round_ss'.
// Requires AVX512F.
func MaskMinRoundSs(src M128, k Mmask8, a M128, b M128, sae int) M128 {
	return M128(maskMinRoundSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), sae))
}

func maskMinRoundSs(src [4]float32, k uint8, a [4]float32, b [4]float32, sae int) [4]float32


// MaskzMinRoundSs: Compare the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', store the minimum value in the lower element of
// 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is not
// set), and copy the upper element from 'a' to the upper element of 'dst'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		IF k[0]
//			dst[31:0] := MIN(a[31:0], b[31:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMINSS'. Intrinsic: '_mm_maskz_min_round_ss'.
// Requires AVX512F.
func MaskzMinRoundSs(k Mmask8, a M128, b M128, sae int) M128 {
	return M128(maskzMinRoundSs(uint8(k), [4]float32(a), [4]float32(b), sae))
}

func maskzMinRoundSs(k uint8, a [4]float32, b [4]float32, sae int) [4]float32


// MinRoundSs: Compare the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', store the minimum value in the lower element of
// 'dst', and copy the upper element from 'a' to the upper element of 'dst'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		dst[31:0] := MIN(a[31:0], b[31:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMINSS'. Intrinsic: '_mm_min_round_ss'.
// Requires AVX512F.
func MinRoundSs(a M128, b M128, sae int) M128 {
	return M128(minRoundSs([4]float32(a), [4]float32(b), sae))
}

func minRoundSs(a [4]float32, b [4]float32, sae int) [4]float32


// MaskMinSd: Compare the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', store the minimum value in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'src' when mask bit 0
// is not set), and copy the upper element from 'a' to the upper element of
// 'dst'. 
//
//		IF k[0]
//			dst[63:0] := MIN(a[63:0], b[63:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMINSD'. Intrinsic: '_mm_mask_min_sd'.
// Requires AVX512F.
func MaskMinSd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskMinSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskMinSd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzMinSd: Compare the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', store the minimum value in the lower element of
// 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is not
// set), and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := MIN(a[63:0], b[63:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMINSD'. Intrinsic: '_mm_maskz_min_sd'.
// Requires AVX512F.
func MaskzMinSd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzMinSd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzMinSd(k uint8, a [2]float64, b [2]float64) [2]float64


// MaskMinSs: Compare the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', store the minimum value in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'src' when mask bit 0
// is not set), and copy the upper element from 'a' to the upper element of
// 'dst'. 
//
//		IF k[0]
//			dst[31:0] := MIN(a[31:0], b[31:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMINSS'. Intrinsic: '_mm_mask_min_ss'.
// Requires AVX512F.
func MaskMinSs(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskMinSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskMinSs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzMinSs: Compare the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', store the minimum value in the lower element of
// 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is not
// set), and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := MIN(a[31:0], b[31:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMINSS'. Intrinsic: '_mm_maskz_min_ss'.
// Requires AVX512F.
func MaskzMinSs(k Mmask8, a M128, b M128) M128 {
	return M128(maskzMinSs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzMinSs(k uint8, a [4]float32, b [4]float32) [4]float32


// MaskMovEpi32: Move packed 32-bit integers from 'a' to 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVDQA32'. Intrinsic: '_mm_mask_mov_epi32'.
// Requires AVX512F.
func MaskMovEpi32(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskMovEpi32([16]byte(src), uint8(k), [16]byte(a)))
}

func maskMovEpi32(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzMovEpi32: Move packed 32-bit integers from 'a' into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVDQA32'. Intrinsic: '_mm_maskz_mov_epi32'.
// Requires AVX512F.
func MaskzMovEpi32(k Mmask8, a M128i) M128i {
	return M128i(maskzMovEpi32(uint8(k), [16]byte(a)))
}

func maskzMovEpi32(k uint8, a [16]byte) [16]byte


// MaskMovEpi321: Move packed 32-bit integers from 'a' to 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQA32'. Intrinsic: '_mm256_mask_mov_epi32'.
// Requires AVX512F.
func MaskMovEpi321(src M256i, k Mmask8, a M256i) M256i {
	return M256i(maskMovEpi321([32]byte(src), uint8(k), [32]byte(a)))
}

func maskMovEpi321(src [32]byte, k uint8, a [32]byte) [32]byte


// MaskzMovEpi321: Move packed 32-bit integers from 'a' into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQA32'. Intrinsic: '_mm256_maskz_mov_epi32'.
// Requires AVX512F.
func MaskzMovEpi321(k Mmask8, a M256i) M256i {
	return M256i(maskzMovEpi321(uint8(k), [32]byte(a)))
}

func maskzMovEpi321(k uint8, a [32]byte) [32]byte


// MaskzMovEpi322: Move packed 32-bit integers from 'a' into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDQA32'. Intrinsic: '_mm512_maskz_mov_epi32'.
// Requires AVX512F.
func MaskzMovEpi322(k Mmask16, a M512i) M512i {
	return M512i(maskzMovEpi322(uint16(k), [64]byte(a)))
}

func maskzMovEpi322(k uint16, a [64]byte) [64]byte


// MaskMovEpi64: Move packed 64-bit integers from 'a' to 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVDQA64'. Intrinsic: '_mm_mask_mov_epi64'.
// Requires AVX512F.
func MaskMovEpi64(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskMovEpi64([16]byte(src), uint8(k), [16]byte(a)))
}

func maskMovEpi64(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzMovEpi64: Move packed 64-bit integers from 'a' into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVDQA64'. Intrinsic: '_mm_maskz_mov_epi64'.
// Requires AVX512F.
func MaskzMovEpi64(k Mmask8, a M128i) M128i {
	return M128i(maskzMovEpi64(uint8(k), [16]byte(a)))
}

func maskzMovEpi64(k uint8, a [16]byte) [16]byte


// MaskMovEpi641: Move packed 64-bit integers from 'a' to 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQA64'. Intrinsic: '_mm256_mask_mov_epi64'.
// Requires AVX512F.
func MaskMovEpi641(src M256i, k Mmask8, a M256i) M256i {
	return M256i(maskMovEpi641([32]byte(src), uint8(k), [32]byte(a)))
}

func maskMovEpi641(src [32]byte, k uint8, a [32]byte) [32]byte


// MaskzMovEpi641: Move packed 64-bit integers from 'a' into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQA64'. Intrinsic: '_mm256_maskz_mov_epi64'.
// Requires AVX512F.
func MaskzMovEpi641(k Mmask8, a M256i) M256i {
	return M256i(maskzMovEpi641(uint8(k), [32]byte(a)))
}

func maskzMovEpi641(k uint8, a [32]byte) [32]byte


// MaskzMovEpi642: Move packed 64-bit integers from 'a' into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDQA64'. Intrinsic: '_mm512_maskz_mov_epi64'.
// Requires AVX512F.
func MaskzMovEpi642(k Mmask8, a M512i) M512i {
	return M512i(maskzMovEpi642(uint8(k), [64]byte(a)))
}

func maskzMovEpi642(k uint8, a [64]byte) [64]byte


// MaskMovPd: Move packed double-precision (64-bit) floating-point elements
// from 'a' to 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVAPD'. Intrinsic: '_mm_mask_mov_pd'.
// Requires AVX512F.
func MaskMovPd(src M128d, k Mmask8, a M128d) M128d {
	return M128d(maskMovPd([2]float64(src), uint8(k), [2]float64(a)))
}

func maskMovPd(src [2]float64, k uint8, a [2]float64) [2]float64


// MaskzMovPd: Move packed double-precision (64-bit) floating-point elements
// from 'a' into 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVAPD'. Intrinsic: '_mm_maskz_mov_pd'.
// Requires AVX512F.
func MaskzMovPd(k Mmask8, a M128d) M128d {
	return M128d(maskzMovPd(uint8(k), [2]float64(a)))
}

func maskzMovPd(k uint8, a [2]float64) [2]float64


// MaskMovPd1: Move packed double-precision (64-bit) floating-point elements
// from 'a' to 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVAPD'. Intrinsic: '_mm256_mask_mov_pd'.
// Requires AVX512F.
func MaskMovPd1(src M256d, k Mmask8, a M256d) M256d {
	return M256d(maskMovPd1([4]float64(src), uint8(k), [4]float64(a)))
}

func maskMovPd1(src [4]float64, k uint8, a [4]float64) [4]float64


// MaskzMovPd1: Move packed double-precision (64-bit) floating-point elements
// from 'a' into 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVAPD'. Intrinsic: '_mm256_maskz_mov_pd'.
// Requires AVX512F.
func MaskzMovPd1(k Mmask8, a M256d) M256d {
	return M256d(maskzMovPd1(uint8(k), [4]float64(a)))
}

func maskzMovPd1(k uint8, a [4]float64) [4]float64


// MaskzMovPd2: Move packed double-precision (64-bit) floating-point elements
// from 'a' into 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVAPD'. Intrinsic: '_mm512_maskz_mov_pd'.
// Requires AVX512F.
func MaskzMovPd2(k Mmask8, a M512d) M512d {
	return M512d(maskzMovPd2(uint8(k), [8]float64(a)))
}

func maskzMovPd2(k uint8, a [8]float64) [8]float64


// MaskMovPs: Move packed single-precision (32-bit) floating-point elements
// from 'a' to 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVAPS'. Intrinsic: '_mm_mask_mov_ps'.
// Requires AVX512F.
func MaskMovPs(src M128, k Mmask8, a M128) M128 {
	return M128(maskMovPs([4]float32(src), uint8(k), [4]float32(a)))
}

func maskMovPs(src [4]float32, k uint8, a [4]float32) [4]float32


// MaskzMovPs: Move packed single-precision (32-bit) floating-point elements
// from 'a' into 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVAPS'. Intrinsic: '_mm_maskz_mov_ps'.
// Requires AVX512F.
func MaskzMovPs(k Mmask8, a M128) M128 {
	return M128(maskzMovPs(uint8(k), [4]float32(a)))
}

func maskzMovPs(k uint8, a [4]float32) [4]float32


// MaskMovPs1: Move packed single-precision (32-bit) floating-point elements
// from 'a' to 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVAPS'. Intrinsic: '_mm256_mask_mov_ps'.
// Requires AVX512F.
func MaskMovPs1(src M256, k Mmask8, a M256) M256 {
	return M256(maskMovPs1([8]float32(src), uint8(k), [8]float32(a)))
}

func maskMovPs1(src [8]float32, k uint8, a [8]float32) [8]float32


// MaskzMovPs1: Move packed single-precision (32-bit) floating-point elements
// from 'a' into 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVAPS'. Intrinsic: '_mm256_maskz_mov_ps'.
// Requires AVX512F.
func MaskzMovPs1(k Mmask8, a M256) M256 {
	return M256(maskzMovPs1(uint8(k), [8]float32(a)))
}

func maskzMovPs1(k uint8, a [8]float32) [8]float32


// MaskzMovPs2: Move packed single-precision (32-bit) floating-point elements
// from 'a' into 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVAPS'. Intrinsic: '_mm512_maskz_mov_ps'.
// Requires AVX512F.
func MaskzMovPs2(k Mmask16, a M512) M512 {
	return M512(maskzMovPs2(uint16(k), [16]float32(a)))
}

func maskzMovPs2(k uint16, a [16]float32) [16]float32


// MaskMoveSd: Move the lower double-precision (64-bit) floating-point element
// from 'b' to the lower element of 'dst' using writemask 'k' (the element is
// copied from 'src' when mask bit 0 is not set), and copy the upper element
// from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := b[63:0]
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVSD'. Intrinsic: '_mm_mask_move_sd'.
// Requires AVX512F.
func MaskMoveSd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskMoveSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskMoveSd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzMoveSd: Move the lower double-precision (64-bit) floating-point element
// from 'b' to the lower element of 'dst' using zeromask 'k' (the element is
// zeroed out when mask bit 0 is not set), and copy the upper element from 'a'
// to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := b[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVSD'. Intrinsic: '_mm_maskz_move_sd'.
// Requires AVX512F.
func MaskzMoveSd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzMoveSd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzMoveSd(k uint8, a [2]float64, b [2]float64) [2]float64


// MaskMoveSs: Move the lower single-precision (32-bit) floating-point element
// from 'b' to the lower element of 'dst' using writemask 'k' (the element is
// copied from 'src' when mask bit 0 is not set), and copy the upper 3 packed
// elements from 'a' to the upper elements of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := b[31:0]
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVSS'. Intrinsic: '_mm_mask_move_ss'.
// Requires AVX512F.
func MaskMoveSs(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskMoveSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskMoveSs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzMoveSs: Move the lower single-precision (32-bit) floating-point element
// from 'b' to the lower element of 'dst' using zeromask 'k' (the element is
// zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements
// from 'a' to the upper elements of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := b[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVSS'. Intrinsic: '_mm_maskz_move_ss'.
// Requires AVX512F.
func MaskzMoveSs(k Mmask8, a M128, b M128) M128 {
	return M128(maskzMoveSs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzMoveSs(k uint8, a [4]float32, b [4]float32) [4]float32


// MaskMovedupPd: Duplicate even-indexed double-precision (64-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp[63:0] := a[63:0]
//		tmp[127:64] := a[63:0]
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVDDUP'. Intrinsic: '_mm_mask_movedup_pd'.
// Requires AVX512F.
func MaskMovedupPd(src M128d, k Mmask8, a M128d) M128d {
	return M128d(maskMovedupPd([2]float64(src), uint8(k), [2]float64(a)))
}

func maskMovedupPd(src [2]float64, k uint8, a [2]float64) [2]float64


// MaskzMovedupPd: Duplicate even-indexed double-precision (64-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp[63:0] := a[63:0]
//		tmp[127:64] := a[63:0]
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVDDUP'. Intrinsic: '_mm_maskz_movedup_pd'.
// Requires AVX512F.
func MaskzMovedupPd(k Mmask8, a M128d) M128d {
	return M128d(maskzMovedupPd(uint8(k), [2]float64(a)))
}

func maskzMovedupPd(k uint8, a [2]float64) [2]float64


// MaskMovedupPd1: Duplicate even-indexed double-precision (64-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp[63:0] := a[63:0]
//		tmp[127:64] := a[63:0]
//		tmp[191:128] := a[191:128]
//		tmp[255:192] := a[191:128]
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDDUP'. Intrinsic: '_mm256_mask_movedup_pd'.
// Requires AVX512F.
func MaskMovedupPd1(src M256d, k Mmask8, a M256d) M256d {
	return M256d(maskMovedupPd1([4]float64(src), uint8(k), [4]float64(a)))
}

func maskMovedupPd1(src [4]float64, k uint8, a [4]float64) [4]float64


// MaskzMovedupPd1: Duplicate even-indexed double-precision (64-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp[63:0] := a[63:0]
//		tmp[127:64] := a[63:0]
//		tmp[191:128] := a[191:128]
//		tmp[255:192] := a[191:128]
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDDUP'. Intrinsic: '_mm256_maskz_movedup_pd'.
// Requires AVX512F.
func MaskzMovedupPd1(k Mmask8, a M256d) M256d {
	return M256d(maskzMovedupPd1(uint8(k), [4]float64(a)))
}

func maskzMovedupPd1(k uint8, a [4]float64) [4]float64


// MaskMovedupPd2: Duplicate even-indexed double-precision (64-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp[63:0] := a[63:0]
//		tmp[127:64] := a[63:0]
//		tmp[191:128] := a[191:128]
//		tmp[255:192] := a[191:128]
//		tmp[319:256] := a[319:256] 
//		tmp[383:320] := a[319:256] 
//		tmp[447:384] := a[447:384]
//		tmp[511:448] := a[447:384]
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDDUP'. Intrinsic: '_mm512_mask_movedup_pd'.
// Requires AVX512F.
func MaskMovedupPd2(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskMovedupPd2([8]float64(src), uint8(k), [8]float64(a)))
}

func maskMovedupPd2(src [8]float64, k uint8, a [8]float64) [8]float64


// MaskzMovedupPd2: Duplicate even-indexed double-precision (64-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp[63:0] := a[63:0]
//		tmp[127:64] := a[63:0]
//		tmp[191:128] := a[191:128]
//		tmp[255:192] := a[191:128]
//		tmp[319:256] := a[319:256] 
//		tmp[383:320] := a[319:256] 
//		tmp[447:384] := a[447:384]
//		tmp[511:448] := a[447:384]
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDDUP'. Intrinsic: '_mm512_maskz_movedup_pd'.
// Requires AVX512F.
func MaskzMovedupPd2(k Mmask8, a M512d) M512d {
	return M512d(maskzMovedupPd2(uint8(k), [8]float64(a)))
}

func maskzMovedupPd2(k uint8, a [8]float64) [8]float64


// MovedupPd: Duplicate even-indexed double-precision (64-bit) floating-point
// elements from 'a', and store the results in 'dst'. 
//
//		tmp[63:0] := a[63:0]
//		tmp[127:64] := a[63:0]
//		tmp[191:128] := a[191:128]
//		tmp[255:192] := a[191:128]
//		tmp[319:256] := a[319:256] 
//		tmp[383:320] := a[319:256] 
//		tmp[447:384] := a[447:384]
//		tmp[511:448] := a[447:384]
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVDDUP'. Intrinsic: '_mm512_movedup_pd'.
// Requires AVX512F.
func MovedupPd(a M512d) M512d {
	return M512d(movedupPd([8]float64(a)))
}

func movedupPd(a [8]float64) [8]float64


// MaskMovehdupPs: Duplicate odd-indexed single-precision (32-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp[31:0] := a[63:32] 
//		tmp[63:32] := a[63:32] 
//		tmp[95:64] := a[127:96] 
//		tmp[127:96] := a[127:96]
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVSHDUP'. Intrinsic: '_mm_mask_movehdup_ps'.
// Requires AVX512F.
func MaskMovehdupPs(src M128, k Mmask8, a M128) M128 {
	return M128(maskMovehdupPs([4]float32(src), uint8(k), [4]float32(a)))
}

func maskMovehdupPs(src [4]float32, k uint8, a [4]float32) [4]float32


// MaskzMovehdupPs: Duplicate odd-indexed single-precision (32-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp[31:0] := a[63:32] 
//		tmp[63:32] := a[63:32] 
//		tmp[95:64] := a[127:96] 
//		tmp[127:96] := a[127:96]
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVSHDUP'. Intrinsic: '_mm_maskz_movehdup_ps'.
// Requires AVX512F.
func MaskzMovehdupPs(k Mmask8, a M128) M128 {
	return M128(maskzMovehdupPs(uint8(k), [4]float32(a)))
}

func maskzMovehdupPs(k uint8, a [4]float32) [4]float32


// MaskMovehdupPs1: Duplicate odd-indexed single-precision (32-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp[31:0] := a[63:32] 
//		tmp[63:32] := a[63:32] 
//		tmp[95:64] := a[127:96] 
//		tmp[127:96] := a[127:96]
//		tmp[159:128] := a[191:160] 
//		tmp[191:160] := a[191:160] 
//		tmp[223:192] := a[255:224] 
//		tmp[255:224] := a[255:224]
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVSHDUP'. Intrinsic: '_mm256_mask_movehdup_ps'.
// Requires AVX512F.
func MaskMovehdupPs1(src M256, k Mmask8, a M256) M256 {
	return M256(maskMovehdupPs1([8]float32(src), uint8(k), [8]float32(a)))
}

func maskMovehdupPs1(src [8]float32, k uint8, a [8]float32) [8]float32


// MaskzMovehdupPs1: Duplicate odd-indexed single-precision (32-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp[31:0] := a[63:32] 
//		tmp[63:32] := a[63:32] 
//		tmp[95:64] := a[127:96] 
//		tmp[127:96] := a[127:96]
//		tmp[159:128] := a[191:160] 
//		tmp[191:160] := a[191:160] 
//		tmp[223:192] := a[255:224] 
//		tmp[255:224] := a[255:224]
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVSHDUP'. Intrinsic: '_mm256_maskz_movehdup_ps'.
// Requires AVX512F.
func MaskzMovehdupPs1(k Mmask8, a M256) M256 {
	return M256(maskzMovehdupPs1(uint8(k), [8]float32(a)))
}

func maskzMovehdupPs1(k uint8, a [8]float32) [8]float32


// MaskMovehdupPs2: Duplicate odd-indexed single-precision (32-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp[31:0] := a[63:32] 
//		tmp[63:32] := a[63:32] 
//		tmp[95:64] := a[127:96] 
//		tmp[127:96] := a[127:96]
//		tmp[159:128] := a[191:160] 
//		tmp[191:160] := a[191:160] 
//		tmp[223:192] := a[255:224] 
//		tmp[255:224] := a[255:224]
//		tmp[287:256] := a[319:288] 
//		tmp[319:288] := a[319:288] 
//		tmp[351:320] := a[383:352] 
//		tmp[383:352] := a[383:352] 
//		tmp[415:384] := a[447:416] 
//		tmp[447:416] := a[447:416] 
//		tmp[479:448] := a[511:480]
//		tmp[511:480] := a[511:480]
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVSHDUP'. Intrinsic: '_mm512_mask_movehdup_ps'.
// Requires AVX512F.
func MaskMovehdupPs2(src M512, k Mmask16, a M512) M512 {
	return M512(maskMovehdupPs2([16]float32(src), uint16(k), [16]float32(a)))
}

func maskMovehdupPs2(src [16]float32, k uint16, a [16]float32) [16]float32


// MaskzMovehdupPs2: Duplicate odd-indexed single-precision (32-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp[31:0] := a[63:32] 
//		tmp[63:32] := a[63:32] 
//		tmp[95:64] := a[127:96] 
//		tmp[127:96] := a[127:96]
//		tmp[159:128] := a[191:160] 
//		tmp[191:160] := a[191:160] 
//		tmp[223:192] := a[255:224] 
//		tmp[255:224] := a[255:224]
//		tmp[287:256] := a[319:288] 
//		tmp[319:288] := a[319:288] 
//		tmp[351:320] := a[383:352] 
//		tmp[383:352] := a[383:352] 
//		tmp[415:384] := a[447:416] 
//		tmp[447:416] := a[447:416] 
//		tmp[479:448] := a[511:480]
//		tmp[511:480] := a[511:480]
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVSHDUP'. Intrinsic: '_mm512_maskz_movehdup_ps'.
// Requires AVX512F.
func MaskzMovehdupPs2(k Mmask16, a M512) M512 {
	return M512(maskzMovehdupPs2(uint16(k), [16]float32(a)))
}

func maskzMovehdupPs2(k uint16, a [16]float32) [16]float32


// MovehdupPs: Duplicate odd-indexed single-precision (32-bit) floating-point
// elements from 'a', and store the results in 'dst'. 
//
//		dst[31:0] := a[63:32] 
//		dst[63:32] := a[63:32] 
//		dst[95:64] := a[127:96] 
//		dst[127:96] := a[127:96]
//		dst[159:128] := a[191:160] 
//		dst[191:160] := a[191:160] 
//		dst[223:192] := a[255:224] 
//		dst[255:224] := a[255:224]
//		dst[287:256] := a[319:288] 
//		dst[319:288] := a[319:288] 
//		dst[351:320] := a[383:352] 
//		dst[383:352] := a[383:352] 
//		dst[415:384] := a[447:416] 
//		dst[447:416] := a[447:416] 
//		dst[479:448] := a[511:480]
//		dst[511:480] := a[511:480]
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVSHDUP'. Intrinsic: '_mm512_movehdup_ps'.
// Requires AVX512F.
func MovehdupPs(a M512) M512 {
	return M512(movehdupPs([16]float32(a)))
}

func movehdupPs(a [16]float32) [16]float32


// MaskMoveldupPs: Duplicate even-indexed single-precision (32-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp[31:0] := a[31:0] 
//		tmp[63:32] := a[31:0] 
//		tmp[95:64] := a[95:64] 
//		tmp[127:96] := a[95:64]
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR	
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVSLDUP'. Intrinsic: '_mm_mask_moveldup_ps'.
// Requires AVX512F.
func MaskMoveldupPs(src M128, k Mmask8, a M128) M128 {
	return M128(maskMoveldupPs([4]float32(src), uint8(k), [4]float32(a)))
}

func maskMoveldupPs(src [4]float32, k uint8, a [4]float32) [4]float32


// MaskzMoveldupPs: Duplicate even-indexed single-precision (32-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp[31:0] := a[31:0] 
//		tmp[63:32] := a[31:0] 
//		tmp[95:64] := a[95:64] 
//		tmp[127:96] := a[95:64]
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVSLDUP'. Intrinsic: '_mm_maskz_moveldup_ps'.
// Requires AVX512F.
func MaskzMoveldupPs(k Mmask8, a M128) M128 {
	return M128(maskzMoveldupPs(uint8(k), [4]float32(a)))
}

func maskzMoveldupPs(k uint8, a [4]float32) [4]float32


// MaskMoveldupPs1: Duplicate even-indexed single-precision (32-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp[31:0] := a[31:0] 
//		tmp[63:32] := a[31:0] 
//		tmp[95:64] := a[95:64] 
//		tmp[127:96] := a[95:64]
//		tmp[159:128] := a[159:128] 
//		tmp[191:160] := a[159:128] 
//		tmp[223:192] := a[223:192] 
//		tmp[255:224] := a[223:192]
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVSLDUP'. Intrinsic: '_mm256_mask_moveldup_ps'.
// Requires AVX512F.
func MaskMoveldupPs1(src M256, k Mmask8, a M256) M256 {
	return M256(maskMoveldupPs1([8]float32(src), uint8(k), [8]float32(a)))
}

func maskMoveldupPs1(src [8]float32, k uint8, a [8]float32) [8]float32


// MaskzMoveldupPs1: Duplicate even-indexed single-precision (32-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp[31:0] := a[31:0] 
//		tmp[63:32] := a[31:0] 
//		tmp[95:64] := a[95:64] 
//		tmp[127:96] := a[95:64]
//		tmp[159:128] := a[159:128] 
//		tmp[191:160] := a[159:128] 
//		tmp[223:192] := a[223:192] 
//		tmp[255:224] := a[223:192]
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVSLDUP'. Intrinsic: '_mm256_maskz_moveldup_ps'.
// Requires AVX512F.
func MaskzMoveldupPs1(k Mmask8, a M256) M256 {
	return M256(maskzMoveldupPs1(uint8(k), [8]float32(a)))
}

func maskzMoveldupPs1(k uint8, a [8]float32) [8]float32


// MaskMoveldupPs2: Duplicate even-indexed single-precision (32-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp[31:0] := a[31:0] 
//		tmp[63:32] := a[31:0] 
//		tmp[95:64] := a[95:64] 
//		tmp[127:96] := a[95:64]
//		tmp[159:128] := a[159:128] 
//		tmp[191:160] := a[159:128] 
//		tmp[223:192] := a[223:192] 
//		tmp[255:224] := a[223:192]
//		tmp[287:256] := a[287:256] 
//		tmp[319:288] := a[287:256] 
//		tmp[351:320] := a[351:320] 
//		tmp[383:352] := a[351:320] 
//		tmp[415:384] := a[415:384] 
//		tmp[447:416] := a[415:384] 
//		tmp[479:448] := a[479:448]
//		tmp[511:480] := a[479:448]
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR	
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVSLDUP'. Intrinsic: '_mm512_mask_moveldup_ps'.
// Requires AVX512F.
func MaskMoveldupPs2(src M512, k Mmask16, a M512) M512 {
	return M512(maskMoveldupPs2([16]float32(src), uint16(k), [16]float32(a)))
}

func maskMoveldupPs2(src [16]float32, k uint16, a [16]float32) [16]float32


// MaskzMoveldupPs2: Duplicate even-indexed single-precision (32-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp[31:0] := a[31:0] 
//		tmp[63:32] := a[31:0] 
//		tmp[95:64] := a[95:64] 
//		tmp[127:96] := a[95:64]
//		tmp[159:128] := a[159:128] 
//		tmp[191:160] := a[159:128] 
//		tmp[223:192] := a[223:192] 
//		tmp[255:224] := a[223:192]
//		tmp[287:256] := a[287:256] 
//		tmp[319:288] := a[287:256] 
//		tmp[351:320] := a[351:320] 
//		tmp[383:352] := a[351:320] 
//		tmp[415:384] := a[415:384] 
//		tmp[447:416] := a[415:384] 
//		tmp[479:448] := a[479:448]
//		tmp[511:480] := a[479:448]
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVSLDUP'. Intrinsic: '_mm512_maskz_moveldup_ps'.
// Requires AVX512F.
func MaskzMoveldupPs2(k Mmask16, a M512) M512 {
	return M512(maskzMoveldupPs2(uint16(k), [16]float32(a)))
}

func maskzMoveldupPs2(k uint16, a [16]float32) [16]float32


// MoveldupPs: Duplicate even-indexed single-precision (32-bit) floating-point
// elements from 'a', and store the results in 'dst'. 
//
//		dst[31:0] := a[31:0] 
//		dst[63:32] := a[31:0] 
//		dst[95:64] := a[95:64] 
//		dst[127:96] := a[95:64]
//		dst[159:128] := a[159:128] 
//		dst[191:160] := a[159:128] 
//		dst[223:192] := a[223:192] 
//		dst[255:224] := a[223:192]
//		dst[287:256] := a[287:256] 
//		dst[319:288] := a[287:256] 
//		dst[351:320] := a[351:320] 
//		dst[383:352] := a[351:320] 
//		dst[415:384] := a[415:384] 
//		dst[447:416] := a[415:384] 
//		dst[479:448] := a[479:448]
//		dst[511:480] := a[479:448]
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVSLDUP'. Intrinsic: '_mm512_moveldup_ps'.
// Requires AVX512F.
func MoveldupPs(a M512) M512 {
	return M512(moveldupPs([16]float32(a)))
}

func moveldupPs(a [16]float32) [16]float32


// MaskMulEpi32: Multiply the low 32-bit integers from each packed 64-bit
// element in 'a' and 'b', and store the signed 64-bit results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMULDQ'. Intrinsic: '_mm_mask_mul_epi32'.
// Requires AVX512F.
func MaskMulEpi32(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskMulEpi32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskMulEpi32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzMulEpi32: Multiply the low 32-bit integers from each packed 64-bit
// element in 'a' and 'b', and store the signed 64-bit results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMULDQ'. Intrinsic: '_mm_maskz_mul_epi32'.
// Requires AVX512F.
func MaskzMulEpi32(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzMulEpi32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzMulEpi32(k uint8, a [16]byte, b [16]byte) [16]byte


// MaskMulEpi321: Multiply the low 32-bit integers from each packed 64-bit
// element in 'a' and 'b', and store the signed 64-bit results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULDQ'. Intrinsic: '_mm256_mask_mul_epi32'.
// Requires AVX512F.
func MaskMulEpi321(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskMulEpi321([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskMulEpi321(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzMulEpi321: Multiply the low 32-bit integers from each packed 64-bit
// element in 'a' and 'b', and store the signed 64-bit results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULDQ'. Intrinsic: '_mm256_maskz_mul_epi32'.
// Requires AVX512F.
func MaskzMulEpi321(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzMulEpi321(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzMulEpi321(k uint8, a [32]byte, b [32]byte) [32]byte


// MaskMulEpi322: Multiply the low 32-bit integers from each packed 64-bit
// element in 'a' and 'b', and store the signed 64-bit results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMULDQ'. Intrinsic: '_mm512_mask_mul_epi32'.
// Requires AVX512F.
func MaskMulEpi322(src M512i, k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskMulEpi322([64]byte(src), uint8(k), [64]byte(a), [64]byte(b)))
}

func maskMulEpi322(src [64]byte, k uint8, a [64]byte, b [64]byte) [64]byte


// MaskzMulEpi322: Multiply the low 32-bit integers from each packed 64-bit
// element in 'a' and 'b', and store the signed 64-bit results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMULDQ'. Intrinsic: '_mm512_maskz_mul_epi32'.
// Requires AVX512F.
func MaskzMulEpi322(k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskzMulEpi322(uint8(k), [64]byte(a), [64]byte(b)))
}

func maskzMulEpi322(k uint8, a [64]byte, b [64]byte) [64]byte


// MulEpi32: Multiply the low 32-bit integers from each packed 64-bit element
// in 'a' and 'b', and store the signed 64-bit results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := a[i+31:i] * b[i+31:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMULDQ'. Intrinsic: '_mm512_mul_epi32'.
// Requires AVX512F.
func MulEpi32(a M512i, b M512i) M512i {
	return M512i(mulEpi32([64]byte(a), [64]byte(b)))
}

func mulEpi32(a [64]byte, b [64]byte) [64]byte


// MaskMulEpu32: Multiply the low unsigned 32-bit integers from each packed
// 64-bit element in 'a' and 'b', and store the unsigned 64-bit results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMULUDQ'. Intrinsic: '_mm_mask_mul_epu32'.
// Requires AVX512F.
func MaskMulEpu32(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskMulEpu32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskMulEpu32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzMulEpu32: Multiply the low unsigned 32-bit integers from each packed
// 64-bit element in 'a' and 'b', and store the unsigned 64-bit results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMULUDQ'. Intrinsic: '_mm_maskz_mul_epu32'.
// Requires AVX512F.
func MaskzMulEpu32(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzMulEpu32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzMulEpu32(k uint8, a [16]byte, b [16]byte) [16]byte


// MaskMulEpu321: Multiply the low unsigned 32-bit integers from each packed
// 64-bit element in 'a' and 'b', and store the unsigned 64-bit results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULUDQ'. Intrinsic: '_mm256_mask_mul_epu32'.
// Requires AVX512F.
func MaskMulEpu321(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskMulEpu321([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskMulEpu321(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzMulEpu321: Multiply the low unsigned 32-bit integers from each packed
// 64-bit element in 'a' and 'b', and store the unsigned 64-bit results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULUDQ'. Intrinsic: '_mm256_maskz_mul_epu32'.
// Requires AVX512F.
func MaskzMulEpu321(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzMulEpu321(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzMulEpu321(k uint8, a [32]byte, b [32]byte) [32]byte


// MaskMulEpu322: Multiply the low unsigned 32-bit integers from each packed
// 64-bit element in 'a' and 'b', and store the unsigned 64-bit results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMULUDQ'. Intrinsic: '_mm512_mask_mul_epu32'.
// Requires AVX512F.
func MaskMulEpu322(src M512i, k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskMulEpu322([64]byte(src), uint8(k), [64]byte(a), [64]byte(b)))
}

func maskMulEpu322(src [64]byte, k uint8, a [64]byte, b [64]byte) [64]byte


// MaskzMulEpu322: Multiply the low unsigned 32-bit integers from each packed
// 64-bit element in 'a' and 'b', and store the unsigned 64-bit results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMULUDQ'. Intrinsic: '_mm512_maskz_mul_epu32'.
// Requires AVX512F.
func MaskzMulEpu322(k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskzMulEpu322(uint8(k), [64]byte(a), [64]byte(b)))
}

func maskzMulEpu322(k uint8, a [64]byte, b [64]byte) [64]byte


// MulEpu32: Multiply the low unsigned 32-bit integers from each packed 64-bit
// element in 'a' and 'b', and store the unsigned 64-bit results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := a[i+31:i] * b[i+31:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMULUDQ'. Intrinsic: '_mm512_mul_epu32'.
// Requires AVX512F.
func MulEpu32(a M512i, b M512i) M512i {
	return M512i(mulEpu32([64]byte(a), [64]byte(b)))
}

func mulEpu32(a [64]byte, b [64]byte) [64]byte


// MaskMulPd: Multiply packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] * b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMULPD'. Intrinsic: '_mm_mask_mul_pd'.
// Requires AVX512F.
func MaskMulPd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskMulPd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskMulPd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzMulPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] * b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMULPD'. Intrinsic: '_mm_maskz_mul_pd'.
// Requires AVX512F.
func MaskzMulPd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzMulPd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzMulPd(k uint8, a [2]float64, b [2]float64) [2]float64


// MaskMulPd1: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] * b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMULPD'. Intrinsic: '_mm256_mask_mul_pd'.
// Requires AVX512F.
func MaskMulPd1(src M256d, k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskMulPd1([4]float64(src), uint8(k), [4]float64(a), [4]float64(b)))
}

func maskMulPd1(src [4]float64, k uint8, a [4]float64, b [4]float64) [4]float64


// MaskzMulPd1: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] * b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMULPD'. Intrinsic: '_mm256_maskz_mul_pd'.
// Requires AVX512F.
func MaskzMulPd1(k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskzMulPd1(uint8(k), [4]float64(a), [4]float64(b)))
}

func maskzMulPd1(k uint8, a [4]float64, b [4]float64) [4]float64


// MaskzMulPd2: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] * b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMULPD'. Intrinsic: '_mm512_maskz_mul_pd'.
// Requires AVX512F.
func MaskzMulPd2(k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskzMulPd2(uint8(k), [8]float64(a), [8]float64(b)))
}

func maskzMulPd2(k uint8, a [8]float64, b [8]float64) [8]float64


// MaskMulPs: Multiply packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set).  RM. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMULPS'. Intrinsic: '_mm_mask_mul_ps'.
// Requires AVX512F.
func MaskMulPs(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskMulPs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskMulPs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzMulPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMULPS'. Intrinsic: '_mm_maskz_mul_ps'.
// Requires AVX512F.
func MaskzMulPs(k Mmask8, a M128, b M128) M128 {
	return M128(maskzMulPs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzMulPs(k uint8, a [4]float32, b [4]float32) [4]float32


// MaskMulPs1: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set).
// RM. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMULPS'. Intrinsic: '_mm256_mask_mul_ps'.
// Requires AVX512F.
func MaskMulPs1(src M256, k Mmask8, a M256, b M256) M256 {
	return M256(maskMulPs1([8]float32(src), uint8(k), [8]float32(a), [8]float32(b)))
}

func maskMulPs1(src [8]float32, k uint8, a [8]float32, b [8]float32) [8]float32


// MaskzMulPs1: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMULPS'. Intrinsic: '_mm256_maskz_mul_ps'.
// Requires AVX512F.
func MaskzMulPs1(k Mmask8, a M256, b M256) M256 {
	return M256(maskzMulPs1(uint8(k), [8]float32(a), [8]float32(b)))
}

func maskzMulPs1(k uint8, a [8]float32, b [8]float32) [8]float32


// MaskzMulPs2: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMULPS'. Intrinsic: '_mm512_maskz_mul_ps'.
// Requires AVX512F.
func MaskzMulPs2(k Mmask16, a M512, b M512) M512 {
	return M512(maskzMulPs2(uint16(k), [16]float32(a), [16]float32(b)))
}

func maskzMulPs2(k uint16, a [16]float32, b [16]float32) [16]float32


// MaskzMulRoundPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] * b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMULPD'. Intrinsic: '_mm512_maskz_mul_round_pd'.
// Requires AVX512F.
func MaskzMulRoundPd(k Mmask8, a M512d, b M512d, rounding int) M512d {
	return M512d(maskzMulRoundPd(uint8(k), [8]float64(a), [8]float64(b), rounding))
}

func maskzMulRoundPd(k uint8, a [8]float64, b [8]float64, rounding int) [8]float64


// MaskzMulRoundPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VMULPS'. Intrinsic: '_mm512_maskz_mul_round_ps'.
// Requires AVX512F.
func MaskzMulRoundPs(k Mmask16, a M512, b M512, rounding int) M512 {
	return M512(maskzMulRoundPs(uint16(k), [16]float32(a), [16]float32(b), rounding))
}

func maskzMulRoundPs(k uint16, a [16]float32, b [16]float32, rounding int) [16]float32


// MaskMulRoundSd: Multiply the lower double-precision (64-bit) floating-point
// element in 'a' and 'b', store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'src' when mask bit 0 is not set),
// and copy the upper element from 'a' to the upper element of 'dst'.
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := a[63:0] * b[63:0]
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMULSD'. Intrinsic: '_mm_mask_mul_round_sd'.
// Requires AVX512F.
func MaskMulRoundSd(src M128d, k Mmask8, a M128d, b M128d, rounding int) M128d {
	return M128d(maskMulRoundSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), rounding))
}

func maskMulRoundSd(src [2]float64, k uint8, a [2]float64, b [2]float64, rounding int) [2]float64


// MaskzMulRoundSd: Multiply the lower double-precision (64-bit) floating-point
// element in 'a' and 'b', store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper element from 'a' to the upper element of 'dst'.
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := a[63:0] * b[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMULSD'. Intrinsic: '_mm_maskz_mul_round_sd'.
// Requires AVX512F.
func MaskzMulRoundSd(k Mmask8, a M128d, b M128d, rounding int) M128d {
	return M128d(maskzMulRoundSd(uint8(k), [2]float64(a), [2]float64(b), rounding))
}

func maskzMulRoundSd(k uint8, a [2]float64, b [2]float64, rounding int) [2]float64


// MulRoundSd: Multiply the lower double-precision (64-bit) floating-point
// element in 'a' and 'b', store the result in the lower element of 'dst', and
// copy the upper element from 'a' to the upper element of 'dst'.
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := a[63:0] * b[63:0]
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMULSD'. Intrinsic: '_mm_mul_round_sd'.
// Requires AVX512F.
func MulRoundSd(a M128d, b M128d, rounding int) M128d {
	return M128d(mulRoundSd([2]float64(a), [2]float64(b), rounding))
}

func mulRoundSd(a [2]float64, b [2]float64, rounding int) [2]float64


// MaskMulRoundSs: Multiply the lower single-precision (32-bit) floating-point
// element in 'a' and 'b', store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'src' when mask bit 0 is not set),
// and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'.
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := a[31:0] * b[31:0]
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMULSS'. Intrinsic: '_mm_mask_mul_round_ss'.
// Requires AVX512F.
func MaskMulRoundSs(src M128, k Mmask8, a M128, b M128, rounding int) M128 {
	return M128(maskMulRoundSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), rounding))
}

func maskMulRoundSs(src [4]float32, k uint8, a [4]float32, b [4]float32, rounding int) [4]float32


// MaskzMulRoundSs: Multiply the lower single-precision (32-bit) floating-point
// element in 'a' and 'b', store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'.
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := a[31:0] * b[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMULSS'. Intrinsic: '_mm_maskz_mul_round_ss'.
// Requires AVX512F.
func MaskzMulRoundSs(k Mmask8, a M128, b M128, rounding int) M128 {
	return M128(maskzMulRoundSs(uint8(k), [4]float32(a), [4]float32(b), rounding))
}

func maskzMulRoundSs(k uint8, a [4]float32, b [4]float32, rounding int) [4]float32


// MulRoundSs: Multiply the lower single-precision (32-bit) floating-point
// element in 'a' and 'b', store the result in the lower element of 'dst', and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'.
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := a[31:0] * b[31:0]
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMULSS'. Intrinsic: '_mm_mul_round_ss'.
// Requires AVX512F.
func MulRoundSs(a M128, b M128, rounding int) M128 {
	return M128(mulRoundSs([4]float32(a), [4]float32(b), rounding))
}

func mulRoundSs(a [4]float32, b [4]float32, rounding int) [4]float32


// MaskMulSd: Multiply the lower double-precision (64-bit) floating-point
// element in 'a' and 'b', store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'src' when mask bit 0 is not set),
// and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := a[63:0] * b[63:0]
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMULSD'. Intrinsic: '_mm_mask_mul_sd'.
// Requires AVX512F.
func MaskMulSd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskMulSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskMulSd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzMulSd: Multiply the lower double-precision (64-bit) floating-point
// element in 'a' and 'b', store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := a[63:0] * b[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMULSD'. Intrinsic: '_mm_maskz_mul_sd'.
// Requires AVX512F.
func MaskzMulSd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzMulSd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzMulSd(k uint8, a [2]float64, b [2]float64) [2]float64


// MaskMulSs: Multiply the lower single-precision (32-bit) floating-point
// element in 'a' and 'b', store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'src' when mask bit 0 is not set),
// and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'. 
//
//		IF k[0]
//			dst[31:0] := a[31:0] * b[31:0]
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMULSS'. Intrinsic: '_mm_mask_mul_ss'.
// Requires AVX512F.
func MaskMulSs(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskMulSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskMulSs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzMulSs: Multiply the lower single-precision (32-bit) floating-point
// element in 'a' and 'b', store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := a[31:0] * b[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMULSS'. Intrinsic: '_mm_maskz_mul_ss'.
// Requires AVX512F.
func MaskzMulSs(k Mmask8, a M128, b M128) M128 {
	return M128(maskzMulSs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzMulSs(k uint8, a [4]float32, b [4]float32) [4]float32


// MaskMulloEpi32: Multiply the packed 32-bit integers in 'a' and 'b',
// producing intermediate 64-bit integers, and store the low 32 bits of the
// intermediate integers in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				tmp[63:0] := a[i+31:i] * b[i+31:i]
//				dst[i+31:i] := tmp[31:0]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMULLD'. Intrinsic: '_mm_mask_mullo_epi32'.
// Requires AVX512F.
func MaskMulloEpi32(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskMulloEpi32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskMulloEpi32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzMulloEpi32: Multiply the packed 32-bit integers in 'a' and 'b',
// producing intermediate 64-bit integers, and store the low 32 bits of the
// intermediate integers in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				tmp[63:0] := a[i+31:i] * b[i+31:i]
//				dst[i+31:i] := tmp[31:0]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMULLD'. Intrinsic: '_mm_maskz_mullo_epi32'.
// Requires AVX512F.
func MaskzMulloEpi32(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzMulloEpi32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzMulloEpi32(k uint8, a [16]byte, b [16]byte) [16]byte


// MaskMulloEpi321: Multiply the packed 32-bit integers in 'a' and 'b',
// producing intermediate 64-bit integers, and store the low 32 bits of the
// intermediate integers in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				tmp[63:0] := a[i+31:i] * b[i+31:i]
//				dst[i+31:i] := tmp[31:0]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULLD'. Intrinsic: '_mm256_mask_mullo_epi32'.
// Requires AVX512F.
func MaskMulloEpi321(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskMulloEpi321([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskMulloEpi321(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzMulloEpi321: Multiply the packed 32-bit integers in 'a' and 'b',
// producing intermediate 64-bit integers, and store the low 32 bits of the
// intermediate integers in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				tmp[63:0] := a[i+31:i] * b[i+31:i]
//				dst[i+31:i] := tmp[31:0]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULLD'. Intrinsic: '_mm256_maskz_mullo_epi32'.
// Requires AVX512F.
func MaskzMulloEpi321(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzMulloEpi321(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzMulloEpi321(k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzMulloEpi322: Multiply the packed 32-bit integers in 'a' and 'b',
// producing intermediate 64-bit integers, and store the low 32 bits of the
// intermediate integers in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				tmp[63:0] := a[i+31:i] * b[i+31:i]
//				dst[i+31:i] := tmp[31:0]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPMULLD'. Intrinsic: '_mm512_maskz_mullo_epi32'.
// Requires AVX512F.
func MaskzMulloEpi322(k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskzMulloEpi322(uint16(k), [64]byte(a), [64]byte(b)))
}

func maskzMulloEpi322(k uint16, a [64]byte, b [64]byte) [64]byte


// MaskMulloxEpi64: Multiplies elements in packed 64-bit integer vectors 'a'
// and 'b' together, storing the lower 64 bits of the result in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] * b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_mullox_epi64'.
// Requires AVX512F.
func MaskMulloxEpi64(src M512i, k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskMulloxEpi64([64]byte(src), uint8(k), [64]byte(a), [64]byte(b)))
}

func maskMulloxEpi64(src [64]byte, k uint8, a [64]byte, b [64]byte) [64]byte


// MulloxEpi64: Multiplies elements in packed 64-bit integer vectors 'a' and
// 'b' together, storing the lower 64 bits of the result in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := a[i+63:i] * b[i+63:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mullox_epi64'.
// Requires AVX512F.
func MulloxEpi64(a M512i, b M512i) M512i {
	return M512i(mulloxEpi64([64]byte(a), [64]byte(b)))
}

func mulloxEpi64(a [64]byte, b [64]byte) [64]byte


// MaskNearbyintPd: Rounds each packed double-precision (64-bit) floating-point
// element in 'a' to the nearest integer value and stores the results as packed
// double-precision floating-point elements in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := NearbyInt(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_nearbyint_pd'.
// Requires AVX512F.
func MaskNearbyintPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskNearbyintPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskNearbyintPd(src [8]float64, k uint8, a [8]float64) [8]float64


// NearbyintPd: Rounds each packed double-precision (64-bit) floating-point
// element in 'a' to the nearest integer value and stores the results as packed
// double-precision floating-point elements in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := NearbyInt(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_nearbyint_pd'.
// Requires AVX512F.
func NearbyintPd(a M512d) M512d {
	return M512d(nearbyintPd([8]float64(a)))
}

func nearbyintPd(a [8]float64) [8]float64


// MaskNearbyintPs: Rounds each packed single-precision (32-bit) floating-point
// element in 'a' to the nearest integer value and stores the results as packed
// double-precision floating-point elements in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := NearbyInt(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_nearbyint_ps'.
// Requires AVX512F.
func MaskNearbyintPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskNearbyintPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskNearbyintPs(src [16]float32, k uint16, a [16]float32) [16]float32


// NearbyintPs: Rounds each packed single-precision (32-bit) floating-point
// element in 'a' to the nearest integer value and stores the results as packed
// double-precision floating-point elements in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := NearbyInt(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_nearbyint_ps'.
// Requires AVX512F.
func NearbyintPs(a M512) M512 {
	return M512(nearbyintPs([16]float32(a)))
}

func nearbyintPs(a [16]float32) [16]float32


// MaskOrEpi32: Compute the bitwise OR of packed 32-bit integers in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] OR b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPORD'. Intrinsic: '_mm_mask_or_epi32'.
// Requires AVX512F.
func MaskOrEpi32(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskOrEpi32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskOrEpi32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzOrEpi32: Compute the bitwise OR of packed 32-bit integers in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] OR b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPORD'. Intrinsic: '_mm_maskz_or_epi32'.
// Requires AVX512F.
func MaskzOrEpi32(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzOrEpi32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzOrEpi32(k uint8, a [16]byte, b [16]byte) [16]byte


// MaskOrEpi321: Compute the bitwise OR of packed 32-bit integers in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] OR b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPORD'. Intrinsic: '_mm256_mask_or_epi32'.
// Requires AVX512F.
func MaskOrEpi321(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskOrEpi321([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskOrEpi321(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzOrEpi321: Compute the bitwise OR of packed 32-bit integers in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] OR b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPORD'. Intrinsic: '_mm256_maskz_or_epi32'.
// Requires AVX512F.
func MaskzOrEpi321(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzOrEpi321(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzOrEpi321(k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzOrEpi322: Compute the bitwise OR of packed 32-bit integers in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] OR b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPORD'. Intrinsic: '_mm512_maskz_or_epi32'.
// Requires AVX512F.
func MaskzOrEpi322(k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskzOrEpi322(uint16(k), [64]byte(a), [64]byte(b)))
}

func maskzOrEpi322(k uint16, a [64]byte, b [64]byte) [64]byte


// MaskOrEpi64: Compute the bitwise OR of packed 64-bit integers in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] OR b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPORQ'. Intrinsic: '_mm_mask_or_epi64'.
// Requires AVX512F.
func MaskOrEpi64(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskOrEpi64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskOrEpi64(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzOrEpi64: Compute the bitwise OR of packed 64-bit integers in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] OR b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPORQ'. Intrinsic: '_mm_maskz_or_epi64'.
// Requires AVX512F.
func MaskzOrEpi64(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzOrEpi64(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzOrEpi64(k uint8, a [16]byte, b [16]byte) [16]byte


// MaskOrEpi641: Compute the bitwise OR of packed 64-bit integers in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] OR b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPORQ'. Intrinsic: '_mm256_mask_or_epi64'.
// Requires AVX512F.
func MaskOrEpi641(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskOrEpi641([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskOrEpi641(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzOrEpi641: Compute the bitwise OR of packed 64-bit integers in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] OR b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPORQ'. Intrinsic: '_mm256_maskz_or_epi64'.
// Requires AVX512F.
func MaskzOrEpi641(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzOrEpi641(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzOrEpi641(k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzOrEpi642: Compute the bitwise OR of packed 64-bit integers in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] OR b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPORQ'. Intrinsic: '_mm512_maskz_or_epi64'.
// Requires AVX512F.
func MaskzOrEpi642(k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskzOrEpi642(uint8(k), [64]byte(a), [64]byte(b)))
}

func maskzOrEpi642(k uint8, a [64]byte, b [64]byte) [64]byte


// MaskPermutePd: Shuffle double-precision (64-bit) floating-point elements in
// 'a' using the control in 'imm8', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		IF (imm8[0] == 0) tmp_dst[63:0] := a[63:0]
//		IF (imm8[0] == 1) tmp_dst[63:0] := a[127:64]
//		IF (imm8[1] == 0) tmp_dst[127:64] := a[63:0]
//		IF (imm8[1] == 1) tmp_dst[127:64] := a[127:64]
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm_mask_permute_pd'.
// Requires AVX512F.
func MaskPermutePd(src M128d, k Mmask8, a M128d, imm8 int) M128d {
	return M128d(maskPermutePd([2]float64(src), uint8(k), [2]float64(a), imm8))
}

func maskPermutePd(src [2]float64, k uint8, a [2]float64, imm8 int) [2]float64


// MaskzPermutePd: Shuffle double-precision (64-bit) floating-point elements in
// 'a' using the control in 'imm8', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		IF (imm8[0] == 0) tmp_dst[63:0] := a[63:0]
//		IF (imm8[0] == 1) tmp_dst[63:0] := a[127:64]
//		IF (imm8[1] == 0) tmp_dst[127:64] := a[63:0]
//		IF (imm8[1] == 1) tmp_dst[127:64] := a[127:64]
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm_maskz_permute_pd'.
// Requires AVX512F.
func MaskzPermutePd(k Mmask8, a M128d, imm8 int) M128d {
	return M128d(maskzPermutePd(uint8(k), [2]float64(a), imm8))
}

func maskzPermutePd(k uint8, a [2]float64, imm8 int) [2]float64


// MaskPermutePd1: Shuffle double-precision (64-bit) floating-point elements in
// 'a' within 128-bit lanes using the control in 'imm8', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		IF (imm8[0] == 0) tmp_dst[63:0] := a[63:0]
//		IF (imm8[0] == 1) tmp_dst[63:0] := a[127:64]
//		IF (imm8[1] == 0) tmp_dst[127:64] := a[63:0]
//		IF (imm8[1] == 1) tmp_dst[127:64] := a[127:64]
//		IF (imm8[2] == 0) tmp_dst[191:128] := a[191:128]
//		IF (imm8[2] == 1) tmp_dst[191:128] := a[255:192]
//		IF (imm8[3] == 0) tmp_dst[255:192] := a[191:128]
//		IF (imm8[3] == 1) tmp_dst[255:192] := a[255:192]
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm256_mask_permute_pd'.
// Requires AVX512F.
func MaskPermutePd1(src M256d, k Mmask8, a M256d, imm8 int) M256d {
	return M256d(maskPermutePd1([4]float64(src), uint8(k), [4]float64(a), imm8))
}

func maskPermutePd1(src [4]float64, k uint8, a [4]float64, imm8 int) [4]float64


// MaskzPermutePd1: Shuffle double-precision (64-bit) floating-point elements
// in 'a' within 128-bit lanes using the control in 'imm8', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		IF (imm8[0] == 0) tmp_dst[63:0] := a[63:0]
//		IF (imm8[0] == 1) tmp_dst[63:0] := a[127:64]
//		IF (imm8[1] == 0) tmp_dst[127:64] := a[63:0]
//		IF (imm8[1] == 1) tmp_dst[127:64] := a[127:64]
//		IF (imm8[2] == 0) tmp_dst[191:128] := a[191:128]
//		IF (imm8[2] == 1) tmp_dst[191:128] := a[255:192]
//		IF (imm8[3] == 0) tmp_dst[255:192] := a[191:128]
//		IF (imm8[3] == 1) tmp_dst[255:192] := a[255:192]
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm256_maskz_permute_pd'.
// Requires AVX512F.
func MaskzPermutePd1(k Mmask8, a M256d, imm8 int) M256d {
	return M256d(maskzPermutePd1(uint8(k), [4]float64(a), imm8))
}

func maskzPermutePd1(k uint8, a [4]float64, imm8 int) [4]float64


// MaskPermutePd2: Shuffle double-precision (64-bit) floating-point elements in
// 'a' within 128-bit lanes using the control in 'imm8', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		IF (imm8[0] == 0) tmp_dst[63:0] := a[63:0]
//		IF (imm8[0] == 1) tmp_dst[63:0] := a[127:64]
//		IF (imm8[1] == 0) tmp_dst[127:64] := a[63:0]
//		IF (imm8[1] == 1) tmp_dst[127:64] := a[127:64]
//		IF (imm8[2] == 0) tmp_dst[191:128] := a[191:128]
//		IF (imm8[2] == 1) tmp_dst[191:128] := a[255:192]
//		IF (imm8[3] == 0) tmp_dst[255:192] := a[191:128]
//		IF (imm8[3] == 1) tmp_dst[255:192] := a[255:192]
//		IF (imm8[4] == 0) tmp_dst[319:256] := a[319:256]
//		IF (imm8[4] == 1) tmp_dst[319:256] := a[383:320]
//		IF (imm8[5] == 0) tmp_dst[383:320] := a[319:256]
//		IF (imm8[5] == 1) tmp_dst[383:320] := a[383:320]
//		IF (imm8[6] == 0) tmp_dst[447:384] := a[447:384]
//		IF (imm8[6] == 1) tmp_dst[447:384] := a[511:448]
//		IF (imm8[7] == 0) tmp_dst[511:448] := a[447:384]
//		IF (imm8[7] == 1) tmp_dst[511:448] := a[511:448]
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm512_mask_permute_pd'.
// Requires AVX512F.
func MaskPermutePd2(src M512d, k Mmask8, a M512d, imm8 int) M512d {
	return M512d(maskPermutePd2([8]float64(src), uint8(k), [8]float64(a), imm8))
}

func maskPermutePd2(src [8]float64, k uint8, a [8]float64, imm8 int) [8]float64


// MaskzPermutePd2: Shuffle double-precision (64-bit) floating-point elements
// in 'a' within 128-bit lanes using the control in 'imm8', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		IF (imm8[0] == 0) tmp_dst[63:0] := a[63:0]
//		IF (imm8[0] == 1) tmp_dst[63:0] := a[127:64]
//		IF (imm8[1] == 0) tmp_dst[127:64] := a[63:0]
//		IF (imm8[1] == 1) tmp_dst[127:64] := a[127:64]
//		IF (imm8[2] == 0) tmp_dst[191:128] := a[191:128]
//		IF (imm8[2] == 1) tmp_dst[191:128] := a[255:192]
//		IF (imm8[3] == 0) tmp_dst[255:192] := a[191:128]
//		IF (imm8[3] == 1) tmp_dst[255:192] := a[255:192]
//		IF (imm8[4] == 0) tmp_dst[319:256] := a[319:256]
//		IF (imm8[4] == 1) tmp_dst[319:256] := a[383:320]
//		IF (imm8[5] == 0) tmp_dst[383:320] := a[319:256]
//		IF (imm8[5] == 1) tmp_dst[383:320] := a[383:320]
//		IF (imm8[6] == 0) tmp_dst[447:384] := a[447:384]
//		IF (imm8[6] == 1) tmp_dst[447:384] := a[511:448]
//		IF (imm8[7] == 0) tmp_dst[511:448] := a[447:384]
//		IF (imm8[7] == 1) tmp_dst[511:448] := a[511:448]
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm512_maskz_permute_pd'.
// Requires AVX512F.
func MaskzPermutePd2(k Mmask8, a M512d, imm8 int) M512d {
	return M512d(maskzPermutePd2(uint8(k), [8]float64(a), imm8))
}

func maskzPermutePd2(k uint8, a [8]float64, imm8 int) [8]float64


// PermutePd: Shuffle double-precision (64-bit) floating-point elements in 'a'
// within 128-bit lanes using the control in 'imm8', and store the results in
// 'dst'. 
//
//		IF (imm8[0] == 0) dst[63:0] := a[63:0]
//		IF (imm8[0] == 1) dst[63:0] := a[127:64]
//		IF (imm8[1] == 0) dst[127:64] := a[63:0]
//		IF (imm8[1] == 1) dst[127:64] := a[127:64]
//		IF (imm8[2] == 0) dst[191:128] := a[191:128]
//		IF (imm8[2] == 1) dst[191:128] := a[255:192]
//		IF (imm8[3] == 0) dst[255:192] := a[191:128]
//		IF (imm8[3] == 1) dst[255:192] := a[255:192]
//		IF (imm8[4] == 0) dst[319:256] := a[319:256]
//		IF (imm8[4] == 1) dst[319:256] := a[383:320]
//		IF (imm8[5] == 0) dst[383:320] := a[319:256]
//		IF (imm8[5] == 1) dst[383:320] := a[383:320]
//		IF (imm8[6] == 0) dst[447:384] := a[447:384]
//		IF (imm8[6] == 1) dst[447:384] := a[511:448]
//		IF (imm8[7] == 0) dst[511:448] := a[447:384]
//		IF (imm8[7] == 1) dst[511:448] := a[511:448]
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm512_permute_pd'.
// Requires AVX512F.
func PermutePd(a M512d, imm8 int) M512d {
	return M512d(permutePd([8]float64(a), imm8))
}

func permutePd(a [8]float64, imm8 int) [8]float64


// MaskPermutePs: Shuffle single-precision (32-bit) floating-point elements in
// 'a' using the control in 'imm8', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm_mask_permute_ps'.
// Requires AVX512F.
func MaskPermutePs(src M128, k Mmask8, a M128, imm8 int) M128 {
	return M128(maskPermutePs([4]float32(src), uint8(k), [4]float32(a), imm8))
}

func maskPermutePs(src [4]float32, k uint8, a [4]float32, imm8 int) [4]float32


// MaskzPermutePs: Shuffle single-precision (32-bit) floating-point elements in
// 'a' using the control in 'imm8', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm_maskz_permute_ps'.
// Requires AVX512F.
func MaskzPermutePs(k Mmask8, a M128, imm8 int) M128 {
	return M128(maskzPermutePs(uint8(k), [4]float32(a), imm8))
}

func maskzPermutePs(k uint8, a [4]float32, imm8 int) [4]float32


// MaskPermutePs1: Shuffle single-precision (32-bit) floating-point elements in
// 'a' within 128-bit lanes using the control in 'imm8', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		tmp_dst[223:192] := SELECT4(a[255:128], imm8[5:4])
//		tmp_dst[255:224] := SELECT4(a[255:128], imm8[7:6])
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm256_mask_permute_ps'.
// Requires AVX512F.
func MaskPermutePs1(src M256, k Mmask8, a M256, imm8 int) M256 {
	return M256(maskPermutePs1([8]float32(src), uint8(k), [8]float32(a), imm8))
}

func maskPermutePs1(src [8]float32, k uint8, a [8]float32, imm8 int) [8]float32


// MaskzPermutePs1: Shuffle single-precision (32-bit) floating-point elements
// in 'a' within 128-bit lanes using the control in 'imm8', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		tmp_dst[223:192] := SELECT4(a[255:128], imm8[5:4])
//		tmp_dst[255:224] := SELECT4(a[255:128], imm8[7:6])
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm256_maskz_permute_ps'.
// Requires AVX512F.
func MaskzPermutePs1(k Mmask8, a M256, imm8 int) M256 {
	return M256(maskzPermutePs1(uint8(k), [8]float32(a), imm8))
}

func maskzPermutePs1(k uint8, a [8]float32, imm8 int) [8]float32


// MaskPermutePs2: Shuffle single-precision (32-bit) floating-point elements in
// 'a' within 128-bit lanes using the control in 'imm8', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		tmp_dst[223:192] := SELECT4(a[255:128], imm8[5:4])
//		tmp_dst[255:224] := SELECT4(a[255:128], imm8[7:6])
//		tmp_dst[287:256] := SELECT4(a[383:256], imm8[1:0])
//		tmp_dst[319:288] := SELECT4(a[383:256], imm8[3:2])
//		tmp_dst[351:320] := SELECT4(a[383:256], imm8[5:4])
//		tmp_dst[383:352] := SELECT4(a[383:256], imm8[7:6])
//		tmp_dst[415:384] := SELECT4(a[511:384], imm8[1:0])
//		tmp_dst[447:416] := SELECT4(a[511:384], imm8[3:2])
//		tmp_dst[479:448] := SELECT4(a[511:384], imm8[5:4])
//		tmp_dst[511:480] := SELECT4(a[511:384], imm8[7:6])
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm512_mask_permute_ps'.
// Requires AVX512F.
func MaskPermutePs2(src M512, k Mmask16, a M512, imm8 int) M512 {
	return M512(maskPermutePs2([16]float32(src), uint16(k), [16]float32(a), imm8))
}

func maskPermutePs2(src [16]float32, k uint16, a [16]float32, imm8 int) [16]float32


// MaskzPermutePs2: Shuffle single-precision (32-bit) floating-point elements
// in 'a' within 128-bit lanes using the control in 'imm8', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		tmp_dst[223:192] := SELECT4(a[255:128], imm8[5:4])
//		tmp_dst[255:224] := SELECT4(a[255:128], imm8[7:6])
//		tmp_dst[287:256] := SELECT4(a[383:256], imm8[1:0])
//		tmp_dst[319:288] := SELECT4(a[383:256], imm8[3:2])
//		tmp_dst[351:320] := SELECT4(a[383:256], imm8[5:4])
//		tmp_dst[383:352] := SELECT4(a[383:256], imm8[7:6])
//		tmp_dst[415:384] := SELECT4(a[511:384], imm8[1:0])
//		tmp_dst[447:416] := SELECT4(a[511:384], imm8[3:2])
//		tmp_dst[479:448] := SELECT4(a[511:384], imm8[5:4])
//		tmp_dst[511:480] := SELECT4(a[511:384], imm8[7:6])
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm512_maskz_permute_ps'.
// Requires AVX512F.
func MaskzPermutePs2(k Mmask16, a M512, imm8 int) M512 {
	return M512(maskzPermutePs2(uint16(k), [16]float32(a), imm8))
}

func maskzPermutePs2(k uint16, a [16]float32, imm8 int) [16]float32


// PermutePs: Shuffle single-precision (32-bit) floating-point elements in 'a'
// within 128-bit lanes using the control in 'imm8', and store the results in
// 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		dst[223:192] := SELECT4(a[255:128], imm8[5:4])
//		dst[255:224] := SELECT4(a[255:128], imm8[7:6])
//		dst[287:256] := SELECT4(a[383:256], imm8[1:0])
//		dst[319:288] := SELECT4(a[383:256], imm8[3:2])
//		dst[351:320] := SELECT4(a[383:256], imm8[5:4])
//		dst[383:352] := SELECT4(a[383:256], imm8[7:6])
//		dst[415:384] := SELECT4(a[511:384], imm8[1:0])
//		dst[447:416] := SELECT4(a[511:384], imm8[3:2])
//		dst[479:448] := SELECT4(a[511:384], imm8[5:4])
//		dst[511:480] := SELECT4(a[511:384], imm8[7:6])
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm512_permute_ps'.
// Requires AVX512F.
func PermutePs(a M512, imm8 int) M512 {
	return M512(permutePs([16]float32(a), imm8))
}

func permutePs(a [16]float32, imm8 int) [16]float32


// MaskPermutevarPd: Shuffle double-precision (64-bit) floating-point elements
// in 'a' using the control in 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		IF (b[1] == 0) tmp_dst[63:0] := a[63:0]
//		IF (b[1] == 1) tmp_dst[63:0] := a[127:64]
//		IF (b[65] == 0) tmp_dst[127:64] := a[63:0]
//		IF (b[65] == 1) tmp_dst[127:64] := a[127:64]
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm_mask_permutevar_pd'.
// Requires AVX512F.
func MaskPermutevarPd(src M128d, k Mmask8, a M128d, b M128i) M128d {
	return M128d(maskPermutevarPd([2]float64(src), uint8(k), [2]float64(a), [16]byte(b)))
}

func maskPermutevarPd(src [2]float64, k uint8, a [2]float64, b [16]byte) [2]float64


// MaskzPermutevarPd: Shuffle double-precision (64-bit) floating-point elements
// in 'a' using the control in 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		IF (b[1] == 0) tmp_dst[63:0] := a[63:0]
//		IF (b[1] == 1) tmp_dst[63:0] := a[127:64]
//		IF (b[65] == 0) tmp_dst[127:64] := a[63:0]
//		IF (b[65] == 1) tmp_dst[127:64] := a[127:64]
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm_maskz_permutevar_pd'.
// Requires AVX512F.
func MaskzPermutevarPd(k Mmask8, a M128d, b M128i) M128d {
	return M128d(maskzPermutevarPd(uint8(k), [2]float64(a), [16]byte(b)))
}

func maskzPermutevarPd(k uint8, a [2]float64, b [16]byte) [2]float64


// MaskPermutevarPd1: Shuffle double-precision (64-bit) floating-point elements
// in 'a' within 128-bit lanes using the control in 'b', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		IF (b[1] == 0) tmp_dst[63:0] := a[63:0]
//		IF (b[1] == 1) tmp_dst[63:0] := a[127:64]
//		IF (b[65] == 0) tmp_dst[127:64] := a[63:0]
//		IF (b[65] == 1) tmp_dst[127:64] := a[127:64]
//		IF (b[129] == 0) tmp_dst[191:128] := a[191:128]
//		IF (b[129] == 1) tmp_dst[191:128] := a[255:192]
//		IF (b[193] == 0) tmp_dst[255:192] := a[191:128]
//		IF (b[193] == 1) tmp_dst[255:192] := a[255:192]
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm256_mask_permutevar_pd'.
// Requires AVX512F.
func MaskPermutevarPd1(src M256d, k Mmask8, a M256d, b M256i) M256d {
	return M256d(maskPermutevarPd1([4]float64(src), uint8(k), [4]float64(a), [32]byte(b)))
}

func maskPermutevarPd1(src [4]float64, k uint8, a [4]float64, b [32]byte) [4]float64


// MaskzPermutevarPd1: Shuffle double-precision (64-bit) floating-point
// elements in 'a' within 128-bit lanes using the control in 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		IF (b[1] == 0) tmp_dst[63:0] := a[63:0]
//		IF (b[1] == 1) tmp_dst[63:0] := a[127:64]
//		IF (b[65] == 0) tmp_dst[127:64] := a[63:0]
//		IF (b[65] == 1) tmp_dst[127:64] := a[127:64]
//		IF (b[129] == 0) tmp_dst[191:128] := a[191:128]
//		IF (b[129] == 1) tmp_dst[191:128] := a[255:192]
//		IF (b[193] == 0) tmp_dst[255:192] := a[191:128]
//		IF (b[193] == 1) tmp_dst[255:192] := a[255:192]
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm256_maskz_permutevar_pd'.
// Requires AVX512F.
func MaskzPermutevarPd1(k Mmask8, a M256d, b M256i) M256d {
	return M256d(maskzPermutevarPd1(uint8(k), [4]float64(a), [32]byte(b)))
}

func maskzPermutevarPd1(k uint8, a [4]float64, b [32]byte) [4]float64


// MaskPermutevarPd2: Shuffle double-precision (64-bit) floating-point elements
// in 'a' within 128-bit lanes using the control in 'b', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		IF (b[1] == 0) tmp_dst[63:0] := a[63:0]
//		IF (b[1] == 1) tmp_dst[63:0] := a[127:64]
//		IF (b[65] == 0) tmp_dst[127:64] := a[63:0]
//		IF (b[65] == 1) tmp_dst[127:64] := a[127:64]
//		IF (b[129] == 0) tmp_dst[191:128] := a[191:128]
//		IF (b[129] == 1) tmp_dst[191:128] := a[255:192]
//		IF (b[193] == 0) tmp_dst[255:192] := a[191:128]
//		IF (b[193] == 1) tmp_dst[255:192] := a[255:192]
//		IF (b[257] == 0) tmp_dst[319:256] := a[319:256]
//		IF (b[257] == 1) tmp_dst[319:256] := a[383:320]
//		IF (b[321] == 0) tmp_dst[383:320] := a[319:256]
//		IF (b[321] == 1) tmp_dst[383:320] := a[383:320]
//		IF (b[385] == 0) tmp_dst[447:384] := a[447:384]
//		IF (b[385] == 1) tmp_dst[447:384] := a[511:448]
//		IF (b[449] == 0) tmp_dst[511:448] := a[447:384]
//		IF (b[449] == 1) tmp_dst[511:448] := a[511:448]
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm512_mask_permutevar_pd'.
// Requires AVX512F.
func MaskPermutevarPd2(src M512d, k Mmask8, a M512d, b M512i) M512d {
	return M512d(maskPermutevarPd2([8]float64(src), uint8(k), [8]float64(a), [64]byte(b)))
}

func maskPermutevarPd2(src [8]float64, k uint8, a [8]float64, b [64]byte) [8]float64


// MaskzPermutevarPd2: Shuffle double-precision (64-bit) floating-point
// elements in 'a' within 128-bit lanes using the control in 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		IF (b[1] == 0) tmp_dst[63:0] := a[63:0]
//		IF (b[1] == 1) tmp_dst[63:0] := a[127:64]
//		IF (b[65] == 0) tmp_dst[127:64] := a[63:0]
//		IF (b[65] == 1) tmp_dst[127:64] := a[127:64]
//		IF (b[129] == 0) tmp_dst[191:128] := a[191:128]
//		IF (b[129] == 1) tmp_dst[191:128] := a[255:192]
//		IF (b[193] == 0) tmp_dst[255:192] := a[191:128]
//		IF (b[193] == 1) tmp_dst[255:192] := a[255:192]
//		IF (b[257] == 0) tmp_dst[319:256] := a[319:256]
//		IF (b[257] == 1) tmp_dst[319:256] := a[383:320]
//		IF (b[321] == 0) tmp_dst[383:320] := a[319:256]
//		IF (b[321] == 1) tmp_dst[383:320] := a[383:320]
//		IF (b[385] == 0) tmp_dst[447:384] := a[447:384]
//		IF (b[385] == 1) tmp_dst[447:384] := a[511:448]
//		IF (b[449] == 0) tmp_dst[511:448] := a[447:384]
//		IF (b[449] == 1) tmp_dst[511:448] := a[511:448]
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm512_maskz_permutevar_pd'.
// Requires AVX512F.
func MaskzPermutevarPd2(k Mmask8, a M512d, b M512i) M512d {
	return M512d(maskzPermutevarPd2(uint8(k), [8]float64(a), [64]byte(b)))
}

func maskzPermutevarPd2(k uint8, a [8]float64, b [64]byte) [8]float64


// PermutevarPd: Shuffle double-precision (64-bit) floating-point elements in
// 'a' within 128-bit lanes using the control in 'b', and store the results in
// 'dst'. 
//
//		IF (b[1] == 0) dst[63:0] := a[63:0]
//		IF (b[1] == 1) dst[63:0] := a[127:64]
//		IF (b[65] == 0) dst[127:64] := a[63:0]
//		IF (b[65] == 1) dst[127:64] := a[127:64]
//		IF (b[129] == 0) dst[191:128] := a[191:128]
//		IF (b[129] == 1) dst[191:128] := a[255:192]
//		IF (b[193] == 0) dst[255:192] := a[191:128]
//		IF (b[193] == 1) dst[255:192] := a[255:192]
//		IF (b[257] == 0) dst[319:256] := a[319:256]
//		IF (b[257] == 1) dst[319:256] := a[383:320]
//		IF (b[321] == 0) dst[383:320] := a[319:256]
//		IF (b[321] == 1) dst[383:320] := a[383:320]
//		IF (b[385] == 0) dst[447:384] := a[447:384]
//		IF (b[385] == 1) dst[447:384] := a[511:448]
//		IF (b[449] == 0) dst[511:448] := a[447:384]
//		IF (b[449] == 1) dst[511:448] := a[511:448]
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm512_permutevar_pd'.
// Requires AVX512F.
func PermutevarPd(a M512d, b M512i) M512d {
	return M512d(permutevarPd([8]float64(a), [64]byte(b)))
}

func permutevarPd(a [8]float64, b [64]byte) [8]float64


// MaskPermutevarPs: Shuffle single-precision (32-bit) floating-point elements
// in 'a' using the control in 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], b[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], b[33:32])
//		tmp_dst[95:64] := SELECT4(a[127:0], b[65:64])
//		tmp_dst[127:96] := SELECT4(a[127:0], b[97:96])
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm_mask_permutevar_ps'.
// Requires AVX512F.
func MaskPermutevarPs(src M128, k Mmask8, a M128, b M128i) M128 {
	return M128(maskPermutevarPs([4]float32(src), uint8(k), [4]float32(a), [16]byte(b)))
}

func maskPermutevarPs(src [4]float32, k uint8, a [4]float32, b [16]byte) [4]float32


// MaskzPermutevarPs: Shuffle single-precision (32-bit) floating-point elements
// in 'a' using the control in 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], b[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], b[33:32])
//		tmp_dst[95:64] := SELECT4(a[127:0], b[65:64])
//		tmp_dst[127:96] := SELECT4(a[127:0], b[97:96])
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm_maskz_permutevar_ps'.
// Requires AVX512F.
func MaskzPermutevarPs(k Mmask8, a M128, b M128i) M128 {
	return M128(maskzPermutevarPs(uint8(k), [4]float32(a), [16]byte(b)))
}

func maskzPermutevarPs(k uint8, a [4]float32, b [16]byte) [4]float32


// MaskPermutevarPs1: Shuffle single-precision (32-bit) floating-point elements
// in 'a' within 128-bit lanes using the control in 'b', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], b[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], b[33:32])
//		tmp_dst[95:64] := SELECT4(a[127:0], b[65:64])
//		tmp_dst[127:96] := SELECT4(a[127:0], b[97:96])
//		tmp_dst[159:128] := SELECT4(a[255:128], b[129:128])
//		tmp_dst[191:160] := SELECT4(a[255:128], b[161:160])
//		tmp_dst[223:192] := SELECT4(a[255:128], b[193:192])
//		tmp_dst[255:224] := SELECT4(a[255:128], b[225:224])
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm256_mask_permutevar_ps'.
// Requires AVX512F.
func MaskPermutevarPs1(src M256, k Mmask8, a M256, b M256i) M256 {
	return M256(maskPermutevarPs1([8]float32(src), uint8(k), [8]float32(a), [32]byte(b)))
}

func maskPermutevarPs1(src [8]float32, k uint8, a [8]float32, b [32]byte) [8]float32


// MaskzPermutevarPs1: Shuffle single-precision (32-bit) floating-point
// elements in 'a' within 128-bit lanes using the control in 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], b[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], b[33:32])
//		tmp_dst[95:64] := SELECT4(a[127:0], b[65:64])
//		tmp_dst[127:96] := SELECT4(a[127:0], b[97:96])
//		tmp_dst[159:128] := SELECT4(a[255:128], b[129:128])
//		tmp_dst[191:160] := SELECT4(a[255:128], b[161:160])
//		tmp_dst[223:192] := SELECT4(a[255:128], b[193:192])
//		tmp_dst[255:224] := SELECT4(a[255:128], b[225:224])
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm256_maskz_permutevar_ps'.
// Requires AVX512F.
func MaskzPermutevarPs1(k Mmask8, a M256, b M256i) M256 {
	return M256(maskzPermutevarPs1(uint8(k), [8]float32(a), [32]byte(b)))
}

func maskzPermutevarPs1(k uint8, a [8]float32, b [32]byte) [8]float32


// MaskPermutevarPs2: Shuffle single-precision (32-bit) floating-point elements
// in 'a' within 128-bit lanes using the control in 'b', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], b[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], b[33:32])
//		tmp_dst[95:64] := SELECT4(a[127:0], b[65:64])
//		tmp_dst[127:96] := SELECT4(a[127:0], b[97:96])
//		tmp_dst[159:128] := SELECT4(a[255:128], b[129:128])
//		tmp_dst[191:160] := SELECT4(a[255:128], b[161:160])
//		tmp_dst[223:192] := SELECT4(a[255:128], b[193:192])
//		tmp_dst[255:224] := SELECT4(a[255:128], b[225:224])
//		tmp_dst[287:256] := SELECT4(a[383:256], b[257:256])
//		tmp_dst[319:288] := SELECT4(a[383:256], b[289:288])
//		tmp_dst[351:320] := SELECT4(a[383:256], b[321:320])
//		tmp_dst[383:352] := SELECT4(a[383:256], b[353:352])
//		tmp_dst[415:384] := SELECT4(a[511:384], b[385:384])
//		tmp_dst[447:416] := SELECT4(a[511:384], b[417:416])
//		tmp_dst[479:448] := SELECT4(a[511:384], b[449:448])
//		tmp_dst[511:480] := SELECT4(a[511:384], b[481:480])
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm512_mask_permutevar_ps'.
// Requires AVX512F.
func MaskPermutevarPs2(src M512, k Mmask16, a M512, b M512i) M512 {
	return M512(maskPermutevarPs2([16]float32(src), uint16(k), [16]float32(a), [64]byte(b)))
}

func maskPermutevarPs2(src [16]float32, k uint16, a [16]float32, b [64]byte) [16]float32


// MaskzPermutevarPs2: Shuffle single-precision (32-bit) floating-point
// elements in 'a' within 128-bit lanes using the control in 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], b[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], b[33:32])
//		tmp_dst[95:64] := SELECT4(a[127:0], b[65:64])
//		tmp_dst[127:96] := SELECT4(a[127:0], b[97:96])
//		tmp_dst[159:128] := SELECT4(a[255:128], b[129:128])
//		tmp_dst[191:160] := SELECT4(a[255:128], b[161:160])
//		tmp_dst[223:192] := SELECT4(a[255:128], b[193:192])
//		tmp_dst[255:224] := SELECT4(a[255:128], b[225:224])
//		tmp_dst[287:256] := SELECT4(a[383:256], b[257:256])
//		tmp_dst[319:288] := SELECT4(a[383:256], b[289:288])
//		tmp_dst[351:320] := SELECT4(a[383:256], b[321:320])
//		tmp_dst[383:352] := SELECT4(a[383:256], b[353:352])
//		tmp_dst[415:384] := SELECT4(a[511:384], b[385:384])
//		tmp_dst[447:416] := SELECT4(a[511:384], b[417:416])
//		tmp_dst[479:448] := SELECT4(a[511:384], b[449:448])
//		tmp_dst[511:480] := SELECT4(a[511:384], b[481:480])
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm512_maskz_permutevar_ps'.
// Requires AVX512F.
func MaskzPermutevarPs2(k Mmask16, a M512, b M512i) M512 {
	return M512(maskzPermutevarPs2(uint16(k), [16]float32(a), [64]byte(b)))
}

func maskzPermutevarPs2(k uint16, a [16]float32, b [64]byte) [16]float32


// PermutevarPs: Shuffle single-precision (32-bit) floating-point elements in
// 'a' within 128-bit lanes using the control in 'b', and store the results in
// 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		dst[31:0] := SELECT4(a[127:0], b[1:0])
//		dst[63:32] := SELECT4(a[127:0], b[33:32])
//		dst[95:64] := SELECT4(a[127:0], b[65:64])
//		dst[127:96] := SELECT4(a[127:0], b[97:96])
//		dst[159:128] := SELECT4(a[255:128], b[129:128])
//		dst[191:160] := SELECT4(a[255:128], b[161:160])
//		dst[223:192] := SELECT4(a[255:128], b[193:192])
//		dst[255:224] := SELECT4(a[255:128], b[225:224])
//		dst[287:256] := SELECT4(a[383:256], b[257:256])
//		dst[319:288] := SELECT4(a[383:256], b[289:288])
//		dst[351:320] := SELECT4(a[383:256], b[321:320])
//		dst[383:352] := SELECT4(a[383:256], b[353:352])
//		dst[415:384] := SELECT4(a[511:384], b[385:384])
//		dst[447:416] := SELECT4(a[511:384], b[417:416])
//		dst[479:448] := SELECT4(a[511:384], b[449:448])
//		dst[511:480] := SELECT4(a[511:384], b[481:480])
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm512_permutevar_ps'.
// Requires AVX512F.
func PermutevarPs(a M512, b M512i) M512 {
	return M512(permutevarPs([16]float32(a), [64]byte(b)))
}

func permutevarPs(a [16]float32, b [64]byte) [16]float32


// MaskPermutexEpi64: Shuffle 64-bit integers in 'a' across lanes lanes using
// the control in 'imm8', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[63:0] := src[63:0]
//			1:	tmp[63:0] := src[127:64]
//			2:	tmp[63:0] := src[191:128]
//			3:	tmp[63:0] := src[255:192]
//			ESAC
//			RETURN tmp[63:0]
//		}
//		
//		tmp_dst[63:0] := SELECT4(a[255:0], imm8[1:0])
//		tmp_dst[127:64] := SELECT4(a[255:0], imm8[3:2])
//		tmp_dst[191:128] := SELECT4(a[255:0], imm8[5:4])
//		tmp_dst[255:192] := SELECT4(a[255:0], imm8[7:6])
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMQ'. Intrinsic: '_mm256_mask_permutex_epi64'.
// Requires AVX512F.
func MaskPermutexEpi64(src M256i, k Mmask8, a M256i, imm8 int) M256i {
	return M256i(maskPermutexEpi64([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func maskPermutexEpi64(src [32]byte, k uint8, a [32]byte, imm8 int) [32]byte


// MaskzPermutexEpi64: Shuffle 64-bit integers in 'a' across lanes using the
// control in 'imm8', and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[63:0] := src[63:0]
//			1:	tmp[63:0] := src[127:64]
//			2:	tmp[63:0] := src[191:128]
//			3:	tmp[63:0] := src[255:192]
//			ESAC
//			RETURN tmp[63:0]
//		}
//		
//		tmp_dst[63:0] := SELECT4(a[255:0], imm8[1:0])
//		tmp_dst[127:64] := SELECT4(a[255:0], imm8[3:2])
//		tmp_dst[191:128] := SELECT4(a[255:0], imm8[5:4])
//		tmp_dst[255:192] := SELECT4(a[255:0], imm8[7:6])
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMQ'. Intrinsic: '_mm256_maskz_permutex_epi64'.
// Requires AVX512F.
func MaskzPermutexEpi64(k Mmask8, a M256i, imm8 int) M256i {
	return M256i(maskzPermutexEpi64(uint8(k), [32]byte(a), imm8))
}

func maskzPermutexEpi64(k uint8, a [32]byte, imm8 int) [32]byte


// PermutexEpi64: Shuffle 64-bit integers in 'a' across lanes using the control
// in 'imm8', and store the results in 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[63:0] := src[63:0]
//			1:	tmp[63:0] := src[127:64]
//			2:	tmp[63:0] := src[191:128]
//			3:	tmp[63:0] := src[255:192]
//			ESAC
//			RETURN tmp[63:0]
//		}
//		
//		dst[63:0] := SELECT4(a[255:0], imm8[1:0])
//		dst[127:64] := SELECT4(a[255:0], imm8[3:2])
//		dst[191:128] := SELECT4(a[255:0], imm8[5:4])
//		dst[255:192] := SELECT4(a[255:0], imm8[7:6])
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMQ'. Intrinsic: '_mm256_permutex_epi64'.
// Requires AVX512F.
func PermutexEpi64(a M256i, imm8 int) M256i {
	return M256i(permutexEpi64([32]byte(a), imm8))
}

func permutexEpi64(a [32]byte, imm8 int) [32]byte


// MaskPermutexEpi641: Shuffle 64-bit integers in 'a' within 256-bit lanes
// using the control in 'imm8', and store the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[63:0] := src[63:0]
//			1:	tmp[63:0] := src[127:64]
//			2:	tmp[63:0] := src[191:128]
//			3:	tmp[63:0] := src[255:192]
//			ESAC
//			RETURN tmp[63:0]
//		}
//		
//		tmp_dst[63:0] := SELECT4(a[255:0], imm8[1:0])
//		tmp_dst[127:64] := SELECT4(a[255:0], imm8[3:2])
//		tmp_dst[191:128] := SELECT4(a[255:0], imm8[5:4])
//		tmp_dst[255:192] := SELECT4(a[255:0], imm8[7:6])
//		tmp_dst[319:256] := SELECT4(a[511:256], imm8[1:0])
//		tmp_dst[383:320] := SELECT4(a[511:256], imm8[3:2])
//		tmp_dst[447:384] := SELECT4(a[511:256], imm8[5:4])
//		tmp_dst[511:448] := SELECT4(a[511:256], imm8[7:6])
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMQ'. Intrinsic: '_mm512_mask_permutex_epi64'.
// Requires AVX512F.
func MaskPermutexEpi641(src M512i, k Mmask8, a M512i, imm8 int) M512i {
	return M512i(maskPermutexEpi641([64]byte(src), uint8(k), [64]byte(a), imm8))
}

func maskPermutexEpi641(src [64]byte, k uint8, a [64]byte, imm8 int) [64]byte


// MaskzPermutexEpi641: Shuffle 64-bit integers in 'a' within 256-bit lanes
// using the control in 'imm8', and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[63:0] := src[63:0]
//			1:	tmp[63:0] := src[127:64]
//			2:	tmp[63:0] := src[191:128]
//			3:	tmp[63:0] := src[255:192]
//			ESAC
//			RETURN tmp[63:0]
//		}
//		
//		tmp_dst[63:0] := SELECT4(a[255:0], imm8[1:0])
//		tmp_dst[127:64] := SELECT4(a[255:0], imm8[3:2])
//		tmp_dst[191:128] := SELECT4(a[255:0], imm8[5:4])
//		tmp_dst[255:192] := SELECT4(a[255:0], imm8[7:6])
//		tmp_dst[319:256] := SELECT4(a[511:256], imm8[1:0])
//		tmp_dst[383:320] := SELECT4(a[511:256], imm8[3:2])
//		tmp_dst[447:384] := SELECT4(a[511:256], imm8[5:4])
//		tmp_dst[511:448] := SELECT4(a[511:256], imm8[7:6])
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMQ'. Intrinsic: '_mm512_maskz_permutex_epi64'.
// Requires AVX512F.
func MaskzPermutexEpi641(k Mmask8, a M512i, imm8 int) M512i {
	return M512i(maskzPermutexEpi641(uint8(k), [64]byte(a), imm8))
}

func maskzPermutexEpi641(k uint8, a [64]byte, imm8 int) [64]byte


// PermutexEpi641: Shuffle 64-bit integers in 'a' within 256-bit lanes using
// the control in 'imm8', and store the results in 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[63:0] := src[63:0]
//			1:	tmp[63:0] := src[127:64]
//			2:	tmp[63:0] := src[191:128]
//			3:	tmp[63:0] := src[255:192]
//			ESAC
//			RETURN tmp[63:0]
//		}
//		
//		dst[63:0] := SELECT4(a[255:0], imm8[1:0])
//		dst[127:64] := SELECT4(a[255:0], imm8[3:2])
//		dst[191:128] := SELECT4(a[255:0], imm8[5:4])
//		dst[255:192] := SELECT4(a[255:0], imm8[7:6])
//		dst[319:256] := SELECT4(a[511:256], imm8[1:0])
//		dst[383:320] := SELECT4(a[511:256], imm8[3:2])
//		dst[447:384] := SELECT4(a[511:256], imm8[5:4])
//		dst[511:448] := SELECT4(a[511:256], imm8[7:6])
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMQ'. Intrinsic: '_mm512_permutex_epi64'.
// Requires AVX512F.
func PermutexEpi641(a M512i, imm8 int) M512i {
	return M512i(permutexEpi641([64]byte(a), imm8))
}

func permutexEpi641(a [64]byte, imm8 int) [64]byte


// MaskPermutexPd: Shuffle double-precision (64-bit) floating-point elements in
// 'a' across lanes using the control in 'imm8', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[63:0] := src[63:0]
//			1:	tmp[63:0] := src[127:64]
//			2:	tmp[63:0] := src[191:128]
//			3:	tmp[63:0] := src[255:192]
//			ESAC
//			RETURN tmp[63:0]
//		}
//		
//		tmp_dst[63:0] := SELECT4(a[255:0], imm8[1:0])
//		tmp_dst[127:64] := SELECT4(a[255:0], imm8[3:2])
//		tmp_dst[191:128] := SELECT4(a[255:0], imm8[5:4])
//		tmp_dst[255:192] := SELECT4(a[255:0], imm8[7:6])
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMPD'. Intrinsic: '_mm256_mask_permutex_pd'.
// Requires AVX512F.
func MaskPermutexPd(src M256d, k Mmask8, a M256d, imm8 int) M256d {
	return M256d(maskPermutexPd([4]float64(src), uint8(k), [4]float64(a), imm8))
}

func maskPermutexPd(src [4]float64, k uint8, a [4]float64, imm8 int) [4]float64


// MaskzPermutexPd: Shuffle double-precision (64-bit) floating-point elements
// in 'a' across lanes using the control in 'imm8', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[63:0] := src[63:0]
//			1:	tmp[63:0] := src[127:64]
//			2:	tmp[63:0] := src[191:128]
//			3:	tmp[63:0] := src[255:192]
//			ESAC
//			RETURN tmp[63:0]
//		}
//		
//		tmp_dst[63:0] := SELECT4(a[255:0], imm8[1:0])
//		tmp_dst[127:64] := SELECT4(a[255:0], imm8[3:2])
//		tmp_dst[191:128] := SELECT4(a[255:0], imm8[5:4])
//		tmp_dst[255:192] := SELECT4(a[255:0], imm8[7:6])
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMPD'. Intrinsic: '_mm256_maskz_permutex_pd'.
// Requires AVX512F.
func MaskzPermutexPd(k Mmask8, a M256d, imm8 int) M256d {
	return M256d(maskzPermutexPd(uint8(k), [4]float64(a), imm8))
}

func maskzPermutexPd(k uint8, a [4]float64, imm8 int) [4]float64


// PermutexPd: Shuffle double-precision (64-bit) floating-point elements in 'a'
// across lanes using the control in 'imm8', and store the results in 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[63:0] := src[63:0]
//			1:	tmp[63:0] := src[127:64]
//			2:	tmp[63:0] := src[191:128]
//			3:	tmp[63:0] := src[255:192]
//			ESAC
//			RETURN tmp[63:0]
//		}
//		
//		dst[63:0] := SELECT4(a[255:0], imm8[1:0])
//		dst[127:64] := SELECT4(a[255:0], imm8[3:2])
//		dst[191:128] := SELECT4(a[255:0], imm8[5:4])
//		dst[255:192] := SELECT4(a[255:0], imm8[7:6])
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMPD'. Intrinsic: '_mm256_permutex_pd'.
// Requires AVX512F.
func PermutexPd(a M256d, imm8 int) M256d {
	return M256d(permutexPd([4]float64(a), imm8))
}

func permutexPd(a [4]float64, imm8 int) [4]float64


// MaskPermutexPd1: Shuffle double-precision (64-bit) floating-point elements
// in 'a' within 256-bit lanes using the control in 'imm8', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[63:0] := src[63:0]
//			1:	tmp[63:0] := src[127:64]
//			2:	tmp[63:0] := src[191:128]
//			3:	tmp[63:0] := src[255:192]
//			ESAC
//			RETURN tmp[63:0]
//		}
//		
//		tmp_dst[63:0] := SELECT4(a[255:0], imm8[1:0])
//		tmp_dst[127:64] := SELECT4(a[255:0], imm8[3:2])
//		tmp_dst[191:128] := SELECT4(a[255:0], imm8[5:4])
//		tmp_dst[255:192] := SELECT4(a[255:0], imm8[7:6])
//		tmp_dst[319:256] := SELECT4(a[511:256], imm8[1:0])
//		tmp_dst[383:320] := SELECT4(a[511:256], imm8[3:2])
//		tmp_dst[447:384] := SELECT4(a[511:256], imm8[5:4])
//		tmp_dst[511:448] := SELECT4(a[511:256], imm8[7:6])
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMPD'. Intrinsic: '_mm512_mask_permutex_pd'.
// Requires AVX512F.
func MaskPermutexPd1(src M512d, k Mmask8, a M512d, imm8 int) M512d {
	return M512d(maskPermutexPd1([8]float64(src), uint8(k), [8]float64(a), imm8))
}

func maskPermutexPd1(src [8]float64, k uint8, a [8]float64, imm8 int) [8]float64


// MaskzPermutexPd1: Shuffle double-precision (64-bit) floating-point elements
// in 'a' within 256-bit lanes using the control in 'imm8', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[63:0] := src[63:0]
//			1:	tmp[63:0] := src[127:64]
//			2:	tmp[63:0] := src[191:128]
//			3:	tmp[63:0] := src[255:192]
//			ESAC
//			RETURN tmp[63:0]
//		}
//		
//		tmp_dst[63:0] := SELECT4(a[255:0], imm8[1:0])
//		tmp_dst[127:64] := SELECT4(a[255:0], imm8[3:2])
//		tmp_dst[191:128] := SELECT4(a[255:0], imm8[5:4])
//		tmp_dst[255:192] := SELECT4(a[255:0], imm8[7:6])
//		tmp_dst[319:256] := SELECT4(a[511:256], imm8[1:0])
//		tmp_dst[383:320] := SELECT4(a[511:256], imm8[3:2])
//		tmp_dst[447:384] := SELECT4(a[511:256], imm8[5:4])
//		tmp_dst[511:448] := SELECT4(a[511:256], imm8[7:6])
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMPD'. Intrinsic: '_mm512_maskz_permutex_pd'.
// Requires AVX512F.
func MaskzPermutexPd1(k Mmask8, a M512d, imm8 int) M512d {
	return M512d(maskzPermutexPd1(uint8(k), [8]float64(a), imm8))
}

func maskzPermutexPd1(k uint8, a [8]float64, imm8 int) [8]float64


// PermutexPd1: Shuffle double-precision (64-bit) floating-point elements in
// 'a' within 256-bit lanes using the control in 'imm8', and store the results
// in 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[63:0] := src[63:0]
//			1:	tmp[63:0] := src[127:64]
//			2:	tmp[63:0] := src[191:128]
//			3:	tmp[63:0] := src[255:192]
//			ESAC
//			RETURN tmp[63:0]
//		}
//		
//		dst[63:0] := SELECT4(a[255:0], imm8[1:0])
//		dst[127:64] := SELECT4(a[255:0], imm8[3:2])
//		dst[191:128] := SELECT4(a[255:0], imm8[5:4])
//		dst[255:192] := SELECT4(a[255:0], imm8[7:6])
//		dst[319:256] := SELECT4(a[511:256], imm8[1:0])
//		dst[383:320] := SELECT4(a[511:256], imm8[3:2])
//		dst[447:384] := SELECT4(a[511:256], imm8[5:4])
//		dst[511:448] := SELECT4(a[511:256], imm8[7:6])
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMPD'. Intrinsic: '_mm512_permutex_pd'.
// Requires AVX512F.
func PermutexPd1(a M512d, imm8 int) M512d {
	return M512d(permutexPd1([8]float64(a), imm8))
}

func permutexPd1(a [8]float64, imm8 int) [8]float64


// MaskPermutex2varEpi32: Shuffle 32-bit integers in 'a' and 'b' using the
// corresponding selector and index in 'idx', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'a' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			off := idx[i+1:i]*32
//			IF k[j]
//				dst[i+31:i] := idx[i+2] ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMT2D'. Intrinsic: '_mm_mask_permutex2var_epi32'.
// Requires AVX512F.
func MaskPermutex2varEpi32(a M128i, k Mmask8, idx M128i, b M128i) M128i {
	return M128i(maskPermutex2varEpi32([16]byte(a), uint8(k), [16]byte(idx), [16]byte(b)))
}

func maskPermutex2varEpi32(a [16]byte, k uint8, idx [16]byte, b [16]byte) [16]byte


// Mask2Permutex2varEpi32: Shuffle 32-bit integers in 'a' and 'b' using the
// corresponding selector and index in 'idx', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'idx' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			off := idx[i+1:i]*32
//			IF k[j]
//				dst[i+31:i] := idx[i+2] ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := idx[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMI2D'. Intrinsic: '_mm_mask2_permutex2var_epi32'.
// Requires AVX512F.
func Mask2Permutex2varEpi32(a M128i, idx M128i, k Mmask8, b M128i) M128i {
	return M128i(mask2Permutex2varEpi32([16]byte(a), [16]byte(idx), uint8(k), [16]byte(b)))
}

func mask2Permutex2varEpi32(a [16]byte, idx [16]byte, k uint8, b [16]byte) [16]byte


// MaskzPermutex2varEpi32: Shuffle 32-bit integers in 'a' and 'b' using the
// corresponding selector and index in 'idx', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			off := idx[i+1:i]*32
//			IF k[j]
//				dst[i+31:i] := (idx[i+2]) ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMI2D, VPERMT2D'. Intrinsic: '_mm_maskz_permutex2var_epi32'.
// Requires AVX512F.
func MaskzPermutex2varEpi32(k Mmask8, a M128i, idx M128i, b M128i) M128i {
	return M128i(maskzPermutex2varEpi32(uint8(k), [16]byte(a), [16]byte(idx), [16]byte(b)))
}

func maskzPermutex2varEpi32(k uint8, a [16]byte, idx [16]byte, b [16]byte) [16]byte


// Permutex2varEpi32: Shuffle 32-bit integers in 'a' and 'b' using the
// corresponding selector and index in 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			off := idx[i+2:i]*32
//			dst[i+31:i] := idx[i+3] ? b[off+31:off] : a[off+31:off]
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMI2D, VPERMT2D'. Intrinsic: '_mm_permutex2var_epi32'.
// Requires AVX512F.
func Permutex2varEpi32(a M128i, idx M128i, b M128i) M128i {
	return M128i(permutex2varEpi32([16]byte(a), [16]byte(idx), [16]byte(b)))
}

func permutex2varEpi32(a [16]byte, idx [16]byte, b [16]byte) [16]byte


// MaskPermutex2varEpi321: Shuffle 32-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			off := idx[i+3:i]*32
//			IF k[j]
//				dst[i+31:i] := idx[i+4] ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMT2D'. Intrinsic: '_mm256_mask_permutex2var_epi32'.
// Requires AVX512F.
func MaskPermutex2varEpi321(a M256i, k Mmask8, idx M256i, b M256i) M256i {
	return M256i(maskPermutex2varEpi321([32]byte(a), uint8(k), [32]byte(idx), [32]byte(b)))
}

func maskPermutex2varEpi321(a [32]byte, k uint8, idx [32]byte, b [32]byte) [32]byte


// Mask2Permutex2varEpi321: Shuffle 32-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'idx' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			off := idx[i+2:i]*32
//			IF k[j]
//				dst[i+31:i] := idx[i+3] ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := idx[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2D'. Intrinsic: '_mm256_mask2_permutex2var_epi32'.
// Requires AVX512F.
func Mask2Permutex2varEpi321(a M256i, idx M256i, k Mmask8, b M256i) M256i {
	return M256i(mask2Permutex2varEpi321([32]byte(a), [32]byte(idx), uint8(k), [32]byte(b)))
}

func mask2Permutex2varEpi321(a [32]byte, idx [32]byte, k uint8, b [32]byte) [32]byte


// MaskzPermutex2varEpi321: Shuffle 32-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			off := idx[i+2:i]*32
//			IF k[j]
//				dst[i+31:i] := (idx[i+3]) ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2D, VPERMT2D'. Intrinsic: '_mm256_maskz_permutex2var_epi32'.
// Requires AVX512F.
func MaskzPermutex2varEpi321(k Mmask8, a M256i, idx M256i, b M256i) M256i {
	return M256i(maskzPermutex2varEpi321(uint8(k), [32]byte(a), [32]byte(idx), [32]byte(b)))
}

func maskzPermutex2varEpi321(k uint8, a [32]byte, idx [32]byte, b [32]byte) [32]byte


// Permutex2varEpi321: Shuffle 32-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			off := idx[i+2:i]*32
//			dst[i+31:i] := idx[i+3] ? b[off+31:off] : a[off+31:off]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2D, VPERMT2D'. Intrinsic: '_mm256_permutex2var_epi32'.
// Requires AVX512F.
func Permutex2varEpi321(a M256i, idx M256i, b M256i) M256i {
	return M256i(permutex2varEpi321([32]byte(a), [32]byte(idx), [32]byte(b)))
}

func permutex2varEpi321(a [32]byte, idx [32]byte, b [32]byte) [32]byte


// MaskPermutex2varEpi322: Shuffle 32-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			off := idx[i+3:i]*32
//			IF k[j]
//				dst[i+31:i] := idx[i+4] ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMT2D'. Intrinsic: '_mm512_mask_permutex2var_epi32'.
// Requires AVX512F.
func MaskPermutex2varEpi322(a M512i, k Mmask16, idx M512i, b M512i) M512i {
	return M512i(maskPermutex2varEpi322([64]byte(a), uint16(k), [64]byte(idx), [64]byte(b)))
}

func maskPermutex2varEpi322(a [64]byte, k uint16, idx [64]byte, b [64]byte) [64]byte


// Mask2Permutex2varEpi322: Shuffle 32-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'idx' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			off := idx[i+3:i]*32
//			IF k[j]
//				dst[i+31:i] := idx[i+4] ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := idx[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMI2D'. Intrinsic: '_mm512_mask2_permutex2var_epi32'.
// Requires AVX512F.
func Mask2Permutex2varEpi322(a M512i, idx M512i, k Mmask16, b M512i) M512i {
	return M512i(mask2Permutex2varEpi322([64]byte(a), [64]byte(idx), uint16(k), [64]byte(b)))
}

func mask2Permutex2varEpi322(a [64]byte, idx [64]byte, k uint16, b [64]byte) [64]byte


// MaskzPermutex2varEpi322: Shuffle 32-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			off := idx[i+3:i]*32
//			IF k[j]
//				dst[i+31:i] := (idx[i+4]) ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMI2D, VPERMT2D'. Intrinsic: '_mm512_maskz_permutex2var_epi32'.
// Requires AVX512F.
func MaskzPermutex2varEpi322(k Mmask16, a M512i, idx M512i, b M512i) M512i {
	return M512i(maskzPermutex2varEpi322(uint16(k), [64]byte(a), [64]byte(idx), [64]byte(b)))
}

func maskzPermutex2varEpi322(k uint16, a [64]byte, idx [64]byte, b [64]byte) [64]byte


// Permutex2varEpi322: Shuffle 32-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			off := idx[i+3:i]*32
//			dst[i+31:i] := idx[i+4] ? b[off+31:off] : a[off+31:off]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMI2D, VPERMT2D'. Intrinsic: '_mm512_permutex2var_epi32'.
// Requires AVX512F.
func Permutex2varEpi322(a M512i, idx M512i, b M512i) M512i {
	return M512i(permutex2varEpi322([64]byte(a), [64]byte(idx), [64]byte(b)))
}

func permutex2varEpi322(a [64]byte, idx [64]byte, b [64]byte) [64]byte


// MaskPermutex2varEpi64: Shuffle 64-bit integers in 'a' and 'b' using the
// corresponding selector and index in 'idx', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'a' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			off := idx[i]*64
//			IF k[j]
//				dst[i+63:i] := idx[i+1] ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMT2Q'. Intrinsic: '_mm_mask_permutex2var_epi64'.
// Requires AVX512F.
func MaskPermutex2varEpi64(a M128i, k Mmask8, idx M128i, b M128i) M128i {
	return M128i(maskPermutex2varEpi64([16]byte(a), uint8(k), [16]byte(idx), [16]byte(b)))
}

func maskPermutex2varEpi64(a [16]byte, k uint8, idx [16]byte, b [16]byte) [16]byte


// Mask2Permutex2varEpi64: Shuffle 64-bit integers in 'a' and 'b' using the
// corresponding selector and index in 'idx', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'idx' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			off := idx[i]*64
//			IF k[j]
//				dst[i+63:i] := idx[i+1] ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := idx[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMI2Q'. Intrinsic: '_mm_mask2_permutex2var_epi64'.
// Requires AVX512F.
func Mask2Permutex2varEpi64(a M128i, idx M128i, k Mmask8, b M128i) M128i {
	return M128i(mask2Permutex2varEpi64([16]byte(a), [16]byte(idx), uint8(k), [16]byte(b)))
}

func mask2Permutex2varEpi64(a [16]byte, idx [16]byte, k uint8, b [16]byte) [16]byte


// MaskzPermutex2varEpi64: Shuffle 64-bit integers in 'a' and 'b' using the
// corresponding selector and index in 'idx', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			off := idx[i]*64
//			IF k[j]
//				dst[i+63:i] := (idx[i+1]) ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMI2Q, VPERMT2Q'. Intrinsic: '_mm_maskz_permutex2var_epi64'.
// Requires AVX512F.
func MaskzPermutex2varEpi64(k Mmask8, a M128i, idx M128i, b M128i) M128i {
	return M128i(maskzPermutex2varEpi64(uint8(k), [16]byte(a), [16]byte(idx), [16]byte(b)))
}

func maskzPermutex2varEpi64(k uint8, a [16]byte, idx [16]byte, b [16]byte) [16]byte


// Permutex2varEpi64: Shuffle 64-bit integers in 'a' and 'b' using the
// corresponding selector and index in 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			off := idx[i]*64
//			dst[i+63:i] := idx[i+1] ? b[off+63:off] : a[off+63:off]
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMI2Q, VPERMT2Q'. Intrinsic: '_mm_permutex2var_epi64'.
// Requires AVX512F.
func Permutex2varEpi64(a M128i, idx M128i, b M128i) M128i {
	return M128i(permutex2varEpi64([16]byte(a), [16]byte(idx), [16]byte(b)))
}

func permutex2varEpi64(a [16]byte, idx [16]byte, b [16]byte) [16]byte


// MaskPermutex2varEpi641: Shuffle 64-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			off := idx[i+1:i]*64
//			IF k[j]
//				dst[i+63:i] := idx[i+2] ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMT2Q'. Intrinsic: '_mm256_mask_permutex2var_epi64'.
// Requires AVX512F.
func MaskPermutex2varEpi641(a M256i, k Mmask8, idx M256i, b M256i) M256i {
	return M256i(maskPermutex2varEpi641([32]byte(a), uint8(k), [32]byte(idx), [32]byte(b)))
}

func maskPermutex2varEpi641(a [32]byte, k uint8, idx [32]byte, b [32]byte) [32]byte


// Mask2Permutex2varEpi641: Shuffle 64-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'idx' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			off := idx[i+1:i]*64
//			IF k[j]
//				dst[i+63:i] := idx[i+2] ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := idx[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2Q'. Intrinsic: '_mm256_mask2_permutex2var_epi64'.
// Requires AVX512F.
func Mask2Permutex2varEpi641(a M256i, idx M256i, k Mmask8, b M256i) M256i {
	return M256i(mask2Permutex2varEpi641([32]byte(a), [32]byte(idx), uint8(k), [32]byte(b)))
}

func mask2Permutex2varEpi641(a [32]byte, idx [32]byte, k uint8, b [32]byte) [32]byte


// MaskzPermutex2varEpi641: Shuffle 64-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			off := idx[i+1:i]*64
//			IF k[j]
//				dst[i+63:i] := (idx[i+2]) ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2Q, VPERMT2Q'. Intrinsic: '_mm256_maskz_permutex2var_epi64'.
// Requires AVX512F.
func MaskzPermutex2varEpi641(k Mmask8, a M256i, idx M256i, b M256i) M256i {
	return M256i(maskzPermutex2varEpi641(uint8(k), [32]byte(a), [32]byte(idx), [32]byte(b)))
}

func maskzPermutex2varEpi641(k uint8, a [32]byte, idx [32]byte, b [32]byte) [32]byte


// Permutex2varEpi641: Shuffle 64-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			off := idx[i+1:i]*64
//			dst[i+63:i] := idx[i+2] ? b[off+63:off] : a[off+63:off]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2Q, VPERMT2Q'. Intrinsic: '_mm256_permutex2var_epi64'.
// Requires AVX512F.
func Permutex2varEpi641(a M256i, idx M256i, b M256i) M256i {
	return M256i(permutex2varEpi641([32]byte(a), [32]byte(idx), [32]byte(b)))
}

func permutex2varEpi641(a [32]byte, idx [32]byte, b [32]byte) [32]byte


// MaskPermutex2varEpi642: Shuffle 64-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			off := idx[i+2:i]*64
//			IF k[j]
//				dst[i+63:i] := idx[i+3] ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMT2Q'. Intrinsic: '_mm512_mask_permutex2var_epi64'.
// Requires AVX512F.
func MaskPermutex2varEpi642(a M512i, k Mmask8, idx M512i, b M512i) M512i {
	return M512i(maskPermutex2varEpi642([64]byte(a), uint8(k), [64]byte(idx), [64]byte(b)))
}

func maskPermutex2varEpi642(a [64]byte, k uint8, idx [64]byte, b [64]byte) [64]byte


// Mask2Permutex2varEpi642: Shuffle 64-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'idx' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			off := idx[i+2:i]*64
//			IF k[j]
//				dst[i+63:i] := idx[i+3] ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := idx[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMI2Q'. Intrinsic: '_mm512_mask2_permutex2var_epi64'.
// Requires AVX512F.
func Mask2Permutex2varEpi642(a M512i, idx M512i, k Mmask8, b M512i) M512i {
	return M512i(mask2Permutex2varEpi642([64]byte(a), [64]byte(idx), uint8(k), [64]byte(b)))
}

func mask2Permutex2varEpi642(a [64]byte, idx [64]byte, k uint8, b [64]byte) [64]byte


// MaskzPermutex2varEpi642: Shuffle 64-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			off := idx[i+2:i]*64
//			IF k[j]
//				dst[i+63:i] := (idx[i+3]) ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMI2Q, VPERMT2Q'. Intrinsic: '_mm512_maskz_permutex2var_epi64'.
// Requires AVX512F.
func MaskzPermutex2varEpi642(k Mmask8, a M512i, idx M512i, b M512i) M512i {
	return M512i(maskzPermutex2varEpi642(uint8(k), [64]byte(a), [64]byte(idx), [64]byte(b)))
}

func maskzPermutex2varEpi642(k uint8, a [64]byte, idx [64]byte, b [64]byte) [64]byte


// Permutex2varEpi642: Shuffle 64-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			off := idx[i+2:i]*64
//			dst[i+63:i] := idx[i+3] ? b[off+63:off] : a[off+63:off]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMI2Q, VPERMT2Q'. Intrinsic: '_mm512_permutex2var_epi64'.
// Requires AVX512F.
func Permutex2varEpi642(a M512i, idx M512i, b M512i) M512i {
	return M512i(permutex2varEpi642([64]byte(a), [64]byte(idx), [64]byte(b)))
}

func permutex2varEpi642(a [64]byte, idx [64]byte, b [64]byte) [64]byte


// MaskPermutex2varPd: Shuffle double-precision (64-bit) floating-point
// elements in 'a' and 'b' using the corresponding selector and index in 'idx',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			off := idx[i]*64
//			IF k[j]
//				dst[i+63:i] := idx[i+1] ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMT2PD'. Intrinsic: '_mm_mask_permutex2var_pd'.
// Requires AVX512F.
func MaskPermutex2varPd(a M128d, k Mmask8, idx M128i, b M128d) M128d {
	return M128d(maskPermutex2varPd([2]float64(a), uint8(k), [16]byte(idx), [2]float64(b)))
}

func maskPermutex2varPd(a [2]float64, k uint8, idx [16]byte, b [2]float64) [2]float64


// Mask2Permutex2varPd: Shuffle double-precision (64-bit) floating-point
// elements in 'a' and 'b' using the corresponding selector and index in 'idx',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'idx' when the corresponding mask bit is not set) 
//
//		FOR j := 0 to 1
//			i := j*64
//			off := idx[i]*64
//			IF k[j]
//				dst[i+63:i] := idx[i+1] ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := idx[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMI2PD'. Intrinsic: '_mm_mask2_permutex2var_pd'.
// Requires AVX512F.
func Mask2Permutex2varPd(a M128d, idx M128i, k Mmask8, b M128d) M128d {
	return M128d(mask2Permutex2varPd([2]float64(a), [16]byte(idx), uint8(k), [2]float64(b)))
}

func mask2Permutex2varPd(a [2]float64, idx [16]byte, k uint8, b [2]float64) [2]float64


// MaskzPermutex2varPd: Shuffle double-precision (64-bit) floating-point
// elements in 'a' and 'b' using the corresponding selector and index in 'idx',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			off := idx[i]*64
//			IF k[j]
//				dst[i+63:i] := (idx[i+1]) ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMI2PD, VPERMT2PD'. Intrinsic: '_mm_maskz_permutex2var_pd'.
// Requires AVX512F.
func MaskzPermutex2varPd(k Mmask8, a M128d, idx M128i, b M128d) M128d {
	return M128d(maskzPermutex2varPd(uint8(k), [2]float64(a), [16]byte(idx), [2]float64(b)))
}

func maskzPermutex2varPd(k uint8, a [2]float64, idx [16]byte, b [2]float64) [2]float64


// Permutex2varPd: Shuffle double-precision (64-bit) floating-point elements in
// 'a' and 'b' using the corresponding selector and index in 'idx', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			off := idx[i]*64
//			dst[i+63:i] := idx[i+1] ? b[off+63:off] : a[off+63:off]
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMI2PD, VPERMT2PD'. Intrinsic: '_mm_permutex2var_pd'.
// Requires AVX512F.
func Permutex2varPd(a M128d, idx M128i, b M128d) M128d {
	return M128d(permutex2varPd([2]float64(a), [16]byte(idx), [2]float64(b)))
}

func permutex2varPd(a [2]float64, idx [16]byte, b [2]float64) [2]float64


// MaskPermutex2varPd1: Shuffle double-precision (64-bit) floating-point
// elements in 'a' and 'b' across lanes using the corresponding selector and
// index in 'idx', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			off := idx[i+1:i]*64
//			IF k[j]
//				dst[i+63:i] := idx[i+2] ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMT2PD'. Intrinsic: '_mm256_mask_permutex2var_pd'.
// Requires AVX512F.
func MaskPermutex2varPd1(a M256d, k Mmask8, idx M256i, b M256d) M256d {
	return M256d(maskPermutex2varPd1([4]float64(a), uint8(k), [32]byte(idx), [4]float64(b)))
}

func maskPermutex2varPd1(a [4]float64, k uint8, idx [32]byte, b [4]float64) [4]float64


// Mask2Permutex2varPd1: Shuffle double-precision (64-bit) floating-point
// elements in 'a' and 'b' across lanes using the corresponding selector and
// index in 'idx', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'idx' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			off := idx[i+1:i]*64
//			IF k[j]
//				dst[i+63:i] := idx[i+2] ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := idx[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2PD'. Intrinsic: '_mm256_mask2_permutex2var_pd'.
// Requires AVX512F.
func Mask2Permutex2varPd1(a M256d, idx M256i, k Mmask8, b M256d) M256d {
	return M256d(mask2Permutex2varPd1([4]float64(a), [32]byte(idx), uint8(k), [4]float64(b)))
}

func mask2Permutex2varPd1(a [4]float64, idx [32]byte, k uint8, b [4]float64) [4]float64


// MaskzPermutex2varPd1: Shuffle double-precision (64-bit) floating-point
// elements in 'a' and 'b' across lanes using the corresponding selector and
// index in 'idx', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			off := idx[i+1:i]*64
//			IF k[j]
//				dst[i+63:i] := (idx[i+2]) ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2PD, VPERMT2PD'. Intrinsic: '_mm256_maskz_permutex2var_pd'.
// Requires AVX512F.
func MaskzPermutex2varPd1(k Mmask8, a M256d, idx M256i, b M256d) M256d {
	return M256d(maskzPermutex2varPd1(uint8(k), [4]float64(a), [32]byte(idx), [4]float64(b)))
}

func maskzPermutex2varPd1(k uint8, a [4]float64, idx [32]byte, b [4]float64) [4]float64


// Permutex2varPd1: Shuffle double-precision (64-bit) floating-point elements
// in 'a' and 'b' across lanes using the corresponding selector and index in
// 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			off := idx[i+1:i]*64
//			dst[i+63:i] := idx[i+2] ? b[off+63:off] : a[off+63:off]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2PD, VPERMT2PD'. Intrinsic: '_mm256_permutex2var_pd'.
// Requires AVX512F.
func Permutex2varPd1(a M256d, idx M256i, b M256d) M256d {
	return M256d(permutex2varPd1([4]float64(a), [32]byte(idx), [4]float64(b)))
}

func permutex2varPd1(a [4]float64, idx [32]byte, b [4]float64) [4]float64


// MaskPermutex2varPd2: Shuffle double-precision (64-bit) floating-point
// elements in 'a' and 'b' across lanes using the corresponding selector and
// index in 'idx', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			off := idx[i+2:i]*64
//			IF k[j]
//				dst[i+63:i] := idx[i+3] ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMT2PD'. Intrinsic: '_mm512_mask_permutex2var_pd'.
// Requires AVX512F.
func MaskPermutex2varPd2(a M512d, k Mmask8, idx M512i, b M512d) M512d {
	return M512d(maskPermutex2varPd2([8]float64(a), uint8(k), [64]byte(idx), [8]float64(b)))
}

func maskPermutex2varPd2(a [8]float64, k uint8, idx [64]byte, b [8]float64) [8]float64


// Mask2Permutex2varPd2: Shuffle double-precision (64-bit) floating-point
// elements in 'a' and 'b' across lanes using the corresponding selector and
// index in 'idx', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'idx' when the corresponding mask bit is not set) 
//
//		FOR j := 0 to 7
//			i := j*64
//			off := idx[i+2:i]*64
//			IF k[j]
//				dst[i+63:i] := idx[i+3] ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := idx[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMI2PD'. Intrinsic: '_mm512_mask2_permutex2var_pd'.
// Requires AVX512F.
func Mask2Permutex2varPd2(a M512d, idx M512i, k Mmask8, b M512d) M512d {
	return M512d(mask2Permutex2varPd2([8]float64(a), [64]byte(idx), uint8(k), [8]float64(b)))
}

func mask2Permutex2varPd2(a [8]float64, idx [64]byte, k uint8, b [8]float64) [8]float64


// MaskzPermutex2varPd2: Shuffle double-precision (64-bit) floating-point
// elements in 'a' and 'b' across lanes using the corresponding selector and
// index in 'idx', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			off := idx[i+2:i]*64
//			IF k[j]
//				dst[i+63:i] := (idx[i+3]) ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMI2PD, VPERMT2PD'. Intrinsic: '_mm512_maskz_permutex2var_pd'.
// Requires AVX512F.
func MaskzPermutex2varPd2(k Mmask8, a M512d, idx M512i, b M512d) M512d {
	return M512d(maskzPermutex2varPd2(uint8(k), [8]float64(a), [64]byte(idx), [8]float64(b)))
}

func maskzPermutex2varPd2(k uint8, a [8]float64, idx [64]byte, b [8]float64) [8]float64


// Permutex2varPd2: Shuffle double-precision (64-bit) floating-point elements
// in 'a' and 'b' across lanes using the corresponding selector and index in
// 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			off := idx[i+2:i]*64
//			dst[i+63:i] := idx[i+3] ? b[off+63:off] : a[off+63:off]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMI2PD, VPERMT2PD'. Intrinsic: '_mm512_permutex2var_pd'.
// Requires AVX512F.
func Permutex2varPd2(a M512d, idx M512i, b M512d) M512d {
	return M512d(permutex2varPd2([8]float64(a), [64]byte(idx), [8]float64(b)))
}

func permutex2varPd2(a [8]float64, idx [64]byte, b [8]float64) [8]float64


// MaskPermutex2varPs: Shuffle single-precision (32-bit) floating-point
// elements in 'a' and 'b' using the corresponding selector and index in 'idx',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			off := idx[i+1:i]*32
//			IF k[j]
//				dst[i+31:i] := idx[i+2] ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMT2PS'. Intrinsic: '_mm_mask_permutex2var_ps'.
// Requires AVX512F.
func MaskPermutex2varPs(a M128, k Mmask8, idx M128i, b M128) M128 {
	return M128(maskPermutex2varPs([4]float32(a), uint8(k), [16]byte(idx), [4]float32(b)))
}

func maskPermutex2varPs(a [4]float32, k uint8, idx [16]byte, b [4]float32) [4]float32


// Mask2Permutex2varPs: Shuffle single-precision (32-bit) floating-point
// elements in 'a' and 'b' using the corresponding selector and index in 'idx',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'idx' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			off := idx[i+1:i]*32
//			IF k[j]
//				dst[i+31:i] := idx[i+2] ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := idx[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMI2PS'. Intrinsic: '_mm_mask2_permutex2var_ps'.
// Requires AVX512F.
func Mask2Permutex2varPs(a M128, idx M128i, k Mmask8, b M128) M128 {
	return M128(mask2Permutex2varPs([4]float32(a), [16]byte(idx), uint8(k), [4]float32(b)))
}

func mask2Permutex2varPs(a [4]float32, idx [16]byte, k uint8, b [4]float32) [4]float32


// MaskzPermutex2varPs: Shuffle single-precision (32-bit) floating-point
// elements in 'a' and 'b' using the corresponding selector and index in 'idx',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			off := idx[i+1:i]*32
//			IF k[j]
//				dst[i+31:i] := (idx[i+2]) ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMI2PS, VPERMT2PS'. Intrinsic: '_mm_maskz_permutex2var_ps'.
// Requires AVX512F.
func MaskzPermutex2varPs(k Mmask8, a M128, idx M128i, b M128) M128 {
	return M128(maskzPermutex2varPs(uint8(k), [4]float32(a), [16]byte(idx), [4]float32(b)))
}

func maskzPermutex2varPs(k uint8, a [4]float32, idx [16]byte, b [4]float32) [4]float32


// Permutex2varPs: Shuffle single-precision (32-bit) floating-point elements in
// 'a' and 'b' using the corresponding selector and index in 'idx', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			off := idx[i+1:i]*32
//			dst[i+31:i] := idx[i+2] ? b[off+31:off] : a[off+31:off]
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMI2PS, VPERMT2PS'. Intrinsic: '_mm_permutex2var_ps'.
// Requires AVX512F.
func Permutex2varPs(a M128, idx M128i, b M128) M128 {
	return M128(permutex2varPs([4]float32(a), [16]byte(idx), [4]float32(b)))
}

func permutex2varPs(a [4]float32, idx [16]byte, b [4]float32) [4]float32


// MaskPermutex2varPs1: Shuffle single-precision (32-bit) floating-point
// elements in 'a' and 'b' across lanes using the corresponding selector and
// index in 'idx', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			off := idx[i+2:i]*32
//			IF k[j]
//				dst[i+31:i] := idx[i+3] ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMT2PS'. Intrinsic: '_mm256_mask_permutex2var_ps'.
// Requires AVX512F.
func MaskPermutex2varPs1(a M256, k Mmask8, idx M256i, b M256) M256 {
	return M256(maskPermutex2varPs1([8]float32(a), uint8(k), [32]byte(idx), [8]float32(b)))
}

func maskPermutex2varPs1(a [8]float32, k uint8, idx [32]byte, b [8]float32) [8]float32


// Mask2Permutex2varPs1: Shuffle single-precision (32-bit) floating-point
// elements in 'a' and 'b' across lanes using the corresponding selector and
// index in 'idx', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'idx' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			off := idx[i+2:i]*32
//			IF k[j]
//				dst[i+31:i] := idx[i+3] ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := idx[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2PS'. Intrinsic: '_mm256_mask2_permutex2var_ps'.
// Requires AVX512F.
func Mask2Permutex2varPs1(a M256, idx M256i, k Mmask8, b M256) M256 {
	return M256(mask2Permutex2varPs1([8]float32(a), [32]byte(idx), uint8(k), [8]float32(b)))
}

func mask2Permutex2varPs1(a [8]float32, idx [32]byte, k uint8, b [8]float32) [8]float32


// MaskzPermutex2varPs1: Shuffle single-precision (32-bit) floating-point
// elements in 'a' and 'b' across lanes using the corresponding selector and
// index in 'idx', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			off := idx[i+2:i]*32
//			IF k[j]
//				dst[i+31:i] := (idx[i+3]) ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2PS, VPERMT2PS'. Intrinsic: '_mm256_maskz_permutex2var_ps'.
// Requires AVX512F.
func MaskzPermutex2varPs1(k Mmask8, a M256, idx M256i, b M256) M256 {
	return M256(maskzPermutex2varPs1(uint8(k), [8]float32(a), [32]byte(idx), [8]float32(b)))
}

func maskzPermutex2varPs1(k uint8, a [8]float32, idx [32]byte, b [8]float32) [8]float32


// Permutex2varPs1: Shuffle single-precision (32-bit) floating-point elements
// in 'a' and 'b' across lanes using the corresponding selector and index in
// 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			off := idx[i+2:i]*32
//			dst[i+31:i] := idx[i+3] ? b[off+31:off] : a[off+31:off]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2PS, VPERMT2PS'. Intrinsic: '_mm256_permutex2var_ps'.
// Requires AVX512F.
func Permutex2varPs1(a M256, idx M256i, b M256) M256 {
	return M256(permutex2varPs1([8]float32(a), [32]byte(idx), [8]float32(b)))
}

func permutex2varPs1(a [8]float32, idx [32]byte, b [8]float32) [8]float32


// MaskPermutex2varPs2: Shuffle single-precision (32-bit) floating-point
// elements in 'a' and 'b' across lanes using the corresponding selector and
// index in 'idx', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			off := idx[i+3:i]*32
//			IF k[j]
//				dst[i+31:i] := idx[i+4] ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMT2PS'. Intrinsic: '_mm512_mask_permutex2var_ps'.
// Requires AVX512F.
func MaskPermutex2varPs2(a M512, k Mmask16, idx M512i, b M512) M512 {
	return M512(maskPermutex2varPs2([16]float32(a), uint16(k), [64]byte(idx), [16]float32(b)))
}

func maskPermutex2varPs2(a [16]float32, k uint16, idx [64]byte, b [16]float32) [16]float32


// Mask2Permutex2varPs2: Shuffle single-precision (32-bit) floating-point
// elements in 'a' and 'b' across lanes using the corresponding selector and
// index in 'idx', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'idx' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			off := idx[i+3:i]*32
//			IF k[j]
//				dst[i+31:i] := idx[i+4] ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := idx[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMI2PS'. Intrinsic: '_mm512_mask2_permutex2var_ps'.
// Requires AVX512F.
func Mask2Permutex2varPs2(a M512, idx M512i, k Mmask16, b M512) M512 {
	return M512(mask2Permutex2varPs2([16]float32(a), [64]byte(idx), uint16(k), [16]float32(b)))
}

func mask2Permutex2varPs2(a [16]float32, idx [64]byte, k uint16, b [16]float32) [16]float32


// MaskzPermutex2varPs2: Shuffle single-precision (32-bit) floating-point
// elements in 'a' and 'b' across lanes using the corresponding selector and
// index in 'idx', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			off := idx[i+3:i]*32
//			IF k[j]
//				dst[i+31:i] := (idx[i+4]) ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMI2PS, VPERMT2PS'. Intrinsic: '_mm512_maskz_permutex2var_ps'.
// Requires AVX512F.
func MaskzPermutex2varPs2(k Mmask16, a M512, idx M512i, b M512) M512 {
	return M512(maskzPermutex2varPs2(uint16(k), [16]float32(a), [64]byte(idx), [16]float32(b)))
}

func maskzPermutex2varPs2(k uint16, a [16]float32, idx [64]byte, b [16]float32) [16]float32


// Permutex2varPs2: Shuffle single-precision (32-bit) floating-point elements
// in 'a' and 'b' across lanes using the corresponding selector and index in
// 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			off := idx[i+3:i]*32
//			dst[i+31:i] := idx[i+4] ? b[off+31:off] : a[off+31:off]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMI2PS, VPERMT2PS'. Intrinsic: '_mm512_permutex2var_ps'.
// Requires AVX512F.
func Permutex2varPs2(a M512, idx M512i, b M512) M512 {
	return M512(permutex2varPs2([16]float32(a), [64]byte(idx), [16]float32(b)))
}

func permutex2varPs2(a [16]float32, idx [64]byte, b [16]float32) [16]float32


// MaskPermutexvarEpi32: Shuffle 32-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			id := idx[i+2:i]*32
//			IF k[j]
//				dst[i+31:i] := a[id+31:id]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMD'. Intrinsic: '_mm256_mask_permutexvar_epi32'.
// Requires AVX512F.
func MaskPermutexvarEpi32(src M256i, k Mmask8, idx M256i, a M256i) M256i {
	return M256i(maskPermutexvarEpi32([32]byte(src), uint8(k), [32]byte(idx), [32]byte(a)))
}

func maskPermutexvarEpi32(src [32]byte, k uint8, idx [32]byte, a [32]byte) [32]byte


// MaskzPermutexvarEpi32: Shuffle 32-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			id := idx[i+2:i]*32
//			IF k[j]
//				dst[i+31:i] := a[id+31:id]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMD'. Intrinsic: '_mm256_maskz_permutexvar_epi32'.
// Requires AVX512F.
func MaskzPermutexvarEpi32(k Mmask8, idx M256i, a M256i) M256i {
	return M256i(maskzPermutexvarEpi32(uint8(k), [32]byte(idx), [32]byte(a)))
}

func maskzPermutexvarEpi32(k uint8, idx [32]byte, a [32]byte) [32]byte


// PermutexvarEpi32: Shuffle 32-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			id := idx[i+2:i]*32
//			dst[i+31:i] := a[id+31:id]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMD'. Intrinsic: '_mm256_permutexvar_epi32'.
// Requires AVX512F.
func PermutexvarEpi32(idx M256i, a M256i) M256i {
	return M256i(permutexvarEpi32([32]byte(idx), [32]byte(a)))
}

func permutexvarEpi32(idx [32]byte, a [32]byte) [32]byte


// MaskPermutexvarEpi321: Shuffle 32-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			id := idx[i+3:i]*32
//			IF k[j]
//				dst[i+31:i] := a[id+31:id]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMD'. Intrinsic: '_mm512_mask_permutexvar_epi32'.
// Requires AVX512F.
func MaskPermutexvarEpi321(src M512i, k Mmask16, idx M512i, a M512i) M512i {
	return M512i(maskPermutexvarEpi321([64]byte(src), uint16(k), [64]byte(idx), [64]byte(a)))
}

func maskPermutexvarEpi321(src [64]byte, k uint16, idx [64]byte, a [64]byte) [64]byte


// MaskzPermutexvarEpi321: Shuffle 32-bit integers in 'a' across lanes using
// the corresponding index in 'idx', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			id := idx[i+3:i]*32
//			IF k[j]
//				dst[i+31:i] := a[id+31:id]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMD'. Intrinsic: '_mm512_maskz_permutexvar_epi32'.
// Requires AVX512F.
func MaskzPermutexvarEpi321(k Mmask16, idx M512i, a M512i) M512i {
	return M512i(maskzPermutexvarEpi321(uint16(k), [64]byte(idx), [64]byte(a)))
}

func maskzPermutexvarEpi321(k uint16, idx [64]byte, a [64]byte) [64]byte


// PermutexvarEpi321: Shuffle 32-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			id := idx[i+3:i]*32
//			dst[i+31:i] := a[id+31:id]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMD'. Intrinsic: '_mm512_permutexvar_epi32'.
// Requires AVX512F.
func PermutexvarEpi321(idx M512i, a M512i) M512i {
	return M512i(permutexvarEpi321([64]byte(idx), [64]byte(a)))
}

func permutexvarEpi321(idx [64]byte, a [64]byte) [64]byte


// MaskPermutexvarEpi64: Shuffle 64-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			id := idx[i+1:i]*64
//			IF k[j]
//				dst[i+63:i] := a[id+63:id]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMQ'. Intrinsic: '_mm256_mask_permutexvar_epi64'.
// Requires AVX512F.
func MaskPermutexvarEpi64(src M256i, k Mmask8, idx M256i, a M256i) M256i {
	return M256i(maskPermutexvarEpi64([32]byte(src), uint8(k), [32]byte(idx), [32]byte(a)))
}

func maskPermutexvarEpi64(src [32]byte, k uint8, idx [32]byte, a [32]byte) [32]byte


// MaskzPermutexvarEpi64: Shuffle 64-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			id := idx[i+1:i]*64
//			IF k[j]
//				dst[i+63:i] := a[id+63:id]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMQ'. Intrinsic: '_mm256_maskz_permutexvar_epi64'.
// Requires AVX512F.
func MaskzPermutexvarEpi64(k Mmask8, idx M256i, a M256i) M256i {
	return M256i(maskzPermutexvarEpi64(uint8(k), [32]byte(idx), [32]byte(a)))
}

func maskzPermutexvarEpi64(k uint8, idx [32]byte, a [32]byte) [32]byte


// PermutexvarEpi64: Shuffle 64-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			id := idx[i+1:i]*64
//			dst[i+63:i] := a[id+63:id]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMQ'. Intrinsic: '_mm256_permutexvar_epi64'.
// Requires AVX512F.
func PermutexvarEpi64(idx M256i, a M256i) M256i {
	return M256i(permutexvarEpi64([32]byte(idx), [32]byte(a)))
}

func permutexvarEpi64(idx [32]byte, a [32]byte) [32]byte


// MaskPermutexvarEpi641: Shuffle 64-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			id := idx[i+2:i]*64
//			IF k[j]
//				dst[i+63:i] := a[id+63:id]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMQ'. Intrinsic: '_mm512_mask_permutexvar_epi64'.
// Requires AVX512F.
func MaskPermutexvarEpi641(src M512i, k Mmask8, idx M512i, a M512i) M512i {
	return M512i(maskPermutexvarEpi641([64]byte(src), uint8(k), [64]byte(idx), [64]byte(a)))
}

func maskPermutexvarEpi641(src [64]byte, k uint8, idx [64]byte, a [64]byte) [64]byte


// MaskzPermutexvarEpi641: Shuffle 64-bit integers in 'a' across lanes using
// the corresponding index in 'idx', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			id := idx[i+2:i]*64
//			IF k[j]
//				dst[i+63:i] := a[id+63:id]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMQ'. Intrinsic: '_mm512_maskz_permutexvar_epi64'.
// Requires AVX512F.
func MaskzPermutexvarEpi641(k Mmask8, idx M512i, a M512i) M512i {
	return M512i(maskzPermutexvarEpi641(uint8(k), [64]byte(idx), [64]byte(a)))
}

func maskzPermutexvarEpi641(k uint8, idx [64]byte, a [64]byte) [64]byte


// PermutexvarEpi641: Shuffle 64-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			id := idx[i+2:i]*64
//			dst[i+63:i] := a[id+63:id]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMQ'. Intrinsic: '_mm512_permutexvar_epi64'.
// Requires AVX512F.
func PermutexvarEpi641(idx M512i, a M512i) M512i {
	return M512i(permutexvarEpi641([64]byte(idx), [64]byte(a)))
}

func permutexvarEpi641(idx [64]byte, a [64]byte) [64]byte


// MaskPermutexvarPd: Shuffle double-precision (64-bit) floating-point elements
// in 'a' across lanes using the corresponding index in 'idx', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			id := idx[i+1:i]*64
//			IF k[j]
//				dst[i+63:i] := a[id+63:id]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMPD'. Intrinsic: '_mm256_mask_permutexvar_pd'.
// Requires AVX512F.
func MaskPermutexvarPd(src M256d, k Mmask8, idx M256i, a M256d) M256d {
	return M256d(maskPermutexvarPd([4]float64(src), uint8(k), [32]byte(idx), [4]float64(a)))
}

func maskPermutexvarPd(src [4]float64, k uint8, idx [32]byte, a [4]float64) [4]float64


// MaskzPermutexvarPd: Shuffle double-precision (64-bit) floating-point
// elements in 'a' across lanes using the corresponding index in 'idx', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			id := idx[i+1:i]*64
//			IF k[j]
//				dst[i+63:i] := a[id+63:id]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMPD'. Intrinsic: '_mm256_maskz_permutexvar_pd'.
// Requires AVX512F.
func MaskzPermutexvarPd(k Mmask8, idx M256i, a M256d) M256d {
	return M256d(maskzPermutexvarPd(uint8(k), [32]byte(idx), [4]float64(a)))
}

func maskzPermutexvarPd(k uint8, idx [32]byte, a [4]float64) [4]float64


// PermutexvarPd: Shuffle double-precision (64-bit) floating-point elements in
// 'a' across lanes using the corresponding index in 'idx', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			id := idx[i+1:i]*64
//			dst[i+63:i] := a[id+63:id]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMPD'. Intrinsic: '_mm256_permutexvar_pd'.
// Requires AVX512F.
func PermutexvarPd(idx M256i, a M256d) M256d {
	return M256d(permutexvarPd([32]byte(idx), [4]float64(a)))
}

func permutexvarPd(idx [32]byte, a [4]float64) [4]float64


// MaskPermutexvarPd1: Shuffle double-precision (64-bit) floating-point
// elements in 'a' across lanes using the corresponding index in 'idx', and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			id := idx[i+2:i]*64
//			IF k[j]
//				dst[i+63:i] := a[id+63:id]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMPD'. Intrinsic: '_mm512_mask_permutexvar_pd'.
// Requires AVX512F.
func MaskPermutexvarPd1(src M512d, k Mmask8, idx M512i, a M512d) M512d {
	return M512d(maskPermutexvarPd1([8]float64(src), uint8(k), [64]byte(idx), [8]float64(a)))
}

func maskPermutexvarPd1(src [8]float64, k uint8, idx [64]byte, a [8]float64) [8]float64


// MaskzPermutexvarPd1: Shuffle double-precision (64-bit) floating-point
// elements in 'a' across lanes using the corresponding index in 'idx', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			id := idx[i+2:i]*64
//			IF k[j]
//				dst[i+63:i] := a[id+63:id]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMPD'. Intrinsic: '_mm512_maskz_permutexvar_pd'.
// Requires AVX512F.
func MaskzPermutexvarPd1(k Mmask8, idx M512i, a M512d) M512d {
	return M512d(maskzPermutexvarPd1(uint8(k), [64]byte(idx), [8]float64(a)))
}

func maskzPermutexvarPd1(k uint8, idx [64]byte, a [8]float64) [8]float64


// PermutexvarPd1: Shuffle double-precision (64-bit) floating-point elements in
// 'a' across lanes using the corresponding index in 'idx', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			id := idx[i+2:i]*64
//			dst[i+63:i] := a[id+63:id]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMPD'. Intrinsic: '_mm512_permutexvar_pd'.
// Requires AVX512F.
func PermutexvarPd1(idx M512i, a M512d) M512d {
	return M512d(permutexvarPd1([64]byte(idx), [8]float64(a)))
}

func permutexvarPd1(idx [64]byte, a [8]float64) [8]float64


// MaskPermutexvarPs: Shuffle single-precision (32-bit) floating-point elements
// in 'a' across lanes using the corresponding index in 'idx', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			id := idx[i+2:i]*32
//			IF k[j]
//				dst[i+31:i] := a[id+31:id]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMPS'. Intrinsic: '_mm256_mask_permutexvar_ps'.
// Requires AVX512F.
func MaskPermutexvarPs(src M256, k Mmask8, idx M256i, a M256) M256 {
	return M256(maskPermutexvarPs([8]float32(src), uint8(k), [32]byte(idx), [8]float32(a)))
}

func maskPermutexvarPs(src [8]float32, k uint8, idx [32]byte, a [8]float32) [8]float32


// MaskzPermutexvarPs: Shuffle single-precision (32-bit) floating-point
// elements in 'a' across lanes using the corresponding index in 'idx', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			id := idx[i+2:i]*32
//			IF k[j]
//				dst[i+31:i] := a[id+31:id]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMPS'. Intrinsic: '_mm256_maskz_permutexvar_ps'.
// Requires AVX512F.
func MaskzPermutexvarPs(k Mmask8, idx M256i, a M256) M256 {
	return M256(maskzPermutexvarPs(uint8(k), [32]byte(idx), [8]float32(a)))
}

func maskzPermutexvarPs(k uint8, idx [32]byte, a [8]float32) [8]float32


// PermutexvarPs: Shuffle single-precision (32-bit) floating-point elements in
// 'a' across lanes using the corresponding index in 'idx'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			id := idx[i+2:i]*32
//			dst[i+31:i] := a[id+31:id]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMPS'. Intrinsic: '_mm256_permutexvar_ps'.
// Requires AVX512F.
func PermutexvarPs(idx M256i, a M256) M256 {
	return M256(permutexvarPs([32]byte(idx), [8]float32(a)))
}

func permutexvarPs(idx [32]byte, a [8]float32) [8]float32


// MaskPermutexvarPs1: Shuffle single-precision (32-bit) floating-point
// elements in 'a' across lanes using the corresponding index in 'idx', and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			id := idx[i+3:i]*32
//			IF k[j]
//				dst[i+31:i] := a[id+31:id]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMPS'. Intrinsic: '_mm512_mask_permutexvar_ps'.
// Requires AVX512F.
func MaskPermutexvarPs1(src M512, k Mmask16, idx M512i, a M512) M512 {
	return M512(maskPermutexvarPs1([16]float32(src), uint16(k), [64]byte(idx), [16]float32(a)))
}

func maskPermutexvarPs1(src [16]float32, k uint16, idx [64]byte, a [16]float32) [16]float32


// MaskzPermutexvarPs1: Shuffle single-precision (32-bit) floating-point
// elements in 'a' across lanes using the corresponding index in 'idx', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			id := idx[i+3:i]*32
//			IF k[j]
//				dst[i+31:i] := a[id+31:id]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMPS'. Intrinsic: '_mm512_maskz_permutexvar_ps'.
// Requires AVX512F.
func MaskzPermutexvarPs1(k Mmask16, idx M512i, a M512) M512 {
	return M512(maskzPermutexvarPs1(uint16(k), [64]byte(idx), [16]float32(a)))
}

func maskzPermutexvarPs1(k uint16, idx [64]byte, a [16]float32) [16]float32


// PermutexvarPs1: Shuffle single-precision (32-bit) floating-point elements in
// 'a' across lanes using the corresponding index in 'idx'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			id := idx[i+3:i]*32
//			dst[i+31:i] := a[id+31:id]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPERMPS'. Intrinsic: '_mm512_permutexvar_ps'.
// Requires AVX512F.
func PermutexvarPs1(idx M512i, a M512) M512 {
	return M512(permutexvarPs1([64]byte(idx), [16]float32(a)))
}

func permutexvarPs1(idx [64]byte, a [16]float32) [16]float32


// MaskPowPd: Compute the exponential value of packed double-precision (64-bit)
// floating-point elements in 'a' raised by packed elements in 'b', and store
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i])^(b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_pow_pd'.
// Requires AVX512F.
func MaskPowPd(src M512d, k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskPowPd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b)))
}

func maskPowPd(src [8]float64, k uint8, a [8]float64, b [8]float64) [8]float64


// PowPd: Compute the exponential value of packed double-precision (64-bit)
// floating-point elements in 'a' raised by packed elements in 'b', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := (a[i+63:i])^(b[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_pow_pd'.
// Requires AVX512F.
func PowPd(a M512d, b M512d) M512d {
	return M512d(powPd([8]float64(a), [8]float64(b)))
}

func powPd(a [8]float64, b [8]float64) [8]float64


// MaskPowPs: Compute the exponential value of packed single-precision (32-bit)
// floating-point elements in 'a' raised by packed elements in 'b', and store
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i])^(b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_pow_ps'.
// Requires AVX512F.
func MaskPowPs(src M512, k Mmask16, a M512, b M512) M512 {
	return M512(maskPowPs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b)))
}

func maskPowPs(src [16]float32, k uint16, a [16]float32, b [16]float32) [16]float32


// PowPs: Compute the exponential value of packed single-precision (32-bit)
// floating-point elements in 'a' raised by packed elements in 'b', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := (a[i+31:i])^(b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_pow_ps'.
// Requires AVX512F.
func PowPs(a M512, b M512) M512 {
	return M512(powPs([16]float32(a), [16]float32(b)))
}

func powPs(a [16]float32, b [16]float32) [16]float32


// MaskRcp14Pd: Compute the approximate reciprocal of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). The maximum relative error for this approximation is
// less than 2^-14. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := APPROXIMATE(1.0/a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRCP14PD'. Intrinsic: '_mm_mask_rcp14_pd'.
// Requires AVX512F.
func MaskRcp14Pd(src M128d, k Mmask8, a M128d) M128d {
	return M128d(maskRcp14Pd([2]float64(src), uint8(k), [2]float64(a)))
}

func maskRcp14Pd(src [2]float64, k uint8, a [2]float64) [2]float64


// MaskzRcp14Pd: Compute the approximate reciprocal of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). The maximum relative error for this approximation is less than
// 2^-14. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := APPROXIMATE(1.0/a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRCP14PD'. Intrinsic: '_mm_maskz_rcp14_pd'.
// Requires AVX512F.
func MaskzRcp14Pd(k Mmask8, a M128d) M128d {
	return M128d(maskzRcp14Pd(uint8(k), [2]float64(a)))
}

func maskzRcp14Pd(k uint8, a [2]float64) [2]float64


// Rcp14Pd: Compute the approximate reciprocal of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'. The
// maximum relative error for this approximation is less than 2^-14. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := APPROXIMATE(1.0/a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRCP14PD'. Intrinsic: '_mm_rcp14_pd'.
// Requires AVX512F.
func Rcp14Pd(a M128d) M128d {
	return M128d(rcp14Pd([2]float64(a)))
}

func rcp14Pd(a [2]float64) [2]float64


// MaskRcp14Pd1: Compute the approximate reciprocal of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). The maximum relative error for this approximation is
// less than 2^-14. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := APPROXIMATE(1.0/a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRCP14PD'. Intrinsic: '_mm256_mask_rcp14_pd'.
// Requires AVX512F.
func MaskRcp14Pd1(src M256d, k Mmask8, a M256d) M256d {
	return M256d(maskRcp14Pd1([4]float64(src), uint8(k), [4]float64(a)))
}

func maskRcp14Pd1(src [4]float64, k uint8, a [4]float64) [4]float64


// MaskzRcp14Pd1: Compute the approximate reciprocal of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). The maximum relative error for this approximation is less than
// 2^-14. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := APPROXIMATE(1.0/a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRCP14PD'. Intrinsic: '_mm256_maskz_rcp14_pd'.
// Requires AVX512F.
func MaskzRcp14Pd1(k Mmask8, a M256d) M256d {
	return M256d(maskzRcp14Pd1(uint8(k), [4]float64(a)))
}

func maskzRcp14Pd1(k uint8, a [4]float64) [4]float64


// Rcp14Pd1: Compute the approximate reciprocal of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'. The
// maximum relative error for this approximation is less than 2^-14. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := APPROXIMATE(1.0/a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRCP14PD'. Intrinsic: '_mm256_rcp14_pd'.
// Requires AVX512F.
func Rcp14Pd1(a M256d) M256d {
	return M256d(rcp14Pd1([4]float64(a)))
}

func rcp14Pd1(a [4]float64) [4]float64


// MaskRcp14Pd2: Compute the approximate reciprocal of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). The maximum relative error for this approximation is
// less than 2^-14. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := APPROXIMATE(1.0/a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRCP14PD'. Intrinsic: '_mm512_mask_rcp14_pd'.
// Requires AVX512F.
func MaskRcp14Pd2(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskRcp14Pd2([8]float64(src), uint8(k), [8]float64(a)))
}

func maskRcp14Pd2(src [8]float64, k uint8, a [8]float64) [8]float64


// MaskzRcp14Pd2: Compute the approximate reciprocal of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). The maximum relative error for this approximation is less than
// 2^-14. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := APPROXIMATE(1.0/a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRCP14PD'. Intrinsic: '_mm512_maskz_rcp14_pd'.
// Requires AVX512F.
func MaskzRcp14Pd2(k Mmask8, a M512d) M512d {
	return M512d(maskzRcp14Pd2(uint8(k), [8]float64(a)))
}

func maskzRcp14Pd2(k uint8, a [8]float64) [8]float64


// Rcp14Pd2: Compute the approximate reciprocal of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'. The
// maximum relative error for this approximation is less than 2^-14. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := APPROXIMATE(1.0/a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRCP14PD'. Intrinsic: '_mm512_rcp14_pd'.
// Requires AVX512F.
func Rcp14Pd2(a M512d) M512d {
	return M512d(rcp14Pd2([8]float64(a)))
}

func rcp14Pd2(a [8]float64) [8]float64


// MaskRcp14Ps: Compute the approximate reciprocal of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). The maximum relative error for this approximation is
// less than 2^-14. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRCP14PS'. Intrinsic: '_mm_mask_rcp14_ps'.
// Requires AVX512F.
func MaskRcp14Ps(src M128, k Mmask8, a M128) M128 {
	return M128(maskRcp14Ps([4]float32(src), uint8(k), [4]float32(a)))
}

func maskRcp14Ps(src [4]float32, k uint8, a [4]float32) [4]float32


// MaskzRcp14Ps: Compute the approximate reciprocal of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). The maximum relative error for this approximation is less than
// 2^-14. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRCP14PS'. Intrinsic: '_mm_maskz_rcp14_ps'.
// Requires AVX512F.
func MaskzRcp14Ps(k Mmask8, a M128) M128 {
	return M128(maskzRcp14Ps(uint8(k), [4]float32(a)))
}

func maskzRcp14Ps(k uint8, a [4]float32) [4]float32


// Rcp14Ps: Compute the approximate reciprocal of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'. The
// maximum relative error for this approximation is less than 2^-14. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRCP14PS'. Intrinsic: '_mm_rcp14_ps'.
// Requires AVX512F.
func Rcp14Ps(a M128) M128 {
	return M128(rcp14Ps([4]float32(a)))
}

func rcp14Ps(a [4]float32) [4]float32


// MaskRcp14Ps1: Compute the approximate reciprocal of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). The maximum relative error for this approximation is
// less than 2^-14. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRCP14PS'. Intrinsic: '_mm256_mask_rcp14_ps'.
// Requires AVX512F.
func MaskRcp14Ps1(src M256, k Mmask8, a M256) M256 {
	return M256(maskRcp14Ps1([8]float32(src), uint8(k), [8]float32(a)))
}

func maskRcp14Ps1(src [8]float32, k uint8, a [8]float32) [8]float32


// MaskzRcp14Ps1: Compute the approximate reciprocal of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). The maximum relative error for this approximation is less than
// 2^-14. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRCP14PS'. Intrinsic: '_mm256_maskz_rcp14_ps'.
// Requires AVX512F.
func MaskzRcp14Ps1(k Mmask8, a M256) M256 {
	return M256(maskzRcp14Ps1(uint8(k), [8]float32(a)))
}

func maskzRcp14Ps1(k uint8, a [8]float32) [8]float32


// Rcp14Ps1: Compute the approximate reciprocal of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'. The
// maximum relative error for this approximation is less than 2^-14. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRCP14PS'. Intrinsic: '_mm256_rcp14_ps'.
// Requires AVX512F.
func Rcp14Ps1(a M256) M256 {
	return M256(rcp14Ps1([8]float32(a)))
}

func rcp14Ps1(a [8]float32) [8]float32


// MaskRcp14Ps2: Compute the approximate reciprocal of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). The maximum relative error for this approximation is
// less than 2^-14. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRCP14PS'. Intrinsic: '_mm512_mask_rcp14_ps'.
// Requires AVX512F.
func MaskRcp14Ps2(src M512, k Mmask16, a M512) M512 {
	return M512(maskRcp14Ps2([16]float32(src), uint16(k), [16]float32(a)))
}

func maskRcp14Ps2(src [16]float32, k uint16, a [16]float32) [16]float32


// MaskzRcp14Ps2: Compute the approximate reciprocal of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). The maximum relative error for this approximation is less than
// 2^-14. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRCP14PS'. Intrinsic: '_mm512_maskz_rcp14_ps'.
// Requires AVX512F.
func MaskzRcp14Ps2(k Mmask16, a M512) M512 {
	return M512(maskzRcp14Ps2(uint16(k), [16]float32(a)))
}

func maskzRcp14Ps2(k uint16, a [16]float32) [16]float32


// Rcp14Ps2: Compute the approximate reciprocal of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'. The
// maximum relative error for this approximation is less than 2^-14. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRCP14PS'. Intrinsic: '_mm512_rcp14_ps'.
// Requires AVX512F.
func Rcp14Ps2(a M512) M512 {
	return M512(rcp14Ps2([16]float32(a)))
}

func rcp14Ps2(a [16]float32) [16]float32


// MaskRcp14Sd: Compute the approximate reciprocal of the lower
// double-precision (64-bit) floating-point element in 'b', store the result in
// the lower element of 'dst' using writemask 'k' (the element is copied from
// 'src' when mask bit 0 is not set), and copy the upper element from 'a' to
// the upper element of 'dst'. The maximum relative error for this
// approximation is less than 2^-14. 
//
//		IF k[0]
//			dst[63:0] := APPROXIMATE(1.0/b[63:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VRCP14SD'. Intrinsic: '_mm_mask_rcp14_sd'.
// Requires AVX512F.
func MaskRcp14Sd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskRcp14Sd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskRcp14Sd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzRcp14Sd: Compute the approximate reciprocal of the lower
// double-precision (64-bit) floating-point element in 'b', store the result in
// the lower element of 'dst' using zeromask 'k' (the element is zeroed out
// when mask bit 0 is not set), and copy the upper element from 'a' to the
// upper element of 'dst'. The maximum relative error for this approximation is
// less than 2^-14. 
//
//		IF k[0]
//			dst[63:0] := APPROXIMATE(1.0/b[63:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VRCP14SD'. Intrinsic: '_mm_maskz_rcp14_sd'.
// Requires AVX512F.
func MaskzRcp14Sd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzRcp14Sd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzRcp14Sd(k uint8, a [2]float64, b [2]float64) [2]float64


// Rcp14Sd: Compute the approximate reciprocal of the lower double-precision
// (64-bit) floating-point element in 'b', store the result in the lower
// element of 'dst', and copy the upper element from 'a' to the upper element
// of 'dst'. The maximum relative error for this approximation is less than
// 2^-14. 
//
//		dst[63:0] := APPROXIMATE(1.0/b[63:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VRCP14SD'. Intrinsic: '_mm_rcp14_sd'.
// Requires AVX512F.
func Rcp14Sd(a M128d, b M128d) M128d {
	return M128d(rcp14Sd([2]float64(a), [2]float64(b)))
}

func rcp14Sd(a [2]float64, b [2]float64) [2]float64


// MaskRcp14Ss: Compute the approximate reciprocal of the lower
// single-precision (32-bit) floating-point element in 'b', store the result in
// the lower element of 'dst' using writemask 'k' (the element is copied from
// 'src' when mask bit 0 is not set), and copy the upper 3 packed elements from
// 'a' to the upper elements of 'dst'. The maximum relative error for this
// approximation is less than 2^-14. 
//
//		IF k[0]
//			dst[31:0] := APPROXIMATE(1.0/b[31:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VRCP14SS'. Intrinsic: '_mm_mask_rcp14_ss'.
// Requires AVX512F.
func MaskRcp14Ss(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskRcp14Ss([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskRcp14Ss(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzRcp14Ss: Compute the approximate reciprocal of the lower
// single-precision (32-bit) floating-point element in 'b', store the result in
// the lower element of 'dst' using zeromask 'k' (the element is zeroed out
// when mask bit 0 is not set), and copy the upper 3 packed elements from 'a'
// to the upper elements of 'dst'. The maximum relative error for this
// approximation is less than 2^-14. 
//
//		IF k[0]
//			dst[31:0] := APPROXIMATE(1.0/b[31:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VRCP14SS'. Intrinsic: '_mm_maskz_rcp14_ss'.
// Requires AVX512F.
func MaskzRcp14Ss(k Mmask8, a M128, b M128) M128 {
	return M128(maskzRcp14Ss(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzRcp14Ss(k uint8, a [4]float32, b [4]float32) [4]float32


// Rcp14Ss: Compute the approximate reciprocal of the lower single-precision
// (32-bit) floating-point element in 'b', store the result in the lower
// element of 'dst', and copy the upper 3 packed elements from 'a' to the upper
// elements of 'dst'. The maximum relative error for this approximation is less
// than 2^-14. 
//
//		dst[31:0] := APPROXIMATE(1.0/b[31:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VRCP14SS'. Intrinsic: '_mm_rcp14_ss'.
// Requires AVX512F.
func Rcp14Ss(a M128, b M128) M128 {
	return M128(rcp14Ss([4]float32(a), [4]float32(b)))
}

func rcp14Ss(a [4]float32, b [4]float32) [4]float32


// MaskRecipPd: Computes the reciprocal of packed double-precision (64-bit)
// floating-point elements in 'a', storing the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (1 / a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_recip_pd'.
// Requires AVX512F.
func MaskRecipPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskRecipPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskRecipPd(src [8]float64, k uint8, a [8]float64) [8]float64


// RecipPd: Computes the reciprocal of packed double-precision (64-bit)
// floating-point elements in 'a', storing the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := (1 / a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_recip_pd'.
// Requires AVX512F.
func RecipPd(a M512d) M512d {
	return M512d(recipPd([8]float64(a)))
}

func recipPd(a [8]float64) [8]float64


// MaskRecipPs: Computes the reciprocal of packed single-precision (32-bit)
// floating-point elements in 'a', storing the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (1 / a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_recip_ps'.
// Requires AVX512F.
func MaskRecipPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskRecipPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskRecipPs(src [16]float32, k uint16, a [16]float32) [16]float32


// RecipPs: Computes the reciprocal of packed single-precision (32-bit)
// floating-point elements in 'a', storing the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := (1 / a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_recip_ps'.
// Requires AVX512F.
func RecipPs(a M512) M512 {
	return M512(recipPs([16]float32(a)))
}

func recipPs(a [16]float32) [16]float32


// RemEpi16: Divide packed 16-bit integers in 'a' by packed elements in 'b',
// and store the remainders as packed 32-bit integers in 'dst'. 
//
//		FOR j := 0 to 31
//			i := 16*j
//			dst[i+15:i] := REMAINDER(a[i+15:i] / b[i+15:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_rem_epi16'.
// Requires AVX512F.
func RemEpi16(a M512i, b M512i) M512i {
	return M512i(remEpi16([64]byte(a), [64]byte(b)))
}

func remEpi16(a [64]byte, b [64]byte) [64]byte


// MaskRemEpi32: Divide packed 32-bit integers in 'a' by packed elements in
// 'b', and store the remainders as packed 32-bit integers in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := REMAINDER(a[i+31:i] / b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_rem_epi32'.
// Requires AVX512F.
func MaskRemEpi32(src M512i, k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskRemEpi32([64]byte(src), uint16(k), [64]byte(a), [64]byte(b)))
}

func maskRemEpi32(src [64]byte, k uint16, a [64]byte, b [64]byte) [64]byte


// RemEpi32: Divide packed 32-bit integers in 'a' by packed elements in 'b',
// and store the remainders as packed 32-bit integers in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := REMAINDER(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_rem_epi32'.
// Requires AVX512F.
func RemEpi32(a M512i, b M512i) M512i {
	return M512i(remEpi32([64]byte(a), [64]byte(b)))
}

func remEpi32(a [64]byte, b [64]byte) [64]byte


// RemEpi64: Divide packed 64-bit integers in 'a' by packed elements in 'b',
// and store the remainders as packed 32-bit integers in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			dst[i+63:i] := REMAINDER(a[i+63:i] / b[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_rem_epi64'.
// Requires AVX512F.
func RemEpi64(a M512i, b M512i) M512i {
	return M512i(remEpi64([64]byte(a), [64]byte(b)))
}

func remEpi64(a [64]byte, b [64]byte) [64]byte


// RemEpi8: Divide packed 8-bit integers in 'a' by packed elements in 'b', and
// store the remainders as packed 32-bit integers in 'dst'. 
//
//		FOR j := 0 to 63
//			i := 8*j
//			dst[i+7:i] := REMAINDER(a[i+7:i] / b[i+7:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_rem_epi8'.
// Requires AVX512F.
func RemEpi8(a M512i, b M512i) M512i {
	return M512i(remEpi8([64]byte(a), [64]byte(b)))
}

func remEpi8(a [64]byte, b [64]byte) [64]byte


// RemEpu16: Divide packed unsigned 16-bit integers in 'a' by packed elements
// in 'b', and store the remainders as packed unsigned 32-bit integers in
// 'dst'. 
//
//		FOR j := 0 to 31
//			i := 16*j
//			dst[i+15:i] := REMAINDER(a[i+15:i] / b[i+15:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_rem_epu16'.
// Requires AVX512F.
func RemEpu16(a M512i, b M512i) M512i {
	return M512i(remEpu16([64]byte(a), [64]byte(b)))
}

func remEpu16(a [64]byte, b [64]byte) [64]byte


// MaskRemEpu32: Divide packed unsigned 32-bit integers in 'a' by packed
// elements in 'b', and store the remainders as packed unsigned 32-bit integers
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := REMAINDER(a[i+31:i] / b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_rem_epu32'.
// Requires AVX512F.
func MaskRemEpu32(src M512i, k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskRemEpu32([64]byte(src), uint16(k), [64]byte(a), [64]byte(b)))
}

func maskRemEpu32(src [64]byte, k uint16, a [64]byte, b [64]byte) [64]byte


// RemEpu32: Divide packed unsigned 32-bit integers in 'a' by packed elements
// in 'b', and store the remainders as packed unsigned 32-bit integers in
// 'dst'. 
//
//		FOR j := 0 to 15
//			i := 32*j
//			dst[i+31:i] := REMAINDER(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_rem_epu32'.
// Requires AVX512F.
func RemEpu32(a M512i, b M512i) M512i {
	return M512i(remEpu32([64]byte(a), [64]byte(b)))
}

func remEpu32(a [64]byte, b [64]byte) [64]byte


// RemEpu64: Divide packed unsigned 64-bit integers in 'a' by packed elements
// in 'b', and store the remainders as packed unsigned 32-bit integers in
// 'dst'. 
//
//		FOR j := 0 to 7
//			i := 64*j
//			dst[i+63:i] := REMAINDER(a[i+63:i] / b[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_rem_epu64'.
// Requires AVX512F.
func RemEpu64(a M512i, b M512i) M512i {
	return M512i(remEpu64([64]byte(a), [64]byte(b)))
}

func remEpu64(a [64]byte, b [64]byte) [64]byte


// RemEpu8: Divide packed unsigned 8-bit integers in 'a' by packed elements in
// 'b', and store the remainders as packed unsigned 32-bit integers in 'dst'. 
//
//		FOR j := 0 to 63
//			i := 8*j
//			dst[i+7:i] := REMAINDER(a[i+7:i] / b[i+7:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_rem_epu8'.
// Requires AVX512F.
func RemEpu8(a M512i, b M512i) M512i {
	return M512i(remEpu8([64]byte(a), [64]byte(b)))
}

func remEpu8(a [64]byte, b [64]byte) [64]byte


// MaskRintPd: Rounds the packed double-precision (64-bit) floating-point
// elements in 'a' to the nearest even integer value and stores the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RoundToNearestEven(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_rint_pd'.
// Requires AVX512F.
func MaskRintPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskRintPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskRintPd(src [8]float64, k uint8, a [8]float64) [8]float64


// RintPd: Rounds the packed double-precision (64-bit) floating-point elements
// in 'a' to the nearest even integer value and stores the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := RoundToNearestEven(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_rint_pd'.
// Requires AVX512F.
func RintPd(a M512d) M512d {
	return M512d(rintPd([8]float64(a)))
}

func rintPd(a [8]float64) [8]float64


// MaskRintPs: Rounds the packed single-precision (32-bit) floating-point
// elements in 'a' to the nearest even integer value and stores the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RoundToNearestEven(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_rint_ps'.
// Requires AVX512F.
func MaskRintPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskRintPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskRintPs(src [16]float32, k uint16, a [16]float32) [16]float32


// RintPs: Rounds the packed single-precision (32-bit) floating-point elements
// in 'a' to the nearest even integer value and stores the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := RoundToNearestEven(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_rint_ps'.
// Requires AVX512F.
func RintPs(a M512) M512 {
	return M512(rintPs([16]float32(a)))
}

func rintPs(a [16]float32) [16]float32


// MaskRolEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the
// left by the number of bits specified in 'imm8', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPROLD'. Intrinsic: '_mm_mask_rol_epi32'.
// Requires AVX512F.
func MaskRolEpi32(src M128i, k Mmask8, a M128i, imm8 int) M128i {
	return M128i(maskRolEpi32([16]byte(src), uint8(k), [16]byte(a), imm8))
}

func maskRolEpi32(src [16]byte, k uint8, a [16]byte, imm8 int) [16]byte


// MaskzRolEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the
// left by the number of bits specified in 'imm8', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPROLD'. Intrinsic: '_mm_maskz_rol_epi32'.
// Requires AVX512F.
func MaskzRolEpi32(k Mmask8, a M128i, imm8 int) M128i {
	return M128i(maskzRolEpi32(uint8(k), [16]byte(a), imm8))
}

func maskzRolEpi32(k uint8, a [16]byte, imm8 int) [16]byte


// RolEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the left
// by the number of bits specified in 'imm8', and store the results in 'dst'. 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPROLD'. Intrinsic: '_mm_rol_epi32'.
// Requires AVX512F.
func RolEpi32(a M128i, imm8 int) M128i {
	return M128i(rolEpi32([16]byte(a), imm8))
}

func rolEpi32(a [16]byte, imm8 int) [16]byte


// MaskRolEpi321: Rotate the bits in each packed 32-bit integer in 'a' to the
// left by the number of bits specified in 'imm8', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLD'. Intrinsic: '_mm256_mask_rol_epi32'.
// Requires AVX512F.
func MaskRolEpi321(src M256i, k Mmask8, a M256i, imm8 int) M256i {
	return M256i(maskRolEpi321([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func maskRolEpi321(src [32]byte, k uint8, a [32]byte, imm8 int) [32]byte


// MaskzRolEpi321: Rotate the bits in each packed 32-bit integer in 'a' to the
// left by the number of bits specified in 'imm8', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLD'. Intrinsic: '_mm256_maskz_rol_epi32'.
// Requires AVX512F.
func MaskzRolEpi321(k Mmask8, a M256i, imm8 int) M256i {
	return M256i(maskzRolEpi321(uint8(k), [32]byte(a), imm8))
}

func maskzRolEpi321(k uint8, a [32]byte, imm8 int) [32]byte


// RolEpi321: Rotate the bits in each packed 32-bit integer in 'a' to the left
// by the number of bits specified in 'imm8', and store the results in 'dst'. 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLD'. Intrinsic: '_mm256_rol_epi32'.
// Requires AVX512F.
func RolEpi321(a M256i, imm8 int) M256i {
	return M256i(rolEpi321([32]byte(a), imm8))
}

func rolEpi321(a [32]byte, imm8 int) [32]byte


// MaskRolEpi322: Rotate the bits in each packed 32-bit integer in 'a' to the
// left by the number of bits specified in 'imm8', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPROLD'. Intrinsic: '_mm512_mask_rol_epi32'.
// Requires AVX512F.
func MaskRolEpi322(src M512i, k Mmask16, a M512i, imm8 int) M512i {
	return M512i(maskRolEpi322([64]byte(src), uint16(k), [64]byte(a), imm8))
}

func maskRolEpi322(src [64]byte, k uint16, a [64]byte, imm8 int) [64]byte


// MaskzRolEpi322: Rotate the bits in each packed 32-bit integer in 'a' to the
// left by the number of bits specified in 'imm8', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPROLD'. Intrinsic: '_mm512_maskz_rol_epi32'.
// Requires AVX512F.
func MaskzRolEpi322(k Mmask16, a M512i, imm8 int) M512i {
	return M512i(maskzRolEpi322(uint16(k), [64]byte(a), imm8))
}

func maskzRolEpi322(k uint16, a [64]byte, imm8 int) [64]byte


// RolEpi322: Rotate the bits in each packed 32-bit integer in 'a' to the left
// by the number of bits specified in 'imm8', and store the results in 'dst'. 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPROLD'. Intrinsic: '_mm512_rol_epi32'.
// Requires AVX512F.
func RolEpi322(a M512i, imm8 int) M512i {
	return M512i(rolEpi322([64]byte(a), imm8))
}

func rolEpi322(a [64]byte, imm8 int) [64]byte


// MaskRolEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the
// left by the number of bits specified in 'imm8', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPROLQ'. Intrinsic: '_mm_mask_rol_epi64'.
// Requires AVX512F.
func MaskRolEpi64(src M128i, k Mmask8, a M128i, imm8 int) M128i {
	return M128i(maskRolEpi64([16]byte(src), uint8(k), [16]byte(a), imm8))
}

func maskRolEpi64(src [16]byte, k uint8, a [16]byte, imm8 int) [16]byte


// MaskzRolEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the
// left by the number of bits specified in 'imm8', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPROLQ'. Intrinsic: '_mm_maskz_rol_epi64'.
// Requires AVX512F.
func MaskzRolEpi64(k Mmask8, a M128i, imm8 int) M128i {
	return M128i(maskzRolEpi64(uint8(k), [16]byte(a), imm8))
}

func maskzRolEpi64(k uint8, a [16]byte, imm8 int) [16]byte


// RolEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the left
// by the number of bits specified in 'imm8', and store the results in 'dst'. 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPROLQ'. Intrinsic: '_mm_rol_epi64'.
// Requires AVX512F.
func RolEpi64(a M128i, imm8 int) M128i {
	return M128i(rolEpi64([16]byte(a), imm8))
}

func rolEpi64(a [16]byte, imm8 int) [16]byte


// MaskRolEpi641: Rotate the bits in each packed 64-bit integer in 'a' to the
// left by the number of bits specified in 'imm8', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLQ'. Intrinsic: '_mm256_mask_rol_epi64'.
// Requires AVX512F.
func MaskRolEpi641(src M256i, k Mmask8, a M256i, imm8 int) M256i {
	return M256i(maskRolEpi641([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func maskRolEpi641(src [32]byte, k uint8, a [32]byte, imm8 int) [32]byte


// MaskzRolEpi641: Rotate the bits in each packed 64-bit integer in 'a' to the
// left by the number of bits specified in 'imm8', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLQ'. Intrinsic: '_mm256_maskz_rol_epi64'.
// Requires AVX512F.
func MaskzRolEpi641(k Mmask8, a M256i, imm8 int) M256i {
	return M256i(maskzRolEpi641(uint8(k), [32]byte(a), imm8))
}

func maskzRolEpi641(k uint8, a [32]byte, imm8 int) [32]byte


// RolEpi641: Rotate the bits in each packed 64-bit integer in 'a' to the left
// by the number of bits specified in 'imm8', and store the results in 'dst'. 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLQ'. Intrinsic: '_mm256_rol_epi64'.
// Requires AVX512F.
func RolEpi641(a M256i, imm8 int) M256i {
	return M256i(rolEpi641([32]byte(a), imm8))
}

func rolEpi641(a [32]byte, imm8 int) [32]byte


// MaskRolEpi642: Rotate the bits in each packed 64-bit integer in 'a' to the
// left by the number of bits specified in 'imm8', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPROLQ'. Intrinsic: '_mm512_mask_rol_epi64'.
// Requires AVX512F.
func MaskRolEpi642(src M512i, k Mmask8, a M512i, imm8 int) M512i {
	return M512i(maskRolEpi642([64]byte(src), uint8(k), [64]byte(a), imm8))
}

func maskRolEpi642(src [64]byte, k uint8, a [64]byte, imm8 int) [64]byte


// MaskzRolEpi642: Rotate the bits in each packed 64-bit integer in 'a' to the
// left by the number of bits specified in 'imm8', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPROLQ'. Intrinsic: '_mm512_maskz_rol_epi64'.
// Requires AVX512F.
func MaskzRolEpi642(k Mmask8, a M512i, imm8 int) M512i {
	return M512i(maskzRolEpi642(uint8(k), [64]byte(a), imm8))
}

func maskzRolEpi642(k uint8, a [64]byte, imm8 int) [64]byte


// RolEpi642: Rotate the bits in each packed 64-bit integer in 'a' to the left
// by the number of bits specified in 'imm8', and store the results in 'dst'. 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPROLQ'. Intrinsic: '_mm512_rol_epi64'.
// Requires AVX512F.
func RolEpi642(a M512i, imm8 int) M512i {
	return M512i(rolEpi642([64]byte(a), imm8))
}

func rolEpi642(a [64]byte, imm8 int) [64]byte


// MaskRolvEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the
// left by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPROLVD'. Intrinsic: '_mm_mask_rolv_epi32'.
// Requires AVX512F.
func MaskRolvEpi32(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskRolvEpi32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskRolvEpi32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzRolvEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the
// left by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPROLVD'. Intrinsic: '_mm_maskz_rolv_epi32'.
// Requires AVX512F.
func MaskzRolvEpi32(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzRolvEpi32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzRolvEpi32(k uint8, a [16]byte, b [16]byte) [16]byte


// RolvEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the left
// by the number of bits specified in the corresponding element of 'b', and
// store the results in 'dst'. 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPROLVD'. Intrinsic: '_mm_rolv_epi32'.
// Requires AVX512F.
func RolvEpi32(a M128i, b M128i) M128i {
	return M128i(rolvEpi32([16]byte(a), [16]byte(b)))
}

func rolvEpi32(a [16]byte, b [16]byte) [16]byte


// MaskRolvEpi321: Rotate the bits in each packed 32-bit integer in 'a' to the
// left by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLVD'. Intrinsic: '_mm256_mask_rolv_epi32'.
// Requires AVX512F.
func MaskRolvEpi321(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskRolvEpi321([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskRolvEpi321(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzRolvEpi321: Rotate the bits in each packed 32-bit integer in 'a' to the
// left by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLVD'. Intrinsic: '_mm256_maskz_rolv_epi32'.
// Requires AVX512F.
func MaskzRolvEpi321(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzRolvEpi321(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzRolvEpi321(k uint8, a [32]byte, b [32]byte) [32]byte


// RolvEpi321: Rotate the bits in each packed 32-bit integer in 'a' to the left
// by the number of bits specified in the corresponding element of 'b', and
// store the results in 'dst'. 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLVD'. Intrinsic: '_mm256_rolv_epi32'.
// Requires AVX512F.
func RolvEpi321(a M256i, b M256i) M256i {
	return M256i(rolvEpi321([32]byte(a), [32]byte(b)))
}

func rolvEpi321(a [32]byte, b [32]byte) [32]byte


// MaskRolvEpi322: Rotate the bits in each packed 32-bit integer in 'a' to the
// left by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPROLVD'. Intrinsic: '_mm512_mask_rolv_epi32'.
// Requires AVX512F.
func MaskRolvEpi322(src M512i, k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskRolvEpi322([64]byte(src), uint16(k), [64]byte(a), [64]byte(b)))
}

func maskRolvEpi322(src [64]byte, k uint16, a [64]byte, b [64]byte) [64]byte


// MaskzRolvEpi322: Rotate the bits in each packed 32-bit integer in 'a' to the
// left by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPROLVD'. Intrinsic: '_mm512_maskz_rolv_epi32'.
// Requires AVX512F.
func MaskzRolvEpi322(k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskzRolvEpi322(uint16(k), [64]byte(a), [64]byte(b)))
}

func maskzRolvEpi322(k uint16, a [64]byte, b [64]byte) [64]byte


// RolvEpi322: Rotate the bits in each packed 32-bit integer in 'a' to the left
// by the number of bits specified in the corresponding element of 'b', and
// store the results in 'dst'. 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPROLVD'. Intrinsic: '_mm512_rolv_epi32'.
// Requires AVX512F.
func RolvEpi322(a M512i, b M512i) M512i {
	return M512i(rolvEpi322([64]byte(a), [64]byte(b)))
}

func rolvEpi322(a [64]byte, b [64]byte) [64]byte


// MaskRolvEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the
// left by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPROLVQ'. Intrinsic: '_mm_mask_rolv_epi64'.
// Requires AVX512F.
func MaskRolvEpi64(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskRolvEpi64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskRolvEpi64(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzRolvEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the
// left by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPROLVQ'. Intrinsic: '_mm_maskz_rolv_epi64'.
// Requires AVX512F.
func MaskzRolvEpi64(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzRolvEpi64(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzRolvEpi64(k uint8, a [16]byte, b [16]byte) [16]byte


// RolvEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the left
// by the number of bits specified in the corresponding element of 'b', and
// store the results in 'dst'. 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPROLVQ'. Intrinsic: '_mm_rolv_epi64'.
// Requires AVX512F.
func RolvEpi64(a M128i, b M128i) M128i {
	return M128i(rolvEpi64([16]byte(a), [16]byte(b)))
}

func rolvEpi64(a [16]byte, b [16]byte) [16]byte


// MaskRolvEpi641: Rotate the bits in each packed 64-bit integer in 'a' to the
// left by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLVQ'. Intrinsic: '_mm256_mask_rolv_epi64'.
// Requires AVX512F.
func MaskRolvEpi641(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskRolvEpi641([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskRolvEpi641(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzRolvEpi641: Rotate the bits in each packed 64-bit integer in 'a' to the
// left by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLVQ'. Intrinsic: '_mm256_maskz_rolv_epi64'.
// Requires AVX512F.
func MaskzRolvEpi641(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzRolvEpi641(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzRolvEpi641(k uint8, a [32]byte, b [32]byte) [32]byte


// RolvEpi641: Rotate the bits in each packed 64-bit integer in 'a' to the left
// by the number of bits specified in the corresponding element of 'b', and
// store the results in 'dst'. 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLVQ'. Intrinsic: '_mm256_rolv_epi64'.
// Requires AVX512F.
func RolvEpi641(a M256i, b M256i) M256i {
	return M256i(rolvEpi641([32]byte(a), [32]byte(b)))
}

func rolvEpi641(a [32]byte, b [32]byte) [32]byte


// MaskRolvEpi642: Rotate the bits in each packed 64-bit integer in 'a' to the
// left by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPROLVQ'. Intrinsic: '_mm512_mask_rolv_epi64'.
// Requires AVX512F.
func MaskRolvEpi642(src M512i, k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskRolvEpi642([64]byte(src), uint8(k), [64]byte(a), [64]byte(b)))
}

func maskRolvEpi642(src [64]byte, k uint8, a [64]byte, b [64]byte) [64]byte


// MaskzRolvEpi642: Rotate the bits in each packed 64-bit integer in 'a' to the
// left by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPROLVQ'. Intrinsic: '_mm512_maskz_rolv_epi64'.
// Requires AVX512F.
func MaskzRolvEpi642(k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskzRolvEpi642(uint8(k), [64]byte(a), [64]byte(b)))
}

func maskzRolvEpi642(k uint8, a [64]byte, b [64]byte) [64]byte


// RolvEpi642: Rotate the bits in each packed 64-bit integer in 'a' to the left
// by the number of bits specified in the corresponding element of 'b', and
// store the results in 'dst'. 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPROLVQ'. Intrinsic: '_mm512_rolv_epi64'.
// Requires AVX512F.
func RolvEpi642(a M512i, b M512i) M512i {
	return M512i(rolvEpi642([64]byte(a), [64]byte(b)))
}

func rolvEpi642(a [64]byte, b [64]byte) [64]byte


// MaskRorEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the
// right by the number of bits specified in 'imm8', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPRORD'. Intrinsic: '_mm_mask_ror_epi32'.
// Requires AVX512F.
func MaskRorEpi32(src M128i, k Mmask8, a M128i, imm8 int) M128i {
	return M128i(maskRorEpi32([16]byte(src), uint8(k), [16]byte(a), imm8))
}

func maskRorEpi32(src [16]byte, k uint8, a [16]byte, imm8 int) [16]byte


// MaskzRorEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the
// right by the number of bits specified in 'imm8', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPRORD'. Intrinsic: '_mm_maskz_ror_epi32'.
// Requires AVX512F.
func MaskzRorEpi32(k Mmask8, a M128i, imm8 int) M128i {
	return M128i(maskzRorEpi32(uint8(k), [16]byte(a), imm8))
}

func maskzRorEpi32(k uint8, a [16]byte, imm8 int) [16]byte


// RorEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the right
// by the number of bits specified in 'imm8', and store the results in 'dst'. 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPRORD'. Intrinsic: '_mm_ror_epi32'.
// Requires AVX512F.
func RorEpi32(a M128i, imm8 int) M128i {
	return M128i(rorEpi32([16]byte(a), imm8))
}

func rorEpi32(a [16]byte, imm8 int) [16]byte


// MaskRorEpi321: Rotate the bits in each packed 32-bit integer in 'a' to the
// right by the number of bits specified in 'imm8', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORD'. Intrinsic: '_mm256_mask_ror_epi32'.
// Requires AVX512F.
func MaskRorEpi321(src M256i, k Mmask8, a M256i, imm8 int) M256i {
	return M256i(maskRorEpi321([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func maskRorEpi321(src [32]byte, k uint8, a [32]byte, imm8 int) [32]byte


// MaskzRorEpi321: Rotate the bits in each packed 32-bit integer in 'a' to the
// right by the number of bits specified in 'imm8', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORD'. Intrinsic: '_mm256_maskz_ror_epi32'.
// Requires AVX512F.
func MaskzRorEpi321(k Mmask8, a M256i, imm8 int) M256i {
	return M256i(maskzRorEpi321(uint8(k), [32]byte(a), imm8))
}

func maskzRorEpi321(k uint8, a [32]byte, imm8 int) [32]byte


// RorEpi321: Rotate the bits in each packed 32-bit integer in 'a' to the right
// by the number of bits specified in 'imm8', and store the results in 'dst'. 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORD'. Intrinsic: '_mm256_ror_epi32'.
// Requires AVX512F.
func RorEpi321(a M256i, imm8 int) M256i {
	return M256i(rorEpi321([32]byte(a), imm8))
}

func rorEpi321(a [32]byte, imm8 int) [32]byte


// MaskRorEpi322: Rotate the bits in each packed 32-bit integer in 'a' to the
// right by the number of bits specified in 'imm8', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPRORD'. Intrinsic: '_mm512_mask_ror_epi32'.
// Requires AVX512F.
func MaskRorEpi322(src M512i, k Mmask16, a M512i, imm8 int) M512i {
	return M512i(maskRorEpi322([64]byte(src), uint16(k), [64]byte(a), imm8))
}

func maskRorEpi322(src [64]byte, k uint16, a [64]byte, imm8 int) [64]byte


// MaskzRorEpi322: Rotate the bits in each packed 32-bit integer in 'a' to the
// right by the number of bits specified in 'imm8', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPRORD'. Intrinsic: '_mm512_maskz_ror_epi32'.
// Requires AVX512F.
func MaskzRorEpi322(k Mmask16, a M512i, imm8 int) M512i {
	return M512i(maskzRorEpi322(uint16(k), [64]byte(a), imm8))
}

func maskzRorEpi322(k uint16, a [64]byte, imm8 int) [64]byte


// RorEpi322: Rotate the bits in each packed 32-bit integer in 'a' to the right
// by the number of bits specified in 'imm8', and store the results in 'dst'. 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPRORD'. Intrinsic: '_mm512_ror_epi32'.
// Requires AVX512F.
func RorEpi322(a M512i, imm8 int) M512i {
	return M512i(rorEpi322([64]byte(a), imm8))
}

func rorEpi322(a [64]byte, imm8 int) [64]byte


// MaskRorEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the
// right by the number of bits specified in 'imm8', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPRORQ'. Intrinsic: '_mm_mask_ror_epi64'.
// Requires AVX512F.
func MaskRorEpi64(src M128i, k Mmask8, a M128i, imm8 int) M128i {
	return M128i(maskRorEpi64([16]byte(src), uint8(k), [16]byte(a), imm8))
}

func maskRorEpi64(src [16]byte, k uint8, a [16]byte, imm8 int) [16]byte


// MaskzRorEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the
// right by the number of bits specified in 'imm8', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPRORQ'. Intrinsic: '_mm_maskz_ror_epi64'.
// Requires AVX512F.
func MaskzRorEpi64(k Mmask8, a M128i, imm8 int) M128i {
	return M128i(maskzRorEpi64(uint8(k), [16]byte(a), imm8))
}

func maskzRorEpi64(k uint8, a [16]byte, imm8 int) [16]byte


// RorEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the right
// by the number of bits specified in 'imm8', and store the results in 'dst'. 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPRORQ'. Intrinsic: '_mm_ror_epi64'.
// Requires AVX512F.
func RorEpi64(a M128i, imm8 int) M128i {
	return M128i(rorEpi64([16]byte(a), imm8))
}

func rorEpi64(a [16]byte, imm8 int) [16]byte


// MaskRorEpi641: Rotate the bits in each packed 64-bit integer in 'a' to the
// right by the number of bits specified in 'imm8', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORQ'. Intrinsic: '_mm256_mask_ror_epi64'.
// Requires AVX512F.
func MaskRorEpi641(src M256i, k Mmask8, a M256i, imm8 int) M256i {
	return M256i(maskRorEpi641([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func maskRorEpi641(src [32]byte, k uint8, a [32]byte, imm8 int) [32]byte


// MaskzRorEpi641: Rotate the bits in each packed 64-bit integer in 'a' to the
// right by the number of bits specified in 'imm8', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORQ'. Intrinsic: '_mm256_maskz_ror_epi64'.
// Requires AVX512F.
func MaskzRorEpi641(k Mmask8, a M256i, imm8 int) M256i {
	return M256i(maskzRorEpi641(uint8(k), [32]byte(a), imm8))
}

func maskzRorEpi641(k uint8, a [32]byte, imm8 int) [32]byte


// RorEpi641: Rotate the bits in each packed 64-bit integer in 'a' to the right
// by the number of bits specified in 'imm8', and store the results in 'dst'. 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORQ'. Intrinsic: '_mm256_ror_epi64'.
// Requires AVX512F.
func RorEpi641(a M256i, imm8 int) M256i {
	return M256i(rorEpi641([32]byte(a), imm8))
}

func rorEpi641(a [32]byte, imm8 int) [32]byte


// MaskRorEpi642: Rotate the bits in each packed 64-bit integer in 'a' to the
// right by the number of bits specified in 'imm8', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPRORQ'. Intrinsic: '_mm512_mask_ror_epi64'.
// Requires AVX512F.
func MaskRorEpi642(src M512i, k Mmask8, a M512i, imm8 int) M512i {
	return M512i(maskRorEpi642([64]byte(src), uint8(k), [64]byte(a), imm8))
}

func maskRorEpi642(src [64]byte, k uint8, a [64]byte, imm8 int) [64]byte


// MaskzRorEpi642: Rotate the bits in each packed 64-bit integer in 'a' to the
// right by the number of bits specified in 'imm8', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPRORQ'. Intrinsic: '_mm512_maskz_ror_epi64'.
// Requires AVX512F.
func MaskzRorEpi642(k Mmask8, a M512i, imm8 int) M512i {
	return M512i(maskzRorEpi642(uint8(k), [64]byte(a), imm8))
}

func maskzRorEpi642(k uint8, a [64]byte, imm8 int) [64]byte


// RorEpi642: Rotate the bits in each packed 64-bit integer in 'a' to the right
// by the number of bits specified in 'imm8', and store the results in 'dst'. 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPRORQ'. Intrinsic: '_mm512_ror_epi64'.
// Requires AVX512F.
func RorEpi642(a M512i, imm8 int) M512i {
	return M512i(rorEpi642([64]byte(a), imm8))
}

func rorEpi642(a [64]byte, imm8 int) [64]byte


// MaskRorvEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the
// right by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPRORVD'. Intrinsic: '_mm_mask_rorv_epi32'.
// Requires AVX512F.
func MaskRorvEpi32(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskRorvEpi32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskRorvEpi32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzRorvEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the
// right by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPRORVD'. Intrinsic: '_mm_maskz_rorv_epi32'.
// Requires AVX512F.
func MaskzRorvEpi32(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzRorvEpi32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzRorvEpi32(k uint8, a [16]byte, b [16]byte) [16]byte


// RorvEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the right
// by the number of bits specified in the corresponding element of 'b', and
// store the results in 'dst'. 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPRORVD'. Intrinsic: '_mm_rorv_epi32'.
// Requires AVX512F.
func RorvEpi32(a M128i, b M128i) M128i {
	return M128i(rorvEpi32([16]byte(a), [16]byte(b)))
}

func rorvEpi32(a [16]byte, b [16]byte) [16]byte


// MaskRorvEpi321: Rotate the bits in each packed 32-bit integer in 'a' to the
// right by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORVD'. Intrinsic: '_mm256_mask_rorv_epi32'.
// Requires AVX512F.
func MaskRorvEpi321(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskRorvEpi321([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskRorvEpi321(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzRorvEpi321: Rotate the bits in each packed 32-bit integer in 'a' to the
// right by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORVD'. Intrinsic: '_mm256_maskz_rorv_epi32'.
// Requires AVX512F.
func MaskzRorvEpi321(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzRorvEpi321(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzRorvEpi321(k uint8, a [32]byte, b [32]byte) [32]byte


// RorvEpi321: Rotate the bits in each packed 32-bit integer in 'a' to the
// right by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst'. 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORVD'. Intrinsic: '_mm256_rorv_epi32'.
// Requires AVX512F.
func RorvEpi321(a M256i, b M256i) M256i {
	return M256i(rorvEpi321([32]byte(a), [32]byte(b)))
}

func rorvEpi321(a [32]byte, b [32]byte) [32]byte


// MaskRorvEpi322: Rotate the bits in each packed 32-bit integer in 'a' to the
// right by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPRORVD'. Intrinsic: '_mm512_mask_rorv_epi32'.
// Requires AVX512F.
func MaskRorvEpi322(src M512i, k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskRorvEpi322([64]byte(src), uint16(k), [64]byte(a), [64]byte(b)))
}

func maskRorvEpi322(src [64]byte, k uint16, a [64]byte, b [64]byte) [64]byte


// MaskzRorvEpi322: Rotate the bits in each packed 32-bit integer in 'a' to the
// right by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPRORVD'. Intrinsic: '_mm512_maskz_rorv_epi32'.
// Requires AVX512F.
func MaskzRorvEpi322(k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskzRorvEpi322(uint16(k), [64]byte(a), [64]byte(b)))
}

func maskzRorvEpi322(k uint16, a [64]byte, b [64]byte) [64]byte


// RorvEpi322: Rotate the bits in each packed 32-bit integer in 'a' to the
// right by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst'. 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPRORVD'. Intrinsic: '_mm512_rorv_epi32'.
// Requires AVX512F.
func RorvEpi322(a M512i, b M512i) M512i {
	return M512i(rorvEpi322([64]byte(a), [64]byte(b)))
}

func rorvEpi322(a [64]byte, b [64]byte) [64]byte


// MaskRorvEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the
// right by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPRORVQ'. Intrinsic: '_mm_mask_rorv_epi64'.
// Requires AVX512F.
func MaskRorvEpi64(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskRorvEpi64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskRorvEpi64(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzRorvEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the
// right by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPRORVQ'. Intrinsic: '_mm_maskz_rorv_epi64'.
// Requires AVX512F.
func MaskzRorvEpi64(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzRorvEpi64(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzRorvEpi64(k uint8, a [16]byte, b [16]byte) [16]byte


// RorvEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the right
// by the number of bits specified in the corresponding element of 'b', and
// store the results in 'dst'. 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPRORVQ'. Intrinsic: '_mm_rorv_epi64'.
// Requires AVX512F.
func RorvEpi64(a M128i, b M128i) M128i {
	return M128i(rorvEpi64([16]byte(a), [16]byte(b)))
}

func rorvEpi64(a [16]byte, b [16]byte) [16]byte


// MaskRorvEpi641: Rotate the bits in each packed 64-bit integer in 'a' to the
// right by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORVQ'. Intrinsic: '_mm256_mask_rorv_epi64'.
// Requires AVX512F.
func MaskRorvEpi641(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskRorvEpi641([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskRorvEpi641(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzRorvEpi641: Rotate the bits in each packed 64-bit integer in 'a' to the
// right by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORVQ'. Intrinsic: '_mm256_maskz_rorv_epi64'.
// Requires AVX512F.
func MaskzRorvEpi641(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzRorvEpi641(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzRorvEpi641(k uint8, a [32]byte, b [32]byte) [32]byte


// RorvEpi641: Rotate the bits in each packed 64-bit integer in 'a' to the
// right by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst'. 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORVQ'. Intrinsic: '_mm256_rorv_epi64'.
// Requires AVX512F.
func RorvEpi641(a M256i, b M256i) M256i {
	return M256i(rorvEpi641([32]byte(a), [32]byte(b)))
}

func rorvEpi641(a [32]byte, b [32]byte) [32]byte


// MaskRorvEpi642: Rotate the bits in each packed 64-bit integer in 'a' to the
// right by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPRORVQ'. Intrinsic: '_mm512_mask_rorv_epi64'.
// Requires AVX512F.
func MaskRorvEpi642(src M512i, k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskRorvEpi642([64]byte(src), uint8(k), [64]byte(a), [64]byte(b)))
}

func maskRorvEpi642(src [64]byte, k uint8, a [64]byte, b [64]byte) [64]byte


// MaskzRorvEpi642: Rotate the bits in each packed 64-bit integer in 'a' to the
// right by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPRORVQ'. Intrinsic: '_mm512_maskz_rorv_epi64'.
// Requires AVX512F.
func MaskzRorvEpi642(k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskzRorvEpi642(uint8(k), [64]byte(a), [64]byte(b)))
}

func maskzRorvEpi642(k uint8, a [64]byte, b [64]byte) [64]byte


// RorvEpi642: Rotate the bits in each packed 64-bit integer in 'a' to the
// right by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst'. 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPRORVQ'. Intrinsic: '_mm512_rorv_epi64'.
// Requires AVX512F.
func RorvEpi642(a M512i, b M512i) M512i {
	return M512i(rorvEpi642([64]byte(a), [64]byte(b)))
}

func rorvEpi642(a [64]byte, b [64]byte) [64]byte


// MaskRoundscalePd: Round packed double-precision (64-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}	
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALEPD'. Intrinsic: '_mm_mask_roundscale_pd'.
// Requires AVX512F.
func MaskRoundscalePd(src M128d, k Mmask8, a M128d, imm8 int) M128d {
	return M128d(maskRoundscalePd([2]float64(src), uint8(k), [2]float64(a), imm8))
}

func maskRoundscalePd(src [2]float64, k uint8, a [2]float64, imm8 int) [2]float64


// MaskzRoundscalePd: Round packed double-precision (64-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}	
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALEPD'. Intrinsic: '_mm_maskz_roundscale_pd'.
// Requires AVX512F.
func MaskzRoundscalePd(k Mmask8, a M128d, imm8 int) M128d {
	return M128d(maskzRoundscalePd(uint8(k), [2]float64(a), imm8))
}

func maskzRoundscalePd(k uint8, a [2]float64, imm8 int) [2]float64


// RoundscalePd: Round packed double-precision (64-bit) floating-point elements
// in 'a' to the number of fraction bits specified by 'imm8', and store the
// results in 'dst'. 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}	
//		
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALEPD'. Intrinsic: '_mm_roundscale_pd'.
// Requires AVX512F.
func RoundscalePd(a M128d, imm8 int) M128d {
	return M128d(roundscalePd([2]float64(a), imm8))
}

func roundscalePd(a [2]float64, imm8 int) [2]float64


// MaskRoundscalePd1: Round packed double-precision (64-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}	
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRNDSCALEPD'. Intrinsic: '_mm256_mask_roundscale_pd'.
// Requires AVX512F.
func MaskRoundscalePd1(src M256d, k Mmask8, a M256d, imm8 int) M256d {
	return M256d(maskRoundscalePd1([4]float64(src), uint8(k), [4]float64(a), imm8))
}

func maskRoundscalePd1(src [4]float64, k uint8, a [4]float64, imm8 int) [4]float64


// MaskzRoundscalePd1: Round packed double-precision (64-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}	
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRNDSCALEPD'. Intrinsic: '_mm256_maskz_roundscale_pd'.
// Requires AVX512F.
func MaskzRoundscalePd1(k Mmask8, a M256d, imm8 int) M256d {
	return M256d(maskzRoundscalePd1(uint8(k), [4]float64(a), imm8))
}

func maskzRoundscalePd1(k uint8, a [4]float64, imm8 int) [4]float64


// RoundscalePd1: Round packed double-precision (64-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst'. 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}	
//		
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRNDSCALEPD'. Intrinsic: '_mm256_roundscale_pd'.
// Requires AVX512F.
func RoundscalePd1(a M256d, imm8 int) M256d {
	return M256d(roundscalePd1([4]float64(a), imm8))
}

func roundscalePd1(a [4]float64, imm8 int) [4]float64


// MaskRoundscalePd2: Round packed double-precision (64-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}	
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRNDSCALEPD'. Intrinsic: '_mm512_mask_roundscale_pd'.
// Requires AVX512F.
func MaskRoundscalePd2(src M512d, k Mmask8, a M512d, imm8 int) M512d {
	return M512d(maskRoundscalePd2([8]float64(src), uint8(k), [8]float64(a), imm8))
}

func maskRoundscalePd2(src [8]float64, k uint8, a [8]float64, imm8 int) [8]float64


// MaskzRoundscalePd2: Round packed double-precision (64-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}	
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRNDSCALEPD'. Intrinsic: '_mm512_maskz_roundscale_pd'.
// Requires AVX512F.
func MaskzRoundscalePd2(k Mmask8, a M512d, imm8 int) M512d {
	return M512d(maskzRoundscalePd2(uint8(k), [8]float64(a), imm8))
}

func maskzRoundscalePd2(k uint8, a [8]float64, imm8 int) [8]float64


// RoundscalePd2: Round packed double-precision (64-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst'. 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}	
//		
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRNDSCALEPD'. Intrinsic: '_mm512_roundscale_pd'.
// Requires AVX512F.
func RoundscalePd2(a M512d, imm8 int) M512d {
	return M512d(roundscalePd2([8]float64(a), imm8))
}

func roundscalePd2(a [8]float64, imm8 int) [8]float64


// MaskRoundscalePs: Round packed single-precision (32-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}	
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALEPS'. Intrinsic: '_mm_mask_roundscale_ps'.
// Requires AVX512F.
func MaskRoundscalePs(src M128, k Mmask8, a M128, imm8 int) M128 {
	return M128(maskRoundscalePs([4]float32(src), uint8(k), [4]float32(a), imm8))
}

func maskRoundscalePs(src [4]float32, k uint8, a [4]float32, imm8 int) [4]float32


// MaskzRoundscalePs: Round packed single-precision (32-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}	
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALEPS'. Intrinsic: '_mm_maskz_roundscale_ps'.
// Requires AVX512F.
func MaskzRoundscalePs(k Mmask8, a M128, imm8 int) M128 {
	return M128(maskzRoundscalePs(uint8(k), [4]float32(a), imm8))
}

func maskzRoundscalePs(k uint8, a [4]float32, imm8 int) [4]float32


// RoundscalePs: Round packed single-precision (32-bit) floating-point elements
// in 'a' to the number of fraction bits specified by 'imm8', and store the
// results in 'dst'. 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}	
//		
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALEPS'. Intrinsic: '_mm_roundscale_ps'.
// Requires AVX512F.
func RoundscalePs(a M128, imm8 int) M128 {
	return M128(roundscalePs([4]float32(a), imm8))
}

func roundscalePs(a [4]float32, imm8 int) [4]float32


// MaskRoundscalePs1: Round packed single-precision (32-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}	
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRNDSCALEPS'. Intrinsic: '_mm256_mask_roundscale_ps'.
// Requires AVX512F.
func MaskRoundscalePs1(src M256, k Mmask8, a M256, imm8 int) M256 {
	return M256(maskRoundscalePs1([8]float32(src), uint8(k), [8]float32(a), imm8))
}

func maskRoundscalePs1(src [8]float32, k uint8, a [8]float32, imm8 int) [8]float32


// MaskzRoundscalePs1: Round packed single-precision (32-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}	
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRNDSCALEPS'. Intrinsic: '_mm256_maskz_roundscale_ps'.
// Requires AVX512F.
func MaskzRoundscalePs1(k Mmask8, a M256, imm8 int) M256 {
	return M256(maskzRoundscalePs1(uint8(k), [8]float32(a), imm8))
}

func maskzRoundscalePs1(k uint8, a [8]float32, imm8 int) [8]float32


// RoundscalePs1: Round packed single-precision (32-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst'. 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}	
//		
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRNDSCALEPS'. Intrinsic: '_mm256_roundscale_ps'.
// Requires AVX512F.
func RoundscalePs1(a M256, imm8 int) M256 {
	return M256(roundscalePs1([8]float32(a), imm8))
}

func roundscalePs1(a [8]float32, imm8 int) [8]float32


// MaskRoundscalePs2: Round packed single-precision (32-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}	
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRNDSCALEPS'. Intrinsic: '_mm512_mask_roundscale_ps'.
// Requires AVX512F.
func MaskRoundscalePs2(src M512, k Mmask16, a M512, imm8 int) M512 {
	return M512(maskRoundscalePs2([16]float32(src), uint16(k), [16]float32(a), imm8))
}

func maskRoundscalePs2(src [16]float32, k uint16, a [16]float32, imm8 int) [16]float32


// MaskzRoundscalePs2: Round packed single-precision (32-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}	
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRNDSCALEPS'. Intrinsic: '_mm512_maskz_roundscale_ps'.
// Requires AVX512F.
func MaskzRoundscalePs2(k Mmask16, a M512, imm8 int) M512 {
	return M512(maskzRoundscalePs2(uint16(k), [16]float32(a), imm8))
}

func maskzRoundscalePs2(k uint16, a [16]float32, imm8 int) [16]float32


// RoundscalePs2: Round packed single-precision (32-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst'. 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}	
//		
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRNDSCALEPS'. Intrinsic: '_mm512_roundscale_ps'.
// Requires AVX512F.
func RoundscalePs2(a M512, imm8 int) M512 {
	return M512(roundscalePs2([16]float32(a), imm8))
}

func roundscalePs2(a [16]float32, imm8 int) [16]float32


// MaskRoundscaleRoundPd: Round packed double-precision (64-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}	
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRNDSCALEPD'. Intrinsic: '_mm512_mask_roundscale_round_pd'.
// Requires AVX512F.
func MaskRoundscaleRoundPd(src M512d, k Mmask8, a M512d, imm8 int, rounding int) M512d {
	return M512d(maskRoundscaleRoundPd([8]float64(src), uint8(k), [8]float64(a), imm8, rounding))
}

func maskRoundscaleRoundPd(src [8]float64, k uint8, a [8]float64, imm8 int, rounding int) [8]float64


// MaskzRoundscaleRoundPd: Round packed double-precision (64-bit)
// floating-point elements in 'a' to the number of fraction bits specified by
// 'imm8', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}	
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRNDSCALEPD'. Intrinsic: '_mm512_maskz_roundscale_round_pd'.
// Requires AVX512F.
func MaskzRoundscaleRoundPd(k Mmask8, a M512d, imm8 int, rounding int) M512d {
	return M512d(maskzRoundscaleRoundPd(uint8(k), [8]float64(a), imm8, rounding))
}

func maskzRoundscaleRoundPd(k uint8, a [8]float64, imm8 int, rounding int) [8]float64


// RoundscaleRoundPd: Round packed double-precision (64-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}	
//		
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRNDSCALEPD'. Intrinsic: '_mm512_roundscale_round_pd'.
// Requires AVX512F.
func RoundscaleRoundPd(a M512d, imm8 int, rounding int) M512d {
	return M512d(roundscaleRoundPd([8]float64(a), imm8, rounding))
}

func roundscaleRoundPd(a [8]float64, imm8 int, rounding int) [8]float64


// MaskRoundscaleRoundPs: Round packed single-precision (32-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}	
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRNDSCALEPS'. Intrinsic: '_mm512_mask_roundscale_round_ps'.
// Requires AVX512F.
func MaskRoundscaleRoundPs(src M512, k Mmask16, a M512, imm8 int, rounding int) M512 {
	return M512(maskRoundscaleRoundPs([16]float32(src), uint16(k), [16]float32(a), imm8, rounding))
}

func maskRoundscaleRoundPs(src [16]float32, k uint16, a [16]float32, imm8 int, rounding int) [16]float32


// MaskzRoundscaleRoundPs: Round packed single-precision (32-bit)
// floating-point elements in 'a' to the number of fraction bits specified by
// 'imm8', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}	
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRNDSCALEPS'. Intrinsic: '_mm512_maskz_roundscale_round_ps'.
// Requires AVX512F.
func MaskzRoundscaleRoundPs(k Mmask16, a M512, imm8 int, rounding int) M512 {
	return M512(maskzRoundscaleRoundPs(uint16(k), [16]float32(a), imm8, rounding))
}

func maskzRoundscaleRoundPs(k uint16, a [16]float32, imm8 int, rounding int) [16]float32


// RoundscaleRoundPs: Round packed single-precision (32-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}	
//		
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRNDSCALEPS'. Intrinsic: '_mm512_roundscale_round_ps'.
// Requires AVX512F.
func RoundscaleRoundPs(a M512, imm8 int, rounding int) M512 {
	return M512(roundscaleRoundPs([16]float32(a), imm8, rounding))
}

func roundscaleRoundPs(a [16]float32, imm8 int, rounding int) [16]float32


// MaskRoundscaleRoundSd: Round the lower double-precision (64-bit)
// floating-point element in 'a' to the number of fraction bits specified by
// 'imm8', store the result in the lower element of 'dst' using writemask 'k'
// (the element is copied from 'src' when mask bit 0 is not set), and copy the
// upper element from 'b' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}		
//		
//		IF k[0]
//			dst[63:0] := RoundTo_IntegerPD(a[63:0], imm8[7:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALESD'. Intrinsic: '_mm_mask_roundscale_round_sd'.
// Requires AVX512F.
func MaskRoundscaleRoundSd(src M128d, k Mmask8, a M128d, b M128d, imm8 int, rounding int) M128d {
	return M128d(maskRoundscaleRoundSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), imm8, rounding))
}

func maskRoundscaleRoundSd(src [2]float64, k uint8, a [2]float64, b [2]float64, imm8 int, rounding int) [2]float64


// MaskzRoundscaleRoundSd: Round the lower double-precision (64-bit)
// floating-point element in 'a' to the number of fraction bits specified by
// 'imm8', store the result in the lower element of 'dst' using zeromask 'k'
// (the element is zeroed out when mask bit 0 is not set), and copy the upper
// element from 'b' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}		
//		
//		IF k[0]
//			dst[63:0] := RoundTo_IntegerPD(a[63:0], imm8[7:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALESD'. Intrinsic: '_mm_maskz_roundscale_round_sd'.
// Requires AVX512F.
func MaskzRoundscaleRoundSd(k Mmask8, a M128d, b M128d, imm8 int, rounding int) M128d {
	return M128d(maskzRoundscaleRoundSd(uint8(k), [2]float64(a), [2]float64(b), imm8, rounding))
}

func maskzRoundscaleRoundSd(k uint8, a [2]float64, b [2]float64, imm8 int, rounding int) [2]float64


// RoundscaleRoundSd: Round the lower double-precision (64-bit) floating-point
// element in 'a' to the number of fraction bits specified by 'imm8', store the
// result in the lower element of 'dst', and copy the upper element from 'b' to
// the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}		
//		
//		dst[63:0] := RoundTo_IntegerPD(a[63:0], imm8[7:0])
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALESD'. Intrinsic: '_mm_roundscale_round_sd'.
// Requires AVX512F.
func RoundscaleRoundSd(a M128d, b M128d, imm8 int, rounding int) M128d {
	return M128d(roundscaleRoundSd([2]float64(a), [2]float64(b), imm8, rounding))
}

func roundscaleRoundSd(a [2]float64, b [2]float64, imm8 int, rounding int) [2]float64


// MaskRoundscaleRoundSs: Round the lower single-precision (32-bit)
// floating-point element in 'a' to the number of fraction bits specified by
// 'imm8', store the result in the lower element of 'dst' using writemask 'k'
// (the element is copied from 'src' when mask bit 0 is not set), and copy the
// upper 3 packed elements from 'b' to the upper elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}
//		
//		IF k[0]
//			dst[31:0] := RoundTo_IntegerPS(a[31:0], imm8[7:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALESS'. Intrinsic: '_mm_mask_roundscale_round_ss'.
// Requires AVX512F.
func MaskRoundscaleRoundSs(src M128, k Mmask8, a M128, b M128, imm8 int, rounding int) M128 {
	return M128(maskRoundscaleRoundSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), imm8, rounding))
}

func maskRoundscaleRoundSs(src [4]float32, k uint8, a [4]float32, b [4]float32, imm8 int, rounding int) [4]float32


// MaskzRoundscaleRoundSs: Round the lower single-precision (32-bit)
// floating-point element in 'a' to the number of fraction bits specified by
// 'imm8', store the result in the lower element of 'dst' using zeromask 'k'
// (the element is zeroed out when mask bit 0 is not set), and copy the upper 3
// packed elements from 'b' to the upper elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}
//		
//		IF k[0]
//			dst[31:0] := RoundTo_IntegerPS(a[31:0], imm8[7:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALESS'. Intrinsic: '_mm_maskz_roundscale_round_ss'.
// Requires AVX512F.
func MaskzRoundscaleRoundSs(k Mmask8, a M128, b M128, imm8 int, rounding int) M128 {
	return M128(maskzRoundscaleRoundSs(uint8(k), [4]float32(a), [4]float32(b), imm8, rounding))
}

func maskzRoundscaleRoundSs(k uint8, a [4]float32, b [4]float32, imm8 int, rounding int) [4]float32


// RoundscaleRoundSs: Round the lower single-precision (32-bit) floating-point
// element in 'a' to the number of fraction bits specified by 'imm8', store the
// result in the lower element of 'dst', and copy the upper 3 packed elements
// from 'b' to the upper elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}
//		
//		dst[31:0] := RoundTo_IntegerPS(a[31:0], imm8[7:0])
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALESS'. Intrinsic: '_mm_roundscale_round_ss'.
// Requires AVX512F.
func RoundscaleRoundSs(a M128, b M128, imm8 int, rounding int) M128 {
	return M128(roundscaleRoundSs([4]float32(a), [4]float32(b), imm8, rounding))
}

func roundscaleRoundSs(a [4]float32, b [4]float32, imm8 int, rounding int) [4]float32


// MaskRoundscaleSd: Round the lower double-precision (64-bit) floating-point
// element in 'a' to the number of fraction bits specified by 'imm8', store the
// result in the lower element of 'dst' using writemask 'k' (the element is
// copied from 'src' when mask bit 0 is not set), and copy the upper element
// from 'b' to the upper element of 'dst'. 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}		
//		
//		IF k[0]
//			dst[63:0] := RoundTo_IntegerPD(a[63:0], imm8[7:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALESD'. Intrinsic: '_mm_mask_roundscale_sd'.
// Requires AVX512F.
func MaskRoundscaleSd(src M128d, k Mmask8, a M128d, b M128d, imm8 int) M128d {
	return M128d(maskRoundscaleSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), imm8))
}

func maskRoundscaleSd(src [2]float64, k uint8, a [2]float64, b [2]float64, imm8 int) [2]float64


// MaskzRoundscaleSd: Round the lower double-precision (64-bit) floating-point
// element in 'a' to the number of fraction bits specified by 'imm8', store the
// result in the lower element of 'dst' using zeromask 'k' (the element is
// zeroed out when mask bit 0 is not set), and copy the upper element from 'b'
// to the upper element of 'dst'. 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}		
//		
//		IF k[0]
//			dst[63:0] := RoundTo_IntegerPD(a[63:0], imm8[7:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALESD'. Intrinsic: '_mm_maskz_roundscale_sd'.
// Requires AVX512F.
func MaskzRoundscaleSd(k Mmask8, a M128d, b M128d, imm8 int) M128d {
	return M128d(maskzRoundscaleSd(uint8(k), [2]float64(a), [2]float64(b), imm8))
}

func maskzRoundscaleSd(k uint8, a [2]float64, b [2]float64, imm8 int) [2]float64


// RoundscaleSd: Round the lower double-precision (64-bit) floating-point
// element in 'a' to the number of fraction bits specified by 'imm8', store the
// result in the lower element of 'dst', and copy the upper element from 'b' to
// the upper element of 'dst'. 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}		
//		
//		dst[63:0] := RoundTo_IntegerPD(a[63:0], imm8[7:0])
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALESD'. Intrinsic: '_mm_roundscale_sd'.
// Requires AVX512F.
func RoundscaleSd(a M128d, b M128d, imm8 int) M128d {
	return M128d(roundscaleSd([2]float64(a), [2]float64(b), imm8))
}

func roundscaleSd(a [2]float64, b [2]float64, imm8 int) [2]float64


// MaskRoundscaleSs: Round the lower single-precision (32-bit) floating-point
// element in 'a' to the number of fraction bits specified by 'imm8', store the
// result in the lower element of 'dst' using writemask 'k' (the element is
// copied from 'src' when mask bit 0 is not set), and copy the upper 3 packed
// elements from 'b' to the upper elements of 'dst'. 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}
//		
//		IF k[0]
//			dst[31:0] := RoundTo_IntegerPS(a[31:0], imm8[7:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALESS'. Intrinsic: '_mm_mask_roundscale_ss'.
// Requires AVX512F.
func MaskRoundscaleSs(src M128, k Mmask8, a M128, b M128, imm8 int) M128 {
	return M128(maskRoundscaleSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), imm8))
}

func maskRoundscaleSs(src [4]float32, k uint8, a [4]float32, b [4]float32, imm8 int) [4]float32


// MaskzRoundscaleSs: Round the lower single-precision (32-bit) floating-point
// element in 'a' to the number of fraction bits specified by 'imm8', store the
// result in the lower element of 'dst' using zeromask 'k' (the element is
// zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements
// from 'b' to the upper elements of 'dst'. 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}
//		
//		IF k[0]
//			dst[31:0] := RoundTo_IntegerPS(a[31:0], imm8[7:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALESS'. Intrinsic: '_mm_maskz_roundscale_ss'.
// Requires AVX512F.
func MaskzRoundscaleSs(k Mmask8, a M128, b M128, imm8 int) M128 {
	return M128(maskzRoundscaleSs(uint8(k), [4]float32(a), [4]float32(b), imm8))
}

func maskzRoundscaleSs(k uint8, a [4]float32, b [4]float32, imm8 int) [4]float32


// RoundscaleSs: Round the lower single-precision (32-bit) floating-point
// element in 'a' to the number of fraction bits specified by 'imm8', store the
// result in the lower element of 'dst', and copy the upper 3 packed elements
// from 'b' to the upper elements of 'dst'. 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}
//		
//		dst[31:0] := RoundTo_IntegerPS(a[31:0], imm8[7:0])
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALESS'. Intrinsic: '_mm_roundscale_ss'.
// Requires AVX512F.
func RoundscaleSs(a M128, b M128, imm8 int) M128 {
	return M128(roundscaleSs([4]float32(a), [4]float32(b), imm8))
}

func roundscaleSs(a [4]float32, b [4]float32, imm8 int) [4]float32


// MaskRsqrt14Pd: Compute the approximate reciprocal square root of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := APPROXIMATE(1.0 / SQRT(a[i+63:i]))
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRSQRT14PD'. Intrinsic: '_mm_mask_rsqrt14_pd'.
// Requires AVX512F.
func MaskRsqrt14Pd(src M128d, k Mmask8, a M128d) M128d {
	return M128d(maskRsqrt14Pd([2]float64(src), uint8(k), [2]float64(a)))
}

func maskRsqrt14Pd(src [2]float64, k uint8, a [2]float64) [2]float64


// MaskzRsqrt14Pd: Compute the approximate reciprocal square root of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := APPROXIMATE(1.0 / SQRT(a[i+63:i]))
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRSQRT14PD'. Intrinsic: '_mm_maskz_rsqrt14_pd'.
// Requires AVX512F.
func MaskzRsqrt14Pd(k Mmask8, a M128d) M128d {
	return M128d(maskzRsqrt14Pd(uint8(k), [2]float64(a)))
}

func maskzRsqrt14Pd(k uint8, a [2]float64) [2]float64


// MaskRsqrt14Pd1: Compute the approximate reciprocal square root of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := APPROXIMATE(1.0 / SQRT(a[i+63:i]))
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRSQRT14PD'. Intrinsic: '_mm256_mask_rsqrt14_pd'.
// Requires AVX512F.
func MaskRsqrt14Pd1(src M256d, k Mmask8, a M256d) M256d {
	return M256d(maskRsqrt14Pd1([4]float64(src), uint8(k), [4]float64(a)))
}

func maskRsqrt14Pd1(src [4]float64, k uint8, a [4]float64) [4]float64


// MaskzRsqrt14Pd1: Compute the approximate reciprocal square root of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := APPROXIMATE(1.0 / SQRT(a[i+63:i]))
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRSQRT14PD'. Intrinsic: '_mm256_maskz_rsqrt14_pd'.
// Requires AVX512F.
func MaskzRsqrt14Pd1(k Mmask8, a M256d) M256d {
	return M256d(maskzRsqrt14Pd1(uint8(k), [4]float64(a)))
}

func maskzRsqrt14Pd1(k uint8, a [4]float64) [4]float64


// MaskRsqrt14Pd2: Compute the approximate reciprocal square root of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := APPROXIMATE(1.0 / SQRT(a[i+63:i]))
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRSQRT14PD'. Intrinsic: '_mm512_mask_rsqrt14_pd'.
// Requires AVX512F.
func MaskRsqrt14Pd2(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskRsqrt14Pd2([8]float64(src), uint8(k), [8]float64(a)))
}

func maskRsqrt14Pd2(src [8]float64, k uint8, a [8]float64) [8]float64


// MaskzRsqrt14Pd2: Compute the approximate reciprocal square root of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := APPROXIMATE(1.0 / SQRT(a[i+63:i]))
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRSQRT14PD'. Intrinsic: '_mm512_maskz_rsqrt14_pd'.
// Requires AVX512F.
func MaskzRsqrt14Pd2(k Mmask8, a M512d) M512d {
	return M512d(maskzRsqrt14Pd2(uint8(k), [8]float64(a)))
}

func maskzRsqrt14Pd2(k uint8, a [8]float64) [8]float64


// Rsqrt14Pd: Compute the approximate reciprocal square root of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst'. The maximum relative error for this approximation is less
// than 2^-14. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := APPROXIMATE(1.0 / SQRT(a[i+63:i]))
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRSQRT14PD'. Intrinsic: '_mm512_rsqrt14_pd'.
// Requires AVX512F.
func Rsqrt14Pd(a M512d) M512d {
	return M512d(rsqrt14Pd([8]float64(a)))
}

func rsqrt14Pd(a [8]float64) [8]float64


// MaskRsqrt14Ps: Compute the approximate reciprocal square root of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := APPROXIMATE(1.0 / SQRT(a[i+31:i]))
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRSQRT14PS'. Intrinsic: '_mm_mask_rsqrt14_ps'.
// Requires AVX512F.
func MaskRsqrt14Ps(src M128, k Mmask8, a M128) M128 {
	return M128(maskRsqrt14Ps([4]float32(src), uint8(k), [4]float32(a)))
}

func maskRsqrt14Ps(src [4]float32, k uint8, a [4]float32) [4]float32


// MaskzRsqrt14Ps: Compute the approximate reciprocal square root of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := APPROXIMATE(1.0 / SQRT(a[i+31:i]))
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRSQRT14PS'. Intrinsic: '_mm_maskz_rsqrt14_ps'.
// Requires AVX512F.
func MaskzRsqrt14Ps(k Mmask8, a M128) M128 {
	return M128(maskzRsqrt14Ps(uint8(k), [4]float32(a)))
}

func maskzRsqrt14Ps(k uint8, a [4]float32) [4]float32


// MaskRsqrt14Ps1: Compute the approximate reciprocal square root of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := APPROXIMATE(1.0 / SQRT(a[i+31:i]))
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRSQRT14PS'. Intrinsic: '_mm256_mask_rsqrt14_ps'.
// Requires AVX512F.
func MaskRsqrt14Ps1(src M256, k Mmask8, a M256) M256 {
	return M256(maskRsqrt14Ps1([8]float32(src), uint8(k), [8]float32(a)))
}

func maskRsqrt14Ps1(src [8]float32, k uint8, a [8]float32) [8]float32


// MaskzRsqrt14Ps1: Compute the approximate reciprocal square root of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := APPROXIMATE(1.0 / SQRT(a[i+31:i]))
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRSQRT14PS'. Intrinsic: '_mm256_maskz_rsqrt14_ps'.
// Requires AVX512F.
func MaskzRsqrt14Ps1(k Mmask8, a M256) M256 {
	return M256(maskzRsqrt14Ps1(uint8(k), [8]float32(a)))
}

func maskzRsqrt14Ps1(k uint8, a [8]float32) [8]float32


// MaskRsqrt14Ps2: Compute the approximate reciprocal square root of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := APPROXIMATE(1.0 / SQRT(a[i+31:i]))
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRSQRT14PS'. Intrinsic: '_mm512_mask_rsqrt14_ps'.
// Requires AVX512F.
func MaskRsqrt14Ps2(src M512, k Mmask16, a M512) M512 {
	return M512(maskRsqrt14Ps2([16]float32(src), uint16(k), [16]float32(a)))
}

func maskRsqrt14Ps2(src [16]float32, k uint16, a [16]float32) [16]float32


// MaskzRsqrt14Ps2: Compute the approximate reciprocal square root of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := APPROXIMATE(1.0 / SQRT(a[i+31:i]))
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRSQRT14PS'. Intrinsic: '_mm512_maskz_rsqrt14_ps'.
// Requires AVX512F.
func MaskzRsqrt14Ps2(k Mmask16, a M512) M512 {
	return M512(maskzRsqrt14Ps2(uint16(k), [16]float32(a)))
}

func maskzRsqrt14Ps2(k uint16, a [16]float32) [16]float32


// Rsqrt14Ps: Compute the approximate reciprocal square root of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst'. The maximum relative error for this approximation is less
// than 2^-14. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := APPROXIMATE(1.0 / SQRT(a[i+31:i]))
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VRSQRT14PS'. Intrinsic: '_mm512_rsqrt14_ps'.
// Requires AVX512F.
func Rsqrt14Ps(a M512) M512 {
	return M512(rsqrt14Ps([16]float32(a)))
}

func rsqrt14Ps(a [16]float32) [16]float32


// MaskRsqrt14Sd: Compute the approximate reciprocal square root of the lower
// double-precision (64-bit) floating-point element in 'b', store the result in
// the lower element of 'dst' using writemask 'k' (the element is copied from
// 'src' when mask bit 0 is not set), and copy the upper element from 'a' to
// the upper element of 'dst'. The maximum relative error for this
// approximation is less than 2^-14. 
//
//		IF k[0]
//			dst[63:0] := APPROXIMATE(1.0 / SQRT(b[63:0]))
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VRSQRT14SD'. Intrinsic: '_mm_mask_rsqrt14_sd'.
// Requires AVX512F.
func MaskRsqrt14Sd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskRsqrt14Sd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskRsqrt14Sd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzRsqrt14Sd: Compute the approximate reciprocal square root of the lower
// double-precision (64-bit) floating-point element in 'b', store the result in
// the lower element of 'dst' using zeromask 'k' (the element is zeroed out
// when mask bit 0 is not set), and copy the upper element from 'a' to the
// upper element of 'dst'. The maximum relative error for this approximation is
// less than 2^-14. 
//
//		IF k[0]
//			dst[63:0] := APPROXIMATE(1.0 / SQRT(b[63:0]))
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VRSQRT14SD'. Intrinsic: '_mm_maskz_rsqrt14_sd'.
// Requires AVX512F.
func MaskzRsqrt14Sd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzRsqrt14Sd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzRsqrt14Sd(k uint8, a [2]float64, b [2]float64) [2]float64


// Rsqrt14Sd: Compute the approximate reciprocal square root of the lower
// double-precision (64-bit) floating-point element in 'b', store the result in
// the lower element of 'dst', and copy the upper element from 'a' to the upper
// element of 'dst'. The maximum relative error for this approximation is less
// than 2^-14. 
//
//		dst[63:0] := APPROXIMATE(1.0 / SQRT(b[63:0]))
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VRSQRT14SD'. Intrinsic: '_mm_rsqrt14_sd'.
// Requires AVX512F.
func Rsqrt14Sd(a M128d, b M128d) M128d {
	return M128d(rsqrt14Sd([2]float64(a), [2]float64(b)))
}

func rsqrt14Sd(a [2]float64, b [2]float64) [2]float64


// MaskRsqrt14Ss: Compute the approximate reciprocal square root of the lower
// single-precision (32-bit) floating-point element in 'b', store the result in
// the lower element of 'dst' using writemask 'k' (the element is copied from
// 'src' when mask bit 0 is not set), and copy the upper 3 packed elements from
// 'a' to the upper elements of 'dst'. The maximum relative error for this
// approximation is less than 2^-14. 
//
//		IF k[0]
//			dst[31:0] := APPROXIMATE(1.0 / SQRT(b[31:0]))
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VRSQRT14SS'. Intrinsic: '_mm_mask_rsqrt14_ss'.
// Requires AVX512F.
func MaskRsqrt14Ss(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskRsqrt14Ss([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskRsqrt14Ss(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzRsqrt14Ss: Compute the approximate reciprocal square root of the lower
// single-precision (32-bit) floating-point element in 'b', store the result in
// the lower element of 'dst' using zeromask 'k' (the element is zeroed out
// when mask bit 0 is not set), and copy the upper 3 packed elements from 'a'
// to the upper elements of 'dst'. The maximum relative error for this
// approximation is less than 2^-14. 
//
//		IF k[0]
//			dst[31:0] := APPROXIMATE(1.0 / SQRT(b[31:0]))
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VRSQRT14SS'. Intrinsic: '_mm_maskz_rsqrt14_ss'.
// Requires AVX512F.
func MaskzRsqrt14Ss(k Mmask8, a M128, b M128) M128 {
	return M128(maskzRsqrt14Ss(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzRsqrt14Ss(k uint8, a [4]float32, b [4]float32) [4]float32


// Rsqrt14Ss: Compute the approximate reciprocal square root of the lower
// single-precision (32-bit) floating-point element in 'b', store the result in
// the lower element of 'dst', and copy the upper 3 packed elements from 'a' to
// the upper elements of 'dst'. The maximum relative error for this
// approximation is less than 2^-14. 
//
//		dst[31:0] := APPROXIMATE(1.0 / SQRT(b[31:0]))
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VRSQRT14SS'. Intrinsic: '_mm_rsqrt14_ss'.
// Requires AVX512F.
func Rsqrt14Ss(a M128, b M128) M128 {
	return M128(rsqrt14Ss([4]float32(a), [4]float32(b)))
}

func rsqrt14Ss(a [4]float32, b [4]float32) [4]float32


// MaskScalefPd: Scale the packed double-precision (64-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFPD'. Intrinsic: '_mm_mask_scalef_pd'.
// Requires AVX512F.
func MaskScalefPd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskScalefPd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskScalefPd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzScalefPd: Scale the packed double-precision (64-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFPD'. Intrinsic: '_mm_maskz_scalef_pd'.
// Requires AVX512F.
func MaskzScalefPd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzScalefPd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzScalefPd(k uint8, a [2]float64, b [2]float64) [2]float64


// ScalefPd: Scale the packed double-precision (64-bit) floating-point elements
// in 'a' using values from 'b', and store the results in 'dst'. 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFPD'. Intrinsic: '_mm_scalef_pd'.
// Requires AVX512F.
func ScalefPd(a M128d, b M128d) M128d {
	return M128d(scalefPd([2]float64(a), [2]float64(b)))
}

func scalefPd(a [2]float64, b [2]float64) [2]float64


// MaskScalefPd1: Scale the packed double-precision (64-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSCALEFPD'. Intrinsic: '_mm256_mask_scalef_pd'.
// Requires AVX512F.
func MaskScalefPd1(src M256d, k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskScalefPd1([4]float64(src), uint8(k), [4]float64(a), [4]float64(b)))
}

func maskScalefPd1(src [4]float64, k uint8, a [4]float64, b [4]float64) [4]float64


// MaskzScalefPd1: Scale the packed double-precision (64-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSCALEFPD'. Intrinsic: '_mm256_maskz_scalef_pd'.
// Requires AVX512F.
func MaskzScalefPd1(k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskzScalefPd1(uint8(k), [4]float64(a), [4]float64(b)))
}

func maskzScalefPd1(k uint8, a [4]float64, b [4]float64) [4]float64


// ScalefPd1: Scale the packed double-precision (64-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst'. 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSCALEFPD'. Intrinsic: '_mm256_scalef_pd'.
// Requires AVX512F.
func ScalefPd1(a M256d, b M256d) M256d {
	return M256d(scalefPd1([4]float64(a), [4]float64(b)))
}

func scalefPd1(a [4]float64, b [4]float64) [4]float64


// MaskScalefPd2: Scale the packed double-precision (64-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSCALEFPD'. Intrinsic: '_mm512_mask_scalef_pd'.
// Requires AVX512F.
func MaskScalefPd2(src M512d, k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskScalefPd2([8]float64(src), uint8(k), [8]float64(a), [8]float64(b)))
}

func maskScalefPd2(src [8]float64, k uint8, a [8]float64, b [8]float64) [8]float64


// MaskzScalefPd2: Scale the packed double-precision (64-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSCALEFPD'. Intrinsic: '_mm512_maskz_scalef_pd'.
// Requires AVX512F.
func MaskzScalefPd2(k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskzScalefPd2(uint8(k), [8]float64(a), [8]float64(b)))
}

func maskzScalefPd2(k uint8, a [8]float64, b [8]float64) [8]float64


// ScalefPd2: Scale the packed double-precision (64-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst'. 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSCALEFPD'. Intrinsic: '_mm512_scalef_pd'.
// Requires AVX512F.
func ScalefPd2(a M512d, b M512d) M512d {
	return M512d(scalefPd2([8]float64(a), [8]float64(b)))
}

func scalefPd2(a [8]float64, b [8]float64) [8]float64


// MaskScalefPs: Scale the packed single-precision (32-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[31:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFPS'. Intrinsic: '_mm_mask_scalef_ps'.
// Requires AVX512F.
func MaskScalefPs(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskScalefPs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskScalefPs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzScalefPs: Scale the packed single-precision (32-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[31:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFPS'. Intrinsic: '_mm_maskz_scalef_ps'.
// Requires AVX512F.
func MaskzScalefPs(k Mmask8, a M128, b M128) M128 {
	return M128(maskzScalefPs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzScalefPs(k uint8, a [4]float32, b [4]float32) [4]float32


// ScalefPs: Scale the packed single-precision (32-bit) floating-point elements
// in 'a' using values from 'b', and store the results in 'dst'. 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[31:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFPS'. Intrinsic: '_mm_scalef_ps'.
// Requires AVX512F.
func ScalefPs(a M128, b M128) M128 {
	return M128(scalefPs([4]float32(a), [4]float32(b)))
}

func scalefPs(a [4]float32, b [4]float32) [4]float32


// MaskScalefPs1: Scale the packed single-precision (32-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[31:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSCALEFPS'. Intrinsic: '_mm256_mask_scalef_ps'.
// Requires AVX512F.
func MaskScalefPs1(src M256, k Mmask8, a M256, b M256) M256 {
	return M256(maskScalefPs1([8]float32(src), uint8(k), [8]float32(a), [8]float32(b)))
}

func maskScalefPs1(src [8]float32, k uint8, a [8]float32, b [8]float32) [8]float32


// MaskzScalefPs1: Scale the packed single-precision (32-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[31:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSCALEFPS'. Intrinsic: '_mm256_maskz_scalef_ps'.
// Requires AVX512F.
func MaskzScalefPs1(k Mmask8, a M256, b M256) M256 {
	return M256(maskzScalefPs1(uint8(k), [8]float32(a), [8]float32(b)))
}

func maskzScalefPs1(k uint8, a [8]float32, b [8]float32) [8]float32


// ScalefPs1: Scale the packed single-precision (32-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst'. 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[31:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSCALEFPS'. Intrinsic: '_mm256_scalef_ps'.
// Requires AVX512F.
func ScalefPs1(a M256, b M256) M256 {
	return M256(scalefPs1([8]float32(a), [8]float32(b)))
}

func scalefPs1(a [8]float32, b [8]float32) [8]float32


// MaskScalefPs2: Scale the packed single-precision (32-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[31:0]
//		}
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSCALEFPS'. Intrinsic: '_mm512_mask_scalef_ps'.
// Requires AVX512F.
func MaskScalefPs2(src M512, k Mmask16, a M512, b M512) M512 {
	return M512(maskScalefPs2([16]float32(src), uint16(k), [16]float32(a), [16]float32(b)))
}

func maskScalefPs2(src [16]float32, k uint16, a [16]float32, b [16]float32) [16]float32


// MaskzScalefPs2: Scale the packed single-precision (32-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[31:0]
//		}
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSCALEFPS'. Intrinsic: '_mm512_maskz_scalef_ps'.
// Requires AVX512F.
func MaskzScalefPs2(k Mmask16, a M512, b M512) M512 {
	return M512(maskzScalefPs2(uint16(k), [16]float32(a), [16]float32(b)))
}

func maskzScalefPs2(k uint16, a [16]float32, b [16]float32) [16]float32


// ScalefPs2: Scale the packed single-precision (32-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst'. 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[31:0]
//		}
//		
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSCALEFPS'. Intrinsic: '_mm512_scalef_ps'.
// Requires AVX512F.
func ScalefPs2(a M512, b M512) M512 {
	return M512(scalefPs2([16]float32(a), [16]float32(b)))
}

func scalefPs2(a [16]float32, b [16]float32) [16]float32


// MaskScalefRoundPd: Scale the packed double-precision (64-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSCALEFPD'. Intrinsic: '_mm512_mask_scalef_round_pd'.
// Requires AVX512F.
func MaskScalefRoundPd(src M512d, k Mmask8, a M512d, b M512d, rounding int) M512d {
	return M512d(maskScalefRoundPd([8]float64(src), uint8(k), [8]float64(a), [8]float64(b), rounding))
}

func maskScalefRoundPd(src [8]float64, k uint8, a [8]float64, b [8]float64, rounding int) [8]float64


// MaskzScalefRoundPd: Scale the packed double-precision (64-bit)
// floating-point elements in 'a' using values from 'b', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSCALEFPD'. Intrinsic: '_mm512_maskz_scalef_round_pd'.
// Requires AVX512F.
func MaskzScalefRoundPd(k Mmask8, a M512d, b M512d, rounding int) M512d {
	return M512d(maskzScalefRoundPd(uint8(k), [8]float64(a), [8]float64(b), rounding))
}

func maskzScalefRoundPd(k uint8, a [8]float64, b [8]float64, rounding int) [8]float64


// ScalefRoundPd: Scale the packed double-precision (64-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSCALEFPD'. Intrinsic: '_mm512_scalef_round_pd'.
// Requires AVX512F.
func ScalefRoundPd(a M512d, b M512d, rounding int) M512d {
	return M512d(scalefRoundPd([8]float64(a), [8]float64(b), rounding))
}

func scalefRoundPd(a [8]float64, b [8]float64, rounding int) [8]float64


// MaskScalefRoundPs: Scale the packed single-precision (32-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[31:0]
//		}
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSCALEFPS'. Intrinsic: '_mm512_mask_scalef_round_ps'.
// Requires AVX512F.
func MaskScalefRoundPs(src M512, k Mmask16, a M512, b M512, rounding int) M512 {
	return M512(maskScalefRoundPs([16]float32(src), uint16(k), [16]float32(a), [16]float32(b), rounding))
}

func maskScalefRoundPs(src [16]float32, k uint16, a [16]float32, b [16]float32, rounding int) [16]float32


// MaskzScalefRoundPs: Scale the packed single-precision (32-bit)
// floating-point elements in 'a' using values from 'b', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[31:0]
//		}
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSCALEFPS'. Intrinsic: '_mm512_maskz_scalef_round_ps'.
// Requires AVX512F.
func MaskzScalefRoundPs(k Mmask16, a M512, b M512, rounding int) M512 {
	return M512(maskzScalefRoundPs(uint16(k), [16]float32(a), [16]float32(b), rounding))
}

func maskzScalefRoundPs(k uint16, a [16]float32, b [16]float32, rounding int) [16]float32


// ScalefRoundPs: Scale the packed single-precision (32-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[31:0]
//		}
//		
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSCALEFPS'. Intrinsic: '_mm512_scalef_round_ps'.
// Requires AVX512F.
func ScalefRoundPs(a M512, b M512, rounding int) M512 {
	return M512(scalefRoundPs([16]float32(a), [16]float32(b), rounding))
}

func scalefRoundPs(a [16]float32, b [16]float32, rounding int) [16]float32


// MaskScalefRoundSd: Scale the packed double-precision (64-bit) floating-point
// elements in 'a' using values from 'b', store the result in the lower element
// of 'dst' using writemask 'k' (the element is copied from 'src' when mask bit
// 0 is not set), and copy the upper element from 'b' to the upper element of
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		IF k[0]
//			dst[63:0] := SCALE(a[63:0], b[63:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFSD'. Intrinsic: '_mm_mask_scalef_round_sd'.
// Requires AVX512F.
func MaskScalefRoundSd(src M128d, k Mmask8, a M128d, b M128d, rounding int) M128d {
	return M128d(maskScalefRoundSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), rounding))
}

func maskScalefRoundSd(src [2]float64, k uint8, a [2]float64, b [2]float64, rounding int) [2]float64


// MaskzScalefRoundSd: Scale the packed double-precision (64-bit)
// floating-point elements in 'a' using values from 'b', store the result in
// the lower element of 'dst' using zeromask 'k' (the element is zeroed out
// when mask bit 0 is not set), and copy the upper element from 'b' to the
// upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		IF k[0]
//			dst[63:0] := SCALE(a[63:0], b[63:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFSD'. Intrinsic: '_mm_maskz_scalef_round_sd'.
// Requires AVX512F.
func MaskzScalefRoundSd(k Mmask8, a M128d, b M128d, rounding int) M128d {
	return M128d(maskzScalefRoundSd(uint8(k), [2]float64(a), [2]float64(b), rounding))
}

func maskzScalefRoundSd(k uint8, a [2]float64, b [2]float64, rounding int) [2]float64


// ScalefRoundSd: Scale the packed double-precision (64-bit) floating-point
// elements in 'a' using values from 'b', store the result in the lower element
// of 'dst', and copy the upper element from 'b' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		dst[63:0] := SCALE(a[63:0], b[63:0])
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFSD'. Intrinsic: '_mm_scalef_round_sd'.
// Requires AVX512F.
func ScalefRoundSd(a M128d, b M128d, rounding int) M128d {
	return M128d(scalefRoundSd([2]float64(a), [2]float64(b), rounding))
}

func scalefRoundSd(a [2]float64, b [2]float64, rounding int) [2]float64


// MaskScalefRoundSs: Scale the packed single-precision (32-bit) floating-point
// elements in 'a' using values from 'b', store the result in the lower element
// of 'dst' using writemask 'k' (the element is copied from 'src' when mask bit
// 0 is not set), and copy the upper 3 packed elements from 'b' to the upper
// elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[63:0]
//		}
//		
//		IF k[0]
//			dst[31:0] := SCALE(a[31:0], b[31:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFSS'. Intrinsic: '_mm_mask_scalef_round_ss'.
// Requires AVX512F.
func MaskScalefRoundSs(src M128, k Mmask8, a M128, b M128, rounding int) M128 {
	return M128(maskScalefRoundSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), rounding))
}

func maskScalefRoundSs(src [4]float32, k uint8, a [4]float32, b [4]float32, rounding int) [4]float32


// MaskzScalefRoundSs: Scale the packed single-precision (32-bit)
// floating-point elements in 'a' using values from 'b', store the result in
// the lower element of 'dst' using zeromask 'k' (the element is zeroed out
// when mask bit 0 is not set), and copy the upper 3 packed elements from 'b'
// to the upper elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[63:0]
//		}
//		
//		IF k[0]
//			dst[31:0] := SCALE(a[31:0], b[31:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFSS'. Intrinsic: '_mm_maskz_scalef_round_ss'.
// Requires AVX512F.
func MaskzScalefRoundSs(k Mmask8, a M128, b M128, rounding int) M128 {
	return M128(maskzScalefRoundSs(uint8(k), [4]float32(a), [4]float32(b), rounding))
}

func maskzScalefRoundSs(k uint8, a [4]float32, b [4]float32, rounding int) [4]float32


// ScalefRoundSs: Scale the packed single-precision (32-bit) floating-point
// elements in 'a' using values from 'b', store the result in the lower element
// of 'dst', and copy the upper 3 packed elements from 'b' to the upper
// elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[63:0]
//		}
//		
//		dst[31:0] := SCALE(a[31:0], b[31:0])
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFSS'. Intrinsic: '_mm_scalef_round_ss'.
// Requires AVX512F.
func ScalefRoundSs(a M128, b M128, rounding int) M128 {
	return M128(scalefRoundSs([4]float32(a), [4]float32(b), rounding))
}

func scalefRoundSs(a [4]float32, b [4]float32, rounding int) [4]float32


// MaskScalefSd: Scale the packed double-precision (64-bit) floating-point
// elements in 'a' using values from 'b', store the result in the lower element
// of 'dst' using writemask 'k' (the element is copied from 'src' when mask bit
// 0 is not set), and copy the upper element from 'b' to the upper element of
// 'dst'. 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		IF k[0]
//			dst[63:0] := SCALE(a[63:0], b[63:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFSD'. Intrinsic: '_mm_mask_scalef_sd'.
// Requires AVX512F.
func MaskScalefSd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskScalefSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskScalefSd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzScalefSd: Scale the packed double-precision (64-bit) floating-point
// elements in 'a' using values from 'b', store the result in the lower element
// of 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is
// not set), and copy the upper element from 'b' to the upper element of 'dst'. 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		IF k[0]
//			dst[63:0] := SCALE(a[63:0], b[63:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFSD'. Intrinsic: '_mm_maskz_scalef_sd'.
// Requires AVX512F.
func MaskzScalefSd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzScalefSd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzScalefSd(k uint8, a [2]float64, b [2]float64) [2]float64


// ScalefSd: Scale the packed double-precision (64-bit) floating-point elements
// in 'a' using values from 'b', store the result in the lower element of
// 'dst', and copy the upper element from 'b' to the upper element of 'dst'. 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		dst[63:0] := SCALE(a[63:0], b[63:0])
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFSD'. Intrinsic: '_mm_scalef_sd'.
// Requires AVX512F.
func ScalefSd(a M128d, b M128d) M128d {
	return M128d(scalefSd([2]float64(a), [2]float64(b)))
}

func scalefSd(a [2]float64, b [2]float64) [2]float64


// MaskScalefSs: Scale the packed single-precision (32-bit) floating-point
// elements in 'a' using values from 'b', store the result in the lower element
// of 'dst' using writemask 'k' (the element is copied from 'src' when mask bit
// 0 is not set), and copy the upper 3 packed elements from 'b' to the upper
// elements of 'dst'. 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[63:0]
//		}
//		
//		IF k[0]
//			dst[31:0] := SCALE(a[31:0], b[31:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFSS'. Intrinsic: '_mm_mask_scalef_ss'.
// Requires AVX512F.
func MaskScalefSs(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskScalefSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskScalefSs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzScalefSs: Scale the packed single-precision (32-bit) floating-point
// elements in 'a' using values from 'b', store the result in the lower element
// of 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is
// not set), and copy the upper 3 packed elements from 'b' to the upper
// elements of 'dst'. 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[63:0]
//		}
//		
//		IF k[0]
//			dst[31:0] := SCALE(a[31:0], b[31:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFSS'. Intrinsic: '_mm_maskz_scalef_ss'.
// Requires AVX512F.
func MaskzScalefSs(k Mmask8, a M128, b M128) M128 {
	return M128(maskzScalefSs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzScalefSs(k uint8, a [4]float32, b [4]float32) [4]float32


// ScalefSs: Scale the packed single-precision (32-bit) floating-point elements
// in 'a' using values from 'b', store the result in the lower element of
// 'dst', and copy the upper 3 packed elements from 'b' to the upper elements
// of 'dst'. 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[63:0]
//		}
//		
//		dst[31:0] := SCALE(a[31:0], b[31:0])
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFSS'. Intrinsic: '_mm_scalef_ss'.
// Requires AVX512F.
func ScalefSs(a M128, b M128) M128 {
	return M128(scalefSs([4]float32(a), [4]float32(b)))
}

func scalefSs(a [4]float32, b [4]float32) [4]float32


// SetEpi32: Set packed 32-bit integers in 'dst' with the supplied values. 
//
//		dst[31:0] := e0
//		dst[63:32] := e1
//		dst[95:64] := e2
//		dst[127:96] := e3
//		dst[159:128] := e4
//		dst[191:160] := e5
//		dst[223:192] := e6
//		dst[255:224] := e7
//		dst[287:256] := e8
//		dst[319:288] := e9
//		dst[351:320] := e10
//		dst[383:352] := e11
//		dst[415:384] := e12
//		dst[447:416] := e13
//		dst[479:448] := e14
//		dst[511:480] := e15
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_set_epi32'.
// Requires AVX512F.
func SetEpi32(e15 int, e14 int, e13 int, e12 int, e11 int, e10 int, e9 int, e8 int, e7 int, e6 int, e5 int, e4 int, e3 int, e2 int, e1 int, e0 int) M512i {
	return M512i(setEpi32(e15, e14, e13, e12, e11, e10, e9, e8, e7, e6, e5, e4, e3, e2, e1, e0))
}

func setEpi32(e15 int, e14 int, e13 int, e12 int, e11 int, e10 int, e9 int, e8 int, e7 int, e6 int, e5 int, e4 int, e3 int, e2 int, e1 int, e0 int) [64]byte


// SetEpi64: Set packed 64-bit integers in 'dst' with the supplied values. 
//
//		dst[63:0] := e0
//		dst[127:64] := e1
//		dst[191:128] := e2
//		dst[255:192] := e3
//		dst[319:256] := e4
//		dst[383:320] := e5
//		dst[447:384] := e6
//		dst[511:448] := e7
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_set_epi64'.
// Requires AVX512F.
func SetEpi64(e7 int64, e6 int64, e5 int64, e4 int64, e3 int64, e2 int64, e1 int64, e0 int64) M512i {
	return M512i(setEpi64(e7, e6, e5, e4, e3, e2, e1, e0))
}

func setEpi64(e7 int64, e6 int64, e5 int64, e4 int64, e3 int64, e2 int64, e1 int64, e0 int64) [64]byte


// SetPd: Set packed double-precision (64-bit) floating-point elements in 'dst'
// with the supplied values. 
//
//		dst[63:0] := e0
//		dst[127:64] := e1
//		dst[191:128] := e2
//		dst[255:192] := e3
//		dst[319:256] := e4
//		dst[383:320] := e5
//		dst[447:384] := e6
//		dst[511:448] := e7
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_set_pd'.
// Requires AVX512F.
func SetPd(e7 float64, e6 float64, e5 float64, e4 float64, e3 float64, e2 float64, e1 float64, e0 float64) M512d {
	return M512d(setPd(e7, e6, e5, e4, e3, e2, e1, e0))
}

func setPd(e7 float64, e6 float64, e5 float64, e4 float64, e3 float64, e2 float64, e1 float64, e0 float64) [8]float64


// SetPs: Set packed single-precision (32-bit) floating-point elements in 'dst'
// with the supplied values. 
//
//		dst[31:0] := e0
//		dst[63:32] := e1
//		dst[95:64] := e2
//		dst[127:96] := e3
//		dst[159:128] := e4
//		dst[191:160] := e5
//		dst[223:192] := e6
//		dst[255:224] := e7
//		dst[287:256] := e8
//		dst[319:288] := e9
//		dst[351:320] := e10
//		dst[383:352] := e11
//		dst[415:384] := e12
//		dst[447:416] := e13
//		dst[479:448] := e14
//		dst[511:480] := e15
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_set_ps'.
// Requires AVX512F.
func SetPs(e15 float32, e14 float32, e13 float32, e12 float32, e11 float32, e10 float32, e9 float32, e8 float32, e7 float32, e6 float32, e5 float32, e4 float32, e3 float32, e2 float32, e1 float32, e0 float32) M512 {
	return M512(setPs(e15, e14, e13, e12, e11, e10, e9, e8, e7, e6, e5, e4, e3, e2, e1, e0))
}

func setPs(e15 float32, e14 float32, e13 float32, e12 float32, e11 float32, e10 float32, e9 float32, e8 float32, e7 float32, e6 float32, e5 float32, e4 float32, e3 float32, e2 float32, e1 float32, e0 float32) [16]float32


// Set1Epi16: Broadcast the low packed 16-bit integer from 'a' to all all
// elements of 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*16
//			dst[i+15:i] := a[15:0]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_set1_epi16'.
// Requires AVX512F.
func Set1Epi16(a int16) M512i {
	return M512i(set1Epi16(a))
}

func set1Epi16(a int16) [64]byte


// MaskSet1Epi32: Broadcast 32-bit integer 'a' to all elements of 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm_mask_set1_epi32'.
// Requires AVX512F.
func MaskSet1Epi32(src M128i, k Mmask8, a int) M128i {
	return M128i(maskSet1Epi32([16]byte(src), uint8(k), a))
}

func maskSet1Epi32(src [16]byte, k uint8, a int) [16]byte


// MaskzSet1Epi32: Broadcast 32-bit integer 'a' to all elements of 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm_maskz_set1_epi32'.
// Requires AVX512F.
func MaskzSet1Epi32(k Mmask8, a int) M128i {
	return M128i(maskzSet1Epi32(uint8(k), a))
}

func maskzSet1Epi32(k uint8, a int) [16]byte


// MaskSet1Epi321: Broadcast 32-bit integer 'a' to all elements of 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm256_mask_set1_epi32'.
// Requires AVX512F.
func MaskSet1Epi321(src M256i, k Mmask8, a int) M256i {
	return M256i(maskSet1Epi321([32]byte(src), uint8(k), a))
}

func maskSet1Epi321(src [32]byte, k uint8, a int) [32]byte


// MaskzSet1Epi321: Broadcast 32-bit integer 'a' to all elements of 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm256_maskz_set1_epi32'.
// Requires AVX512F.
func MaskzSet1Epi321(k Mmask8, a int) M256i {
	return M256i(maskzSet1Epi321(uint8(k), a))
}

func maskzSet1Epi321(k uint8, a int) [32]byte


// MaskSet1Epi322: Broadcast 32-bit integer 'a' to all elements of 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm512_mask_set1_epi32'.
// Requires AVX512F.
func MaskSet1Epi322(src M512i, k Mmask16, a int) M512i {
	return M512i(maskSet1Epi322([64]byte(src), uint16(k), a))
}

func maskSet1Epi322(src [64]byte, k uint16, a int) [64]byte


// MaskzSet1Epi322: Broadcast 32-bit integer 'a' to all elements of 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm512_maskz_set1_epi32'.
// Requires AVX512F.
func MaskzSet1Epi322(k Mmask16, a int) M512i {
	return M512i(maskzSet1Epi322(uint16(k), a))
}

func maskzSet1Epi322(k uint16, a int) [64]byte


// Set1Epi32: Broadcast 32-bit integer 'a' to all elements of 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := a[31:0]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm512_set1_epi32'.
// Requires AVX512F.
func Set1Epi32(a int) M512i {
	return M512i(set1Epi32(a))
}

func set1Epi32(a int) [64]byte


// MaskSet1Epi64: Broadcast 64-bit integer 'a' to all elements of 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm_mask_set1_epi64'.
// Requires AVX512F.
func MaskSet1Epi64(src M128i, k Mmask8, a int64) M128i {
	return M128i(maskSet1Epi64([16]byte(src), uint8(k), a))
}

func maskSet1Epi64(src [16]byte, k uint8, a int64) [16]byte


// MaskzSet1Epi64: Broadcast 64-bit integer 'a' to all elements of 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm_maskz_set1_epi64'.
// Requires AVX512F.
func MaskzSet1Epi64(k Mmask8, a int64) M128i {
	return M128i(maskzSet1Epi64(uint8(k), a))
}

func maskzSet1Epi64(k uint8, a int64) [16]byte


// MaskSet1Epi641: Broadcast 64-bit integer 'a' to all elements of 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm256_mask_set1_epi64'.
// Requires AVX512F.
func MaskSet1Epi641(src M256i, k Mmask8, a int64) M256i {
	return M256i(maskSet1Epi641([32]byte(src), uint8(k), a))
}

func maskSet1Epi641(src [32]byte, k uint8, a int64) [32]byte


// MaskzSet1Epi641: Broadcast 64-bit integer 'a' to all elements of 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm256_maskz_set1_epi64'.
// Requires AVX512F.
func MaskzSet1Epi641(k Mmask8, a int64) M256i {
	return M256i(maskzSet1Epi641(uint8(k), a))
}

func maskzSet1Epi641(k uint8, a int64) [32]byte


// MaskSet1Epi642: Broadcast 64-bit integer 'a' to all elements of 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm512_mask_set1_epi64'.
// Requires AVX512F.
func MaskSet1Epi642(src M512i, k Mmask8, a int64) M512i {
	return M512i(maskSet1Epi642([64]byte(src), uint8(k), a))
}

func maskSet1Epi642(src [64]byte, k uint8, a int64) [64]byte


// MaskzSet1Epi642: Broadcast 64-bit integer 'a' to all elements of 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm512_maskz_set1_epi64'.
// Requires AVX512F.
func MaskzSet1Epi642(k Mmask8, a int64) M512i {
	return M512i(maskzSet1Epi642(uint8(k), a))
}

func maskzSet1Epi642(k uint8, a int64) [64]byte


// Set1Epi64: Broadcast 64-bit integer 'a' to all elements of 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := a[63:0]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm512_set1_epi64'.
// Requires AVX512F.
func Set1Epi64(a int64) M512i {
	return M512i(set1Epi64(a))
}

func set1Epi64(a int64) [64]byte


// Set1Epi8: Broadcast 8-bit integer 'a' to all elements of 'dst'. 
//
//		FOR j := 0 to 63
//			i := j*8
//			dst[i+7:i] := a[7:0]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_set1_epi8'.
// Requires AVX512F.
func Set1Epi8(a byte) M512i {
	return M512i(set1Epi8(a))
}

func set1Epi8(a byte) [64]byte


// Set1Pd: Broadcast double-precision (64-bit) floating-point value 'a' to all
// elements of 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := a[63:0]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_set1_pd'.
// Requires AVX512F.
func Set1Pd(a float64) M512d {
	return M512d(set1Pd(a))
}

func set1Pd(a float64) [8]float64


// Set1Ps: Broadcast single-precision (32-bit) floating-point value 'a' to all
// elements of 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := a[31:0]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_set1_ps'.
// Requires AVX512F.
func Set1Ps(a float32) M512 {
	return M512(set1Ps(a))
}

func set1Ps(a float32) [16]float32


// Set4Epi32: Set packed 32-bit integers in 'dst' with the repeated 4 element
// sequence. 
//
//		dst[31:0] := d
//		dst[63:32] := c
//		dst[95:64] := b
//		dst[127:96] := a
//		dst[159:128] := d
//		dst[191:160] := c
//		dst[223:192] := b
//		dst[255:224] := a
//		dst[287:256] := d
//		dst[319:288] := c
//		dst[351:320] := b
//		dst[383:352] := a
//		dst[415:384] := d
//		dst[447:416] := c
//		dst[479:448] := b
//		dst[511:480] := a
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_set4_epi32'.
// Requires AVX512F.
func Set4Epi32(d int, c int, b int, a int) M512i {
	return M512i(set4Epi32(d, c, b, a))
}

func set4Epi32(d int, c int, b int, a int) [64]byte


// Set4Epi64: Set packed 64-bit integers in 'dst' with the repeated 4 element
// sequence. 
//
//		dst[63:0] := d
//		dst[127:64] := c
//		dst[191:128] := b
//		dst[255:192] := a
//		dst[319:256] := d
//		dst[383:320] := c
//		dst[447:384] := b
//		dst[511:448] := a
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_set4_epi64'.
// Requires AVX512F.
func Set4Epi64(d int64, c int64, b int64, a int64) M512i {
	return M512i(set4Epi64(d, c, b, a))
}

func set4Epi64(d int64, c int64, b int64, a int64) [64]byte


// Set4Pd: Set packed double-precision (64-bit) floating-point elements in
// 'dst' with the repeated 4 element sequence. 
//
//		dst[63:0] := d
//		dst[127:64] := c
//		dst[191:128] := b
//		dst[255:192] := a
//		dst[319:256] := d
//		dst[383:320] := c
//		dst[447:384] := b
//		dst[511:448] := a
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_set4_pd'.
// Requires AVX512F.
func Set4Pd(d float64, c float64, b float64, a float64) M512d {
	return M512d(set4Pd(d, c, b, a))
}

func set4Pd(d float64, c float64, b float64, a float64) [8]float64


// Set4Ps: Set packed single-precision (32-bit) floating-point elements in
// 'dst' with the repeated 4 element sequence. 
//
//		dst[31:0] := d
//		dst[63:32] := c
//		dst[95:64] := b
//		dst[127:96] := a
//		dst[159:128] := d
//		dst[191:160] := c
//		dst[223:192] := b
//		dst[255:224] := a
//		dst[287:256] := d
//		dst[319:288] := c
//		dst[351:320] := b
//		dst[383:352] := a
//		dst[415:384] := d
//		dst[447:416] := c
//		dst[479:448] := b
//		dst[511:480] := a
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_set4_ps'.
// Requires AVX512F.
func Set4Ps(d float32, c float32, b float32, a float32) M512 {
	return M512(set4Ps(d, c, b, a))
}

func set4Ps(d float32, c float32, b float32, a float32) [16]float32


// SetrEpi32: Set packed 32-bit integers in 'dst' with the supplied values in
// reverse order. 
//
//		dst[31:0] := e15
//		dst[63:32] := e14
//		dst[95:64] := e13
//		dst[127:96] := e12
//		dst[159:128] := e11
//		dst[191:160] := e10
//		dst[223:192] := e9
//		dst[255:224] := e8
//		dst[287:256] := e7
//		dst[319:288] := e6
//		dst[351:320] := e5
//		dst[383:352] := e4
//		dst[415:384] := e3
//		dst[447:416] := e2
//		dst[479:448] := e1
//		dst[511:480] := e0
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_setr_epi32'.
// Requires AVX512F.
func SetrEpi32(e15 int, e14 int, e13 int, e12 int, e11 int, e10 int, e9 int, e8 int, e7 int, e6 int, e5 int, e4 int, e3 int, e2 int, e1 int, e0 int) M512i {
	return M512i(setrEpi32(e15, e14, e13, e12, e11, e10, e9, e8, e7, e6, e5, e4, e3, e2, e1, e0))
}

func setrEpi32(e15 int, e14 int, e13 int, e12 int, e11 int, e10 int, e9 int, e8 int, e7 int, e6 int, e5 int, e4 int, e3 int, e2 int, e1 int, e0 int) [64]byte


// SetrEpi64: Set packed 64-bit integers in 'dst' with the supplied values in
// reverse order. 
//
//		dst[63:0] := e7
//		dst[127:64] := e6
//		dst[191:128] := e5
//		dst[255:192] := e4
//		dst[319:256] := e3
//		dst[383:320] := e2
//		dst[447:384] := e1
//		dst[511:448] := e0
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_setr_epi64'.
// Requires AVX512F.
func SetrEpi64(e7 int64, e6 int64, e5 int64, e4 int64, e3 int64, e2 int64, e1 int64, e0 int64) M512i {
	return M512i(setrEpi64(e7, e6, e5, e4, e3, e2, e1, e0))
}

func setrEpi64(e7 int64, e6 int64, e5 int64, e4 int64, e3 int64, e2 int64, e1 int64, e0 int64) [64]byte


// SetrPd: Set packed double-precision (64-bit) floating-point elements in
// 'dst' with the supplied values in reverse order. 
//
//		dst[63:0] := e7
//		dst[127:64] := e6
//		dst[191:128] := e5
//		dst[255:192] := e4
//		dst[319:256] := e3
//		dst[383:320] := e2
//		dst[447:384] := e1
//		dst[511:448] := e0
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_setr_pd'.
// Requires AVX512F.
func SetrPd(e7 float64, e6 float64, e5 float64, e4 float64, e3 float64, e2 float64, e1 float64, e0 float64) M512d {
	return M512d(setrPd(e7, e6, e5, e4, e3, e2, e1, e0))
}

func setrPd(e7 float64, e6 float64, e5 float64, e4 float64, e3 float64, e2 float64, e1 float64, e0 float64) [8]float64


// SetrPs: Set packed single-precision (32-bit) floating-point elements in
// 'dst' with the supplied values in reverse order. 
//
//		dst[31:0] := e15
//		dst[63:32] := e14
//		dst[95:64] := e13
//		dst[127:96] := e12
//		dst[159:128] := e11
//		dst[191:160] := e10
//		dst[223:192] := e9
//		dst[255:224] := e8
//		dst[287:256] := e7
//		dst[319:288] := e6
//		dst[351:320] := e5
//		dst[383:352] := e4
//		dst[415:384] := e3
//		dst[447:416] := e2
//		dst[479:448] := e1
//		dst[511:480] := e0
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_setr_ps'.
// Requires AVX512F.
func SetrPs(e15 float32, e14 float32, e13 float32, e12 float32, e11 float32, e10 float32, e9 float32, e8 float32, e7 float32, e6 float32, e5 float32, e4 float32, e3 float32, e2 float32, e1 float32, e0 float32) M512 {
	return M512(setrPs(e15, e14, e13, e12, e11, e10, e9, e8, e7, e6, e5, e4, e3, e2, e1, e0))
}

func setrPs(e15 float32, e14 float32, e13 float32, e12 float32, e11 float32, e10 float32, e9 float32, e8 float32, e7 float32, e6 float32, e5 float32, e4 float32, e3 float32, e2 float32, e1 float32, e0 float32) [16]float32


// Setr4Epi32: Set packed 32-bit integers in 'dst' with the repeated 4 element
// sequence in reverse order. 
//
//		dst[31:0] := a
//		dst[63:32] := b
//		dst[95:64] := c
//		dst[127:96] := d
//		dst[159:128] := a
//		dst[191:160] := b
//		dst[223:192] := c
//		dst[255:224] := d
//		dst[287:256] := a
//		dst[319:288] := b
//		dst[351:320] := c
//		dst[383:352] := d
//		dst[415:384] := a
//		dst[447:416] := b
//		dst[479:448] := c
//		dst[511:480] := d
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_setr4_epi32'.
// Requires AVX512F.
func Setr4Epi32(d int, c int, b int, a int) M512i {
	return M512i(setr4Epi32(d, c, b, a))
}

func setr4Epi32(d int, c int, b int, a int) [64]byte


// Setr4Epi64: Set packed 64-bit integers in 'dst' with the repeated 4 element
// sequence in reverse order. 
//
//		dst[63:0] := a
//		dst[127:64] := b
//		dst[191:128] := c
//		dst[255:192] := d
//		dst[319:256] := a
//		dst[383:320] := b
//		dst[447:384] := c
//		dst[511:448] := d
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_setr4_epi64'.
// Requires AVX512F.
func Setr4Epi64(d int64, c int64, b int64, a int64) M512i {
	return M512i(setr4Epi64(d, c, b, a))
}

func setr4Epi64(d int64, c int64, b int64, a int64) [64]byte


// Setr4Pd: Set packed double-precision (64-bit) floating-point elements in
// 'dst' with the repeated 4 element sequence in reverse order. 
//
//		dst[63:0] := a
//		dst[127:64] := b
//		dst[191:128] := c
//		dst[255:192] := d
//		dst[319:256] := a
//		dst[383:320] := b
//		dst[447:384] := c
//		dst[511:448] := d
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_setr4_pd'.
// Requires AVX512F.
func Setr4Pd(d float64, c float64, b float64, a float64) M512d {
	return M512d(setr4Pd(d, c, b, a))
}

func setr4Pd(d float64, c float64, b float64, a float64) [8]float64


// Setr4Ps: Set packed single-precision (32-bit) floating-point elements in
// 'dst' with the repeated 4 element sequence in reverse order. 
//
//		dst[31:0] := a
//		dst[63:32] := b
//		dst[95:64] := c
//		dst[127:96] := d
//		dst[159:128] := a
//		dst[191:160] := b
//		dst[223:192] := c
//		dst[255:224] := d
//		dst[287:256] := a
//		dst[319:288] := b
//		dst[351:320] := c
//		dst[383:352] := d
//		dst[415:384] := a
//		dst[447:416] := b
//		dst[479:448] := c
//		dst[511:480] := d
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_setr4_ps'.
// Requires AVX512F.
func Setr4Ps(d float32, c float32, b float32, a float32) M512 {
	return M512(setr4Ps(d, c, b, a))
}

func setr4Ps(d float32, c float32, b float32, a float32) [16]float32


// Setzero: Return vector of type __m512 with all elements set to zero. 
//
//		dst[MAX:0] := 0
//
// Instruction: 'VPXORQ'. Intrinsic: '_mm512_setzero'.
// Requires AVX512F.
func Setzero() M512 {
	return M512(setzero())
}

func setzero() [16]float32


// SetzeroEpi32: Return vector of type __m512i with all elements set to zero. 
//
//		dst[MAX:0] := 0
//
// Instruction: 'VPXORQ'. Intrinsic: '_mm512_setzero_epi32'.
// Requires AVX512F.
func SetzeroEpi32() M512i {
	return M512i(setzeroEpi32())
}

func setzeroEpi32() [64]byte


// SetzeroPd: Return vector of type __m512d with all elements set to zero. 
//
//		dst[MAX:0] := 0
//
// Instruction: 'VPXORQ'. Intrinsic: '_mm512_setzero_pd'.
// Requires AVX512F.
func SetzeroPd() M512d {
	return M512d(setzeroPd())
}

func setzeroPd() [8]float64


// SetzeroPs: Return vector of type __m512 with all elements set to zero. 
//
//		dst[MAX:0] := 0
//
// Instruction: 'VPXORQ'. Intrinsic: '_mm512_setzero_ps'.
// Requires AVX512F.
func SetzeroPs() M512 {
	return M512(setzeroPs())
}

func setzeroPs() [16]float32


// SetzeroSi512: Return vector of type __m512i with all elements set to zero. 
//
//		dst[MAX:0] := 0
//
// Instruction: 'VPXORQ'. Intrinsic: '_mm512_setzero_si512'.
// Requires AVX512F.
func SetzeroSi512() M512i {
	return M512i(setzeroSi512())
}

func setzeroSi512() [64]byte


// MaskShuffleEpi32: Shuffle 32-bit integers in 'a' using the control in
// 'imm8', and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSHUFD'. Intrinsic: '_mm_mask_shuffle_epi32'.
// Requires AVX512F.
func MaskShuffleEpi32(src M128i, k Mmask8, a M128i, imm8 MMPERMENUM) M128i {
	return M128i(maskShuffleEpi32([16]byte(src), uint8(k), [16]byte(a), imm8))
}

func maskShuffleEpi32(src [16]byte, k uint8, a [16]byte, imm8 MMPERMENUM) [16]byte


// MaskzShuffleEpi32: Shuffle 32-bit integers in 'a' using the control in
// 'imm8', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSHUFD'. Intrinsic: '_mm_maskz_shuffle_epi32'.
// Requires AVX512F.
func MaskzShuffleEpi32(k Mmask8, a M128i, imm8 MMPERMENUM) M128i {
	return M128i(maskzShuffleEpi32(uint8(k), [16]byte(a), imm8))
}

func maskzShuffleEpi32(k uint8, a [16]byte, imm8 MMPERMENUM) [16]byte


// MaskShuffleEpi321: Shuffle 32-bit integers in 'a' within 128-bit lanes using
// the control in 'imm8', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		tmp_dst[223:192] := SELECT4(a[255:128], imm8[5:4])
//		tmp_dst[255:224] := SELECT4(a[255:128], imm8[7:6])
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSHUFD'. Intrinsic: '_mm256_mask_shuffle_epi32'.
// Requires AVX512F.
func MaskShuffleEpi321(src M256i, k Mmask8, a M256i, imm8 MMPERMENUM) M256i {
	return M256i(maskShuffleEpi321([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func maskShuffleEpi321(src [32]byte, k uint8, a [32]byte, imm8 MMPERMENUM) [32]byte


// MaskzShuffleEpi321: Shuffle 32-bit integers in 'a' within 128-bit lanes
// using the control in 'imm8', and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		tmp_dst[223:192] := SELECT4(a[255:128], imm8[5:4])
//		tmp_dst[255:224] := SELECT4(a[255:128], imm8[7:6])
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSHUFD'. Intrinsic: '_mm256_maskz_shuffle_epi32'.
// Requires AVX512F.
func MaskzShuffleEpi321(k Mmask8, a M256i, imm8 MMPERMENUM) M256i {
	return M256i(maskzShuffleEpi321(uint8(k), [32]byte(a), imm8))
}

func maskzShuffleEpi321(k uint8, a [32]byte, imm8 MMPERMENUM) [32]byte


// MaskzShuffleEpi322: Shuffle 32-bit integers in 'a' within 128-bit lanes
// using the control in 'imm8', and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		tmp_dst[223:192] := SELECT4(a[255:128], imm8[5:4])
//		tmp_dst[255:224] := SELECT4(a[255:128], imm8[7:6])
//		tmp_dst[287:256] := SELECT4(a[383:256], imm8[1:0])
//		tmp_dst[319:288] := SELECT4(a[383:256], imm8[3:2])
//		tmp_dst[351:320] := SELECT4(a[383:256], imm8[5:4])
//		tmp_dst[383:352] := SELECT4(a[383:256], imm8[7:6])
//		tmp_dst[415:384] := SELECT4(a[511:384], imm8[1:0])
//		tmp_dst[447:416] := SELECT4(a[511:384], imm8[3:2])
//		tmp_dst[479:448] := SELECT4(a[511:384], imm8[5:4])
//		tmp_dst[511:480] := SELECT4(a[511:384], imm8[7:6])
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSHUFD'. Intrinsic: '_mm512_maskz_shuffle_epi32'.
// Requires AVX512F.
func MaskzShuffleEpi322(k Mmask16, a M512i, imm8 MMPERMENUM) M512i {
	return M512i(maskzShuffleEpi322(uint16(k), [64]byte(a), imm8))
}

func maskzShuffleEpi322(k uint16, a [64]byte, imm8 MMPERMENUM) [64]byte


// MaskShuffleF32x4: Shuffle 128-bits (composed of 4 single-precision (32-bit)
// floating-point elements) selected by 'imm8' from 'a' and 'b', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		SELECT2(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT2(a[255:0], imm8[0])
//		tmp_dst[255:128] := SELECT2(b[255:0], imm8[1])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFF32X4'. Intrinsic: '_mm256_mask_shuffle_f32x4'.
// Requires AVX512F.
func MaskShuffleF32x4(src M256, k Mmask8, a M256, b M256, imm8 int) M256 {
	return M256(maskShuffleF32x4([8]float32(src), uint8(k), [8]float32(a), [8]float32(b), imm8))
}

func maskShuffleF32x4(src [8]float32, k uint8, a [8]float32, b [8]float32, imm8 int) [8]float32


// MaskzShuffleF32x4: Shuffle 128-bits (composed of 4 single-precision (32-bit)
// floating-point elements) selected by 'imm8' from 'a' and 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		SELECT2(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT2(a[255:0], imm8[0])
//		tmp_dst[255:128] := SELECT2(b[255:0], imm8[1])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFF32X4'. Intrinsic: '_mm256_maskz_shuffle_f32x4'.
// Requires AVX512F.
func MaskzShuffleF32x4(k Mmask8, a M256, b M256, imm8 int) M256 {
	return M256(maskzShuffleF32x4(uint8(k), [8]float32(a), [8]float32(b), imm8))
}

func maskzShuffleF32x4(k uint8, a [8]float32, b [8]float32, imm8 int) [8]float32


// ShuffleF32x4: Shuffle 128-bits (composed of 4 single-precision (32-bit)
// floating-point elements) selected by 'imm8' from 'a' and 'b', and store the
// results in 'dst'. 
//
//		SELECT2(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		dst[127:0] := SELECT2(a[255:0], imm8[0])
//		dst[255:128] := SELECT2(b[255:0], imm8[1])
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFF32X4'. Intrinsic: '_mm256_shuffle_f32x4'.
// Requires AVX512F.
func ShuffleF32x4(a M256, b M256, imm8 int) M256 {
	return M256(shuffleF32x4([8]float32(a), [8]float32(b), imm8))
}

func shuffleF32x4(a [8]float32, b [8]float32, imm8 int) [8]float32


// MaskShuffleF32x41: Shuffle 128-bits (composed of 4 single-precision (32-bit)
// floating-point elements) selected by 'imm8' from 'a' and 'b', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			2:	tmp[127:0] := src[383:256]
//			3:	tmp[127:0] := src[511:384]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT4(a[511:0], imm8[1:0])
//		tmp_dst[255:128] := SELECT4(a[511:0], imm8[3:2])
//		tmp_dst[383:256] := SELECT4(b[511:0], imm8[5:4])
//		tmp_dst[511:384] := SELECT4(b[511:0], imm8[7:6])
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFF32X4'. Intrinsic: '_mm512_mask_shuffle_f32x4'.
// Requires AVX512F.
func MaskShuffleF32x41(src M512, k Mmask16, a M512, b M512, imm8 int) M512 {
	return M512(maskShuffleF32x41([16]float32(src), uint16(k), [16]float32(a), [16]float32(b), imm8))
}

func maskShuffleF32x41(src [16]float32, k uint16, a [16]float32, b [16]float32, imm8 int) [16]float32


// MaskzShuffleF32x41: Shuffle 128-bits (composed of 4 single-precision
// (32-bit) floating-point elements) selected by 'imm8' from 'a' and 'b', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			2:	tmp[127:0] := src[383:256]
//			3:	tmp[127:0] := src[511:384]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT4(a[511:0], imm8[1:0])
//		tmp_dst[255:128] := SELECT4(a[511:0], imm8[3:2])
//		tmp_dst[383:256] := SELECT4(b[511:0], imm8[5:4])
//		tmp_dst[511:384] := SELECT4(b[511:0], imm8[7:6])
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFF32X4'. Intrinsic: '_mm512_maskz_shuffle_f32x4'.
// Requires AVX512F.
func MaskzShuffleF32x41(k Mmask16, a M512, b M512, imm8 int) M512 {
	return M512(maskzShuffleF32x41(uint16(k), [16]float32(a), [16]float32(b), imm8))
}

func maskzShuffleF32x41(k uint16, a [16]float32, b [16]float32, imm8 int) [16]float32


// ShuffleF32x41: Shuffle 128-bits (composed of 4 single-precision (32-bit)
// floating-point elements) selected by 'imm8' from 'a' and 'b', and store the
// results in 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			2:	tmp[127:0] := src[383:256]
//			3:	tmp[127:0] := src[511:384]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		dst[127:0] := SELECT4(a[511:0], imm8[1:0])
//		dst[255:128] := SELECT4(a[511:0], imm8[3:2])
//		dst[383:256] := SELECT4(b[511:0], imm8[5:4])
//		dst[511:384] := SELECT4(b[511:0], imm8[7:6])
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFF32X4'. Intrinsic: '_mm512_shuffle_f32x4'.
// Requires AVX512F.
func ShuffleF32x41(a M512, b M512, imm8 int) M512 {
	return M512(shuffleF32x41([16]float32(a), [16]float32(b), imm8))
}

func shuffleF32x41(a [16]float32, b [16]float32, imm8 int) [16]float32


// MaskShuffleF64x2: Shuffle 128-bits (composed of 2 double-precision (64-bit)
// floating-point elements) selected by 'imm8' from 'a' and 'b', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT2(a[255:0], imm8[0])
//		tmp_dst[255:128] := SELECT2(b[255:0], imm8[1])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFF64X2'. Intrinsic: '_mm256_mask_shuffle_f64x2'.
// Requires AVX512F.
func MaskShuffleF64x2(src M256d, k Mmask8, a M256d, b M256d, imm8 int) M256d {
	return M256d(maskShuffleF64x2([4]float64(src), uint8(k), [4]float64(a), [4]float64(b), imm8))
}

func maskShuffleF64x2(src [4]float64, k uint8, a [4]float64, b [4]float64, imm8 int) [4]float64


// MaskzShuffleF64x2: Shuffle 128-bits (composed of 2 double-precision (64-bit)
// floating-point elements) selected by 'imm8' from 'a' and 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT2(a[255:0], imm8[0])
//		tmp_dst[255:128] := SELECT2(b[255:0], imm8[1])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFF64X2'. Intrinsic: '_mm256_maskz_shuffle_f64x2'.
// Requires AVX512F.
func MaskzShuffleF64x2(k Mmask8, a M256d, b M256d, imm8 int) M256d {
	return M256d(maskzShuffleF64x2(uint8(k), [4]float64(a), [4]float64(b), imm8))
}

func maskzShuffleF64x2(k uint8, a [4]float64, b [4]float64, imm8 int) [4]float64


// ShuffleF64x2: Shuffle 128-bits (composed of 2 double-precision (64-bit)
// floating-point elements) selected by 'imm8' from 'a' and 'b', and store the
// results in 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		dst[127:0] := SELECT2(a[255:0], imm8[0])
//		dst[255:128] := SELECT2(b[255:0], imm8[1])
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFF64X2'. Intrinsic: '_mm256_shuffle_f64x2'.
// Requires AVX512F.
func ShuffleF64x2(a M256d, b M256d, imm8 int) M256d {
	return M256d(shuffleF64x2([4]float64(a), [4]float64(b), imm8))
}

func shuffleF64x2(a [4]float64, b [4]float64, imm8 int) [4]float64


// MaskShuffleF64x21: Shuffle 128-bits (composed of 2 double-precision (64-bit)
// floating-point elements) selected by 'imm8' from 'a' and 'b', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			2:	tmp[127:0] := src[383:256]
//			3:	tmp[127:0] := src[511:384]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT4(a[511:0], imm8[1:0])
//		tmp_dst[255:128] := SELECT4(a[511:0], imm8[3:2])
//		tmp_dst[383:256] := SELECT4(b[511:0], imm8[5:4])
//		tmp_dst[511:384] := SELECT4(b[511:0], imm8[7:6])
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFF64X2'. Intrinsic: '_mm512_mask_shuffle_f64x2'.
// Requires AVX512F.
func MaskShuffleF64x21(src M512d, k Mmask8, a M512d, b M512d, imm8 int) M512d {
	return M512d(maskShuffleF64x21([8]float64(src), uint8(k), [8]float64(a), [8]float64(b), imm8))
}

func maskShuffleF64x21(src [8]float64, k uint8, a [8]float64, b [8]float64, imm8 int) [8]float64


// MaskzShuffleF64x21: Shuffle 128-bits (composed of 2 double-precision
// (64-bit) floating-point elements) selected by 'imm8' from 'a' and 'b', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			2:	tmp[127:0] := src[383:256]
//			3:	tmp[127:0] := src[511:384]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT4(a[511:0], imm8[1:0])
//		tmp_dst[255:128] := SELECT4(a[511:0], imm8[3:2])
//		tmp_dst[383:256] := SELECT4(b[511:0], imm8[5:4])
//		tmp_dst[511:384] := SELECT4(b[511:0], imm8[7:6])
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFF64X2'. Intrinsic: '_mm512_maskz_shuffle_f64x2'.
// Requires AVX512F.
func MaskzShuffleF64x21(k Mmask8, a M512d, b M512d, imm8 int) M512d {
	return M512d(maskzShuffleF64x21(uint8(k), [8]float64(a), [8]float64(b), imm8))
}

func maskzShuffleF64x21(k uint8, a [8]float64, b [8]float64, imm8 int) [8]float64


// ShuffleF64x21: Shuffle 128-bits (composed of 2 double-precision (64-bit)
// floating-point elements) selected by 'imm8' from 'a' and 'b', and store the
// results in 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			2:	tmp[127:0] := src[383:256]
//			3:	tmp[127:0] := src[511:384]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		dst[127:0] := SELECT4(a[511:0], imm8[1:0])
//		dst[255:128] := SELECT4(a[511:0], imm8[3:2])
//		dst[383:256] := SELECT4(b[511:0], imm8[5:4])
//		dst[511:384] := SELECT4(b[511:0], imm8[7:6])
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFF64X2'. Intrinsic: '_mm512_shuffle_f64x2'.
// Requires AVX512F.
func ShuffleF64x21(a M512d, b M512d, imm8 int) M512d {
	return M512d(shuffleF64x21([8]float64(a), [8]float64(b), imm8))
}

func shuffleF64x21(a [8]float64, b [8]float64, imm8 int) [8]float64


// MaskShuffleI32x4: Shuffle 128-bits (composed of 4 32-bit integers) selected
// by 'imm8' from 'a' and 'b', and store the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		SELECT2(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT2(a[255:0], imm8[1:0])
//		tmp_dst[255:128] := SELECT2(b[255:0], imm8[3:2])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFI32X4'. Intrinsic: '_mm256_mask_shuffle_i32x4'.
// Requires AVX512F.
func MaskShuffleI32x4(src M256i, k Mmask8, a M256i, b M256i, imm8 int) M256i {
	return M256i(maskShuffleI32x4([32]byte(src), uint8(k), [32]byte(a), [32]byte(b), imm8))
}

func maskShuffleI32x4(src [32]byte, k uint8, a [32]byte, b [32]byte, imm8 int) [32]byte


// MaskzShuffleI32x4: Shuffle 128-bits (composed of 4 32-bit integers) selected
// by 'imm8' from 'a' and 'b', and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		SELECT2(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT2(a[255:0], imm8[1:0])
//		tmp_dst[255:128] := SELECT2(b[255:0], imm8[3:2])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFI32X4'. Intrinsic: '_mm256_maskz_shuffle_i32x4'.
// Requires AVX512F.
func MaskzShuffleI32x4(k Mmask8, a M256i, b M256i, imm8 int) M256i {
	return M256i(maskzShuffleI32x4(uint8(k), [32]byte(a), [32]byte(b), imm8))
}

func maskzShuffleI32x4(k uint8, a [32]byte, b [32]byte, imm8 int) [32]byte


// ShuffleI32x4: Shuffle 128-bits (composed of 4 32-bit integers) selected by
// 'imm8' from 'a' and 'b', and store the results in 'dst'. 
//
//		SELECT2(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		dst[127:0] := SELECT2(a[255:0], imm8[1:0])
//		dst[255:128] := SELECT2(b[255:0], imm8[3:2])
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFI32X4'. Intrinsic: '_mm256_shuffle_i32x4'.
// Requires AVX512F.
func ShuffleI32x4(a M256i, b M256i, imm8 int) M256i {
	return M256i(shuffleI32x4([32]byte(a), [32]byte(b), imm8))
}

func shuffleI32x4(a [32]byte, b [32]byte, imm8 int) [32]byte


// MaskShuffleI32x41: Shuffle 128-bits (composed of 4 32-bit integers) selected
// by 'imm8' from 'a' and 'b', and store the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			2:	tmp[127:0] := src[383:256]
//			3:	tmp[127:0] := src[511:384]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT4(a[511:0], imm8[1:0])
//		tmp_dst[255:128] := SELECT4(a[511:0], imm8[3:2])
//		tmp_dst[383:256] := SELECT4(b[511:0], imm8[5:4])
//		tmp_dst[511:384] := SELECT4(b[511:0], imm8[7:6])
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFI32X4'. Intrinsic: '_mm512_mask_shuffle_i32x4'.
// Requires AVX512F.
func MaskShuffleI32x41(src M512i, k Mmask16, a M512i, b M512i, imm8 int) M512i {
	return M512i(maskShuffleI32x41([64]byte(src), uint16(k), [64]byte(a), [64]byte(b), imm8))
}

func maskShuffleI32x41(src [64]byte, k uint16, a [64]byte, b [64]byte, imm8 int) [64]byte


// MaskzShuffleI32x41: Shuffle 128-bits (composed of 4 32-bit integers)
// selected by 'imm8' from 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			2:	tmp[127:0] := src[383:256]
//			3:	tmp[127:0] := src[511:384]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT4(a[511:0], imm8[1:0])
//		tmp_dst[255:128] := SELECT4(a[511:0], imm8[3:2])
//		tmp_dst[383:256] := SELECT4(b[511:0], imm8[5:4])
//		tmp_dst[511:384] := SELECT4(b[511:0], imm8[7:6])
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFI32X4'. Intrinsic: '_mm512_maskz_shuffle_i32x4'.
// Requires AVX512F.
func MaskzShuffleI32x41(k Mmask16, a M512i, b M512i, imm8 int) M512i {
	return M512i(maskzShuffleI32x41(uint16(k), [64]byte(a), [64]byte(b), imm8))
}

func maskzShuffleI32x41(k uint16, a [64]byte, b [64]byte, imm8 int) [64]byte


// ShuffleI32x41: Shuffle 128-bits (composed of 4 32-bit integers) selected by
// 'imm8' from 'a' and 'b', and store the results in 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			2:	tmp[127:0] := src[383:256]
//			3:	tmp[127:0] := src[511:384]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		dst[127:0] := SELECT4(a[511:0], imm8[1:0])
//		dst[255:128] := SELECT4(a[511:0], imm8[3:2])
//		dst[383:256] := SELECT4(b[511:0], imm8[5:4])
//		dst[511:384] := SELECT4(b[511:0], imm8[7:6])
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFI32X4'. Intrinsic: '_mm512_shuffle_i32x4'.
// Requires AVX512F.
func ShuffleI32x41(a M512i, b M512i, imm8 int) M512i {
	return M512i(shuffleI32x41([64]byte(a), [64]byte(b), imm8))
}

func shuffleI32x41(a [64]byte, b [64]byte, imm8 int) [64]byte


// MaskShuffleI64x2: Shuffle 128-bits (composed of 2 64-bit integers) selected
// by 'imm8' from 'a' and 'b', and store the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		SELECT2(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT2(a[255:0], imm8[1:0])
//		tmp_dst[255:128] := SELECT2(b[255:0], imm8[3:2])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFI64X2'. Intrinsic: '_mm256_mask_shuffle_i64x2'.
// Requires AVX512F.
func MaskShuffleI64x2(src M256i, k Mmask8, a M256i, b M256i, imm8 int) M256i {
	return M256i(maskShuffleI64x2([32]byte(src), uint8(k), [32]byte(a), [32]byte(b), imm8))
}

func maskShuffleI64x2(src [32]byte, k uint8, a [32]byte, b [32]byte, imm8 int) [32]byte


// MaskzShuffleI64x2: Shuffle 128-bits (composed of 2 64-bit integers) selected
// by 'imm8' from 'a' and 'b', and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		SELECT2(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT2(a[255:0], imm8[1:0])
//		tmp_dst[255:128] := SELECT2(b[255:0], imm8[3:2])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFI64X2'. Intrinsic: '_mm256_maskz_shuffle_i64x2'.
// Requires AVX512F.
func MaskzShuffleI64x2(k Mmask8, a M256i, b M256i, imm8 int) M256i {
	return M256i(maskzShuffleI64x2(uint8(k), [32]byte(a), [32]byte(b), imm8))
}

func maskzShuffleI64x2(k uint8, a [32]byte, b [32]byte, imm8 int) [32]byte


// ShuffleI64x2: Shuffle 128-bits (composed of 2 64-bit integers) selected by
// 'imm8' from 'a' and 'b', and store the results in 'dst'. 
//
//		SELECT2(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		dst[127:0] := SELECT2(a[255:0], imm8[1:0])
//		dst[255:128] := SELECT2(b[255:0], imm8[3:2])
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFI64X2'. Intrinsic: '_mm256_shuffle_i64x2'.
// Requires AVX512F.
func ShuffleI64x2(a M256i, b M256i, imm8 int) M256i {
	return M256i(shuffleI64x2([32]byte(a), [32]byte(b), imm8))
}

func shuffleI64x2(a [32]byte, b [32]byte, imm8 int) [32]byte


// MaskShuffleI64x21: Shuffle 128-bits (composed of 2 64-bit integers) selected
// by 'imm8' from 'a' and 'b', and store the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			2:	tmp[127:0] := src[383:256]
//			3:	tmp[127:0] := src[511:384]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT4(a[511:0], imm8[1:0])
//		tmp_dst[255:128] := SELECT4(a[511:0], imm8[3:2])
//		tmp_dst[383:256] := SELECT4(b[511:0], imm8[5:4])
//		tmp_dst[511:384] := SELECT4(b[511:0], imm8[7:6])
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFI64X2'. Intrinsic: '_mm512_mask_shuffle_i64x2'.
// Requires AVX512F.
func MaskShuffleI64x21(src M512i, k Mmask8, a M512i, b M512i, imm8 int) M512i {
	return M512i(maskShuffleI64x21([64]byte(src), uint8(k), [64]byte(a), [64]byte(b), imm8))
}

func maskShuffleI64x21(src [64]byte, k uint8, a [64]byte, b [64]byte, imm8 int) [64]byte


// MaskzShuffleI64x21: Shuffle 128-bits (composed of 2 64-bit integers)
// selected by 'imm8' from 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			2:	tmp[127:0] := src[383:256]
//			3:	tmp[127:0] := src[511:384]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT4(a[511:0], imm8[1:0])
//		tmp_dst[255:128] := SELECT4(a[511:0], imm8[3:2])
//		tmp_dst[383:256] := SELECT4(b[511:0], imm8[5:4])
//		tmp_dst[511:384] := SELECT4(b[511:0], imm8[7:6])
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFI64X2'. Intrinsic: '_mm512_maskz_shuffle_i64x2'.
// Requires AVX512F.
func MaskzShuffleI64x21(k Mmask8, a M512i, b M512i, imm8 int) M512i {
	return M512i(maskzShuffleI64x21(uint8(k), [64]byte(a), [64]byte(b), imm8))
}

func maskzShuffleI64x21(k uint8, a [64]byte, b [64]byte, imm8 int) [64]byte


// ShuffleI64x21: Shuffle 128-bits (composed of 2 64-bit integers) selected by
// 'imm8' from 'a' and 'b', and store the results in 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			2:	tmp[127:0] := src[383:256]
//			3:	tmp[127:0] := src[511:384]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		dst[127:0] := SELECT4(a[511:0], imm8[1:0])
//		dst[255:128] := SELECT4(a[511:0], imm8[3:2])
//		dst[383:256] := SELECT4(b[511:0], imm8[5:4])
//		dst[511:384] := SELECT4(b[511:0], imm8[7:6])
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFI64X2'. Intrinsic: '_mm512_shuffle_i64x2'.
// Requires AVX512F.
func ShuffleI64x21(a M512i, b M512i, imm8 int) M512i {
	return M512i(shuffleI64x21([64]byte(a), [64]byte(b), imm8))
}

func shuffleI64x21(a [64]byte, b [64]byte, imm8 int) [64]byte


// MaskShufflePd: Shuffle double-precision (64-bit) floating-point elements
// using the control in 'imm8', and store the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		tmp_dst[63:0] := (imm8[0] == 0) ? a[63:0] : a[127:64]
//		tmp_dst[127:64] := (imm8[1] == 0) ? b[63:0] : b[127:64]
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSHUFPD'. Intrinsic: '_mm_mask_shuffle_pd'.
// Requires AVX512F.
func MaskShufflePd(src M128d, k Mmask8, a M128d, b M128d, imm8 int) M128d {
	return M128d(maskShufflePd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), imm8))
}

func maskShufflePd(src [2]float64, k uint8, a [2]float64, b [2]float64, imm8 int) [2]float64


// MaskzShufflePd: Shuffle double-precision (64-bit) floating-point elements
// using the control in 'imm8', and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		tmp_dst[63:0] := (imm8[0] == 0) ? a[63:0] : a[127:64]
//		tmp_dst[127:64] := (imm8[1] == 0) ? b[63:0] : b[127:64]
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSHUFPD'. Intrinsic: '_mm_maskz_shuffle_pd'.
// Requires AVX512F.
func MaskzShufflePd(k Mmask8, a M128d, b M128d, imm8 int) M128d {
	return M128d(maskzShufflePd(uint8(k), [2]float64(a), [2]float64(b), imm8))
}

func maskzShufflePd(k uint8, a [2]float64, b [2]float64, imm8 int) [2]float64


// MaskShufflePd1: Shuffle double-precision (64-bit) floating-point elements
// within 128-bit lanes using the control in 'imm8', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		tmp_dst[63:0] := (imm8[0] == 0) ? a[63:0] : a[127:64]
//		tmp_dst[127:64] := (imm8[1] == 0) ? b[63:0] : b[127:64]
//		tmp_dst[191:128] := (imm8[2] == 0) ? a[191:128] : a[255:192]
//		tmp_dst[255:192] := (imm8[3] == 0) ? b[191:128] : b[255:192]
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFPD'. Intrinsic: '_mm256_mask_shuffle_pd'.
// Requires AVX512F.
func MaskShufflePd1(src M256d, k Mmask8, a M256d, b M256d, imm8 int) M256d {
	return M256d(maskShufflePd1([4]float64(src), uint8(k), [4]float64(a), [4]float64(b), imm8))
}

func maskShufflePd1(src [4]float64, k uint8, a [4]float64, b [4]float64, imm8 int) [4]float64


// MaskzShufflePd1: Shuffle double-precision (64-bit) floating-point elements
// within 128-bit lanes using the control in 'imm8', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		tmp_dst[63:0] := (imm8[0] == 0) ? a[63:0] : a[127:64]
//		tmp_dst[127:64] := (imm8[1] == 0) ? b[63:0] : b[127:64]
//		tmp_dst[191:128] := (imm8[2] == 0) ? a[191:128] : a[255:192]
//		tmp_dst[255:192] := (imm8[3] == 0) ? b[191:128] : b[255:192]
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFPD'. Intrinsic: '_mm256_maskz_shuffle_pd'.
// Requires AVX512F.
func MaskzShufflePd1(k Mmask8, a M256d, b M256d, imm8 int) M256d {
	return M256d(maskzShufflePd1(uint8(k), [4]float64(a), [4]float64(b), imm8))
}

func maskzShufflePd1(k uint8, a [4]float64, b [4]float64, imm8 int) [4]float64


// MaskShufflePd2: Shuffle double-precision (64-bit) floating-point elements
// within 128-bit lanes using the control in 'imm8', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		tmp_dst[63:0] := (imm8[0] == 0) ? a[63:0] : a[127:64]
//		tmp_dst[127:64] := (imm8[1] == 0) ? b[63:0] : b[127:64]
//		tmp_dst[191:128] := (imm8[2] == 0) ? a[191:128] : a[255:192]
//		tmp_dst[255:192] := (imm8[3] == 0) ? b[191:128] : b[255:192]
//		tmp_dst[319:256] := (imm8[4] == 0) ? a[319:256] : a[383:320]
//		tmp_dst[383:320] := (imm8[5] == 0) ? b[319:256] : b[383:320]
//		tmp_dst[447:384] := (imm8[6] == 0) ? a[447:384] : a[511:448]
//		tmp_dst[511:448] := (imm8[7] == 0) ? b[447:384] : b[511:448]
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFPD'. Intrinsic: '_mm512_mask_shuffle_pd'.
// Requires AVX512F.
func MaskShufflePd2(src M512d, k Mmask8, a M512d, b M512d, imm8 int) M512d {
	return M512d(maskShufflePd2([8]float64(src), uint8(k), [8]float64(a), [8]float64(b), imm8))
}

func maskShufflePd2(src [8]float64, k uint8, a [8]float64, b [8]float64, imm8 int) [8]float64


// MaskzShufflePd2: Shuffle double-precision (64-bit) floating-point elements
// within 128-bit lanes using the control in 'imm8', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		tmp_dst[63:0] := (imm8[0] == 0) ? a[63:0] : a[127:64]
//		tmp_dst[127:64] := (imm8[1] == 0) ? b[63:0] : b[127:64]
//		tmp_dst[191:128] := (imm8[2] == 0) ? a[191:128] : a[255:192]
//		tmp_dst[255:192] := (imm8[3] == 0) ? b[191:128] : b[255:192]
//		tmp_dst[319:256] := (imm8[4] == 0) ? a[319:256] : a[383:320]
//		tmp_dst[383:320] := (imm8[5] == 0) ? b[319:256] : b[383:320]
//		tmp_dst[447:384] := (imm8[6] == 0) ? a[447:384] : a[511:448]
//		tmp_dst[511:448] := (imm8[7] == 0) ? b[447:384] : b[511:448]
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFPD'. Intrinsic: '_mm512_maskz_shuffle_pd'.
// Requires AVX512F.
func MaskzShufflePd2(k Mmask8, a M512d, b M512d, imm8 int) M512d {
	return M512d(maskzShufflePd2(uint8(k), [8]float64(a), [8]float64(b), imm8))
}

func maskzShufflePd2(k uint8, a [8]float64, b [8]float64, imm8 int) [8]float64


// ShufflePd: Shuffle double-precision (64-bit) floating-point elements within
// 128-bit lanes using the control in 'imm8', and store the results in 'dst'. 
//
//		dst[63:0] := (imm8[0] == 0) ? a[63:0] : a[127:64]
//		dst[127:64] := (imm8[1] == 0) ? b[63:0] : b[127:64]
//		dst[191:128] := (imm8[2] == 0) ? a[191:128] : a[255:192]
//		dst[255:192] := (imm8[3] == 0) ? b[191:128] : b[255:192]
//		dst[319:256] := (imm8[4] == 0) ? a[319:256] : a[383:320]
//		dst[383:320] := (imm8[5] == 0) ? b[319:256] : b[383:320]
//		dst[447:384] := (imm8[6] == 0) ? a[447:384] : a[511:448]
//		dst[511:448] := (imm8[7] == 0) ? b[447:384] : b[511:448]
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFPD'. Intrinsic: '_mm512_shuffle_pd'.
// Requires AVX512F.
func ShufflePd(a M512d, b M512d, imm8 int) M512d {
	return M512d(shufflePd([8]float64(a), [8]float64(b), imm8))
}

func shufflePd(a [8]float64, b [8]float64, imm8 int) [8]float64


// MaskShufflePs: Shuffle single-precision (32-bit) floating-point elements in
// 'a' using the control in 'imm8', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(b[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(b[127:0], imm8[7:6])
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSHUFPS'. Intrinsic: '_mm_mask_shuffle_ps'.
// Requires AVX512F.
func MaskShufflePs(src M128, k Mmask8, a M128, b M128, imm8 int) M128 {
	return M128(maskShufflePs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), imm8))
}

func maskShufflePs(src [4]float32, k uint8, a [4]float32, b [4]float32, imm8 int) [4]float32


// MaskzShufflePs: Shuffle single-precision (32-bit) floating-point elements in
// 'a' using the control in 'imm8', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(b[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(b[127:0], imm8[7:6])
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSHUFPS'. Intrinsic: '_mm_maskz_shuffle_ps'.
// Requires AVX512F.
func MaskzShufflePs(k Mmask8, a M128, b M128, imm8 int) M128 {
	return M128(maskzShufflePs(uint8(k), [4]float32(a), [4]float32(b), imm8))
}

func maskzShufflePs(k uint8, a [4]float32, b [4]float32, imm8 int) [4]float32


// MaskShufflePs1: Shuffle single-precision (32-bit) floating-point elements in
// 'a' within 128-bit lanes using the control in 'imm8', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(b[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(b[127:0], imm8[7:6])
//		tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		tmp_dst[223:192] := SELECT4(b[255:128], imm8[5:4])
//		tmp_dst[255:224] := SELECT4(b[255:128], imm8[7:6])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFPS'. Intrinsic: '_mm256_mask_shuffle_ps'.
// Requires AVX512F.
func MaskShufflePs1(src M256, k Mmask8, a M256, b M256, imm8 int) M256 {
	return M256(maskShufflePs1([8]float32(src), uint8(k), [8]float32(a), [8]float32(b), imm8))
}

func maskShufflePs1(src [8]float32, k uint8, a [8]float32, b [8]float32, imm8 int) [8]float32


// MaskzShufflePs1: Shuffle single-precision (32-bit) floating-point elements
// in 'a' within 128-bit lanes using the control in 'imm8', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(b[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(b[127:0], imm8[7:6])
//		tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		tmp_dst[223:192] := SELECT4(b[255:128], imm8[5:4])
//		tmp_dst[255:224] := SELECT4(b[255:128], imm8[7:6])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFPS'. Intrinsic: '_mm256_maskz_shuffle_ps'.
// Requires AVX512F.
func MaskzShufflePs1(k Mmask8, a M256, b M256, imm8 int) M256 {
	return M256(maskzShufflePs1(uint8(k), [8]float32(a), [8]float32(b), imm8))
}

func maskzShufflePs1(k uint8, a [8]float32, b [8]float32, imm8 int) [8]float32


// MaskShufflePs2: Shuffle single-precision (32-bit) floating-point elements in
// 'a' within 128-bit lanes using the control in 'imm8', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(b[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(b[127:0], imm8[7:6])
//		tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		tmp_dst[223:192] := SELECT4(b[255:128], imm8[5:4])
//		tmp_dst[255:224] := SELECT4(b[255:128], imm8[7:6])
//		tmp_dst[287:256] := SELECT4(a[383:256], imm8[1:0])
//		tmp_dst[319:288] := SELECT4(a[383:256], imm8[3:2])
//		tmp_dst[351:320] := SELECT4(b[383:256], imm8[5:4])
//		tmp_dst[383:352] := SELECT4(b[383:256], imm8[7:6])
//		tmp_dst[415:384] := SELECT4(a[511:384], imm8[1:0])
//		tmp_dst[447:416] := SELECT4(a[511:384], imm8[3:2])
//		tmp_dst[479:448] := SELECT4(b[511:384], imm8[5:4])
//		tmp_dst[511:480] := SELECT4(b[511:384], imm8[7:6])
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFPS'. Intrinsic: '_mm512_mask_shuffle_ps'.
// Requires AVX512F.
func MaskShufflePs2(src M512, k Mmask16, a M512, b M512, imm8 int) M512 {
	return M512(maskShufflePs2([16]float32(src), uint16(k), [16]float32(a), [16]float32(b), imm8))
}

func maskShufflePs2(src [16]float32, k uint16, a [16]float32, b [16]float32, imm8 int) [16]float32


// MaskzShufflePs2: Shuffle single-precision (32-bit) floating-point elements
// in 'a' within 128-bit lanes using the control in 'imm8', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(b[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(b[127:0], imm8[7:6])
//		tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		tmp_dst[223:192] := SELECT4(b[255:128], imm8[5:4])
//		tmp_dst[255:224] := SELECT4(b[255:128], imm8[7:6])
//		tmp_dst[287:256] := SELECT4(a[383:256], imm8[1:0])
//		tmp_dst[319:288] := SELECT4(a[383:256], imm8[3:2])
//		tmp_dst[351:320] := SELECT4(b[383:256], imm8[5:4])
//		tmp_dst[383:352] := SELECT4(b[383:256], imm8[7:6])
//		tmp_dst[415:384] := SELECT4(a[511:384], imm8[1:0])
//		tmp_dst[447:416] := SELECT4(a[511:384], imm8[3:2])
//		tmp_dst[479:448] := SELECT4(b[511:384], imm8[5:4])
//		tmp_dst[511:480] := SELECT4(b[511:384], imm8[7:6])
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFPS'. Intrinsic: '_mm512_maskz_shuffle_ps'.
// Requires AVX512F.
func MaskzShufflePs2(k Mmask16, a M512, b M512, imm8 int) M512 {
	return M512(maskzShufflePs2(uint16(k), [16]float32(a), [16]float32(b), imm8))
}

func maskzShufflePs2(k uint16, a [16]float32, b [16]float32, imm8 int) [16]float32


// ShufflePs: Shuffle single-precision (32-bit) floating-point elements in 'a'
// within 128-bit lanes using the control in 'imm8', and store the results in
// 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		dst[95:64] := SELECT4(b[127:0], imm8[5:4])
//		dst[127:96] := SELECT4(b[127:0], imm8[7:6])
//		dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		dst[223:192] := SELECT4(b[255:128], imm8[5:4])
//		dst[255:224] := SELECT4(b[255:128], imm8[7:6])
//		dst[287:256] := SELECT4(a[383:256], imm8[1:0])
//		dst[319:288] := SELECT4(a[383:256], imm8[3:2])
//		dst[351:320] := SELECT4(b[383:256], imm8[5:4])
//		dst[383:352] := SELECT4(b[383:256], imm8[7:6])
//		dst[415:384] := SELECT4(a[511:384], imm8[1:0])
//		dst[447:416] := SELECT4(a[511:384], imm8[3:2])
//		dst[479:448] := SELECT4(b[511:384], imm8[5:4])
//		dst[511:480] := SELECT4(b[511:384], imm8[7:6])
//		dst[MAX:512] := 0
//
// Instruction: 'VSHUFPS'. Intrinsic: '_mm512_shuffle_ps'.
// Requires AVX512F.
func ShufflePs(a M512, b M512, imm8 int) M512 {
	return M512(shufflePs([16]float32(a), [16]float32(b), imm8))
}

func shufflePs(a [16]float32, b [16]float32, imm8 int) [16]float32


// MaskSinPd: Compute the sine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SIN(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_sin_pd'.
// Requires AVX512F.
func MaskSinPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskSinPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskSinPd(src [8]float64, k uint8, a [8]float64) [8]float64


// SinPd: Compute the sine of packed double-precision (64-bit) floating-point
// elements in 'a' expressed in radians, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := SIN(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_sin_pd'.
// Requires AVX512F.
func SinPd(a M512d) M512d {
	return M512d(sinPd([8]float64(a)))
}

func sinPd(a [8]float64) [8]float64


// MaskSinPs: Compute the sine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SIN(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_sin_ps'.
// Requires AVX512F.
func MaskSinPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskSinPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskSinPs(src [16]float32, k uint16, a [16]float32) [16]float32


// SinPs: Compute the sine of packed single-precision (32-bit) floating-point
// elements in 'a' expressed in radians, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := SIN(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_sin_ps'.
// Requires AVX512F.
func SinPs(a M512) M512 {
	return M512(sinPs([16]float32(a)))
}

func sinPs(a [16]float32) [16]float32


// MaskSincosPd: Computes the sine and cosine of the packed double-precision
// (64-bit) floating-point elements in 'a' and stores the results of the sine
// computation in 'dst' and the results of the cosine computation in 'cos_res'.
// Elements are written to their respective locations using writemask 'k'
// (elements are copied from 'sin_src' or 'cos_src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SIN(a[i+63:i])
//				cos_res[i+63:i] := COS(a[i+63:i])
//			ELSE
//				dst[i+63:i] := sin_src[i+63:i]
//				cos_res[i+63:i] := cos_src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//		cos_res[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_sincos_pd'.
// Requires AVX512F.
func MaskSincosPd(cos_res M512d, sin_src M512d, cos_src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskSincosPd([8]float64(cos_res), [8]float64(sin_src), [8]float64(cos_src), uint8(k), [8]float64(a)))
}

func maskSincosPd(cos_res [8]float64, sin_src [8]float64, cos_src [8]float64, k uint8, a [8]float64) [8]float64


// SincosPd: Computes the sine and cosine of the packed double-precision
// (64-bit) floating-point elements in 'a' and stores the results of the sine
// computation in 'dst' and the results of the cosine computation in 'cos_res'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := SIN(a[i+63:i])
//			cos_res[i+63:i] := COS(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//		cos_res[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_sincos_pd'.
// Requires AVX512F.
func SincosPd(cos_res M512d, a M512d) M512d {
	return M512d(sincosPd([8]float64(cos_res), [8]float64(a)))
}

func sincosPd(cos_res [8]float64, a [8]float64) [8]float64


// MaskSincosPs: Computes the sine and cosine of the packed single-precision
// (32-bit) floating-point elements in 'a' and stores the results of the sine
// computation in 'dst' and the results of the cosine computation in 'cos_res'.
// Elements are written to their respective locations using writemask 'k'
// (elements are copied from 'sin_src' or 'cos_src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SIN(a[i+31:i])
//				cos_res[i+31:i] := COS(a[i+31:i])
//			ELSE
//				dst[i+31:i] := sin_src[i+31:i]
//				cos_res[i+31:i] := cos_src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//		cos_res[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_sincos_ps'.
// Requires AVX512F.
func MaskSincosPs(cos_res M512, sin_src M512, cos_src M512, k Mmask16, a M512) M512 {
	return M512(maskSincosPs([16]float32(cos_res), [16]float32(sin_src), [16]float32(cos_src), uint16(k), [16]float32(a)))
}

func maskSincosPs(cos_res [16]float32, sin_src [16]float32, cos_src [16]float32, k uint16, a [16]float32) [16]float32


// SincosPs: Computes the sine and cosine of the packed single-precision
// (32-bit) floating-point elements in 'a' and stores the results of the sine
// computation in 'dst' and the results of the cosine computation in 'cos_res'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := SIN(a[i+31:i])
//			cos_res[i+31:i] := COS(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//		cos_res[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_sincos_ps'.
// Requires AVX512F.
func SincosPs(cos_res M512, a M512) M512 {
	return M512(sincosPs([16]float32(cos_res), [16]float32(a)))
}

func sincosPs(cos_res [16]float32, a [16]float32) [16]float32


// MaskSindPd: Compute the sine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in degrees, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SIND(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_sind_pd'.
// Requires AVX512F.
func MaskSindPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskSindPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskSindPd(src [8]float64, k uint8, a [8]float64) [8]float64


// SindPd: Compute the sine of packed double-precision (64-bit) floating-point
// elements in 'a' expressed in degrees, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := SIND(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_sind_pd'.
// Requires AVX512F.
func SindPd(a M512d) M512d {
	return M512d(sindPd([8]float64(a)))
}

func sindPd(a [8]float64) [8]float64


// MaskSindPs: Compute the sine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in degrees, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SIND(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_sind_ps'.
// Requires AVX512F.
func MaskSindPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskSindPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskSindPs(src [16]float32, k uint16, a [16]float32) [16]float32


// SindPs: Compute the sine of packed single-precision (32-bit) floating-point
// elements in 'a' expressed in degrees, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := SIND(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_sind_ps'.
// Requires AVX512F.
func SindPs(a M512) M512 {
	return M512(sindPs([16]float32(a)))
}

func sindPs(a [16]float32) [16]float32


// MaskSinhPd: Compute the hyperbolic sine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SINH(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_sinh_pd'.
// Requires AVX512F.
func MaskSinhPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskSinhPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskSinhPd(src [8]float64, k uint8, a [8]float64) [8]float64


// SinhPd: Compute the hyperbolic sine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := SINH(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_sinh_pd'.
// Requires AVX512F.
func SinhPd(a M512d) M512d {
	return M512d(sinhPd([8]float64(a)))
}

func sinhPd(a [8]float64) [8]float64


// MaskSinhPs: Compute the hyperbolic sine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SINH(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_sinh_ps'.
// Requires AVX512F.
func MaskSinhPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskSinhPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskSinhPs(src [16]float32, k uint16, a [16]float32) [16]float32


// SinhPs: Compute the hyperbolic sine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := SINH(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_sinh_ps'.
// Requires AVX512F.
func SinhPs(a M512) M512 {
	return M512(sinhPs([16]float32(a)))
}

func sinhPs(a [16]float32) [16]float32


// MaskSllEpi32: Shift packed 32-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSLLD'. Intrinsic: '_mm_mask_sll_epi32'.
// Requires AVX512F.
func MaskSllEpi32(src M128i, k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskSllEpi32([16]byte(src), uint8(k), [16]byte(a), [16]byte(count)))
}

func maskSllEpi32(src [16]byte, k uint8, a [16]byte, count [16]byte) [16]byte


// MaskzSllEpi32: Shift packed 32-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSLLD'. Intrinsic: '_mm_maskz_sll_epi32'.
// Requires AVX512F.
func MaskzSllEpi32(k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskzSllEpi32(uint8(k), [16]byte(a), [16]byte(count)))
}

func maskzSllEpi32(k uint8, a [16]byte, count [16]byte) [16]byte


// MaskSllEpi321: Shift packed 32-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLD'. Intrinsic: '_mm256_mask_sll_epi32'.
// Requires AVX512F.
func MaskSllEpi321(src M256i, k Mmask8, a M256i, count M128i) M256i {
	return M256i(maskSllEpi321([32]byte(src), uint8(k), [32]byte(a), [16]byte(count)))
}

func maskSllEpi321(src [32]byte, k uint8, a [32]byte, count [16]byte) [32]byte


// MaskzSllEpi321: Shift packed 32-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLD'. Intrinsic: '_mm256_maskz_sll_epi32'.
// Requires AVX512F.
func MaskzSllEpi321(k Mmask8, a M256i, count M128i) M256i {
	return M256i(maskzSllEpi321(uint8(k), [32]byte(a), [16]byte(count)))
}

func maskzSllEpi321(k uint8, a [32]byte, count [16]byte) [32]byte


// MaskSllEpi322: Shift packed 32-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLD'. Intrinsic: '_mm512_mask_sll_epi32'.
// Requires AVX512F.
func MaskSllEpi322(src M512i, k Mmask16, a M512i, count M128i) M512i {
	return M512i(maskSllEpi322([64]byte(src), uint16(k), [64]byte(a), [16]byte(count)))
}

func maskSllEpi322(src [64]byte, k uint16, a [64]byte, count [16]byte) [64]byte


// MaskzSllEpi322: Shift packed 32-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLD'. Intrinsic: '_mm512_maskz_sll_epi32'.
// Requires AVX512F.
func MaskzSllEpi322(k Mmask16, a M512i, count M128i) M512i {
	return M512i(maskzSllEpi322(uint16(k), [64]byte(a), [16]byte(count)))
}

func maskzSllEpi322(k uint16, a [64]byte, count [16]byte) [64]byte


// SllEpi32: Shift packed 32-bit integers in 'a' left by 'count' while shifting
// in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF count[63:0] > 31
//				dst[i+31:i] := 0
//			ELSE
//				dst[i+31:i] := ZeroExtend(a[i+31:i] << count[63:0])
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLD'. Intrinsic: '_mm512_sll_epi32'.
// Requires AVX512F.
func SllEpi32(a M512i, count M128i) M512i {
	return M512i(sllEpi32([64]byte(a), [16]byte(count)))
}

func sllEpi32(a [64]byte, count [16]byte) [64]byte


// MaskSllEpi64: Shift packed 64-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm_mask_sll_epi64'.
// Requires AVX512F.
func MaskSllEpi64(src M128i, k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskSllEpi64([16]byte(src), uint8(k), [16]byte(a), [16]byte(count)))
}

func maskSllEpi64(src [16]byte, k uint8, a [16]byte, count [16]byte) [16]byte


// MaskzSllEpi64: Shift packed 64-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm_maskz_sll_epi64'.
// Requires AVX512F.
func MaskzSllEpi64(k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskzSllEpi64(uint8(k), [16]byte(a), [16]byte(count)))
}

func maskzSllEpi64(k uint8, a [16]byte, count [16]byte) [16]byte


// MaskSllEpi641: Shift packed 64-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm256_mask_sll_epi64'.
// Requires AVX512F.
func MaskSllEpi641(src M256i, k Mmask8, a M256i, count M128i) M256i {
	return M256i(maskSllEpi641([32]byte(src), uint8(k), [32]byte(a), [16]byte(count)))
}

func maskSllEpi641(src [32]byte, k uint8, a [32]byte, count [16]byte) [32]byte


// MaskzSllEpi641: Shift packed 64-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm256_maskz_sll_epi64'.
// Requires AVX512F.
func MaskzSllEpi641(k Mmask8, a M256i, count M128i) M256i {
	return M256i(maskzSllEpi641(uint8(k), [32]byte(a), [16]byte(count)))
}

func maskzSllEpi641(k uint8, a [32]byte, count [16]byte) [32]byte


// MaskSllEpi642: Shift packed 64-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm512_mask_sll_epi64'.
// Requires AVX512F.
func MaskSllEpi642(src M512i, k Mmask8, a M512i, count M128i) M512i {
	return M512i(maskSllEpi642([64]byte(src), uint8(k), [64]byte(a), [16]byte(count)))
}

func maskSllEpi642(src [64]byte, k uint8, a [64]byte, count [16]byte) [64]byte


// MaskzSllEpi642: Shift packed 64-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm512_maskz_sll_epi64'.
// Requires AVX512F.
func MaskzSllEpi642(k Mmask8, a M512i, count M128i) M512i {
	return M512i(maskzSllEpi642(uint8(k), [64]byte(a), [16]byte(count)))
}

func maskzSllEpi642(k uint8, a [64]byte, count [16]byte) [64]byte


// SllEpi64: Shift packed 64-bit integers in 'a' left by 'count' while shifting
// in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF count[63:0] > 63
//				dst[i+63:i] := 0
//			ELSE
//				dst[i+63:i] := ZeroExtend(a[i+63:i] << count[63:0])
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm512_sll_epi64'.
// Requires AVX512F.
func SllEpi64(a M512i, count M128i) M512i {
	return M512i(sllEpi64([64]byte(a), [16]byte(count)))
}

func sllEpi64(a [64]byte, count [16]byte) [64]byte


// MaskSlliEpi32: Shift packed 32-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSLLD'. Intrinsic: '_mm_mask_slli_epi32'.
// Requires AVX512F.
func MaskSlliEpi32(src M128i, k Mmask8, a M128i, imm8 uint32) M128i {
	return M128i(maskSlliEpi32([16]byte(src), uint8(k), [16]byte(a), imm8))
}

func maskSlliEpi32(src [16]byte, k uint8, a [16]byte, imm8 uint32) [16]byte


// MaskzSlliEpi32: Shift packed 32-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSLLD'. Intrinsic: '_mm_maskz_slli_epi32'.
// Requires AVX512F.
func MaskzSlliEpi32(k Mmask8, a M128i, imm8 uint32) M128i {
	return M128i(maskzSlliEpi32(uint8(k), [16]byte(a), imm8))
}

func maskzSlliEpi32(k uint8, a [16]byte, imm8 uint32) [16]byte


// MaskSlliEpi321: Shift packed 32-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLD'. Intrinsic: '_mm256_mask_slli_epi32'.
// Requires AVX512F.
func MaskSlliEpi321(src M256i, k Mmask8, a M256i, imm8 uint32) M256i {
	return M256i(maskSlliEpi321([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func maskSlliEpi321(src [32]byte, k uint8, a [32]byte, imm8 uint32) [32]byte


// MaskzSlliEpi321: Shift packed 32-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLD'. Intrinsic: '_mm256_maskz_slli_epi32'.
// Requires AVX512F.
func MaskzSlliEpi321(k Mmask8, a M256i, imm8 uint32) M256i {
	return M256i(maskzSlliEpi321(uint8(k), [32]byte(a), imm8))
}

func maskzSlliEpi321(k uint8, a [32]byte, imm8 uint32) [32]byte


// MaskzSlliEpi322: Shift packed 32-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLD'. Intrinsic: '_mm512_maskz_slli_epi32'.
// Requires AVX512F.
func MaskzSlliEpi322(k Mmask16, a M512i, imm8 uint32) M512i {
	return M512i(maskzSlliEpi322(uint16(k), [64]byte(a), imm8))
}

func maskzSlliEpi322(k uint16, a [64]byte, imm8 uint32) [64]byte


// MaskSlliEpi64: Shift packed 64-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm_mask_slli_epi64'.
// Requires AVX512F.
func MaskSlliEpi64(src M128i, k Mmask8, a M128i, imm8 uint32) M128i {
	return M128i(maskSlliEpi64([16]byte(src), uint8(k), [16]byte(a), imm8))
}

func maskSlliEpi64(src [16]byte, k uint8, a [16]byte, imm8 uint32) [16]byte


// MaskzSlliEpi64: Shift packed 64-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm_maskz_slli_epi64'.
// Requires AVX512F.
func MaskzSlliEpi64(k Mmask8, a M128i, imm8 uint32) M128i {
	return M128i(maskzSlliEpi64(uint8(k), [16]byte(a), imm8))
}

func maskzSlliEpi64(k uint8, a [16]byte, imm8 uint32) [16]byte


// MaskSlliEpi641: Shift packed 64-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm256_mask_slli_epi64'.
// Requires AVX512F.
func MaskSlliEpi641(src M256i, k Mmask8, a M256i, imm8 uint32) M256i {
	return M256i(maskSlliEpi641([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func maskSlliEpi641(src [32]byte, k uint8, a [32]byte, imm8 uint32) [32]byte


// MaskzSlliEpi641: Shift packed 64-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm256_maskz_slli_epi64'.
// Requires AVX512F.
func MaskzSlliEpi641(k Mmask8, a M256i, imm8 uint32) M256i {
	return M256i(maskzSlliEpi641(uint8(k), [32]byte(a), imm8))
}

func maskzSlliEpi641(k uint8, a [32]byte, imm8 uint32) [32]byte


// MaskSlliEpi642: Shift packed 64-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm512_mask_slli_epi64'.
// Requires AVX512F.
func MaskSlliEpi642(src M512i, k Mmask8, a M512i, imm8 uint32) M512i {
	return M512i(maskSlliEpi642([64]byte(src), uint8(k), [64]byte(a), imm8))
}

func maskSlliEpi642(src [64]byte, k uint8, a [64]byte, imm8 uint32) [64]byte


// MaskzSlliEpi642: Shift packed 64-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm512_maskz_slli_epi64'.
// Requires AVX512F.
func MaskzSlliEpi642(k Mmask8, a M512i, imm8 uint32) M512i {
	return M512i(maskzSlliEpi642(uint8(k), [64]byte(a), imm8))
}

func maskzSlliEpi642(k uint8, a [64]byte, imm8 uint32) [64]byte


// SlliEpi64: Shift packed 64-bit integers in 'a' left by 'imm8' while shifting
// in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF imm8[7:0] > 63
//				dst[i+63:i] := 0
//			ELSE
//				dst[i+63:i] := ZeroExtend(a[i+63:i] << imm8[7:0])
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm512_slli_epi64'.
// Requires AVX512F.
func SlliEpi64(a M512i, imm8 uint32) M512i {
	return M512i(slliEpi64([64]byte(a), imm8))
}

func slliEpi64(a [64]byte, imm8 uint32) [64]byte


// MaskSllvEpi32: Shift packed 32-bit integers in 'a' left by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[i+31:i] << count[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSLLVD'. Intrinsic: '_mm_mask_sllv_epi32'.
// Requires AVX512F.
func MaskSllvEpi32(src M128i, k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskSllvEpi32([16]byte(src), uint8(k), [16]byte(a), [16]byte(count)))
}

func maskSllvEpi32(src [16]byte, k uint8, a [16]byte, count [16]byte) [16]byte


// MaskzSllvEpi32: Shift packed 32-bit integers in 'a' left by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[i+31:i] << count[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSLLVD'. Intrinsic: '_mm_maskz_sllv_epi32'.
// Requires AVX512F.
func MaskzSllvEpi32(k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskzSllvEpi32(uint8(k), [16]byte(a), [16]byte(count)))
}

func maskzSllvEpi32(k uint8, a [16]byte, count [16]byte) [16]byte


// MaskSllvEpi321: Shift packed 32-bit integers in 'a' left by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[i+31:i] << count[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLVD'. Intrinsic: '_mm256_mask_sllv_epi32'.
// Requires AVX512F.
func MaskSllvEpi321(src M256i, k Mmask8, a M256i, count M256i) M256i {
	return M256i(maskSllvEpi321([32]byte(src), uint8(k), [32]byte(a), [32]byte(count)))
}

func maskSllvEpi321(src [32]byte, k uint8, a [32]byte, count [32]byte) [32]byte


// MaskzSllvEpi321: Shift packed 32-bit integers in 'a' left by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[i+31:i] << count[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLVD'. Intrinsic: '_mm256_maskz_sllv_epi32'.
// Requires AVX512F.
func MaskzSllvEpi321(k Mmask8, a M256i, count M256i) M256i {
	return M256i(maskzSllvEpi321(uint8(k), [32]byte(a), [32]byte(count)))
}

func maskzSllvEpi321(k uint8, a [32]byte, count [32]byte) [32]byte


// MaskzSllvEpi322: Shift packed 32-bit integers in 'a' left by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[i+31:i] << count[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLVD'. Intrinsic: '_mm512_maskz_sllv_epi32'.
// Requires AVX512F.
func MaskzSllvEpi322(k Mmask16, a M512i, count M512i) M512i {
	return M512i(maskzSllvEpi322(uint16(k), [64]byte(a), [64]byte(count)))
}

func maskzSllvEpi322(k uint16, a [64]byte, count [64]byte) [64]byte


// MaskSllvEpi64: Shift packed 64-bit integers in 'a' left by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[i+63:i] << count[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSLLVQ'. Intrinsic: '_mm_mask_sllv_epi64'.
// Requires AVX512F.
func MaskSllvEpi64(src M128i, k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskSllvEpi64([16]byte(src), uint8(k), [16]byte(a), [16]byte(count)))
}

func maskSllvEpi64(src [16]byte, k uint8, a [16]byte, count [16]byte) [16]byte


// MaskzSllvEpi64: Shift packed 64-bit integers in 'a' left by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[i+63:i] << count[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSLLVQ'. Intrinsic: '_mm_maskz_sllv_epi64'.
// Requires AVX512F.
func MaskzSllvEpi64(k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskzSllvEpi64(uint8(k), [16]byte(a), [16]byte(count)))
}

func maskzSllvEpi64(k uint8, a [16]byte, count [16]byte) [16]byte


// MaskSllvEpi641: Shift packed 64-bit integers in 'a' left by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[i+63:i] << count[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLVQ'. Intrinsic: '_mm256_mask_sllv_epi64'.
// Requires AVX512F.
func MaskSllvEpi641(src M256i, k Mmask8, a M256i, count M256i) M256i {
	return M256i(maskSllvEpi641([32]byte(src), uint8(k), [32]byte(a), [32]byte(count)))
}

func maskSllvEpi641(src [32]byte, k uint8, a [32]byte, count [32]byte) [32]byte


// MaskzSllvEpi641: Shift packed 64-bit integers in 'a' left by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[i+63:i] << count[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLVQ'. Intrinsic: '_mm256_maskz_sllv_epi64'.
// Requires AVX512F.
func MaskzSllvEpi641(k Mmask8, a M256i, count M256i) M256i {
	return M256i(maskzSllvEpi641(uint8(k), [32]byte(a), [32]byte(count)))
}

func maskzSllvEpi641(k uint8, a [32]byte, count [32]byte) [32]byte


// MaskSllvEpi642: Shift packed 64-bit integers in 'a' left by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[i+63:i] << count[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLVQ'. Intrinsic: '_mm512_mask_sllv_epi64'.
// Requires AVX512F.
func MaskSllvEpi642(src M512i, k Mmask8, a M512i, count M512i) M512i {
	return M512i(maskSllvEpi642([64]byte(src), uint8(k), [64]byte(a), [64]byte(count)))
}

func maskSllvEpi642(src [64]byte, k uint8, a [64]byte, count [64]byte) [64]byte


// MaskzSllvEpi642: Shift packed 64-bit integers in 'a' left by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[i+63:i] << count[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLVQ'. Intrinsic: '_mm512_maskz_sllv_epi64'.
// Requires AVX512F.
func MaskzSllvEpi642(k Mmask8, a M512i, count M512i) M512i {
	return M512i(maskzSllvEpi642(uint8(k), [64]byte(a), [64]byte(count)))
}

func maskzSllvEpi642(k uint8, a [64]byte, count [64]byte) [64]byte


// SllvEpi64: Shift packed 64-bit integers in 'a' left by the amount specified
// by the corresponding element in 'count' while shifting in zeros, and store
// the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ZeroExtend(a[i+63:i] << count[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSLLVQ'. Intrinsic: '_mm512_sllv_epi64'.
// Requires AVX512F.
func SllvEpi64(a M512i, count M512i) M512i {
	return M512i(sllvEpi64([64]byte(a), [64]byte(count)))
}

func sllvEpi64(a [64]byte, count [64]byte) [64]byte


// MaskSqrtPd: Compute the square root of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SQRT(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSQRTPD'. Intrinsic: '_mm_mask_sqrt_pd'.
// Requires AVX512F.
func MaskSqrtPd(src M128d, k Mmask8, a M128d) M128d {
	return M128d(maskSqrtPd([2]float64(src), uint8(k), [2]float64(a)))
}

func maskSqrtPd(src [2]float64, k uint8, a [2]float64) [2]float64


// MaskzSqrtPd: Compute the square root of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SQRT(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSQRTPD'. Intrinsic: '_mm_maskz_sqrt_pd'.
// Requires AVX512F.
func MaskzSqrtPd(k Mmask8, a M128d) M128d {
	return M128d(maskzSqrtPd(uint8(k), [2]float64(a)))
}

func maskzSqrtPd(k uint8, a [2]float64) [2]float64


// MaskSqrtPd1: Compute the square root of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SQRT(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSQRTPD'. Intrinsic: '_mm256_mask_sqrt_pd'.
// Requires AVX512F.
func MaskSqrtPd1(src M256d, k Mmask8, a M256d) M256d {
	return M256d(maskSqrtPd1([4]float64(src), uint8(k), [4]float64(a)))
}

func maskSqrtPd1(src [4]float64, k uint8, a [4]float64) [4]float64


// MaskzSqrtPd1: Compute the square root of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SQRT(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSQRTPD'. Intrinsic: '_mm256_maskz_sqrt_pd'.
// Requires AVX512F.
func MaskzSqrtPd1(k Mmask8, a M256d) M256d {
	return M256d(maskzSqrtPd1(uint8(k), [4]float64(a)))
}

func maskzSqrtPd1(k uint8, a [4]float64) [4]float64


// MaskSqrtPd2: Compute the square root of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SQRT(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSQRTPD'. Intrinsic: '_mm512_mask_sqrt_pd'.
// Requires AVX512F.
func MaskSqrtPd2(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskSqrtPd2([8]float64(src), uint8(k), [8]float64(a)))
}

func maskSqrtPd2(src [8]float64, k uint8, a [8]float64) [8]float64


// MaskzSqrtPd2: Compute the square root of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SQRT(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSQRTPD'. Intrinsic: '_mm512_maskz_sqrt_pd'.
// Requires AVX512F.
func MaskzSqrtPd2(k Mmask8, a M512d) M512d {
	return M512d(maskzSqrtPd2(uint8(k), [8]float64(a)))
}

func maskzSqrtPd2(k uint8, a [8]float64) [8]float64


// SqrtPd: Compute the square root of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := SQRT(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSQRTPD'. Intrinsic: '_mm512_sqrt_pd'.
// Requires AVX512F.
func SqrtPd(a M512d) M512d {
	return M512d(sqrtPd([8]float64(a)))
}

func sqrtPd(a [8]float64) [8]float64


// MaskSqrtPs: Compute the square root of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SQRT(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSQRTPS'. Intrinsic: '_mm_mask_sqrt_ps'.
// Requires AVX512F.
func MaskSqrtPs(src M128, k Mmask8, a M128) M128 {
	return M128(maskSqrtPs([4]float32(src), uint8(k), [4]float32(a)))
}

func maskSqrtPs(src [4]float32, k uint8, a [4]float32) [4]float32


// MaskzSqrtPs: Compute the square root of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SQRT(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSQRTPS'. Intrinsic: '_mm_maskz_sqrt_ps'.
// Requires AVX512F.
func MaskzSqrtPs(k Mmask8, a M128) M128 {
	return M128(maskzSqrtPs(uint8(k), [4]float32(a)))
}

func maskzSqrtPs(k uint8, a [4]float32) [4]float32


// MaskSqrtPs1: Compute the square root of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SQRT(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSQRTPS'. Intrinsic: '_mm256_mask_sqrt_ps'.
// Requires AVX512F.
func MaskSqrtPs1(src M256, k Mmask8, a M256) M256 {
	return M256(maskSqrtPs1([8]float32(src), uint8(k), [8]float32(a)))
}

func maskSqrtPs1(src [8]float32, k uint8, a [8]float32) [8]float32


// MaskzSqrtPs1: Compute the square root of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SQRT(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSQRTPS'. Intrinsic: '_mm256_maskz_sqrt_ps'.
// Requires AVX512F.
func MaskzSqrtPs1(k Mmask8, a M256) M256 {
	return M256(maskzSqrtPs1(uint8(k), [8]float32(a)))
}

func maskzSqrtPs1(k uint8, a [8]float32) [8]float32


// MaskSqrtPs2: Compute the square root of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SQRT(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSQRTPS'. Intrinsic: '_mm512_mask_sqrt_ps'.
// Requires AVX512F.
func MaskSqrtPs2(src M512, k Mmask16, a M512) M512 {
	return M512(maskSqrtPs2([16]float32(src), uint16(k), [16]float32(a)))
}

func maskSqrtPs2(src [16]float32, k uint16, a [16]float32) [16]float32


// MaskzSqrtPs2: Compute the square root of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SQRT(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSQRTPS'. Intrinsic: '_mm512_maskz_sqrt_ps'.
// Requires AVX512F.
func MaskzSqrtPs2(k Mmask16, a M512) M512 {
	return M512(maskzSqrtPs2(uint16(k), [16]float32(a)))
}

func maskzSqrtPs2(k uint16, a [16]float32) [16]float32


// SqrtPs: Compute the square root of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := SQRT(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSQRTPS'. Intrinsic: '_mm512_sqrt_ps'.
// Requires AVX512F.
func SqrtPs(a M512) M512 {
	return M512(sqrtPs([16]float32(a)))
}

func sqrtPs(a [16]float32) [16]float32


// MaskSqrtRoundPd: Compute the square root of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SQRT(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSQRTPD'. Intrinsic: '_mm512_mask_sqrt_round_pd'.
// Requires AVX512F.
func MaskSqrtRoundPd(src M512d, k Mmask8, a M512d, rounding int) M512d {
	return M512d(maskSqrtRoundPd([8]float64(src), uint8(k), [8]float64(a), rounding))
}

func maskSqrtRoundPd(src [8]float64, k uint8, a [8]float64, rounding int) [8]float64


// MaskzSqrtRoundPd: Compute the square root of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SQRT(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSQRTPD'. Intrinsic: '_mm512_maskz_sqrt_round_pd'.
// Requires AVX512F.
func MaskzSqrtRoundPd(k Mmask8, a M512d, rounding int) M512d {
	return M512d(maskzSqrtRoundPd(uint8(k), [8]float64(a), rounding))
}

func maskzSqrtRoundPd(k uint8, a [8]float64, rounding int) [8]float64


// SqrtRoundPd: Compute the square root of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := SQRT(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSQRTPD'. Intrinsic: '_mm512_sqrt_round_pd'.
// Requires AVX512F.
func SqrtRoundPd(a M512d, rounding int) M512d {
	return M512d(sqrtRoundPd([8]float64(a), rounding))
}

func sqrtRoundPd(a [8]float64, rounding int) [8]float64


// MaskSqrtRoundPs: Compute the square root of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SQRT(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSQRTPS'. Intrinsic: '_mm512_mask_sqrt_round_ps'.
// Requires AVX512F.
func MaskSqrtRoundPs(src M512, k Mmask16, a M512, rounding int) M512 {
	return M512(maskSqrtRoundPs([16]float32(src), uint16(k), [16]float32(a), rounding))
}

func maskSqrtRoundPs(src [16]float32, k uint16, a [16]float32, rounding int) [16]float32


// MaskzSqrtRoundPs: Compute the square root of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SQRT(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSQRTPS'. Intrinsic: '_mm512_maskz_sqrt_round_ps'.
// Requires AVX512F.
func MaskzSqrtRoundPs(k Mmask16, a M512, rounding int) M512 {
	return M512(maskzSqrtRoundPs(uint16(k), [16]float32(a), rounding))
}

func maskzSqrtRoundPs(k uint16, a [16]float32, rounding int) [16]float32


// SqrtRoundPs: Compute the square root of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := SQRT(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSQRTPS'. Intrinsic: '_mm512_sqrt_round_ps'.
// Requires AVX512F.
func SqrtRoundPs(a M512, rounding int) M512 {
	return M512(sqrtRoundPs([16]float32(a), rounding))
}

func sqrtRoundPs(a [16]float32, rounding int) [16]float32


// MaskSqrtRoundSd: Compute the square root of the lower double-precision
// (64-bit) floating-point element in 'a', store the result in the lower
// element of 'dst' using writemask 'k' (the element is copied from 'src' when
// mask bit 0 is not set), and copy the upper element from 'b' to the upper
// element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := SQRT(a[63:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSQRTSD'. Intrinsic: '_mm_mask_sqrt_round_sd'.
// Requires AVX512F.
func MaskSqrtRoundSd(src M128d, k Mmask8, a M128d, b M128d, rounding int) M128d {
	return M128d(maskSqrtRoundSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), rounding))
}

func maskSqrtRoundSd(src [2]float64, k uint8, a [2]float64, b [2]float64, rounding int) [2]float64


// MaskzSqrtRoundSd: Compute the square root of the lower double-precision
// (64-bit) floating-point element in 'a', store the result in the lower
// element of 'dst' using zeromask 'k' (the element is zeroed out when mask bit
// 0 is not set), and copy the upper element from 'b' to the upper element of
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := SQRT(a[63:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSQRTSD'. Intrinsic: '_mm_maskz_sqrt_round_sd'.
// Requires AVX512F.
func MaskzSqrtRoundSd(k Mmask8, a M128d, b M128d, rounding int) M128d {
	return M128d(maskzSqrtRoundSd(uint8(k), [2]float64(a), [2]float64(b), rounding))
}

func maskzSqrtRoundSd(k uint8, a [2]float64, b [2]float64, rounding int) [2]float64


// SqrtRoundSd: Compute the square root of the lower double-precision (64-bit)
// floating-point element in 'a', store the result in the lower element of
// 'dst', and copy the upper element from 'b' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := SQRT(a[63:0])
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSQRTSD'. Intrinsic: '_mm_sqrt_round_sd'.
// Requires AVX512F.
func SqrtRoundSd(a M128d, b M128d, rounding int) M128d {
	return M128d(sqrtRoundSd([2]float64(a), [2]float64(b), rounding))
}

func sqrtRoundSd(a [2]float64, b [2]float64, rounding int) [2]float64


// MaskSqrtRoundSs: Compute the square root of the lower single-precision
// (32-bit) floating-point element in 'a', store the result in the lower
// element of 'dst' using writemask 'k' (the element is copied from 'src' when
// mask bit 0 is not set), and copy the upper 3 packed elements from 'b' to the
// upper elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := SQRT(a[31:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSQRTSS'. Intrinsic: '_mm_mask_sqrt_round_ss'.
// Requires AVX512F.
func MaskSqrtRoundSs(src M128, k Mmask8, a M128, b M128, rounding int) M128 {
	return M128(maskSqrtRoundSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), rounding))
}

func maskSqrtRoundSs(src [4]float32, k uint8, a [4]float32, b [4]float32, rounding int) [4]float32


// MaskzSqrtRoundSs: Compute the square root of the lower single-precision
// (32-bit) floating-point element in 'a', store the result in the lower
// element of 'dst' using zeromask 'k' (the element is zeroed out when mask bit
// 0 is not set), and copy the upper 3 packed elements from 'b' to the upper
// elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := SQRT(a[31:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSQRTSS'. Intrinsic: '_mm_maskz_sqrt_round_ss'.
// Requires AVX512F.
func MaskzSqrtRoundSs(k Mmask8, a M128, b M128, rounding int) M128 {
	return M128(maskzSqrtRoundSs(uint8(k), [4]float32(a), [4]float32(b), rounding))
}

func maskzSqrtRoundSs(k uint8, a [4]float32, b [4]float32, rounding int) [4]float32


// SqrtRoundSs: Compute the square root of the lower single-precision (32-bit)
// floating-point element in 'a', store the result in the lower element of
// 'dst', and copy the upper 3 packed elements from 'b' to the upper elements
// of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := SQRT(a[31:0])
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSQRTSS'. Intrinsic: '_mm_sqrt_round_ss'.
// Requires AVX512F.
func SqrtRoundSs(a M128, b M128, rounding int) M128 {
	return M128(sqrtRoundSs([4]float32(a), [4]float32(b), rounding))
}

func sqrtRoundSs(a [4]float32, b [4]float32, rounding int) [4]float32


// MaskSqrtSd: Compute the square root of the lower double-precision (64-bit)
// floating-point element in 'a', store the result in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'src' when mask bit 0
// is not set), and copy the upper element from 'b' to the upper element of
// 'dst'. 
//
//		IF k[0]
//			dst[63:0] := SQRT(a[63:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSQRTSD'. Intrinsic: '_mm_mask_sqrt_sd'.
// Requires AVX512F.
func MaskSqrtSd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskSqrtSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskSqrtSd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzSqrtSd: Compute the square root of the lower double-precision (64-bit)
// floating-point element in 'a', store the result in the lower element of
// 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is not
// set), and copy the upper element from 'b' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := SQRT(a[63:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSQRTSD'. Intrinsic: '_mm_maskz_sqrt_sd'.
// Requires AVX512F.
func MaskzSqrtSd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzSqrtSd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzSqrtSd(k uint8, a [2]float64, b [2]float64) [2]float64


// MaskSqrtSs: Compute the square root of the lower single-precision (32-bit)
// floating-point element in 'a', store the result in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'src' when mask bit 0
// is not set), and copy the upper 3 packed elements from 'b' to the upper
// elements of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := SQRT(a[31:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSQRTSS'. Intrinsic: '_mm_mask_sqrt_ss'.
// Requires AVX512F.
func MaskSqrtSs(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskSqrtSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskSqrtSs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzSqrtSs: Compute the square root of the lower single-precision (32-bit)
// floating-point element in 'a', store the result in the lower element of
// 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is not
// set), and copy the upper 3 packed elements from 'b' to the upper elements of
// 'dst'. 
//
//		IF k[0]
//			dst[31:0] := SQRT(a[31:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSQRTSS'. Intrinsic: '_mm_maskz_sqrt_ss'.
// Requires AVX512F.
func MaskzSqrtSs(k Mmask8, a M128, b M128) M128 {
	return M128(maskzSqrtSs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzSqrtSs(k uint8, a [4]float32, b [4]float32) [4]float32


// MaskSraEpi32: Shift packed 32-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := SignBit
//				ELSE
//					dst[i+31:i] := SignExtend(a[i+31:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAD'. Intrinsic: '_mm_mask_sra_epi32'.
// Requires AVX512F.
func MaskSraEpi32(src M128i, k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskSraEpi32([16]byte(src), uint8(k), [16]byte(a), [16]byte(count)))
}

func maskSraEpi32(src [16]byte, k uint8, a [16]byte, count [16]byte) [16]byte


// MaskzSraEpi32: Shift packed 32-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := SignBit
//				ELSE
//					dst[i+31:i] := SignExtend(a[i+31:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAD'. Intrinsic: '_mm_maskz_sra_epi32'.
// Requires AVX512F.
func MaskzSraEpi32(k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskzSraEpi32(uint8(k), [16]byte(a), [16]byte(count)))
}

func maskzSraEpi32(k uint8, a [16]byte, count [16]byte) [16]byte


// MaskSraEpi321: Shift packed 32-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := SignBit
//				ELSE
//					dst[i+31:i] := SignExtend(a[i+31:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAD'. Intrinsic: '_mm256_mask_sra_epi32'.
// Requires AVX512F.
func MaskSraEpi321(src M256i, k Mmask8, a M256i, count M128i) M256i {
	return M256i(maskSraEpi321([32]byte(src), uint8(k), [32]byte(a), [16]byte(count)))
}

func maskSraEpi321(src [32]byte, k uint8, a [32]byte, count [16]byte) [32]byte


// MaskzSraEpi321: Shift packed 32-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := SignBit
//				ELSE
//					dst[i+31:i] := SignExtend(a[i+31:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAD'. Intrinsic: '_mm256_maskz_sra_epi32'.
// Requires AVX512F.
func MaskzSraEpi321(k Mmask8, a M256i, count M128i) M256i {
	return M256i(maskzSraEpi321(uint8(k), [32]byte(a), [16]byte(count)))
}

func maskzSraEpi321(k uint8, a [32]byte, count [16]byte) [32]byte


// MaskSraEpi322: Shift packed 32-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := SignBit
//				ELSE
//					dst[i+31:i] := SignExtend(a[i+31:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAD'. Intrinsic: '_mm512_mask_sra_epi32'.
// Requires AVX512F.
func MaskSraEpi322(src M512i, k Mmask16, a M512i, count M128i) M512i {
	return M512i(maskSraEpi322([64]byte(src), uint16(k), [64]byte(a), [16]byte(count)))
}

func maskSraEpi322(src [64]byte, k uint16, a [64]byte, count [16]byte) [64]byte


// MaskzSraEpi322: Shift packed 32-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := SignBit
//				ELSE
//					dst[i+31:i] := SignExtend(a[i+31:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAD'. Intrinsic: '_mm512_maskz_sra_epi32'.
// Requires AVX512F.
func MaskzSraEpi322(k Mmask16, a M512i, count M128i) M512i {
	return M512i(maskzSraEpi322(uint16(k), [64]byte(a), [16]byte(count)))
}

func maskzSraEpi322(k uint16, a [64]byte, count [16]byte) [64]byte


// SraEpi32: Shift packed 32-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF count[63:0] > 31
//				dst[i+31:i] := SignBit
//			ELSE
//				dst[i+31:i] := SignExtend(a[i+31:i] >> count[63:0])
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAD'. Intrinsic: '_mm512_sra_epi32'.
// Requires AVX512F.
func SraEpi32(a M512i, count M128i) M512i {
	return M512i(sraEpi32([64]byte(a), [16]byte(count)))
}

func sraEpi32(a [64]byte, count [16]byte) [64]byte


// MaskSraEpi64: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := SignBit
//				ELSE
//					dst[i+63:i] := SignExtend(a[i+63:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm_mask_sra_epi64'.
// Requires AVX512F.
func MaskSraEpi64(src M128i, k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskSraEpi64([16]byte(src), uint8(k), [16]byte(a), [16]byte(count)))
}

func maskSraEpi64(src [16]byte, k uint8, a [16]byte, count [16]byte) [16]byte


// MaskzSraEpi64: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := SignBit
//				ELSE
//					dst[i+63:i] := SignExtend(a[i+63:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm_maskz_sra_epi64'.
// Requires AVX512F.
func MaskzSraEpi64(k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskzSraEpi64(uint8(k), [16]byte(a), [16]byte(count)))
}

func maskzSraEpi64(k uint8, a [16]byte, count [16]byte) [16]byte


// SraEpi64: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF count[63:0] > 63
//				dst[i+63:i] := SignBit
//			ELSE
//				dst[i+63:i] := SignExtend(a[i+63:i] >> count[63:0])
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm_sra_epi64'.
// Requires AVX512F.
func SraEpi64(a M128i, count M128i) M128i {
	return M128i(sraEpi64([16]byte(a), [16]byte(count)))
}

func sraEpi64(a [16]byte, count [16]byte) [16]byte


// MaskSraEpi641: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := SignBit
//				ELSE
//					dst[i+63:i] := SignExtend(a[i+63:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm256_mask_sra_epi64'.
// Requires AVX512F.
func MaskSraEpi641(src M256i, k Mmask8, a M256i, count M128i) M256i {
	return M256i(maskSraEpi641([32]byte(src), uint8(k), [32]byte(a), [16]byte(count)))
}

func maskSraEpi641(src [32]byte, k uint8, a [32]byte, count [16]byte) [32]byte


// MaskzSraEpi641: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := SignBit
//				ELSE
//					dst[i+63:i] := SignExtend(a[i+63:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm256_maskz_sra_epi64'.
// Requires AVX512F.
func MaskzSraEpi641(k Mmask8, a M256i, count M128i) M256i {
	return M256i(maskzSraEpi641(uint8(k), [32]byte(a), [16]byte(count)))
}

func maskzSraEpi641(k uint8, a [32]byte, count [16]byte) [32]byte


// SraEpi641: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF count[63:0] > 63
//				dst[i+63:i] := SignBit
//			ELSE
//				dst[i+63:i] := SignExtend(a[i+63:i] >> count[63:0])
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm256_sra_epi64'.
// Requires AVX512F.
func SraEpi641(a M256i, count M128i) M256i {
	return M256i(sraEpi641([32]byte(a), [16]byte(count)))
}

func sraEpi641(a [32]byte, count [16]byte) [32]byte


// MaskSraEpi642: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := SignBit
//				ELSE
//					dst[i+63:i] := SignExtend(a[i+63:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm512_mask_sra_epi64'.
// Requires AVX512F.
func MaskSraEpi642(src M512i, k Mmask8, a M512i, count M128i) M512i {
	return M512i(maskSraEpi642([64]byte(src), uint8(k), [64]byte(a), [16]byte(count)))
}

func maskSraEpi642(src [64]byte, k uint8, a [64]byte, count [16]byte) [64]byte


// MaskzSraEpi642: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := SignBit
//				ELSE
//					dst[i+63:i] := SignExtend(a[i+63:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm512_maskz_sra_epi64'.
// Requires AVX512F.
func MaskzSraEpi642(k Mmask8, a M512i, count M128i) M512i {
	return M512i(maskzSraEpi642(uint8(k), [64]byte(a), [16]byte(count)))
}

func maskzSraEpi642(k uint8, a [64]byte, count [16]byte) [64]byte


// SraEpi642: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF count[63:0] > 63
//				dst[i+63:i] := SignBit
//			ELSE
//				dst[i+63:i] := SignExtend(a[i+63:i] >> count[63:0])
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm512_sra_epi64'.
// Requires AVX512F.
func SraEpi642(a M512i, count M128i) M512i {
	return M512i(sraEpi642([64]byte(a), [16]byte(count)))
}

func sraEpi642(a [64]byte, count [16]byte) [64]byte


// MaskSraiEpi32: Shift packed 32-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := SignBit
//				ELSE
//					dst[i+31:i] := SignExtend(a[i+31:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAD'. Intrinsic: '_mm_mask_srai_epi32'.
// Requires AVX512F.
func MaskSraiEpi32(src M128i, k Mmask8, a M128i, imm8 uint32) M128i {
	return M128i(maskSraiEpi32([16]byte(src), uint8(k), [16]byte(a), imm8))
}

func maskSraiEpi32(src [16]byte, k uint8, a [16]byte, imm8 uint32) [16]byte


// MaskzSraiEpi32: Shift packed 32-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := SignBit
//				ELSE
//					dst[i+31:i] := SignExtend(a[i+31:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAD'. Intrinsic: '_mm_maskz_srai_epi32'.
// Requires AVX512F.
func MaskzSraiEpi32(k Mmask8, a M128i, imm8 uint32) M128i {
	return M128i(maskzSraiEpi32(uint8(k), [16]byte(a), imm8))
}

func maskzSraiEpi32(k uint8, a [16]byte, imm8 uint32) [16]byte


// MaskSraiEpi321: Shift packed 32-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := SignBit
//				ELSE
//					dst[i+31:i] := SignExtend(a[i+31:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAD'. Intrinsic: '_mm256_mask_srai_epi32'.
// Requires AVX512F.
func MaskSraiEpi321(src M256i, k Mmask8, a M256i, imm8 uint32) M256i {
	return M256i(maskSraiEpi321([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func maskSraiEpi321(src [32]byte, k uint8, a [32]byte, imm8 uint32) [32]byte


// MaskzSraiEpi321: Shift packed 32-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := SignBit
//				ELSE
//					dst[i+31:i] := SignExtend(a[i+31:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAD'. Intrinsic: '_mm256_maskz_srai_epi32'.
// Requires AVX512F.
func MaskzSraiEpi321(k Mmask8, a M256i, imm8 uint32) M256i {
	return M256i(maskzSraiEpi321(uint8(k), [32]byte(a), imm8))
}

func maskzSraiEpi321(k uint8, a [32]byte, imm8 uint32) [32]byte


// MaskzSraiEpi322: Shift packed 32-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := SignBit
//				ELSE
//					dst[i+31:i] := SignExtend(a[i+31:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAD'. Intrinsic: '_mm512_maskz_srai_epi32'.
// Requires AVX512F.
func MaskzSraiEpi322(k Mmask16, a M512i, imm8 uint32) M512i {
	return M512i(maskzSraiEpi322(uint16(k), [64]byte(a), imm8))
}

func maskzSraiEpi322(k uint16, a [64]byte, imm8 uint32) [64]byte


// MaskSraiEpi64: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := SignBit
//				ELSE
//					dst[i+63:i] := SignExtend(a[i+63:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm_mask_srai_epi64'.
// Requires AVX512F.
func MaskSraiEpi64(src M128i, k Mmask8, a M128i, imm8 uint32) M128i {
	return M128i(maskSraiEpi64([16]byte(src), uint8(k), [16]byte(a), imm8))
}

func maskSraiEpi64(src [16]byte, k uint8, a [16]byte, imm8 uint32) [16]byte


// MaskzSraiEpi64: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := SignBit
//				ELSE
//					dst[i+63:i] := SignExtend(a[i+63:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm_maskz_srai_epi64'.
// Requires AVX512F.
func MaskzSraiEpi64(k Mmask8, a M128i, imm8 uint32) M128i {
	return M128i(maskzSraiEpi64(uint8(k), [16]byte(a), imm8))
}

func maskzSraiEpi64(k uint8, a [16]byte, imm8 uint32) [16]byte


// SraiEpi64: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF imm8[7:0] > 63
//				dst[i+63:i] := SignBit
//			ELSE
//				dst[i+63:i] := SignExtend(a[i+63:i] << imm8[7:0])
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm_srai_epi64'.
// Requires AVX512F.
func SraiEpi64(a M128i, imm8 uint32) M128i {
	return M128i(sraiEpi64([16]byte(a), imm8))
}

func sraiEpi64(a [16]byte, imm8 uint32) [16]byte


// MaskSraiEpi641: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := SignBit
//				ELSE
//					dst[i+63:i] := SignExtend(a[i+63:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm256_mask_srai_epi64'.
// Requires AVX512F.
func MaskSraiEpi641(src M256i, k Mmask8, a M256i, imm8 uint32) M256i {
	return M256i(maskSraiEpi641([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func maskSraiEpi641(src [32]byte, k uint8, a [32]byte, imm8 uint32) [32]byte


// MaskzSraiEpi641: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := SignBit
//				ELSE
//					dst[i+63:i] := SignExtend(a[i+63:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm256_maskz_srai_epi64'.
// Requires AVX512F.
func MaskzSraiEpi641(k Mmask8, a M256i, imm8 uint32) M256i {
	return M256i(maskzSraiEpi641(uint8(k), [32]byte(a), imm8))
}

func maskzSraiEpi641(k uint8, a [32]byte, imm8 uint32) [32]byte


// SraiEpi641: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF imm8[7:0] > 63
//				dst[i+63:i] := SignBit
//			ELSE
//				dst[i+63:i] := SignExtend(a[i+63:i] << imm8[7:0])
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm256_srai_epi64'.
// Requires AVX512F.
func SraiEpi641(a M256i, imm8 uint32) M256i {
	return M256i(sraiEpi641([32]byte(a), imm8))
}

func sraiEpi641(a [32]byte, imm8 uint32) [32]byte


// MaskSraiEpi642: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := SignBit
//				ELSE
//					dst[i+63:i] := SignExtend(a[i+63:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm512_mask_srai_epi64'.
// Requires AVX512F.
func MaskSraiEpi642(src M512i, k Mmask8, a M512i, imm8 uint32) M512i {
	return M512i(maskSraiEpi642([64]byte(src), uint8(k), [64]byte(a), imm8))
}

func maskSraiEpi642(src [64]byte, k uint8, a [64]byte, imm8 uint32) [64]byte


// MaskzSraiEpi642: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := SignBit
//				ELSE
//					dst[i+63:i] := SignExtend(a[i+63:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm512_maskz_srai_epi64'.
// Requires AVX512F.
func MaskzSraiEpi642(k Mmask8, a M512i, imm8 uint32) M512i {
	return M512i(maskzSraiEpi642(uint8(k), [64]byte(a), imm8))
}

func maskzSraiEpi642(k uint8, a [64]byte, imm8 uint32) [64]byte


// SraiEpi642: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF imm8[7:0] > 63
//				dst[i+63:i] := SignBit
//			ELSE
//				dst[i+63:i] := SignExtend(a[i+63:i] << imm8[7:0])
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm512_srai_epi64'.
// Requires AVX512F.
func SraiEpi642(a M512i, imm8 uint32) M512i {
	return M512i(sraiEpi642([64]byte(a), imm8))
}

func sraiEpi642(a [64]byte, imm8 uint32) [64]byte


// MaskSravEpi32: Shift packed 32-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[i+31:i] >> count[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAVD'. Intrinsic: '_mm_mask_srav_epi32'.
// Requires AVX512F.
func MaskSravEpi32(src M128i, k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskSravEpi32([16]byte(src), uint8(k), [16]byte(a), [16]byte(count)))
}

func maskSravEpi32(src [16]byte, k uint8, a [16]byte, count [16]byte) [16]byte


// MaskzSravEpi32: Shift packed 32-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[i+31:i] >> count[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAVD'. Intrinsic: '_mm_maskz_srav_epi32'.
// Requires AVX512F.
func MaskzSravEpi32(k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskzSravEpi32(uint8(k), [16]byte(a), [16]byte(count)))
}

func maskzSravEpi32(k uint8, a [16]byte, count [16]byte) [16]byte


// MaskSravEpi321: Shift packed 32-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[i+31:i] >> count[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAVD'. Intrinsic: '_mm256_mask_srav_epi32'.
// Requires AVX512F.
func MaskSravEpi321(src M256i, k Mmask8, a M256i, count M256i) M256i {
	return M256i(maskSravEpi321([32]byte(src), uint8(k), [32]byte(a), [32]byte(count)))
}

func maskSravEpi321(src [32]byte, k uint8, a [32]byte, count [32]byte) [32]byte


// MaskzSravEpi321: Shift packed 32-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[i+31:i] >> count[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAVD'. Intrinsic: '_mm256_maskz_srav_epi32'.
// Requires AVX512F.
func MaskzSravEpi321(k Mmask8, a M256i, count M256i) M256i {
	return M256i(maskzSravEpi321(uint8(k), [32]byte(a), [32]byte(count)))
}

func maskzSravEpi321(k uint8, a [32]byte, count [32]byte) [32]byte


// MaskzSravEpi322: Shift packed 32-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[i+31:i] >> count[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAVD'. Intrinsic: '_mm512_maskz_srav_epi32'.
// Requires AVX512F.
func MaskzSravEpi322(k Mmask16, a M512i, count M512i) M512i {
	return M512i(maskzSravEpi322(uint16(k), [64]byte(a), [64]byte(count)))
}

func maskzSravEpi322(k uint16, a [64]byte, count [64]byte) [64]byte


// MaskSravEpi64: Shift packed 64-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[i+63:i] >> count[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAVQ'. Intrinsic: '_mm_mask_srav_epi64'.
// Requires AVX512F.
func MaskSravEpi64(src M128i, k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskSravEpi64([16]byte(src), uint8(k), [16]byte(a), [16]byte(count)))
}

func maskSravEpi64(src [16]byte, k uint8, a [16]byte, count [16]byte) [16]byte


// MaskzSravEpi64: Shift packed 64-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[i+63:i] >> count[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAVQ'. Intrinsic: '_mm_maskz_srav_epi64'.
// Requires AVX512F.
func MaskzSravEpi64(k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskzSravEpi64(uint8(k), [16]byte(a), [16]byte(count)))
}

func maskzSravEpi64(k uint8, a [16]byte, count [16]byte) [16]byte


// SravEpi64: Shift packed 64-bit integers in 'a' right by the amount specified
// by the corresponding element in 'count' while shifting in sign bits, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := SignExtend(a[i+63:i] >> count[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAVQ'. Intrinsic: '_mm_srav_epi64'.
// Requires AVX512F.
func SravEpi64(a M128i, count M128i) M128i {
	return M128i(sravEpi64([16]byte(a), [16]byte(count)))
}

func sravEpi64(a [16]byte, count [16]byte) [16]byte


// MaskSravEpi641: Shift packed 64-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[i+63:i] >> count[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAVQ'. Intrinsic: '_mm256_mask_srav_epi64'.
// Requires AVX512F.
func MaskSravEpi641(src M256i, k Mmask8, a M256i, count M256i) M256i {
	return M256i(maskSravEpi641([32]byte(src), uint8(k), [32]byte(a), [32]byte(count)))
}

func maskSravEpi641(src [32]byte, k uint8, a [32]byte, count [32]byte) [32]byte


// MaskzSravEpi641: Shift packed 64-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[i+63:i] >> count[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAVQ'. Intrinsic: '_mm256_maskz_srav_epi64'.
// Requires AVX512F.
func MaskzSravEpi641(k Mmask8, a M256i, count M256i) M256i {
	return M256i(maskzSravEpi641(uint8(k), [32]byte(a), [32]byte(count)))
}

func maskzSravEpi641(k uint8, a [32]byte, count [32]byte) [32]byte


// SravEpi641: Shift packed 64-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := SignExtend(a[i+63:i] >> count[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAVQ'. Intrinsic: '_mm256_srav_epi64'.
// Requires AVX512F.
func SravEpi641(a M256i, count M256i) M256i {
	return M256i(sravEpi641([32]byte(a), [32]byte(count)))
}

func sravEpi641(a [32]byte, count [32]byte) [32]byte


// MaskSravEpi642: Shift packed 64-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[i+63:i] >> count[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAVQ'. Intrinsic: '_mm512_mask_srav_epi64'.
// Requires AVX512F.
func MaskSravEpi642(src M512i, k Mmask8, a M512i, count M512i) M512i {
	return M512i(maskSravEpi642([64]byte(src), uint8(k), [64]byte(a), [64]byte(count)))
}

func maskSravEpi642(src [64]byte, k uint8, a [64]byte, count [64]byte) [64]byte


// MaskzSravEpi642: Shift packed 64-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[i+63:i] >> count[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAVQ'. Intrinsic: '_mm512_maskz_srav_epi64'.
// Requires AVX512F.
func MaskzSravEpi642(k Mmask8, a M512i, count M512i) M512i {
	return M512i(maskzSravEpi642(uint8(k), [64]byte(a), [64]byte(count)))
}

func maskzSravEpi642(k uint8, a [64]byte, count [64]byte) [64]byte


// SravEpi642: Shift packed 64-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := SignExtend(a[i+63:i] >> count[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRAVQ'. Intrinsic: '_mm512_srav_epi64'.
// Requires AVX512F.
func SravEpi642(a M512i, count M512i) M512i {
	return M512i(sravEpi642([64]byte(a), [64]byte(count)))
}

func sravEpi642(a [64]byte, count [64]byte) [64]byte


// MaskSrlEpi32: Shift packed 32-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRLD'. Intrinsic: '_mm_mask_srl_epi32'.
// Requires AVX512F.
func MaskSrlEpi32(src M128i, k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskSrlEpi32([16]byte(src), uint8(k), [16]byte(a), [16]byte(count)))
}

func maskSrlEpi32(src [16]byte, k uint8, a [16]byte, count [16]byte) [16]byte


// MaskzSrlEpi32: Shift packed 32-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRLD'. Intrinsic: '_mm_maskz_srl_epi32'.
// Requires AVX512F.
func MaskzSrlEpi32(k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskzSrlEpi32(uint8(k), [16]byte(a), [16]byte(count)))
}

func maskzSrlEpi32(k uint8, a [16]byte, count [16]byte) [16]byte


// MaskSrlEpi321: Shift packed 32-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLD'. Intrinsic: '_mm256_mask_srl_epi32'.
// Requires AVX512F.
func MaskSrlEpi321(src M256i, k Mmask8, a M256i, count M128i) M256i {
	return M256i(maskSrlEpi321([32]byte(src), uint8(k), [32]byte(a), [16]byte(count)))
}

func maskSrlEpi321(src [32]byte, k uint8, a [32]byte, count [16]byte) [32]byte


// MaskzSrlEpi321: Shift packed 32-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLD'. Intrinsic: '_mm256_maskz_srl_epi32'.
// Requires AVX512F.
func MaskzSrlEpi321(k Mmask8, a M256i, count M128i) M256i {
	return M256i(maskzSrlEpi321(uint8(k), [32]byte(a), [16]byte(count)))
}

func maskzSrlEpi321(k uint8, a [32]byte, count [16]byte) [32]byte


// MaskSrlEpi322: Shift packed 32-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLD'. Intrinsic: '_mm512_mask_srl_epi32'.
// Requires AVX512F.
func MaskSrlEpi322(src M512i, k Mmask16, a M512i, count M128i) M512i {
	return M512i(maskSrlEpi322([64]byte(src), uint16(k), [64]byte(a), [16]byte(count)))
}

func maskSrlEpi322(src [64]byte, k uint16, a [64]byte, count [16]byte) [64]byte


// MaskzSrlEpi322: Shift packed 32-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLD'. Intrinsic: '_mm512_maskz_srl_epi32'.
// Requires AVX512F.
func MaskzSrlEpi322(k Mmask16, a M512i, count M128i) M512i {
	return M512i(maskzSrlEpi322(uint16(k), [64]byte(a), [16]byte(count)))
}

func maskzSrlEpi322(k uint16, a [64]byte, count [16]byte) [64]byte


// SrlEpi32: Shift packed 32-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF count[63:0] > 31
//				dst[i+31:i] := 0
//			ELSE
//				dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[63:0])
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLD'. Intrinsic: '_mm512_srl_epi32'.
// Requires AVX512F.
func SrlEpi32(a M512i, count M128i) M512i {
	return M512i(srlEpi32([64]byte(a), [16]byte(count)))
}

func srlEpi32(a [64]byte, count [16]byte) [64]byte


// MaskSrlEpi64: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm_mask_srl_epi64'.
// Requires AVX512F.
func MaskSrlEpi64(src M128i, k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskSrlEpi64([16]byte(src), uint8(k), [16]byte(a), [16]byte(count)))
}

func maskSrlEpi64(src [16]byte, k uint8, a [16]byte, count [16]byte) [16]byte


// MaskzSrlEpi64: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm_maskz_srl_epi64'.
// Requires AVX512F.
func MaskzSrlEpi64(k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskzSrlEpi64(uint8(k), [16]byte(a), [16]byte(count)))
}

func maskzSrlEpi64(k uint8, a [16]byte, count [16]byte) [16]byte


// MaskSrlEpi641: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm256_mask_srl_epi64'.
// Requires AVX512F.
func MaskSrlEpi641(src M256i, k Mmask8, a M256i, count M128i) M256i {
	return M256i(maskSrlEpi641([32]byte(src), uint8(k), [32]byte(a), [16]byte(count)))
}

func maskSrlEpi641(src [32]byte, k uint8, a [32]byte, count [16]byte) [32]byte


// MaskzSrlEpi641: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm256_maskz_srl_epi64'.
// Requires AVX512F.
func MaskzSrlEpi641(k Mmask8, a M256i, count M128i) M256i {
	return M256i(maskzSrlEpi641(uint8(k), [32]byte(a), [16]byte(count)))
}

func maskzSrlEpi641(k uint8, a [32]byte, count [16]byte) [32]byte


// MaskSrlEpi642: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm512_mask_srl_epi64'.
// Requires AVX512F.
func MaskSrlEpi642(src M512i, k Mmask8, a M512i, count M128i) M512i {
	return M512i(maskSrlEpi642([64]byte(src), uint8(k), [64]byte(a), [16]byte(count)))
}

func maskSrlEpi642(src [64]byte, k uint8, a [64]byte, count [16]byte) [64]byte


// MaskzSrlEpi642: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm512_maskz_srl_epi64'.
// Requires AVX512F.
func MaskzSrlEpi642(k Mmask8, a M512i, count M128i) M512i {
	return M512i(maskzSrlEpi642(uint8(k), [64]byte(a), [16]byte(count)))
}

func maskzSrlEpi642(k uint8, a [64]byte, count [16]byte) [64]byte


// SrlEpi64: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF count[63:0] > 63
//				dst[i+63:i] := 0
//			ELSE
//				dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[63:0])
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm512_srl_epi64'.
// Requires AVX512F.
func SrlEpi64(a M512i, count M128i) M512i {
	return M512i(srlEpi64([64]byte(a), [16]byte(count)))
}

func srlEpi64(a [64]byte, count [16]byte) [64]byte


// MaskSrliEpi32: Shift packed 32-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRLD'. Intrinsic: '_mm_mask_srli_epi32'.
// Requires AVX512F.
func MaskSrliEpi32(src M128i, k Mmask8, a M128i, imm8 uint32) M128i {
	return M128i(maskSrliEpi32([16]byte(src), uint8(k), [16]byte(a), imm8))
}

func maskSrliEpi32(src [16]byte, k uint8, a [16]byte, imm8 uint32) [16]byte


// MaskzSrliEpi32: Shift packed 32-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRLD'. Intrinsic: '_mm_maskz_srli_epi32'.
// Requires AVX512F.
func MaskzSrliEpi32(k Mmask8, a M128i, imm8 uint32) M128i {
	return M128i(maskzSrliEpi32(uint8(k), [16]byte(a), imm8))
}

func maskzSrliEpi32(k uint8, a [16]byte, imm8 uint32) [16]byte


// MaskSrliEpi321: Shift packed 32-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLD'. Intrinsic: '_mm256_mask_srli_epi32'.
// Requires AVX512F.
func MaskSrliEpi321(src M256i, k Mmask8, a M256i, imm8 uint32) M256i {
	return M256i(maskSrliEpi321([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func maskSrliEpi321(src [32]byte, k uint8, a [32]byte, imm8 uint32) [32]byte


// MaskzSrliEpi321: Shift packed 32-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLD'. Intrinsic: '_mm256_maskz_srli_epi32'.
// Requires AVX512F.
func MaskzSrliEpi321(k Mmask8, a M256i, imm8 uint32) M256i {
	return M256i(maskzSrliEpi321(uint8(k), [32]byte(a), imm8))
}

func maskzSrliEpi321(k uint8, a [32]byte, imm8 uint32) [32]byte


// MaskzSrliEpi322: Shift packed 32-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLD'. Intrinsic: '_mm512_maskz_srli_epi32'.
// Requires AVX512F.
func MaskzSrliEpi322(k Mmask16, a M512i, imm8 uint32) M512i {
	return M512i(maskzSrliEpi322(uint16(k), [64]byte(a), imm8))
}

func maskzSrliEpi322(k uint16, a [64]byte, imm8 uint32) [64]byte


// MaskSrliEpi64: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm_mask_srli_epi64'.
// Requires AVX512F.
func MaskSrliEpi64(src M128i, k Mmask8, a M128i, imm8 uint32) M128i {
	return M128i(maskSrliEpi64([16]byte(src), uint8(k), [16]byte(a), imm8))
}

func maskSrliEpi64(src [16]byte, k uint8, a [16]byte, imm8 uint32) [16]byte


// MaskzSrliEpi64: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm_maskz_srli_epi64'.
// Requires AVX512F.
func MaskzSrliEpi64(k Mmask8, a M128i, imm8 uint32) M128i {
	return M128i(maskzSrliEpi64(uint8(k), [16]byte(a), imm8))
}

func maskzSrliEpi64(k uint8, a [16]byte, imm8 uint32) [16]byte


// MaskSrliEpi641: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm256_mask_srli_epi64'.
// Requires AVX512F.
func MaskSrliEpi641(src M256i, k Mmask8, a M256i, imm8 uint32) M256i {
	return M256i(maskSrliEpi641([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func maskSrliEpi641(src [32]byte, k uint8, a [32]byte, imm8 uint32) [32]byte


// MaskzSrliEpi641: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm256_maskz_srli_epi64'.
// Requires AVX512F.
func MaskzSrliEpi641(k Mmask8, a M256i, imm8 uint32) M256i {
	return M256i(maskzSrliEpi641(uint8(k), [32]byte(a), imm8))
}

func maskzSrliEpi641(k uint8, a [32]byte, imm8 uint32) [32]byte


// MaskSrliEpi642: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm512_mask_srli_epi64'.
// Requires AVX512F.
func MaskSrliEpi642(src M512i, k Mmask8, a M512i, imm8 uint32) M512i {
	return M512i(maskSrliEpi642([64]byte(src), uint8(k), [64]byte(a), imm8))
}

func maskSrliEpi642(src [64]byte, k uint8, a [64]byte, imm8 uint32) [64]byte


// MaskzSrliEpi642: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm512_maskz_srli_epi64'.
// Requires AVX512F.
func MaskzSrliEpi642(k Mmask8, a M512i, imm8 uint32) M512i {
	return M512i(maskzSrliEpi642(uint8(k), [64]byte(a), imm8))
}

func maskzSrliEpi642(k uint8, a [64]byte, imm8 uint32) [64]byte


// SrliEpi64: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF imm8[7:0] > 63
//				dst[i+63:i] := 0
//			ELSE
//				dst[i+63:i] := ZeroExtend(a[i+63:i] >> imm8[7:0])
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm512_srli_epi64'.
// Requires AVX512F.
func SrliEpi64(a M512i, imm8 uint32) M512i {
	return M512i(srliEpi64([64]byte(a), imm8))
}

func srliEpi64(a [64]byte, imm8 uint32) [64]byte


// MaskSrlvEpi32: Shift packed 32-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRLVD'. Intrinsic: '_mm_mask_srlv_epi32'.
// Requires AVX512F.
func MaskSrlvEpi32(src M128i, k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskSrlvEpi32([16]byte(src), uint8(k), [16]byte(a), [16]byte(count)))
}

func maskSrlvEpi32(src [16]byte, k uint8, a [16]byte, count [16]byte) [16]byte


// MaskzSrlvEpi32: Shift packed 32-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRLVD'. Intrinsic: '_mm_maskz_srlv_epi32'.
// Requires AVX512F.
func MaskzSrlvEpi32(k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskzSrlvEpi32(uint8(k), [16]byte(a), [16]byte(count)))
}

func maskzSrlvEpi32(k uint8, a [16]byte, count [16]byte) [16]byte


// MaskSrlvEpi321: Shift packed 32-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLVD'. Intrinsic: '_mm256_mask_srlv_epi32'.
// Requires AVX512F.
func MaskSrlvEpi321(src M256i, k Mmask8, a M256i, count M256i) M256i {
	return M256i(maskSrlvEpi321([32]byte(src), uint8(k), [32]byte(a), [32]byte(count)))
}

func maskSrlvEpi321(src [32]byte, k uint8, a [32]byte, count [32]byte) [32]byte


// MaskzSrlvEpi321: Shift packed 32-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLVD'. Intrinsic: '_mm256_maskz_srlv_epi32'.
// Requires AVX512F.
func MaskzSrlvEpi321(k Mmask8, a M256i, count M256i) M256i {
	return M256i(maskzSrlvEpi321(uint8(k), [32]byte(a), [32]byte(count)))
}

func maskzSrlvEpi321(k uint8, a [32]byte, count [32]byte) [32]byte


// MaskzSrlvEpi322: Shift packed 32-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLVD'. Intrinsic: '_mm512_maskz_srlv_epi32'.
// Requires AVX512F.
func MaskzSrlvEpi322(k Mmask16, a M512i, count M512i) M512i {
	return M512i(maskzSrlvEpi322(uint16(k), [64]byte(a), [64]byte(count)))
}

func maskzSrlvEpi322(k uint16, a [64]byte, count [64]byte) [64]byte


// MaskSrlvEpi64: Shift packed 64-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRLVQ'. Intrinsic: '_mm_mask_srlv_epi64'.
// Requires AVX512F.
func MaskSrlvEpi64(src M128i, k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskSrlvEpi64([16]byte(src), uint8(k), [16]byte(a), [16]byte(count)))
}

func maskSrlvEpi64(src [16]byte, k uint8, a [16]byte, count [16]byte) [16]byte


// MaskzSrlvEpi64: Shift packed 64-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRLVQ'. Intrinsic: '_mm_maskz_srlv_epi64'.
// Requires AVX512F.
func MaskzSrlvEpi64(k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskzSrlvEpi64(uint8(k), [16]byte(a), [16]byte(count)))
}

func maskzSrlvEpi64(k uint8, a [16]byte, count [16]byte) [16]byte


// MaskSrlvEpi641: Shift packed 64-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLVQ'. Intrinsic: '_mm256_mask_srlv_epi64'.
// Requires AVX512F.
func MaskSrlvEpi641(src M256i, k Mmask8, a M256i, count M256i) M256i {
	return M256i(maskSrlvEpi641([32]byte(src), uint8(k), [32]byte(a), [32]byte(count)))
}

func maskSrlvEpi641(src [32]byte, k uint8, a [32]byte, count [32]byte) [32]byte


// MaskzSrlvEpi641: Shift packed 64-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLVQ'. Intrinsic: '_mm256_maskz_srlv_epi64'.
// Requires AVX512F.
func MaskzSrlvEpi641(k Mmask8, a M256i, count M256i) M256i {
	return M256i(maskzSrlvEpi641(uint8(k), [32]byte(a), [32]byte(count)))
}

func maskzSrlvEpi641(k uint8, a [32]byte, count [32]byte) [32]byte


// MaskSrlvEpi642: Shift packed 64-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLVQ'. Intrinsic: '_mm512_mask_srlv_epi64'.
// Requires AVX512F.
func MaskSrlvEpi642(src M512i, k Mmask8, a M512i, count M512i) M512i {
	return M512i(maskSrlvEpi642([64]byte(src), uint8(k), [64]byte(a), [64]byte(count)))
}

func maskSrlvEpi642(src [64]byte, k uint8, a [64]byte, count [64]byte) [64]byte


// MaskzSrlvEpi642: Shift packed 64-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLVQ'. Intrinsic: '_mm512_maskz_srlv_epi64'.
// Requires AVX512F.
func MaskzSrlvEpi642(k Mmask8, a M512i, count M512i) M512i {
	return M512i(maskzSrlvEpi642(uint8(k), [64]byte(a), [64]byte(count)))
}

func maskzSrlvEpi642(k uint8, a [64]byte, count [64]byte) [64]byte


// SrlvEpi64: Shift packed 64-bit integers in 'a' right by the amount specified
// by the corresponding element in 'count' while shifting in zeros, and store
// the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSRLVQ'. Intrinsic: '_mm512_srlv_epi64'.
// Requires AVX512F.
func SrlvEpi64(a M512i, count M512i) M512i {
	return M512i(srlvEpi64([64]byte(a), [64]byte(count)))
}

func srlvEpi64(a [64]byte, count [64]byte) [64]byte


// MaskStoreEpi32: Store packed 32-bit integers from 'a' into memory using
// writemask 'k'.
// 	'mem_addr' must be aligned on a 16-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVDQA32'. Intrinsic: '_mm_mask_store_epi32'.
// Requires AVX512F.
func MaskStoreEpi32(mem_addr uintptr, k Mmask8, a M128i)  {
	maskStoreEpi32(uintptr(mem_addr), uint8(k), [16]byte(a))
}

func maskStoreEpi32(mem_addr uintptr, k uint8, a [16]byte) 


// MaskStoreEpi321: Store packed 32-bit integers from 'a' into memory using
// writemask 'k'.
// 	'mem_addr' must be aligned on a 32-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVDQA32'. Intrinsic: '_mm256_mask_store_epi32'.
// Requires AVX512F.
func MaskStoreEpi321(mem_addr uintptr, k Mmask8, a M256i)  {
	maskStoreEpi321(uintptr(mem_addr), uint8(k), [32]byte(a))
}

func maskStoreEpi321(mem_addr uintptr, k uint8, a [32]byte) 


// MaskStoreEpi64: Store packed 64-bit integers from 'a' into memory using
// writemask 'k'.
// 	'mem_addr' must be aligned on a 16-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVDQA64'. Intrinsic: '_mm_mask_store_epi64'.
// Requires AVX512F.
func MaskStoreEpi64(mem_addr uintptr, k Mmask8, a M128i)  {
	maskStoreEpi64(uintptr(mem_addr), uint8(k), [16]byte(a))
}

func maskStoreEpi64(mem_addr uintptr, k uint8, a [16]byte) 


// MaskStoreEpi641: Store packed 64-bit integers from 'a' into memory using
// writemask 'k'.
// 	'mem_addr' must be aligned on a 32-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVDQA64'. Intrinsic: '_mm256_mask_store_epi64'.
// Requires AVX512F.
func MaskStoreEpi641(mem_addr uintptr, k Mmask8, a M256i)  {
	maskStoreEpi641(uintptr(mem_addr), uint8(k), [32]byte(a))
}

func maskStoreEpi641(mem_addr uintptr, k uint8, a [32]byte) 


// MaskStorePd: Store packed double-precision (64-bit) floating-point elements
// from 'a' into memory using writemask 'k'.
// 	'mem_addr' must be aligned on a 16-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVAPD'. Intrinsic: '_mm_mask_store_pd'.
// Requires AVX512F.
func MaskStorePd(mem_addr uintptr, k Mmask8, a M128d)  {
	maskStorePd(uintptr(mem_addr), uint8(k), [2]float64(a))
}

func maskStorePd(mem_addr uintptr, k uint8, a [2]float64) 


// MaskStorePd1: Store packed double-precision (64-bit) floating-point elements
// from 'a' into memory using writemask 'k'.
// 	'mem_addr' must be aligned on a 32-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVAPD'. Intrinsic: '_mm256_mask_store_pd'.
// Requires AVX512F.
func MaskStorePd1(mem_addr uintptr, k Mmask8, a M256d)  {
	maskStorePd1(uintptr(mem_addr), uint8(k), [4]float64(a))
}

func maskStorePd1(mem_addr uintptr, k uint8, a [4]float64) 


// MaskStorePs: Store packed single-precision (32-bit) floating-point elements
// from 'a' into memory using writemask 'k'.
// 	'mem_addr' must be aligned on a 16-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVAPS'. Intrinsic: '_mm_mask_store_ps'.
// Requires AVX512F.
func MaskStorePs(mem_addr uintptr, k Mmask8, a M128)  {
	maskStorePs(uintptr(mem_addr), uint8(k), [4]float32(a))
}

func maskStorePs(mem_addr uintptr, k uint8, a [4]float32) 


// MaskStorePs1: Store packed single-precision (32-bit) floating-point elements
// from 'a' into memory using writemask 'k'.
// 	'mem_addr' must be aligned on a 32-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVAPS'. Intrinsic: '_mm256_mask_store_ps'.
// Requires AVX512F.
func MaskStorePs1(mem_addr uintptr, k Mmask8, a M256)  {
	maskStorePs1(uintptr(mem_addr), uint8(k), [8]float32(a))
}

func maskStorePs1(mem_addr uintptr, k uint8, a [8]float32) 


// MaskStoreSd: Store the lower double-precision (64-bit) floating-point
// element from 'a' into memory using writemask 'k'.
// 	'mem_addr' must be aligned on a 16-byte boundary or a general-protection
// exception may be generated. 
//
//		IF k[0]
//			MEM[mem_addr+63:mem_addr] := a[63:0]
//		FI
//
// Instruction: 'VMOVSD'. Intrinsic: '_mm_mask_store_sd'.
// Requires AVX512F.
func MaskStoreSd(mem_addr float64, k Mmask8, a M128d)  {
	maskStoreSd(mem_addr, uint8(k), [2]float64(a))
}

func maskStoreSd(mem_addr float64, k uint8, a [2]float64) 


// MaskStoreSs: Store the lower single-precision (32-bit) floating-point
// element from 'a' into memory using writemask 'k'.
// 	'mem_addr' must be aligned on a 16-byte boundary or a general-protection
// exception may be generated. 
//
//		IF k[0]
//			MEM[mem_addr+31:mem_addr] := a[31:0]
//		FI
//
// Instruction: 'VMOVSS'. Intrinsic: '_mm_mask_store_ss'.
// Requires AVX512F.
func MaskStoreSs(mem_addr float32, k Mmask8, a M128)  {
	maskStoreSs(mem_addr, uint8(k), [4]float32(a))
}

func maskStoreSs(mem_addr float32, k uint8, a [4]float32) 


// MaskStoreuEpi32: Store packed 32-bit integers from 'a' into memory using
// writemask 'k'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVDQU32'. Intrinsic: '_mm_mask_storeu_epi32'.
// Requires AVX512F.
func MaskStoreuEpi32(mem_addr uintptr, k Mmask8, a M128i)  {
	maskStoreuEpi32(uintptr(mem_addr), uint8(k), [16]byte(a))
}

func maskStoreuEpi32(mem_addr uintptr, k uint8, a [16]byte) 


// MaskStoreuEpi321: Store packed 32-bit integers from 'a' into memory using
// writemask 'k'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVDQU32'. Intrinsic: '_mm256_mask_storeu_epi32'.
// Requires AVX512F.
func MaskStoreuEpi321(mem_addr uintptr, k Mmask8, a M256i)  {
	maskStoreuEpi321(uintptr(mem_addr), uint8(k), [32]byte(a))
}

func maskStoreuEpi321(mem_addr uintptr, k uint8, a [32]byte) 


// MaskStoreuEpi322: Store packed 32-bit integers from 'a' into memory using
// writemask 'k'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVDQU32'. Intrinsic: '_mm512_mask_storeu_epi32'.
// Requires AVX512F.
func MaskStoreuEpi322(mem_addr uintptr, k Mmask16, a M512i)  {
	maskStoreuEpi322(uintptr(mem_addr), uint16(k), [64]byte(a))
}

func maskStoreuEpi322(mem_addr uintptr, k uint16, a [64]byte) 


// MaskStoreuEpi64: Store packed 64-bit integers from 'a' into memory using
// writemask 'k'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVDQU64'. Intrinsic: '_mm_mask_storeu_epi64'.
// Requires AVX512F.
func MaskStoreuEpi64(mem_addr uintptr, k Mmask8, a M128i)  {
	maskStoreuEpi64(uintptr(mem_addr), uint8(k), [16]byte(a))
}

func maskStoreuEpi64(mem_addr uintptr, k uint8, a [16]byte) 


// MaskStoreuEpi641: Store packed 64-bit integers from 'a' into memory using
// writemask 'k'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVDQU64'. Intrinsic: '_mm256_mask_storeu_epi64'.
// Requires AVX512F.
func MaskStoreuEpi641(mem_addr uintptr, k Mmask8, a M256i)  {
	maskStoreuEpi641(uintptr(mem_addr), uint8(k), [32]byte(a))
}

func maskStoreuEpi641(mem_addr uintptr, k uint8, a [32]byte) 


// MaskStoreuEpi642: Store packed 64-bit integers from 'a' into memory using
// writemask 'k'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVDQU64'. Intrinsic: '_mm512_mask_storeu_epi64'.
// Requires AVX512F.
func MaskStoreuEpi642(mem_addr uintptr, k Mmask8, a M512i)  {
	maskStoreuEpi642(uintptr(mem_addr), uint8(k), [64]byte(a))
}

func maskStoreuEpi642(mem_addr uintptr, k uint8, a [64]byte) 


// MaskStoreuPd: Store packed double-precision (64-bit) floating-point elements
// from 'a' into memory using writemask 'k'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVUPD'. Intrinsic: '_mm_mask_storeu_pd'.
// Requires AVX512F.
func MaskStoreuPd(mem_addr uintptr, k Mmask8, a M128d)  {
	maskStoreuPd(uintptr(mem_addr), uint8(k), [2]float64(a))
}

func maskStoreuPd(mem_addr uintptr, k uint8, a [2]float64) 


// MaskStoreuPd1: Store packed double-precision (64-bit) floating-point
// elements from 'a' into memory using writemask 'k'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVUPD'. Intrinsic: '_mm256_mask_storeu_pd'.
// Requires AVX512F.
func MaskStoreuPd1(mem_addr uintptr, k Mmask8, a M256d)  {
	maskStoreuPd1(uintptr(mem_addr), uint8(k), [4]float64(a))
}

func maskStoreuPd1(mem_addr uintptr, k uint8, a [4]float64) 


// MaskStoreuPd2: Store packed double-precision (64-bit) floating-point
// elements from 'a' into memory using writemask 'k'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVUPD'. Intrinsic: '_mm512_mask_storeu_pd'.
// Requires AVX512F.
func MaskStoreuPd2(mem_addr uintptr, k Mmask8, a M512d)  {
	maskStoreuPd2(uintptr(mem_addr), uint8(k), [8]float64(a))
}

func maskStoreuPd2(mem_addr uintptr, k uint8, a [8]float64) 


// StoreuPd: Store 512-bits (composed of 8 packed double-precision (64-bit)
// floating-point elements) from 'a' into memory. 
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		MEM[mem_addr+511:mem_addr] := a[511:0]
//
// Instruction: 'VMOVUPD'. Intrinsic: '_mm512_storeu_pd'.
// Requires AVX512F.
func StoreuPd(mem_addr uintptr, a M512d)  {
	storeuPd(uintptr(mem_addr), [8]float64(a))
}

func storeuPd(mem_addr uintptr, a [8]float64) 


// MaskStoreuPs: Store packed single-precision (32-bit) floating-point elements
// from 'a' into memory using writemask 'k'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVUPS'. Intrinsic: '_mm_mask_storeu_ps'.
// Requires AVX512F.
func MaskStoreuPs(mem_addr uintptr, k Mmask8, a M128)  {
	maskStoreuPs(uintptr(mem_addr), uint8(k), [4]float32(a))
}

func maskStoreuPs(mem_addr uintptr, k uint8, a [4]float32) 


// MaskStoreuPs1: Store packed single-precision (32-bit) floating-point
// elements from 'a' into memory using writemask 'k'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVUPS'. Intrinsic: '_mm256_mask_storeu_ps'.
// Requires AVX512F.
func MaskStoreuPs1(mem_addr uintptr, k Mmask8, a M256)  {
	maskStoreuPs1(uintptr(mem_addr), uint8(k), [8]float32(a))
}

func maskStoreuPs1(mem_addr uintptr, k uint8, a [8]float32) 


// MaskStoreuPs2: Store packed single-precision (32-bit) floating-point
// elements from 'a' into memory using writemask 'k'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVUPS'. Intrinsic: '_mm512_mask_storeu_ps'.
// Requires AVX512F.
func MaskStoreuPs2(mem_addr uintptr, k Mmask16, a M512)  {
	maskStoreuPs2(uintptr(mem_addr), uint16(k), [16]float32(a))
}

func maskStoreuPs2(mem_addr uintptr, k uint16, a [16]float32) 


// StoreuPs: Store 512-bits (composed of 16 packed single-precision (32-bit)
// floating-point elements) from 'a' into memory. 
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		MEM[mem_addr+511:mem_addr] := a[511:0]
//
// Instruction: 'VMOVUPS'. Intrinsic: '_mm512_storeu_ps'.
// Requires AVX512F.
func StoreuPs(mem_addr uintptr, a M512)  {
	storeuPs(uintptr(mem_addr), [16]float32(a))
}

func storeuPs(mem_addr uintptr, a [16]float32) 


// StoreuSi512: Store 512-bits of integer data from 'a' into memory.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		MEM[mem_addr+511:mem_addr] := a[511:0]
//
// Instruction: 'VMOVDQU32'. Intrinsic: '_mm512_storeu_si512'.
// Requires AVX512F.
func StoreuSi512(mem_addr uintptr, a M512i)  {
	storeuSi512(uintptr(mem_addr), [64]byte(a))
}

func storeuSi512(mem_addr uintptr, a [64]byte) 


// StreamLoadSi512: Load 512-bits of integer data from memory into 'dst' using
// a non-temporal memory hint. 
// 	'mem_addr' must be aligned on a 64-byte boundary or a general-protection
// exception may be generated. 
//
//		dst[511:0] := MEM[mem_addr+511:mem_addr]
//		dst[MAX:512] := 0
//
// Instruction: 'VMOVNTDQA'. Intrinsic: '_mm512_stream_load_si512'.
// Requires AVX512F.
func StreamLoadSi512(mem_addr uintptr) M512i {
	return M512i(streamLoadSi512(uintptr(mem_addr)))
}

func streamLoadSi512(mem_addr uintptr) [64]byte


// StreamPd: Store 512-bits (composed of 8 packed double-precision (64-bit)
// floating-point elements) from 'a' into memory using a non-temporal memory
// hint. 
// 	'mem_addr' must be aligned on a 64-byte boundary or a general-protection
// exception may be generated. 
//
//		MEM[mem_addr+511:mem_addr] := a[511:0]
//
// Instruction: 'VMOVNTPD'. Intrinsic: '_mm512_stream_pd'.
// Requires AVX512F.
func StreamPd(mem_addr uintptr, a M512d)  {
	streamPd(uintptr(mem_addr), [8]float64(a))
}

func streamPd(mem_addr uintptr, a [8]float64) 


// StreamPs: Store 512-bits (composed of 16 packed single-precision (32-bit)
// floating-point elements) from 'a' into memory using a non-temporal memory
// hint. 
// 	'mem_addr' must be aligned on a 64-byte boundary or a general-protection
// exception may be generated. 
//
//		MEM[mem_addr+511:mem_addr] := a[511:0]
//
// Instruction: 'VMOVNTPS'. Intrinsic: '_mm512_stream_ps'.
// Requires AVX512F.
func StreamPs(mem_addr uintptr, a M512)  {
	streamPs(uintptr(mem_addr), [16]float32(a))
}

func streamPs(mem_addr uintptr, a [16]float32) 


// StreamSi512: Store 512-bits of integer data from 'a' into memory using a
// non-temporal memory hint. 
// 	'mem_addr' must be aligned on a 64-byte boundary or a general-protection
// exception may be generated. 
//
//		MEM[mem_addr+511:mem_addr] := a[511:0]
//
// Instruction: 'VMOVNTDQA'. Intrinsic: '_mm512_stream_si512'.
// Requires AVX512F.
func StreamSi512(mem_addr uintptr, a M512i)  {
	streamSi512(uintptr(mem_addr), [64]byte(a))
}

func streamSi512(mem_addr uintptr, a [64]byte) 


// MaskSubEpi32: Subtract packed 32-bit integers in 'b' from packed 32-bit
// integers in 'a', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] - b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSUBD'. Intrinsic: '_mm_mask_sub_epi32'.
// Requires AVX512F.
func MaskSubEpi32(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskSubEpi32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskSubEpi32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzSubEpi32: Subtract packed 32-bit integers in 'b' from packed 32-bit
// integers in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] - b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSUBD'. Intrinsic: '_mm_maskz_sub_epi32'.
// Requires AVX512F.
func MaskzSubEpi32(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzSubEpi32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzSubEpi32(k uint8, a [16]byte, b [16]byte) [16]byte


// MaskSubEpi321: Subtract packed 32-bit integers in 'b' from packed 32-bit
// integers in 'a', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] - b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBD'. Intrinsic: '_mm256_mask_sub_epi32'.
// Requires AVX512F.
func MaskSubEpi321(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskSubEpi321([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskSubEpi321(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzSubEpi321: Subtract packed 32-bit integers in 'b' from packed 32-bit
// integers in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] - b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBD'. Intrinsic: '_mm256_maskz_sub_epi32'.
// Requires AVX512F.
func MaskzSubEpi321(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzSubEpi321(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzSubEpi321(k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzSubEpi322: Subtract packed 32-bit integers in 'b' from packed 32-bit
// integers in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] - b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSUBD'. Intrinsic: '_mm512_maskz_sub_epi32'.
// Requires AVX512F.
func MaskzSubEpi322(k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskzSubEpi322(uint16(k), [64]byte(a), [64]byte(b)))
}

func maskzSubEpi322(k uint16, a [64]byte, b [64]byte) [64]byte


// MaskSubEpi64: Subtract packed 64-bit integers in 'b' from packed 64-bit
// integers in 'a', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] - b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSUBQ'. Intrinsic: '_mm_mask_sub_epi64'.
// Requires AVX512F.
func MaskSubEpi64(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskSubEpi64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskSubEpi64(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzSubEpi64: Subtract packed 64-bit integers in 'b' from packed 64-bit
// integers in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] - b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSUBQ'. Intrinsic: '_mm_maskz_sub_epi64'.
// Requires AVX512F.
func MaskzSubEpi64(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzSubEpi64(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzSubEpi64(k uint8, a [16]byte, b [16]byte) [16]byte


// MaskSubEpi641: Subtract packed 64-bit integers in 'b' from packed 64-bit
// integers in 'a', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] - b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBQ'. Intrinsic: '_mm256_mask_sub_epi64'.
// Requires AVX512F.
func MaskSubEpi641(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskSubEpi641([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskSubEpi641(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzSubEpi641: Subtract packed 64-bit integers in 'b' from packed 64-bit
// integers in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] - b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBQ'. Intrinsic: '_mm256_maskz_sub_epi64'.
// Requires AVX512F.
func MaskzSubEpi641(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzSubEpi641(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzSubEpi641(k uint8, a [32]byte, b [32]byte) [32]byte


// MaskSubEpi642: Subtract packed 64-bit integers in 'b' from packed 64-bit
// integers in 'a', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] - b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSUBQ'. Intrinsic: '_mm512_mask_sub_epi64'.
// Requires AVX512F.
func MaskSubEpi642(src M512i, k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskSubEpi642([64]byte(src), uint8(k), [64]byte(a), [64]byte(b)))
}

func maskSubEpi642(src [64]byte, k uint8, a [64]byte, b [64]byte) [64]byte


// MaskzSubEpi642: Subtract packed 64-bit integers in 'b' from packed 64-bit
// integers in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] - b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSUBQ'. Intrinsic: '_mm512_maskz_sub_epi64'.
// Requires AVX512F.
func MaskzSubEpi642(k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskzSubEpi642(uint8(k), [64]byte(a), [64]byte(b)))
}

func maskzSubEpi642(k uint8, a [64]byte, b [64]byte) [64]byte


// SubEpi64: Subtract packed 64-bit integers in 'b' from packed 64-bit integers
// in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := a[i+63:i] - b[i+63:i]
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPSUBQ'. Intrinsic: '_mm512_sub_epi64'.
// Requires AVX512F.
func SubEpi64(a M512i, b M512i) M512i {
	return M512i(subEpi64([64]byte(a), [64]byte(b)))
}

func subEpi64(a [64]byte, b [64]byte) [64]byte


// MaskSubPd: Subtract packed double-precision (64-bit) floating-point elements
// in 'b' from packed double-precision (64-bit) floating-point elements in 'a',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] - b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSUBPD'. Intrinsic: '_mm_mask_sub_pd'.
// Requires AVX512F.
func MaskSubPd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskSubPd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskSubPd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzSubPd: Subtract packed double-precision (64-bit) floating-point
// elements in 'b' from packed double-precision (64-bit) floating-point
// elements in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] - b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSUBPD'. Intrinsic: '_mm_maskz_sub_pd'.
// Requires AVX512F.
func MaskzSubPd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzSubPd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzSubPd(k uint8, a [2]float64, b [2]float64) [2]float64


// MaskSubPd1: Subtract packed double-precision (64-bit) floating-point
// elements in 'b' from packed double-precision (64-bit) floating-point
// elements in 'a', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] - b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSUBPD'. Intrinsic: '_mm256_mask_sub_pd'.
// Requires AVX512F.
func MaskSubPd1(src M256d, k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskSubPd1([4]float64(src), uint8(k), [4]float64(a), [4]float64(b)))
}

func maskSubPd1(src [4]float64, k uint8, a [4]float64, b [4]float64) [4]float64


// MaskzSubPd1: Subtract packed double-precision (64-bit) floating-point
// elements in 'b' from packed double-precision (64-bit) floating-point
// elements in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] - b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSUBPD'. Intrinsic: '_mm256_maskz_sub_pd'.
// Requires AVX512F.
func MaskzSubPd1(k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskzSubPd1(uint8(k), [4]float64(a), [4]float64(b)))
}

func maskzSubPd1(k uint8, a [4]float64, b [4]float64) [4]float64


// MaskzSubPd2: Subtract packed double-precision (64-bit) floating-point
// elements in 'b' from packed double-precision (64-bit) floating-point
// elements in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] - b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSUBPD'. Intrinsic: '_mm512_maskz_sub_pd'.
// Requires AVX512F.
func MaskzSubPd2(k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskzSubPd2(uint8(k), [8]float64(a), [8]float64(b)))
}

func maskzSubPd2(k uint8, a [8]float64, b [8]float64) [8]float64


// MaskSubPs: Subtract packed single-precision (32-bit) floating-point elements
// in 'b' from packed single-precision (32-bit) floating-point elements in 'a',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] - b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSUBPS'. Intrinsic: '_mm_mask_sub_ps'.
// Requires AVX512F.
func MaskSubPs(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskSubPs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskSubPs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzSubPs: Subtract packed single-precision (32-bit) floating-point
// elements in 'b' from packed single-precision (32-bit) floating-point
// elements in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] - b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSUBPS'. Intrinsic: '_mm_maskz_sub_ps'.
// Requires AVX512F.
func MaskzSubPs(k Mmask8, a M128, b M128) M128 {
	return M128(maskzSubPs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzSubPs(k uint8, a [4]float32, b [4]float32) [4]float32


// MaskSubPs1: Subtract packed single-precision (32-bit) floating-point
// elements in 'b' from packed single-precision (32-bit) floating-point
// elements in 'a', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] - b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSUBPS'. Intrinsic: '_mm256_mask_sub_ps'.
// Requires AVX512F.
func MaskSubPs1(src M256, k Mmask8, a M256, b M256) M256 {
	return M256(maskSubPs1([8]float32(src), uint8(k), [8]float32(a), [8]float32(b)))
}

func maskSubPs1(src [8]float32, k uint8, a [8]float32, b [8]float32) [8]float32


// MaskzSubPs1: Subtract packed single-precision (32-bit) floating-point
// elements in 'b' from packed single-precision (32-bit) floating-point
// elements in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] - b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSUBPS'. Intrinsic: '_mm256_maskz_sub_ps'.
// Requires AVX512F.
func MaskzSubPs1(k Mmask8, a M256, b M256) M256 {
	return M256(maskzSubPs1(uint8(k), [8]float32(a), [8]float32(b)))
}

func maskzSubPs1(k uint8, a [8]float32, b [8]float32) [8]float32


// MaskzSubPs2: Subtract packed single-precision (32-bit) floating-point
// elements in 'b' from packed single-precision (32-bit) floating-point
// elements in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] - b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSUBPS'. Intrinsic: '_mm512_maskz_sub_ps'.
// Requires AVX512F.
func MaskzSubPs2(k Mmask16, a M512, b M512) M512 {
	return M512(maskzSubPs2(uint16(k), [16]float32(a), [16]float32(b)))
}

func maskzSubPs2(k uint16, a [16]float32, b [16]float32) [16]float32


// MaskzSubRoundPd: Subtract packed double-precision (64-bit) floating-point
// elements in 'b' from packed double-precision (64-bit) floating-point
// elements in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] - b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSUBPD'. Intrinsic: '_mm512_maskz_sub_round_pd'.
// Requires AVX512F.
func MaskzSubRoundPd(k Mmask8, a M512d, b M512d, rounding int) M512d {
	return M512d(maskzSubRoundPd(uint8(k), [8]float64(a), [8]float64(b), rounding))
}

func maskzSubRoundPd(k uint8, a [8]float64, b [8]float64, rounding int) [8]float64


// MaskzSubRoundPs: Subtract packed single-precision (32-bit) floating-point
// elements in 'b' from packed single-precision (32-bit) floating-point
// elements in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] - b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VSUBPS'. Intrinsic: '_mm512_maskz_sub_round_ps'.
// Requires AVX512F.
func MaskzSubRoundPs(k Mmask16, a M512, b M512, rounding int) M512 {
	return M512(maskzSubRoundPs(uint16(k), [16]float32(a), [16]float32(b), rounding))
}

func maskzSubRoundPs(k uint16, a [16]float32, b [16]float32, rounding int) [16]float32


// MaskSubRoundSd: Subtract the lower double-precision (64-bit) floating-point
// element in 'b' from the lower double-precision (64-bit) floating-point
// element in 'a', store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'src' when mask bit 0 is not set),
// and copy the upper element from 'a' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := a[63:0] - b[63:0]
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSUBSD'. Intrinsic: '_mm_mask_sub_round_sd'.
// Requires AVX512F.
func MaskSubRoundSd(src M128d, k Mmask8, a M128d, b M128d, rounding int) M128d {
	return M128d(maskSubRoundSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), rounding))
}

func maskSubRoundSd(src [2]float64, k uint8, a [2]float64, b [2]float64, rounding int) [2]float64


// MaskzSubRoundSd: Subtract the lower double-precision (64-bit) floating-point
// element in 'b' from the lower double-precision (64-bit) floating-point
// element in 'a', store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper element from 'a' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := a[63:0] - b[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSUBSD'. Intrinsic: '_mm_maskz_sub_round_sd'.
// Requires AVX512F.
func MaskzSubRoundSd(k Mmask8, a M128d, b M128d, rounding int) M128d {
	return M128d(maskzSubRoundSd(uint8(k), [2]float64(a), [2]float64(b), rounding))
}

func maskzSubRoundSd(k uint8, a [2]float64, b [2]float64, rounding int) [2]float64


// SubRoundSd: Subtract the lower double-precision (64-bit) floating-point
// element in 'b' from the lower double-precision (64-bit) floating-point
// element in 'a', store the result in the lower element of 'dst', and copy the
// upper element from 'a' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := a[63:0] - b[63:0]
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSUBSD'. Intrinsic: '_mm_sub_round_sd'.
// Requires AVX512F.
func SubRoundSd(a M128d, b M128d, rounding int) M128d {
	return M128d(subRoundSd([2]float64(a), [2]float64(b), rounding))
}

func subRoundSd(a [2]float64, b [2]float64, rounding int) [2]float64


// MaskSubRoundSs: Subtract the lower single-precision (32-bit) floating-point
// element in 'b' from the lower single-precision (32-bit) floating-point
// element in 'a', store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'src' when mask bit 0 is not set),
// and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := a[31:0] - b[31:0]
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSUBSS'. Intrinsic: '_mm_mask_sub_round_ss'.
// Requires AVX512F.
func MaskSubRoundSs(src M128, k Mmask8, a M128, b M128, rounding int) M128 {
	return M128(maskSubRoundSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), rounding))
}

func maskSubRoundSs(src [4]float32, k uint8, a [4]float32, b [4]float32, rounding int) [4]float32


// MaskzSubRoundSs: Subtract the lower single-precision (32-bit) floating-point
// element in 'b' from the lower single-precision (32-bit) floating-point
// element in 'a', store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := a[31:0] - b[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSUBSS'. Intrinsic: '_mm_maskz_sub_round_ss'.
// Requires AVX512F.
func MaskzSubRoundSs(k Mmask8, a M128, b M128, rounding int) M128 {
	return M128(maskzSubRoundSs(uint8(k), [4]float32(a), [4]float32(b), rounding))
}

func maskzSubRoundSs(k uint8, a [4]float32, b [4]float32, rounding int) [4]float32


// SubRoundSs: Subtract the lower single-precision (32-bit) floating-point
// element in 'b' from the lower single-precision (32-bit) floating-point
// element in 'a', store the result in the lower element of 'dst', and copy the
// upper 3 packed elements from 'a' to the upper elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := a[31:0] - b[31:0]
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSUBSS'. Intrinsic: '_mm_sub_round_ss'.
// Requires AVX512F.
func SubRoundSs(a M128, b M128, rounding int) M128 {
	return M128(subRoundSs([4]float32(a), [4]float32(b), rounding))
}

func subRoundSs(a [4]float32, b [4]float32, rounding int) [4]float32


// MaskSubSd: Subtract the lower double-precision (64-bit) floating-point
// element in 'b' from the lower double-precision (64-bit) floating-point
// element in 'a', store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'src' when mask bit 0 is not set),
// and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := a[63:0] - b[63:0]
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSUBSD'. Intrinsic: '_mm_mask_sub_sd'.
// Requires AVX512F.
func MaskSubSd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskSubSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskSubSd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzSubSd: Subtract the lower double-precision (64-bit) floating-point
// element in 'b' from the lower double-precision (64-bit) floating-point
// element in 'a', store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := a[63:0] - b[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSUBSD'. Intrinsic: '_mm_maskz_sub_sd'.
// Requires AVX512F.
func MaskzSubSd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzSubSd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzSubSd(k uint8, a [2]float64, b [2]float64) [2]float64


// MaskSubSs: Subtract the lower single-precision (32-bit) floating-point
// element in 'b' from the lower single-precision (32-bit) floating-point
// element in 'a', store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'src' when mask bit 0 is not set),
// and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'. 
//
//		IF k[0]
//			dst[31:0] := a[31:0] - b[31:0]
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSUBSS'. Intrinsic: '_mm_mask_sub_ss'.
// Requires AVX512F.
func MaskSubSs(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskSubSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskSubSs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzSubSs: Subtract the lower single-precision (32-bit) floating-point
// element in 'b' from the lower single-precision (32-bit) floating-point
// element in 'a', store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := a[31:0] - b[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSUBSS'. Intrinsic: '_mm_maskz_sub_ss'.
// Requires AVX512F.
func MaskzSubSs(k Mmask8, a M128, b M128) M128 {
	return M128(maskzSubSs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzSubSs(k uint8, a [4]float32, b [4]float32) [4]float32


// MaskSvmlRoundPd: Round the packed double-precision (64-bit) floating-point
// elements in 'a' to the nearest integer value, and store the results as
// packed double-precision floating-point elements in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ROUND(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i] 
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_svml_round_pd'.
// Requires AVX512F.
func MaskSvmlRoundPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskSvmlRoundPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskSvmlRoundPd(src [8]float64, k uint8, a [8]float64) [8]float64


// SvmlRoundPd: Round the packed double-precision (64-bit) floating-point
// elements in 'a' to the nearest integer value, and store the results as
// packed double-precision floating-point elements in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := ROUND(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_svml_round_pd'.
// Requires AVX512F.
func SvmlRoundPd(a M512d) M512d {
	return M512d(svmlRoundPd([8]float64(a)))
}

func svmlRoundPd(a [8]float64) [8]float64


// MaskTanPd: Compute the tangent of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := TAN(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_tan_pd'.
// Requires AVX512F.
func MaskTanPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskTanPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskTanPd(src [8]float64, k uint8, a [8]float64) [8]float64


// TanPd: Compute the tangent of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := TAN(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_tan_pd'.
// Requires AVX512F.
func TanPd(a M512d) M512d {
	return M512d(tanPd([8]float64(a)))
}

func tanPd(a [8]float64) [8]float64


// MaskTanPs: Compute the tangent of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := TAN(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_tan_ps'.
// Requires AVX512F.
func MaskTanPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskTanPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskTanPs(src [16]float32, k uint16, a [16]float32) [16]float32


// TanPs: Compute the tangent of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := TAN(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_tan_ps'.
// Requires AVX512F.
func TanPs(a M512) M512 {
	return M512(tanPs([16]float32(a)))
}

func tanPs(a [16]float32) [16]float32


// MaskTandPd: Compute the tangent of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in degrees, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := TAND(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_tand_pd'.
// Requires AVX512F.
func MaskTandPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskTandPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskTandPd(src [8]float64, k uint8, a [8]float64) [8]float64


// TandPd: Compute the tangent of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in degrees, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := TAND(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_tand_pd'.
// Requires AVX512F.
func TandPd(a M512d) M512d {
	return M512d(tandPd([8]float64(a)))
}

func tandPd(a [8]float64) [8]float64


// MaskTandPs: Compute the tangent of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in degrees, and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := TAND(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_tand_ps'.
// Requires AVX512F.
func MaskTandPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskTandPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskTandPs(src [16]float32, k uint16, a [16]float32) [16]float32


// TandPs: Compute the tangent of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in degrees, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := TAND(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_tand_ps'.
// Requires AVX512F.
func TandPs(a M512) M512 {
	return M512(tandPs([16]float32(a)))
}

func tandPs(a [16]float32) [16]float32


// MaskTanhPd: Compute the hyperbolic tangent of packed double-precision
// (64-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := TANH(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_tanh_pd'.
// Requires AVX512F.
func MaskTanhPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskTanhPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskTanhPd(src [8]float64, k uint8, a [8]float64) [8]float64


// TanhPd: Compute the hyperbolic tangent of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := TANH(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_tanh_pd'.
// Requires AVX512F.
func TanhPd(a M512d) M512d {
	return M512d(tanhPd([8]float64(a)))
}

func tanhPd(a [8]float64) [8]float64


// MaskTanhPs: Compute the hyperbolic tangent of packed single-precision
// (32-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := TANH(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_tanh_ps'.
// Requires AVX512F.
func MaskTanhPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskTanhPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskTanhPs(src [16]float32, k uint16, a [16]float32) [16]float32


// TanhPs: Compute the hyperbolic tangent of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := TANH(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_tanh_ps'.
// Requires AVX512F.
func TanhPs(a M512) M512 {
	return M512(tanhPs([16]float32(a)))
}

func tanhPs(a [16]float32) [16]float32


// MaskTernarylogicEpi32: Bitwise ternary logic that provides the capability to
// implement any three-operand binary function; the specific binary function is
// specified by value in 'imm8'. For each bit in each packed 32-bit integer,
// the corresponding bit from 'src', 'a', and 'b' are used to form a 3 bit
// index into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst' using writemask 'k' at 32-bit granularity (32-bit
// elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				FOR h := 0 to 31
//					index[2:0] := (src[i+h] << 2) OR (a[i+h] << 1) OR b[i+h]
//					dst[i+h] := imm8[index[2:0]]
//				ENDFOR
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPTERNLOGD'. Intrinsic: '_mm_mask_ternarylogic_epi32'.
// Requires AVX512F.
func MaskTernarylogicEpi32(src M128i, k Mmask8, a M128i, b M128i, imm8 int) M128i {
	return M128i(maskTernarylogicEpi32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b), imm8))
}

func maskTernarylogicEpi32(src [16]byte, k uint8, a [16]byte, b [16]byte, imm8 int) [16]byte


// MaskzTernarylogicEpi32: Bitwise ternary logic that provides the capability
// to implement any three-operand binary function; the specific binary function
// is specified by value in 'imm8'. For each bit in each packed 32-bit integer,
// the corresponding bit from 'a', 'b', and 'c' are used to form a 3 bit index
// into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst' using zeromask 'k' at 32-bit granularity (32-bit
// elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				FOR h := 0 to 31
//					index[2:0] := (a[i+h] << 2) OR (b[i+h] << 1) OR c[i+h]
//					dst[i+h] := imm8[index[2:0]]
//				ENDFOR
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPTERNLOGD'. Intrinsic: '_mm_maskz_ternarylogic_epi32'.
// Requires AVX512F.
func MaskzTernarylogicEpi32(k Mmask8, a M128i, b M128i, c M128i, imm8 int) M128i {
	return M128i(maskzTernarylogicEpi32(uint8(k), [16]byte(a), [16]byte(b), [16]byte(c), imm8))
}

func maskzTernarylogicEpi32(k uint8, a [16]byte, b [16]byte, c [16]byte, imm8 int) [16]byte


// TernarylogicEpi32: Bitwise ternary logic that provides the capability to
// implement any three-operand binary function; the specific binary function is
// specified by value in 'imm8'. For each bit in each packed 32-bit integer,
// the corresponding bit from 'a', 'b', and 'c' are used to form a 3 bit index
// into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			FOR h := 0 to 31
//				index[2:0] := (a[i+h] << 2) OR (b[i+h] << 1) OR c[i+h]
//				dst[i+h] := imm8[index[2:0]]
//			ENDFOR
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPTERNLOGD'. Intrinsic: '_mm_ternarylogic_epi32'.
// Requires AVX512F.
func TernarylogicEpi32(a M128i, b M128i, c M128i, imm8 int) M128i {
	return M128i(ternarylogicEpi32([16]byte(a), [16]byte(b), [16]byte(c), imm8))
}

func ternarylogicEpi32(a [16]byte, b [16]byte, c [16]byte, imm8 int) [16]byte


// MaskTernarylogicEpi321: Bitwise ternary logic that provides the capability
// to implement any three-operand binary function; the specific binary function
// is specified by value in 'imm8'. For each bit in each packed 32-bit integer,
// the corresponding bit from 'src', 'a', and 'b' are used to form a 3 bit
// index into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst' using writemask 'k' at 32-bit granularity (32-bit
// elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				FOR h := 0 to 31
//					index[2:0] := (src[i+h] << 2) OR (a[i+h] << 1) OR b[i+h]
//					dst[i+h] := imm8[index[2:0]]
//				ENDFOR
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPTERNLOGD'. Intrinsic: '_mm256_mask_ternarylogic_epi32'.
// Requires AVX512F.
func MaskTernarylogicEpi321(src M256i, k Mmask8, a M256i, b M256i, imm8 int) M256i {
	return M256i(maskTernarylogicEpi321([32]byte(src), uint8(k), [32]byte(a), [32]byte(b), imm8))
}

func maskTernarylogicEpi321(src [32]byte, k uint8, a [32]byte, b [32]byte, imm8 int) [32]byte


// MaskzTernarylogicEpi321: Bitwise ternary logic that provides the capability
// to implement any three-operand binary function; the specific binary function
// is specified by value in 'imm8'. For each bit in each packed 32-bit integer,
// the corresponding bit from 'a', 'b', and 'c' are used to form a 3 bit index
// into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst' using zeromask 'k' at 32-bit granularity (32-bit
// elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				FOR h := 0 to 31
//					index[2:0] := (a[i+h] << 2) OR (b[i+h] << 1) OR c[i+h]
//					dst[i+h] := imm8[index[2:0]]
//				ENDFOR
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPTERNLOGD'. Intrinsic: '_mm256_maskz_ternarylogic_epi32'.
// Requires AVX512F.
func MaskzTernarylogicEpi321(k Mmask8, a M256i, b M256i, c M256i, imm8 int) M256i {
	return M256i(maskzTernarylogicEpi321(uint8(k), [32]byte(a), [32]byte(b), [32]byte(c), imm8))
}

func maskzTernarylogicEpi321(k uint8, a [32]byte, b [32]byte, c [32]byte, imm8 int) [32]byte


// TernarylogicEpi321: Bitwise ternary logic that provides the capability to
// implement any three-operand binary function; the specific binary function is
// specified by value in 'imm8'. For each bit in each packed 32-bit integer,
// the corresponding bit from 'a', 'b', and 'c' are used to form a 3 bit index
// into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			FOR h := 0 to 31
//				index[2:0] := (a[i+h] << 2) OR (b[i+h] << 1) OR c[i+h]
//				dst[i+h] := imm8[index[2:0]]
//			ENDFOR
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPTERNLOGD'. Intrinsic: '_mm256_ternarylogic_epi32'.
// Requires AVX512F.
func TernarylogicEpi321(a M256i, b M256i, c M256i, imm8 int) M256i {
	return M256i(ternarylogicEpi321([32]byte(a), [32]byte(b), [32]byte(c), imm8))
}

func ternarylogicEpi321(a [32]byte, b [32]byte, c [32]byte, imm8 int) [32]byte


// MaskTernarylogicEpi322: Bitwise ternary logic that provides the capability
// to implement any three-operand binary function; the specific binary function
// is specified by value in 'imm8'. For each bit in each packed 32-bit integer,
// the corresponding bit from 'src', 'a', and 'b' are used to form a 3 bit
// index into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst' using writemask 'k' at 32-bit granularity (32-bit
// elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				FOR h := 0 to 31
//					index[2:0] := (src[i+h] << 2) OR (a[i+h] << 1) OR b[i+h]
//					dst[i+h] := imm8[index[2:0]]
//				ENDFOR
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPTERNLOGD'. Intrinsic: '_mm512_mask_ternarylogic_epi32'.
// Requires AVX512F.
func MaskTernarylogicEpi322(src M512i, k Mmask16, a M512i, b M512i, imm8 int) M512i {
	return M512i(maskTernarylogicEpi322([64]byte(src), uint16(k), [64]byte(a), [64]byte(b), imm8))
}

func maskTernarylogicEpi322(src [64]byte, k uint16, a [64]byte, b [64]byte, imm8 int) [64]byte


// MaskzTernarylogicEpi322: Bitwise ternary logic that provides the capability
// to implement any three-operand binary function; the specific binary function
// is specified by value in 'imm8'. For each bit in each packed 32-bit integer,
// the corresponding bit from 'a', 'b', and 'c' are used to form a 3 bit index
// into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst' using zeromask 'k' at 32-bit granularity (32-bit
// elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				FOR h := 0 to 31
//					index[2:0] := (a[i+h] << 2) OR (b[i+h] << 1) OR c[i+h]
//					dst[i+h] := imm8[index[2:0]]
//				ENDFOR
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPTERNLOGD'. Intrinsic: '_mm512_maskz_ternarylogic_epi32'.
// Requires AVX512F.
func MaskzTernarylogicEpi322(k Mmask16, a M512i, b M512i, c M512i, imm8 int) M512i {
	return M512i(maskzTernarylogicEpi322(uint16(k), [64]byte(a), [64]byte(b), [64]byte(c), imm8))
}

func maskzTernarylogicEpi322(k uint16, a [64]byte, b [64]byte, c [64]byte, imm8 int) [64]byte


// TernarylogicEpi322: Bitwise ternary logic that provides the capability to
// implement any three-operand binary function; the specific binary function is
// specified by value in 'imm8'. For each bit in each packed 32-bit integer,
// the corresponding bit from 'a', 'b', and 'c' are used to form a 3 bit index
// into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			FOR h := 0 to 31
//				index[2:0] := (a[i+h] << 2) OR (b[i+h] << 1) OR c[i+h]
//				dst[i+h] := imm8[index[2:0]]
//			ENDFOR
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPTERNLOGD'. Intrinsic: '_mm512_ternarylogic_epi32'.
// Requires AVX512F.
func TernarylogicEpi322(a M512i, b M512i, c M512i, imm8 int) M512i {
	return M512i(ternarylogicEpi322([64]byte(a), [64]byte(b), [64]byte(c), imm8))
}

func ternarylogicEpi322(a [64]byte, b [64]byte, c [64]byte, imm8 int) [64]byte


// MaskTernarylogicEpi64: Bitwise ternary logic that provides the capability to
// implement any three-operand binary function; the specific binary function is
// specified by value in 'imm8'. For each bit in each packed 64-bit integer,
// the corresponding bit from 'src', 'a', and 'b' are used to form a 3 bit
// index into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst' using writemask 'k' at 64-bit granularity (64-bit
// elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				FOR h := 0 to 63
//					index[2:0] := (src[i+h] << 2) OR (a[i+h] << 1) OR b[i+h]
//					dst[i+h] := imm8[index[2:0]]
//				ENDFOR
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPTERNLOGQ'. Intrinsic: '_mm_mask_ternarylogic_epi64'.
// Requires AVX512F.
func MaskTernarylogicEpi64(src M128i, k Mmask8, a M128i, b M128i, imm8 int) M128i {
	return M128i(maskTernarylogicEpi64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b), imm8))
}

func maskTernarylogicEpi64(src [16]byte, k uint8, a [16]byte, b [16]byte, imm8 int) [16]byte


// MaskzTernarylogicEpi64: Bitwise ternary logic that provides the capability
// to implement any three-operand binary function; the specific binary function
// is specified by value in 'imm8'. For each bit in each packed 64-bit integer,
// the corresponding bit from 'a', 'b', and 'c' are used to form a 3 bit index
// into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst' using zeromask 'k' at 64-bit granularity (64-bit
// elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				FOR h := 0 to 63
//					index[2:0] := (a[i+h] << 2) OR (b[i+h] << 1) OR c[i+h]
//					dst[i+h] := imm8[index[2:0]]
//				ENDFOR
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPTERNLOGQ'. Intrinsic: '_mm_maskz_ternarylogic_epi64'.
// Requires AVX512F.
func MaskzTernarylogicEpi64(k Mmask8, a M128i, b M128i, c M128i, imm8 int) M128i {
	return M128i(maskzTernarylogicEpi64(uint8(k), [16]byte(a), [16]byte(b), [16]byte(c), imm8))
}

func maskzTernarylogicEpi64(k uint8, a [16]byte, b [16]byte, c [16]byte, imm8 int) [16]byte


// TernarylogicEpi64: Bitwise ternary logic that provides the capability to
// implement any three-operand binary function; the specific binary function is
// specified by value in 'imm8'. For each bit in each packed 64-bit integer,
// the corresponding bit from 'a', 'b', and 'c' are used to form a 3 bit index
// into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			FOR h := 0 to 63
//				index[2:0] := (a[i+h] << 2) OR (b[i+h] << 1) OR c[i+h]
//				dst[i+h] := imm8[index[2:0]]
//			ENDFOR
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPTERNLOGQ'. Intrinsic: '_mm_ternarylogic_epi64'.
// Requires AVX512F.
func TernarylogicEpi64(a M128i, b M128i, c M128i, imm8 int) M128i {
	return M128i(ternarylogicEpi64([16]byte(a), [16]byte(b), [16]byte(c), imm8))
}

func ternarylogicEpi64(a [16]byte, b [16]byte, c [16]byte, imm8 int) [16]byte


// MaskTernarylogicEpi641: Bitwise ternary logic that provides the capability
// to implement any three-operand binary function; the specific binary function
// is specified by value in 'imm8'. For each bit in each packed 64-bit integer,
// the corresponding bit from 'src', 'a', and 'b' are used to form a 3 bit
// index into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst' using writemask 'k' at 64-bit granularity (64-bit
// elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				FOR h := 0 to 63
//					index[2:0] := (src[i+h] << 2) OR (a[i+h] << 1) OR b[i+h]
//					dst[i+h] := imm8[index[2:0]]
//				ENDFOR
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPTERNLOGQ'. Intrinsic: '_mm256_mask_ternarylogic_epi64'.
// Requires AVX512F.
func MaskTernarylogicEpi641(src M256i, k Mmask8, a M256i, b M256i, imm8 int) M256i {
	return M256i(maskTernarylogicEpi641([32]byte(src), uint8(k), [32]byte(a), [32]byte(b), imm8))
}

func maskTernarylogicEpi641(src [32]byte, k uint8, a [32]byte, b [32]byte, imm8 int) [32]byte


// MaskzTernarylogicEpi641: Bitwise ternary logic that provides the capability
// to implement any three-operand binary function; the specific binary function
// is specified by value in 'imm8'. For each bit in each packed 64-bit integer,
// the corresponding bit from 'a', 'b', and 'c' are used to form a 3 bit index
// into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst' using zeromask 'k' at 64-bit granularity (64-bit
// elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				FOR h := 0 to 63
//					index[2:0] := (a[i+h] << 2) OR (b[i+h] << 1) OR c[i+h]
//					dst[i+h] := imm8[index[2:0]]
//				ENDFOR
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPTERNLOGQ'. Intrinsic: '_mm256_maskz_ternarylogic_epi64'.
// Requires AVX512F.
func MaskzTernarylogicEpi641(k Mmask8, a M256i, b M256i, c M256i, imm8 int) M256i {
	return M256i(maskzTernarylogicEpi641(uint8(k), [32]byte(a), [32]byte(b), [32]byte(c), imm8))
}

func maskzTernarylogicEpi641(k uint8, a [32]byte, b [32]byte, c [32]byte, imm8 int) [32]byte


// TernarylogicEpi641: Bitwise ternary logic that provides the capability to
// implement any three-operand binary function; the specific binary function is
// specified by value in 'imm8'. For each bit in each packed 64-bit integer,
// the corresponding bit from 'a', 'b', and 'c' are used to form a 3 bit index
// into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			FOR h := 0 to 63
//				index[2:0] := (a[i+h] << 2) OR (b[i+h] << 1) OR c[i+h]
//				dst[i+h] := imm8[index[2:0]]
//			ENDFOR
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPTERNLOGQ'. Intrinsic: '_mm256_ternarylogic_epi64'.
// Requires AVX512F.
func TernarylogicEpi641(a M256i, b M256i, c M256i, imm8 int) M256i {
	return M256i(ternarylogicEpi641([32]byte(a), [32]byte(b), [32]byte(c), imm8))
}

func ternarylogicEpi641(a [32]byte, b [32]byte, c [32]byte, imm8 int) [32]byte


// MaskTernarylogicEpi642: Bitwise ternary logic that provides the capability
// to implement any three-operand binary function; the specific binary function
// is specified by value in 'imm8'. For each bit in each packed 64-bit integer,
// the corresponding bit from 'src', 'a', and 'b' are used to form a 3 bit
// index into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst' using writemask 'k' at 64-bit granularity (64-bit
// elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				FOR h := 0 to 63
//					index[2:0] := (src[i+h] << 2) OR (a[i+h] << 1) OR b[i+h]
//					dst[i+h] := imm8[index[2:0]]
//				ENDFOR
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPTERNLOGQ'. Intrinsic: '_mm512_mask_ternarylogic_epi64'.
// Requires AVX512F.
func MaskTernarylogicEpi642(src M512i, k Mmask8, a M512i, b M512i, imm8 int) M512i {
	return M512i(maskTernarylogicEpi642([64]byte(src), uint8(k), [64]byte(a), [64]byte(b), imm8))
}

func maskTernarylogicEpi642(src [64]byte, k uint8, a [64]byte, b [64]byte, imm8 int) [64]byte


// MaskzTernarylogicEpi642: Bitwise ternary logic that provides the capability
// to implement any three-operand binary function; the specific binary function
// is specified by value in 'imm8'. For each bit in each packed 64-bit integer,
// the corresponding bit from 'a', 'b', and 'c' are used to form a 3 bit index
// into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst' using zeromask 'k' at 64-bit granularity (64-bit
// elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				FOR h := 0 to 63
//					index[2:0] := (a[i+h] << 2) OR (b[i+h] << 1) OR c[i+h]
//					dst[i+h] := imm8[index[2:0]]
//				ENDFOR
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPTERNLOGQ'. Intrinsic: '_mm512_maskz_ternarylogic_epi64'.
// Requires AVX512F.
func MaskzTernarylogicEpi642(k Mmask8, a M512i, b M512i, c M512i, imm8 int) M512i {
	return M512i(maskzTernarylogicEpi642(uint8(k), [64]byte(a), [64]byte(b), [64]byte(c), imm8))
}

func maskzTernarylogicEpi642(k uint8, a [64]byte, b [64]byte, c [64]byte, imm8 int) [64]byte


// TernarylogicEpi642: Bitwise ternary logic that provides the capability to
// implement any three-operand binary function; the specific binary function is
// specified by value in 'imm8'. For each bit in each packed 64-bit integer,
// the corresponding bit from 'a', 'b', and 'c' are used to form a 3 bit index
// into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			FOR h := 0 to 63
//				index[2:0] := (a[i+h] << 2) OR (b[i+h] << 1) OR c[i+h]
//				dst[i+h] := imm8[index[2:0]]
//			ENDFOR
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPTERNLOGQ'. Intrinsic: '_mm512_ternarylogic_epi64'.
// Requires AVX512F.
func TernarylogicEpi642(a M512i, b M512i, c M512i, imm8 int) M512i {
	return M512i(ternarylogicEpi642([64]byte(a), [64]byte(b), [64]byte(c), imm8))
}

func ternarylogicEpi642(a [64]byte, b [64]byte, c [64]byte, imm8 int) [64]byte


// MaskTestEpi32Mask: Compute the bitwise AND of packed 32-bit integers in 'a'
// and 'b', producing intermediate 32-bit values, and set the corresponding bit
// in result mask 'k' (subject to writemask 'k') if the intermediate value is
// non-zero. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ((a[i+31:i] AND b[i+31:i]) != 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPTESTMD'. Intrinsic: '_mm_mask_test_epi32_mask'.
// Requires AVX512F.
func MaskTestEpi32Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskTestEpi32Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskTestEpi32Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// TestEpi32Mask: Compute the bitwise AND of packed 32-bit integers in 'a' and
// 'b', producing intermediate 32-bit values, and set the corresponding bit in
// result mask 'k' if the intermediate value is non-zero. 
//
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ((a[i+31:i] AND b[i+31:i]) != 0) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPTESTMD'. Intrinsic: '_mm_test_epi32_mask'.
// Requires AVX512F.
func TestEpi32Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(testEpi32Mask([16]byte(a), [16]byte(b)))
}

func testEpi32Mask(a [16]byte, b [16]byte) uint8


// MaskTestEpi32Mask1: Compute the bitwise AND of packed 32-bit integers in 'a'
// and 'b', producing intermediate 32-bit values, and set the corresponding bit
// in result mask 'k' (subject to writemask 'k') if the intermediate value is
// non-zero. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ((a[i+31:i] AND b[i+31:i]) != 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPTESTMD'. Intrinsic: '_mm256_mask_test_epi32_mask'.
// Requires AVX512F.
func MaskTestEpi32Mask1(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskTestEpi32Mask1(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskTestEpi32Mask1(k1 uint8, a [32]byte, b [32]byte) uint8


// TestEpi32Mask1: Compute the bitwise AND of packed 32-bit integers in 'a' and
// 'b', producing intermediate 32-bit values, and set the corresponding bit in
// result mask 'k' if the intermediate value is non-zero. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ((a[i+31:i] AND b[i+31:i]) != 0) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPTESTMD'. Intrinsic: '_mm256_test_epi32_mask'.
// Requires AVX512F.
func TestEpi32Mask1(a M256i, b M256i) Mmask8 {
	return Mmask8(testEpi32Mask1([32]byte(a), [32]byte(b)))
}

func testEpi32Mask1(a [32]byte, b [32]byte) uint8


// MaskTestEpi64Mask: Compute the bitwise AND of packed 64-bit integers in 'a'
// and 'b', producing intermediate 64-bit values, and set the corresponding bit
// in result mask 'k' (subject to writemask 'k') if the intermediate value is
// non-zero. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ((a[i+63:i] AND b[i+63:i]) != 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPTESTMQ'. Intrinsic: '_mm_mask_test_epi64_mask'.
// Requires AVX512F.
func MaskTestEpi64Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskTestEpi64Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskTestEpi64Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// TestEpi64Mask: Compute the bitwise AND of packed 64-bit integers in 'a' and
// 'b', producing intermediate 64-bit values, and set the corresponding bit in
// result mask 'k' if the intermediate value is non-zero. 
//
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ((a[i+63:i] AND b[i+63:i]) != 0) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPTESTMQ'. Intrinsic: '_mm_test_epi64_mask'.
// Requires AVX512F.
func TestEpi64Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(testEpi64Mask([16]byte(a), [16]byte(b)))
}

func testEpi64Mask(a [16]byte, b [16]byte) uint8


// MaskTestEpi64Mask1: Compute the bitwise AND of packed 64-bit integers in 'a'
// and 'b', producing intermediate 64-bit values, and set the corresponding bit
// in result mask 'k' (subject to writemask 'k') if the intermediate value is
// non-zero. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ((a[i+63:i] AND b[i+63:i]) != 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPTESTMQ'. Intrinsic: '_mm256_mask_test_epi64_mask'.
// Requires AVX512F.
func MaskTestEpi64Mask1(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskTestEpi64Mask1(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskTestEpi64Mask1(k1 uint8, a [32]byte, b [32]byte) uint8


// TestEpi64Mask1: Compute the bitwise AND of packed 64-bit integers in 'a' and
// 'b', producing intermediate 64-bit values, and set the corresponding bit in
// result mask 'k' if the intermediate value is non-zero. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ((a[i+63:i] AND b[i+63:i]) != 0) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPTESTMQ'. Intrinsic: '_mm256_test_epi64_mask'.
// Requires AVX512F.
func TestEpi64Mask1(a M256i, b M256i) Mmask8 {
	return Mmask8(testEpi64Mask1([32]byte(a), [32]byte(b)))
}

func testEpi64Mask1(a [32]byte, b [32]byte) uint8


// MaskTestEpi64Mask2: Compute the bitwise AND of packed 64-bit integers in 'a'
// and 'b', producing intermediate 64-bit values, and set the corresponding bit
// in result mask 'k' (subject to writemask 'k') if the intermediate value is
// non-zero. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ((a[i+63:i] AND b[i+63:i]) != 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPTESTMQ'. Intrinsic: '_mm512_mask_test_epi64_mask'.
// Requires AVX512F.
func MaskTestEpi64Mask2(k1 Mmask8, a M512i, b M512i) Mmask8 {
	return Mmask8(maskTestEpi64Mask2(uint8(k1), [64]byte(a), [64]byte(b)))
}

func maskTestEpi64Mask2(k1 uint8, a [64]byte, b [64]byte) uint8


// TestEpi64Mask2: Compute the bitwise AND of packed 64-bit integers in 'a' and
// 'b', producing intermediate 64-bit values, and set the corresponding bit in
// result mask 'k' if the intermediate value is non-zero. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ((a[i+63:i] AND b[i+63:i]) != 0) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPTESTMQ'. Intrinsic: '_mm512_test_epi64_mask'.
// Requires AVX512F.
func TestEpi64Mask2(a M512i, b M512i) Mmask8 {
	return Mmask8(testEpi64Mask2([64]byte(a), [64]byte(b)))
}

func testEpi64Mask2(a [64]byte, b [64]byte) uint8


// MaskTestnEpi32Mask: Compute the bitwise NAND of packed 32-bit integers in
// 'a' and 'b', producing intermediate 32-bit values, and set the corresponding
// bit in result mask 'k' (subject to writemask 'k') if the intermediate value
// is zero. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ((a[i+31:i] AND b[i+31:i]) == 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPTESTNMD'. Intrinsic: '_mm_mask_testn_epi32_mask'.
// Requires AVX512F.
func MaskTestnEpi32Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskTestnEpi32Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskTestnEpi32Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// TestnEpi32Mask: Compute the bitwise NAND of packed 32-bit integers in 'a'
// and 'b', producing intermediate 32-bit values, and set the corresponding bit
// in result mask 'k' if the intermediate value is zero. 
//
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ((a[i+31:i] NAND b[i+31:i]) == 0) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPTESTNMD'. Intrinsic: '_mm_testn_epi32_mask'.
// Requires AVX512F.
func TestnEpi32Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(testnEpi32Mask([16]byte(a), [16]byte(b)))
}

func testnEpi32Mask(a [16]byte, b [16]byte) uint8


// MaskTestnEpi32Mask1: Compute the bitwise NAND of packed 32-bit integers in
// 'a' and 'b', producing intermediate 32-bit values, and set the corresponding
// bit in result mask 'k' (subject to writemask 'k') if the intermediate value
// is zero. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ((a[i+31:i] AND b[i+31:i]) == 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPTESTNMD'. Intrinsic: '_mm256_mask_testn_epi32_mask'.
// Requires AVX512F.
func MaskTestnEpi32Mask1(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskTestnEpi32Mask1(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskTestnEpi32Mask1(k1 uint8, a [32]byte, b [32]byte) uint8


// TestnEpi32Mask1: Compute the bitwise NAND of packed 32-bit integers in 'a'
// and 'b', producing intermediate 32-bit values, and set the corresponding bit
// in result mask 'k' if the intermediate value is zero. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ((a[i+31:i] AND b[i+31:i]) == 0) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPTESTNMD'. Intrinsic: '_mm256_testn_epi32_mask'.
// Requires AVX512F.
func TestnEpi32Mask1(a M256i, b M256i) Mmask8 {
	return Mmask8(testnEpi32Mask1([32]byte(a), [32]byte(b)))
}

func testnEpi32Mask1(a [32]byte, b [32]byte) uint8


// MaskTestnEpi32Mask2: Compute the bitwise NAND of packed 32-bit integers in
// 'a' and 'b', producing intermediate 32-bit values, and set the corresponding
// bit in result mask 'k' (subject to writemask 'k') if the intermediate value
// is zero. 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k1[j]
//				k[j] := ((a[i+31:i] AND b[i+31:i]) == 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPTESTNMD'. Intrinsic: '_mm512_mask_testn_epi32_mask'.
// Requires AVX512F.
func MaskTestnEpi32Mask2(k1 Mmask16, a M512i, b M512i) Mmask16 {
	return Mmask16(maskTestnEpi32Mask2(uint16(k1), [64]byte(a), [64]byte(b)))
}

func maskTestnEpi32Mask2(k1 uint16, a [64]byte, b [64]byte) uint16


// TestnEpi32Mask2: Compute the bitwise NAND of packed 32-bit integers in 'a'
// and 'b', producing intermediate 32-bit values, and set the corresponding bit
// in result mask 'k' if the intermediate value is zero. 
//
//		FOR j := 0 to 15
//			i := j*32
//			k[j] := ((a[i+31:i] AND b[i+31:i]) == 0) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPTESTNMD'. Intrinsic: '_mm512_testn_epi32_mask'.
// Requires AVX512F.
func TestnEpi32Mask2(a M512i, b M512i) Mmask16 {
	return Mmask16(testnEpi32Mask2([64]byte(a), [64]byte(b)))
}

func testnEpi32Mask2(a [64]byte, b [64]byte) uint16


// MaskTestnEpi64Mask: Compute the bitwise NAND of packed 64-bit integers in
// 'a' and 'b', producing intermediate 64-bit values, and set the corresponding
// bit in result mask 'k' (subject to writemask 'k') if the intermediate value
// is zero. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ((a[i+63:i] AND b[i+63:i]) == 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPTESTNMQ'. Intrinsic: '_mm_mask_testn_epi64_mask'.
// Requires AVX512F.
func MaskTestnEpi64Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskTestnEpi64Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskTestnEpi64Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// TestnEpi64Mask: Compute the bitwise NAND of packed 64-bit integers in 'a'
// and 'b', producing intermediate 64-bit values, and set the corresponding bit
// in result mask 'k' if the intermediate value is zero. 
//
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ((a[i+63:i] AND b[i+63:i]) == 0) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPTESTNMQ'. Intrinsic: '_mm_testn_epi64_mask'.
// Requires AVX512F.
func TestnEpi64Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(testnEpi64Mask([16]byte(a), [16]byte(b)))
}

func testnEpi64Mask(a [16]byte, b [16]byte) uint8


// MaskTestnEpi64Mask1: Compute the bitwise NAND of packed 64-bit integers in
// 'a' and 'b', producing intermediate 64-bit values, and set the corresponding
// bit in result mask 'k' (subject to writemask 'k') if the intermediate value
// is zero. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ((a[i+63:i] AND b[i+63:i]) == 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPTESTNMQ'. Intrinsic: '_mm256_mask_testn_epi64_mask'.
// Requires AVX512F.
func MaskTestnEpi64Mask1(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskTestnEpi64Mask1(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskTestnEpi64Mask1(k1 uint8, a [32]byte, b [32]byte) uint8


// TestnEpi64Mask1: Compute the bitwise NAND of packed 64-bit integers in 'a'
// and 'b', producing intermediate 64-bit values, and set the corresponding bit
// in result mask 'k' if the intermediate value is zero. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ((a[i+63:i] AND b[i+63:i]) == 0) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPTESTNMQ'. Intrinsic: '_mm256_testn_epi64_mask'.
// Requires AVX512F.
func TestnEpi64Mask1(a M256i, b M256i) Mmask8 {
	return Mmask8(testnEpi64Mask1([32]byte(a), [32]byte(b)))
}

func testnEpi64Mask1(a [32]byte, b [32]byte) uint8


// MaskTestnEpi64Mask2: Compute the bitwise NAND of packed 64-bit integers in
// 'a' and 'b', producing intermediate 64-bit values, and set the corresponding
// bit in result mask 'k' (subject to writemask 'k') if the intermediate value
// is zero. 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k1[j]
//				k[j] := ((a[i+63:i] AND b[i+63:i]) == 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPTESTNMQ'. Intrinsic: '_mm512_mask_testn_epi64_mask'.
// Requires AVX512F.
func MaskTestnEpi64Mask2(k1 Mmask8, a M512i, b M512i) Mmask8 {
	return Mmask8(maskTestnEpi64Mask2(uint8(k1), [64]byte(a), [64]byte(b)))
}

func maskTestnEpi64Mask2(k1 uint8, a [64]byte, b [64]byte) uint8


// TestnEpi64Mask2: Compute the bitwise NAND of packed 64-bit integers in 'a'
// and 'b', producing intermediate 64-bit values, and set the corresponding bit
// in result mask 'k' if the intermediate value is zero. 
//
//		FOR j := 0 to 7
//			i := j*64
//			k[j] := ((a[i+63:i] AND b[i+63:i]) == 0) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPTESTNMQ'. Intrinsic: '_mm512_testn_epi64_mask'.
// Requires AVX512F.
func TestnEpi64Mask2(a M512i, b M512i) Mmask8 {
	return Mmask8(testnEpi64Mask2([64]byte(a), [64]byte(b)))
}

func testnEpi64Mask2(a [64]byte, b [64]byte) uint8


// MaskTruncPd: Truncate the packed double-precision (64-bit) floating-point
// elements in 'a', and store the results as packed double-precision
// floating-point elements in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := TRUNCATE(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_trunc_pd'.
// Requires AVX512F.
func MaskTruncPd(src M512d, k Mmask8, a M512d) M512d {
	return M512d(maskTruncPd([8]float64(src), uint8(k), [8]float64(a)))
}

func maskTruncPd(src [8]float64, k uint8, a [8]float64) [8]float64


// TruncPd: Truncate the packed double-precision (64-bit) floating-point
// elements in 'a', and store the results as packed double-precision
// floating-point elements in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*64
//			dst[i+63:i] := TRUNCATE(a[i+63:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_trunc_pd'.
// Requires AVX512F.
func TruncPd(a M512d) M512d {
	return M512d(truncPd([8]float64(a)))
}

func truncPd(a [8]float64) [8]float64


// MaskTruncPs: Truncate the packed single-precision (32-bit) floating-point
// elements in 'a', and store the results as packed single-precision
// floating-point elements in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := TRUNCATE(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_mask_trunc_ps'.
// Requires AVX512F.
func MaskTruncPs(src M512, k Mmask16, a M512) M512 {
	return M512(maskTruncPs([16]float32(src), uint16(k), [16]float32(a)))
}

func maskTruncPs(src [16]float32, k uint16, a [16]float32) [16]float32


// TruncPs: Truncate the packed single-precision (32-bit) floating-point
// elements in 'a', and store the results as packed single-precision
// floating-point elements in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*32
//			dst[i+31:i] := TRUNCATE(a[i+31:i])
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: '...'. Intrinsic: '_mm512_trunc_ps'.
// Requires AVX512F.
func TruncPs(a M512) M512 {
	return M512(truncPs([16]float32(a)))
}

func truncPs(a [16]float32) [16]float32


// Undefined: Return vector of type __m512 with undefined elements. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_undefined'.
// Requires AVX512F.
func Undefined() M512 {
	return M512(undefined())
}

func undefined() [16]float32


// UndefinedEpi32: Return vector of type __m512i with undefined elements. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_undefined_epi32'.
// Requires AVX512F.
func UndefinedEpi32() M512i {
	return M512i(undefinedEpi32())
}

func undefinedEpi32() [64]byte


// UndefinedPd: Return vector of type __m512d with undefined elements. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_undefined_pd'.
// Requires AVX512F.
func UndefinedPd() M512d {
	return M512d(undefinedPd())
}

func undefinedPd() [8]float64


// UndefinedPs: Return vector of type __m512 with undefined elements. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm512_undefined_ps'.
// Requires AVX512F.
func UndefinedPs() M512 {
	return M512(undefinedPs())
}

func undefinedPs() [16]float32


// MaskUnpackhiEpi32: Unpack and interleave 32-bit integers from the high half
// of 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPUNPCKHDQ'. Intrinsic: '_mm_mask_unpackhi_epi32'.
// Requires AVX512F.
func MaskUnpackhiEpi32(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskUnpackhiEpi32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskUnpackhiEpi32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzUnpackhiEpi32: Unpack and interleave 32-bit integers from the high half
// of 'a' and 'b', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPUNPCKHDQ'. Intrinsic: '_mm_maskz_unpackhi_epi32'.
// Requires AVX512F.
func MaskzUnpackhiEpi32(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzUnpackhiEpi32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzUnpackhiEpi32(k uint8, a [16]byte, b [16]byte) [16]byte


// MaskUnpackhiEpi321: Unpack and interleave 32-bit integers from the high half
// of each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKHDQ'. Intrinsic: '_mm256_mask_unpackhi_epi32'.
// Requires AVX512F.
func MaskUnpackhiEpi321(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskUnpackhiEpi321([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskUnpackhiEpi321(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzUnpackhiEpi321: Unpack and interleave 32-bit integers from the high
// half of each 128-bit lane in 'a' and 'b', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKHDQ'. Intrinsic: '_mm256_maskz_unpackhi_epi32'.
// Requires AVX512F.
func MaskzUnpackhiEpi321(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzUnpackhiEpi321(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzUnpackhiEpi321(k uint8, a [32]byte, b [32]byte) [32]byte


// MaskUnpackhiEpi322: Unpack and interleave 32-bit integers from the high half
// of each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_HIGH_DWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_HIGH_DWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKHDQ'. Intrinsic: '_mm512_mask_unpackhi_epi32'.
// Requires AVX512F.
func MaskUnpackhiEpi322(src M512i, k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskUnpackhiEpi322([64]byte(src), uint16(k), [64]byte(a), [64]byte(b)))
}

func maskUnpackhiEpi322(src [64]byte, k uint16, a [64]byte, b [64]byte) [64]byte


// MaskzUnpackhiEpi322: Unpack and interleave 32-bit integers from the high
// half of each 128-bit lane in 'a' and 'b', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_HIGH_DWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_HIGH_DWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKHDQ'. Intrinsic: '_mm512_maskz_unpackhi_epi32'.
// Requires AVX512F.
func MaskzUnpackhiEpi322(k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskzUnpackhiEpi322(uint16(k), [64]byte(a), [64]byte(b)))
}

func maskzUnpackhiEpi322(k uint16, a [64]byte, b [64]byte) [64]byte


// UnpackhiEpi32: Unpack and interleave 32-bit integers from the high half of
// each 128-bit lane in 'a' and 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}	
//		
//		dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
//		dst[383:256] := INTERLEAVE_HIGH_DWORDS(a[383:256], b[383:256])
//		dst[511:384] := INTERLEAVE_HIGH_DWORDS(a[511:384], b[511:384])
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKHDQ'. Intrinsic: '_mm512_unpackhi_epi32'.
// Requires AVX512F.
func UnpackhiEpi32(a M512i, b M512i) M512i {
	return M512i(unpackhiEpi32([64]byte(a), [64]byte(b)))
}

func unpackhiEpi32(a [64]byte, b [64]byte) [64]byte


// MaskUnpackhiEpi64: Unpack and interleave 64-bit integers from the high half
// of 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPUNPCKHQDQ'. Intrinsic: '_mm_mask_unpackhi_epi64'.
// Requires AVX512F.
func MaskUnpackhiEpi64(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskUnpackhiEpi64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskUnpackhiEpi64(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzUnpackhiEpi64: Unpack and interleave 64-bit integers from the high half
// of 'a' and 'b', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPUNPCKHQDQ'. Intrinsic: '_mm_maskz_unpackhi_epi64'.
// Requires AVX512F.
func MaskzUnpackhiEpi64(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzUnpackhiEpi64(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzUnpackhiEpi64(k uint8, a [16]byte, b [16]byte) [16]byte


// MaskUnpackhiEpi641: Unpack and interleave 64-bit integers from the high half
// of each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKHQDQ'. Intrinsic: '_mm256_mask_unpackhi_epi64'.
// Requires AVX512F.
func MaskUnpackhiEpi641(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskUnpackhiEpi641([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskUnpackhiEpi641(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzUnpackhiEpi641: Unpack and interleave 64-bit integers from the high
// half of each 128-bit lane in 'a' and 'b', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKHQDQ'. Intrinsic: '_mm256_maskz_unpackhi_epi64'.
// Requires AVX512F.
func MaskzUnpackhiEpi641(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzUnpackhiEpi641(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzUnpackhiEpi641(k uint8, a [32]byte, b [32]byte) [32]byte


// MaskUnpackhiEpi642: Unpack and interleave 64-bit integers from the high half
// of each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_HIGH_QWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_HIGH_QWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKHQDQ'. Intrinsic: '_mm512_mask_unpackhi_epi64'.
// Requires AVX512F.
func MaskUnpackhiEpi642(src M512i, k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskUnpackhiEpi642([64]byte(src), uint8(k), [64]byte(a), [64]byte(b)))
}

func maskUnpackhiEpi642(src [64]byte, k uint8, a [64]byte, b [64]byte) [64]byte


// MaskzUnpackhiEpi642: Unpack and interleave 64-bit integers from the high
// half of each 128-bit lane in 'a' and 'b', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_HIGH_QWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_HIGH_QWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKHQDQ'. Intrinsic: '_mm512_maskz_unpackhi_epi64'.
// Requires AVX512F.
func MaskzUnpackhiEpi642(k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskzUnpackhiEpi642(uint8(k), [64]byte(a), [64]byte(b)))
}

func maskzUnpackhiEpi642(k uint8, a [64]byte, b [64]byte) [64]byte


// UnpackhiEpi64: Unpack and interleave 64-bit integers from the high half of
// each 128-bit lane in 'a' and 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
//		dst[383:256] := INTERLEAVE_HIGH_QWORDS(a[383:256], b[383:256])
//		dst[511:384] := INTERLEAVE_HIGH_QWORDS(a[511:384], b[511:384])
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKHQDQ'. Intrinsic: '_mm512_unpackhi_epi64'.
// Requires AVX512F.
func UnpackhiEpi64(a M512i, b M512i) M512i {
	return M512i(unpackhiEpi64([64]byte(a), [64]byte(b)))
}

func unpackhiEpi64(a [64]byte, b [64]byte) [64]byte


// MaskUnpackhiPd: Unpack and interleave double-precision (64-bit)
// floating-point elements from the high half of 'a' and 'b', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VUNPCKHPD'. Intrinsic: '_mm_mask_unpackhi_pd'.
// Requires AVX512F.
func MaskUnpackhiPd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskUnpackhiPd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskUnpackhiPd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzUnpackhiPd: Unpack and interleave double-precision (64-bit)
// floating-point elements from the high half of 'a' and 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VUNPCKHPD'. Intrinsic: '_mm_maskz_unpackhi_pd'.
// Requires AVX512F.
func MaskzUnpackhiPd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzUnpackhiPd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzUnpackhiPd(k uint8, a [2]float64, b [2]float64) [2]float64


// MaskUnpackhiPd1: Unpack and interleave double-precision (64-bit)
// floating-point elements from the high half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VUNPCKHPD'. Intrinsic: '_mm256_mask_unpackhi_pd'.
// Requires AVX512F.
func MaskUnpackhiPd1(src M256d, k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskUnpackhiPd1([4]float64(src), uint8(k), [4]float64(a), [4]float64(b)))
}

func maskUnpackhiPd1(src [4]float64, k uint8, a [4]float64, b [4]float64) [4]float64


// MaskzUnpackhiPd1: Unpack and interleave double-precision (64-bit)
// floating-point elements from the high half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VUNPCKHPD'. Intrinsic: '_mm256_maskz_unpackhi_pd'.
// Requires AVX512F.
func MaskzUnpackhiPd1(k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskzUnpackhiPd1(uint8(k), [4]float64(a), [4]float64(b)))
}

func maskzUnpackhiPd1(k uint8, a [4]float64, b [4]float64) [4]float64


// MaskUnpackhiPd2: Unpack and interleave double-precision (64-bit)
// floating-point elements from the high half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_HIGH_QWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_HIGH_QWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VUNPCKHPD'. Intrinsic: '_mm512_mask_unpackhi_pd'.
// Requires AVX512F.
func MaskUnpackhiPd2(src M512d, k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskUnpackhiPd2([8]float64(src), uint8(k), [8]float64(a), [8]float64(b)))
}

func maskUnpackhiPd2(src [8]float64, k uint8, a [8]float64, b [8]float64) [8]float64


// MaskzUnpackhiPd2: Unpack and interleave double-precision (64-bit)
// floating-point elements from the high half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_HIGH_QWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_HIGH_QWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VUNPCKHPD'. Intrinsic: '_mm512_maskz_unpackhi_pd'.
// Requires AVX512F.
func MaskzUnpackhiPd2(k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskzUnpackhiPd2(uint8(k), [8]float64(a), [8]float64(b)))
}

func maskzUnpackhiPd2(k uint8, a [8]float64, b [8]float64) [8]float64


// UnpackhiPd: Unpack and interleave double-precision (64-bit) floating-point
// elements from the high half of each 128-bit lane in 'a' and 'b', and store
// the results in 'dst'. 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
//		dst[383:256] := INTERLEAVE_HIGH_QWORDS(a[383:256], b[383:256])
//		dst[511:384] := INTERLEAVE_HIGH_QWORDS(a[511:384], b[511:384])
//		dst[MAX:512] := 0
//
// Instruction: 'VUNPCKHPD'. Intrinsic: '_mm512_unpackhi_pd'.
// Requires AVX512F.
func UnpackhiPd(a M512d, b M512d) M512d {
	return M512d(unpackhiPd([8]float64(a), [8]float64(b)))
}

func unpackhiPd(a [8]float64, b [8]float64) [8]float64


// MaskUnpackhiPs: Unpack and interleave single-precision (32-bit)
// floating-point elements from the high half of 'a' and 'b', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VUNPCKHPS'. Intrinsic: '_mm_mask_unpackhi_ps'.
// Requires AVX512F.
func MaskUnpackhiPs(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskUnpackhiPs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskUnpackhiPs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzUnpackhiPs: Unpack and interleave single-precision (32-bit)
// floating-point elements from the high half of 'a' and 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VUNPCKHPS'. Intrinsic: '_mm_maskz_unpackhi_ps'.
// Requires AVX512F.
func MaskzUnpackhiPs(k Mmask8, a M128, b M128) M128 {
	return M128(maskzUnpackhiPs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzUnpackhiPs(k uint8, a [4]float32, b [4]float32) [4]float32


// MaskUnpackhiPs1: Unpack and interleave single-precision (32-bit)
// floating-point elements from the high half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VUNPCKHPS'. Intrinsic: '_mm256_mask_unpackhi_ps'.
// Requires AVX512F.
func MaskUnpackhiPs1(src M256, k Mmask8, a M256, b M256) M256 {
	return M256(maskUnpackhiPs1([8]float32(src), uint8(k), [8]float32(a), [8]float32(b)))
}

func maskUnpackhiPs1(src [8]float32, k uint8, a [8]float32, b [8]float32) [8]float32


// MaskzUnpackhiPs1: Unpack and interleave single-precision (32-bit)
// floating-point elements from the high half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VUNPCKHPS'. Intrinsic: '_mm256_maskz_unpackhi_ps'.
// Requires AVX512F.
func MaskzUnpackhiPs1(k Mmask8, a M256, b M256) M256 {
	return M256(maskzUnpackhiPs1(uint8(k), [8]float32(a), [8]float32(b)))
}

func maskzUnpackhiPs1(k uint8, a [8]float32, b [8]float32) [8]float32


// MaskUnpackhiPs2: Unpack and interleave single-precision (32-bit)
// floating-point elements from the high half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_HIGH_DWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_HIGH_DWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VUNPCKHPS'. Intrinsic: '_mm512_mask_unpackhi_ps'.
// Requires AVX512F.
func MaskUnpackhiPs2(src M512, k Mmask16, a M512, b M512) M512 {
	return M512(maskUnpackhiPs2([16]float32(src), uint16(k), [16]float32(a), [16]float32(b)))
}

func maskUnpackhiPs2(src [16]float32, k uint16, a [16]float32, b [16]float32) [16]float32


// MaskzUnpackhiPs2: Unpack and interleave single-precision (32-bit)
// floating-point elements from the high half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_HIGH_DWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_HIGH_DWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VUNPCKHPS'. Intrinsic: '_mm512_maskz_unpackhi_ps'.
// Requires AVX512F.
func MaskzUnpackhiPs2(k Mmask16, a M512, b M512) M512 {
	return M512(maskzUnpackhiPs2(uint16(k), [16]float32(a), [16]float32(b)))
}

func maskzUnpackhiPs2(k uint16, a [16]float32, b [16]float32) [16]float32


// UnpackhiPs: Unpack and interleave single-precision (32-bit) floating-point
// elements from the high half of each 128-bit lane in 'a' and 'b', and store
// the results in 'dst'. 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}	
//		
//		dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
//		dst[383:256] := INTERLEAVE_HIGH_DWORDS(a[383:256], b[383:256])
//		dst[511:384] := INTERLEAVE_HIGH_DWORDS(a[511:384], b[511:384])
//		dst[MAX:512] := 0
//
// Instruction: 'VUNPCKHPS'. Intrinsic: '_mm512_unpackhi_ps'.
// Requires AVX512F.
func UnpackhiPs(a M512, b M512) M512 {
	return M512(unpackhiPs([16]float32(a), [16]float32(b)))
}

func unpackhiPs(a [16]float32, b [16]float32) [16]float32


// MaskUnpackloEpi32: Unpack and interleave 32-bit integers from the low half
// of 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPUNPCKLDQ'. Intrinsic: '_mm_mask_unpacklo_epi32'.
// Requires AVX512F.
func MaskUnpackloEpi32(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskUnpackloEpi32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskUnpackloEpi32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzUnpackloEpi32: Unpack and interleave 32-bit integers from the low half
// of 'a' and 'b', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPUNPCKLDQ'. Intrinsic: '_mm_maskz_unpacklo_epi32'.
// Requires AVX512F.
func MaskzUnpackloEpi32(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzUnpackloEpi32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzUnpackloEpi32(k uint8, a [16]byte, b [16]byte) [16]byte


// MaskUnpackloEpi321: Unpack and interleave 32-bit integers from the low half
// of each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKLDQ'. Intrinsic: '_mm256_mask_unpacklo_epi32'.
// Requires AVX512F.
func MaskUnpackloEpi321(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskUnpackloEpi321([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskUnpackloEpi321(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzUnpackloEpi321: Unpack and interleave 32-bit integers from the low half
// of each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKLDQ'. Intrinsic: '_mm256_maskz_unpacklo_epi32'.
// Requires AVX512F.
func MaskzUnpackloEpi321(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzUnpackloEpi321(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzUnpackloEpi321(k uint8, a [32]byte, b [32]byte) [32]byte


// MaskUnpackloEpi322: Unpack and interleave 32-bit integers from the low half
// of each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_DWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_DWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKLDQ'. Intrinsic: '_mm512_mask_unpacklo_epi32'.
// Requires AVX512F.
func MaskUnpackloEpi322(src M512i, k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskUnpackloEpi322([64]byte(src), uint16(k), [64]byte(a), [64]byte(b)))
}

func maskUnpackloEpi322(src [64]byte, k uint16, a [64]byte, b [64]byte) [64]byte


// MaskzUnpackloEpi322: Unpack and interleave 32-bit integers from the low half
// of each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_DWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_DWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKLDQ'. Intrinsic: '_mm512_maskz_unpacklo_epi32'.
// Requires AVX512F.
func MaskzUnpackloEpi322(k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskzUnpackloEpi322(uint16(k), [64]byte(a), [64]byte(b)))
}

func maskzUnpackloEpi322(k uint16, a [64]byte, b [64]byte) [64]byte


// UnpackloEpi32: Unpack and interleave 32-bit integers from the low half of
// each 128-bit lane in 'a' and 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
//		dst[383:256] := INTERLEAVE_DWORDS(a[383:256], b[383:256])
//		dst[511:384] := INTERLEAVE_DWORDS(a[511:384], b[511:384])
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKLDQ'. Intrinsic: '_mm512_unpacklo_epi32'.
// Requires AVX512F.
func UnpackloEpi32(a M512i, b M512i) M512i {
	return M512i(unpackloEpi32([64]byte(a), [64]byte(b)))
}

func unpackloEpi32(a [64]byte, b [64]byte) [64]byte


// MaskUnpackloEpi64: Unpack and interleave 64-bit integers from the low half
// of 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPUNPCKLQDQ'. Intrinsic: '_mm_mask_unpacklo_epi64'.
// Requires AVX512F.
func MaskUnpackloEpi64(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskUnpackloEpi64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskUnpackloEpi64(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzUnpackloEpi64: Unpack and interleave 64-bit integers from the low half
// of 'a' and 'b', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPUNPCKLQDQ'. Intrinsic: '_mm_maskz_unpacklo_epi64'.
// Requires AVX512F.
func MaskzUnpackloEpi64(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzUnpackloEpi64(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzUnpackloEpi64(k uint8, a [16]byte, b [16]byte) [16]byte


// MaskUnpackloEpi641: Unpack and interleave 64-bit integers from the low half
// of each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKLQDQ'. Intrinsic: '_mm256_mask_unpacklo_epi64'.
// Requires AVX512F.
func MaskUnpackloEpi641(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskUnpackloEpi641([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskUnpackloEpi641(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzUnpackloEpi641: Unpack and interleave 64-bit integers from the low half
// of each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKLQDQ'. Intrinsic: '_mm256_maskz_unpacklo_epi64'.
// Requires AVX512F.
func MaskzUnpackloEpi641(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzUnpackloEpi641(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzUnpackloEpi641(k uint8, a [32]byte, b [32]byte) [32]byte


// MaskUnpackloEpi642: Unpack and interleave 64-bit integers from the low half
// of each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_QWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_QWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKLQDQ'. Intrinsic: '_mm512_mask_unpacklo_epi64'.
// Requires AVX512F.
func MaskUnpackloEpi642(src M512i, k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskUnpackloEpi642([64]byte(src), uint8(k), [64]byte(a), [64]byte(b)))
}

func maskUnpackloEpi642(src [64]byte, k uint8, a [64]byte, b [64]byte) [64]byte


// MaskzUnpackloEpi642: Unpack and interleave 64-bit integers from the low half
// of each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_QWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_QWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKLQDQ'. Intrinsic: '_mm512_maskz_unpacklo_epi64'.
// Requires AVX512F.
func MaskzUnpackloEpi642(k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskzUnpackloEpi642(uint8(k), [64]byte(a), [64]byte(b)))
}

func maskzUnpackloEpi642(k uint8, a [64]byte, b [64]byte) [64]byte


// UnpackloEpi64: Unpack and interleave 64-bit integers from the low half of
// each 128-bit lane in 'a' and 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
//		dst[383:256] := INTERLEAVE_QWORDS(a[383:256], b[383:256])
//		dst[511:384] := INTERLEAVE_QWORDS(a[511:384], b[511:384])
//		dst[MAX:512] := 0
//
// Instruction: 'VPUNPCKLQDQ'. Intrinsic: '_mm512_unpacklo_epi64'.
// Requires AVX512F.
func UnpackloEpi64(a M512i, b M512i) M512i {
	return M512i(unpackloEpi64([64]byte(a), [64]byte(b)))
}

func unpackloEpi64(a [64]byte, b [64]byte) [64]byte


// MaskUnpackloPd: Unpack and interleave double-precision (64-bit)
// floating-point elements from the low half of 'a' and 'b', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VUNPCKLPD'. Intrinsic: '_mm_mask_unpacklo_pd'.
// Requires AVX512F.
func MaskUnpackloPd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskUnpackloPd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskUnpackloPd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzUnpackloPd: Unpack and interleave double-precision (64-bit)
// floating-point elements from the low half of 'a' and 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VUNPCKLPD'. Intrinsic: '_mm_maskz_unpacklo_pd'.
// Requires AVX512F.
func MaskzUnpackloPd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzUnpackloPd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzUnpackloPd(k uint8, a [2]float64, b [2]float64) [2]float64


// MaskUnpackloPd1: Unpack and interleave double-precision (64-bit)
// floating-point elements from the low half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VUNPCKLPD'. Intrinsic: '_mm256_mask_unpacklo_pd'.
// Requires AVX512F.
func MaskUnpackloPd1(src M256d, k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskUnpackloPd1([4]float64(src), uint8(k), [4]float64(a), [4]float64(b)))
}

func maskUnpackloPd1(src [4]float64, k uint8, a [4]float64, b [4]float64) [4]float64


// MaskzUnpackloPd1: Unpack and interleave double-precision (64-bit)
// floating-point elements from the low half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VUNPCKLPD'. Intrinsic: '_mm256_maskz_unpacklo_pd'.
// Requires AVX512F.
func MaskzUnpackloPd1(k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskzUnpackloPd1(uint8(k), [4]float64(a), [4]float64(b)))
}

func maskzUnpackloPd1(k uint8, a [4]float64, b [4]float64) [4]float64


// MaskUnpackloPd2: Unpack and interleave double-precision (64-bit)
// floating-point elements from the low half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_QWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_QWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VUNPCKLPD'. Intrinsic: '_mm512_mask_unpacklo_pd'.
// Requires AVX512F.
func MaskUnpackloPd2(src M512d, k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskUnpackloPd2([8]float64(src), uint8(k), [8]float64(a), [8]float64(b)))
}

func maskUnpackloPd2(src [8]float64, k uint8, a [8]float64, b [8]float64) [8]float64


// MaskzUnpackloPd2: Unpack and interleave double-precision (64-bit)
// floating-point elements from the low half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_QWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_QWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VUNPCKLPD'. Intrinsic: '_mm512_maskz_unpacklo_pd'.
// Requires AVX512F.
func MaskzUnpackloPd2(k Mmask8, a M512d, b M512d) M512d {
	return M512d(maskzUnpackloPd2(uint8(k), [8]float64(a), [8]float64(b)))
}

func maskzUnpackloPd2(k uint8, a [8]float64, b [8]float64) [8]float64


// UnpackloPd: Unpack and interleave double-precision (64-bit) floating-point
// elements from the low half of each 128-bit lane in 'a' and 'b', and store
// the results in 'dst'. 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
//		dst[383:256] := INTERLEAVE_QWORDS(a[383:256], b[383:256])
//		dst[511:384] := INTERLEAVE_QWORDS(a[511:384], b[511:384])
//		dst[MAX:512] := 0
//
// Instruction: 'VUNPCKLPD'. Intrinsic: '_mm512_unpacklo_pd'.
// Requires AVX512F.
func UnpackloPd(a M512d, b M512d) M512d {
	return M512d(unpackloPd([8]float64(a), [8]float64(b)))
}

func unpackloPd(a [8]float64, b [8]float64) [8]float64


// MaskUnpackloPs: Unpack and interleave single-precision (32-bit)
// floating-point elements from the low half of 'a' and 'b', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VUNPCKLPS'. Intrinsic: '_mm_mask_unpacklo_ps'.
// Requires AVX512F.
func MaskUnpackloPs(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskUnpackloPs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskUnpackloPs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzUnpackloPs: Unpack and interleave single-precision (32-bit)
// floating-point elements from the low half of 'a' and 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VUNPCKLPS'. Intrinsic: '_mm_maskz_unpacklo_ps'.
// Requires AVX512F.
func MaskzUnpackloPs(k Mmask8, a M128, b M128) M128 {
	return M128(maskzUnpackloPs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzUnpackloPs(k uint8, a [4]float32, b [4]float32) [4]float32


// MaskUnpackloPs1: Unpack and interleave single-precision (32-bit)
// floating-point elements from the low half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VUNPCKLPS'. Intrinsic: '_mm256_mask_unpacklo_ps'.
// Requires AVX512F.
func MaskUnpackloPs1(src M256, k Mmask8, a M256, b M256) M256 {
	return M256(maskUnpackloPs1([8]float32(src), uint8(k), [8]float32(a), [8]float32(b)))
}

func maskUnpackloPs1(src [8]float32, k uint8, a [8]float32, b [8]float32) [8]float32


// MaskzUnpackloPs1: Unpack and interleave single-precision (32-bit)
// floating-point elements from the low half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VUNPCKLPS'. Intrinsic: '_mm256_maskz_unpacklo_ps'.
// Requires AVX512F.
func MaskzUnpackloPs1(k Mmask8, a M256, b M256) M256 {
	return M256(maskzUnpackloPs1(uint8(k), [8]float32(a), [8]float32(b)))
}

func maskzUnpackloPs1(k uint8, a [8]float32, b [8]float32) [8]float32


// MaskUnpackloPs2: Unpack and interleave single-precision (32-bit)
// floating-point elements from the low half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_DWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_DWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VUNPCKLPS'. Intrinsic: '_mm512_mask_unpacklo_ps'.
// Requires AVX512F.
func MaskUnpackloPs2(src M512, k Mmask16, a M512, b M512) M512 {
	return M512(maskUnpackloPs2([16]float32(src), uint16(k), [16]float32(a), [16]float32(b)))
}

func maskUnpackloPs2(src [16]float32, k uint16, a [16]float32, b [16]float32) [16]float32


// MaskzUnpackloPs2: Unpack and interleave single-precision (32-bit)
// floating-point elements from the low half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
//		tmp_dst[383:256] := INTERLEAVE_DWORDS(a[383:256], b[383:256])
//		tmp_dst[511:384] := INTERLEAVE_DWORDS(a[511:384], b[511:384])
//		
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VUNPCKLPS'. Intrinsic: '_mm512_maskz_unpacklo_ps'.
// Requires AVX512F.
func MaskzUnpackloPs2(k Mmask16, a M512, b M512) M512 {
	return M512(maskzUnpackloPs2(uint16(k), [16]float32(a), [16]float32(b)))
}

func maskzUnpackloPs2(k uint16, a [16]float32, b [16]float32) [16]float32


// UnpackloPs: Unpack and interleave single-precision (32-bit) floating-point
// elements from the low half of each 128-bit lane in 'a' and 'b', and store
// the results in 'dst'. 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
//		dst[383:256] := INTERLEAVE_DWORDS(a[383:256], b[383:256])
//		dst[511:384] := INTERLEAVE_DWORDS(a[511:384], b[511:384])
//		dst[MAX:512] := 0
//
// Instruction: 'VUNPCKLPS'. Intrinsic: '_mm512_unpacklo_ps'.
// Requires AVX512F.
func UnpackloPs(a M512, b M512) M512 {
	return M512(unpackloPs([16]float32(a), [16]float32(b)))
}

func unpackloPs(a [16]float32, b [16]float32) [16]float32


// MaskXorEpi32: Compute the bitwise XOR of packed 32-bit integers in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPXORD'. Intrinsic: '_mm_mask_xor_epi32'.
// Requires AVX512F.
func MaskXorEpi32(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskXorEpi32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskXorEpi32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzXorEpi32: Compute the bitwise XOR of packed 32-bit integers in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPXORD'. Intrinsic: '_mm_maskz_xor_epi32'.
// Requires AVX512F.
func MaskzXorEpi32(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzXorEpi32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzXorEpi32(k uint8, a [16]byte, b [16]byte) [16]byte


// MaskXorEpi321: Compute the bitwise XOR of packed 32-bit integers in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPXORD'. Intrinsic: '_mm256_mask_xor_epi32'.
// Requires AVX512F.
func MaskXorEpi321(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskXorEpi321([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskXorEpi321(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzXorEpi321: Compute the bitwise XOR of packed 32-bit integers in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPXORD'. Intrinsic: '_mm256_maskz_xor_epi32'.
// Requires AVX512F.
func MaskzXorEpi321(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzXorEpi321(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzXorEpi321(k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzXorEpi322: Compute the bitwise XOR of packed 32-bit integers in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPXORD'. Intrinsic: '_mm512_maskz_xor_epi32'.
// Requires AVX512F.
func MaskzXorEpi322(k Mmask16, a M512i, b M512i) M512i {
	return M512i(maskzXorEpi322(uint16(k), [64]byte(a), [64]byte(b)))
}

func maskzXorEpi322(k uint16, a [64]byte, b [64]byte) [64]byte


// MaskXorEpi64: Compute the bitwise XOR of packed 64-bit integers in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPXORQ'. Intrinsic: '_mm_mask_xor_epi64'.
// Requires AVX512F.
func MaskXorEpi64(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskXorEpi64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskXorEpi64(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzXorEpi64: Compute the bitwise XOR of packed 64-bit integers in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPXORQ'. Intrinsic: '_mm_maskz_xor_epi64'.
// Requires AVX512F.
func MaskzXorEpi64(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzXorEpi64(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzXorEpi64(k uint8, a [16]byte, b [16]byte) [16]byte


// MaskXorEpi641: Compute the bitwise XOR of packed 64-bit integers in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPXORQ'. Intrinsic: '_mm256_mask_xor_epi64'.
// Requires AVX512F.
func MaskXorEpi641(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskXorEpi641([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskXorEpi641(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzXorEpi641: Compute the bitwise XOR of packed 64-bit integers in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPXORQ'. Intrinsic: '_mm256_maskz_xor_epi64'.
// Requires AVX512F.
func MaskzXorEpi641(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzXorEpi641(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzXorEpi641(k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzXorEpi642: Compute the bitwise XOR of packed 64-bit integers in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:512] := 0
//
// Instruction: 'VPXORQ'. Intrinsic: '_mm512_maskz_xor_epi64'.
// Requires AVX512F.
func MaskzXorEpi642(k Mmask8, a M512i, b M512i) M512i {
	return M512i(maskzXorEpi642(uint8(k), [64]byte(a), [64]byte(b)))
}

func maskzXorEpi642(k uint8, a [64]byte, b [64]byte) [64]byte

