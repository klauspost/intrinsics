package m128

import . "github.com/klauspost/intrinsics/x86"

// Abs16: Compute the absolute value of packed 16-bit integers in 'a', and
// store the unsigned results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			dst[i+15:i] := ABS(a[i+15:i])
//		ENDFOR
//
// Instruction: 'PABSW'. Intrinsic: '_mm_abs_epi16'.
// Requires SSSE3.
func Abs16(a M128i) M128i {
	return M128i(abs16([16]byte(a)))
}

func abs16(a [16]byte) [16]byte


// MaskAbs16: Compute the absolute value of packed 16-bit integers in 'a', and
// store the unsigned results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := ABS(a[i+15:i])
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPABSW'. Intrinsic: '_mm_mask_abs_epi16'.
// Requires AVX512BW.
func MaskAbs16(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskAbs16([16]byte(src), uint8(k), [16]byte(a)))
}

func maskAbs16(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzAbs16: Compute the absolute value of packed 16-bit integers in 'a', and
// store the unsigned results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := ABS(a[i+15:i])
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPABSW'. Intrinsic: '_mm_maskz_abs_epi16'.
// Requires AVX512BW.
func MaskzAbs16(k Mmask8, a M128i) M128i {
	return M128i(maskzAbs16(uint8(k), [16]byte(a)))
}

func maskzAbs16(k uint8, a [16]byte) [16]byte


// Abs32: Compute the absolute value of packed 32-bit integers in 'a', and
// store the unsigned results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := ABS(a[i+31:i])
//		ENDFOR
//
// Instruction: 'PABSD'. Intrinsic: '_mm_abs_epi32'.
// Requires SSSE3.
func Abs32(a M128i) M128i {
	return M128i(abs32([16]byte(a)))
}

func abs32(a [16]byte) [16]byte


// MaskAbs32: Compute the absolute value of packed 32-bit integers in 'a', and
// store the unsigned results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ABS(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPABSD'. Intrinsic: '_mm_mask_abs_epi32'.
// Requires AVX512F.
func MaskAbs32(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskAbs32([16]byte(src), uint8(k), [16]byte(a)))
}

func maskAbs32(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzAbs32: Compute the absolute value of packed 32-bit integers in 'a', and
// store the unsigned results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ABS(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPABSD'. Intrinsic: '_mm_maskz_abs_epi32'.
// Requires AVX512F.
func MaskzAbs32(k Mmask8, a M128i) M128i {
	return M128i(maskzAbs32(uint8(k), [16]byte(a)))
}

func maskzAbs32(k uint8, a [16]byte) [16]byte


// Abs64: Compute the absolute value of packed 64-bit integers in 'a', and
// store the unsigned results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := ABS(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPABSQ'. Intrinsic: '_mm_abs_epi64'.
// Requires AVX512F.
func Abs64(a M128i) M128i {
	return M128i(abs64([16]byte(a)))
}

func abs64(a [16]byte) [16]byte


// MaskAbs64: Compute the absolute value of packed 64-bit integers in 'a', and
// store the unsigned results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ABS(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPABSQ'. Intrinsic: '_mm_mask_abs_epi64'.
// Requires AVX512F.
func MaskAbs64(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskAbs64([16]byte(src), uint8(k), [16]byte(a)))
}

func maskAbs64(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzAbs64: Compute the absolute value of packed 64-bit integers in 'a', and
// store the unsigned results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ABS(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPABSQ'. Intrinsic: '_mm_maskz_abs_epi64'.
// Requires AVX512F.
func MaskzAbs64(k Mmask8, a M128i) M128i {
	return M128i(maskzAbs64(uint8(k), [16]byte(a)))
}

func maskzAbs64(k uint8, a [16]byte) [16]byte


// Abs8: Compute the absolute value of packed 8-bit integers in 'a', and store
// the unsigned results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*8
//			dst[i+7:i] := ABS(a[i+7:i])
//		ENDFOR
//
// Instruction: 'PABSB'. Intrinsic: '_mm_abs_epi8'.
// Requires SSSE3.
func Abs8(a M128i) M128i {
	return M128i(abs8([16]byte(a)))
}

func abs8(a [16]byte) [16]byte


// MaskAbs8: Compute the absolute value of packed 8-bit integers in 'a', and
// store the unsigned results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := ABS(a[i+7:i])
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPABSB'. Intrinsic: '_mm_mask_abs_epi8'.
// Requires AVX512BW.
func MaskAbs8(src M128i, k Mmask16, a M128i) M128i {
	return M128i(maskAbs8([16]byte(src), uint16(k), [16]byte(a)))
}

func maskAbs8(src [16]byte, k uint16, a [16]byte) [16]byte


// MaskzAbs8: Compute the absolute value of packed 8-bit integers in 'a', and
// store the unsigned results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := ABS(a[i+7:i])
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPABSB'. Intrinsic: '_mm_maskz_abs_epi8'.
// Requires AVX512BW.
func MaskzAbs8(k Mmask16, a M128i) M128i {
	return M128i(maskzAbs8(uint16(k), [16]byte(a)))
}

func maskzAbs8(k uint16, a [16]byte) [16]byte


// AcosPd: Compute the inverse cosine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := ACOS(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_acos_pd'.
// Requires SSE.
func AcosPd(a M128d) M128d {
	return M128d(acosPd([2]float64(a)))
}

func acosPd(a [2]float64) [2]float64


// AcosPs: Compute the inverse cosine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := ACOS(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_acos_ps'.
// Requires SSE.
func AcosPs(a M128) M128 {
	return M128(acosPs([4]float32(a)))
}

func acosPs(a [4]float32) [4]float32


// AcoshPd: Compute the inverse hyperbolic cosine of packed double-precision
// (64-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := ACOSH(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_acosh_pd'.
// Requires SSE.
func AcoshPd(a M128d) M128d {
	return M128d(acoshPd([2]float64(a)))
}

func acoshPd(a [2]float64) [2]float64


// AcoshPs: Compute the inverse hyperbolic cosine of packed single-precision
// (32-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := ACOSH(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_acosh_ps'.
// Requires SSE.
func AcoshPs(a M128) M128 {
	return M128(acoshPs([4]float32(a)))
}

func acoshPs(a [4]float32) [4]float32


// Add16: Add packed 16-bit integers in 'a' and 'b', and store the results in
// 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			dst[i+15:i] := a[i+15:i] + b[i+15:i]
//		ENDFOR
//
// Instruction: 'PADDW'. Intrinsic: '_mm_add_epi16'.
// Requires SSE2.
func Add16(a M128i, b M128i) M128i {
	return M128i(add16([16]byte(a), [16]byte(b)))
}

func add16(a [16]byte, b [16]byte) [16]byte


// MaskAdd16: Add packed 16-bit integers in 'a' and 'b', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := a[i+15:i] + b[i+15:i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPADDW'. Intrinsic: '_mm_mask_add_epi16'.
// Requires AVX512BW.
func MaskAdd16(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskAdd16([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskAdd16(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzAdd16: Add packed 16-bit integers in 'a' and 'b', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := a[i+15:i] + b[i+15:i]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPADDW'. Intrinsic: '_mm_maskz_add_epi16'.
// Requires AVX512BW.
func MaskzAdd16(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzAdd16(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzAdd16(k uint8, a [16]byte, b [16]byte) [16]byte


// Add32: Add packed 32-bit integers in 'a' and 'b', and store the results in
// 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := a[i+31:i] + b[i+31:i]
//		ENDFOR
//
// Instruction: 'PADDD'. Intrinsic: '_mm_add_epi32'.
// Requires SSE2.
func Add32(a M128i, b M128i) M128i {
	return M128i(add32([16]byte(a), [16]byte(b)))
}

func add32(a [16]byte, b [16]byte) [16]byte


// MaskAdd32: Add packed 32-bit integers in 'a' and 'b', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] + b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPADDD'. Intrinsic: '_mm_mask_add_epi32'.
// Requires AVX512F.
func MaskAdd32(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskAdd32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskAdd32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzAdd32: Add packed 32-bit integers in 'a' and 'b', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] + b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPADDD'. Intrinsic: '_mm_maskz_add_epi32'.
// Requires AVX512F.
func MaskzAdd32(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzAdd32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzAdd32(k uint8, a [16]byte, b [16]byte) [16]byte


// Add64: Add packed 64-bit integers in 'a' and 'b', and store the results in
// 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := a[i+63:i] + b[i+63:i]
//		ENDFOR
//
// Instruction: 'PADDQ'. Intrinsic: '_mm_add_epi64'.
// Requires SSE2.
func Add64(a M128i, b M128i) M128i {
	return M128i(add64([16]byte(a), [16]byte(b)))
}

func add64(a [16]byte, b [16]byte) [16]byte


// MaskAdd64: Add packed 64-bit integers in 'a' and 'b', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] + b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPADDQ'. Intrinsic: '_mm_mask_add_epi64'.
// Requires AVX512F.
func MaskAdd64(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskAdd64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskAdd64(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzAdd64: Add packed 64-bit integers in 'a' and 'b', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] + b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPADDQ'. Intrinsic: '_mm_maskz_add_epi64'.
// Requires AVX512F.
func MaskzAdd64(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzAdd64(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzAdd64(k uint8, a [16]byte, b [16]byte) [16]byte


// Add8: Add packed 8-bit integers in 'a' and 'b', and store the results in
// 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*8
//			dst[i+7:i] := a[i+7:i] + b[i+7:i]
//		ENDFOR
//
// Instruction: 'PADDB'. Intrinsic: '_mm_add_epi8'.
// Requires SSE2.
func Add8(a M128i, b M128i) M128i {
	return M128i(add8([16]byte(a), [16]byte(b)))
}

func add8(a [16]byte, b [16]byte) [16]byte


// MaskAdd8: Add packed 8-bit integers in 'a' and 'b', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := a[i+7:i] + b[i+7:i]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPADDB'. Intrinsic: '_mm_mask_add_epi8'.
// Requires AVX512BW.
func MaskAdd8(src M128i, k Mmask16, a M128i, b M128i) M128i {
	return M128i(maskAdd8([16]byte(src), uint16(k), [16]byte(a), [16]byte(b)))
}

func maskAdd8(src [16]byte, k uint16, a [16]byte, b [16]byte) [16]byte


// MaskzAdd8: Add packed 8-bit integers in 'a' and 'b', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := a[i+7:i] + b[i+7:i]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPADDB'. Intrinsic: '_mm_maskz_add_epi8'.
// Requires AVX512BW.
func MaskzAdd8(k Mmask16, a M128i, b M128i) M128i {
	return M128i(maskzAdd8(uint16(k), [16]byte(a), [16]byte(b)))
}

func maskzAdd8(k uint16, a [16]byte, b [16]byte) [16]byte


// AddPd: Add packed double-precision (64-bit) floating-point elements in 'a'
// and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := a[i+63:i] + b[i+63:i]
//		ENDFOR
//
// Instruction: 'ADDPD'. Intrinsic: '_mm_add_pd'.
// Requires SSE2.
func AddPd(a M128d, b M128d) M128d {
	return M128d(addPd([2]float64(a), [2]float64(b)))
}

func addPd(a [2]float64, b [2]float64) [2]float64


// MaskAddPd: Add packed double-precision (64-bit) floating-point elements in
// 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] + b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VADDPD'. Intrinsic: '_mm_mask_add_pd'.
// Requires AVX512VL.
func MaskAddPd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskAddPd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskAddPd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzAddPd: Add packed double-precision (64-bit) floating-point elements in
// 'a' and 'b', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] + b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VADDPD'. Intrinsic: '_mm_maskz_add_pd'.
// Requires AVX512VL.
func MaskzAddPd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzAddPd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzAddPd(k uint8, a [2]float64, b [2]float64) [2]float64


// AddPs: Add packed single-precision (32-bit) floating-point elements in 'a'
// and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := a[i+31:i] + b[i+31:i]
//		ENDFOR
//
// Instruction: 'ADDPS'. Intrinsic: '_mm_add_ps'.
// Requires SSE.
func AddPs(a M128, b M128) M128 {
	return M128(addPs([4]float32(a), [4]float32(b)))
}

func addPs(a [4]float32, b [4]float32) [4]float32


// MaskAddPs: Add packed single-precision (32-bit) floating-point elements in
// 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] + b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VADDPS'. Intrinsic: '_mm_mask_add_ps'.
// Requires AVX512VL.
func MaskAddPs(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskAddPs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskAddPs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzAddPs: Add packed single-precision (32-bit) floating-point elements in
// 'a' and 'b', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] + b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VADDPS'. Intrinsic: '_mm_maskz_add_ps'.
// Requires AVX512VL.
func MaskzAddPs(k Mmask8, a M128, b M128) M128 {
	return M128(maskzAddPs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzAddPs(k uint8, a [4]float32, b [4]float32) [4]float32


// AddRoundSd: Add the lower double-precision (64-bit) floating-point element
// in 'a' and 'b', store the result in the lower element of 'dst', and copy the
// upper element from 'a' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := a[63:0] + b[63:0]
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VADDSD'. Intrinsic: '_mm_add_round_sd'.
// Requires AVX512F.
func AddRoundSd(a M128d, b M128d, rounding int) M128d {
	return M128d(addRoundSd([2]float64(a), [2]float64(b), rounding))
}

func addRoundSd(a [2]float64, b [2]float64, rounding int) [2]float64


// MaskAddRoundSd: Add the lower double-precision (64-bit) floating-point
// element in 'a' and 'b', store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'src' when mask bit 0 is not set),
// and copy the upper element from 'a' to the upper element of 'dst'.
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := a[63:0] + b[63:0]
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VADDSD'. Intrinsic: '_mm_mask_add_round_sd'.
// Requires AVX512F.
func MaskAddRoundSd(src M128d, k Mmask8, a M128d, b M128d, rounding int) M128d {
	return M128d(maskAddRoundSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), rounding))
}

func maskAddRoundSd(src [2]float64, k uint8, a [2]float64, b [2]float64, rounding int) [2]float64


// MaskzAddRoundSd: Add the lower double-precision (64-bit) floating-point
// element in 'a' and 'b', store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper element from 'a' to the upper element of 'dst'.
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := a[63:0] + b[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VADDSD'. Intrinsic: '_mm_maskz_add_round_sd'.
// Requires AVX512F.
func MaskzAddRoundSd(k Mmask8, a M128d, b M128d, rounding int) M128d {
	return M128d(maskzAddRoundSd(uint8(k), [2]float64(a), [2]float64(b), rounding))
}

func maskzAddRoundSd(k uint8, a [2]float64, b [2]float64, rounding int) [2]float64


// AddRoundSs: Add the lower single-precision (32-bit) floating-point element
// in 'a' and 'b', store the result in the lower element of 'dst', and copy the
// upper 3 packed elements from 'a' to the upper elements of 'dst'.
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := a[31:0] + b[31:0]
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VADDSS'. Intrinsic: '_mm_add_round_ss'.
// Requires AVX512F.
func AddRoundSs(a M128, b M128, rounding int) M128 {
	return M128(addRoundSs([4]float32(a), [4]float32(b), rounding))
}

func addRoundSs(a [4]float32, b [4]float32, rounding int) [4]float32


// MaskAddRoundSs: Add the lower single-precision (32-bit) floating-point
// element in 'a' and 'b', store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'src' when mask bit 0 is not set),
// and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'. 
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := a[31:0] + b[31:0]
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VADDSS'. Intrinsic: '_mm_mask_add_round_ss'.
// Requires AVX512F.
func MaskAddRoundSs(src M128, k Mmask8, a M128, b M128, rounding int) M128 {
	return M128(maskAddRoundSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), rounding))
}

func maskAddRoundSs(src [4]float32, k uint8, a [4]float32, b [4]float32, rounding int) [4]float32


// MaskzAddRoundSs: Add the lower single-precision (32-bit) floating-point
// element in 'a' and 'b', store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := a[31:0] + b[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VADDSS'. Intrinsic: '_mm_maskz_add_round_ss'.
// Requires AVX512F.
func MaskzAddRoundSs(k Mmask8, a M128, b M128, rounding int) M128 {
	return M128(maskzAddRoundSs(uint8(k), [4]float32(a), [4]float32(b), rounding))
}

func maskzAddRoundSs(k uint8, a [4]float32, b [4]float32, rounding int) [4]float32


// AddSd: Add the lower double-precision (64-bit) floating-point element in 'a'
// and 'b', store the result in the lower element of 'dst', and copy the upper
// element from 'a' to the upper element of 'dst'. 
//
//		dst[63:0] := a[63:0] + b[63:0]
//		dst[127:64] := a[127:64]
//
// Instruction: 'ADDSD'. Intrinsic: '_mm_add_sd'.
// Requires SSE2.
func AddSd(a M128d, b M128d) M128d {
	return M128d(addSd([2]float64(a), [2]float64(b)))
}

func addSd(a [2]float64, b [2]float64) [2]float64


// MaskAddSd: Add the lower double-precision (64-bit) floating-point element in
// 'a' and 'b', store the result in the lower element of 'dst' using writemask
// 'k' (the element is copied from 'src' when mask bit 0 is not set), and copy
// the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := a[63:0] + b[63:0]
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VADDSD'. Intrinsic: '_mm_mask_add_sd'.
// Requires AVX512F.
func MaskAddSd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskAddSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskAddSd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzAddSd: Add the lower double-precision (64-bit) floating-point element
// in 'a' and 'b', store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := a[63:0] + b[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VADDSD'. Intrinsic: '_mm_maskz_add_sd'.
// Requires AVX512F.
func MaskzAddSd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzAddSd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzAddSd(k uint8, a [2]float64, b [2]float64) [2]float64


// AddSs: Add the lower single-precision (32-bit) floating-point element in 'a'
// and 'b', store the result in the lower element of 'dst', and copy the upper
// 3 packed elements from 'a' to the upper elements of 'dst'. 
//
//		dst[31:0] := a[31:0] + b[31:0]
//		dst[127:32] := a[127:32]
//
// Instruction: 'ADDSS'. Intrinsic: '_mm_add_ss'.
// Requires SSE.
func AddSs(a M128, b M128) M128 {
	return M128(addSs([4]float32(a), [4]float32(b)))
}

func addSs(a [4]float32, b [4]float32) [4]float32


// MaskAddSs: Add the lower single-precision (32-bit) floating-point element in
// 'a' and 'b', store the result in the lower element of 'dst' using writemask
// 'k' (the element is copied from 'src' when mask bit 0 is not set), and copy
// the upper 3 packed elements from 'a' to the upper elements of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := a[31:0] + b[31:0]
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VADDSS'. Intrinsic: '_mm_mask_add_ss'.
// Requires AVX512F.
func MaskAddSs(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskAddSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskAddSs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzAddSs: Add the lower single-precision (32-bit) floating-point element
// in 'a' and 'b', store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := a[31:0] + b[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VADDSS'. Intrinsic: '_mm_maskz_add_ss'.
// Requires AVX512F.
func MaskzAddSs(k Mmask8, a M128, b M128) M128 {
	return M128(maskzAddSs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzAddSs(k uint8, a [4]float32, b [4]float32) [4]float32


// Adds16: Add packed 16-bit integers in 'a' and 'b' using saturation, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			dst[i+15:i] := Saturate_To_Int16( a[i+15:i] + b[i+15:i] )
//		ENDFOR
//
// Instruction: 'PADDSW'. Intrinsic: '_mm_adds_epi16'.
// Requires SSE2.
func Adds16(a M128i, b M128i) M128i {
	return M128i(adds16([16]byte(a), [16]byte(b)))
}

func adds16(a [16]byte, b [16]byte) [16]byte


// MaskAdds16: Add packed 16-bit integers in 'a' and 'b' using saturation, and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := Saturate_To_Int16( a[i+15:i] + b[i+15:i] )
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPADDSW'. Intrinsic: '_mm_mask_adds_epi16'.
// Requires AVX512BW.
func MaskAdds16(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskAdds16([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskAdds16(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzAdds16: Add packed 16-bit integers in 'a' and 'b' using saturation, and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := Saturate_To_Int16( a[i+15:i] + b[i+15:i] )
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPADDSW'. Intrinsic: '_mm_maskz_adds_epi16'.
// Requires AVX512BW.
func MaskzAdds16(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzAdds16(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzAdds16(k uint8, a [16]byte, b [16]byte) [16]byte


// Adds8: Add packed 8-bit integers in 'a' and 'b' using saturation, and store
// the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*8
//			dst[i+7:i] := Saturate_To_Int8( a[i+7:i] + b[i+7:i] )
//		ENDFOR
//
// Instruction: 'PADDSB'. Intrinsic: '_mm_adds_epi8'.
// Requires SSE2.
func Adds8(a M128i, b M128i) M128i {
	return M128i(adds8([16]byte(a), [16]byte(b)))
}

func adds8(a [16]byte, b [16]byte) [16]byte


// MaskAdds8: Add packed 8-bit integers in 'a' and 'b' using saturation, and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := Saturate_To_Int8( a[i+7:i] + b[i+7:i] )
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPADDSB'. Intrinsic: '_mm_mask_adds_epi8'.
// Requires AVX512BW.
func MaskAdds8(src M128i, k Mmask16, a M128i, b M128i) M128i {
	return M128i(maskAdds8([16]byte(src), uint16(k), [16]byte(a), [16]byte(b)))
}

func maskAdds8(src [16]byte, k uint16, a [16]byte, b [16]byte) [16]byte


// MaskzAdds8: Add packed 8-bit integers in 'a' and 'b' using saturation, and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := Saturate_To_Int8( a[i+7:i] + b[i+7:i] )
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPADDSB'. Intrinsic: '_mm_maskz_adds_epi8'.
// Requires AVX512BW.
func MaskzAdds8(k Mmask16, a M128i, b M128i) M128i {
	return M128i(maskzAdds8(uint16(k), [16]byte(a), [16]byte(b)))
}

func maskzAdds8(k uint16, a [16]byte, b [16]byte) [16]byte


// AddsEpu16: Add packed unsigned 16-bit integers in 'a' and 'b' using
// saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			dst[i+15:i] := Saturate_To_UnsignedInt16( a[i+15:i] + b[i+15:i] )
//		ENDFOR
//
// Instruction: 'PADDUSW'. Intrinsic: '_mm_adds_epu16'.
// Requires SSE2.
func AddsEpu16(a M128i, b M128i) M128i {
	return M128i(addsEpu16([16]byte(a), [16]byte(b)))
}

func addsEpu16(a [16]byte, b [16]byte) [16]byte


// MaskAddsEpu16: Add packed unsigned 16-bit integers in 'a' and 'b' using
// saturation, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := Saturate_To_UnsignedInt16( a[i+15:i] + b[i+15:i] )
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPADDUSW'. Intrinsic: '_mm_mask_adds_epu16'.
// Requires AVX512BW.
func MaskAddsEpu16(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskAddsEpu16([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskAddsEpu16(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzAddsEpu16: Add packed unsigned 16-bit integers in 'a' and 'b' using
// saturation, and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := Saturate_To_UnsignedInt16( a[i+15:i] + b[i+15:i] )
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPADDUSW'. Intrinsic: '_mm_maskz_adds_epu16'.
// Requires AVX512BW.
func MaskzAddsEpu16(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzAddsEpu16(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzAddsEpu16(k uint8, a [16]byte, b [16]byte) [16]byte


// AddsEpu8: Add packed unsigned 8-bit integers in 'a' and 'b' using
// saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*8
//			dst[i+7:i] := Saturate_To_UnsignedInt8( a[i+7:i] + b[i+7:i] )
//		ENDFOR
//
// Instruction: 'PADDUSB'. Intrinsic: '_mm_adds_epu8'.
// Requires SSE2.
func AddsEpu8(a M128i, b M128i) M128i {
	return M128i(addsEpu8([16]byte(a), [16]byte(b)))
}

func addsEpu8(a [16]byte, b [16]byte) [16]byte


// MaskAddsEpu8: Add packed unsigned 8-bit integers in 'a' and 'b' using
// saturation, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := Saturate_To_UnsignedInt8( a[i+7:i] + b[i+7:i] )
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPADDUSB'. Intrinsic: '_mm_mask_adds_epu8'.
// Requires AVX512BW.
func MaskAddsEpu8(src M128i, k Mmask16, a M128i, b M128i) M128i {
	return M128i(maskAddsEpu8([16]byte(src), uint16(k), [16]byte(a), [16]byte(b)))
}

func maskAddsEpu8(src [16]byte, k uint16, a [16]byte, b [16]byte) [16]byte


// MaskzAddsEpu8: Add packed unsigned 8-bit integers in 'a' and 'b' using
// saturation, and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := Saturate_To_UnsignedInt8( a[i+7:i] + b[i+7:i] )
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPADDUSB'. Intrinsic: '_mm_maskz_adds_epu8'.
// Requires AVX512BW.
func MaskzAddsEpu8(k Mmask16, a M128i, b M128i) M128i {
	return M128i(maskzAddsEpu8(uint16(k), [16]byte(a), [16]byte(b)))
}

func maskzAddsEpu8(k uint16, a [16]byte, b [16]byte) [16]byte


// AddsubPd: Alternatively add and subtract packed double-precision (64-bit)
// floating-point elements in 'a' to/from packed elements in 'b', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF (j is even) 
//				dst[i+63:i] := a[i+63:i] - b[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i] + b[i+63:i]
//			FI
//		ENDFOR
//
// Instruction: 'ADDSUBPD'. Intrinsic: '_mm_addsub_pd'.
// Requires SSE3.
func AddsubPd(a M128d, b M128d) M128d {
	return M128d(addsubPd([2]float64(a), [2]float64(b)))
}

func addsubPd(a [2]float64, b [2]float64) [2]float64


// AddsubPs: Alternatively add and subtract packed single-precision (32-bit)
// floating-point elements in 'a' to/from packed elements in 'b', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF (j is even) 
//				dst[i+31:i] := a[i+31:i] - b[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i] + b[i+31:i]
//			FI
//		ENDFOR
//
// Instruction: 'ADDSUBPS'. Intrinsic: '_mm_addsub_ps'.
// Requires SSE3.
func AddsubPs(a M128, b M128) M128 {
	return M128(addsubPs([4]float32(a), [4]float32(b)))
}

func addsubPs(a [4]float32, b [4]float32) [4]float32


// AesdecSi128: Perform one round of an AES decryption flow on data (state) in
// 'a' using the round key in 'RoundKey', and store the result in 'dst'." 
//
//		state := a
//		a[127:0] := InvShiftRows(a[127:0])
//		a[127:0] := InvSubBytes(a[127:0])
//		a[127:0] := InvMixColumns(a[127:0])
//		dst[127:0] := a[127:0] XOR RoundKey[127:0]
//
// Instruction: 'AESDEC'. Intrinsic: '_mm_aesdec_si128'.
// Requires AES.
func AesdecSi128(a M128i, RoundKey M128i) M128i {
	return M128i(aesdecSi128([16]byte(a), [16]byte(RoundKey)))
}

func aesdecSi128(a [16]byte, RoundKey [16]byte) [16]byte


// AesdeclastSi128: Perform the last round of an AES decryption flow on data
// (state) in 'a' using the round key in 'RoundKey', and store the result in
// 'dst'." 
//
//		state := a
//		a[127:0] := InvShiftRows(a[127:0])
//		a[127:0] := InvSubBytes(a[127:0])
//		dst[127:0] := a[127:0] XOR RoundKey[127:0]
//
// Instruction: 'AESDECLAST'. Intrinsic: '_mm_aesdeclast_si128'.
// Requires AES.
func AesdeclastSi128(a M128i, RoundKey M128i) M128i {
	return M128i(aesdeclastSi128([16]byte(a), [16]byte(RoundKey)))
}

func aesdeclastSi128(a [16]byte, RoundKey [16]byte) [16]byte


// AesencSi128: Perform one round of an AES encryption flow on data (state) in
// 'a' using the round key in 'RoundKey', and store the result in 'dst'." 
//
//		state := a
//		a[127:0] := ShiftRows(a[127:0])
//		a[127:0] := SubBytes(a[127:0])
//		a[127:0] := MixColumns(a[127:0])
//		dst[127:0] := a[127:0] XOR RoundKey[127:0]
//
// Instruction: 'AESENC'. Intrinsic: '_mm_aesenc_si128'.
// Requires AES.
func AesencSi128(a M128i, RoundKey M128i) M128i {
	return M128i(aesencSi128([16]byte(a), [16]byte(RoundKey)))
}

func aesencSi128(a [16]byte, RoundKey [16]byte) [16]byte


// AesenclastSi128: Perform the last round of an AES encryption flow on data
// (state) in 'a' using the round key in 'RoundKey', and store the result in
// 'dst'." 
//
//		state := a
//		a[127:0] := ShiftRows(a[127:0])
//		a[127:0] := SubBytes(a[127:0])
//		dst[127:0] := a[127:0] XOR RoundKey[127:0]
//
// Instruction: 'AESENCLAST'. Intrinsic: '_mm_aesenclast_si128'.
// Requires AES.
func AesenclastSi128(a M128i, RoundKey M128i) M128i {
	return M128i(aesenclastSi128([16]byte(a), [16]byte(RoundKey)))
}

func aesenclastSi128(a [16]byte, RoundKey [16]byte) [16]byte


// AesimcSi128: Perform the InvMixColumns transformation on 'a' and store the
// result in 'dst'. 
//
//		dst[127:0] := InvMixColumns(a[127:0])
//
// Instruction: 'AESIMC'. Intrinsic: '_mm_aesimc_si128'.
// Requires AES.
func AesimcSi128(a M128i) M128i {
	return M128i(aesimcSi128([16]byte(a)))
}

func aesimcSi128(a [16]byte) [16]byte


// AeskeygenassistSi128: Assist in expanding the AES cipher key by computing
// steps towards generating a round key for encryption cipher using data from
// 'a' and an 8-bit round constant specified in 'imm8', and store the result in
// 'dst'." 
//
//		X3[31:0] := a[127:96]
//		X2[31:0] := a[95:64]
//		X1[31:0] := a[63:32]
//		X0[31:0] := a[31:0]
//		RCON[31:0] := ZeroExtend(imm8[7:0]);
//		dst[31:0] := SubWord(X1)
//		dst[63:32] := (RotWord(SubWord(X1)) XOR RCON;
//		dst[95:64] := SubWord(X3)
//		dst[127:96] := RotWord(SubWord(X3)) XOR RCON;
//
// Instruction: 'AESKEYGENASSIST'. Intrinsic: '_mm_aeskeygenassist_si128'.
// Requires AES.
func AeskeygenassistSi128(a M128i, imm8 int) M128i {
	return M128i(aeskeygenassistSi128([16]byte(a), imm8))
}

func aeskeygenassistSi128(a [16]byte, imm8 int) [16]byte


// Alignr32: Concatenate 'a' and 'b' into a 32-byte immediate result, shift the
// result right by 'count' 32-bit elements, and store the low 16 bytes (4
// elements) in 'dst'. 
//
//		temp[255:128] := a[127:0]
//		temp[127:0] := b[127:0]
//		temp[255:0] := temp[255:0] >> (32*count)
//		dst[127:0] := temp[127:0]
//		dst[MAX:128] := 0
//
// Instruction: 'VALIGND'. Intrinsic: '_mm_alignr_epi32'.
// Requires AVX512VL.
func Alignr32(a M128i, b M128i, count int) M128i {
	return M128i(alignr32([16]byte(a), [16]byte(b), count))
}

func alignr32(a [16]byte, b [16]byte, count int) [16]byte


// MaskAlignr32: Concatenate 'a' and 'b' into a 32-byte immediate result, shift
// the result right by 'count' 32-bit elements, and store the low 16 bytes (4
// elements) in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		temp[255:128] := a[127:0]
//		temp[127:0] := b[127:0]
//		temp[255:0] := temp[255:0] >> (32*count)
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := temp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VALIGND'. Intrinsic: '_mm_mask_alignr_epi32'.
// Requires AVX512VL.
func MaskAlignr32(src M128i, k Mmask8, a M128i, b M128i, count int) M128i {
	return M128i(maskAlignr32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b), count))
}

func maskAlignr32(src [16]byte, k uint8, a [16]byte, b [16]byte, count int) [16]byte


// MaskzAlignr32: Concatenate 'a' and 'b' into a 32-byte immediate result,
// shift the result right by 'count' 32-bit elements, and store the low 16
// bytes (4 elements) in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		temp[255:128] := a[127:0]
//		temp[127:0] := b[127:0]
//		temp[255:0] := temp[255:0] >> (32*count)
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := temp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VALIGND'. Intrinsic: '_mm_maskz_alignr_epi32'.
// Requires AVX512VL.
func MaskzAlignr32(k Mmask8, a M128i, b M128i, count int) M128i {
	return M128i(maskzAlignr32(uint8(k), [16]byte(a), [16]byte(b), count))
}

func maskzAlignr32(k uint8, a [16]byte, b [16]byte, count int) [16]byte


// Alignr64: Concatenate 'a' and 'b' into a 32-byte immediate result, shift the
// result right by 'count' 64-bit elements, and store the low 16 bytes (2
// elements) in 'dst'. 
//
//		temp[255:128] := a[127:0]
//		temp[127:0] := b[127:0]
//		temp[255:0] := temp[255:0] >> (64*count)
//		dst[127:0] := temp[127:0]
//		dst[MAX:128] := 0
//
// Instruction: 'VALIGNQ'. Intrinsic: '_mm_alignr_epi64'.
// Requires AVX512VL.
func Alignr64(a M128i, b M128i, count int) M128i {
	return M128i(alignr64([16]byte(a), [16]byte(b), count))
}

func alignr64(a [16]byte, b [16]byte, count int) [16]byte


// MaskAlignr64: Concatenate 'a' and 'b' into a 32-byte immediate result, shift
// the result right by 'count' 64-bit elements, and store the low 16 bytes (2
// elements) in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		temp[255:128] := a[127:0]
//		temp[127:0] := b[127:0]
//		temp[255:0] := temp[255:0] >> (64*count)
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := temp[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VALIGNQ'. Intrinsic: '_mm_mask_alignr_epi64'.
// Requires AVX512VL.
func MaskAlignr64(src M128i, k Mmask8, a M128i, b M128i, count int) M128i {
	return M128i(maskAlignr64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b), count))
}

func maskAlignr64(src [16]byte, k uint8, a [16]byte, b [16]byte, count int) [16]byte


// MaskzAlignr64: Concatenate 'a' and 'b' into a 32-byte immediate result,
// shift the result right by 'count' 64-bit elements, and store the low 16
// bytes (2 elements) in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		temp[255:128] := a[127:0]
//		temp[127:0] := b[127:0]
//		temp[255:0] := temp[255:0] >> (64*count)
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := temp[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VALIGNQ'. Intrinsic: '_mm_maskz_alignr_epi64'.
// Requires AVX512VL.
func MaskzAlignr64(k Mmask8, a M128i, b M128i, count int) M128i {
	return M128i(maskzAlignr64(uint8(k), [16]byte(a), [16]byte(b), count))
}

func maskzAlignr64(k uint8, a [16]byte, b [16]byte, count int) [16]byte


// Alignr8: Concatenate 16-byte blocks in 'a' and 'b' into a 32-byte temporary
// result, shift the result right by 'count' bytes, and store the low 16 bytes
// in 'dst'. 
//
//		tmp[255:0] := ((a[127:0] << 128) OR b[127:0]) >> (count[7:0]*8)
//		dst[127:0] := tmp[127:0]
//
// Instruction: 'PALIGNR'. Intrinsic: '_mm_alignr_epi8'.
// Requires SSSE3.
func Alignr8(a M128i, b M128i, count int) M128i {
	return M128i(alignr8([16]byte(a), [16]byte(b), count))
}

func alignr8(a [16]byte, b [16]byte, count int) [16]byte


// MaskAlignr8: Concatenate pairs of 16-byte blocks in 'a' and 'b' into a
// 32-byte temporary result, shift the result right by 'count' bytes, and store
// the low 16 bytes in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		tmp_dst[255:0] := ((a[127:0] << 128) OR b[127:0]) >> (count[7:0]*8)
//		
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := tmp_dst[i+7:i]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPALIGNR'. Intrinsic: '_mm_mask_alignr_epi8'.
// Requires AVX512BW.
func MaskAlignr8(src M128i, k Mmask16, a M128i, b M128i, count int) M128i {
	return M128i(maskAlignr8([16]byte(src), uint16(k), [16]byte(a), [16]byte(b), count))
}

func maskAlignr8(src [16]byte, k uint16, a [16]byte, b [16]byte, count int) [16]byte


// MaskzAlignr8: Concatenate pairs of 16-byte blocks in 'a' and 'b' into a
// 32-byte temporary result, shift the result right by 'count' bytes, and store
// the low 16 bytes in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		tmp_dst[255:0] := ((a[127:0] << 128) OR b[127:0]) >> (count[7:0]*8)
//		
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := tmp_dst[i+7:i]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPALIGNR'. Intrinsic: '_mm_maskz_alignr_epi8'.
// Requires AVX512BW.
func MaskzAlignr8(k Mmask16, a M128i, b M128i, count int) M128i {
	return M128i(maskzAlignr8(uint16(k), [16]byte(a), [16]byte(b), count))
}

func maskzAlignr8(k uint16, a [16]byte, b [16]byte, count int) [16]byte


// MaskAnd32: Compute the bitwise AND of packed 32-bit integers in 'a' and 'b',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] AND b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPANDD'. Intrinsic: '_mm_mask_and_epi32'.
// Requires AVX512F.
func MaskAnd32(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskAnd32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskAnd32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzAnd32: Compute the bitwise AND of packed 32-bit integers in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] AND b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPANDD'. Intrinsic: '_mm_maskz_and_epi32'.
// Requires AVX512F.
func MaskzAnd32(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzAnd32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzAnd32(k uint8, a [16]byte, b [16]byte) [16]byte


// MaskAnd64: Compute the bitwise AND of packed 64-bit integers in 'a' and 'b',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] AND b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPANDQ'. Intrinsic: '_mm_mask_and_epi64'.
// Requires AVX512F.
func MaskAnd64(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskAnd64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskAnd64(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzAnd64: Compute the bitwise AND of packed 64-bit integers in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] AND b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPANDQ'. Intrinsic: '_mm_maskz_and_epi64'.
// Requires AVX512F.
func MaskzAnd64(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzAnd64(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzAnd64(k uint8, a [16]byte, b [16]byte) [16]byte


// AndPd: Compute the bitwise AND of packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := (a[i+63:i] AND b[i+63:i])
//		ENDFOR
//
// Instruction: 'ANDPD'. Intrinsic: '_mm_and_pd'.
// Requires SSE2.
func AndPd(a M128d, b M128d) M128d {
	return M128d(andPd([2]float64(a), [2]float64(b)))
}

func andPd(a [2]float64, b [2]float64) [2]float64


// MaskAndPd: Compute the bitwise AND of packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] AND b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VANDPD'. Intrinsic: '_mm_mask_and_pd'.
// Requires AVX512DQ.
func MaskAndPd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskAndPd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskAndPd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzAndPd: Compute the bitwise AND of packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] AND b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VANDPD'. Intrinsic: '_mm_maskz_and_pd'.
// Requires AVX512DQ.
func MaskzAndPd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzAndPd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzAndPd(k uint8, a [2]float64, b [2]float64) [2]float64


// AndPs: Compute the bitwise AND of packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := (a[i+31:i] AND b[i+31:i])
//		ENDFOR
//
// Instruction: 'ANDPS'. Intrinsic: '_mm_and_ps'.
// Requires SSE.
func AndPs(a M128, b M128) M128 {
	return M128(andPs([4]float32(a), [4]float32(b)))
}

func andPs(a [4]float32, b [4]float32) [4]float32


// MaskAndPs: Compute the bitwise AND of packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] AND b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VANDPS'. Intrinsic: '_mm_mask_and_ps'.
// Requires AVX512DQ.
func MaskAndPs(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskAndPs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskAndPs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzAndPs: Compute the bitwise AND of packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] AND b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VANDPS'. Intrinsic: '_mm_maskz_and_ps'.
// Requires AVX512DQ.
func MaskzAndPs(k Mmask8, a M128, b M128) M128 {
	return M128(maskzAndPs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzAndPs(k uint8, a [4]float32, b [4]float32) [4]float32


// AndSi128: Compute the bitwise AND of 128 bits (representing integer data) in
// 'a' and 'b', and store the result in 'dst'. 
//
//		dst[127:0] := (a[127:0] AND b[127:0])
//
// Instruction: 'PAND'. Intrinsic: '_mm_and_si128'.
// Requires SSE2.
func AndSi128(a M128i, b M128i) M128i {
	return M128i(andSi128([16]byte(a), [16]byte(b)))
}

func andSi128(a [16]byte, b [16]byte) [16]byte


// MaskAndnot32: Compute the bitwise AND NOT of packed 32-bit integers in 'a'
// and 'b', and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ((NOT a[i+31:i]) AND b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPANDND'. Intrinsic: '_mm_mask_andnot_epi32'.
// Requires AVX512F.
func MaskAndnot32(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskAndnot32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskAndnot32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzAndnot32: Compute the bitwise AND NOT of packed 32-bit integers in 'a'
// and 'b', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (NOT a[i+31:i]) AND b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPANDND'. Intrinsic: '_mm_maskz_andnot_epi32'.
// Requires AVX512F.
func MaskzAndnot32(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzAndnot32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzAndnot32(k uint8, a [16]byte, b [16]byte) [16]byte


// MaskAndnot64: Compute the bitwise AND NOT of packed 64-bit integers in 'a'
// and 'b', and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ((NOT a[i+63:i]) AND b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPANDNQ'. Intrinsic: '_mm_mask_andnot_epi64'.
// Requires AVX512F.
func MaskAndnot64(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskAndnot64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskAndnot64(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzAndnot64: Compute the bitwise AND NOT of packed 64-bit integers in 'a'
// and 'b', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (NOT a[i+63:i]) AND b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPANDNQ'. Intrinsic: '_mm_maskz_andnot_epi64'.
// Requires AVX512F.
func MaskzAndnot64(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzAndnot64(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzAndnot64(k uint8, a [16]byte, b [16]byte) [16]byte


// AndnotPd: Compute the bitwise AND NOT of packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := ((NOT a[i+63:i]) AND b[i+63:i])
//		ENDFOR
//
// Instruction: 'ANDNPD'. Intrinsic: '_mm_andnot_pd'.
// Requires SSE2.
func AndnotPd(a M128d, b M128d) M128d {
	return M128d(andnotPd([2]float64(a), [2]float64(b)))
}

func andnotPd(a [2]float64, b [2]float64) [2]float64


// MaskAndnotPd: Compute the bitwise AND NOT of packed double-precision
// (64-bit) floating-point elements in 'a' and 'b', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ((NOT a[i+63:i]) AND b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VANDNPD'. Intrinsic: '_mm_mask_andnot_pd'.
// Requires AVX512DQ.
func MaskAndnotPd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskAndnotPd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskAndnotPd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzAndnotPd: Compute the bitwise AND NOT of packed double-precision
// (64-bit) floating-point elements in 'a' and 'b', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ((NOT a[i+63:i]) AND b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VANDNPD'. Intrinsic: '_mm_maskz_andnot_pd'.
// Requires AVX512DQ.
func MaskzAndnotPd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzAndnotPd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzAndnotPd(k uint8, a [2]float64, b [2]float64) [2]float64


// AndnotPs: Compute the bitwise AND NOT of packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := ((NOT a[i+31:i]) AND b[i+31:i])
//		ENDFOR
//
// Instruction: 'ANDNPS'. Intrinsic: '_mm_andnot_ps'.
// Requires SSE.
func AndnotPs(a M128, b M128) M128 {
	return M128(andnotPs([4]float32(a), [4]float32(b)))
}

func andnotPs(a [4]float32, b [4]float32) [4]float32


// MaskAndnotPs: Compute the bitwise AND NOT of packed single-precision
// (32-bit) floating-point elements in 'a' and 'b', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ((NOT a[i+31:i]) AND b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VANDNPS'. Intrinsic: '_mm_mask_andnot_ps'.
// Requires AVX512DQ.
func MaskAndnotPs(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskAndnotPs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskAndnotPs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzAndnotPs: Compute the bitwise AND NOT of packed single-precision
// (32-bit) floating-point elements in 'a' and 'b', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ((NOT a[i+31:i]) AND b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VANDNPS'. Intrinsic: '_mm_maskz_andnot_ps'.
// Requires AVX512DQ.
func MaskzAndnotPs(k Mmask8, a M128, b M128) M128 {
	return M128(maskzAndnotPs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzAndnotPs(k uint8, a [4]float32, b [4]float32) [4]float32


// AndnotSi128: Compute the bitwise AND NOT of 128 bits (representing integer
// data) in 'a' and 'b', and store the result in 'dst'. 
//
//		dst[127:0] := ((NOT a[127:0]) AND b[127:0])
//
// Instruction: 'PANDN'. Intrinsic: '_mm_andnot_si128'.
// Requires SSE2.
func AndnotSi128(a M128i, b M128i) M128i {
	return M128i(andnotSi128([16]byte(a), [16]byte(b)))
}

func andnotSi128(a [16]byte, b [16]byte) [16]byte


// AsinPd: Compute the inverse sine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := ASIN(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_asin_pd'.
// Requires SSE.
func AsinPd(a M128d) M128d {
	return M128d(asinPd([2]float64(a)))
}

func asinPd(a [2]float64) [2]float64


// AsinPs: Compute the inverse sine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := ASIN(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_asin_ps'.
// Requires SSE.
func AsinPs(a M128) M128 {
	return M128(asinPs([4]float32(a)))
}

func asinPs(a [4]float32) [4]float32


// AsinhPd: Compute the inverse hyperbolic sine of packed double-precision
// (64-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := ASINH(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_asinh_pd'.
// Requires SSE.
func AsinhPd(a M128d) M128d {
	return M128d(asinhPd([2]float64(a)))
}

func asinhPd(a [2]float64) [2]float64


// AsinhPs: Compute the inverse hyperbolic sine of packed single-precision
// (32-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := ASINH(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_asinh_ps'.
// Requires SSE.
func AsinhPs(a M128) M128 {
	return M128(asinhPs([4]float32(a)))
}

func asinhPs(a [4]float32) [4]float32


// AtanPd: Compute the inverse tangent of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := ATAN(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_atan_pd'.
// Requires SSE.
func AtanPd(a M128d) M128d {
	return M128d(atanPd([2]float64(a)))
}

func atanPd(a [2]float64) [2]float64


// AtanPs: Compute the inverse tangent of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := ATAN(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_atan_ps'.
// Requires SSE.
func AtanPs(a M128) M128 {
	return M128(atanPs([4]float32(a)))
}

func atanPs(a [4]float32) [4]float32


// Atan2Pd: Compute the inverse tangent of packed double-precision (64-bit)
// floating-point elements in 'a' divided by packed elements in 'b', and store
// the results in 'dst' expressed in radians. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := ATAN(a[i+63:i] / b[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_atan2_pd'.
// Requires SSE.
func Atan2Pd(a M128d, b M128d) M128d {
	return M128d(atan2Pd([2]float64(a), [2]float64(b)))
}

func atan2Pd(a [2]float64, b [2]float64) [2]float64


// Atan2Ps: Compute the inverse tangent of packed single-precision (32-bit)
// floating-point elements in 'a' divided by packed elements in 'b', and store
// the results in 'dst' expressed in radians. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := ATAN(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_atan2_ps'.
// Requires SSE.
func Atan2Ps(a M128, b M128) M128 {
	return M128(atan2Ps([4]float32(a), [4]float32(b)))
}

func atan2Ps(a [4]float32, b [4]float32) [4]float32


// AtanhPd: Compute the inverse hyperbolic tangent of packed double-precision
// (64-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := ATANH(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_atanh_pd'.
// Requires SSE.
func AtanhPd(a M128d) M128d {
	return M128d(atanhPd([2]float64(a)))
}

func atanhPd(a [2]float64) [2]float64


// AtanhPs: Compute the inverse hyperbolic tangent of packed single-precision
// (32-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := ATANH(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_atanh_ps'.
// Requires SSE.
func AtanhPs(a M128) M128 {
	return M128(atanhPs([4]float32(a)))
}

func atanhPs(a [4]float32) [4]float32


// AvgEpu16: Average packed unsigned 16-bit integers in 'a' and 'b', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			dst[i+15:i] := (a[i+15:i] + b[i+15:i] + 1) >> 1
//		ENDFOR
//
// Instruction: 'PAVGW'. Intrinsic: '_mm_avg_epu16'.
// Requires SSE2.
func AvgEpu16(a M128i, b M128i) M128i {
	return M128i(avgEpu16([16]byte(a), [16]byte(b)))
}

func avgEpu16(a [16]byte, b [16]byte) [16]byte


// MaskAvgEpu16: Average packed unsigned 16-bit integers in 'a' and 'b', and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := (a[i+15:i] + b[i+15:i] + 1) >> 1
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPAVGW'. Intrinsic: '_mm_mask_avg_epu16'.
// Requires AVX512BW.
func MaskAvgEpu16(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskAvgEpu16([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskAvgEpu16(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzAvgEpu16: Average packed unsigned 16-bit integers in 'a' and 'b', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := (a[i+15:i] + b[i+15:i] + 1) >> 1
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPAVGW'. Intrinsic: '_mm_maskz_avg_epu16'.
// Requires AVX512BW.
func MaskzAvgEpu16(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzAvgEpu16(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzAvgEpu16(k uint8, a [16]byte, b [16]byte) [16]byte


// AvgEpu8: Average packed unsigned 8-bit integers in 'a' and 'b', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*8
//			dst[i+7:i] := (a[i+7:i] + b[i+7:i] + 1) >> 1
//		ENDFOR
//
// Instruction: 'PAVGB'. Intrinsic: '_mm_avg_epu8'.
// Requires SSE2.
func AvgEpu8(a M128i, b M128i) M128i {
	return M128i(avgEpu8([16]byte(a), [16]byte(b)))
}

func avgEpu8(a [16]byte, b [16]byte) [16]byte


// MaskAvgEpu8: Average packed unsigned 8-bit integers in 'a' and 'b', and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := (a[i+7:i] + b[i+7:i] + 1) >> 1
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPAVGB'. Intrinsic: '_mm_mask_avg_epu8'.
// Requires AVX512BW.
func MaskAvgEpu8(src M128i, k Mmask16, a M128i, b M128i) M128i {
	return M128i(maskAvgEpu8([16]byte(src), uint16(k), [16]byte(a), [16]byte(b)))
}

func maskAvgEpu8(src [16]byte, k uint16, a [16]byte, b [16]byte) [16]byte


// MaskzAvgEpu8: Average packed unsigned 8-bit integers in 'a' and 'b', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := (a[i+7:i] + b[i+7:i] + 1) >> 1
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPAVGB'. Intrinsic: '_mm_maskz_avg_epu8'.
// Requires AVX512BW.
func MaskzAvgEpu8(k Mmask16, a M128i, b M128i) M128i {
	return M128i(maskzAvgEpu8(uint16(k), [16]byte(a), [16]byte(b)))
}

func maskzAvgEpu8(k uint16, a [16]byte, b [16]byte) [16]byte


// Blend16: Blend packed 16-bit integers from 'a' and 'b' using control mask
// 'imm8', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF imm8[j%8]
//				dst[i+15:i] := b[i+15:i]
//			ELSE
//				dst[i+15:i] := a[i+15:i]
//			FI
//		ENDFOR
//
// Instruction: 'PBLENDW'. Intrinsic: '_mm_blend_epi16'.
// Requires SSE4.1.
func Blend16(a M128i, b M128i, imm8 int) M128i {
	return M128i(blend16([16]byte(a), [16]byte(b), imm8))
}

func blend16(a [16]byte, b [16]byte, imm8 int) [16]byte


// MaskBlend16: Blend packed 16-bit integers from 'a' and 'b' using control
// mask 'k', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := b[i+15:i]
//			ELSE
//				dst[i+15:i] := a[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBLENDMW'. Intrinsic: '_mm_mask_blend_epi16'.
// Requires AVX512BW.
func MaskBlend16(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskBlend16(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskBlend16(k uint8, a [16]byte, b [16]byte) [16]byte


// Blend32: Blend packed 32-bit integers from 'a' and 'b' using control mask
// 'imm8', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF imm8[j%8]
//				dst[i+31:i] := b[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBLENDD'. Intrinsic: '_mm_blend_epi32'.
// Requires AVX2.
func Blend32(a M128i, b M128i, imm8 int) M128i {
	return M128i(blend32([16]byte(a), [16]byte(b), imm8))
}

func blend32(a [16]byte, b [16]byte, imm8 int) [16]byte


// MaskBlend32: Blend packed 32-bit integers from 'a' and 'b' using control
// mask 'k', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := b[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBLENDMD'. Intrinsic: '_mm_mask_blend_epi32'.
// Requires AVX512F.
func MaskBlend32(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskBlend32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskBlend32(k uint8, a [16]byte, b [16]byte) [16]byte


// MaskBlend64: Blend packed 64-bit integers from 'a' and 'b' using control
// mask 'k', and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := b[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBLENDMQ'. Intrinsic: '_mm_mask_blend_epi64'.
// Requires AVX512F.
func MaskBlend64(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskBlend64(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskBlend64(k uint8, a [16]byte, b [16]byte) [16]byte


// MaskBlend8: Blend packed 8-bit integers from 'a' and 'b' using control mask
// 'k', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := b[i+7:i]
//			ELSE
//				dst[i+7:i] := a[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBLENDMB'. Intrinsic: '_mm_mask_blend_epi8'.
// Requires AVX512BW.
func MaskBlend8(k Mmask16, a M128i, b M128i) M128i {
	return M128i(maskBlend8(uint16(k), [16]byte(a), [16]byte(b)))
}

func maskBlend8(k uint16, a [16]byte, b [16]byte) [16]byte


// BlendPd: Blend packed double-precision (64-bit) floating-point elements from
// 'a' and 'b' using control mask 'imm8', and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF imm8[j%8]
//				dst[i+63:i] := b[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//
// Instruction: 'BLENDPD'. Intrinsic: '_mm_blend_pd'.
// Requires SSE4.1.
func BlendPd(a M128d, b M128d, imm8 int) M128d {
	return M128d(blendPd([2]float64(a), [2]float64(b), imm8))
}

func blendPd(a [2]float64, b [2]float64, imm8 int) [2]float64


// MaskBlendPd: Blend packed double-precision (64-bit) floating-point elements
// from 'a' and 'b' using control mask 'k', and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := b[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VBLENDMPD'. Intrinsic: '_mm_mask_blend_pd'.
// Requires AVX512F.
func MaskBlendPd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskBlendPd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskBlendPd(k uint8, a [2]float64, b [2]float64) [2]float64


// BlendPs: Blend packed single-precision (32-bit) floating-point elements from
// 'a' and 'b' using control mask 'imm8', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF imm8[j%8]
//				dst[i+31:i] := b[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//
// Instruction: 'BLENDPS'. Intrinsic: '_mm_blend_ps'.
// Requires SSE4.1.
func BlendPs(a M128, b M128, imm8 int) M128 {
	return M128(blendPs([4]float32(a), [4]float32(b), imm8))
}

func blendPs(a [4]float32, b [4]float32, imm8 int) [4]float32


// MaskBlendPs: Blend packed single-precision (32-bit) floating-point elements
// from 'a' and 'b' using control mask 'k', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := b[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VBLENDMPS'. Intrinsic: '_mm_mask_blend_ps'.
// Requires AVX512F.
func MaskBlendPs(k Mmask8, a M128, b M128) M128 {
	return M128(maskBlendPs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskBlendPs(k uint8, a [4]float32, b [4]float32) [4]float32


// Blendv8: Blend packed 8-bit integers from 'a' and 'b' using 'mask', and
// store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF mask[i+7]
//				dst[i+7:i] := b[i+7:i]
//			ELSE
//				dst[i+7:i] := a[i+7:i]
//			FI
//		ENDFOR
//
// Instruction: 'PBLENDVB'. Intrinsic: '_mm_blendv_epi8'.
// Requires SSE4.1.
func Blendv8(a M128i, b M128i, mask M128i) M128i {
	return M128i(blendv8([16]byte(a), [16]byte(b), [16]byte(mask)))
}

func blendv8(a [16]byte, b [16]byte, mask [16]byte) [16]byte


// BlendvPd: Blend packed double-precision (64-bit) floating-point elements
// from 'a' and 'b' using 'mask', and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF mask[i+63]
//				dst[i+63:i] := b[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//
// Instruction: 'BLENDVPD'. Intrinsic: '_mm_blendv_pd'.
// Requires SSE4.1.
func BlendvPd(a M128d, b M128d, mask M128d) M128d {
	return M128d(blendvPd([2]float64(a), [2]float64(b), [2]float64(mask)))
}

func blendvPd(a [2]float64, b [2]float64, mask [2]float64) [2]float64


// BlendvPs: Blend packed single-precision (32-bit) floating-point elements
// from 'a' and 'b' using 'mask', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF mask[i+31]
//				dst[i+31:i] := b[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//
// Instruction: 'BLENDVPS'. Intrinsic: '_mm_blendv_ps'.
// Requires SSE4.1.
func BlendvPs(a M128, b M128, mask M128) M128 {
	return M128(blendvPs([4]float32(a), [4]float32(b), [4]float32(mask)))
}

func blendvPs(a [4]float32, b [4]float32, mask [4]float32) [4]float32


// BroadcastSs: Broadcast a single-precision (32-bit) floating-point element
// from memory to all elements of 'dst'. 
//
//		tmp[31:0] = MEM[mem_addr+31:mem_addr]
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := tmp[31:0]
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VBROADCASTSS'. Intrinsic: '_mm_broadcast_ss'.
// Requires AVX.
func BroadcastSs(mem_addr float32) M128 {
	return M128(broadcastSs(mem_addr))
}

func broadcastSs(mem_addr float32) [4]float32


// Broadcastb8: Broadcast the low packed 8-bit integer from 'a' to all elements
// of 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*8
//			dst[i+7:i] := a[7:0]
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBROADCASTB'. Intrinsic: '_mm_broadcastb_epi8'.
// Requires AVX2.
func Broadcastb8(a M128i) M128i {
	return M128i(broadcastb8([16]byte(a)))
}

func broadcastb8(a [16]byte) [16]byte


// MaskBroadcastb8: Broadcast the low packed 8-bit integer from 'a' to all
// elements of 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := a[7:0]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBROADCASTB'. Intrinsic: '_mm_mask_broadcastb_epi8'.
// Requires AVX512BW.
func MaskBroadcastb8(src M128i, k Mmask16, a M128i) M128i {
	return M128i(maskBroadcastb8([16]byte(src), uint16(k), [16]byte(a)))
}

func maskBroadcastb8(src [16]byte, k uint16, a [16]byte) [16]byte


// MaskzBroadcastb8: Broadcast the low packed 8-bit integer from 'a' to all
// elements of 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := a[7:0]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBROADCASTB'. Intrinsic: '_mm_maskz_broadcastb_epi8'.
// Requires AVX512BW.
func MaskzBroadcastb8(k Mmask16, a M128i) M128i {
	return M128i(maskzBroadcastb8(uint16(k), [16]byte(a)))
}

func maskzBroadcastb8(k uint16, a [16]byte) [16]byte


// Broadcastd32: Broadcast the low packed 32-bit integer from 'a' to all
// elements of 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := a[31:0]
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm_broadcastd_epi32'.
// Requires AVX2.
func Broadcastd32(a M128i) M128i {
	return M128i(broadcastd32([16]byte(a)))
}

func broadcastd32(a [16]byte) [16]byte


// MaskBroadcastd32: Broadcast the low packed 32-bit integer from 'a' to all
// elements of 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm_mask_broadcastd_epi32'.
// Requires AVX512F.
func MaskBroadcastd32(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskBroadcastd32([16]byte(src), uint8(k), [16]byte(a)))
}

func maskBroadcastd32(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzBroadcastd32: Broadcast the low packed 32-bit integer from 'a' to all
// elements of 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm_maskz_broadcastd_epi32'.
// Requires AVX512F.
func MaskzBroadcastd32(k Mmask8, a M128i) M128i {
	return M128i(maskzBroadcastd32(uint8(k), [16]byte(a)))
}

func maskzBroadcastd32(k uint8, a [16]byte) [16]byte


// Broadcastmb64: Broadcast the low 8-bits from input mask 'k' to all 64-bit
// elements of 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := ZeroExtend(k[7:0])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBROADCASTMB2Q'. Intrinsic: '_mm_broadcastmb_epi64'.
// Requires AVX512CD.
func Broadcastmb64(k Mmask8) M128i {
	return M128i(broadcastmb64(uint8(k)))
}

func broadcastmb64(k uint8) [16]byte


// Broadcastmw32: Broadcast the low 16-bits from input mask 'k' to all 32-bit
// elements of 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := ZeroExtend(k[15:0])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBROADCASTMW2D'. Intrinsic: '_mm_broadcastmw_epi32'.
// Requires AVX512CD.
func Broadcastmw32(k Mmask16) M128i {
	return M128i(broadcastmw32(uint16(k)))
}

func broadcastmw32(k uint16) [16]byte


// Broadcastq64: Broadcast the low packed 64-bit integer from 'a' to all
// elements of 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := a[63:0]
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm_broadcastq_epi64'.
// Requires AVX2.
func Broadcastq64(a M128i) M128i {
	return M128i(broadcastq64([16]byte(a)))
}

func broadcastq64(a [16]byte) [16]byte


// MaskBroadcastq64: Broadcast the low packed 64-bit integer from 'a' to all
// elements of 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm_mask_broadcastq_epi64'.
// Requires AVX512F.
func MaskBroadcastq64(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskBroadcastq64([16]byte(src), uint8(k), [16]byte(a)))
}

func maskBroadcastq64(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzBroadcastq64: Broadcast the low packed 64-bit integer from 'a' to all
// elements of 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm_maskz_broadcastq_epi64'.
// Requires AVX512F.
func MaskzBroadcastq64(k Mmask8, a M128i) M128i {
	return M128i(maskzBroadcastq64(uint8(k), [16]byte(a)))
}

func maskzBroadcastq64(k uint8, a [16]byte) [16]byte


// BroadcastsdPd: Broadcast the low double-precision (64-bit) floating-point
// element from 'a' to all elements of 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := a[63:0]
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'MOVDDUP'. Intrinsic: '_mm_broadcastsd_pd'.
// Requires AVX2.
func BroadcastsdPd(a M128d) M128d {
	return M128d(broadcastsdPd([2]float64(a)))
}

func broadcastsdPd(a [2]float64) [2]float64


// BroadcastssPs: Broadcast the low single-precision (32-bit) floating-point
// element from 'a' to all elements of 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := a[31:0]
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VBROADCASTSS'. Intrinsic: '_mm_broadcastss_ps'.
// Requires AVX2.
func BroadcastssPs(a M128) M128 {
	return M128(broadcastssPs([4]float32(a)))
}

func broadcastssPs(a [4]float32) [4]float32


// MaskBroadcastssPs: Broadcast the low single-precision (32-bit)
// floating-point element from 'a' to all elements of 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VBROADCASTSS'. Intrinsic: '_mm_mask_broadcastss_ps'.
// Requires AVX512F.
func MaskBroadcastssPs(src M128, k Mmask8, a M128) M128 {
	return M128(maskBroadcastssPs([4]float32(src), uint8(k), [4]float32(a)))
}

func maskBroadcastssPs(src [4]float32, k uint8, a [4]float32) [4]float32


// MaskzBroadcastssPs: Broadcast the low single-precision (32-bit)
// floating-point element from 'a' to all elements of 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VBROADCASTSS'. Intrinsic: '_mm_maskz_broadcastss_ps'.
// Requires AVX512F.
func MaskzBroadcastssPs(k Mmask8, a M128) M128 {
	return M128(maskzBroadcastssPs(uint8(k), [4]float32(a)))
}

func maskzBroadcastssPs(k uint8, a [4]float32) [4]float32


// Broadcastw16: Broadcast the low packed 16-bit integer from 'a' to all
// elements of 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			dst[i+15:i] := a[15:0]
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBROADCASTW'. Intrinsic: '_mm_broadcastw_epi16'.
// Requires AVX2.
func Broadcastw16(a M128i) M128i {
	return M128i(broadcastw16([16]byte(a)))
}

func broadcastw16(a [16]byte) [16]byte


// MaskBroadcastw16: Broadcast the low packed 16-bit integer from 'a' to all
// elements of 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := a[15:0]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBROADCASTW'. Intrinsic: '_mm_mask_broadcastw_epi16'.
// Requires AVX512BW.
func MaskBroadcastw16(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskBroadcastw16([16]byte(src), uint8(k), [16]byte(a)))
}

func maskBroadcastw16(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzBroadcastw16: Broadcast the low packed 16-bit integer from 'a' to all
// elements of 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := a[15:0]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBROADCASTW'. Intrinsic: '_mm_maskz_broadcastw_epi16'.
// Requires AVX512BW.
func MaskzBroadcastw16(k Mmask8, a M128i) M128i {
	return M128i(maskzBroadcastw16(uint8(k), [16]byte(a)))
}

func maskzBroadcastw16(k uint8, a [16]byte) [16]byte


// BslliSi128: Shift 'a' left by 'imm8' bytes while shifting in zeros, and
// store the results in 'dst'. 
//
//		tmp := imm8[7:0]
//		IF tmp > 15
//			tmp := 16
//		FI
//		dst[127:0] := a[127:0] << (tmp*8)
//
// Instruction: 'PSLLDQ'. Intrinsic: '_mm_bslli_si128'.
// Requires SSE2.
func BslliSi128(a M128i, imm8 int) M128i {
	return M128i(bslliSi128([16]byte(a), imm8))
}

func bslliSi128(a [16]byte, imm8 int) [16]byte


// BsrliSi128: Shift 'a' right by 'imm8' bytes while shifting in zeros, and
// store the results in 'dst'. 
//
//		tmp := imm8[7:0]
//		IF tmp > 15
//			tmp := 16
//		FI
//		dst[127:0] := a[127:0] >> (tmp*8)
//
// Instruction: 'PSRLDQ'. Intrinsic: '_mm_bsrli_si128'.
// Requires SSE2.
func BsrliSi128(a M128i, imm8 int) M128i {
	return M128i(bsrliSi128([16]byte(a), imm8))
}

func bsrliSi128(a [16]byte, imm8 int) [16]byte


// CastpdPs: Cast vector of type __m128d to type __m128. This intrinsic is only
// used for compilation and does not generate any instructions, thus it has
// zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm_castpd_ps'.
// Requires SSE2.
func CastpdPs(a M128d) M128 {
	return M128(castpdPs([2]float64(a)))
}

func castpdPs(a [2]float64) [4]float32


// CastpdSi128: Cast vector of type __m128d to type __m128i. This intrinsic is
// only used for compilation and does not generate any instructions, thus it
// has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm_castpd_si128'.
// Requires SSE2.
func CastpdSi128(a M128d) M128i {
	return M128i(castpdSi128([2]float64(a)))
}

func castpdSi128(a [2]float64) [16]byte


// CastpsPd: Cast vector of type __m128 to type __m128d. This intrinsic is only
// used for compilation and does not generate any instructions, thus it has
// zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm_castps_pd'.
// Requires SSE2.
func CastpsPd(a M128) M128d {
	return M128d(castpsPd([4]float32(a)))
}

func castpsPd(a [4]float32) [2]float64


// CastpsSi128: Cast vector of type __m128 to type __m128i. This intrinsic is
// only used for compilation and does not generate any instructions, thus it
// has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm_castps_si128'.
// Requires SSE2.
func CastpsSi128(a M128) M128i {
	return M128i(castpsSi128([4]float32(a)))
}

func castpsSi128(a [4]float32) [16]byte


// Castsi128Pd: Cast vector of type __m128i to type __m128d. This intrinsic is
// only used for compilation and does not generate any instructions, thus it
// has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm_castsi128_pd'.
// Requires SSE2.
func Castsi128Pd(a M128i) M128d {
	return M128d(castsi128Pd([16]byte(a)))
}

func castsi128Pd(a [16]byte) [2]float64


// Castsi128Ps: Cast vector of type __m128i to type __m128. This intrinsic is
// only used for compilation and does not generate any instructions, thus it
// has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm_castsi128_ps'.
// Requires SSE2.
func Castsi128Ps(a M128i) M128 {
	return M128(castsi128Ps([16]byte(a)))
}

func castsi128Ps(a [16]byte) [4]float32


// CbrtPd: Compute the cube root of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := CubeRoot(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_cbrt_pd'.
// Requires SSE.
func CbrtPd(a M128d) M128d {
	return M128d(cbrtPd([2]float64(a)))
}

func cbrtPd(a [2]float64) [2]float64


// CbrtPs: Compute the cube root of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := CubeRoot(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_cbrt_ps'.
// Requires SSE.
func CbrtPs(a M128) M128 {
	return M128(cbrtPs([4]float32(a)))
}

func cbrtPs(a [4]float32) [4]float32


// CdfnormPd: Compute the cumulative distribution function of packed
// double-precision (64-bit) floating-point elements in 'a' using the normal
// distribution, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := CDFNormal(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_cdfnorm_pd'.
// Requires SSE.
func CdfnormPd(a M128d) M128d {
	return M128d(cdfnormPd([2]float64(a)))
}

func cdfnormPd(a [2]float64) [2]float64


// CdfnormPs: Compute the cumulative distribution function of packed
// single-precision (32-bit) floating-point elements in 'a' using the normal
// distribution, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := CDFNormal(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_cdfnorm_ps'.
// Requires SSE.
func CdfnormPs(a M128) M128 {
	return M128(cdfnormPs([4]float32(a)))
}

func cdfnormPs(a [4]float32) [4]float32


// CdfnorminvPd: Compute the inverse cumulative distribution function of packed
// double-precision (64-bit) floating-point elements in 'a' using the normal
// distribution, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := InverseCDFNormal(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_cdfnorminv_pd'.
// Requires SSE.
func CdfnorminvPd(a M128d) M128d {
	return M128d(cdfnorminvPd([2]float64(a)))
}

func cdfnorminvPd(a [2]float64) [2]float64


// CdfnorminvPs: Compute the inverse cumulative distribution function of packed
// single-precision (32-bit) floating-point elements in 'a' using the normal
// distribution, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := InverseCDFNormal(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_cdfnorminv_ps'.
// Requires SSE.
func CdfnorminvPs(a M128) M128 {
	return M128(cdfnorminvPs([4]float32(a)))
}

func cdfnorminvPs(a [4]float32) [4]float32


// CeilPd: Round the packed double-precision (64-bit) floating-point elements
// in 'a' up to an integer value, and store the results as packed
// double-precision floating-point elements in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := CEIL(a[i+63:i])
//		ENDFOR
//
// Instruction: 'ROUNDPD'. Intrinsic: '_mm_ceil_pd'.
// Requires SSE4.1.
func CeilPd(a M128d) M128d {
	return M128d(ceilPd([2]float64(a)))
}

func ceilPd(a [2]float64) [2]float64


// CeilPs: Round the packed single-precision (32-bit) floating-point elements
// in 'a' up to an integer value, and store the results as packed
// single-precision floating-point elements in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := CEIL(a[i+31:i])
//		ENDFOR
//
// Instruction: 'ROUNDPS'. Intrinsic: '_mm_ceil_ps'.
// Requires SSE4.1.
func CeilPs(a M128) M128 {
	return M128(ceilPs([4]float32(a)))
}

func ceilPs(a [4]float32) [4]float32


// CeilSd: Round the lower double-precision (64-bit) floating-point element in
// 'b' up to an integer value, store the result as a double-precision
// floating-point element in the lower element of 'dst', and copy the upper
// element from 'a' to the upper element of 'dst'. 
//
//		dst[63:0] := CEIL(b[63:0])
//		dst[127:64] := a[127:64]
//
// Instruction: 'ROUNDSD'. Intrinsic: '_mm_ceil_sd'.
// Requires SSE4.1.
func CeilSd(a M128d, b M128d) M128d {
	return M128d(ceilSd([2]float64(a), [2]float64(b)))
}

func ceilSd(a [2]float64, b [2]float64) [2]float64


// CeilSs: Round the lower single-precision (32-bit) floating-point element in
// 'b' up to an integer value, store the result as a single-precision
// floating-point element in the lower element of 'dst', and copy the upper 3
// packed elements from 'a' to the upper elements of 'dst'. 
//
//		dst[31:0] := CEIL(b[31:0])
//		dst[127:32] := a[127:32]
//
// Instruction: 'ROUNDSS'. Intrinsic: '_mm_ceil_ss'.
// Requires SSE4.1.
func CeilSs(a M128, b M128) M128 {
	return M128(ceilSs([4]float32(a), [4]float32(b)))
}

func ceilSs(a [4]float32, b [4]float32) [4]float32


// CexpPs: Compute the exponential value of 'e' raised to the power of packed
// complex single-precision (32-bit) floating-point elements in 'a', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := e^(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_cexp_ps'.
// Requires SSE.
func CexpPs(a M128) M128 {
	return M128(cexpPs([4]float32(a)))
}

func cexpPs(a [4]float32) [4]float32


// Clmulepi64Si128: Perform a carry-less multiplication of two 64-bit integers,
// selected from 'a' and 'b' according to 'imm8', and store the results in
// 'dst'. 
//
//		IF (imm8[0] = 0)
//			TEMP1 := a[63:0];
//		ELSE
//			TEMP1 := a[127:64];
//		FI 
//		IF (imm8[4] = 0)
//			TEMP2 := b[63:0];
//		ELSE 
//			TEMP2 := b[127:64];
//		FI
//		
//		FOR i := 0 to 63
//			TEMP[i] := (TEMP1[0] and TEMP2[i]);
//			FOR j := 1 to i
//				TEMP [i] := TEMP [i] XOR (TEMP1[j] AND TEMP2[i-j])
//			ENDFOR 
//			dst[i] := TEMP[i];
//		ENDFOR
//		FOR i := 64 to 127
//			TEMP [i] := 0;
//			FOR j := (i - 63) to 63
//				TEMP [i] := TEMP [i] XOR (TEMP1[j] AND TEMP2[i-j])
//			ENDFOR
//			dst[i] := TEMP[i];
//		ENDFOR
//		dst[127] := 0
//
// Instruction: 'PCLMULQDQ'. Intrinsic: '_mm_clmulepi64_si128'.
// Requires PCLMULQDQ.
func Clmulepi64Si128(a M128i, b M128i, imm8 int) M128i {
	return M128i(clmulepi64Si128([16]byte(a), [16]byte(b), imm8))
}

func clmulepi64Si128(a [16]byte, b [16]byte, imm8 int) [16]byte


// ClogPs: Compute the natural logarithm of packed complex single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := ln(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_clog_ps'.
// Requires SSE.
func ClogPs(a M128) M128 {
	return M128(clogPs([4]float32(a)))
}

func clogPs(a [4]float32) [4]float32


// Cmp16Mask: Compare packed 16-bit integers in 'a' and 'b' based on the
// comparison operand specified by 'imm8', and store the results in mask vector
// 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 7
//			i := j*16
//			k[j] := ( a[i+15:i] OP b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm_cmp_epi16_mask'.
// Requires AVX512BW.
func Cmp16Mask(a M128i, b M128i, imm8 int) Mmask8 {
	return Mmask8(cmp16Mask([16]byte(a), [16]byte(b), imm8))
}

func cmp16Mask(a [16]byte, b [16]byte, imm8 int) uint8


// MaskCmp16Mask: Compare packed 16-bit integers in 'a' and 'b' based on the
// comparison operand specified by 'imm8', and store the results in mask vector
// 'k1' using zeromask 'k' (elements are zeroed out when the corresponding mask
// bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 7
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] OP b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm_mask_cmp_epi16_mask'.
// Requires AVX512BW.
func MaskCmp16Mask(k1 Mmask8, a M128i, b M128i, imm8 int) Mmask8 {
	return Mmask8(maskCmp16Mask(uint8(k1), [16]byte(a), [16]byte(b), imm8))
}

func maskCmp16Mask(k1 uint8, a [16]byte, b [16]byte, imm8 int) uint8


// Cmp32Mask: Compare packed 32-bit integers in 'a' and 'b' based on the
// comparison operand specified by 'imm8', and store the results in mask vector
// 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm_cmp_epi32_mask'.
// Requires AVX512F.
func Cmp32Mask(a M128i, b M128i, imm8 uint8) Mmask8 {
	return Mmask8(cmp32Mask([16]byte(a), [16]byte(b), imm8))
}

func cmp32Mask(a [16]byte, b [16]byte, imm8 uint8) uint8


// MaskCmp32Mask: Compare packed 32-bit integers in 'a' and 'b' based on the
// comparison operand specified by 'imm8', and store the results in mask vector
// 'k1' using zeromask 'k' (elements are zeroed out when the corresponding mask
// bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm_mask_cmp_epi32_mask'.
// Requires AVX512F.
func MaskCmp32Mask(k1 Mmask8, a M128i, b M128i, imm8 uint8) Mmask8 {
	return Mmask8(maskCmp32Mask(uint8(k1), [16]byte(a), [16]byte(b), imm8))
}

func maskCmp32Mask(k1 uint8, a [16]byte, b [16]byte, imm8 uint8) uint8


// Cmp64Mask: Compare packed 64-bit integers in 'a' and 'b' based on the
// comparison operand specified by 'imm8', and store the results in mask vector
// 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm_cmp_epi64_mask'.
// Requires AVX512F.
func Cmp64Mask(a M128i, b M128i, imm8 uint8) Mmask8 {
	return Mmask8(cmp64Mask([16]byte(a), [16]byte(b), imm8))
}

func cmp64Mask(a [16]byte, b [16]byte, imm8 uint8) uint8


// MaskCmp64Mask: Compare packed 64-bit integers in 'a' and 'b' based on the
// comparison operand specified by 'imm8', and store the results in mask vector
// 'k1' using zeromask 'k' (elements are zeroed out when the corresponding mask
// bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm_mask_cmp_epi64_mask'.
// Requires AVX512F.
func MaskCmp64Mask(k1 Mmask8, a M128i, b M128i, imm8 uint8) Mmask8 {
	return Mmask8(maskCmp64Mask(uint8(k1), [16]byte(a), [16]byte(b), imm8))
}

func maskCmp64Mask(k1 uint8, a [16]byte, b [16]byte, imm8 uint8) uint8


// Cmp8Mask: Compare packed 8-bit integers in 'a' and 'b' based on the
// comparison operand specified by 'imm8', and store the results in mask vector
// 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 15
//			i := j*8
//			k[j] := ( a[i+7:i] OP b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm_cmp_epi8_mask'.
// Requires AVX512BW.
func Cmp8Mask(a M128i, b M128i, imm8 int) Mmask16 {
	return Mmask16(cmp8Mask([16]byte(a), [16]byte(b), imm8))
}

func cmp8Mask(a [16]byte, b [16]byte, imm8 int) uint16


// MaskCmp8Mask: Compare packed 8-bit integers in 'a' and 'b' based on the
// comparison operand specified by 'imm8', and store the results in mask vector
// 'k' using zeromask 'k1' (elements are zeroed out when the corresponding mask
// bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 15
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] OP b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm_mask_cmp_epi8_mask'.
// Requires AVX512BW.
func MaskCmp8Mask(k1 Mmask16, a M128i, b M128i, imm8 int) Mmask16 {
	return Mmask16(maskCmp8Mask(uint16(k1), [16]byte(a), [16]byte(b), imm8))
}

func maskCmp8Mask(k1 uint16, a [16]byte, b [16]byte, imm8 int) uint16


// CmpEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b' based
// on the comparison operand specified by 'imm8', and store the results in mask
// vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 7
//			i := j*16
//			k[j] := ( a[i+15:i] OP b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm_cmp_epu16_mask'.
// Requires AVX512BW.
func CmpEpu16Mask(a M128i, b M128i, imm8 int) Mmask8 {
	return Mmask8(cmpEpu16Mask([16]byte(a), [16]byte(b), imm8))
}

func cmpEpu16Mask(a [16]byte, b [16]byte, imm8 int) uint8


// MaskCmpEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b'
// based on the comparison operand specified by 'imm8', and store the results
// in mask vector 'k1' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 7
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] OP b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm_mask_cmp_epu16_mask'.
// Requires AVX512BW.
func MaskCmpEpu16Mask(k1 Mmask8, a M128i, b M128i, imm8 int) Mmask8 {
	return Mmask8(maskCmpEpu16Mask(uint8(k1), [16]byte(a), [16]byte(b), imm8))
}

func maskCmpEpu16Mask(k1 uint8, a [16]byte, b [16]byte, imm8 int) uint8


// CmpEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b' based
// on the comparison operand specified by 'imm8', and store the results in mask
// vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm_cmp_epu32_mask'.
// Requires AVX512F.
func CmpEpu32Mask(a M128i, b M128i, imm8 uint8) Mmask8 {
	return Mmask8(cmpEpu32Mask([16]byte(a), [16]byte(b), imm8))
}

func cmpEpu32Mask(a [16]byte, b [16]byte, imm8 uint8) uint8


// MaskCmpEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// based on the comparison operand specified by 'imm8', and store the results
// in mask vector 'k1' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm_mask_cmp_epu32_mask'.
// Requires AVX512F.
func MaskCmpEpu32Mask(k1 Mmask8, a M128i, b M128i, imm8 uint8) Mmask8 {
	return Mmask8(maskCmpEpu32Mask(uint8(k1), [16]byte(a), [16]byte(b), imm8))
}

func maskCmpEpu32Mask(k1 uint8, a [16]byte, b [16]byte, imm8 uint8) uint8


// CmpEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b' based
// on the comparison operand specified by 'imm8', and store the results in mask
// vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm_cmp_epu64_mask'.
// Requires AVX512F.
func CmpEpu64Mask(a M128i, b M128i, imm8 uint8) Mmask8 {
	return Mmask8(cmpEpu64Mask([16]byte(a), [16]byte(b), imm8))
}

func cmpEpu64Mask(a [16]byte, b [16]byte, imm8 uint8) uint8


// MaskCmpEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// based on the comparison operand specified by 'imm8', and store the results
// in mask vector 'k1' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm_mask_cmp_epu64_mask'.
// Requires AVX512F.
func MaskCmpEpu64Mask(k1 Mmask8, a M128i, b M128i, imm8 uint8) Mmask8 {
	return Mmask8(maskCmpEpu64Mask(uint8(k1), [16]byte(a), [16]byte(b), imm8))
}

func maskCmpEpu64Mask(k1 uint8, a [16]byte, b [16]byte, imm8 uint8) uint8


// CmpEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' based on
// the comparison operand specified by 'imm8', and store the results in mask
// vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 15
//			i := j*8
//			k[j] := ( a[i+7:i] OP b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm_cmp_epu8_mask'.
// Requires AVX512BW.
func CmpEpu8Mask(a M128i, b M128i, imm8 int) Mmask16 {
	return Mmask16(cmpEpu8Mask([16]byte(a), [16]byte(b), imm8))
}

func cmpEpu8Mask(a [16]byte, b [16]byte, imm8 int) uint16


// MaskCmpEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' based
// on the comparison operand specified by 'imm8', and store the results in mask
// vector 'k' using zeromask 'k1' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 15
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] OP b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm_mask_cmp_epu8_mask'.
// Requires AVX512BW.
func MaskCmpEpu8Mask(k1 Mmask16, a M128i, b M128i, imm8 int) Mmask16 {
	return Mmask16(maskCmpEpu8Mask(uint16(k1), [16]byte(a), [16]byte(b), imm8))
}

func maskCmpEpu8Mask(k1 uint16, a [16]byte, b [16]byte, imm8 int) uint16


// CmpPd: Compare packed double-precision (64-bit) floating-point elements in
// 'a' and 'b' based on the comparison operand specified by 'imm8', and store
// the results in 'dst'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := ( a[i+63:i] OP b[i+63:i] ) ? 0xFFFFFFFFFFFFFFFF : 0
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCMPPD'. Intrinsic: '_mm_cmp_pd'.
// Requires AVX.
func CmpPd(a M128d, b M128d, imm8 int) M128d {
	return M128d(cmpPd([2]float64(a), [2]float64(b), imm8))
}

func cmpPd(a [2]float64, b [2]float64, imm8 int) [2]float64


// CmpPdMask: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b' based on the comparison operand specified by 'imm8', and
// store the results in mask vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VCMPPD'. Intrinsic: '_mm_cmp_pd_mask'.
// Requires AVX512F.
func CmpPdMask(a M128d, b M128d, imm8 int) Mmask8 {
	return Mmask8(cmpPdMask([2]float64(a), [2]float64(b), imm8))
}

func cmpPdMask(a [2]float64, b [2]float64, imm8 int) uint8


// MaskCmpPdMask: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' based on the comparison operand specified by 'imm8',
// and store the results in mask vector 'k' using zeromask 'k1' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VCMPPD'. Intrinsic: '_mm_mask_cmp_pd_mask'.
// Requires AVX512F.
func MaskCmpPdMask(k1 Mmask8, a M128d, b M128d, imm8 int) Mmask8 {
	return Mmask8(maskCmpPdMask(uint8(k1), [2]float64(a), [2]float64(b), imm8))
}

func maskCmpPdMask(k1 uint8, a [2]float64, b [2]float64, imm8 int) uint8


// CmpPs: Compare packed single-precision (32-bit) floating-point elements in
// 'a' and 'b' based on the comparison operand specified by 'imm8', and store
// the results in 'dst'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := ( a[i+31:i] OP b[i+31:i] ) ? 0xFFFFFFFF : 0
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCMPPS'. Intrinsic: '_mm_cmp_ps'.
// Requires AVX.
func CmpPs(a M128, b M128, imm8 int) M128 {
	return M128(cmpPs([4]float32(a), [4]float32(b), imm8))
}

func cmpPs(a [4]float32, b [4]float32, imm8 int) [4]float32


// CmpPsMask: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b' based on the comparison operand specified by 'imm8', and
// store the results in mask vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VCMPPS'. Intrinsic: '_mm_cmp_ps_mask'.
// Requires AVX512F.
func CmpPsMask(a M128, b M128, imm8 int) Mmask8 {
	return Mmask8(cmpPsMask([4]float32(a), [4]float32(b), imm8))
}

func cmpPsMask(a [4]float32, b [4]float32, imm8 int) uint8


// MaskCmpPsMask: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' based on the comparison operand specified by 'imm8',
// and store the results in mask vector 'k' using zeromask 'k1' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VCMPPS'. Intrinsic: '_mm_mask_cmp_ps_mask'.
// Requires AVX512F.
func MaskCmpPsMask(k1 Mmask8, a M128, b M128, imm8 int) Mmask8 {
	return Mmask8(maskCmpPsMask(uint8(k1), [4]float32(a), [4]float32(b), imm8))
}

func maskCmpPsMask(k1 uint8, a [4]float32, b [4]float32, imm8 int) uint8


// CmpRoundSdMask: Compare the lower double-precision (64-bit) floating-point
// element in 'a' and 'b' based on the comparison operand specified by 'imm8',
// and store the result in mask vector 'k'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		
//		k[0] := ( a[63:0] OP b[63:0] ) ? 1 : 0
//		
//		k[MAX:1] := 0
//
// Instruction: 'VCMPSD'. Intrinsic: '_mm_cmp_round_sd_mask'.
// Requires AVX512F.
func CmpRoundSdMask(a M128d, b M128d, imm8 int, sae int) Mmask8 {
	return Mmask8(cmpRoundSdMask([2]float64(a), [2]float64(b), imm8, sae))
}

func cmpRoundSdMask(a [2]float64, b [2]float64, imm8 int, sae int) uint8


// MaskCmpRoundSdMask: Compare the lower double-precision (64-bit)
// floating-point element in 'a' and 'b' based on the comparison operand
// specified by 'imm8', and store the result in mask vector 'k' using zeromask
// 'k1' (the element is zeroed out when mask bit 0 is not set).
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		
//		IF k1[0]
//			k[0] := ( a[63:0] OP b[63:0] ) ? 1 : 0
//		ELSE
//			k[0] := 0
//		FI
//		k[MAX:1] := 0
//
// Instruction: 'VCMPSD'. Intrinsic: '_mm_mask_cmp_round_sd_mask'.
// Requires AVX512F.
func MaskCmpRoundSdMask(k1 Mmask8, a M128d, b M128d, imm8 int, sae int) Mmask8 {
	return Mmask8(maskCmpRoundSdMask(uint8(k1), [2]float64(a), [2]float64(b), imm8, sae))
}

func maskCmpRoundSdMask(k1 uint8, a [2]float64, b [2]float64, imm8 int, sae int) uint8


// CmpRoundSsMask: Compare the lower single-precision (32-bit) floating-point
// element in 'a' and 'b' based on the comparison operand specified by 'imm8',
// and store the result in mask vector 'k'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		
//		k[0] := ( a[31:0] OP b[31:0] ) ? 1 : 0
//		
//		k[MAX:1] := 0
//
// Instruction: 'VCMPSS'. Intrinsic: '_mm_cmp_round_ss_mask'.
// Requires AVX512F.
func CmpRoundSsMask(a M128, b M128, imm8 int, sae int) Mmask8 {
	return Mmask8(cmpRoundSsMask([4]float32(a), [4]float32(b), imm8, sae))
}

func cmpRoundSsMask(a [4]float32, b [4]float32, imm8 int, sae int) uint8


// MaskCmpRoundSsMask: Compare the lower single-precision (32-bit)
// floating-point element in 'a' and 'b' based on the comparison operand
// specified by 'imm8', and store the result in mask vector 'k' using zeromask
// 'k1' (the element is zeroed out when mask bit 0 is not set). 
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		
//		IF k1[0]
//			k[0] := ( a[31:0] OP b[31:0] ) ? 1 : 0
//		ELSE
//			k[0] := 0
//		FI
//		k[MAX:1] := 0
//
// Instruction: 'VCMPSS'. Intrinsic: '_mm_mask_cmp_round_ss_mask'.
// Requires AVX512F.
func MaskCmpRoundSsMask(k1 Mmask8, a M128, b M128, imm8 int, sae int) Mmask8 {
	return Mmask8(maskCmpRoundSsMask(uint8(k1), [4]float32(a), [4]float32(b), imm8, sae))
}

func maskCmpRoundSsMask(k1 uint8, a [4]float32, b [4]float32, imm8 int, sae int) uint8


// CmpSd: Compare the lower double-precision (64-bit) floating-point element in
// 'a' and 'b' based on the comparison operand specified by 'imm8', store the
// result in the lower element of 'dst', and copy the upper element from 'a' to
// the upper element of 'dst'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		
//		dst[63:0] := ( a[63:0] OP b[63:0] ) ? 0xFFFFFFFFFFFFFFFF : 0
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VCMPSD'. Intrinsic: '_mm_cmp_sd'.
// Requires AVX.
func CmpSd(a M128d, b M128d, imm8 int) M128d {
	return M128d(cmpSd([2]float64(a), [2]float64(b), imm8))
}

func cmpSd(a [2]float64, b [2]float64, imm8 int) [2]float64


// CmpSdMask: Compare the lower double-precision (64-bit) floating-point
// element in 'a' and 'b' based on the comparison operand specified by 'imm8',
// and store the result in mask vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		
//		k[0] := ( a[63:0] OP b[63:0] ) ? 1 : 0
//		
//		k[MAX:1] := 0
//
// Instruction: 'VCMPSD'. Intrinsic: '_mm_cmp_sd_mask'.
// Requires AVX512F.
func CmpSdMask(a M128d, b M128d, imm8 int) Mmask8 {
	return Mmask8(cmpSdMask([2]float64(a), [2]float64(b), imm8))
}

func cmpSdMask(a [2]float64, b [2]float64, imm8 int) uint8


// MaskCmpSdMask: Compare the lower double-precision (64-bit) floating-point
// element in 'a' and 'b' based on the comparison operand specified by 'imm8',
// and store the result in mask vector 'k' using zeromask 'k1' (the element is
// zeroed out when mask bit 0 is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		
//		IF k1[0]
//			k[0] := ( a[63:0] OP b[63:0] ) ? 1 : 0
//		ELSE
//			k[0] := 0
//		FI
//		k[MAX:1] := 0
//
// Instruction: 'VCMPSD'. Intrinsic: '_mm_mask_cmp_sd_mask'.
// Requires AVX512F.
func MaskCmpSdMask(k1 Mmask8, a M128d, b M128d, imm8 int) Mmask8 {
	return Mmask8(maskCmpSdMask(uint8(k1), [2]float64(a), [2]float64(b), imm8))
}

func maskCmpSdMask(k1 uint8, a [2]float64, b [2]float64, imm8 int) uint8


// CmpSs: Compare the lower single-precision (32-bit) floating-point element in
// 'a' and 'b' based on the comparison operand specified by 'imm8', store the
// result in the lower element of 'dst', and copy the upper 3 packed elements
// from 'a' to the upper elements of 'dst'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		
//		dst[31:0] := ( a[31:0] OP b[31:0] ) ? 0xFFFFFFFF : 0
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VCMPSS'. Intrinsic: '_mm_cmp_ss'.
// Requires AVX.
func CmpSs(a M128, b M128, imm8 int) M128 {
	return M128(cmpSs([4]float32(a), [4]float32(b), imm8))
}

func cmpSs(a [4]float32, b [4]float32, imm8 int) [4]float32


// CmpSsMask: Compare the lower single-precision (32-bit) floating-point
// element in 'a' and 'b' based on the comparison operand specified by 'imm8',
// and store the result in mask vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		
//		k[0] := ( a[31:0] OP b[31:0] ) ? 1 : 0
//		
//		k[MAX:1] := 0
//
// Instruction: 'VCMPSS'. Intrinsic: '_mm_cmp_ss_mask'.
// Requires AVX512F.
func CmpSsMask(a M128, b M128, imm8 int) Mmask8 {
	return Mmask8(cmpSsMask([4]float32(a), [4]float32(b), imm8))
}

func cmpSsMask(a [4]float32, b [4]float32, imm8 int) uint8


// MaskCmpSsMask: Compare the lower single-precision (32-bit) floating-point
// element in 'a' and 'b' based on the comparison operand specified by 'imm8',
// and store the result in mask vector 'k' using zeromask 'k1' (the element is
// zeroed out when mask bit 0 is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		
//		IF k1[0]
//			k[0] := ( a[31:0] OP b[31:0] ) ? 1 : 0
//		ELSE
//			k[0] := 0
//		FI
//		k[MAX:1] := 0
//
// Instruction: 'VCMPSS'. Intrinsic: '_mm_mask_cmp_ss_mask'.
// Requires AVX512F.
func MaskCmpSsMask(k1 Mmask8, a M128, b M128, imm8 int) Mmask8 {
	return Mmask8(maskCmpSsMask(uint8(k1), [4]float32(a), [4]float32(b), imm8))
}

func maskCmpSsMask(k1 uint8, a [4]float32, b [4]float32, imm8 int) uint8


// Cmpeq16: Compare packed 16-bit integers in 'a' and 'b' for equality, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			dst[i+15:i] := ( a[i+15:i] == b[i+15:i] ) ? 0xFFFF : 0
//		ENDFOR
//
// Instruction: 'PCMPEQW'. Intrinsic: '_mm_cmpeq_epi16'.
// Requires SSE2.
func Cmpeq16(a M128i, b M128i) M128i {
	return M128i(cmpeq16([16]byte(a), [16]byte(b)))
}

func cmpeq16(a [16]byte, b [16]byte) [16]byte


// Cmpeq16Mask: Compare packed 16-bit integers in 'a' and 'b' for equality, and
// store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			k[j] := ( a[i+15:i] == b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm_cmpeq_epi16_mask'.
// Requires AVX512BW.
func Cmpeq16Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpeq16Mask([16]byte(a), [16]byte(b)))
}

func cmpeq16Mask(a [16]byte, b [16]byte) uint8


// MaskCmpeq16Mask: Compare packed 16-bit integers in 'a' and 'b' for equality,
// and store the results in mask vector 'k1' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] == b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm_mask_cmpeq_epi16_mask'.
// Requires AVX512BW.
func MaskCmpeq16Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpeq16Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpeq16Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// Cmpeq32: Compare packed 32-bit integers in 'a' and 'b' for equality, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := ( a[i+31:i] == b[i+31:i] ) ? 0xFFFFFFFF : 0
//		ENDFOR
//
// Instruction: 'PCMPEQD'. Intrinsic: '_mm_cmpeq_epi32'.
// Requires SSE2.
func Cmpeq32(a M128i, b M128i) M128i {
	return M128i(cmpeq32([16]byte(a), [16]byte(b)))
}

func cmpeq32(a [16]byte, b [16]byte) [16]byte


// Cmpeq32Mask: Compare packed 32-bit integers in 'a' and 'b' for equality, and
// store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm_cmpeq_epi32_mask'.
// Requires AVX512F.
func Cmpeq32Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpeq32Mask([16]byte(a), [16]byte(b)))
}

func cmpeq32Mask(a [16]byte, b [16]byte) uint8


// MaskCmpeq32Mask: Compare packed 32-bit integers in 'a' and 'b' for equality,
// and store the results in mask vector 'k1' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm_mask_cmpeq_epi32_mask'.
// Requires AVX512F.
func MaskCmpeq32Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpeq32Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpeq32Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// Cmpeq64: Compare packed 64-bit integers in 'a' and 'b' for equality, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := ( a[i+63:i] == b[i+63:i] ) ? 0xFFFFFFFFFFFFFFFF : 0
//		ENDFOR
//
// Instruction: 'PCMPEQQ'. Intrinsic: '_mm_cmpeq_epi64'.
// Requires SSE4.1.
func Cmpeq64(a M128i, b M128i) M128i {
	return M128i(cmpeq64([16]byte(a), [16]byte(b)))
}

func cmpeq64(a [16]byte, b [16]byte) [16]byte


// Cmpeq64Mask: Compare packed 64-bit integers in 'a' and 'b' for equality, and
// store the results in mask vector 'k'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm_cmpeq_epi64_mask'.
// Requires AVX512F.
func Cmpeq64Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpeq64Mask([16]byte(a), [16]byte(b)))
}

func cmpeq64Mask(a [16]byte, b [16]byte) uint8


// MaskCmpeq64Mask: Compare packed 64-bit integers in 'a' and 'b' for equality,
// and store the results in mask vector 'k1' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm_mask_cmpeq_epi64_mask'.
// Requires AVX512F.
func MaskCmpeq64Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpeq64Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpeq64Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// Cmpeq8: Compare packed 8-bit integers in 'a' and 'b' for equality, and store
// the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*8
//			dst[i+7:i] := ( a[i+7:i] == b[i+7:i] ) ? 0xFF : 0
//		ENDFOR
//
// Instruction: 'PCMPEQB'. Intrinsic: '_mm_cmpeq_epi8'.
// Requires SSE2.
func Cmpeq8(a M128i, b M128i) M128i {
	return M128i(cmpeq8([16]byte(a), [16]byte(b)))
}

func cmpeq8(a [16]byte, b [16]byte) [16]byte


// Cmpeq8Mask: Compare packed 8-bit integers in 'a' and 'b' for equality, and
// store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*8
//			k[j] := ( a[i+7:i] == b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm_cmpeq_epi8_mask'.
// Requires AVX512BW.
func Cmpeq8Mask(a M128i, b M128i) Mmask16 {
	return Mmask16(cmpeq8Mask([16]byte(a), [16]byte(b)))
}

func cmpeq8Mask(a [16]byte, b [16]byte) uint16


// MaskCmpeq8Mask: Compare packed 8-bit integers in 'a' and 'b' for equality,
// and store the results in mask vector 'k' using zeromask 'k1' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] == b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm_mask_cmpeq_epi8_mask'.
// Requires AVX512BW.
func MaskCmpeq8Mask(k1 Mmask16, a M128i, b M128i) Mmask16 {
	return Mmask16(maskCmpeq8Mask(uint16(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpeq8Mask(k1 uint16, a [16]byte, b [16]byte) uint16


// CmpeqEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			k[j] := ( a[i+15:i] == b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm_cmpeq_epu16_mask'.
// Requires AVX512BW.
func CmpeqEpu16Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpeqEpu16Mask([16]byte(a), [16]byte(b)))
}

func cmpeqEpu16Mask(a [16]byte, b [16]byte) uint8


// MaskCmpeqEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b'
// for equality, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] == b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm_mask_cmpeq_epu16_mask'.
// Requires AVX512BW.
func MaskCmpeqEpu16Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpeqEpu16Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpeqEpu16Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// CmpeqEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm_cmpeq_epu32_mask'.
// Requires AVX512F.
func CmpeqEpu32Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpeqEpu32Mask([16]byte(a), [16]byte(b)))
}

func cmpeqEpu32Mask(a [16]byte, b [16]byte) uint8


// MaskCmpeqEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for equality, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm_mask_cmpeq_epu32_mask'.
// Requires AVX512F.
func MaskCmpeqEpu32Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpeqEpu32Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpeqEpu32Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// CmpeqEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm_cmpeq_epu64_mask'.
// Requires AVX512F.
func CmpeqEpu64Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpeqEpu64Mask([16]byte(a), [16]byte(b)))
}

func cmpeqEpu64Mask(a [16]byte, b [16]byte) uint8


// MaskCmpeqEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for equality, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm_mask_cmpeq_epu64_mask'.
// Requires AVX512F.
func MaskCmpeqEpu64Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpeqEpu64Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpeqEpu64Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// CmpeqEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*8
//			k[j] := ( a[i+7:i] == b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm_cmpeq_epu8_mask'.
// Requires AVX512BW.
func CmpeqEpu8Mask(a M128i, b M128i) Mmask16 {
	return Mmask16(cmpeqEpu8Mask([16]byte(a), [16]byte(b)))
}

func cmpeqEpu8Mask(a [16]byte, b [16]byte) uint16


// MaskCmpeqEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k' using zeromask 'k1'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] == b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm_mask_cmpeq_epu8_mask'.
// Requires AVX512BW.
func MaskCmpeqEpu8Mask(k1 Mmask16, a M128i, b M128i) Mmask16 {
	return Mmask16(maskCmpeqEpu8Mask(uint16(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpeqEpu8Mask(k1 uint16, a [16]byte, b [16]byte) uint16


// CmpeqPd: Compare packed double-precision (64-bit) floating-point elements in
// 'a' and 'b' for equality, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := (a[i+63:i] == b[i+63:i]) ? 0xFFFFFFFFFFFFFFFF : 0
//		ENDFOR
//
// Instruction: 'CMPPD'. Intrinsic: '_mm_cmpeq_pd'.
// Requires SSE2.
func CmpeqPd(a M128d, b M128d) M128d {
	return M128d(cmpeqPd([2]float64(a), [2]float64(b)))
}

func cmpeqPd(a [2]float64, b [2]float64) [2]float64


// CmpeqPs: Compare packed single-precision (32-bit) floating-point elements in
// 'a' and 'b' for equality, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := ( a[i+31:i] == b[i+31:i] ) ? 0xffffffff : 0
//		ENDFOR
//
// Instruction: 'CMPPS'. Intrinsic: '_mm_cmpeq_ps'.
// Requires SSE.
func CmpeqPs(a M128, b M128) M128 {
	return M128(cmpeqPs([4]float32(a), [4]float32(b)))
}

func cmpeqPs(a [4]float32, b [4]float32) [4]float32


// CmpeqSd: Compare the lower double-precision (64-bit) floating-point elements
// in 'a' and 'b' for equality, store the result in the lower element of 'dst',
// and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		dst[63:0] := (a[63:0] == b[63:0]) ? 0xFFFFFFFFFFFFFFFF : 0
//		dst[127:64] := a[127:64]
//
// Instruction: 'CMPSD'. Intrinsic: '_mm_cmpeq_sd'.
// Requires SSE2.
func CmpeqSd(a M128d, b M128d) M128d {
	return M128d(cmpeqSd([2]float64(a), [2]float64(b)))
}

func cmpeqSd(a [2]float64, b [2]float64) [2]float64


// CmpeqSs: Compare the lower single-precision (32-bit) floating-point elements
// in 'a' and 'b' for equality, store the result in the lower element of 'dst',
// and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'. 
//
//		dst[31:0] := ( a[31:0] == b[31:0] ) ? 0xffffffff : 0
//		dst[127:32] := a[127:32]
//
// Instruction: 'CMPSS'. Intrinsic: '_mm_cmpeq_ss'.
// Requires SSE.
func CmpeqSs(a M128, b M128) M128 {
	return M128(cmpeqSs([4]float32(a), [4]float32(b)))
}

func cmpeqSs(a [4]float32, b [4]float32) [4]float32


// Cmpge16Mask: Compare packed 16-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			k[j] := ( a[i+15:i] >= b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm_cmpge_epi16_mask'.
// Requires AVX512BW.
func Cmpge16Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpge16Mask([16]byte(a), [16]byte(b)))
}

func cmpge16Mask(a [16]byte, b [16]byte) uint8


// MaskCmpge16Mask: Compare packed 16-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] >= b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm_mask_cmpge_epi16_mask'.
// Requires AVX512BW.
func MaskCmpge16Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpge16Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpge16Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// Cmpge32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ( a[i+31:i] >= b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm_cmpge_epi32_mask'.
// Requires AVX512F.
func Cmpge32Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpge32Mask([16]byte(a), [16]byte(b)))
}

func cmpge32Mask(a [16]byte, b [16]byte) uint8


// MaskCmpge32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] >= b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm_mask_cmpge_epi32_mask'.
// Requires AVX512F.
func MaskCmpge32Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpge32Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpge32Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// Cmpge64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ( a[i+63:i] >= b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm_cmpge_epi64_mask'.
// Requires AVX512F.
func Cmpge64Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpge64Mask([16]byte(a), [16]byte(b)))
}

func cmpge64Mask(a [16]byte, b [16]byte) uint8


// MaskCmpge64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] >= b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm_mask_cmpge_epi64_mask'.
// Requires AVX512F.
func MaskCmpge64Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpge64Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpge64Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// Cmpge8Mask: Compare packed 8-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*8
//			k[j] := ( a[i+7:i] >= b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm_cmpge_epi8_mask'.
// Requires AVX512BW.
func Cmpge8Mask(a M128i, b M128i) Mmask16 {
	return Mmask16(cmpge8Mask([16]byte(a), [16]byte(b)))
}

func cmpge8Mask(a [16]byte, b [16]byte) uint16


// MaskCmpge8Mask: Compare packed 8-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k' using
// zeromask 'k1' (elements are zeroed out when the corresponding mask bit is
// not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] >= b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm_mask_cmpge_epi8_mask'.
// Requires AVX512BW.
func MaskCmpge8Mask(k1 Mmask16, a M128i, b M128i) Mmask16 {
	return Mmask16(maskCmpge8Mask(uint16(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpge8Mask(k1 uint16, a [16]byte, b [16]byte) uint16


// CmpgeEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			k[j] := ( a[i+15:i] >= b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm_cmpge_epu16_mask'.
// Requires AVX512BW.
func CmpgeEpu16Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpgeEpu16Mask([16]byte(a), [16]byte(b)))
}

func cmpgeEpu16Mask(a [16]byte, b [16]byte) uint8


// MaskCmpgeEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b'
// for greater-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] >= b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm_mask_cmpge_epu16_mask'.
// Requires AVX512BW.
func MaskCmpgeEpu16Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpgeEpu16Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpgeEpu16Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// CmpgeEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ( a[i+31:i] >= b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm_cmpge_epu32_mask'.
// Requires AVX512F.
func CmpgeEpu32Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpgeEpu32Mask([16]byte(a), [16]byte(b)))
}

func cmpgeEpu32Mask(a [16]byte, b [16]byte) uint8


// MaskCmpgeEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for greater-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] >= b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm_mask_cmpge_epu32_mask'.
// Requires AVX512F.
func MaskCmpgeEpu32Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpgeEpu32Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpgeEpu32Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// CmpgeEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ( a[i+63:i] >= b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm_cmpge_epu64_mask'.
// Requires AVX512F.
func CmpgeEpu64Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpgeEpu64Mask([16]byte(a), [16]byte(b)))
}

func cmpgeEpu64Mask(a [16]byte, b [16]byte) uint8


// MaskCmpgeEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for greater-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] >= b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm_mask_cmpge_epu64_mask'.
// Requires AVX512F.
func MaskCmpgeEpu64Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpgeEpu64Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpgeEpu64Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// CmpgeEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*8
//			k[j] := ( a[i+7:i] >= b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm_cmpge_epu8_mask'.
// Requires AVX512BW.
func CmpgeEpu8Mask(a M128i, b M128i) Mmask16 {
	return Mmask16(cmpgeEpu8Mask([16]byte(a), [16]byte(b)))
}

func cmpgeEpu8Mask(a [16]byte, b [16]byte) uint16


// MaskCmpgeEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k' using
// zeromask 'k1' (elements are zeroed out when the corresponding mask bit is
// not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] >= b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm_mask_cmpge_epu8_mask'.
// Requires AVX512BW.
func MaskCmpgeEpu8Mask(k1 Mmask16, a M128i, b M128i) Mmask16 {
	return Mmask16(maskCmpgeEpu8Mask(uint16(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpgeEpu8Mask(k1 uint16, a [16]byte, b [16]byte) uint16


// CmpgePd: Compare packed double-precision (64-bit) floating-point elements in
// 'a' and 'b' for greater-than-or-equal, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := (a[i+63:i] >= b[i+63:i]) ? 0xFFFFFFFFFFFFFFFF : 0
//		ENDFOR
//
// Instruction: 'CMPPD'. Intrinsic: '_mm_cmpge_pd'.
// Requires SSE2.
func CmpgePd(a M128d, b M128d) M128d {
	return M128d(cmpgePd([2]float64(a), [2]float64(b)))
}

func cmpgePd(a [2]float64, b [2]float64) [2]float64


// CmpgePs: Compare packed single-precision (32-bit) floating-point elements in
// 'a' and 'b' for greater-than-or-equal, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := ( a[i+31:i] >= b[i+31:i] ) ? 0xffffffff : 0
//		ENDFOR
//
// Instruction: 'CMPPS'. Intrinsic: '_mm_cmpge_ps'.
// Requires SSE.
func CmpgePs(a M128, b M128) M128 {
	return M128(cmpgePs([4]float32(a), [4]float32(b)))
}

func cmpgePs(a [4]float32, b [4]float32) [4]float32


// CmpgeSd: Compare the lower double-precision (64-bit) floating-point elements
// in 'a' and 'b' for greater-than-or-equal, store the result in the lower
// element of 'dst', and copy the upper element from 'a' to the upper element
// of 'dst'. 
//
//		dst[63:0] := (a[63:0] >= b[63:0]) ? 0xFFFFFFFFFFFFFFFF : 0
//		dst[127:64] := a[127:64]
//
// Instruction: 'CMPSD'. Intrinsic: '_mm_cmpge_sd'.
// Requires SSE2.
func CmpgeSd(a M128d, b M128d) M128d {
	return M128d(cmpgeSd([2]float64(a), [2]float64(b)))
}

func cmpgeSd(a [2]float64, b [2]float64) [2]float64


// CmpgeSs: Compare the lower single-precision (32-bit) floating-point elements
// in 'a' and 'b' for greater-than-or-equal, store the result in the lower
// element of 'dst', and copy the upper 3 packed elements from 'a' to the upper
// elements of 'dst'. 
//
//		dst[31:0] := ( a[31:0] >= b[31:0] ) ? 0xffffffff : 0
//		dst[127:32] := a[127:32]
//
// Instruction: 'CMPSS'. Intrinsic: '_mm_cmpge_ss'.
// Requires SSE.
func CmpgeSs(a M128, b M128) M128 {
	return M128(cmpgeSs([4]float32(a), [4]float32(b)))
}

func cmpgeSs(a [4]float32, b [4]float32) [4]float32


// Cmpgt16: Compare packed 16-bit integers in 'a' and 'b' for greater-than, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			dst[i+15:i] := ( a[i+15:i] > b[i+15:i] ) ? 0xFFFF : 0
//		ENDFOR
//
// Instruction: 'PCMPGTW'. Intrinsic: '_mm_cmpgt_epi16'.
// Requires SSE2.
func Cmpgt16(a M128i, b M128i) M128i {
	return M128i(cmpgt16([16]byte(a), [16]byte(b)))
}

func cmpgt16(a [16]byte, b [16]byte) [16]byte


// Cmpgt16Mask: Compare packed 16-bit integers in 'a' and 'b' for greater-than,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			k[j] := ( a[i+15:i] > b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm_cmpgt_epi16_mask'.
// Requires AVX512BW.
func Cmpgt16Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpgt16Mask([16]byte(a), [16]byte(b)))
}

func cmpgt16Mask(a [16]byte, b [16]byte) uint8


// MaskCmpgt16Mask: Compare packed 16-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] >== b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm_mask_cmpgt_epi16_mask'.
// Requires AVX512BW.
func MaskCmpgt16Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpgt16Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpgt16Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// Cmpgt32: Compare packed 32-bit integers in 'a' and 'b' for greater-than, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := ( a[i+31:i] > b[i+31:i] ) ? 0xFFFFFFFF : 0
//		ENDFOR
//
// Instruction: 'PCMPGTD'. Intrinsic: '_mm_cmpgt_epi32'.
// Requires SSE2.
func Cmpgt32(a M128i, b M128i) M128i {
	return M128i(cmpgt32([16]byte(a), [16]byte(b)))
}

func cmpgt32(a [16]byte, b [16]byte) [16]byte


// Cmpgt32Mask: Compare packed 32-bit integers in 'a' and 'b' for greater-than,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ( a[i+31:i] > b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm_cmpgt_epi32_mask'.
// Requires AVX512F.
func Cmpgt32Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpgt32Mask([16]byte(a), [16]byte(b)))
}

func cmpgt32Mask(a [16]byte, b [16]byte) uint8


// MaskCmpgt32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] > b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm_mask_cmpgt_epi32_mask'.
// Requires AVX512F.
func MaskCmpgt32Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpgt32Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpgt32Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// Cmpgt64: Compare packed 64-bit integers in 'a' and 'b' for greater-than, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := ( a[i+63:i] > b[i+63:i] ) ? 0xFFFFFFFFFFFFFFFF : 0
//		ENDFOR
//
// Instruction: 'PCMPGTQ'. Intrinsic: '_mm_cmpgt_epi64'.
// Requires SSE4.2.
func Cmpgt64(a M128i, b M128i) M128i {
	return M128i(cmpgt64([16]byte(a), [16]byte(b)))
}

func cmpgt64(a [16]byte, b [16]byte) [16]byte


// Cmpgt64Mask: Compare packed 64-bit integers in 'a' and 'b' for greater-than,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ( a[i+63:i] > b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm_cmpgt_epi64_mask'.
// Requires AVX512F.
func Cmpgt64Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpgt64Mask([16]byte(a), [16]byte(b)))
}

func cmpgt64Mask(a [16]byte, b [16]byte) uint8


// MaskCmpgt64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] > b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm_mask_cmpgt_epi64_mask'.
// Requires AVX512F.
func MaskCmpgt64Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpgt64Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpgt64Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// Cmpgt8: Compare packed 8-bit integers in 'a' and 'b' for greater-than, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*8
//			dst[i+7:i] := ( a[i+7:i] > b[i+7:i] ) ? 0xFF : 0
//		ENDFOR
//
// Instruction: 'PCMPGTB'. Intrinsic: '_mm_cmpgt_epi8'.
// Requires SSE2.
func Cmpgt8(a M128i, b M128i) M128i {
	return M128i(cmpgt8([16]byte(a), [16]byte(b)))
}

func cmpgt8(a [16]byte, b [16]byte) [16]byte


// Cmpgt8Mask: Compare packed 8-bit integers in 'a' and 'b' for greater-than,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*8
//			k[j] := ( a[i+7:i] > b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm_cmpgt_epi8_mask'.
// Requires AVX512BW.
func Cmpgt8Mask(a M128i, b M128i) Mmask16 {
	return Mmask16(cmpgt8Mask([16]byte(a), [16]byte(b)))
}

func cmpgt8Mask(a [16]byte, b [16]byte) uint16


// MaskCmpgt8Mask: Compare packed 8-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k' using zeromask 'k1'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] > b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm_mask_cmpgt_epi8_mask'.
// Requires AVX512BW.
func MaskCmpgt8Mask(k1 Mmask16, a M128i, b M128i) Mmask16 {
	return Mmask16(maskCmpgt8Mask(uint16(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpgt8Mask(k1 uint16, a [16]byte, b [16]byte) uint16


// CmpgtEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			k[j] := ( a[i+15:i] > b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm_cmpgt_epu16_mask'.
// Requires AVX512BW.
func CmpgtEpu16Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpgtEpu16Mask([16]byte(a), [16]byte(b)))
}

func cmpgtEpu16Mask(a [16]byte, b [16]byte) uint8


// MaskCmpgtEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b'
// for greater-than, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] >== b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm_mask_cmpgt_epu16_mask'.
// Requires AVX512BW.
func MaskCmpgtEpu16Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpgtEpu16Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpgtEpu16Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// CmpgtEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ( a[i+31:i] > b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm_cmpgt_epu32_mask'.
// Requires AVX512F.
func CmpgtEpu32Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpgtEpu32Mask([16]byte(a), [16]byte(b)))
}

func cmpgtEpu32Mask(a [16]byte, b [16]byte) uint8


// MaskCmpgtEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for greater-than, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] > b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm_mask_cmpgt_epu32_mask'.
// Requires AVX512F.
func MaskCmpgtEpu32Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpgtEpu32Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpgtEpu32Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// CmpgtEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ( a[i+63:i] > b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm_cmpgt_epu64_mask'.
// Requires AVX512F.
func CmpgtEpu64Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpgtEpu64Mask([16]byte(a), [16]byte(b)))
}

func cmpgtEpu64Mask(a [16]byte, b [16]byte) uint8


// MaskCmpgtEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for greater-than, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] > b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm_mask_cmpgt_epu64_mask'.
// Requires AVX512F.
func MaskCmpgtEpu64Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpgtEpu64Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpgtEpu64Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// CmpgtEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*8
//			k[j] := ( a[i+7:i] > b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm_cmpgt_epu8_mask'.
// Requires AVX512BW.
func CmpgtEpu8Mask(a M128i, b M128i) Mmask16 {
	return Mmask16(cmpgtEpu8Mask([16]byte(a), [16]byte(b)))
}

func cmpgtEpu8Mask(a [16]byte, b [16]byte) uint16


// MaskCmpgtEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k' using zeromask 'k1'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] > b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm_mask_cmpgt_epu8_mask'.
// Requires AVX512BW.
func MaskCmpgtEpu8Mask(k1 Mmask16, a M128i, b M128i) Mmask16 {
	return Mmask16(maskCmpgtEpu8Mask(uint16(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpgtEpu8Mask(k1 uint16, a [16]byte, b [16]byte) uint16


// CmpgtPd: Compare packed double-precision (64-bit) floating-point elements in
// 'a' and 'b' for greater-than, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := (a[i+63:i] > b[i+63:i]) ? 0xFFFFFFFFFFFFFFFF : 0
//		ENDFOR
//
// Instruction: 'CMPPD'. Intrinsic: '_mm_cmpgt_pd'.
// Requires SSE2.
func CmpgtPd(a M128d, b M128d) M128d {
	return M128d(cmpgtPd([2]float64(a), [2]float64(b)))
}

func cmpgtPd(a [2]float64, b [2]float64) [2]float64


// CmpgtPs: Compare packed single-precision (32-bit) floating-point elements in
// 'a' and 'b' for greater-than, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := ( a[i+31:i] > b[i+31:i] ) ? 0xffffffff : 0
//		ENDFOR
//
// Instruction: 'CMPPS'. Intrinsic: '_mm_cmpgt_ps'.
// Requires SSE.
func CmpgtPs(a M128, b M128) M128 {
	return M128(cmpgtPs([4]float32(a), [4]float32(b)))
}

func cmpgtPs(a [4]float32, b [4]float32) [4]float32


// CmpgtSd: Compare the lower double-precision (64-bit) floating-point elements
// in 'a' and 'b' for greater-than, store the result in the lower element of
// 'dst', and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		dst[63:0] := (a[63:0] > b[63:0]) ? 0xFFFFFFFFFFFFFFFF : 0
//		dst[127:64] := a[127:64]
//
// Instruction: 'CMPSD'. Intrinsic: '_mm_cmpgt_sd'.
// Requires SSE2.
func CmpgtSd(a M128d, b M128d) M128d {
	return M128d(cmpgtSd([2]float64(a), [2]float64(b)))
}

func cmpgtSd(a [2]float64, b [2]float64) [2]float64


// CmpgtSs: Compare the lower single-precision (32-bit) floating-point elements
// in 'a' and 'b' for greater-than, store the result in the lower element of
// 'dst', and copy the upper 3 packed elements from 'a' to the upper elements
// of 'dst'. 
//
//		dst[31:0] := ( a[31:0] > b[31:0] ) ? 0xffffffff : 0
//		dst[127:32] := a[127:32]
//
// Instruction: 'CMPSS'. Intrinsic: '_mm_cmpgt_ss'.
// Requires SSE.
func CmpgtSs(a M128, b M128) M128 {
	return M128(cmpgtSs([4]float32(a), [4]float32(b)))
}

func cmpgtSs(a [4]float32, b [4]float32) [4]float32


// Cmple16Mask: Compare packed 16-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			k[j] := ( a[i+15:i] <= b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm_cmple_epi16_mask'.
// Requires AVX512BW.
func Cmple16Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmple16Mask([16]byte(a), [16]byte(b)))
}

func cmple16Mask(a [16]byte, b [16]byte) uint8


// MaskCmple16Mask: Compare packed 16-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] <= b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm_mask_cmple_epi16_mask'.
// Requires AVX512BW.
func MaskCmple16Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmple16Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmple16Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// Cmple32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ( a[i+31:i] <= b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm_cmple_epi32_mask'.
// Requires AVX512F.
func Cmple32Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmple32Mask([16]byte(a), [16]byte(b)))
}

func cmple32Mask(a [16]byte, b [16]byte) uint8


// MaskCmple32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] <= b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm_mask_cmple_epi32_mask'.
// Requires AVX512F.
func MaskCmple32Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmple32Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmple32Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// Cmple64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ( a[i+63:i] <= b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm_cmple_epi64_mask'.
// Requires AVX512F.
func Cmple64Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmple64Mask([16]byte(a), [16]byte(b)))
}

func cmple64Mask(a [16]byte, b [16]byte) uint8


// MaskCmple64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] <= b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm_mask_cmple_epi64_mask'.
// Requires AVX512F.
func MaskCmple64Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmple64Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmple64Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// Cmple8Mask: Compare packed 8-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*8
//			k[j] := ( a[i+7:i] <= b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm_cmple_epi8_mask'.
// Requires AVX512BW.
func Cmple8Mask(a M128i, b M128i) Mmask16 {
	return Mmask16(cmple8Mask([16]byte(a), [16]byte(b)))
}

func cmple8Mask(a [16]byte, b [16]byte) uint16


// MaskCmple8Mask: Compare packed 8-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k' using zeromask
// 'k1' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] <= b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm_mask_cmple_epi8_mask'.
// Requires AVX512BW.
func MaskCmple8Mask(k1 Mmask16, a M128i, b M128i) Mmask16 {
	return Mmask16(maskCmple8Mask(uint16(k1), [16]byte(a), [16]byte(b)))
}

func maskCmple8Mask(k1 uint16, a [16]byte, b [16]byte) uint16


// CmpleEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			k[j] := ( a[i+15:i] <= b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm_cmple_epu16_mask'.
// Requires AVX512BW.
func CmpleEpu16Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpleEpu16Mask([16]byte(a), [16]byte(b)))
}

func cmpleEpu16Mask(a [16]byte, b [16]byte) uint8


// MaskCmpleEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b'
// for less-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] <= b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm_mask_cmple_epu16_mask'.
// Requires AVX512BW.
func MaskCmpleEpu16Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpleEpu16Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpleEpu16Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// CmpleEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ( a[i+31:i] <= b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm_cmple_epu32_mask'.
// Requires AVX512F.
func CmpleEpu32Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpleEpu32Mask([16]byte(a), [16]byte(b)))
}

func cmpleEpu32Mask(a [16]byte, b [16]byte) uint8


// MaskCmpleEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for less-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] <= b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm_mask_cmple_epu32_mask'.
// Requires AVX512F.
func MaskCmpleEpu32Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpleEpu32Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpleEpu32Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// CmpleEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ( a[i+63:i] <= b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm_cmple_epu64_mask'.
// Requires AVX512F.
func CmpleEpu64Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpleEpu64Mask([16]byte(a), [16]byte(b)))
}

func cmpleEpu64Mask(a [16]byte, b [16]byte) uint8


// MaskCmpleEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for less-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] <= b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm_mask_cmple_epu64_mask'.
// Requires AVX512F.
func MaskCmpleEpu64Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpleEpu64Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpleEpu64Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// CmpleEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*8
//			k[j] := ( a[i+7:i] <= b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm_cmple_epu8_mask'.
// Requires AVX512BW.
func CmpleEpu8Mask(a M128i, b M128i) Mmask16 {
	return Mmask16(cmpleEpu8Mask([16]byte(a), [16]byte(b)))
}

func cmpleEpu8Mask(a [16]byte, b [16]byte) uint16


// MaskCmpleEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k' using zeromask
// 'k1' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] <= b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm_mask_cmple_epu8_mask'.
// Requires AVX512BW.
func MaskCmpleEpu8Mask(k1 Mmask16, a M128i, b M128i) Mmask16 {
	return Mmask16(maskCmpleEpu8Mask(uint16(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpleEpu8Mask(k1 uint16, a [16]byte, b [16]byte) uint16


// CmplePd: Compare packed double-precision (64-bit) floating-point elements in
// 'a' and 'b' for less-than-or-equal, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := (a[i+63:i] <= b[i+63:i]) ? 0xFFFFFFFFFFFFFFFF : 0
//		ENDFOR
//
// Instruction: 'CMPPD'. Intrinsic: '_mm_cmple_pd'.
// Requires SSE2.
func CmplePd(a M128d, b M128d) M128d {
	return M128d(cmplePd([2]float64(a), [2]float64(b)))
}

func cmplePd(a [2]float64, b [2]float64) [2]float64


// CmplePs: Compare packed single-precision (32-bit) floating-point elements in
// 'a' and 'b' for less-than-or-equal, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := ( a[i+31:i] <= b[i+31:i] ) ? 0xffffffff : 0
//		ENDFOR
//
// Instruction: 'CMPPS'. Intrinsic: '_mm_cmple_ps'.
// Requires SSE.
func CmplePs(a M128, b M128) M128 {
	return M128(cmplePs([4]float32(a), [4]float32(b)))
}

func cmplePs(a [4]float32, b [4]float32) [4]float32


// CmpleSd: Compare the lower double-precision (64-bit) floating-point elements
// in 'a' and 'b' for less-than-or-equal, store the result in the lower element
// of 'dst', and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		dst[63:0] := (a[63:0] <= b[63:0]) ? 0xFFFFFFFFFFFFFFFF : 0
//		dst[127:64] := a[127:64]
//
// Instruction: 'CMPSD'. Intrinsic: '_mm_cmple_sd'.
// Requires SSE2.
func CmpleSd(a M128d, b M128d) M128d {
	return M128d(cmpleSd([2]float64(a), [2]float64(b)))
}

func cmpleSd(a [2]float64, b [2]float64) [2]float64


// CmpleSs: Compare the lower single-precision (32-bit) floating-point elements
// in 'a' and 'b' for less-than-or-equal, store the result in the lower element
// of 'dst', and copy the upper 3 packed elements from 'a' to the upper
// elements of 'dst'. 
//
//		dst[31:0] := ( a[31:0] <= b[31:0] ) ? 0xffffffff : 0
//		dst[127:32] := a[127:32]
//
// Instruction: 'CMPSS'. Intrinsic: '_mm_cmple_ss'.
// Requires SSE.
func CmpleSs(a M128, b M128) M128 {
	return M128(cmpleSs([4]float32(a), [4]float32(b)))
}

func cmpleSs(a [4]float32, b [4]float32) [4]float32


// Cmplt16: Compare packed 16-bit integers in 'a' and 'b' for less-than, and
// store the results in 'dst'. Note: This intrinsic emits the pcmpgtw
// instruction with the order of the operands switched. 
//
//		FOR j := 0 to 7
//			i := j*16
//			dst[i+15:i] := ( a[i+15:i] < b[i+15:i] ) ? 0xFFFF : 0
//		ENDFOR
//
// Instruction: 'PCMPGTW'. Intrinsic: '_mm_cmplt_epi16'.
// Requires SSE2.
func Cmplt16(a M128i, b M128i) M128i {
	return M128i(cmplt16([16]byte(a), [16]byte(b)))
}

func cmplt16(a [16]byte, b [16]byte) [16]byte


// Cmplt16Mask: Compare packed 16-bit integers in 'a' and 'b' for less-than,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			k[j] := ( a[i+15:i] < b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm_cmplt_epi16_mask'.
// Requires AVX512BW.
func Cmplt16Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmplt16Mask([16]byte(a), [16]byte(b)))
}

func cmplt16Mask(a [16]byte, b [16]byte) uint8


// MaskCmplt16Mask: Compare packed 16-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] < b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm_mask_cmplt_epi16_mask'.
// Requires AVX512BW.
func MaskCmplt16Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmplt16Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmplt16Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// Cmplt32: Compare packed 32-bit integers in 'a' and 'b' for less-than, and
// store the results in 'dst'. Note: This intrinsic emits the pcmpgtd
// instruction with the order of the operands switched. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := ( a[i+31:i] < b[i+31:i] ) ? 0xFFFFFFFF : 0
//		ENDFOR
//
// Instruction: 'PCMPGTD'. Intrinsic: '_mm_cmplt_epi32'.
// Requires SSE2.
func Cmplt32(a M128i, b M128i) M128i {
	return M128i(cmplt32([16]byte(a), [16]byte(b)))
}

func cmplt32(a [16]byte, b [16]byte) [16]byte


// Cmplt32Mask: Compare packed 32-bit integers in 'a' and 'b' for less-than,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ( a[i+31:i] < b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm_cmplt_epi32_mask'.
// Requires AVX512F.
func Cmplt32Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmplt32Mask([16]byte(a), [16]byte(b)))
}

func cmplt32Mask(a [16]byte, b [16]byte) uint8


// MaskCmplt32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] < b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm_mask_cmplt_epi32_mask'.
// Requires AVX512F.
func MaskCmplt32Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmplt32Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmplt32Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// Cmplt64Mask: Compare packed 64-bit integers in 'a' and 'b' for less-than,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ( a[i+63:i] < b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm_cmplt_epi64_mask'.
// Requires AVX512F.
func Cmplt64Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmplt64Mask([16]byte(a), [16]byte(b)))
}

func cmplt64Mask(a [16]byte, b [16]byte) uint8


// MaskCmplt64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] < b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm_mask_cmplt_epi64_mask'.
// Requires AVX512F.
func MaskCmplt64Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmplt64Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmplt64Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// Cmplt8: Compare packed 8-bit integers in 'a' and 'b' for less-than, and
// store the results in 'dst'. Note: This intrinsic emits the pcmpgtb
// instruction with the order of the operands switched. 
//
//		FOR j := 0 to 15
//			i := j*8
//			dst[i+7:i] := ( a[i+7:i] < b[i+7:i] ) ? 0xFF : 0
//		ENDFOR
//
// Instruction: 'PCMPGTB'. Intrinsic: '_mm_cmplt_epi8'.
// Requires SSE2.
func Cmplt8(a M128i, b M128i) M128i {
	return M128i(cmplt8([16]byte(a), [16]byte(b)))
}

func cmplt8(a [16]byte, b [16]byte) [16]byte


// Cmplt8Mask: Compare packed 8-bit integers in 'a' and 'b' for less-than, and
// store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*8
//			k[j] := ( a[i+7:i] < b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm_cmplt_epi8_mask'.
// Requires AVX512BW.
func Cmplt8Mask(a M128i, b M128i) Mmask16 {
	return Mmask16(cmplt8Mask([16]byte(a), [16]byte(b)))
}

func cmplt8Mask(a [16]byte, b [16]byte) uint16


// MaskCmplt8Mask: Compare packed 8-bit integers in 'a' and 'b' for less-than,
// and store the results in mask vector 'k' using zeromask 'k1' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] < b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm_mask_cmplt_epi8_mask'.
// Requires AVX512BW.
func MaskCmplt8Mask(k1 Mmask16, a M128i, b M128i) Mmask16 {
	return Mmask16(maskCmplt8Mask(uint16(k1), [16]byte(a), [16]byte(b)))
}

func maskCmplt8Mask(k1 uint16, a [16]byte, b [16]byte) uint16


// CmpltEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			k[j] := ( a[i+15:i] < b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm_cmplt_epu16_mask'.
// Requires AVX512BW.
func CmpltEpu16Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpltEpu16Mask([16]byte(a), [16]byte(b)))
}

func cmpltEpu16Mask(a [16]byte, b [16]byte) uint8


// MaskCmpltEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b'
// for less-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] < b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm_mask_cmplt_epu16_mask'.
// Requires AVX512BW.
func MaskCmpltEpu16Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpltEpu16Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpltEpu16Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// CmpltEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ( a[i+31:i] < b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm_cmplt_epu32_mask'.
// Requires AVX512F.
func CmpltEpu32Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpltEpu32Mask([16]byte(a), [16]byte(b)))
}

func cmpltEpu32Mask(a [16]byte, b [16]byte) uint8


// MaskCmpltEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for less-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] < b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm_mask_cmplt_epu32_mask'.
// Requires AVX512F.
func MaskCmpltEpu32Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpltEpu32Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpltEpu32Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// CmpltEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ( a[i+63:i] < b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm_cmplt_epu64_mask'.
// Requires AVX512F.
func CmpltEpu64Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpltEpu64Mask([16]byte(a), [16]byte(b)))
}

func cmpltEpu64Mask(a [16]byte, b [16]byte) uint8


// MaskCmpltEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for less-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] < b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm_mask_cmplt_epu64_mask'.
// Requires AVX512F.
func MaskCmpltEpu64Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpltEpu64Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpltEpu64Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// CmpltEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*8
//			k[j] := ( a[i+7:i] < b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm_cmplt_epu8_mask'.
// Requires AVX512BW.
func CmpltEpu8Mask(a M128i, b M128i) Mmask16 {
	return Mmask16(cmpltEpu8Mask([16]byte(a), [16]byte(b)))
}

func cmpltEpu8Mask(a [16]byte, b [16]byte) uint16


// MaskCmpltEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k' using zeromask 'k1'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] < b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm_mask_cmplt_epu8_mask'.
// Requires AVX512BW.
func MaskCmpltEpu8Mask(k1 Mmask16, a M128i, b M128i) Mmask16 {
	return Mmask16(maskCmpltEpu8Mask(uint16(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpltEpu8Mask(k1 uint16, a [16]byte, b [16]byte) uint16


// CmpltPd: Compare packed double-precision (64-bit) floating-point elements in
// 'a' and 'b' for less-than, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := (a[i+63:i] < b[i+63:i]) ? 0xFFFFFFFFFFFFFFFF : 0
//		ENDFOR
//
// Instruction: 'CMPPD'. Intrinsic: '_mm_cmplt_pd'.
// Requires SSE2.
func CmpltPd(a M128d, b M128d) M128d {
	return M128d(cmpltPd([2]float64(a), [2]float64(b)))
}

func cmpltPd(a [2]float64, b [2]float64) [2]float64


// CmpltPs: Compare packed single-precision (32-bit) floating-point elements in
// 'a' and 'b' for less-than, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := ( a[i+31:i] < b[i+31:i] ) ? 0xffffffff : 0
//		ENDFOR
//
// Instruction: 'CMPPS'. Intrinsic: '_mm_cmplt_ps'.
// Requires SSE.
func CmpltPs(a M128, b M128) M128 {
	return M128(cmpltPs([4]float32(a), [4]float32(b)))
}

func cmpltPs(a [4]float32, b [4]float32) [4]float32


// CmpltSd: Compare the lower double-precision (64-bit) floating-point elements
// in 'a' and 'b' for less-than, store the result in the lower element of
// 'dst', and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		dst[63:0] := (a[63:0] < b[63:0]) ? 0xFFFFFFFFFFFFFFFF : 0
//		dst[127:64] := a[127:64]
//
// Instruction: 'CMPSD'. Intrinsic: '_mm_cmplt_sd'.
// Requires SSE2.
func CmpltSd(a M128d, b M128d) M128d {
	return M128d(cmpltSd([2]float64(a), [2]float64(b)))
}

func cmpltSd(a [2]float64, b [2]float64) [2]float64


// CmpltSs: Compare the lower single-precision (32-bit) floating-point elements
// in 'a' and 'b' for less-than, store the result in the lower element of
// 'dst', and copy the upper 3 packed elements from 'a' to the upper elements
// of 'dst'. 
//
//		dst[31:0] := ( a[31:0] < b[31:0] ) ? 0xffffffff : 0
//		dst[127:32] := a[127:32]
//
// Instruction: 'CMPSS'. Intrinsic: '_mm_cmplt_ss'.
// Requires SSE.
func CmpltSs(a M128, b M128) M128 {
	return M128(cmpltSs([4]float32(a), [4]float32(b)))
}

func cmpltSs(a [4]float32, b [4]float32) [4]float32


// Cmpneq16Mask: Compare packed 16-bit integers in 'a' and 'b' for not-equal,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			k[j] := ( a[i+15:i] != b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm_cmpneq_epi16_mask'.
// Requires AVX512BW.
func Cmpneq16Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpneq16Mask([16]byte(a), [16]byte(b)))
}

func cmpneq16Mask(a [16]byte, b [16]byte) uint8


// MaskCmpneq16Mask: Compare packed 16-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] != b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm_mask_cmpneq_epi16_mask'.
// Requires AVX512BW.
func MaskCmpneq16Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpneq16Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpneq16Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// Cmpneq32Mask: Compare packed 32-bit integers in 'a' and 'b' for not-equal,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm_cmpneq_epi32_mask'.
// Requires AVX512F.
func Cmpneq32Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpneq32Mask([16]byte(a), [16]byte(b)))
}

func cmpneq32Mask(a [16]byte, b [16]byte) uint8


// MaskCmpneq32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm_mask_cmpneq_epi32_mask'.
// Requires AVX512F.
func MaskCmpneq32Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpneq32Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpneq32Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// Cmpneq64Mask: Compare packed 64-bit integers in 'a' and 'b' for not-equal,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm_cmpneq_epi64_mask'.
// Requires AVX512F.
func Cmpneq64Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpneq64Mask([16]byte(a), [16]byte(b)))
}

func cmpneq64Mask(a [16]byte, b [16]byte) uint8


// MaskCmpneq64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm_mask_cmpneq_epi64_mask'.
// Requires AVX512F.
func MaskCmpneq64Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpneq64Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpneq64Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// Cmpneq8Mask: Compare packed 8-bit integers in 'a' and 'b' for not-equal, and
// store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*8
//			k[j] := ( a[i+7:i] != b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm_cmpneq_epi8_mask'.
// Requires AVX512BW.
func Cmpneq8Mask(a M128i, b M128i) Mmask16 {
	return Mmask16(cmpneq8Mask([16]byte(a), [16]byte(b)))
}

func cmpneq8Mask(a [16]byte, b [16]byte) uint16


// MaskCmpneq8Mask: Compare packed 8-bit integers in 'a' and 'b' for not-equal,
// and store the results in mask vector 'k' using zeromask 'k1' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] != b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm_mask_cmpneq_epi8_mask'.
// Requires AVX512BW.
func MaskCmpneq8Mask(k1 Mmask16, a M128i, b M128i) Mmask16 {
	return Mmask16(maskCmpneq8Mask(uint16(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpneq8Mask(k1 uint16, a [16]byte, b [16]byte) uint16


// CmpneqEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			k[j] := ( a[i+15:i] != b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm_cmpneq_epu16_mask'.
// Requires AVX512BW.
func CmpneqEpu16Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpneqEpu16Mask([16]byte(a), [16]byte(b)))
}

func cmpneqEpu16Mask(a [16]byte, b [16]byte) uint8


// MaskCmpneqEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b'
// for not-equal, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] != b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm_mask_cmpneq_epu16_mask'.
// Requires AVX512BW.
func MaskCmpneqEpu16Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpneqEpu16Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpneqEpu16Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// CmpneqEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm_cmpneq_epu32_mask'.
// Requires AVX512F.
func CmpneqEpu32Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpneqEpu32Mask([16]byte(a), [16]byte(b)))
}

func cmpneqEpu32Mask(a [16]byte, b [16]byte) uint8


// MaskCmpneqEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for not-equal, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm_mask_cmpneq_epu32_mask'.
// Requires AVX512F.
func MaskCmpneqEpu32Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpneqEpu32Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpneqEpu32Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// CmpneqEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm_cmpneq_epu64_mask'.
// Requires AVX512F.
func CmpneqEpu64Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(cmpneqEpu64Mask([16]byte(a), [16]byte(b)))
}

func cmpneqEpu64Mask(a [16]byte, b [16]byte) uint8


// MaskCmpneqEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for not-equal, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm_mask_cmpneq_epu64_mask'.
// Requires AVX512F.
func MaskCmpneqEpu64Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskCmpneqEpu64Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpneqEpu64Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// CmpneqEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*8
//			k[j] := ( a[i+7:i] != b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm_cmpneq_epu8_mask'.
// Requires AVX512BW.
func CmpneqEpu8Mask(a M128i, b M128i) Mmask16 {
	return Mmask16(cmpneqEpu8Mask([16]byte(a), [16]byte(b)))
}

func cmpneqEpu8Mask(a [16]byte, b [16]byte) uint16


// MaskCmpneqEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b'
// for not-equal, and store the results in mask vector 'k' using zeromask 'k1'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] != b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm_mask_cmpneq_epu8_mask'.
// Requires AVX512BW.
func MaskCmpneqEpu8Mask(k1 Mmask16, a M128i, b M128i) Mmask16 {
	return Mmask16(maskCmpneqEpu8Mask(uint16(k1), [16]byte(a), [16]byte(b)))
}

func maskCmpneqEpu8Mask(k1 uint16, a [16]byte, b [16]byte) uint16


// CmpneqPd: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b' for not-equal, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := (a[i+63:i] != b[i+63:i]) ? 0xFFFFFFFFFFFFFFFF : 0
//		ENDFOR
//
// Instruction: 'CMPPD'. Intrinsic: '_mm_cmpneq_pd'.
// Requires SSE2.
func CmpneqPd(a M128d, b M128d) M128d {
	return M128d(cmpneqPd([2]float64(a), [2]float64(b)))
}

func cmpneqPd(a [2]float64, b [2]float64) [2]float64


// CmpneqPs: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b' for not-equal, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := ( a[i+31:i] != b[i+31:i] ) ? 0xffffffff : 0
//		ENDFOR
//
// Instruction: 'CMPPS'. Intrinsic: '_mm_cmpneq_ps'.
// Requires SSE.
func CmpneqPs(a M128, b M128) M128 {
	return M128(cmpneqPs([4]float32(a), [4]float32(b)))
}

func cmpneqPs(a [4]float32, b [4]float32) [4]float32


// CmpneqSd: Compare the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b' for not-equal, store the result in the lower element
// of 'dst', and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		dst[63:0] := (a[63:0] != b[63:0]) ? 0xFFFFFFFFFFFFFFFF : 0
//		dst[127:64] := a[127:64]
//
// Instruction: 'CMPSD'. Intrinsic: '_mm_cmpneq_sd'.
// Requires SSE2.
func CmpneqSd(a M128d, b M128d) M128d {
	return M128d(cmpneqSd([2]float64(a), [2]float64(b)))
}

func cmpneqSd(a [2]float64, b [2]float64) [2]float64


// CmpneqSs: Compare the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b' for not-equal, store the result in the lower element
// of 'dst', and copy the upper 3 packed elements from 'a' to the upper
// elements of 'dst'. 
//
//		dst[31:0] := ( a[31:0] != b[31:0] ) ? 0xffffffff : 0
//		dst[127:32] := a[127:32]
//
// Instruction: 'CMPSS'. Intrinsic: '_mm_cmpneq_ss'.
// Requires SSE.
func CmpneqSs(a M128, b M128) M128 {
	return M128(cmpneqSs([4]float32(a), [4]float32(b)))
}

func cmpneqSs(a [4]float32, b [4]float32) [4]float32


// CmpngePd: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b' for not-greater-than-or-equal, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := !(a[i+63:i] >= b[i+63:i]) ? 0xFFFFFFFFFFFFFFFF : 0
//		ENDFOR
//
// Instruction: 'CMPPD'. Intrinsic: '_mm_cmpnge_pd'.
// Requires SSE2.
func CmpngePd(a M128d, b M128d) M128d {
	return M128d(cmpngePd([2]float64(a), [2]float64(b)))
}

func cmpngePd(a [2]float64, b [2]float64) [2]float64


// CmpngePs: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b' for not-greater-than-or-equal, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := !( a[i+31:i] >= b[i+31:i] ) ? 0xffffffff : 0
//		ENDFOR
//
// Instruction: 'CMPPS'. Intrinsic: '_mm_cmpnge_ps'.
// Requires SSE.
func CmpngePs(a M128, b M128) M128 {
	return M128(cmpngePs([4]float32(a), [4]float32(b)))
}

func cmpngePs(a [4]float32, b [4]float32) [4]float32


// CmpngeSd: Compare the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b' for not-greater-than-or-equal, store the result in
// the lower element of 'dst', and copy the upper element from 'a' to the upper
// element of 'dst'. 
//
//		dst[63:0] := !(a[63:0] >= b[63:0]) ? 0xFFFFFFFFFFFFFFFF : 0
//		dst[127:64] := a[127:64]
//
// Instruction: 'CMPSD'. Intrinsic: '_mm_cmpnge_sd'.
// Requires SSE2.
func CmpngeSd(a M128d, b M128d) M128d {
	return M128d(cmpngeSd([2]float64(a), [2]float64(b)))
}

func cmpngeSd(a [2]float64, b [2]float64) [2]float64


// CmpngeSs: Compare the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b' for not-greater-than-or-equal, store the result in
// the lower element of 'dst', and copy the upper 3 packed elements from 'a' to
// the upper elements of 'dst'. 
//
//		dst[31:0] := !( a[31:0] >= b[31:0] ) ? 0xffffffff : 0
//		dst[127:32] := a[127:32]
//
// Instruction: 'CMPSS'. Intrinsic: '_mm_cmpnge_ss'.
// Requires SSE.
func CmpngeSs(a M128, b M128) M128 {
	return M128(cmpngeSs([4]float32(a), [4]float32(b)))
}

func cmpngeSs(a [4]float32, b [4]float32) [4]float32


// CmpngtPd: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b' for not-greater-than, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := !(a[i+63:i] > b[i+63:i]) ? 0xFFFFFFFFFFFFFFFF : 0
//		ENDFOR
//
// Instruction: 'CMPPD'. Intrinsic: '_mm_cmpngt_pd'.
// Requires SSE2.
func CmpngtPd(a M128d, b M128d) M128d {
	return M128d(cmpngtPd([2]float64(a), [2]float64(b)))
}

func cmpngtPd(a [2]float64, b [2]float64) [2]float64


// CmpngtPs: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b' for not-greater-than, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := !( a[i+31:i] > b[i+31:i] ) ? 0xffffffff : 0
//		ENDFOR
//
// Instruction: 'CMPPS'. Intrinsic: '_mm_cmpngt_ps'.
// Requires SSE.
func CmpngtPs(a M128, b M128) M128 {
	return M128(cmpngtPs([4]float32(a), [4]float32(b)))
}

func cmpngtPs(a [4]float32, b [4]float32) [4]float32


// CmpngtSd: Compare the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b' for not-greater-than, store the result in the lower
// element of 'dst', and copy the upper element from 'a' to the upper element
// of 'dst'. 
//
//		dst[63:0] := !(a[63:0] > b[63:0]) ? 0xFFFFFFFFFFFFFFFF : 0
//		dst[127:64] := a[127:64]
//
// Instruction: 'CMPSD'. Intrinsic: '_mm_cmpngt_sd'.
// Requires SSE2.
func CmpngtSd(a M128d, b M128d) M128d {
	return M128d(cmpngtSd([2]float64(a), [2]float64(b)))
}

func cmpngtSd(a [2]float64, b [2]float64) [2]float64


// CmpngtSs: Compare the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b' for not-greater-than, store the result in the lower
// element of 'dst', and copy the upper 3 packed elements from 'a' to the upper
// elements of 'dst'. 
//
//		dst[31:0] := !( a[31:0] > b[31:0] ) ? 0xffffffff : 0
//		dst[127:32] := a[127:32]
//
// Instruction: 'CMPSS'. Intrinsic: '_mm_cmpngt_ss'.
// Requires SSE.
func CmpngtSs(a M128, b M128) M128 {
	return M128(cmpngtSs([4]float32(a), [4]float32(b)))
}

func cmpngtSs(a [4]float32, b [4]float32) [4]float32


// CmpnlePd: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b' for not-less-than-or-equal, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := !(a[i+63:i] <= b[i+63:i]) ? 0xFFFFFFFFFFFFFFFF : 0
//		ENDFOR
//
// Instruction: 'CMPPD'. Intrinsic: '_mm_cmpnle_pd'.
// Requires SSE2.
func CmpnlePd(a M128d, b M128d) M128d {
	return M128d(cmpnlePd([2]float64(a), [2]float64(b)))
}

func cmpnlePd(a [2]float64, b [2]float64) [2]float64


// CmpnlePs: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b' for not-less-than-or-equal, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := !( a[i+31:i] <= b[i+31:i] ) ? 0xffffffff : 0
//		ENDFOR
//
// Instruction: 'CMPPS'. Intrinsic: '_mm_cmpnle_ps'.
// Requires SSE.
func CmpnlePs(a M128, b M128) M128 {
	return M128(cmpnlePs([4]float32(a), [4]float32(b)))
}

func cmpnlePs(a [4]float32, b [4]float32) [4]float32


// CmpnleSd: Compare the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b' for not-less-than-or-equal, store the result in the
// lower element of 'dst', and copy the upper element from 'a' to the upper
// element of 'dst'. 
//
//		dst[63:0] := !(a[63:0] <= b[63:0]) ? 0xFFFFFFFFFFFFFFFF : 0
//		dst[127:64] := a[127:64]
//
// Instruction: 'CMPSD'. Intrinsic: '_mm_cmpnle_sd'.
// Requires SSE2.
func CmpnleSd(a M128d, b M128d) M128d {
	return M128d(cmpnleSd([2]float64(a), [2]float64(b)))
}

func cmpnleSd(a [2]float64, b [2]float64) [2]float64


// CmpnleSs: Compare the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b' for not-less-than-or-equal, store the result in the
// lower element of 'dst', and copy the upper 3 packed elements from 'a' to the
// upper elements of 'dst'. 
//
//		dst[31:0] := !( a[31:0] <= b[31:0] ) ? 0xffffffff : 0
//		dst[127:32] := a[127:32]
//
// Instruction: 'CMPSS'. Intrinsic: '_mm_cmpnle_ss'.
// Requires SSE.
func CmpnleSs(a M128, b M128) M128 {
	return M128(cmpnleSs([4]float32(a), [4]float32(b)))
}

func cmpnleSs(a [4]float32, b [4]float32) [4]float32


// CmpnltPd: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b' for not-less-than, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := !(a[i+63:i] < b[i+63:i]) ? 0xFFFFFFFFFFFFFFFF : 0
//		ENDFOR
//
// Instruction: 'CMPPD'. Intrinsic: '_mm_cmpnlt_pd'.
// Requires SSE2.
func CmpnltPd(a M128d, b M128d) M128d {
	return M128d(cmpnltPd([2]float64(a), [2]float64(b)))
}

func cmpnltPd(a [2]float64, b [2]float64) [2]float64


// CmpnltPs: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b' for not-less-than, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := !( a[i+31:i] < b[i+31:i] ) ? 0xffffffff : 0
//		ENDFOR
//
// Instruction: 'CMPPS'. Intrinsic: '_mm_cmpnlt_ps'.
// Requires SSE.
func CmpnltPs(a M128, b M128) M128 {
	return M128(cmpnltPs([4]float32(a), [4]float32(b)))
}

func cmpnltPs(a [4]float32, b [4]float32) [4]float32


// CmpnltSd: Compare the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b' for not-less-than, store the result in the lower
// element of 'dst', and copy the upper element from 'a' to the upper element
// of 'dst'. 
//
//		dst[63:0] := !(a[63:0] < b[63:0]) ? 0xFFFFFFFFFFFFFFFF : 0
//		dst[127:64] := a[127:64]
//
// Instruction: 'CMPSD'. Intrinsic: '_mm_cmpnlt_sd'.
// Requires SSE2.
func CmpnltSd(a M128d, b M128d) M128d {
	return M128d(cmpnltSd([2]float64(a), [2]float64(b)))
}

func cmpnltSd(a [2]float64, b [2]float64) [2]float64


// CmpnltSs: Compare the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b' for not-less-than, store the result in the lower
// element of 'dst', and copy the upper 3 packed elements from 'a' to the upper
// elements of 'dst'. 
//
//		dst[31:0] := !( a[31:0] < b[31:0] ) ? 0xffffffff : 0
//		dst[127:32] := a[127:32]
//
// Instruction: 'CMPSS'. Intrinsic: '_mm_cmpnlt_ss'.
// Requires SSE.
func CmpnltSs(a M128, b M128) M128 {
	return M128(cmpnltSs([4]float32(a), [4]float32(b)))
}

func cmpnltSs(a [4]float32, b [4]float32) [4]float32


// CmpordPd: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b' to see if neither is NaN, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := (a[i+63:i] != NaN AND b[i+63:i] != NaN) ? 0xFFFFFFFFFFFFFFFF : 0
//		ENDFOR
//
// Instruction: 'CMPPD'. Intrinsic: '_mm_cmpord_pd'.
// Requires SSE2.
func CmpordPd(a M128d, b M128d) M128d {
	return M128d(cmpordPd([2]float64(a), [2]float64(b)))
}

func cmpordPd(a [2]float64, b [2]float64) [2]float64


// CmpordPs: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b' to see if neither is NaN, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := ( a[i+31:i] != NaN AND b[i+31:i] != NaN ) ? 0xffffffff : 0
//		ENDFOR
//
// Instruction: 'CMPPS'. Intrinsic: '_mm_cmpord_ps'.
// Requires SSE.
func CmpordPs(a M128, b M128) M128 {
	return M128(cmpordPs([4]float32(a), [4]float32(b)))
}

func cmpordPs(a [4]float32, b [4]float32) [4]float32


// CmpordSd: Compare the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b' to see if neither is NaN, store the result in the
// lower element of 'dst', and copy the upper element from 'a' to the upper
// element of 'dst'. 
//
//		dst[63:0] := (a[63:0] != NaN AND b[63:0] != NaN) ? 0xFFFFFFFFFFFFFFFF : 0
//		dst[127:64] := a[127:64]
//
// Instruction: 'CMPSD'. Intrinsic: '_mm_cmpord_sd'.
// Requires SSE2.
func CmpordSd(a M128d, b M128d) M128d {
	return M128d(cmpordSd([2]float64(a), [2]float64(b)))
}

func cmpordSd(a [2]float64, b [2]float64) [2]float64


// CmpordSs: Compare the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b' to see if neither is NaN, store the result in the
// lower element of 'dst', and copy the upper 3 packed elements from 'a' to the
// upper elements of 'dst'. 
//
//		dst[31:0] := ( a[31:0] != NaN AND b[31:0] != NaN ) ? 0xffffffff : 0
//		dst[127:32] := a[127:32]
//
// Instruction: 'CMPSS'. Intrinsic: '_mm_cmpord_ss'.
// Requires SSE.
func CmpordSs(a M128, b M128) M128 {
	return M128(cmpordSs([4]float32(a), [4]float32(b)))
}

func cmpordSs(a [4]float32, b [4]float32) [4]float32


// CmpunordPd: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b' to see if either is NaN, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := (a[i+63:i] != NaN OR b[i+63:i] != NaN) ? 0xFFFFFFFFFFFFFFFF : 0
//		ENDFOR
//
// Instruction: 'CMPPD'. Intrinsic: '_mm_cmpunord_pd'.
// Requires SSE2.
func CmpunordPd(a M128d, b M128d) M128d {
	return M128d(cmpunordPd([2]float64(a), [2]float64(b)))
}

func cmpunordPd(a [2]float64, b [2]float64) [2]float64


// CmpunordPs: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b' to see if either is NaN, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := ( a[i+31:i] != NaN OR b[i+31:i] != NaN ) ? 0xffffffff : 0
//		ENDFOR
//
// Instruction: 'CMPPS'. Intrinsic: '_mm_cmpunord_ps'.
// Requires SSE.
func CmpunordPs(a M128, b M128) M128 {
	return M128(cmpunordPs([4]float32(a), [4]float32(b)))
}

func cmpunordPs(a [4]float32, b [4]float32) [4]float32


// CmpunordSd: Compare the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b' to see if either is NaN, store the result in the
// lower element of 'dst', and copy the upper element from 'a' to the upper
// element of 'dst'. 
//
//		dst[63:0] := (a[63:0] != NaN OR b[63:0] != NaN) ? 0xFFFFFFFFFFFFFFFF : 0
//		dst[127:64] := a[127:64]
//
// Instruction: 'CMPSD'. Intrinsic: '_mm_cmpunord_sd'.
// Requires SSE2.
func CmpunordSd(a M128d, b M128d) M128d {
	return M128d(cmpunordSd([2]float64(a), [2]float64(b)))
}

func cmpunordSd(a [2]float64, b [2]float64) [2]float64


// CmpunordSs: Compare the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b' to see if either is NaN, store the result in the
// lower element of 'dst', and copy the upper 3 packed elements from 'a' to the
// upper elements of 'dst'. 
//
//		dst[31:0] := ( a[31:0] != NaN OR b[31:0] != NaN ) ? 0xffffffff : 0
//		dst[127:32] := a[127:32]
//
// Instruction: 'CMPSS'. Intrinsic: '_mm_cmpunord_ss'.
// Requires SSE.
func CmpunordSs(a M128, b M128) M128 {
	return M128(cmpunordSs([4]float32(a), [4]float32(b)))
}

func cmpunordSs(a [4]float32, b [4]float32) [4]float32


// ComiRoundSd: Compare the lower double-precision (64-bit) floating-point
// element in 'a' and 'b' based on the comparison operand specified by 'imm8',
// and return the boolean result (0 or 1).
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		
//		RETURN ( a[63:0] OP b[63:0] ) ? 1 : 0
//
// Instruction: 'VCOMISD'. Intrinsic: '_mm_comi_round_sd'.
// Requires AVX512F.
func ComiRoundSd(a M128d, b M128d, imm8 int, sae int) int {
	return int(comiRoundSd([2]float64(a), [2]float64(b), imm8, sae))
}

func comiRoundSd(a [2]float64, b [2]float64, imm8 int, sae int) int


// ComiRoundSs: Compare the lower single-precision (32-bit) floating-point
// element in 'a' and 'b' based on the comparison operand specified by 'imm8',
// and return the boolean result (0 or 1).
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		
//		RETURN ( a[31:0] OP b[31:0] ) ? 1 : 0
//
// Instruction: 'VCOMISS'. Intrinsic: '_mm_comi_round_ss'.
// Requires AVX512F.
func ComiRoundSs(a M128, b M128, imm8 int, sae int) int {
	return int(comiRoundSs([4]float32(a), [4]float32(b), imm8, sae))
}

func comiRoundSs(a [4]float32, b [4]float32, imm8 int, sae int) int


// ComieqSd: Compare the lower double-precision (64-bit) floating-point element
// in 'a' and 'b' for equality, and return the boolean result (0 or 1). 
//
//		RETURN ( a[63:0] == b[63:0] ) ? 1 : 0
//
// Instruction: 'COMISD'. Intrinsic: '_mm_comieq_sd'.
// Requires SSE2.
func ComieqSd(a M128d, b M128d) int {
	return int(comieqSd([2]float64(a), [2]float64(b)))
}

func comieqSd(a [2]float64, b [2]float64) int


// ComieqSs: Compare the lower single-precision (32-bit) floating-point element
// in 'a' and 'b' for equality, and return the boolean result (0 or 1). 
//
//		RETURN ( a[31:0] == b[31:0] ) ? 1 : 0
//
// Instruction: 'COMISS'. Intrinsic: '_mm_comieq_ss'.
// Requires SSE.
func ComieqSs(a M128, b M128) int {
	return int(comieqSs([4]float32(a), [4]float32(b)))
}

func comieqSs(a [4]float32, b [4]float32) int


// ComigeSd: Compare the lower double-precision (64-bit) floating-point element
// in 'a' and 'b' for greater-than-or-equal, and return the boolean result (0
// or 1). 
//
//		RETURN ( a[63:0] >= b[63:0] ) ? 1 : 0
//
// Instruction: 'COMISD'. Intrinsic: '_mm_comige_sd'.
// Requires SSE2.
func ComigeSd(a M128d, b M128d) int {
	return int(comigeSd([2]float64(a), [2]float64(b)))
}

func comigeSd(a [2]float64, b [2]float64) int


// ComigeSs: Compare the lower single-precision (32-bit) floating-point element
// in 'a' and 'b' for greater-than-or-equal, and return the boolean result (0
// or 1). 
//
//		RETURN ( a[31:0] >= b[31:0] ) ? 1 : 0
//
// Instruction: 'COMISS'. Intrinsic: '_mm_comige_ss'.
// Requires SSE.
func ComigeSs(a M128, b M128) int {
	return int(comigeSs([4]float32(a), [4]float32(b)))
}

func comigeSs(a [4]float32, b [4]float32) int


// ComigtSd: Compare the lower double-precision (64-bit) floating-point element
// in 'a' and 'b' for greater-than, and return the boolean result (0 or 1). 
//
//		RETURN ( a[63:0] > b[63:0] ) ? 1 : 0
//
// Instruction: 'COMISD'. Intrinsic: '_mm_comigt_sd'.
// Requires SSE2.
func ComigtSd(a M128d, b M128d) int {
	return int(comigtSd([2]float64(a), [2]float64(b)))
}

func comigtSd(a [2]float64, b [2]float64) int


// ComigtSs: Compare the lower single-precision (32-bit) floating-point element
// in 'a' and 'b' for greater-than, and return the boolean result (0 or 1). 
//
//		RETURN ( a[31:0] > b[31:0] ) ? 1 : 0
//
// Instruction: 'COMISS'. Intrinsic: '_mm_comigt_ss'.
// Requires SSE.
func ComigtSs(a M128, b M128) int {
	return int(comigtSs([4]float32(a), [4]float32(b)))
}

func comigtSs(a [4]float32, b [4]float32) int


// ComileSd: Compare the lower double-precision (64-bit) floating-point element
// in 'a' and 'b' for less-than-or-equal, and return the boolean result (0 or
// 1). 
//
//		RETURN ( a[63:0] <= b[63:0] ) ? 1 : 0
//
// Instruction: 'COMISD'. Intrinsic: '_mm_comile_sd'.
// Requires SSE2.
func ComileSd(a M128d, b M128d) int {
	return int(comileSd([2]float64(a), [2]float64(b)))
}

func comileSd(a [2]float64, b [2]float64) int


// ComileSs: Compare the lower single-precision (32-bit) floating-point element
// in 'a' and 'b' for less-than-or-equal, and return the boolean result (0 or
// 1). 
//
//		RETURN ( a[31:0] <= b[31:0] ) ? 1 : 0
//
// Instruction: 'COMISS'. Intrinsic: '_mm_comile_ss'.
// Requires SSE.
func ComileSs(a M128, b M128) int {
	return int(comileSs([4]float32(a), [4]float32(b)))
}

func comileSs(a [4]float32, b [4]float32) int


// ComiltSd: Compare the lower double-precision (64-bit) floating-point element
// in 'a' and 'b' for less-than, and return the boolean result (0 or 1). 
//
//		RETURN ( a[63:0] < b[63:0] ) ? 1 : 0
//
// Instruction: 'COMISD'. Intrinsic: '_mm_comilt_sd'.
// Requires SSE2.
func ComiltSd(a M128d, b M128d) int {
	return int(comiltSd([2]float64(a), [2]float64(b)))
}

func comiltSd(a [2]float64, b [2]float64) int


// ComiltSs: Compare the lower single-precision (32-bit) floating-point element
// in 'a' and 'b' for less-than, and return the boolean result (0 or 1). 
//
//		RETURN ( a[31:0] < b[31:0] ) ? 1 : 0
//
// Instruction: 'COMISS'. Intrinsic: '_mm_comilt_ss'.
// Requires SSE.
func ComiltSs(a M128, b M128) int {
	return int(comiltSs([4]float32(a), [4]float32(b)))
}

func comiltSs(a [4]float32, b [4]float32) int


// ComineqSd: Compare the lower double-precision (64-bit) floating-point
// element in 'a' and 'b' for not-equal, and return the boolean result (0 or
// 1). 
//
//		RETURN ( a[63:0] != b[63:0] ) ? 1 : 0
//
// Instruction: 'COMISD'. Intrinsic: '_mm_comineq_sd'.
// Requires SSE2.
func ComineqSd(a M128d, b M128d) int {
	return int(comineqSd([2]float64(a), [2]float64(b)))
}

func comineqSd(a [2]float64, b [2]float64) int


// ComineqSs: Compare the lower single-precision (32-bit) floating-point
// element in 'a' and 'b' for not-equal, and return the boolean result (0 or
// 1). 
//
//		RETURN ( a[31:0] != b[31:0] ) ? 1 : 0
//
// Instruction: 'COMISS'. Intrinsic: '_mm_comineq_ss'.
// Requires SSE.
func ComineqSs(a M128, b M128) int {
	return int(comineqSs([4]float32(a), [4]float32(b)))
}

func comineqSs(a [4]float32, b [4]float32) int


// MaskCompress32: Contiguously store the active 32-bit integers in 'a' (those
// with their respective bit set in writemask 'k') to 'dst', and pass through
// the remaining elements from 'src'. 
//
//		size := 32
//		m := 0
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[127:m] := src[127:m]
//		dst[MAX:128] := 0
//
// Instruction: 'VPCOMPRESSD'. Intrinsic: '_mm_mask_compress_epi32'.
// Requires AVX512F.
func MaskCompress32(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCompress32([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCompress32(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCompress32: Contiguously store the active 32-bit integers in 'a' (those
// with their respective bit set in zeromask 'k') to 'dst', and set the
// remaining elements to zero. 
//
//		size := 32
//		m := 0
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[127:m] := 0
//		dst[MAX:128] := 0
//
// Instruction: 'VPCOMPRESSD'. Intrinsic: '_mm_maskz_compress_epi32'.
// Requires AVX512F.
func MaskzCompress32(k Mmask8, a M128i) M128i {
	return M128i(maskzCompress32(uint8(k), [16]byte(a)))
}

func maskzCompress32(k uint8, a [16]byte) [16]byte


// MaskCompress64: Contiguously store the active 64-bit integers in 'a' (those
// with their respective bit set in writemask 'k') to 'dst', and pass through
// the remaining elements from 'src'. 
//
//		size := 64
//		m := 0
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[127:m] := src[127:m]
//		dst[MAX:128] := 0
//
// Instruction: 'VPCOMPRESSQ'. Intrinsic: '_mm_mask_compress_epi64'.
// Requires AVX512F.
func MaskCompress64(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCompress64([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCompress64(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCompress64: Contiguously store the active 64-bit integers in 'a' (those
// with their respective bit set in zeromask 'k') to 'dst', and set the
// remaining elements to zero. 
//
//		size := 64
//		m := 0
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[127:m] := 0
//		dst[MAX:128] := 0
//
// Instruction: 'VPCOMPRESSQ'. Intrinsic: '_mm_maskz_compress_epi64'.
// Requires AVX512F.
func MaskzCompress64(k Mmask8, a M128i) M128i {
	return M128i(maskzCompress64(uint8(k), [16]byte(a)))
}

func maskzCompress64(k uint8, a [16]byte) [16]byte


// MaskCompressPd: Contiguously store the active double-precision (64-bit)
// floating-point elements in 'a' (those with their respective bit set in
// writemask 'k') to 'dst', and pass through the remaining elements from 'src'. 
//
//		size := 64
//		m := 0
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[127:m] := src[127:m]
//		dst[MAX:128] := 0
//
// Instruction: 'VCOMPRESSPD'. Intrinsic: '_mm_mask_compress_pd'.
// Requires AVX512F.
func MaskCompressPd(src M128d, k Mmask8, a M128d) M128d {
	return M128d(maskCompressPd([2]float64(src), uint8(k), [2]float64(a)))
}

func maskCompressPd(src [2]float64, k uint8, a [2]float64) [2]float64


// MaskzCompressPd: Contiguously store the active double-precision (64-bit)
// floating-point elements in 'a' (those with their respective bit set in
// zeromask 'k') to 'dst', and set the remaining elements to zero. 
//
//		size := 64
//		m := 0
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[127:m] := 0
//		dst[MAX:128] := 0
//
// Instruction: 'VCOMPRESSPD'. Intrinsic: '_mm_maskz_compress_pd'.
// Requires AVX512F.
func MaskzCompressPd(k Mmask8, a M128d) M128d {
	return M128d(maskzCompressPd(uint8(k), [2]float64(a)))
}

func maskzCompressPd(k uint8, a [2]float64) [2]float64


// MaskCompressPs: Contiguously store the active single-precision (32-bit)
// floating-point elements in 'a' (those with their respective bit set in
// writemask 'k') to 'dst', and pass through the remaining elements from 'src'. 
//
//		size := 32
//		m := 0
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[127:m] := src[127:m]
//		dst[MAX:128] := 0
//
// Instruction: 'VCOMPRESSPS'. Intrinsic: '_mm_mask_compress_ps'.
// Requires AVX512F.
func MaskCompressPs(src M128, k Mmask8, a M128) M128 {
	return M128(maskCompressPs([4]float32(src), uint8(k), [4]float32(a)))
}

func maskCompressPs(src [4]float32, k uint8, a [4]float32) [4]float32


// MaskzCompressPs: Contiguously store the active single-precision (32-bit)
// floating-point elements in 'a' (those with their respective bit set in
// zeromask 'k') to 'dst', and set the remaining elements to zero. 
//
//		size := 32
//		m := 0
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[127:m] := 0
//		dst[MAX:128] := 0
//
// Instruction: 'VCOMPRESSPS'. Intrinsic: '_mm_maskz_compress_ps'.
// Requires AVX512F.
func MaskzCompressPs(k Mmask8, a M128) M128 {
	return M128(maskzCompressPs(uint8(k), [4]float32(a)))
}

func maskzCompressPs(k uint8, a [4]float32) [4]float32


// MaskCompressstoreu32: Contiguously store the active 32-bit integers in 'a'
// (those with their respective bit set in writemask 'k') to unaligned memory
// at 'base_addr'. 
//
//		size := 32
//		m := base_addr
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				MEM[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//
// Instruction: 'VPCOMPRESSD'. Intrinsic: '_mm_mask_compressstoreu_epi32'.
// Requires AVX512F.
func MaskCompressstoreu32(base_addr uintptr, k Mmask8, a M128i)  {
	maskCompressstoreu32(uintptr(base_addr), uint8(k), [16]byte(a))
}

func maskCompressstoreu32(base_addr uintptr, k uint8, a [16]byte) 


// MaskCompressstoreu64: Contiguously store the active 64-bit integers in 'a'
// (those with their respective bit set in writemask 'k') to unaligned memory
// at 'base_addr'. 
//
//		size := 64
//		m := base_addr
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				MEM[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//
// Instruction: 'VPCOMPRESSQ'. Intrinsic: '_mm_mask_compressstoreu_epi64'.
// Requires AVX512F.
func MaskCompressstoreu64(base_addr uintptr, k Mmask8, a M128i)  {
	maskCompressstoreu64(uintptr(base_addr), uint8(k), [16]byte(a))
}

func maskCompressstoreu64(base_addr uintptr, k uint8, a [16]byte) 


// MaskCompressstoreuPd: Contiguously store the active double-precision
// (64-bit) floating-point elements in 'a' (those with their respective bit set
// in writemask 'k') to unaligned memory at 'base_addr'. 
//
//		size := 64
//		m := base_addr
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				MEM[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//
// Instruction: 'VCOMPRESSPD'. Intrinsic: '_mm_mask_compressstoreu_pd'.
// Requires AVX512F.
func MaskCompressstoreuPd(base_addr uintptr, k Mmask8, a M128d)  {
	maskCompressstoreuPd(uintptr(base_addr), uint8(k), [2]float64(a))
}

func maskCompressstoreuPd(base_addr uintptr, k uint8, a [2]float64) 


// MaskCompressstoreuPs: Contiguously store the active single-precision
// (32-bit) floating-point elements in 'a' (those with their respective bit set
// in writemask 'k') to unaligned memory at 'base_addr'. 
//
//		size := 32
//		m := base_addr
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				MEM[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//
// Instruction: 'VCOMPRESSPS'. Intrinsic: '_mm_mask_compressstoreu_ps'.
// Requires AVX512F.
func MaskCompressstoreuPs(base_addr uintptr, k Mmask8, a M128)  {
	maskCompressstoreuPs(uintptr(base_addr), uint8(k), [4]float32(a))
}

func maskCompressstoreuPs(base_addr uintptr, k uint8, a [4]float32) 


// Conflict32: Test each 32-bit element of 'a' for equality with all other
// elements in 'a' closer to the least significant bit. Each element's
// comparison forms a zero extended bit vector in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			FOR k := 0 to j-1
//				m := k*32
//				dst[i+k] := (a[i+31:i] == a[m+31:m]) ? 1 : 0
//			ENDFOR
//			dst[i+31:i+j] := 0
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPCONFLICTD'. Intrinsic: '_mm_conflict_epi32'.
// Requires AVX512CD.
func Conflict32(a M128i) M128i {
	return M128i(conflict32([16]byte(a)))
}

func conflict32(a [16]byte) [16]byte


// MaskConflict32: Test each 32-bit element of 'a' for equality with all other
// elements in 'a' closer to the least significant bit using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set).
// Each element's comparison forms a zero extended bit vector in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[i]
//				FOR l := 0 to j-1
//					m := l*32
//					dst[i+l] := (a[i+31:i] == a[m+31:m]) ? 1 : 0
//				ENDFOR
//				dst[i+31:i+j] := 0
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPCONFLICTD'. Intrinsic: '_mm_mask_conflict_epi32'.
// Requires AVX512CD.
func MaskConflict32(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskConflict32([16]byte(src), uint8(k), [16]byte(a)))
}

func maskConflict32(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzConflict32: Test each 32-bit element of 'a' for equality with all other
// elements in 'a' closer to the least significant bit using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). Each
// element's comparison forms a zero extended bit vector in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[i]
//				FOR l := 0 to j-1
//					m := l*32
//					dst[i+l] := (a[i+31:i] == a[m+31:m]) ? 1 : 0
//				ENDFOR
//				dst[i+31:i+j] := 0
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPCONFLICTD'. Intrinsic: '_mm_maskz_conflict_epi32'.
// Requires AVX512CD.
func MaskzConflict32(k Mmask8, a M128i) M128i {
	return M128i(maskzConflict32(uint8(k), [16]byte(a)))
}

func maskzConflict32(k uint8, a [16]byte) [16]byte


// Conflict64: Test each 64-bit element of 'a' for equality with all other
// elements in 'a' closer to the least significant bit. Each element's
// comparison forms a zero extended bit vector in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			FOR k := 0 to j-1
//				m := k*64
//				dst[i+k] := (a[i+63:i] == a[m+63:m]) ? 1 : 0
//			ENDFOR
//			dst[i+63:i+j] := 0
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPCONFLICTQ'. Intrinsic: '_mm_conflict_epi64'.
// Requires AVX512CD.
func Conflict64(a M128i) M128i {
	return M128i(conflict64([16]byte(a)))
}

func conflict64(a [16]byte) [16]byte


// MaskConflict64: Test each 64-bit element of 'a' for equality with all other
// elements in 'a' closer to the least significant bit using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set).
// Each element's comparison forms a zero extended bit vector in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				FOR l := 0 to j-1
//					m := l*64
//					dst[i+l] := (a[i+63:i] == a[m+63:m]) ? 1 : 0
//				ENDFOR
//				dst[i+63:i+j] := 0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPCONFLICTQ'. Intrinsic: '_mm_mask_conflict_epi64'.
// Requires AVX512CD.
func MaskConflict64(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskConflict64([16]byte(src), uint8(k), [16]byte(a)))
}

func maskConflict64(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzConflict64: Test each 64-bit element of 'a' for equality with all other
// elements in 'a' closer to the least significant bit using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). Each
// element's comparison forms a zero extended bit vector in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				FOR l := 0 to j-1
//					m := l*64
//					dst[i+l] := (a[i+63:i] == a[m+63:m]) ? 1 : 0
//				ENDFOR
//				dst[i+63:i+j] := 0
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPCONFLICTQ'. Intrinsic: '_mm_maskz_conflict_epi64'.
// Requires AVX512CD.
func MaskzConflict64(k Mmask8, a M128i) M128i {
	return M128i(maskzConflict64(uint8(k), [16]byte(a)))
}

func maskzConflict64(k uint8, a [16]byte) [16]byte


// CosPd: Compute the cosine of packed double-precision (64-bit) floating-point
// elements in 'a' expressed in radians, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := COS(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_cos_pd'.
// Requires SSE.
func CosPd(a M128d) M128d {
	return M128d(cosPd([2]float64(a)))
}

func cosPd(a [2]float64) [2]float64


// CosPs: Compute the cosine of packed single-precision (32-bit) floating-point
// elements in 'a' expressed in radians, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := COS(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_cos_ps'.
// Requires SSE.
func CosPs(a M128) M128 {
	return M128(cosPs([4]float32(a)))
}

func cosPs(a [4]float32) [4]float32


// CosdPd: Compute the cosine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in degrees, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := COSD(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_cosd_pd'.
// Requires SSE.
func CosdPd(a M128d) M128d {
	return M128d(cosdPd([2]float64(a)))
}

func cosdPd(a [2]float64) [2]float64


// CosdPs: Compute the cosine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in degrees, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := COSD(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_cosd_ps'.
// Requires SSE.
func CosdPs(a M128) M128 {
	return M128(cosdPs([4]float32(a)))
}

func cosdPs(a [4]float32) [4]float32


// CoshPd: Compute the hyperbolic cosine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := COSH(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_cosh_pd'.
// Requires SSE.
func CoshPd(a M128d) M128d {
	return M128d(coshPd([2]float64(a)))
}

func coshPd(a [2]float64) [2]float64


// CoshPs: Compute the hyperbolic cosine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := COSH(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_cosh_ps'.
// Requires SSE.
func CoshPs(a M128) M128 {
	return M128(coshPs([4]float32(a)))
}

func coshPs(a [4]float32) [4]float32


// CsqrtPs: Compute the square root of packed complex single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := SQRT(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_csqrt_ps'.
// Requires SSE.
func CsqrtPs(a M128) M128 {
	return M128(csqrtPs([4]float32(a)))
}

func csqrtPs(a [4]float32) [4]float32


// CvtPs2pi: Convert packed single-precision (32-bit) floating-point elements
// in 'a' to packed 32-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := 32*j
//			dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
//		ENDFOR
//
// Instruction: 'CVTPS2PI'. Intrinsic: '_mm_cvt_ps2pi'.
// Requires SSE.
func CvtPs2pi(a M128) M64 {
	return M64(cvtPs2pi([4]float32(a)))
}

func cvtPs2pi(a [4]float32) M64


// CvtRoundi32Ss: Convert the 32-bit integer 'b' to a single-precision (32-bit)
// floating-point element, store the result in the lower element of 'dst', and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_Int32_To_FP32(b[31:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTSI2SS'. Intrinsic: '_mm_cvt_roundi32_ss'.
// Requires AVX512F.
func CvtRoundi32Ss(a M128, b int, rounding int) M128 {
	return M128(cvtRoundi32Ss([4]float32(a), b, rounding))
}

func cvtRoundi32Ss(a [4]float32, b int, rounding int) [4]float32


// CvtRoundi64Sd: Convert the 64-bit integer 'b' to a double-precision (64-bit)
// floating-point element, store the result in the lower element of 'dst', and
// copy the upper element from 'a' to the upper element of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_Int64_To_FP64(b[63:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTSI2SD'. Intrinsic: '_mm_cvt_roundi64_sd'.
// Requires AVX512F.
func CvtRoundi64Sd(a M128d, b int64, rounding int) M128d {
	return M128d(cvtRoundi64Sd([2]float64(a), b, rounding))
}

func cvtRoundi64Sd(a [2]float64, b int64, rounding int) [2]float64


// CvtRoundi64Ss: Convert the 64-bit integer 'b' to a single-precision (32-bit)
// floating-point element, store the result in the lower element of 'dst', and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_Int64_To_FP32(b[63:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTSI2SS'. Intrinsic: '_mm_cvt_roundi64_ss'.
// Requires AVX512F.
func CvtRoundi64Ss(a M128, b int64, rounding int) M128 {
	return M128(cvtRoundi64Ss([4]float32(a), b, rounding))
}

func cvtRoundi64Ss(a [4]float32, b int64, rounding int) [4]float32


// MaskCvtRoundpsPh: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed half-precision (16-bit) floating-point elements,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 3
//			i := 16*j
//			l := 32*j
//			IF k[j]
//				dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm_mask_cvt_roundps_ph'.
// Requires AVX512F.
func MaskCvtRoundpsPh(src M128i, k Mmask8, a M128, rounding int) M128i {
	return M128i(maskCvtRoundpsPh([16]byte(src), uint8(k), [4]float32(a), rounding))
}

func maskCvtRoundpsPh(src [16]byte, k uint8, a [4]float32, rounding int) [16]byte


// MaskzCvtRoundpsPh: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed half-precision (16-bit) floating-point elements,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 3
//			i := 16*j
//			l := 32*j
//			IF k[j]
//				dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm_maskz_cvt_roundps_ph'.
// Requires AVX512F.
func MaskzCvtRoundpsPh(k Mmask8, a M128, rounding int) M128i {
	return M128i(maskzCvtRoundpsPh(uint8(k), [4]float32(a), rounding))
}

func maskzCvtRoundpsPh(k uint8, a [4]float32, rounding int) [16]byte


// CvtRoundsdI32: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to a 32-bit integer, and store the result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_FP64_To_Int32(a[63:0])
//
// Instruction: 'VCVTSD2SI'. Intrinsic: '_mm_cvt_roundsd_i32'.
// Requires AVX512F.
func CvtRoundsdI32(a M128d, rounding int) int {
	return int(cvtRoundsdI32([2]float64(a), rounding))
}

func cvtRoundsdI32(a [2]float64, rounding int) int


// CvtRoundsdI64: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to a 64-bit integer, and store the result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_FP64_To_Int64(a[63:0])
//
// Instruction: 'VCVTSD2SI'. Intrinsic: '_mm_cvt_roundsd_i64'.
// Requires AVX512F.
func CvtRoundsdI64(a M128d, rounding int) int64 {
	return int64(cvtRoundsdI64([2]float64(a), rounding))
}

func cvtRoundsdI64(a [2]float64, rounding int) int64


// CvtRoundsdSi32: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to a 32-bit integer, and store the result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_FP64_To_Int32(a[63:0])
//
// Instruction: 'VCVTSD2SI'. Intrinsic: '_mm_cvt_roundsd_si32'.
// Requires AVX512F.
func CvtRoundsdSi32(a M128d, rounding int) int {
	return int(cvtRoundsdSi32([2]float64(a), rounding))
}

func cvtRoundsdSi32(a [2]float64, rounding int) int


// CvtRoundsdSi64: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to a 64-bit integer, and store the result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_FP64_To_Int64(a[63:0])
//
// Instruction: 'VCVTSD2SI'. Intrinsic: '_mm_cvt_roundsd_si64'.
// Requires AVX512F.
func CvtRoundsdSi64(a M128d, rounding int) int64 {
	return int64(cvtRoundsdSi64([2]float64(a), rounding))
}

func cvtRoundsdSi64(a [2]float64, rounding int) int64


// CvtRoundsdSs: Convert the lower double-precision (64-bit) floating-point
// element in 'b' to a single-precision (32-bit) floating-point element, store
// the result in the lower element of 'dst', and copy the upper 3 packed
// elements from 'a' to the upper elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_FP64_To_FP32(b[63:0])
//		dst[127:32] := a[127:31]
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTSD2SS'. Intrinsic: '_mm_cvt_roundsd_ss'.
// Requires AVX512F.
func CvtRoundsdSs(a M128, b M128d, rounding int) M128 {
	return M128(cvtRoundsdSs([4]float32(a), [2]float64(b), rounding))
}

func cvtRoundsdSs(a [4]float32, b [2]float64, rounding int) [4]float32


// MaskCvtRoundsdSs: Convert the lower double-precision (64-bit) floating-point
// element in 'b' to a single-precision (32-bit) floating-point element, store
// the result in the lower element of 'dst' using writemask 'k' (the element is
// copied from 'src' when mask bit 0 is not set), and copy the upper element
// from 'a' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := Convert_FP64_To_FP32(b[63:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:31]
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTSD2SS'. Intrinsic: '_mm_mask_cvt_roundsd_ss'.
// Requires AVX512F.
func MaskCvtRoundsdSs(src M128, k Mmask8, a M128, b M128d, rounding int) M128 {
	return M128(maskCvtRoundsdSs([4]float32(src), uint8(k), [4]float32(a), [2]float64(b), rounding))
}

func maskCvtRoundsdSs(src [4]float32, k uint8, a [4]float32, b [2]float64, rounding int) [4]float32


// MaskzCvtRoundsdSs: Convert the lower double-precision (64-bit)
// floating-point element in 'b' to a single-precision (32-bit) floating-point
// element, store the result in the lower element of 'dst' using zeromask 'k'
// (the element is zeroed out when mask bit 0 is not set), and copy the upper 3
// packed elements from 'a' to the upper elements of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := Convert_FP64_To_FP32(b[63:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:31]
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTSD2SS'. Intrinsic: '_mm_maskz_cvt_roundsd_ss'.
// Requires AVX512F.
func MaskzCvtRoundsdSs(k Mmask8, a M128, b M128d, rounding int) M128 {
	return M128(maskzCvtRoundsdSs(uint8(k), [4]float32(a), [2]float64(b), rounding))
}

func maskzCvtRoundsdSs(k uint8, a [4]float32, b [2]float64, rounding int) [4]float32


// CvtRoundsdU32: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to an unsigned 32-bit integer, and store the result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_FP64_To_UnsignedInt32(a[63:0])
//
// Instruction: 'VCVTSD2USI'. Intrinsic: '_mm_cvt_roundsd_u32'.
// Requires AVX512F.
func CvtRoundsdU32(a M128d, rounding int) uint32 {
	return uint32(cvtRoundsdU32([2]float64(a), rounding))
}

func cvtRoundsdU32(a [2]float64, rounding int) uint32


// CvtRoundsdU64: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to an unsigned 64-bit integer, and store the result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_FP64_To_UnsignedInt64(a[63:0])
//
// Instruction: 'VCVTSD2USI'. Intrinsic: '_mm_cvt_roundsd_u64'.
// Requires AVX512F.
func CvtRoundsdU64(a M128d, rounding int) uint64 {
	return uint64(cvtRoundsdU64([2]float64(a), rounding))
}

func cvtRoundsdU64(a [2]float64, rounding int) uint64


// CvtRoundsi32Ss: Convert the 32-bit integer 'b' to a single-precision
// (32-bit) floating-point element, store the result in the lower element of
// 'dst', and copy the upper 3 packed elements from 'a' to the upper elements
// of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_Int32_To_FP32(b[31:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTSI2SS'. Intrinsic: '_mm_cvt_roundsi32_ss'.
// Requires AVX512F.
func CvtRoundsi32Ss(a M128, b int, rounding int) M128 {
	return M128(cvtRoundsi32Ss([4]float32(a), b, rounding))
}

func cvtRoundsi32Ss(a [4]float32, b int, rounding int) [4]float32


// CvtRoundsi64Sd: Convert the 64-bit integer 'b' to a double-precision
// (64-bit) floating-point element, store the result in the lower element of
// 'dst', and copy the upper element from 'a' to the upper element of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_Int64_To_FP64(b[63:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTSI2SD'. Intrinsic: '_mm_cvt_roundsi64_sd'.
// Requires AVX512F.
func CvtRoundsi64Sd(a M128d, b int64, rounding int) M128d {
	return M128d(cvtRoundsi64Sd([2]float64(a), b, rounding))
}

func cvtRoundsi64Sd(a [2]float64, b int64, rounding int) [2]float64


// CvtRoundsi64Ss: Convert the 64-bit integer 'b' to a single-precision
// (32-bit) floating-point element, store the result in the lower element of
// 'dst', and copy the upper 3 packed elements from 'a' to the upper elements
// of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_Int64_To_FP32(b[63:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTSI2SS'. Intrinsic: '_mm_cvt_roundsi64_ss'.
// Requires AVX512F.
func CvtRoundsi64Ss(a M128, b int64, rounding int) M128 {
	return M128(cvtRoundsi64Ss([4]float32(a), b, rounding))
}

func cvtRoundsi64Ss(a [4]float32, b int64, rounding int) [4]float32


// CvtRoundssI32: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to a 32-bit integer, and store the result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_FP32_To_Int32(a[31:0])
//
// Instruction: 'VCVTSS2SI'. Intrinsic: '_mm_cvt_roundss_i32'.
// Requires AVX512F.
func CvtRoundssI32(a M128, rounding int) int {
	return int(cvtRoundssI32([4]float32(a), rounding))
}

func cvtRoundssI32(a [4]float32, rounding int) int


// CvtRoundssI64: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to a 64-bit integer, and store the result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_FP32_To_Int64(a[31:0])
//
// Instruction: 'VCVTSS2SI'. Intrinsic: '_mm_cvt_roundss_i64'.
// Requires AVX512F.
func CvtRoundssI64(a M128, rounding int) int64 {
	return int64(cvtRoundssI64([4]float32(a), rounding))
}

func cvtRoundssI64(a [4]float32, rounding int) int64


// CvtRoundssSd: Convert the lower single-precision (32-bit) floating-point
// element in 'b' to a double-precision (64-bit) floating-point element, store
// the result in the lower element of 'dst', and copy the upper element from
// 'a' to the upper element of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_FP32_To_FP64(b[31:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTSS2SD'. Intrinsic: '_mm_cvt_roundss_sd'.
// Requires AVX512F.
func CvtRoundssSd(a M128d, b M128, rounding int) M128d {
	return M128d(cvtRoundssSd([2]float64(a), [4]float32(b), rounding))
}

func cvtRoundssSd(a [2]float64, b [4]float32, rounding int) [2]float64


// MaskCvtRoundssSd: Convert the lower single-precision (32-bit) floating-point
// element in 'b' to a double-precision (64-bit) floating-point element, store
// the result in the lower element of 'dst' using writemask 'k' (the element is
// copied from 'src' when mask bit 0 is not set), and copy the upper element
// from 'a' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := Convert_FP32_To_FP64(b[31:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTSS2SD'. Intrinsic: '_mm_mask_cvt_roundss_sd'.
// Requires AVX512F.
func MaskCvtRoundssSd(src M128d, k Mmask8, a M128d, b M128, rounding int) M128d {
	return M128d(maskCvtRoundssSd([2]float64(src), uint8(k), [2]float64(a), [4]float32(b), rounding))
}

func maskCvtRoundssSd(src [2]float64, k uint8, a [2]float64, b [4]float32, rounding int) [2]float64


// MaskzCvtRoundssSd: Convert the lower single-precision (32-bit)
// floating-point element in 'b' to a double-precision (64-bit) floating-point
// element, store the result in the lower element of 'dst' using zeromask 'k'
// (the element is zeroed out when mask bit 0 is not set), and copy the upper
// element from 'a' to the upper element of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := Convert_FP32_To_FP64(b[31:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTSS2SD'. Intrinsic: '_mm_maskz_cvt_roundss_sd'.
// Requires AVX512F.
func MaskzCvtRoundssSd(k Mmask8, a M128d, b M128, rounding int) M128d {
	return M128d(maskzCvtRoundssSd(uint8(k), [2]float64(a), [4]float32(b), rounding))
}

func maskzCvtRoundssSd(k uint8, a [2]float64, b [4]float32, rounding int) [2]float64


// CvtRoundssSi32: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to a 32-bit integer, and store the result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_FP32_To_Int32(a[31:0])
//
// Instruction: 'VCVTSS2SI'. Intrinsic: '_mm_cvt_roundss_si32'.
// Requires AVX512F.
func CvtRoundssSi32(a M128, rounding int) int {
	return int(cvtRoundssSi32([4]float32(a), rounding))
}

func cvtRoundssSi32(a [4]float32, rounding int) int


// CvtRoundssSi64: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to a 64-bit integer, and store the result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_FP32_To_Int64(a[31:0])
//
// Instruction: 'VCVTSS2SI'. Intrinsic: '_mm_cvt_roundss_si64'.
// Requires AVX512F.
func CvtRoundssSi64(a M128, rounding int) int64 {
	return int64(cvtRoundssSi64([4]float32(a), rounding))
}

func cvtRoundssSi64(a [4]float32, rounding int) int64


// CvtRoundssU32: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to an unsigned 32-bit integer, and store the result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_FP32_To_UnsignedInt32(a[31:0])
//
// Instruction: 'VCVTSS2USI'. Intrinsic: '_mm_cvt_roundss_u32'.
// Requires AVX512F.
func CvtRoundssU32(a M128, rounding int) uint32 {
	return uint32(cvtRoundssU32([4]float32(a), rounding))
}

func cvtRoundssU32(a [4]float32, rounding int) uint32


// CvtRoundssU64: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to an unsigned 64-bit integer, and store the result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_FP32_To_UnsignedInt64(a[31:0])
//
// Instruction: 'VCVTSS2USI'. Intrinsic: '_mm_cvt_roundss_u64'.
// Requires AVX512F.
func CvtRoundssU64(a M128, rounding int) uint64 {
	return uint64(cvtRoundssU64([4]float32(a), rounding))
}

func cvtRoundssU64(a [4]float32, rounding int) uint64


// CvtRoundu32Ss: Convert the unsigned 32-bit integer 'b' to a single-precision
// (32-bit) floating-point element, store the result in the lower element of
// 'dst', and copy the upper 3 packed elements from 'a' to the upper elements
// of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_UnsignedInt32_To_FP32(b[31:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTUSI2SS'. Intrinsic: '_mm_cvt_roundu32_ss'.
// Requires AVX512F.
func CvtRoundu32Ss(a M128, b uint32, rounding int) M128 {
	return M128(cvtRoundu32Ss([4]float32(a), b, rounding))
}

func cvtRoundu32Ss(a [4]float32, b uint32, rounding int) [4]float32


// CvtRoundu64Sd: Convert the unsigned 64-bit integer 'b' to a double-precision
// (64-bit) floating-point element, store the result in the lower element of
// 'dst', and copy the upper element from 'a' to the upper element of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_UnsignedInt64_To_FP64(b[63:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTUSI2SD'. Intrinsic: '_mm_cvt_roundu64_sd'.
// Requires AVX512F.
func CvtRoundu64Sd(a M128d, b uint64, rounding int) M128d {
	return M128d(cvtRoundu64Sd([2]float64(a), b, rounding))
}

func cvtRoundu64Sd(a [2]float64, b uint64, rounding int) [2]float64


// CvtRoundu64Ss: Convert the unsigned 64-bit integer 'b' to a single-precision
// (32-bit) floating-point element, store the result in the lower element of
// 'dst', and copy the upper 3 packed elements from 'a' to the upper elements
// of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_UnsignedInt64_To_FP32(b[63:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTUSI2SS'. Intrinsic: '_mm_cvt_roundu64_ss'.
// Requires AVX512F.
func CvtRoundu64Ss(a M128, b uint64, rounding int) M128 {
	return M128(cvtRoundu64Ss([4]float32(a), b, rounding))
}

func cvtRoundu64Ss(a [4]float32, b uint64, rounding int) [4]float32


// CvtSi2ss: Convert the 32-bit integer 'b' to a single-precision (32-bit)
// floating-point element, store the result in the lower element of 'dst', and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'. 
//
//		dst[31:0] := Convert_Int32_To_FP32(b[31:0])
//		dst[127:32] := a[127:32]
//
// Instruction: 'CVTSI2SS'. Intrinsic: '_mm_cvt_si2ss'.
// Requires SSE.
func CvtSi2ss(a M128, b int) M128 {
	return M128(cvtSi2ss([4]float32(a), b))
}

func cvtSi2ss(a [4]float32, b int) [4]float32


// CvtSs2si: Convert the lower single-precision (32-bit) floating-point element
// in 'a' to a 32-bit integer, and store the result in 'dst'. 
//
//		dst[31:0] := Convert_FP32_To_Int32(a[31:0])
//
// Instruction: 'CVTSS2SI'. Intrinsic: '_mm_cvt_ss2si'.
// Requires SSE.
func CvtSs2si(a M128) int {
	return int(cvtSs2si([4]float32(a)))
}

func cvtSs2si(a [4]float32) int


// Cvtepi1632: Sign extend packed 16-bit integers in 'a' to packed 32-bit
// integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			k := 16*j
//			dst[i+31:i] := SignExtend(a[k+15:k])
//		ENDFOR
//
// Instruction: 'PMOVSXWD'. Intrinsic: '_mm_cvtepi16_epi32'.
// Requires SSE4.1.
func Cvtepi1632(a M128i) M128i {
	return M128i(cvtepi1632([16]byte(a)))
}

func cvtepi1632(a [16]byte) [16]byte


// MaskCvtepi1632: Sign extend packed 16-bit integers in 'a' to packed 32-bit
// integers, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			l := j*16
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[l+15:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSXWD'. Intrinsic: '_mm_mask_cvtepi16_epi32'.
// Requires AVX512F.
func MaskCvtepi1632(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtepi1632([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi1632(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepi1632: Sign extend packed 16-bit integers in 'a' to packed 32-bit
// integers, and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[l+15:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSXWD'. Intrinsic: '_mm_maskz_cvtepi16_epi32'.
// Requires AVX512F.
func MaskzCvtepi1632(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtepi1632(uint8(k), [16]byte(a)))
}

func maskzCvtepi1632(k uint8, a [16]byte) [16]byte


// Cvtepi1664: Sign extend packed 16-bit integers in 'a' to packed 64-bit
// integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			k := 16*j
//			dst[i+63:i] := SignExtend(a[k+15:k])
//		ENDFOR
//
// Instruction: 'PMOVSXWQ'. Intrinsic: '_mm_cvtepi16_epi64'.
// Requires SSE4.1.
func Cvtepi1664(a M128i) M128i {
	return M128i(cvtepi1664([16]byte(a)))
}

func cvtepi1664(a [16]byte) [16]byte


// MaskCvtepi1664: Sign extend packed 16-bit integers in the low 4 bytes of 'a'
// to packed 64-bit integers, and store the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+15:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSXWQ'. Intrinsic: '_mm_mask_cvtepi16_epi64'.
// Requires AVX512F.
func MaskCvtepi1664(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtepi1664([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi1664(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepi1664: Sign extend packed 16-bit integers in the low 4 bytes of
// 'a' to packed 64-bit integers, and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+15:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSXWQ'. Intrinsic: '_mm_maskz_cvtepi16_epi64'.
// Requires AVX512F.
func MaskzCvtepi1664(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtepi1664(uint8(k), [16]byte(a)))
}

func maskzCvtepi1664(k uint8, a [16]byte) [16]byte


// Cvtepi168: Convert packed 16-bit integers in 'a' to packed 8-bit integers
// with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 16*j
//			l := 8*j
//			dst[l+7:l] := Truncate_Int16_To_Int8(a[i+15:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVWB'. Intrinsic: '_mm_cvtepi16_epi8'.
// Requires AVX512BW.
func Cvtepi168(a M128i) M128i {
	return M128i(cvtepi168([16]byte(a)))
}

func cvtepi168(a [16]byte) [16]byte


// MaskCvtepi168: Convert packed 16-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 16*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int16_To_Int8(a[i+15:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVWB'. Intrinsic: '_mm_mask_cvtepi16_epi8'.
// Requires AVX512BW.
func MaskCvtepi168(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtepi168([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi168(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepi168: Convert packed 16-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 16*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int16_To_Int8(a[i+15:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVWB'. Intrinsic: '_mm_maskz_cvtepi16_epi8'.
// Requires AVX512BW.
func MaskzCvtepi168(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtepi168(uint8(k), [16]byte(a)))
}

func maskzCvtepi168(k uint8, a [16]byte) [16]byte


// MaskCvtepi16Storeu8: Convert packed 16-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the active results (those with their
// respective bit set in writemask 'k') to unaligned memory at 'base_addr'. 
//
//		FOR j := 0 to 7
//			i := 16*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Truncate_Int16_To_Int8(a[i+15:i])
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVWB'. Intrinsic: '_mm_mask_cvtepi16_storeu_epi8'.
// Requires AVX512BW.
func MaskCvtepi16Storeu8(base_addr uintptr, k Mmask8, a M128i)  {
	maskCvtepi16Storeu8(uintptr(base_addr), uint8(k), [16]byte(a))
}

func maskCvtepi16Storeu8(base_addr uintptr, k uint8, a [16]byte) 


// Cvtepi3216: Convert packed 32-bit integers in 'a' to packed 16-bit integers
// with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			k := 16*j
//			dst[k+15:k] := Truncate_Int32_To_Int16(a[i+31:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVDW'. Intrinsic: '_mm_cvtepi32_epi16'.
// Requires AVX512F.
func Cvtepi3216(a M128i) M128i {
	return M128i(cvtepi3216([16]byte(a)))
}

func cvtepi3216(a [16]byte) [16]byte


// MaskCvtepi3216: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Truncate_Int32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVDW'. Intrinsic: '_mm_mask_cvtepi32_epi16'.
// Requires AVX512F.
func MaskCvtepi3216(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtepi3216([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi3216(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepi3216: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Truncate_Int32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVDW'. Intrinsic: '_mm_maskz_cvtepi32_epi16'.
// Requires AVX512F.
func MaskzCvtepi3216(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtepi3216(uint8(k), [16]byte(a)))
}

func maskzCvtepi3216(k uint8, a [16]byte) [16]byte


// Cvtepi3264: Sign extend packed 32-bit integers in 'a' to packed 64-bit
// integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			k := 32*j
//			dst[i+63:i] := SignExtend(a[k+31:k])
//		ENDFOR
//
// Instruction: 'PMOVSXDQ'. Intrinsic: '_mm_cvtepi32_epi64'.
// Requires SSE4.1.
func Cvtepi3264(a M128i) M128i {
	return M128i(cvtepi3264([16]byte(a)))
}

func cvtepi3264(a [16]byte) [16]byte


// MaskCvtepi3264: Sign extend packed 32-bit integers in 'a' to packed 64-bit
// integers, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSXDQ'. Intrinsic: '_mm_mask_cvtepi32_epi64'.
// Requires AVX512F.
func MaskCvtepi3264(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtepi3264([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi3264(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepi3264: Sign extend packed 32-bit integers in 'a' to packed 64-bit
// integers, and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+31:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSXDQ'. Intrinsic: '_mm_maskz_cvtepi32_epi64'.
// Requires AVX512F.
func MaskzCvtepi3264(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtepi3264(uint8(k), [16]byte(a)))
}

func maskzCvtepi3264(k uint8, a [16]byte) [16]byte


// Cvtepi328: Convert packed 32-bit integers in 'a' to packed 8-bit integers
// with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			k := 8*j
//			dst[k+7:k] := Truncate_Int32_To_Int8(a[i+31:i])
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVDB'. Intrinsic: '_mm_cvtepi32_epi8'.
// Requires AVX512F.
func Cvtepi328(a M128i) M128i {
	return M128i(cvtepi328([16]byte(a)))
}

func cvtepi328(a [16]byte) [16]byte


// MaskCvtepi328: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVDB'. Intrinsic: '_mm_mask_cvtepi32_epi8'.
// Requires AVX512F.
func MaskCvtepi328(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtepi328([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi328(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepi328: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVDB'. Intrinsic: '_mm_maskz_cvtepi32_epi8'.
// Requires AVX512F.
func MaskzCvtepi328(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtepi328(uint8(k), [16]byte(a)))
}

func maskzCvtepi328(k uint8, a [16]byte) [16]byte


// Cvtepi32Pd: Convert packed 32-bit integers in 'a' to packed double-precision
// (64-bit) floating-point elements, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*32
//			m := j*64
//			dst[m+63:m] := Convert_Int32_To_FP64(a[i+31:i])
//		ENDFOR
//
// Instruction: 'CVTDQ2PD'. Intrinsic: '_mm_cvtepi32_pd'.
// Requires SSE2.
func Cvtepi32Pd(a M128i) M128d {
	return M128d(cvtepi32Pd([16]byte(a)))
}

func cvtepi32Pd(a [16]byte) [2]float64


// MaskCvtepi32Pd: Convert packed 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*32
//			m := j*64
//			IF k[j]
//				dst[m+63:m] := Convert_Int32_To_FP64(a[i+31:i])
//			ELSE
//				dst[m+63:m] := src[m+63:m]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTDQ2PD'. Intrinsic: '_mm_mask_cvtepi32_pd'.
// Requires AVX512F.
func MaskCvtepi32Pd(src M128d, k Mmask8, a M128i) M128d {
	return M128d(maskCvtepi32Pd([2]float64(src), uint8(k), [16]byte(a)))
}

func maskCvtepi32Pd(src [2]float64, k uint8, a [16]byte) [2]float64


// MaskzCvtepi32Pd: Convert packed 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*32
//			m := j*64
//			IF k[j]
//				dst[m+63:m] := Convert_Int32_To_FP64(a[i+31:i])
//			ELSE
//				dst[m+63:m] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTDQ2PD'. Intrinsic: '_mm_maskz_cvtepi32_pd'.
// Requires AVX512F.
func MaskzCvtepi32Pd(k Mmask8, a M128i) M128d {
	return M128d(maskzCvtepi32Pd(uint8(k), [16]byte(a)))
}

func maskzCvtepi32Pd(k uint8, a [16]byte) [2]float64


// Cvtepi32Ps: Convert packed 32-bit integers in 'a' to packed single-precision
// (32-bit) floating-point elements, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
//		ENDFOR
//
// Instruction: 'CVTDQ2PS'. Intrinsic: '_mm_cvtepi32_ps'.
// Requires SSE2.
func Cvtepi32Ps(a M128i) M128 {
	return M128(cvtepi32Ps([16]byte(a)))
}

func cvtepi32Ps(a [16]byte) [4]float32


// MaskCvtepi32Ps: Convert packed 32-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTDQ2PS'. Intrinsic: '_mm_mask_cvtepi32_ps'.
// Requires AVX512F.
func MaskCvtepi32Ps(src M128, k Mmask8, a M128i) M128 {
	return M128(maskCvtepi32Ps([4]float32(src), uint8(k), [16]byte(a)))
}

func maskCvtepi32Ps(src [4]float32, k uint8, a [16]byte) [4]float32


// MaskzCvtepi32Ps: Convert packed 32-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTDQ2PS'. Intrinsic: '_mm_maskz_cvtepi32_ps'.
// Requires AVX512F.
func MaskzCvtepi32Ps(k Mmask8, a M128i) M128 {
	return M128(maskzCvtepi32Ps(uint8(k), [16]byte(a)))
}

func maskzCvtepi32Ps(k uint8, a [16]byte) [4]float32


// MaskCvtepi32Storeu16: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the active results (those with their
// respective bit set in writemask 'k') to unaligned memory at 'base_addr'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				MEM[base_addr+l+15:base_addr+l] := Truncate_Int32_To_Int16(a[i+31:i])
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVDW'. Intrinsic: '_mm_mask_cvtepi32_storeu_epi16'.
// Requires AVX512F.
func MaskCvtepi32Storeu16(base_addr uintptr, k Mmask8, a M128i)  {
	maskCvtepi32Storeu16(uintptr(base_addr), uint8(k), [16]byte(a))
}

func maskCvtepi32Storeu16(base_addr uintptr, k uint8, a [16]byte) 


// MaskCvtepi32Storeu8: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the active results (those with their
// respective bit set in writemask 'k') to unaligned memory at 'base_addr'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Truncate_Int32_To_Int8(a[i+31:i])
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVDB'. Intrinsic: '_mm_mask_cvtepi32_storeu_epi8'.
// Requires AVX512F.
func MaskCvtepi32Storeu8(base_addr uintptr, k Mmask8, a M128i)  {
	maskCvtepi32Storeu8(uintptr(base_addr), uint8(k), [16]byte(a))
}

func maskCvtepi32Storeu8(base_addr uintptr, k uint8, a [16]byte) 


// Cvtepi6416: Convert packed 64-bit integers in 'a' to packed 16-bit integers
// with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			k := 16*j
//			dst[k+15:k] := Truncate_Int64_To_Int16(a[i+63:i])
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVQW'. Intrinsic: '_mm_cvtepi64_epi16'.
// Requires AVX512F.
func Cvtepi6416(a M128i) M128i {
	return M128i(cvtepi6416([16]byte(a)))
}

func cvtepi6416(a [16]byte) [16]byte


// MaskCvtepi6416: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Truncate_Int64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVQW'. Intrinsic: '_mm_mask_cvtepi64_epi16'.
// Requires AVX512F.
func MaskCvtepi6416(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtepi6416([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi6416(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepi6416: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Truncate_Int64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVQW'. Intrinsic: '_mm_maskz_cvtepi64_epi16'.
// Requires AVX512F.
func MaskzCvtepi6416(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtepi6416(uint8(k), [16]byte(a)))
}

func maskzCvtepi6416(k uint8, a [16]byte) [16]byte


// Cvtepi6432: Convert packed 64-bit integers in 'a' to packed 32-bit integers
// with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			k := 32*j
//			dst[k+31:k] := Truncate_Int64_To_Int32(a[i+63:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVQD'. Intrinsic: '_mm_cvtepi64_epi32'.
// Requires AVX512F.
func Cvtepi6432(a M128i) M128i {
	return M128i(cvtepi6432([16]byte(a)))
}

func cvtepi6432(a [16]byte) [16]byte


// MaskCvtepi6432: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Truncate_Int64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := src[l+31:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVQD'. Intrinsic: '_mm_mask_cvtepi64_epi32'.
// Requires AVX512F.
func MaskCvtepi6432(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtepi6432([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi6432(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepi6432: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Truncate_Int64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVQD'. Intrinsic: '_mm_maskz_cvtepi64_epi32'.
// Requires AVX512F.
func MaskzCvtepi6432(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtepi6432(uint8(k), [16]byte(a)))
}

func maskzCvtepi6432(k uint8, a [16]byte) [16]byte


// Cvtepi648: Convert packed 64-bit integers in 'a' to packed 8-bit integers
// with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			k := 8*j
//			dst[k+7:k] := Truncate_Int64_To_Int8(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQB'. Intrinsic: '_mm_cvtepi64_epi8'.
// Requires AVX512F.
func Cvtepi648(a M128i) M128i {
	return M128i(cvtepi648([16]byte(a)))
}

func cvtepi648(a [16]byte) [16]byte


// MaskCvtepi648: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQB'. Intrinsic: '_mm_mask_cvtepi64_epi8'.
// Requires AVX512F.
func MaskCvtepi648(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtepi648([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi648(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepi648: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQB'. Intrinsic: '_mm_maskz_cvtepi64_epi8'.
// Requires AVX512F.
func MaskzCvtepi648(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtepi648(uint8(k), [16]byte(a)))
}

func maskzCvtepi648(k uint8, a [16]byte) [16]byte


// Cvtepi64Pd: Convert packed 64-bit integers in 'a' to packed double-precision
// (64-bit) floating-point elements, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTQQ2PD'. Intrinsic: '_mm_cvtepi64_pd'.
// Requires AVX512DQ.
func Cvtepi64Pd(a M128i) M128d {
	return M128d(cvtepi64Pd([16]byte(a)))
}

func cvtepi64Pd(a [16]byte) [2]float64


// MaskCvtepi64Pd: Convert packed 64-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTQQ2PD'. Intrinsic: '_mm_mask_cvtepi64_pd'.
// Requires AVX512DQ.
func MaskCvtepi64Pd(src M128d, k Mmask8, a M128i) M128d {
	return M128d(maskCvtepi64Pd([2]float64(src), uint8(k), [16]byte(a)))
}

func maskCvtepi64Pd(src [2]float64, k uint8, a [16]byte) [2]float64


// MaskzCvtepi64Pd: Convert packed 64-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTQQ2PD'. Intrinsic: '_mm_maskz_cvtepi64_pd'.
// Requires AVX512DQ.
func MaskzCvtepi64Pd(k Mmask8, a M128i) M128d {
	return M128d(maskzCvtepi64Pd(uint8(k), [16]byte(a)))
}

func maskzCvtepi64Pd(k uint8, a [16]byte) [2]float64


// Cvtepi64Ps: Convert packed 64-bit integers in 'a' to packed single-precision
// (32-bit) floating-point elements, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			l := j*32
//			dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTQQ2PS'. Intrinsic: '_mm_cvtepi64_ps'.
// Requires AVX512DQ.
func Cvtepi64Ps(a M128i) M128 {
	return M128(cvtepi64Ps([16]byte(a)))
}

func cvtepi64Ps(a [16]byte) [4]float32


// MaskCvtepi64Ps: Convert packed 64-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := src[l+31:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTQQ2PS'. Intrinsic: '_mm_mask_cvtepi64_ps'.
// Requires AVX512DQ.
func MaskCvtepi64Ps(src M128, k Mmask8, a M128i) M128 {
	return M128(maskCvtepi64Ps([4]float32(src), uint8(k), [16]byte(a)))
}

func maskCvtepi64Ps(src [4]float32, k uint8, a [16]byte) [4]float32


// MaskzCvtepi64Ps: Convert packed 64-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTQQ2PS'. Intrinsic: '_mm_maskz_cvtepi64_ps'.
// Requires AVX512DQ.
func MaskzCvtepi64Ps(k Mmask8, a M128i) M128 {
	return M128(maskzCvtepi64Ps(uint8(k), [16]byte(a)))
}

func maskzCvtepi64Ps(k uint8, a [16]byte) [4]float32


// MaskCvtepi64Storeu16: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the active results (those with their
// respective bit set in writemask 'k') to unaligned memory at 'base_addr'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				MEM[base_addr+l+15:base_addr+l] := Truncate_Int64_To_Int16(a[i+63:i])
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVQW'. Intrinsic: '_mm_mask_cvtepi64_storeu_epi16'.
// Requires AVX512F.
func MaskCvtepi64Storeu16(base_addr uintptr, k Mmask8, a M128i)  {
	maskCvtepi64Storeu16(uintptr(base_addr), uint8(k), [16]byte(a))
}

func maskCvtepi64Storeu16(base_addr uintptr, k uint8, a [16]byte) 


// MaskCvtepi64Storeu32: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with truncation, and store the active results (those with their
// respective bit set in writemask 'k') to unaligned memory at 'base_addr'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				MEM[base_addr+l+31:base_addr+l] := Truncate_Int64_To_Int32(a[i+63:i])
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVQD'. Intrinsic: '_mm_mask_cvtepi64_storeu_epi32'.
// Requires AVX512F.
func MaskCvtepi64Storeu32(base_addr uintptr, k Mmask8, a M128i)  {
	maskCvtepi64Storeu32(uintptr(base_addr), uint8(k), [16]byte(a))
}

func maskCvtepi64Storeu32(base_addr uintptr, k uint8, a [16]byte) 


// MaskCvtepi64Storeu8: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the active results (those with their
// respective bit set in writemask 'k') to unaligned memory at 'base_addr'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Truncate_Int64_To_Int8(a[i+63:i])
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQB'. Intrinsic: '_mm_mask_cvtepi64_storeu_epi8'.
// Requires AVX512F.
func MaskCvtepi64Storeu8(base_addr uintptr, k Mmask8, a M128i)  {
	maskCvtepi64Storeu8(uintptr(base_addr), uint8(k), [16]byte(a))
}

func maskCvtepi64Storeu8(base_addr uintptr, k uint8, a [16]byte) 


// Cvtepi816: Sign extend packed 8-bit integers in 'a' to packed 16-bit
// integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*8
//			l := j*16
//			dst[l+15:l] := SignExtend(a[i+7:i])
//		ENDFOR
//
// Instruction: 'PMOVSXBW'. Intrinsic: '_mm_cvtepi8_epi16'.
// Requires SSE4.1.
func Cvtepi816(a M128i) M128i {
	return M128i(cvtepi816([16]byte(a)))
}

func cvtepi816(a [16]byte) [16]byte


// MaskCvtepi816: Sign extend packed 8-bit integers in 'a' to packed 16-bit
// integers, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*8
//			l := j*16
//			IF k[j]
//				dst[l+15:l] := SignExtend(a[i+7:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSXBW'. Intrinsic: '_mm_mask_cvtepi8_epi16'.
// Requires AVX512BW.
func MaskCvtepi816(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtepi816([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi816(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepi816: Sign extend packed 8-bit integers in 'a' to packed 16-bit
// integers, and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*8
//			l := j*16
//			IF k[j]
//				dst[l+15:l] := SignExtend(a[i+7:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSXBW'. Intrinsic: '_mm_maskz_cvtepi8_epi16'.
// Requires AVX512BW.
func MaskzCvtepi816(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtepi816(uint8(k), [16]byte(a)))
}

func maskzCvtepi816(k uint8, a [16]byte) [16]byte


// Cvtepi832: Sign extend packed 8-bit integers in 'a' to packed 32-bit
// integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			k := 8*j
//			dst[i+31:i] := SignExtend(a[k+7:k])
//		ENDFOR
//
// Instruction: 'PMOVSXBD'. Intrinsic: '_mm_cvtepi8_epi32'.
// Requires SSE4.1.
func Cvtepi832(a M128i) M128i {
	return M128i(cvtepi832([16]byte(a)))
}

func cvtepi832(a [16]byte) [16]byte


// MaskCvtepi832: Sign extend packed 8-bit integers in the low 4 bytes of 'a'
// to packed 32-bit integers, and store the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[l+7:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSXBD'. Intrinsic: '_mm_mask_cvtepi8_epi32'.
// Requires AVX512F.
func MaskCvtepi832(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtepi832([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi832(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepi832: Sign extend packed 8-bit integers in the low 4 bytes of 'a'
// to packed 32-bit integers, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[l+7:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSXBD'. Intrinsic: '_mm_maskz_cvtepi8_epi32'.
// Requires AVX512F.
func MaskzCvtepi832(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtepi832(uint8(k), [16]byte(a)))
}

func maskzCvtepi832(k uint8, a [16]byte) [16]byte


// Cvtepi864: Sign extend packed 8-bit integers in the low 8 bytes of 'a' to
// packed 64-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			k := 8*j
//			dst[i+63:i] := SignExtend(a[k+7:k])
//		ENDFOR
//
// Instruction: 'PMOVSXBQ'. Intrinsic: '_mm_cvtepi8_epi64'.
// Requires SSE4.1.
func Cvtepi864(a M128i) M128i {
	return M128i(cvtepi864([16]byte(a)))
}

func cvtepi864(a [16]byte) [16]byte


// MaskCvtepi864: Sign extend packed 8-bit integers in the low 2 bytes of 'a'
// to packed 64-bit integers, and store the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+7:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSXBQ'. Intrinsic: '_mm_mask_cvtepi8_epi64'.
// Requires AVX512F.
func MaskCvtepi864(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtepi864([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi864(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepi864: Sign extend packed 8-bit integers in the low 2 bytes of 'a'
// to packed 64-bit integers, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+7:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSXBQ'. Intrinsic: '_mm_maskz_cvtepi8_epi64'.
// Requires AVX512F.
func MaskzCvtepi864(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtepi864(uint8(k), [16]byte(a)))
}

func maskzCvtepi864(k uint8, a [16]byte) [16]byte


// Cvtepu1632: Zero extend packed unsigned 16-bit integers in 'a' to packed
// 32-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			k := 16*j
//			dst[i+31:i] := ZeroExtend(a[k+15:k])
//		ENDFOR
//
// Instruction: 'PMOVZXWD'. Intrinsic: '_mm_cvtepu16_epi32'.
// Requires SSE4.1.
func Cvtepu1632(a M128i) M128i {
	return M128i(cvtepu1632([16]byte(a)))
}

func cvtepu1632(a [16]byte) [16]byte


// MaskCvtepu1632: Zero extend packed unsigned 16-bit integers in 'a' to packed
// 32-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[l+15:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVZXWD'. Intrinsic: '_mm_mask_cvtepu16_epi32'.
// Requires AVX512F.
func MaskCvtepu1632(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtepu1632([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepu1632(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepu1632: Zero extend packed unsigned 16-bit integers in 'a' to
// packed 32-bit integers, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[l+15:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVZXWD'. Intrinsic: '_mm_maskz_cvtepu16_epi32'.
// Requires AVX512F.
func MaskzCvtepu1632(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtepu1632(uint8(k), [16]byte(a)))
}

func maskzCvtepu1632(k uint8, a [16]byte) [16]byte


// Cvtepu1664: Zero extend packed unsigned 16-bit integers in 'a' to packed
// 64-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			k := 16*j
//			dst[i+63:i] := ZeroExtend(a[k+15:k])
//		ENDFOR
//
// Instruction: 'PMOVZXWQ'. Intrinsic: '_mm_cvtepu16_epi64'.
// Requires SSE4.1.
func Cvtepu1664(a M128i) M128i {
	return M128i(cvtepu1664([16]byte(a)))
}

func cvtepu1664(a [16]byte) [16]byte


// MaskCvtepu1664: Zero extend packed unsigned 16-bit integers in the low 4
// bytes of 'a' to packed 64-bit integers, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+15:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVZXWQ'. Intrinsic: '_mm_mask_cvtepu16_epi64'.
// Requires AVX512F.
func MaskCvtepu1664(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtepu1664([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepu1664(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepu1664: Zero extend packed unsigned 16-bit integers in the low 4
// bytes of 'a' to packed 64-bit integers, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+15:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVZXWQ'. Intrinsic: '_mm_maskz_cvtepu16_epi64'.
// Requires AVX512F.
func MaskzCvtepu1664(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtepu1664(uint8(k), [16]byte(a)))
}

func maskzCvtepu1664(k uint8, a [16]byte) [16]byte


// Cvtepu3264: Zero extend packed unsigned 32-bit integers in 'a' to packed
// 64-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			k := 32*j
//			dst[i+63:i] := ZeroExtend(a[k+31:k])
//		ENDFOR
//
// Instruction: 'PMOVZXDQ'. Intrinsic: '_mm_cvtepu32_epi64'.
// Requires SSE4.1.
func Cvtepu3264(a M128i) M128i {
	return M128i(cvtepu3264([16]byte(a)))
}

func cvtepu3264(a [16]byte) [16]byte


// MaskCvtepu3264: Zero extend packed unsigned 32-bit integers in 'a' to packed
// 64-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVZXDQ'. Intrinsic: '_mm_mask_cvtepu32_epi64'.
// Requires AVX512F.
func MaskCvtepu3264(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtepu3264([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepu3264(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepu3264: Zero extend packed unsigned 32-bit integers in 'a' to
// packed 64-bit integers, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+31:l])
//			ELSE 
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVZXDQ'. Intrinsic: '_mm_maskz_cvtepu32_epi64'.
// Requires AVX512F.
func MaskzCvtepu3264(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtepu3264(uint8(k), [16]byte(a)))
}

func maskzCvtepu3264(k uint8, a [16]byte) [16]byte


// Cvtepu32Pd: Convert packed unsigned 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			l := j*32
//			dst[i+63:i] := ConvertUnsignedIntegerTo_FP64(a[l+31:l])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTUDQ2PD'. Intrinsic: '_mm_cvtepu32_pd'.
// Requires AVX512F.
func Cvtepu32Pd(a M128i) M128d {
	return M128d(cvtepu32Pd([16]byte(a)))
}

func cvtepu32Pd(a [16]byte) [2]float64


// MaskCvtepu32Pd: Convert packed unsigned 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := ConvertUnsignedIntegerTo_FP64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTUDQ2PD'. Intrinsic: '_mm_mask_cvtepu32_pd'.
// Requires AVX512F.
func MaskCvtepu32Pd(src M128d, k Mmask8, a M128i) M128d {
	return M128d(maskCvtepu32Pd([2]float64(src), uint8(k), [16]byte(a)))
}

func maskCvtepu32Pd(src [2]float64, k uint8, a [16]byte) [2]float64


// MaskzCvtepu32Pd: Convert packed unsigned 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := ConvertUnsignedIntegerTo_FP64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTUDQ2PD'. Intrinsic: '_mm_maskz_cvtepu32_pd'.
// Requires AVX512F.
func MaskzCvtepu32Pd(k Mmask8, a M128i) M128d {
	return M128d(maskzCvtepu32Pd(uint8(k), [16]byte(a)))
}

func maskzCvtepu32Pd(k uint8, a [16]byte) [2]float64


// Cvtepu64Pd: Convert packed unsigned 64-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := ConvertUnsignedInt64_To_FP64(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTUQQ2PD'. Intrinsic: '_mm_cvtepu64_pd'.
// Requires AVX512DQ.
func Cvtepu64Pd(a M128i) M128d {
	return M128d(cvtepu64Pd([16]byte(a)))
}

func cvtepu64Pd(a [16]byte) [2]float64


// MaskCvtepu64Pd: Convert packed unsigned 64-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ConvertUnsignedInt64_To_FP64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTUQQ2PD'. Intrinsic: '_mm_mask_cvtepu64_pd'.
// Requires AVX512DQ.
func MaskCvtepu64Pd(src M128d, k Mmask8, a M128i) M128d {
	return M128d(maskCvtepu64Pd([2]float64(src), uint8(k), [16]byte(a)))
}

func maskCvtepu64Pd(src [2]float64, k uint8, a [16]byte) [2]float64


// MaskzCvtepu64Pd: Convert packed unsigned 64-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ConvertUnsignedInt64_To_FP64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTUQQ2PD'. Intrinsic: '_mm_maskz_cvtepu64_pd'.
// Requires AVX512DQ.
func MaskzCvtepu64Pd(k Mmask8, a M128i) M128d {
	return M128d(maskzCvtepu64Pd(uint8(k), [16]byte(a)))
}

func maskzCvtepu64Pd(k uint8, a [16]byte) [2]float64


// Cvtepu64Ps: Convert packed unsigned 64-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			l := j*32
//			dst[l+31:l] := ConvertUnsignedInt64_To_FP32(a[i+63:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTUQQ2PS'. Intrinsic: '_mm_cvtepu64_ps'.
// Requires AVX512DQ.
func Cvtepu64Ps(a M128i) M128 {
	return M128(cvtepu64Ps([16]byte(a)))
}

func cvtepu64Ps(a [16]byte) [4]float32


// MaskCvtepu64Ps: Convert packed unsigned 64-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[l+31:l] := ConvertUnsignedInt64_To_FP32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := src[l+31:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTUQQ2PS'. Intrinsic: '_mm_mask_cvtepu64_ps'.
// Requires AVX512DQ.
func MaskCvtepu64Ps(src M128, k Mmask8, a M128i) M128 {
	return M128(maskCvtepu64Ps([4]float32(src), uint8(k), [16]byte(a)))
}

func maskCvtepu64Ps(src [4]float32, k uint8, a [16]byte) [4]float32


// MaskzCvtepu64Ps: Convert packed unsigned 64-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[l+31:l] := ConvertUnsignedInt64_To_FP32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTUQQ2PS'. Intrinsic: '_mm_maskz_cvtepu64_ps'.
// Requires AVX512DQ.
func MaskzCvtepu64Ps(k Mmask8, a M128i) M128 {
	return M128(maskzCvtepu64Ps(uint8(k), [16]byte(a)))
}

func maskzCvtepu64Ps(k uint8, a [16]byte) [4]float32


// Cvtepu816: Zero extend packed unsigned 8-bit integers in 'a' to packed
// 16-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*8
//			l := j*16
//			dst[l+15:l] := ZeroExtend(a[i+7:i])
//		ENDFOR
//
// Instruction: 'PMOVZXBW'. Intrinsic: '_mm_cvtepu8_epi16'.
// Requires SSE4.1.
func Cvtepu816(a M128i) M128i {
	return M128i(cvtepu816([16]byte(a)))
}

func cvtepu816(a [16]byte) [16]byte


// MaskCvtepu816: Zero extend packed unsigned 8-bit integers in 'a' to packed
// 16-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*8
//			l := j*16
//			IF k[j]
//				dst[l+15:l] := ZeroExtend(a[i+7:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVZXBW'. Intrinsic: '_mm_mask_cvtepu8_epi16'.
// Requires AVX512BW.
func MaskCvtepu816(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtepu816([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepu816(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepu816: Zero extend packed unsigned 8-bit integers in 'a' to packed
// 16-bit integers, and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*8
//			l := j*16
//			IF k[j]
//				dst[l+15:l] := ZeroExtend(a[i+7:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVZXBW'. Intrinsic: '_mm_maskz_cvtepu8_epi16'.
// Requires AVX512BW.
func MaskzCvtepu816(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtepu816(uint8(k), [16]byte(a)))
}

func maskzCvtepu816(k uint8, a [16]byte) [16]byte


// Cvtepu832: Zero extend packed unsigned 8-bit integers in 'a' to packed
// 32-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			k := 8*j
//			dst[i+31:i] := ZeroExtend(a[k+7:k])
//		ENDFOR
//
// Instruction: 'PMOVZXBD'. Intrinsic: '_mm_cvtepu8_epi32'.
// Requires SSE4.1.
func Cvtepu832(a M128i) M128i {
	return M128i(cvtepu832([16]byte(a)))
}

func cvtepu832(a [16]byte) [16]byte


// MaskCvtepu832: Zero extend packed unsigned 8-bit integers in the low 4 bytes
// of 'a' to packed 32-bit integers, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[l+7:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVZXBD'. Intrinsic: '_mm_mask_cvtepu8_epi32'.
// Requires AVX512F.
func MaskCvtepu832(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtepu832([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepu832(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepu832: Zero extend packed unsigned 8-bit integers in th elow 4
// bytes of 'a' to packed 32-bit integers, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[l+7:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVZXBD'. Intrinsic: '_mm_maskz_cvtepu8_epi32'.
// Requires AVX512F.
func MaskzCvtepu832(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtepu832(uint8(k), [16]byte(a)))
}

func maskzCvtepu832(k uint8, a [16]byte) [16]byte


// Cvtepu864: Zero extend packed unsigned 8-bit integers in the low 8 byte sof
// 'a' to packed 64-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			k := 8*j
//			dst[i+63:i] := ZeroExtend(a[k+7:k])
//		ENDFOR
//
// Instruction: 'PMOVZXBQ'. Intrinsic: '_mm_cvtepu8_epi64'.
// Requires SSE4.1.
func Cvtepu864(a M128i) M128i {
	return M128i(cvtepu864([16]byte(a)))
}

func cvtepu864(a [16]byte) [16]byte


// MaskCvtepu864: Zero extend packed unsigned 8-bit integers in the low 2 bytes
// of 'a' to packed 64-bit integers, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+7:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVZXBQ'. Intrinsic: '_mm_mask_cvtepu8_epi64'.
// Requires AVX512F.
func MaskCvtepu864(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtepu864([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepu864(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtepu864: Zero extend packed unsigned 8-bit integers in the low 2
// bytes of 'a' to packed 64-bit integers, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+7:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVZXBQ'. Intrinsic: '_mm_maskz_cvtepu8_epi64'.
// Requires AVX512F.
func MaskzCvtepu864(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtepu864(uint8(k), [16]byte(a)))
}

func maskzCvtepu864(k uint8, a [16]byte) [16]byte


// Cvti32Sd: Convert the 32-bit integer 'b' to a double-precision (64-bit)
// floating-point element, store the result in the lower element of 'dst', and
// copy the upper element from 'a' to the upper element of 'dst'. 
//
//		dst[63:0] := Convert_Int32_To_FP64(b[31:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTSI2SD'. Intrinsic: '_mm_cvti32_sd'.
// Requires AVX512F.
func Cvti32Sd(a M128d, b int) M128d {
	return M128d(cvti32Sd([2]float64(a), b))
}

func cvti32Sd(a [2]float64, b int) [2]float64


// Cvti32Ss: Convert the 32-bit integer 'b' to a single-precision (32-bit)
// floating-point element, store the result in the lower element of 'dst', and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'. 
//
//		dst[31:0] := Convert_Int32_To_FP32(b[31:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTSI2SS'. Intrinsic: '_mm_cvti32_ss'.
// Requires AVX512F.
func Cvti32Ss(a M128, b int) M128 {
	return M128(cvti32Ss([4]float32(a), b))
}

func cvti32Ss(a [4]float32, b int) [4]float32


// Cvti64Sd: Convert the 64-bit integer 'b' to a double-precision (64-bit)
// floating-point element, store the result in the lower element of 'dst', and
// copy the upper element from 'a' to the upper element of 'dst'. 
//
//		dst[63:0] := Convert_Int64_To_FP64(b[63:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTSI2SD'. Intrinsic: '_mm_cvti64_sd'.
// Requires AVX512F.
func Cvti64Sd(a M128d, b int64) M128d {
	return M128d(cvti64Sd([2]float64(a), b))
}

func cvti64Sd(a [2]float64, b int64) [2]float64


// Cvti64Ss: Convert the 64-bit integer 'b' to a single-precision (32-bit)
// floating-point element, store the result in the lower element of 'dst', and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'. 
//
//		dst[31:0] := Convert_Int64_To_FP32(b[63:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTSI2SS'. Intrinsic: '_mm_cvti64_ss'.
// Requires AVX512F.
func Cvti64Ss(a M128, b int64) M128 {
	return M128(cvti64Ss([4]float32(a), b))
}

func cvti64Ss(a [4]float32, b int64) [4]float32


// Cvtpd32: Convert packed double-precision (64-bit) floating-point elements in
// 'a' to packed 32-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_Int32(a[k+63:k])
//		ENDFOR
//
// Instruction: 'CVTPD2DQ'. Intrinsic: '_mm_cvtpd_epi32'.
// Requires SSE2.
func Cvtpd32(a M128d) M128i {
	return M128i(cvtpd32([2]float64(a)))
}

func cvtpd32(a [2]float64) [16]byte


// MaskCvtpd32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*32
//			l := j*64
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_Int32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTPD2DQ'. Intrinsic: '_mm_mask_cvtpd_epi32'.
// Requires AVX512F.
func MaskCvtpd32(src M128i, k Mmask8, a M128d) M128i {
	return M128i(maskCvtpd32([16]byte(src), uint8(k), [2]float64(a)))
}

func maskCvtpd32(src [16]byte, k uint8, a [2]float64) [16]byte


// MaskzCvtpd32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 1
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_Int32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTPD2DQ'. Intrinsic: '_mm_maskz_cvtpd_epi32'.
// Requires AVX512F.
func MaskzCvtpd32(k Mmask8, a M128d) M128i {
	return M128i(maskzCvtpd32(uint8(k), [2]float64(a)))
}

func maskzCvtpd32(k uint8, a [2]float64) [16]byte


// Cvtpd64: Convert packed double-precision (64-bit) floating-point elements in
// 'a' to packed 64-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := Convert_FP64_To_Int64(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPD2QQ'. Intrinsic: '_mm_cvtpd_epi64'.
// Requires AVX512DQ.
func Cvtpd64(a M128d) M128i {
	return M128i(cvtpd64([2]float64(a)))
}

func cvtpd64(a [2]float64) [16]byte


// MaskCvtpd64: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 64-bit integers, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_FP64_To_Int64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPD2QQ'. Intrinsic: '_mm_mask_cvtpd_epi64'.
// Requires AVX512DQ.
func MaskCvtpd64(src M128i, k Mmask8, a M128d) M128i {
	return M128i(maskCvtpd64([16]byte(src), uint8(k), [2]float64(a)))
}

func maskCvtpd64(src [16]byte, k uint8, a [2]float64) [16]byte


// MaskzCvtpd64: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 64-bit integers, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_FP64_To_Int64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPD2QQ'. Intrinsic: '_mm_maskz_cvtpd_epi64'.
// Requires AVX512DQ.
func MaskzCvtpd64(k Mmask8, a M128d) M128i {
	return M128i(maskzCvtpd64(uint8(k), [2]float64(a)))
}

func maskzCvtpd64(k uint8, a [2]float64) [16]byte


// CvtpdEpu32: Convert packed double-precision (64-bit) floating-point elements
// in 'a' to packed unsigned 32-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[k+63:k])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTPD2UDQ'. Intrinsic: '_mm_cvtpd_epu32'.
// Requires AVX512F.
func CvtpdEpu32(a M128d) M128i {
	return M128i(cvtpdEpu32([2]float64(a)))
}

func cvtpdEpu32(a [2]float64) [16]byte


// MaskCvtpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*32
//			l := j*64
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTPD2UDQ'. Intrinsic: '_mm_mask_cvtpd_epu32'.
// Requires AVX512F.
func MaskCvtpdEpu32(src M128i, k Mmask8, a M128d) M128i {
	return M128i(maskCvtpdEpu32([16]byte(src), uint8(k), [2]float64(a)))
}

func maskCvtpdEpu32(src [16]byte, k uint8, a [2]float64) [16]byte


// MaskzCvtpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTPD2UDQ'. Intrinsic: '_mm_maskz_cvtpd_epu32'.
// Requires AVX512F.
func MaskzCvtpdEpu32(k Mmask8, a M128d) M128i {
	return M128i(maskzCvtpdEpu32(uint8(k), [2]float64(a)))
}

func maskzCvtpdEpu32(k uint8, a [2]float64) [16]byte


// CvtpdEpu64: Convert packed double-precision (64-bit) floating-point elements
// in 'a' to packed unsigned 64-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := Convert_FP64_To_UnsignedInt64(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPD2UQQ'. Intrinsic: '_mm_cvtpd_epu64'.
// Requires AVX512DQ.
func CvtpdEpu64(a M128d) M128i {
	return M128i(cvtpdEpu64([2]float64(a)))
}

func cvtpdEpu64(a [2]float64) [16]byte


// MaskCvtpdEpu64: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 64-bit integers, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_FP64_To_UnsignedInt64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPD2UQQ'. Intrinsic: '_mm_mask_cvtpd_epu64'.
// Requires AVX512DQ.
func MaskCvtpdEpu64(src M128i, k Mmask8, a M128d) M128i {
	return M128i(maskCvtpdEpu64([16]byte(src), uint8(k), [2]float64(a)))
}

func maskCvtpdEpu64(src [16]byte, k uint8, a [2]float64) [16]byte


// MaskzCvtpdEpu64: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 64-bit integers, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_FP64_To_UnsignedInt64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPD2UQQ'. Intrinsic: '_mm_maskz_cvtpd_epu64'.
// Requires AVX512DQ.
func MaskzCvtpdEpu64(k Mmask8, a M128d) M128i {
	return M128i(maskzCvtpdEpu64(uint8(k), [2]float64(a)))
}

func maskzCvtpdEpu64(k uint8, a [2]float64) [16]byte


// CvtpdPs: Convert packed double-precision (64-bit) floating-point elements in
// 'a' to packed single-precision (32-bit) floating-point elements, and store
// the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_FP32(a[k+63:k])
//		ENDFOR
//
// Instruction: 'CVTPD2PS'. Intrinsic: '_mm_cvtpd_ps'.
// Requires SSE2.
func CvtpdPs(a M128d) M128 {
	return M128(cvtpdPs([2]float64(a)))
}

func cvtpdPs(a [2]float64) [4]float32


// MaskCvtpdPs: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed single-precision (32-bit) floating-point elements,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_FP32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTPD2PS'. Intrinsic: '_mm_mask_cvtpd_ps'.
// Requires AVX512F.
func MaskCvtpdPs(src M128, k Mmask8, a M128d) M128 {
	return M128(maskCvtpdPs([4]float32(src), uint8(k), [2]float64(a)))
}

func maskCvtpdPs(src [4]float32, k uint8, a [2]float64) [4]float32


// MaskzCvtpdPs: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed single-precision (32-bit) floating-point elements,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*32
//			l := j*64
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_FP32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTPD2PS'. Intrinsic: '_mm_maskz_cvtpd_ps'.
// Requires AVX512F.
func MaskzCvtpdPs(k Mmask8, a M128d) M128 {
	return M128(maskzCvtpdPs(uint8(k), [2]float64(a)))
}

func maskzCvtpdPs(k uint8, a [2]float64) [4]float32


// CvtphPs: Convert packed half-precision (16-bit) floating-point elements in
// 'a' to packed single-precision (32-bit) floating-point elements, and store
// the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			m := j*16
//			dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPH2PS'. Intrinsic: '_mm_cvtph_ps'.
// Requires FP16C.
func CvtphPs(a M128i) M128 {
	return M128(cvtphPs([16]byte(a)))
}

func cvtphPs(a [16]byte) [4]float32


// MaskCvtphPs: Convert packed half-precision (16-bit) floating-point elements
// in 'a' to packed single-precision (32-bit) floating-point elements, and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			m := j*16
//			IF k[j]
//				dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPH2PS'. Intrinsic: '_mm_mask_cvtph_ps'.
// Requires AVX512F.
func MaskCvtphPs(src M128, k Mmask8, a M128i) M128 {
	return M128(maskCvtphPs([4]float32(src), uint8(k), [16]byte(a)))
}

func maskCvtphPs(src [4]float32, k uint8, a [16]byte) [4]float32


// MaskzCvtphPs: Convert packed half-precision (16-bit) floating-point elements
// in 'a' to packed single-precision (32-bit) floating-point elements, and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			m := j*16
//			IF k[j]
//				dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPH2PS'. Intrinsic: '_mm_maskz_cvtph_ps'.
// Requires AVX512F.
func MaskzCvtphPs(k Mmask8, a M128i) M128 {
	return M128(maskzCvtphPs(uint8(k), [16]byte(a)))
}

func maskzCvtphPs(k uint8, a [16]byte) [4]float32


// Cvtpi16Ps: Convert packed 16-bit integers in 'a' to packed single-precision
// (32-bit) floating-point elements, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*16
//			m := j*32
//			dst[m+31:m] := Convert_Int16_To_FP32(a[i+15:i])
//		ENDFOR
//
// Instruction: '...'. Intrinsic: '_mm_cvtpi16_ps'.
// Requires SSE.
func Cvtpi16Ps(a M64) M128 {
	return M128(cvtpi16Ps(a))
}

func cvtpi16Ps(a M64) [4]float32


// Cvtpi32Pd: Convert packed 32-bit integers in 'a' to packed double-precision
// (64-bit) floating-point elements, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*32
//			m := j*64
//			dst[m+63:m] := Convert_Int32_To_FP64(a[i+31:i])
//		ENDFOR
//
// Instruction: 'CVTPI2PD'. Intrinsic: '_mm_cvtpi32_pd'.
// Requires SSE2.
func Cvtpi32Pd(a M64) M128d {
	return M128d(cvtpi32Pd(a))
}

func cvtpi32Pd(a M64) [2]float64


// Cvtpi32Ps: Convert packed 32-bit integers in 'b' to packed single-precision
// (32-bit) floating-point elements, store the results in the lower 2 elements
// of 'dst', and copy the upper 2 packed elements from 'a' to the upper
// elements of 'dst'. 
//
//		dst[31:0] := Convert_Int32_To_FP32(b[31:0])
//		dst[63:32] := Convert_Int32_To_FP32(b[63:32])
//		dst[95:64] := a[95:64]
//		dst[127:96] := a[127:96]
//
// Instruction: 'CVTPI2PS'. Intrinsic: '_mm_cvtpi32_ps'.
// Requires SSE.
func Cvtpi32Ps(a M128, b M64) M128 {
	return M128(cvtpi32Ps([4]float32(a), b))
}

func cvtpi32Ps(a [4]float32, b M64) [4]float32


// Cvtpi32x2Ps: Convert packed 32-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, store the results in the
// lower 2 elements of 'dst', then covert the packed 32-bit integers in 'a' to
// single-precision (32-bit) floating-point element, and store the results in
// the upper 2 elements of 'dst'. 
//
//		dst[31:0] := Convert_Int32_To_FP32(a[31:0])
//		dst[63:32] := Convert_Int32_To_FP32(a[63:32])
//		dst[95:64] := Convert_Int32_To_FP32(b[31:0])
//		dst[127:96] := Convert_Int32_To_FP32(b[63:32])
//
// Instruction: '...'. Intrinsic: '_mm_cvtpi32x2_ps'.
// Requires SSE.
func Cvtpi32x2Ps(a M64, b M64) M128 {
	return M128(cvtpi32x2Ps(a, b))
}

func cvtpi32x2Ps(a M64, b M64) [4]float32


// Cvtpi8Ps: Convert the lower packed 8-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*8
//			m := j*32
//			dst[m+31:m] := Convert_Int8_To_FP32(a[i+7:i])
//		ENDFOR
//
// Instruction: '...'. Intrinsic: '_mm_cvtpi8_ps'.
// Requires SSE.
func Cvtpi8Ps(a M64) M128 {
	return M128(cvtpi8Ps(a))
}

func cvtpi8Ps(a M64) [4]float32


// Cvtps32: Convert packed single-precision (32-bit) floating-point elements in
// 'a' to packed 32-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
//		ENDFOR
//
// Instruction: 'CVTPS2DQ'. Intrinsic: '_mm_cvtps_epi32'.
// Requires SSE2.
func Cvtps32(a M128) M128i {
	return M128i(cvtps32([4]float32(a)))
}

func cvtps32(a [4]float32) [16]byte


// MaskCvtps32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPS2DQ'. Intrinsic: '_mm_mask_cvtps_epi32'.
// Requires AVX512F.
func MaskCvtps32(src M128i, k Mmask8, a M128) M128i {
	return M128i(maskCvtps32([16]byte(src), uint8(k), [4]float32(a)))
}

func maskCvtps32(src [16]byte, k uint8, a [4]float32) [16]byte


// MaskzCvtps32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPS2DQ'. Intrinsic: '_mm_maskz_cvtps_epi32'.
// Requires AVX512F.
func MaskzCvtps32(k Mmask8, a M128) M128i {
	return M128i(maskzCvtps32(uint8(k), [4]float32(a)))
}

func maskzCvtps32(k uint8, a [4]float32) [16]byte


// Cvtps64: Convert packed single-precision (32-bit) floating-point elements in
// 'a' to packed 64-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			l := j*32
//			dst[i+63:i] := Convert_FP32_To_Int64(a[l+31:l])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPS2QQ'. Intrinsic: '_mm_cvtps_epi64'.
// Requires AVX512DQ.
func Cvtps64(a M128) M128i {
	return M128i(cvtps64([4]float32(a)))
}

func cvtps64(a [4]float32) [16]byte


// MaskCvtps64: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 64-bit integers, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_Int64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPS2QQ'. Intrinsic: '_mm_mask_cvtps_epi64'.
// Requires AVX512DQ.
func MaskCvtps64(src M128i, k Mmask8, a M128) M128i {
	return M128i(maskCvtps64([16]byte(src), uint8(k), [4]float32(a)))
}

func maskCvtps64(src [16]byte, k uint8, a [4]float32) [16]byte


// MaskzCvtps64: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 64-bit integers, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_Int64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPS2QQ'. Intrinsic: '_mm_maskz_cvtps_epi64'.
// Requires AVX512DQ.
func MaskzCvtps64(k Mmask8, a M128) M128i {
	return M128i(maskzCvtps64(uint8(k), [4]float32(a)))
}

func maskzCvtps64(k uint8, a [4]float32) [16]byte


// CvtpsEpu32: Convert packed single-precision (32-bit) floating-point elements
// in 'a' to packed unsigned 32-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPS2UDQ'. Intrinsic: '_mm_cvtps_epu32'.
// Requires AVX512F.
func CvtpsEpu32(a M128) M128i {
	return M128i(cvtpsEpu32([4]float32(a)))
}

func cvtpsEpu32(a [4]float32) [16]byte


// MaskCvtpsEpu32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPS2UDQ'. Intrinsic: '_mm_mask_cvtps_epu32'.
// Requires AVX512F.
func MaskCvtpsEpu32(src M128i, k Mmask8, a M128) M128i {
	return M128i(maskCvtpsEpu32([16]byte(src), uint8(k), [4]float32(a)))
}

func maskCvtpsEpu32(src [16]byte, k uint8, a [4]float32) [16]byte


// MaskzCvtpsEpu32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPS2UDQ'. Intrinsic: '_mm_maskz_cvtps_epu32'.
// Requires AVX512F.
func MaskzCvtpsEpu32(k Mmask8, a M128) M128i {
	return M128i(maskzCvtpsEpu32(uint8(k), [4]float32(a)))
}

func maskzCvtpsEpu32(k uint8, a [4]float32) [16]byte


// CvtpsEpu64: Convert packed single-precision (32-bit) floating-point elements
// in 'a' to packed unsigned 64-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			l := j*32
//			dst[i+63:i] := Convert_FP32_To_UnsignedInt64(a[l+31:l])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPS2UQQ'. Intrinsic: '_mm_cvtps_epu64'.
// Requires AVX512DQ.
func CvtpsEpu64(a M128) M128i {
	return M128i(cvtpsEpu64([4]float32(a)))
}

func cvtpsEpu64(a [4]float32) [16]byte


// MaskCvtpsEpu64: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 64-bit integers, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_UnsignedInt64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPS2UQQ'. Intrinsic: '_mm_mask_cvtps_epu64'.
// Requires AVX512DQ.
func MaskCvtpsEpu64(src M128i, k Mmask8, a M128) M128i {
	return M128i(maskCvtpsEpu64([16]byte(src), uint8(k), [4]float32(a)))
}

func maskCvtpsEpu64(src [16]byte, k uint8, a [4]float32) [16]byte


// MaskzCvtpsEpu64: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 64-bit integers, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_UnsignedInt64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPS2UQQ'. Intrinsic: '_mm_maskz_cvtps_epu64'.
// Requires AVX512DQ.
func MaskzCvtpsEpu64(k Mmask8, a M128) M128i {
	return M128i(maskzCvtpsEpu64(uint8(k), [4]float32(a)))
}

func maskzCvtpsEpu64(k uint8, a [4]float32) [16]byte


// CvtpsPd: Convert packed single-precision (32-bit) floating-point elements in
// 'a' to packed double-precision (64-bit) floating-point elements, and store
// the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			k := 32*j
//			dst[i+63:i] := Convert_FP32_To_FP64(a[k+31:k])
//		ENDFOR
//
// Instruction: 'CVTPS2PD'. Intrinsic: '_mm_cvtps_pd'.
// Requires SSE2.
func CvtpsPd(a M128) M128d {
	return M128d(cvtpsPd([4]float32(a)))
}

func cvtpsPd(a [4]float32) [2]float64


// CvtpsPh: Convert packed single-precision (32-bit) floating-point elements in
// 'a' to packed half-precision (16-bit) floating-point elements, and store the
// results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 3
//			i := 16*j
//			l := 32*j
//			dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm_cvtps_ph'.
// Requires FP16C.
func CvtpsPh(a M128, rounding int) M128i {
	return M128i(cvtpsPh([4]float32(a), rounding))
}

func cvtpsPh(a [4]float32, rounding int) [16]byte


// MaskCvtpsPh: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed half-precision (16-bit) floating-point elements,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 3
//			i := 16*j
//			l := 32*j
//			IF k[j]
//				dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm_mask_cvtps_ph'.
// Requires AVX512F.
func MaskCvtpsPh(src M128i, k Mmask8, a M128, rounding int) M128i {
	return M128i(maskCvtpsPh([16]byte(src), uint8(k), [4]float32(a), rounding))
}

func maskCvtpsPh(src [16]byte, k uint8, a [4]float32, rounding int) [16]byte


// MaskzCvtpsPh: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed half-precision (16-bit) floating-point elements,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 3
//			i := 16*j
//			l := 32*j
//			IF k[j]
//				dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm_maskz_cvtps_ph'.
// Requires AVX512F.
func MaskzCvtpsPh(k Mmask8, a M128, rounding int) M128i {
	return M128i(maskzCvtpsPh(uint8(k), [4]float32(a), rounding))
}

func maskzCvtpsPh(k uint8, a [4]float32, rounding int) [16]byte


// Cvtpu16Ps: Convert packed unsigned 16-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*16
//			m := j*32
//			dst[m+31:m] := Convert_UnsignedInt16_To_FP32(a[i+15:i])
//		ENDFOR
//
// Instruction: '...'. Intrinsic: '_mm_cvtpu16_ps'.
// Requires SSE.
func Cvtpu16Ps(a M64) M128 {
	return M128(cvtpu16Ps(a))
}

func cvtpu16Ps(a M64) [4]float32


// Cvtpu8Ps: Convert the lower packed unsigned 8-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*8
//			m := j*32
//			dst[m+31:m] := Convert_UnsignedInt8_To_FP32(a[i+7:i])
//		ENDFOR
//
// Instruction: '...'. Intrinsic: '_mm_cvtpu8_ps'.
// Requires SSE.
func Cvtpu8Ps(a M64) M128 {
	return M128(cvtpu8Ps(a))
}

func cvtpu8Ps(a M64) [4]float32


// CvtsdF64: Copy the lower double-precision (64-bit) floating-point element of
// 'a' to 'dst'. 
//
//		dst[63:0] := a[63:0]
//
// Instruction: 'MOVSD'. Intrinsic: '_mm_cvtsd_f64'.
// Requires SSE2.
func CvtsdF64(a M128d) float64 {
	return float64(cvtsdF64([2]float64(a)))
}

func cvtsdF64(a [2]float64) float64


// CvtsdI32: Convert the lower double-precision (64-bit) floating-point element
// in 'a' to a 32-bit integer, and store the result in 'dst'. 
//
//		dst[31:0] := Convert_FP64_To_Int32(a[63:0])
//
// Instruction: 'VCVTSD2SI'. Intrinsic: '_mm_cvtsd_i32'.
// Requires AVX512F.
func CvtsdI32(a M128d) int {
	return int(cvtsdI32([2]float64(a)))
}

func cvtsdI32(a [2]float64) int


// CvtsdI64: Convert the lower double-precision (64-bit) floating-point element
// in 'a' to a 64-bit integer, and store the result in 'dst'. 
//
//		dst[63:0] := Convert_FP64_To_Int64(a[63:0])
//
// Instruction: 'VCVTSD2SI'. Intrinsic: '_mm_cvtsd_i64'.
// Requires AVX512F.
func CvtsdI64(a M128d) int64 {
	return int64(cvtsdI64([2]float64(a)))
}

func cvtsdI64(a [2]float64) int64


// CvtsdSi32: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to a 32-bit integer, and store the result in 'dst'. 
//
//		dst[31:0] := Convert_FP64_To_Int32(a[63:0])
//
// Instruction: 'CVTSD2SI'. Intrinsic: '_mm_cvtsd_si32'.
// Requires SSE2.
func CvtsdSi32(a M128d) int {
	return int(cvtsdSi32([2]float64(a)))
}

func cvtsdSi32(a [2]float64) int


// CvtsdSi64: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to a 64-bit integer, and store the result in 'dst'. 
//
//		dst[63:0] := Convert_FP64_To_Int64(a[63:0])
//
// Instruction: 'CVTSD2SI'. Intrinsic: '_mm_cvtsd_si64'.
// Requires SSE2.
func CvtsdSi64(a M128d) int64 {
	return int64(cvtsdSi64([2]float64(a)))
}

func cvtsdSi64(a [2]float64) int64


// CvtsdSi64x: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to a 64-bit integer, and store the result in 'dst'. 
//
//		dst[63:0] := Convert_FP64_To_Int64(a[63:0])
//
// Instruction: 'CVTSD2SI'. Intrinsic: '_mm_cvtsd_si64x'.
// Requires SSE2.
func CvtsdSi64x(a M128d) int64 {
	return int64(cvtsdSi64x([2]float64(a)))
}

func cvtsdSi64x(a [2]float64) int64


// CvtsdSs: Convert the lower double-precision (64-bit) floating-point element
// in 'b' to a single-precision (32-bit) floating-point element, store the
// result in the lower element of 'dst', and copy the upper element from 'a' to
// the upper element of 'dst'. 
//
//		dst[31:0] := Convert_FP64_To_FP32(b[63:0])
//		dst[127:32] := a[127:31]
//		dst[MAX:64] := 0
//
// Instruction: 'CVTSD2SS'. Intrinsic: '_mm_cvtsd_ss'.
// Requires SSE2.
func CvtsdSs(a M128, b M128d) M128 {
	return M128(cvtsdSs([4]float32(a), [2]float64(b)))
}

func cvtsdSs(a [4]float32, b [2]float64) [4]float32


// MaskCvtsdSs: Convert the lower double-precision (64-bit) floating-point
// element in 'b' to a single-precision (32-bit) floating-point element, store
// the result in the lower element of 'dst' using writemask 'k' (the element is
// copied from 'src' when mask bit 0 is not set), and copy the upper element
// from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := Convert_FP64_To_FP32(b[63:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:31]
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTSD2SS'. Intrinsic: '_mm_mask_cvtsd_ss'.
// Requires AVX512F.
func MaskCvtsdSs(src M128, k Mmask8, a M128, b M128d) M128 {
	return M128(maskCvtsdSs([4]float32(src), uint8(k), [4]float32(a), [2]float64(b)))
}

func maskCvtsdSs(src [4]float32, k uint8, a [4]float32, b [2]float64) [4]float32


// MaskzCvtsdSs: Convert the lower double-precision (64-bit) floating-point
// element in 'b' to a single-precision (32-bit) floating-point element, store
// the result in the lower element of 'dst' using zeromask 'k' (the element is
// zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements
// from 'a' to the upper elements of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := Convert_FP64_To_FP32(b[63:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:31]
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTSD2SS'. Intrinsic: '_mm_maskz_cvtsd_ss'.
// Requires AVX512F.
func MaskzCvtsdSs(k Mmask8, a M128, b M128d) M128 {
	return M128(maskzCvtsdSs(uint8(k), [4]float32(a), [2]float64(b)))
}

func maskzCvtsdSs(k uint8, a [4]float32, b [2]float64) [4]float32


// CvtsdU32: Convert the lower double-precision (64-bit) floating-point element
// in 'a' to an unsigned 32-bit integer, and store the result in 'dst'. 
//
//		dst[31:0] := Convert_FP64_To_UnsignedInt32(a[63:0])
//
// Instruction: 'VCVTSD2USI'. Intrinsic: '_mm_cvtsd_u32'.
// Requires AVX512F.
func CvtsdU32(a M128d) uint32 {
	return uint32(cvtsdU32([2]float64(a)))
}

func cvtsdU32(a [2]float64) uint32


// CvtsdU64: Convert the lower double-precision (64-bit) floating-point element
// in 'a' to an unsigned 64-bit integer, and store the result in 'dst'. 
//
//		dst[63:0] := Convert_FP64_To_UnsignedInt64(a[63:0])
//
// Instruction: 'VCVTSD2USI'. Intrinsic: '_mm_cvtsd_u64'.
// Requires AVX512F.
func CvtsdU64(a M128d) uint64 {
	return uint64(cvtsdU64([2]float64(a)))
}

func cvtsdU64(a [2]float64) uint64


// Cvtsepi168: Convert packed 16-bit integers in 'a' to packed 8-bit integers
// with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 16*j
//			l := 8*j
//			dst[l+7:l] := Saturate_Int16_To_Int8(a[i+15:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSWB'. Intrinsic: '_mm_cvtsepi16_epi8'.
// Requires AVX512BW.
func Cvtsepi168(a M128i) M128i {
	return M128i(cvtsepi168([16]byte(a)))
}

func cvtsepi168(a [16]byte) [16]byte


// MaskCvtsepi168: Convert packed 16-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := 16*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int16_To_Int8(a[i+15:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSWB'. Intrinsic: '_mm_mask_cvtsepi16_epi8'.
// Requires AVX512BW.
func MaskCvtsepi168(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtsepi168([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtsepi168(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtsepi168: Convert packed 16-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := 16*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int16_To_Int8(a[i+15:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSWB'. Intrinsic: '_mm_maskz_cvtsepi16_epi8'.
// Requires AVX512BW.
func MaskzCvtsepi168(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtsepi168(uint8(k), [16]byte(a)))
}

func maskzCvtsepi168(k uint8, a [16]byte) [16]byte


// MaskCvtsepi16Storeu8: Convert packed 16-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the active results (those with
// their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 7
//			i := 16*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Saturate_Int16_To_Int8(a[i+15:i])
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSWB'. Intrinsic: '_mm_mask_cvtsepi16_storeu_epi8'.
// Requires AVX512BW.
func MaskCvtsepi16Storeu8(base_addr uintptr, k Mmask8, a M128i)  {
	maskCvtsepi16Storeu8(uintptr(base_addr), uint8(k), [16]byte(a))
}

func maskCvtsepi16Storeu8(base_addr uintptr, k uint8, a [16]byte) 


// Cvtsepi3216: Convert packed 32-bit integers in 'a' to packed 16-bit integers
// with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			k := 16*j
//			dst[k+15:k] := Saturate_Int32_To_Int16(a[i+31:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSDW'. Intrinsic: '_mm_cvtsepi32_epi16'.
// Requires AVX512F.
func Cvtsepi3216(a M128i) M128i {
	return M128i(cvtsepi3216([16]byte(a)))
}

func cvtsepi3216(a [16]byte) [16]byte


// MaskCvtsepi3216: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_Int32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSDW'. Intrinsic: '_mm_mask_cvtsepi32_epi16'.
// Requires AVX512F.
func MaskCvtsepi3216(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtsepi3216([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtsepi3216(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtsepi3216: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_Int32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSDW'. Intrinsic: '_mm_maskz_cvtsepi32_epi16'.
// Requires AVX512F.
func MaskzCvtsepi3216(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtsepi3216(uint8(k), [16]byte(a)))
}

func maskzCvtsepi3216(k uint8, a [16]byte) [16]byte


// Cvtsepi328: Convert packed 32-bit integers in 'a' to packed 8-bit integers
// with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			k := 8*j
//			dst[k+7:k] := Saturate_Int32_To_Int8(a[i+31:i])
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVSDB'. Intrinsic: '_mm_cvtsepi32_epi8'.
// Requires AVX512F.
func Cvtsepi328(a M128i) M128i {
	return M128i(cvtsepi328([16]byte(a)))
}

func cvtsepi328(a [16]byte) [16]byte


// MaskCvtsepi328: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVSDB'. Intrinsic: '_mm_mask_cvtsepi32_epi8'.
// Requires AVX512F.
func MaskCvtsepi328(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtsepi328([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtsepi328(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtsepi328: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVSDB'. Intrinsic: '_mm_maskz_cvtsepi32_epi8'.
// Requires AVX512F.
func MaskzCvtsepi328(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtsepi328(uint8(k), [16]byte(a)))
}

func maskzCvtsepi328(k uint8, a [16]byte) [16]byte


// MaskCvtsepi32Storeu16: Convert packed 32-bit integers in 'a' to packed
// 16-bit integers with signed saturation, and store the active results (those
// with their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				MEM[base_addr+l+15:base_addr+l] := Saturate_Int32_To_Int16(a[i+31:i])
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSDW'. Intrinsic: '_mm_mask_cvtsepi32_storeu_epi16'.
// Requires AVX512F.
func MaskCvtsepi32Storeu16(base_addr uintptr, k Mmask8, a M128i)  {
	maskCvtsepi32Storeu16(uintptr(base_addr), uint8(k), [16]byte(a))
}

func maskCvtsepi32Storeu16(base_addr uintptr, k uint8, a [16]byte) 


// MaskCvtsepi32Storeu8: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the active results (those with
// their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Saturate_Int32_To_Int8(a[i+31:i])
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVSDB'. Intrinsic: '_mm_mask_cvtsepi32_storeu_epi8'.
// Requires AVX512F.
func MaskCvtsepi32Storeu8(base_addr uintptr, k Mmask8, a M128i)  {
	maskCvtsepi32Storeu8(uintptr(base_addr), uint8(k), [16]byte(a))
}

func maskCvtsepi32Storeu8(base_addr uintptr, k uint8, a [16]byte) 


// Cvtsepi6416: Convert packed 64-bit integers in 'a' to packed 16-bit integers
// with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			k := 16*j
//			dst[k+15:k] := Saturate_Int64_To_Int16(a[i+63:i])
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVSQW'. Intrinsic: '_mm_cvtsepi64_epi16'.
// Requires AVX512F.
func Cvtsepi6416(a M128i) M128i {
	return M128i(cvtsepi6416([16]byte(a)))
}

func cvtsepi6416(a [16]byte) [16]byte


// MaskCvtsepi6416: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_Int64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVSQW'. Intrinsic: '_mm_mask_cvtsepi64_epi16'.
// Requires AVX512F.
func MaskCvtsepi6416(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtsepi6416([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtsepi6416(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtsepi6416: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_Int64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVSQW'. Intrinsic: '_mm_maskz_cvtsepi64_epi16'.
// Requires AVX512F.
func MaskzCvtsepi6416(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtsepi6416(uint8(k), [16]byte(a)))
}

func maskzCvtsepi6416(k uint8, a [16]byte) [16]byte


// Cvtsepi6432: Convert packed 64-bit integers in 'a' to packed 32-bit integers
// with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			k := 32*j
//			dst[k+31:k] := Saturate_Int64_To_Int32(a[i+63:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSQD'. Intrinsic: '_mm_cvtsepi64_epi32'.
// Requires AVX512F.
func Cvtsepi6432(a M128i) M128i {
	return M128i(cvtsepi6432([16]byte(a)))
}

func cvtsepi6432(a [16]byte) [16]byte


// MaskCvtsepi6432: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Saturate_Int64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := src[l+31:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSQD'. Intrinsic: '_mm_mask_cvtsepi64_epi32'.
// Requires AVX512F.
func MaskCvtsepi6432(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtsepi6432([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtsepi6432(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtsepi6432: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Saturate_Int64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSQD'. Intrinsic: '_mm_maskz_cvtsepi64_epi32'.
// Requires AVX512F.
func MaskzCvtsepi6432(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtsepi6432(uint8(k), [16]byte(a)))
}

func maskzCvtsepi6432(k uint8, a [16]byte) [16]byte


// Cvtsepi648: Convert packed 64-bit integers in 'a' to packed 8-bit integers
// with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			k := 8*j
//			dst[k+7:k] := Saturate_Int64_To_Int8(a[i+63:i])
//		ENDFOR
//		dst[MAX:16] := 0
//
// Instruction: 'VPMOVSQB'. Intrinsic: '_mm_cvtsepi64_epi8'.
// Requires AVX512F.
func Cvtsepi648(a M128i) M128i {
	return M128i(cvtsepi648([16]byte(a)))
}

func cvtsepi648(a [16]byte) [16]byte


// MaskCvtsepi648: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:16] := 0
//
// Instruction: 'VPMOVSQB'. Intrinsic: '_mm_mask_cvtsepi64_epi8'.
// Requires AVX512F.
func MaskCvtsepi648(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtsepi648([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtsepi648(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtsepi648: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:16] := 0
//
// Instruction: 'VPMOVSQB'. Intrinsic: '_mm_maskz_cvtsepi64_epi8'.
// Requires AVX512F.
func MaskzCvtsepi648(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtsepi648(uint8(k), [16]byte(a)))
}

func maskzCvtsepi648(k uint8, a [16]byte) [16]byte


// MaskCvtsepi64Storeu16: Convert packed 64-bit integers in 'a' to packed
// 16-bit integers with signed saturation, and store the active results (those
// with their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				MEM[base_addr+l+15:base_addr+l] := Saturate_Int64_To_Int16(a[i+63:i])
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVSQW'. Intrinsic: '_mm_mask_cvtsepi64_storeu_epi16'.
// Requires AVX512F.
func MaskCvtsepi64Storeu16(base_addr uintptr, k Mmask8, a M128i)  {
	maskCvtsepi64Storeu16(uintptr(base_addr), uint8(k), [16]byte(a))
}

func maskCvtsepi64Storeu16(base_addr uintptr, k uint8, a [16]byte) 


// MaskCvtsepi64Storeu32: Convert packed 64-bit integers in 'a' to packed
// 32-bit integers with signed saturation, and store the active results (those
// with their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				MEM[base_addr+l+31:base_addr+l] := Saturate_Int64_To_Int32(a[i+63:i])
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSQD'. Intrinsic: '_mm_mask_cvtsepi64_storeu_epi32'.
// Requires AVX512F.
func MaskCvtsepi64Storeu32(base_addr uintptr, k Mmask8, a M128i)  {
	maskCvtsepi64Storeu32(uintptr(base_addr), uint8(k), [16]byte(a))
}

func maskCvtsepi64Storeu32(base_addr uintptr, k uint8, a [16]byte) 


// MaskCvtsepi64Storeu8: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the active results (those with
// their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Saturate_Int64_To_Int8(a[i+63:i])
//			FI
//		ENDFOR
//		dst[MAX:16] := 0
//
// Instruction: 'VPMOVSQB'. Intrinsic: '_mm_mask_cvtsepi64_storeu_epi8'.
// Requires AVX512F.
func MaskCvtsepi64Storeu8(base_addr uintptr, k Mmask8, a M128i)  {
	maskCvtsepi64Storeu8(uintptr(base_addr), uint8(k), [16]byte(a))
}

func maskCvtsepi64Storeu8(base_addr uintptr, k uint8, a [16]byte) 


// CvtshSs: Convert the half-precision (16-bit) floating-point value 'a' to a
// single-precision (32-bit) floating-point value, and store the result in
// 'dst'. 
//
//		dst[31:0] := Convert_FP16_To_FP32(a[15:0])
//
// Instruction: '...'. Intrinsic: '_cvtsh_ss'.
func CvtshSs(a uint16) float32 {
	return float32(cvtshSs(a))
}

func cvtshSs(a uint16) float32


// Cvtsi128Si32: Copy the lower 32-bit integer in 'a' to 'dst'. 
//
//		dst[31:0] := a[31:0]
//
// Instruction: 'MOVD'. Intrinsic: '_mm_cvtsi128_si32'.
// Requires SSE2.
func Cvtsi128Si32(a M128i) int {
	return int(cvtsi128Si32([16]byte(a)))
}

func cvtsi128Si32(a [16]byte) int


// Cvtsi128Si64: Copy the lower 64-bit integer in 'a' to 'dst'. 
//
//		dst[63:0] := a[63:0]
//
// Instruction: 'MOVQ'. Intrinsic: '_mm_cvtsi128_si64'.
// Requires SSE2.
func Cvtsi128Si64(a M128i) int64 {
	return int64(cvtsi128Si64([16]byte(a)))
}

func cvtsi128Si64(a [16]byte) int64


// Cvtsi128Si64x: Copy the lower 64-bit integer in 'a' to 'dst'. 
//
//		dst[63:0] := a[63:0]
//
// Instruction: 'MOVQ'. Intrinsic: '_mm_cvtsi128_si64x'.
// Requires SSE2.
func Cvtsi128Si64x(a M128i) int64 {
	return int64(cvtsi128Si64x([16]byte(a)))
}

func cvtsi128Si64x(a [16]byte) int64


// Cvtsi32Sd: Convert the 32-bit integer 'b' to a double-precision (64-bit)
// floating-point element, store the result in the lower element of 'dst', and
// copy the upper element from 'a' to the upper element of 'dst'. 
//
//		dst[63:0] := Convert_Int32_To_FP64(b[31:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'CVTSI2SD'. Intrinsic: '_mm_cvtsi32_sd'.
// Requires SSE2.
func Cvtsi32Sd(a M128d, b int) M128d {
	return M128d(cvtsi32Sd([2]float64(a), b))
}

func cvtsi32Sd(a [2]float64, b int) [2]float64


// Cvtsi32Si128: Copy 32-bit integer 'a' to the lower elements of 'dst', and
// zero the upper elements of 'dst'. 
//
//		dst[31:0] := a[31:0]
//		dst[127:32] := 0
//
// Instruction: 'MOVD'. Intrinsic: '_mm_cvtsi32_si128'.
// Requires SSE2.
func Cvtsi32Si128(a int) M128i {
	return M128i(cvtsi32Si128(a))
}

func cvtsi32Si128(a int) [16]byte


// Cvtsi32Ss: Convert the 32-bit integer 'b' to a single-precision (32-bit)
// floating-point element, store the result in the lower element of 'dst', and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'. 
//
//		dst[31:0] := Convert_Int32_To_FP32(b[31:0])
//		dst[127:32] := a[127:32]
//
// Instruction: 'CVTSI2SS'. Intrinsic: '_mm_cvtsi32_ss'.
// Requires SSE.
func Cvtsi32Ss(a M128, b int) M128 {
	return M128(cvtsi32Ss([4]float32(a), b))
}

func cvtsi32Ss(a [4]float32, b int) [4]float32


// Cvtsi64Sd: Convert the 64-bit integer 'b' to a double-precision (64-bit)
// floating-point element, store the result in the lower element of 'dst', and
// copy the upper element from 'a' to the upper element of 'dst'. 
//
//		dst[63:0] := Convert_Int64_To_FP64(b[63:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'CVTSI2SD'. Intrinsic: '_mm_cvtsi64_sd'.
// Requires SSE2.
func Cvtsi64Sd(a M128d, b int64) M128d {
	return M128d(cvtsi64Sd([2]float64(a), b))
}

func cvtsi64Sd(a [2]float64, b int64) [2]float64


// Cvtsi64Si128: Copy 64-bit integer 'a' to the lower element of 'dst', and
// zero the upper element. 
//
//		dst[63:0] := a[63:0]
//		dst[127:64] := 0
//
// Instruction: 'MOVQ'. Intrinsic: '_mm_cvtsi64_si128'.
// Requires SSE2.
func Cvtsi64Si128(a int64) M128i {
	return M128i(cvtsi64Si128(a))
}

func cvtsi64Si128(a int64) [16]byte


// Cvtsi64Ss: Convert the 64-bit integer 'b' to a single-precision (32-bit)
// floating-point element, store the result in the lower element of 'dst', and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'. 
//
//		dst[31:0] := Convert_Int64_To_FP32(b[63:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'CVTSI2SS'. Intrinsic: '_mm_cvtsi64_ss'.
// Requires SSE.
func Cvtsi64Ss(a M128, b int64) M128 {
	return M128(cvtsi64Ss([4]float32(a), b))
}

func cvtsi64Ss(a [4]float32, b int64) [4]float32


// Cvtsi64xSd: Convert the 64-bit integer 'b' to a double-precision (64-bit)
// floating-point element, store the result in the lower element of 'dst', and
// copy the upper element from 'a' to the upper element of 'dst'. 
//
//		dst[63:0] := Convert_Int64_To_FP64(b[63:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'CVTSI2SD'. Intrinsic: '_mm_cvtsi64x_sd'.
// Requires SSE2.
func Cvtsi64xSd(a M128d, b int64) M128d {
	return M128d(cvtsi64xSd([2]float64(a), b))
}

func cvtsi64xSd(a [2]float64, b int64) [2]float64


// Cvtsi64xSi128: Copy 64-bit integer 'a' to the lower element of 'dst', and
// zero the upper element. 
//
//		dst[63:0] := a[63:0]
//		dst[127:64] := 0
//
// Instruction: 'MOVQ'. Intrinsic: '_mm_cvtsi64x_si128'.
// Requires SSE2.
func Cvtsi64xSi128(a int64) M128i {
	return M128i(cvtsi64xSi128(a))
}

func cvtsi64xSi128(a int64) [16]byte


// CvtssF32: Copy the lower single-precision (32-bit) floating-point element of
// 'a' to 'dst'. 
//
//		dst[31:0] := a[31:0]
//
// Instruction: 'MOVSS'. Intrinsic: '_mm_cvtss_f32'.
// Requires SSE.
func CvtssF32(a M128) float32 {
	return float32(cvtssF32([4]float32(a)))
}

func cvtssF32(a [4]float32) float32


// CvtssI32: Convert the lower single-precision (32-bit) floating-point element
// in 'a' to a 32-bit integer, and store the result in 'dst'. 
//
//		dst[31:0] := Convert_FP32_To_Int32(a[31:0])
//
// Instruction: 'VCVTSS2SI'. Intrinsic: '_mm_cvtss_i32'.
// Requires AVX512F.
func CvtssI32(a M128) int {
	return int(cvtssI32([4]float32(a)))
}

func cvtssI32(a [4]float32) int


// CvtssI64: Convert the lower single-precision (32-bit) floating-point element
// in 'a' to a 64-bit integer, and store the result in 'dst'. 
//
//		dst[63:0] := Convert_FP32_To_Int64(a[31:0])
//
// Instruction: 'VCVTSS2SI'. Intrinsic: '_mm_cvtss_i64'.
// Requires AVX512F.
func CvtssI64(a M128) int64 {
	return int64(cvtssI64([4]float32(a)))
}

func cvtssI64(a [4]float32) int64


// CvtssSd: Convert the lower single-precision (32-bit) floating-point element
// in 'b' to a double-precision (64-bit) floating-point element, store the
// result in the lower element of 'dst', and copy the upper element from 'a' to
// the upper element of 'dst'. 
//
//		dst[63:0] := Convert_FP32_To_FP64(b[31:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:64] := 0
//
// Instruction: 'CVTSS2SD'. Intrinsic: '_mm_cvtss_sd'.
// Requires SSE2.
func CvtssSd(a M128d, b M128) M128d {
	return M128d(cvtssSd([2]float64(a), [4]float32(b)))
}

func cvtssSd(a [2]float64, b [4]float32) [2]float64


// MaskCvtssSd: Convert the lower single-precision (32-bit) floating-point
// element in 'b' to a double-precision (64-bit) floating-point element, store
// the result in the lower element of 'dst' using writemask 'k' (the element is
// copied from 'src' when mask bit 0 is not set), and copy the upper element
// from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := Convert_FP32_To_FP64(b[31:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTSS2SD'. Intrinsic: '_mm_mask_cvtss_sd'.
// Requires AVX512F.
func MaskCvtssSd(src M128d, k Mmask8, a M128d, b M128) M128d {
	return M128d(maskCvtssSd([2]float64(src), uint8(k), [2]float64(a), [4]float32(b)))
}

func maskCvtssSd(src [2]float64, k uint8, a [2]float64, b [4]float32) [2]float64


// MaskzCvtssSd: Convert the lower single-precision (32-bit) floating-point
// element in 'b' to a double-precision (64-bit) floating-point element, store
// the result in the lower element of 'dst' using zeromask 'k' (the element is
// zeroed out when mask bit 0 is not set), and copy the upper element from 'a'
// to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := Convert_FP32_To_FP64(b[31:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTSS2SD'. Intrinsic: '_mm_maskz_cvtss_sd'.
// Requires AVX512F.
func MaskzCvtssSd(k Mmask8, a M128d, b M128) M128d {
	return M128d(maskzCvtssSd(uint8(k), [2]float64(a), [4]float32(b)))
}

func maskzCvtssSd(k uint8, a [2]float64, b [4]float32) [2]float64


// CvtssSh: Convert the single-precision (32-bit) floating-point value 'a' to a
// half-precision (16-bit) floating-point value, and store the result in 'dst'. 
//
//		dst[15:0] := Convert_FP32_To_FP16(a[31:0])
//
// Instruction: '...'. Intrinsic: '_cvtss_sh'.
func CvtssSh(a float32, imm8 int) uint16 {
	return uint16(cvtssSh(a, imm8))
}

func cvtssSh(a float32, imm8 int) uint16


// CvtssSi32: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to a 32-bit integer, and store the result in 'dst'. 
//
//		dst[31:0] := Convert_FP32_To_Int32(a[31:0])
//
// Instruction: 'CVTSS2SI'. Intrinsic: '_mm_cvtss_si32'.
// Requires SSE.
func CvtssSi32(a M128) int {
	return int(cvtssSi32([4]float32(a)))
}

func cvtssSi32(a [4]float32) int


// CvtssSi64: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to a 64-bit integer, and store the result in 'dst'. 
//
//		dst[63:0] := Convert_FP32_To_Int64(a[31:0])
//
// Instruction: 'CVTSS2SI'. Intrinsic: '_mm_cvtss_si64'.
// Requires SSE.
func CvtssSi64(a M128) int64 {
	return int64(cvtssSi64([4]float32(a)))
}

func cvtssSi64(a [4]float32) int64


// CvtssU32: Convert the lower single-precision (32-bit) floating-point element
// in 'a' to an unsigned 32-bit integer, and store the result in 'dst'. 
//
//		dst[31:0] := Convert_FP32_To_UnsignedInt32(a[31:0])
//
// Instruction: 'VCVTSS2USI'. Intrinsic: '_mm_cvtss_u32'.
// Requires AVX512F.
func CvtssU32(a M128) uint32 {
	return uint32(cvtssU32([4]float32(a)))
}

func cvtssU32(a [4]float32) uint32


// CvtssU64: Convert the lower single-precision (32-bit) floating-point element
// in 'a' to an unsigned 64-bit integer, and store the result in 'dst'. 
//
//		dst[63:0] := Convert_FP32_To_UnsignedInt64(a[31:0])
//
// Instruction: 'VCVTSS2USI'. Intrinsic: '_mm_cvtss_u64'.
// Requires AVX512F.
func CvtssU64(a M128) uint64 {
	return uint64(cvtssU64([4]float32(a)))
}

func cvtssU64(a [4]float32) uint64


// CvttPs2pi: Convert packed single-precision (32-bit) floating-point elements
// in 'a' to packed 32-bit integers with truncation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 1
//			i := 32*j
//			dst[i+31:i] := Convert_FP32_To_Int32_Truncate(a[i+31:i])
//		ENDFOR
//
// Instruction: 'CVTTPS2PI'. Intrinsic: '_mm_cvtt_ps2pi'.
// Requires SSE.
func CvttPs2pi(a M128) M64 {
	return M64(cvttPs2pi([4]float32(a)))
}

func cvttPs2pi(a [4]float32) M64


// CvttRoundsdI32: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to a 32-bit integer with truncation, and store the result in
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_FP64_To_Int32_Truncate(a[63:0])
//
// Instruction: 'VCVTTSD2SI'. Intrinsic: '_mm_cvtt_roundsd_i32'.
// Requires AVX512F.
func CvttRoundsdI32(a M128d, rounding int) int {
	return int(cvttRoundsdI32([2]float64(a), rounding))
}

func cvttRoundsdI32(a [2]float64, rounding int) int


// CvttRoundsdI64: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to a 64-bit integer with truncation, and store the result in
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_FP64_To_Int64_Truncate(a[63:0])
//
// Instruction: 'VCVTTSD2SI'. Intrinsic: '_mm_cvtt_roundsd_i64'.
// Requires AVX512F.
func CvttRoundsdI64(a M128d, rounding int) int64 {
	return int64(cvttRoundsdI64([2]float64(a), rounding))
}

func cvttRoundsdI64(a [2]float64, rounding int) int64


// CvttRoundsdSi32: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to a 32-bit integer with truncation, and store the result in
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_FP64_To_Int32_Truncate(a[63:0])
//
// Instruction: 'VCVTTSD2SI'. Intrinsic: '_mm_cvtt_roundsd_si32'.
// Requires AVX512F.
func CvttRoundsdSi32(a M128d, rounding int) int {
	return int(cvttRoundsdSi32([2]float64(a), rounding))
}

func cvttRoundsdSi32(a [2]float64, rounding int) int


// CvttRoundsdSi64: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to a 64-bit integer with truncation, and store the result in
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_FP64_To_Int64_Truncate(a[63:0])
//
// Instruction: 'VCVTTSD2SI'. Intrinsic: '_mm_cvtt_roundsd_si64'.
// Requires AVX512F.
func CvttRoundsdSi64(a M128d, rounding int) int64 {
	return int64(cvttRoundsdSi64([2]float64(a), rounding))
}

func cvttRoundsdSi64(a [2]float64, rounding int) int64


// CvttRoundsdU32: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to an unsigned 32-bit integer with truncation, and store the
// result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_FP64_To_UnsignedInt32_Truncate(a[63:0])
//
// Instruction: 'VCVTTSD2USI'. Intrinsic: '_mm_cvtt_roundsd_u32'.
// Requires AVX512F.
func CvttRoundsdU32(a M128d, rounding int) uint32 {
	return uint32(cvttRoundsdU32([2]float64(a), rounding))
}

func cvttRoundsdU32(a [2]float64, rounding int) uint32


// CvttRoundsdU64: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to an unsigned 64-bit integer with truncation, and store the
// result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_FP64_To_UnsignedInt64_Truncate(a[63:0])
//
// Instruction: 'VCVTTSD2USI'. Intrinsic: '_mm_cvtt_roundsd_u64'.
// Requires AVX512F.
func CvttRoundsdU64(a M128d, rounding int) uint64 {
	return uint64(cvttRoundsdU64([2]float64(a), rounding))
}

func cvttRoundsdU64(a [2]float64, rounding int) uint64


// CvttRoundssI32: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to a 32-bit integer with truncation, and store the result in
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_FP32_To_Int32_Truncate(a[31:0])
//
// Instruction: 'VCVTTSS2SI'. Intrinsic: '_mm_cvtt_roundss_i32'.
// Requires AVX512F.
func CvttRoundssI32(a M128, rounding int) int {
	return int(cvttRoundssI32([4]float32(a), rounding))
}

func cvttRoundssI32(a [4]float32, rounding int) int


// CvttRoundssI64: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to a 64-bit integer with truncation, and store the result in
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_FP32_To_Int64_Truncate(a[31:0])
//
// Instruction: 'VCVTTSS2SI'. Intrinsic: '_mm_cvtt_roundss_i64'.
// Requires AVX512F.
func CvttRoundssI64(a M128, rounding int) int64 {
	return int64(cvttRoundssI64([4]float32(a), rounding))
}

func cvttRoundssI64(a [4]float32, rounding int) int64


// CvttRoundssSi32: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to a 32-bit integer with truncation, and store the result in
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_FP32_To_Int32_Truncate(a[31:0])
//
// Instruction: 'VCVTTSS2SI'. Intrinsic: '_mm_cvtt_roundss_si32'.
// Requires AVX512F.
func CvttRoundssSi32(a M128, rounding int) int {
	return int(cvttRoundssSi32([4]float32(a), rounding))
}

func cvttRoundssSi32(a [4]float32, rounding int) int


// CvttRoundssSi64: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to a 64-bit integer with truncation, and store the result in
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_FP32_To_Int64_Truncate(a[31:0])
//
// Instruction: 'VCVTTSS2SI'. Intrinsic: '_mm_cvtt_roundss_si64'.
// Requires AVX512F.
func CvttRoundssSi64(a M128, rounding int) int64 {
	return int64(cvttRoundssSi64([4]float32(a), rounding))
}

func cvttRoundssSi64(a [4]float32, rounding int) int64


// CvttRoundssU32: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to an unsigned 32-bit integer with truncation, and store the
// result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := Convert_FP32_To_UnsignedInt32_Truncate(a[31:0])
//
// Instruction: 'VCVTTSS2USI'. Intrinsic: '_mm_cvtt_roundss_u32'.
// Requires AVX512F.
func CvttRoundssU32(a M128, rounding int) uint32 {
	return uint32(cvttRoundssU32([4]float32(a), rounding))
}

func cvttRoundssU32(a [4]float32, rounding int) uint32


// CvttRoundssU64: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to an unsigned 64-bit integer with truncation, and store the
// result in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := Convert_FP32_To_UnsignedInt64_Truncate(a[31:0])
//
// Instruction: 'VCVTTSS2USI'. Intrinsic: '_mm_cvtt_roundss_u64'.
// Requires AVX512F.
func CvttRoundssU64(a M128, rounding int) uint64 {
	return uint64(cvttRoundssU64([4]float32(a), rounding))
}

func cvttRoundssU64(a [4]float32, rounding int) uint64


// CvttSs2si: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to a 32-bit integer with truncation, and store the result in
// 'dst'. 
//
//		dst[31:0] := Convert_FP32_To_Int32_Truncate(a[31:0])
//
// Instruction: 'CVTTSS2SI'. Intrinsic: '_mm_cvtt_ss2si'.
// Requires SSE.
func CvttSs2si(a M128) int {
	return int(cvttSs2si([4]float32(a)))
}

func cvttSs2si(a [4]float32) int


// Cvttpd32: Convert packed double-precision (64-bit) floating-point elements
// in 'a' to packed 32-bit integers with truncation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 1
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_Int32_Truncate(a[k+63:k])
//		ENDFOR
//
// Instruction: 'CVTTPD2DQ'. Intrinsic: '_mm_cvttpd_epi32'.
// Requires SSE2.
func Cvttpd32(a M128d) M128i {
	return M128i(cvttpd32([2]float64(a)))
}

func cvttpd32(a [2]float64) [16]byte


// MaskCvttpd32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_Int32_Truncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTTPD2DQ'. Intrinsic: '_mm_mask_cvttpd_epi32'.
// Requires AVX512F.
func MaskCvttpd32(src M128i, k Mmask8, a M128d) M128i {
	return M128i(maskCvttpd32([16]byte(src), uint8(k), [2]float64(a)))
}

func maskCvttpd32(src [16]byte, k uint8, a [2]float64) [16]byte


// MaskzCvttpd32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_Int32_Truncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTTPD2DQ'. Intrinsic: '_mm_maskz_cvttpd_epi32'.
// Requires AVX512F.
func MaskzCvttpd32(k Mmask8, a M128d) M128i {
	return M128i(maskzCvttpd32(uint8(k), [2]float64(a)))
}

func maskzCvttpd32(k uint8, a [2]float64) [16]byte


// Cvttpd64: Convert packed double-precision (64-bit) floating-point elements
// in 'a' to packed 64-bit integers with truncation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := Convert_FP64_To_Int64_Truncate(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPD2QQ'. Intrinsic: '_mm_cvttpd_epi64'.
// Requires AVX512DQ.
func Cvttpd64(a M128d) M128i {
	return M128i(cvttpd64([2]float64(a)))
}

func cvttpd64(a [2]float64) [16]byte


// MaskCvttpd64: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 64-bit integers with truncation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_FP64_To_Int64_Truncate(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPD2QQ'. Intrinsic: '_mm_mask_cvttpd_epi64'.
// Requires AVX512DQ.
func MaskCvttpd64(src M128i, k Mmask8, a M128d) M128i {
	return M128i(maskCvttpd64([16]byte(src), uint8(k), [2]float64(a)))
}

func maskCvttpd64(src [16]byte, k uint8, a [2]float64) [16]byte


// MaskzCvttpd64: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 64-bit integers with truncation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_FP64_To_Int64_Truncate(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPD2QQ'. Intrinsic: '_mm_maskz_cvttpd_epi64'.
// Requires AVX512DQ.
func MaskzCvttpd64(k Mmask8, a M128d) M128i {
	return M128i(maskzCvttpd64(uint8(k), [2]float64(a)))
}

func maskzCvttpd64(k uint8, a [2]float64) [16]byte


// CvttpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[k+63:k])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTTPD2UDQ'. Intrinsic: '_mm_cvttpd_epu32'.
// Requires AVX512F.
func CvttpdEpu32(a M128d) M128i {
	return M128i(cvttpdEpu32([2]float64(a)))
}

func cvttpdEpu32(a [2]float64) [16]byte


// MaskCvttpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTTPD2UDQ'. Intrinsic: '_mm_mask_cvttpd_epu32'.
// Requires AVX512F.
func MaskCvttpdEpu32(src M128i, k Mmask8, a M128d) M128i {
	return M128i(maskCvttpdEpu32([16]byte(src), uint8(k), [2]float64(a)))
}

func maskCvttpdEpu32(src [16]byte, k uint8, a [2]float64) [16]byte


// MaskzCvttpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VCVTTPD2UDQ'. Intrinsic: '_mm_maskz_cvttpd_epu32'.
// Requires AVX512F.
func MaskzCvttpdEpu32(k Mmask8, a M128d) M128i {
	return M128i(maskzCvttpdEpu32(uint8(k), [2]float64(a)))
}

func maskzCvttpdEpu32(k uint8, a [2]float64) [16]byte


// CvttpdEpu64: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 64-bit integers with truncation, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := Convert_FP64_To_UnsignedInt64_Truncate(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPD2UQQ'. Intrinsic: '_mm_cvttpd_epu64'.
// Requires AVX512DQ.
func CvttpdEpu64(a M128d) M128i {
	return M128i(cvttpdEpu64([2]float64(a)))
}

func cvttpdEpu64(a [2]float64) [16]byte


// MaskCvttpdEpu64: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 64-bit integers with truncation, and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_FP64_To_UnsignedInt64_Truncate(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPD2UQQ'. Intrinsic: '_mm_mask_cvttpd_epu64'.
// Requires AVX512DQ.
func MaskCvttpdEpu64(src M128i, k Mmask8, a M128d) M128i {
	return M128i(maskCvttpdEpu64([16]byte(src), uint8(k), [2]float64(a)))
}

func maskCvttpdEpu64(src [16]byte, k uint8, a [2]float64) [16]byte


// MaskzCvttpdEpu64: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 64-bit integers with truncation, and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_FP64_To_UnsignedInt64_Truncate(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPD2UQQ'. Intrinsic: '_mm_maskz_cvttpd_epu64'.
// Requires AVX512DQ.
func MaskzCvttpdEpu64(k Mmask8, a M128d) M128i {
	return M128i(maskzCvttpdEpu64(uint8(k), [2]float64(a)))
}

func maskzCvttpdEpu64(k uint8, a [2]float64) [16]byte


// Cvttps32: Convert packed single-precision (32-bit) floating-point elements
// in 'a' to packed 32-bit integers with truncation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			dst[i+31:i] := Convert_FP32_To_Int32_Truncate(a[i+31:i])
//		ENDFOR
//
// Instruction: 'CVTTPS2DQ'. Intrinsic: '_mm_cvttps_epi32'.
// Requires SSE2.
func Cvttps32(a M128) M128i {
	return M128i(cvttps32([4]float32(a)))
}

func cvttps32(a [4]float32) [16]byte


// MaskCvttps32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_Int32_Truncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPS2DQ'. Intrinsic: '_mm_mask_cvttps_epi32'.
// Requires AVX512F.
func MaskCvttps32(src M128i, k Mmask8, a M128) M128i {
	return M128i(maskCvttps32([16]byte(src), uint8(k), [4]float32(a)))
}

func maskCvttps32(src [16]byte, k uint8, a [4]float32) [16]byte


// MaskzCvttps32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*i
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_IntegerTruncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPS2DQ'. Intrinsic: '_mm_maskz_cvttps_epi32'.
// Requires AVX512F.
func MaskzCvttps32(k Mmask8, a M128) M128i {
	return M128i(maskzCvttps32(uint8(k), [4]float32(a)))
}

func maskzCvttps32(k uint8, a [4]float32) [16]byte


// Cvttps64: Convert packed single-precision (32-bit) floating-point elements
// in 'a' to packed 64-bit integers with truncation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			l := j*32
//			dst[i+63:i] := Convert_FP32_To_Int64_Truncate(a[l+31:l])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPS2QQ'. Intrinsic: '_mm_cvttps_epi64'.
// Requires AVX512DQ.
func Cvttps64(a M128) M128i {
	return M128i(cvttps64([4]float32(a)))
}

func cvttps64(a [4]float32) [16]byte


// MaskCvttps64: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 64-bit integers with truncation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_Int64_Truncate(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPS2QQ'. Intrinsic: '_mm_mask_cvttps_epi64'.
// Requires AVX512DQ.
func MaskCvttps64(src M128i, k Mmask8, a M128) M128i {
	return M128i(maskCvttps64([16]byte(src), uint8(k), [4]float32(a)))
}

func maskCvttps64(src [16]byte, k uint8, a [4]float32) [16]byte


// MaskzCvttps64: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 64-bit integers with truncation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_Int64_Truncate(a[l+31:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPS2QQ'. Intrinsic: '_mm_maskz_cvttps_epi64'.
// Requires AVX512DQ.
func MaskzCvttps64(k Mmask8, a M128) M128i {
	return M128i(maskzCvttps64(uint8(k), [4]float32(a)))
}

func maskzCvttps64(k uint8, a [4]float32) [16]byte


// CvttpsEpu32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			dst[i+31:i] := Convert_FP32_To_UnsignedInt32_Truncate(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPS2UDQ'. Intrinsic: '_mm_cvttps_epu32'.
// Requires AVX512F.
func CvttpsEpu32(a M128) M128i {
	return M128i(cvttpsEpu32([4]float32(a)))
}

func cvttpsEpu32(a [4]float32) [16]byte


// MaskCvttpsEpu32: Convert packed double-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPS2UDQ'. Intrinsic: '_mm_mask_cvttps_epu32'.
// Requires AVX512F.
func MaskCvttpsEpu32(src M128i, k Mmask8, a M128) M128i {
	return M128i(maskCvttpsEpu32([16]byte(src), uint8(k), [4]float32(a)))
}

func maskCvttpsEpu32(src [16]byte, k uint8, a [4]float32) [16]byte


// MaskzCvttpsEpu32: Convert packed double-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPS2UDQ'. Intrinsic: '_mm_maskz_cvttps_epu32'.
// Requires AVX512F.
func MaskzCvttpsEpu32(k Mmask8, a M128) M128i {
	return M128i(maskzCvttpsEpu32(uint8(k), [4]float32(a)))
}

func maskzCvttpsEpu32(k uint8, a [4]float32) [16]byte


// CvttpsEpu64: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 64-bit integers with truncation, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			l := j*32
//			dst[i+63:i] := Convert_FP32_To_UnsignedInt64_Truncate(a[l+31:l])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPS2UQQ'. Intrinsic: '_mm_cvttps_epu64'.
// Requires AVX512DQ.
func CvttpsEpu64(a M128) M128i {
	return M128i(cvttpsEpu64([4]float32(a)))
}

func cvttpsEpu64(a [4]float32) [16]byte


// MaskCvttpsEpu64: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 64-bit integers with truncation, and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_UnsignedInt64_Truncate(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPS2UQQ'. Intrinsic: '_mm_mask_cvttps_epu64'.
// Requires AVX512DQ.
func MaskCvttpsEpu64(src M128i, k Mmask8, a M128) M128i {
	return M128i(maskCvttpsEpu64([16]byte(src), uint8(k), [4]float32(a)))
}

func maskCvttpsEpu64(src [16]byte, k uint8, a [4]float32) [16]byte


// MaskzCvttpsEpu64: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 64-bit integers with truncation, and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_UnsignedInt64_Truncate(a[l+31:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPS2UQQ'. Intrinsic: '_mm_maskz_cvttps_epu64'.
// Requires AVX512DQ.
func MaskzCvttpsEpu64(k Mmask8, a M128) M128i {
	return M128i(maskzCvttpsEpu64(uint8(k), [4]float32(a)))
}

func maskzCvttpsEpu64(k uint8, a [4]float32) [16]byte


// CvttsdI32: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to a 32-bit integer with truncation, and store the result in
// 'dst'. 
//
//		dst[31:0] := Convert_FP64_To_Int32_Truncate(a[63:0])
//
// Instruction: 'VCVTTSD2SI'. Intrinsic: '_mm_cvttsd_i32'.
// Requires AVX512F.
func CvttsdI32(a M128d) int {
	return int(cvttsdI32([2]float64(a)))
}

func cvttsdI32(a [2]float64) int


// CvttsdI64: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to a 64-bit integer with truncation, and store the result in
// 'dst'. 
//
//		dst[63:0] := Convert_FP64_To_Int64_Truncate(a[63:0])
//
// Instruction: 'VCVTTSD2SI'. Intrinsic: '_mm_cvttsd_i64'.
// Requires AVX512F.
func CvttsdI64(a M128d) int64 {
	return int64(cvttsdI64([2]float64(a)))
}

func cvttsdI64(a [2]float64) int64


// CvttsdSi32: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to a 32-bit integer with truncation, and store the result in
// 'dst'. 
//
//		dst[31:0] := Convert_FP64_To_Int32_Truncate(a[63:0])
//
// Instruction: 'CVTTSD2SI'. Intrinsic: '_mm_cvttsd_si32'.
// Requires SSE2.
func CvttsdSi32(a M128d) int {
	return int(cvttsdSi32([2]float64(a)))
}

func cvttsdSi32(a [2]float64) int


// CvttsdSi64: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to a 64-bit integer with truncation, and store the result in
// 'dst'. 
//
//		dst[63:0] := Convert_FP64_To_Int64_Truncate(a[63:0])
//
// Instruction: 'CVTTSD2SI'. Intrinsic: '_mm_cvttsd_si64'.
// Requires SSE2.
func CvttsdSi64(a M128d) int64 {
	return int64(cvttsdSi64([2]float64(a)))
}

func cvttsdSi64(a [2]float64) int64


// CvttsdSi64x: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to a 64-bit integer with truncation, and store the result in
// 'dst'. 
//
//		dst[63:0] := Convert_FP64_To_Int64_Truncate(a[63:0])
//
// Instruction: 'CVTTSD2SI'. Intrinsic: '_mm_cvttsd_si64x'.
// Requires SSE2.
func CvttsdSi64x(a M128d) int64 {
	return int64(cvttsdSi64x([2]float64(a)))
}

func cvttsdSi64x(a [2]float64) int64


// CvttsdU32: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to an unsigned 32-bit integer with truncation, and store the
// result in 'dst'. 
//
//		dst[31:0] := Convert_FP64_To_UnsignedInt32_Truncate(a[63:0])
//
// Instruction: 'VCVTTSD2USI'. Intrinsic: '_mm_cvttsd_u32'.
// Requires AVX512F.
func CvttsdU32(a M128d) uint32 {
	return uint32(cvttsdU32([2]float64(a)))
}

func cvttsdU32(a [2]float64) uint32


// CvttsdU64: Convert the lower double-precision (64-bit) floating-point
// element in 'a' to an unsigned 64-bit integer with truncation, and store the
// result in 'dst'. 
//
//		dst[63:0] := Convert_FP64_To_UnsignedInt64_Truncate(a[63:0])
//
// Instruction: 'VCVTTSD2USI'. Intrinsic: '_mm_cvttsd_u64'.
// Requires AVX512F.
func CvttsdU64(a M128d) uint64 {
	return uint64(cvttsdU64([2]float64(a)))
}

func cvttsdU64(a [2]float64) uint64


// CvttssI32: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to a 32-bit integer with truncation, and store the result in
// 'dst'. 
//
//		dst[31:0] := Convert_FP32_To_Int32_Truncate(a[31:0])
//
// Instruction: 'VCVTTSS2SI'. Intrinsic: '_mm_cvttss_i32'.
// Requires AVX512F.
func CvttssI32(a M128) int {
	return int(cvttssI32([4]float32(a)))
}

func cvttssI32(a [4]float32) int


// CvttssI64: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to a 64-bit integer with truncation, and store the result in
// 'dst'. 
//
//		dst[63:0] := Convert_FP32_To_Int64_Truncate(a[31:0])
//
// Instruction: 'VCVTTSS2SI'. Intrinsic: '_mm_cvttss_i64'.
// Requires AVX512F.
func CvttssI64(a M128) int64 {
	return int64(cvttssI64([4]float32(a)))
}

func cvttssI64(a [4]float32) int64


// CvttssSi32: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to a 32-bit integer with truncation, and store the result in
// 'dst'. 
//
//		dst[31:0] := Convert_FP32_To_Int32_Truncate(a[31:0])
//
// Instruction: 'CVTTSS2SI'. Intrinsic: '_mm_cvttss_si32'.
// Requires SSE.
func CvttssSi32(a M128) int {
	return int(cvttssSi32([4]float32(a)))
}

func cvttssSi32(a [4]float32) int


// CvttssSi64: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to a 64-bit integer with truncation, and store the result in
// 'dst'. 
//
//		dst[63:0] := Convert_FP64_To_Int32_Truncate(a[31:0])
//
// Instruction: 'CVTTSS2SI'. Intrinsic: '_mm_cvttss_si64'.
// Requires SSE.
func CvttssSi64(a M128) int64 {
	return int64(cvttssSi64([4]float32(a)))
}

func cvttssSi64(a [4]float32) int64


// CvttssU32: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to an unsigned 32-bit integer with truncation, and store the
// result in 'dst'. 
//
//		dst[31:0] := Convert_FP32_To_UnsignedInt32_Truncate(a[31:0])
//
// Instruction: 'VCVTTSS2USI'. Intrinsic: '_mm_cvttss_u32'.
// Requires AVX512F.
func CvttssU32(a M128) uint32 {
	return uint32(cvttssU32([4]float32(a)))
}

func cvttssU32(a [4]float32) uint32


// CvttssU64: Convert the lower single-precision (32-bit) floating-point
// element in 'a' to an unsigned 64-bit integer with truncation, and store the
// result in 'dst'. 
//
//		dst[63:0] := Convert_FP32_To_UnsignedInt64_Truncate(a[31:0])
//
// Instruction: 'VCVTTSS2USI'. Intrinsic: '_mm_cvttss_u64'.
// Requires AVX512F.
func CvttssU64(a M128) uint64 {
	return uint64(cvttssU64([4]float32(a)))
}

func cvttssU64(a [4]float32) uint64


// Cvtu32Sd: Convert the unsigned 32-bit integer 'b' to a double-precision
// (64-bit) floating-point element, store the result in the lower element of
// 'dst', and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		dst[63:0] := Convert_UnsignedInt32_To_FP64(b[31:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTUSI2SD'. Intrinsic: '_mm_cvtu32_sd'.
// Requires AVX512F.
func Cvtu32Sd(a M128d, b uint32) M128d {
	return M128d(cvtu32Sd([2]float64(a), b))
}

func cvtu32Sd(a [2]float64, b uint32) [2]float64


// Cvtu32Ss: Convert the unsigned 32-bit integer 'b' to a single-precision
// (32-bit) floating-point element, store the result in the lower element of
// 'dst', and copy the upper 3 packed elements from 'a' to the upper elements
// of 'dst'. 
//
//		dst[31:0] := Convert_UnsignedInt32_To_FP32(b[31:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTUSI2SS'. Intrinsic: '_mm_cvtu32_ss'.
// Requires AVX512F.
func Cvtu32Ss(a M128, b uint32) M128 {
	return M128(cvtu32Ss([4]float32(a), b))
}

func cvtu32Ss(a [4]float32, b uint32) [4]float32


// Cvtu64Sd: Convert the unsigned 64-bit integer 'b' to a double-precision
// (64-bit) floating-point element, store the result in the lower element of
// 'dst', and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		dst[63:0] := Convert_UnsignedInt64_To_FP64(b[63:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTUSI2SD'. Intrinsic: '_mm_cvtu64_sd'.
// Requires AVX512F.
func Cvtu64Sd(a M128d, b uint64) M128d {
	return M128d(cvtu64Sd([2]float64(a), b))
}

func cvtu64Sd(a [2]float64, b uint64) [2]float64


// Cvtu64Ss: Convert the unsigned 64-bit integer 'b' to a single-precision
// (32-bit) floating-point element, store the result in the lower element of
// 'dst', and copy the upper 3 packed elements from 'a' to the upper elements
// of 'dst'. 
//
//		dst[31:0] := Convert_UnsignedInt64_To_FP32(b[63:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTUSI2SS'. Intrinsic: '_mm_cvtu64_ss'.
// Requires AVX512F.
func Cvtu64Ss(a M128, b uint64) M128 {
	return M128(cvtu64Ss([4]float32(a), b))
}

func cvtu64Ss(a [4]float32, b uint64) [4]float32


// Cvtusepi168: Convert packed unsigned 16-bit integers in 'a' to packed
// unsigned 8-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 7
//			i := 16*j
//			l := 8*j
//			dst[l+7:l] := Saturate_UnsignedInt16_To_Int8(a[i+15:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSWB'. Intrinsic: '_mm_cvtusepi16_epi8'.
// Requires AVX512BW.
func Cvtusepi168(a M128i) M128i {
	return M128i(cvtusepi168([16]byte(a)))
}

func cvtusepi168(a [16]byte) [16]byte


// MaskCvtusepi168: Convert packed unsigned 16-bit integers in 'a' to packed
// unsigned 8-bit integers with unsigned saturation, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 16*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt16_To_Int8(a[i+15:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSWB'. Intrinsic: '_mm_mask_cvtusepi16_epi8'.
// Requires AVX512BW.
func MaskCvtusepi168(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtusepi168([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtusepi168(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtusepi168: Convert packed unsigned 16-bit integers in 'a' to packed
// unsigned 8-bit integers with unsigned saturation, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 16*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt16_To_Int8(a[i+15:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSWB'. Intrinsic: '_mm_maskz_cvtusepi16_epi8'.
// Requires AVX512BW.
func MaskzCvtusepi168(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtusepi168(uint8(k), [16]byte(a)))
}

func maskzCvtusepi168(k uint8, a [16]byte) [16]byte


// MaskCvtusepi16Storeu8: Convert packed unsigned 16-bit integers in 'a' to
// packed unsigned 8-bit integers with unsigned saturation, and store the
// active results (those with their respective bit set in writemask 'k') to
// unaligned memory at 'base_addr'. 
//
//		FOR j := 0 to 7
//			i := 16*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Saturate_UnsignedInt16_To_Int8(a[i+15:i])
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSWB'. Intrinsic: '_mm_mask_cvtusepi16_storeu_epi8'.
// Requires AVX512BW.
func MaskCvtusepi16Storeu8(base_addr uintptr, k Mmask8, a M128i)  {
	maskCvtusepi16Storeu8(uintptr(base_addr), uint8(k), [16]byte(a))
}

func maskCvtusepi16Storeu8(base_addr uintptr, k uint8, a [16]byte) 


// Cvtusepi3216: Convert packed unsigned 32-bit integers in 'a' to packed
// unsigned 16-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			k := 16*j
//			dst[k+15:k] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSDW'. Intrinsic: '_mm_cvtusepi32_epi16'.
// Requires AVX512F.
func Cvtusepi3216(a M128i) M128i {
	return M128i(cvtusepi3216([16]byte(a)))
}

func cvtusepi3216(a [16]byte) [16]byte


// MaskCvtusepi3216: Convert packed unsigned 32-bit integers in 'a' to packed
// unsigned 16-bit integers with unsigned saturation, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSDW'. Intrinsic: '_mm_mask_cvtusepi32_epi16'.
// Requires AVX512F.
func MaskCvtusepi3216(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtusepi3216([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtusepi3216(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtusepi3216: Convert packed unsigned 32-bit integers in 'a' to packed
// unsigned 16-bit integers with unsigned saturation, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSDW'. Intrinsic: '_mm_maskz_cvtusepi32_epi16'.
// Requires AVX512F.
func MaskzCvtusepi3216(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtusepi3216(uint8(k), [16]byte(a)))
}

func maskzCvtusepi3216(k uint8, a [16]byte) [16]byte


// Cvtusepi328: Convert packed unsigned 32-bit integers in 'a' to packed
// unsigned 8-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			k := 8*j
//			dst[k+7:k] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVUSDB'. Intrinsic: '_mm_cvtusepi32_epi8'.
// Requires AVX512F.
func Cvtusepi328(a M128i) M128i {
	return M128i(cvtusepi328([16]byte(a)))
}

func cvtusepi328(a [16]byte) [16]byte


// MaskCvtusepi328: Convert packed unsigned 32-bit integers in 'a' to packed
// unsigned 8-bit integers with unsigned saturation, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVUSDB'. Intrinsic: '_mm_mask_cvtusepi32_epi8'.
// Requires AVX512F.
func MaskCvtusepi328(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtusepi328([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtusepi328(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtusepi328: Convert packed unsigned 32-bit integers in 'a' to packed
// unsigned 8-bit integers with unsigned saturation, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVUSDB'. Intrinsic: '_mm_maskz_cvtusepi32_epi8'.
// Requires AVX512F.
func MaskzCvtusepi328(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtusepi328(uint8(k), [16]byte(a)))
}

func maskzCvtusepi328(k uint8, a [16]byte) [16]byte


// MaskCvtusepi32Storeu16: Convert packed unsigned 32-bit integers in 'a' to
// packed unsigned 16-bit integers with unsigned saturation, and store the
// active results (those with their respective bit set in writemask 'k') to
// unaligned memory at 'base_addr'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				MEM[base_addr+l+15:base_addr+l] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSDW'. Intrinsic: '_mm_mask_cvtusepi32_storeu_epi16'.
// Requires AVX512F.
func MaskCvtusepi32Storeu16(base_addr uintptr, k Mmask8, a M128i)  {
	maskCvtusepi32Storeu16(uintptr(base_addr), uint8(k), [16]byte(a))
}

func maskCvtusepi32Storeu16(base_addr uintptr, k uint8, a [16]byte) 


// MaskCvtusepi32Storeu8: Convert packed unsigned 32-bit integers in 'a' to
// packed unsigned 8-bit integers with unsigned saturation, and store the
// active results (those with their respective bit set in writemask 'k') to
// unaligned memory at 'base_addr'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVUSDB'. Intrinsic: '_mm_mask_cvtusepi32_storeu_epi8'.
// Requires AVX512F.
func MaskCvtusepi32Storeu8(base_addr uintptr, k Mmask8, a M128i)  {
	maskCvtusepi32Storeu8(uintptr(base_addr), uint8(k), [16]byte(a))
}

func maskCvtusepi32Storeu8(base_addr uintptr, k uint8, a [16]byte) 


// Cvtusepi6416: Convert packed unsigned 64-bit integers in 'a' to packed
// unsigned 16-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			k := 16*j
//			dst[k+15:k] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVUSQW'. Intrinsic: '_mm_cvtusepi64_epi16'.
// Requires AVX512F.
func Cvtusepi6416(a M128i) M128i {
	return M128i(cvtusepi6416([16]byte(a)))
}

func cvtusepi6416(a [16]byte) [16]byte


// MaskCvtusepi6416: Convert packed unsigned 64-bit integers in 'a' to packed
// unsigned 16-bit integers with unsigned saturation, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVUSQW'. Intrinsic: '_mm_mask_cvtusepi64_epi16'.
// Requires AVX512F.
func MaskCvtusepi6416(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtusepi6416([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtusepi6416(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtusepi6416: Convert packed unsigned 64-bit integers in 'a' to packed
// unsigned 16-bit integers with unsigned saturation, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVUSQW'. Intrinsic: '_mm_maskz_cvtusepi64_epi16'.
// Requires AVX512F.
func MaskzCvtusepi6416(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtusepi6416(uint8(k), [16]byte(a)))
}

func maskzCvtusepi6416(k uint8, a [16]byte) [16]byte


// Cvtusepi6432: Convert packed unsigned 64-bit integers in 'a' to packed
// unsigned 32-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			k := 32*j
//			dst[k+31:k] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSQD'. Intrinsic: '_mm_cvtusepi64_epi32'.
// Requires AVX512F.
func Cvtusepi6432(a M128i) M128i {
	return M128i(cvtusepi6432([16]byte(a)))
}

func cvtusepi6432(a [16]byte) [16]byte


// MaskCvtusepi6432: Convert packed unsigned 64-bit integers in 'a' to packed
// unsigned 32-bit integers with unsigned saturation, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := src[l+31:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSQD'. Intrinsic: '_mm_mask_cvtusepi64_epi32'.
// Requires AVX512F.
func MaskCvtusepi6432(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtusepi6432([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtusepi6432(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtusepi6432: Convert packed unsigned 64-bit integers in 'a' to packed
// unsigned 32-bit integers with unsigned saturation, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSQD'. Intrinsic: '_mm_maskz_cvtusepi64_epi32'.
// Requires AVX512F.
func MaskzCvtusepi6432(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtusepi6432(uint8(k), [16]byte(a)))
}

func maskzCvtusepi6432(k uint8, a [16]byte) [16]byte


// Cvtusepi648: Convert packed unsigned 64-bit integers in 'a' to packed
// unsigned 8-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			k := 8*j
//			dst[k+7:k] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
//		ENDFOR
//		dst[MAX:16] := 0
//
// Instruction: 'VPMOVUSQB'. Intrinsic: '_mm_cvtusepi64_epi8'.
// Requires AVX512F.
func Cvtusepi648(a M128i) M128i {
	return M128i(cvtusepi648([16]byte(a)))
}

func cvtusepi648(a [16]byte) [16]byte


// MaskCvtusepi648: Convert packed unsigned 64-bit integers in 'a' to packed
// unsigned 8-bit integers with unsigned saturation, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:16] := 0
//
// Instruction: 'VPMOVUSQB'. Intrinsic: '_mm_mask_cvtusepi64_epi8'.
// Requires AVX512F.
func MaskCvtusepi648(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskCvtusepi648([16]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtusepi648(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzCvtusepi648: Convert packed unsigned 64-bit integers in 'a' to packed
// unsigned 8-bit integers with unsigned saturation, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:16] := 0
//
// Instruction: 'VPMOVUSQB'. Intrinsic: '_mm_maskz_cvtusepi64_epi8'.
// Requires AVX512F.
func MaskzCvtusepi648(k Mmask8, a M128i) M128i {
	return M128i(maskzCvtusepi648(uint8(k), [16]byte(a)))
}

func maskzCvtusepi648(k uint8, a [16]byte) [16]byte


// MaskCvtusepi64Storeu16: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 16-bit integers with unsigned saturation, and store the
// active results (those with their respective bit set in writemask 'k') to
// unaligned memory at 'base_addr'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				MEM[base_addr+l+15:base_addr+l] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVUSQW'. Intrinsic: '_mm_mask_cvtusepi64_storeu_epi16'.
// Requires AVX512F.
func MaskCvtusepi64Storeu16(base_addr uintptr, k Mmask8, a M128i)  {
	maskCvtusepi64Storeu16(uintptr(base_addr), uint8(k), [16]byte(a))
}

func maskCvtusepi64Storeu16(base_addr uintptr, k uint8, a [16]byte) 


// MaskCvtusepi64Storeu32: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 32-bit integers with unsigned saturation, and store the
// active results (those with their respective bit set in writemask 'k') to
// unaligned memory at 'base_addr'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				MEM[base_addr+l+31:base_addr+l] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSQD'. Intrinsic: '_mm_mask_cvtusepi64_storeu_epi32'.
// Requires AVX512F.
func MaskCvtusepi64Storeu32(base_addr uintptr, k Mmask8, a M128i)  {
	maskCvtusepi64Storeu32(uintptr(base_addr), uint8(k), [16]byte(a))
}

func maskCvtusepi64Storeu32(base_addr uintptr, k uint8, a [16]byte) 


// MaskCvtusepi64Storeu8: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 8-bit integers with unsigned saturation, and store the
// active results (those with their respective bit set in writemask 'k') to
// unaligned memory at 'base_addr'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
//			FI
//		ENDFOR
//		dst[MAX:16] := 0
//
// Instruction: 'VPMOVUSQB'. Intrinsic: '_mm_mask_cvtusepi64_storeu_epi8'.
// Requires AVX512F.
func MaskCvtusepi64Storeu8(base_addr uintptr, k Mmask8, a M128i)  {
	maskCvtusepi64Storeu8(uintptr(base_addr), uint8(k), [16]byte(a))
}

func maskCvtusepi64Storeu8(base_addr uintptr, k uint8, a [16]byte) 


// DbsadEpu8: Compute the sum of absolute differences (SADs) of quadruplets of
// unsigned 8-bit integers in 'a' compared to those in 'b', and store the
// 16-bit results in 'dst'.
// 	Four SADs are performed on four 8-bit quadruplets for each 64-bit lane. The
// first two SADs use the lower 8-bit quadruplet of the lane from 'a', and the
// last two SADs use the uppper 8-bit quadruplet of the lane from 'a'.
// Quadruplets from 'b' are selected according to the control in 'imm8', and
// each SAD in each 64-bit lane uses the selected quadruplet at 8-bit offsets. 
//
//		tmp[31:0] := select(b[127:0], imm8[1:0])
//		tmp[63:32] := select(b[127:0], imm8[3:2])
//		tmp[95:64] := select(b[127:0], imm8[5:4])
//		tmp[127:96] := select(b[127:0], imm8[7:6])
//		
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+15:i] := ABS(a[i+7:i] - tmp[i+7:i]) + ABS(a[i+15:i+8] - tmp[i+15:i+8])
//						 + ABS(a[i+23:i+16] - tmp[i+23:i+16]) + ABS(a[i+31:i+24] - tmp[i+31:i+24])
//			
//			dst[i+31:i+16] := ABS(a[i+7:i] - tmp[i+15:i+8]) + ABS(a[i+15:i+8] - tmp[i+23:i+16])
//						 + ABS(a[i+23:i+16] - tmp[i+31:i+24]) + ABS(a[i+31:i+24] - tmp[i+39:i+32])
//			
//			dst[i+47:i+32] := ABS(a[i+39:i+32] - tmp[i+23:i+16]) + ABS(a[i+47:i+40] - tmp[i+31:i+24])
//						 + ABS(a[i+55:i+48] - tmp[i+39:i+32]) + ABS(a[i+63:i+56] - tmp[i+47:i+40])
//			
//			dst[i+63:i+48] := ABS(a[i+39:i+32] - tmp[i+31:i+24]) + ABS(a[i+47:i+40] - tmp[i+39:i+32])
//						 + ABS(a[i+55:i+48] - tmp[i+47:i+40]) + ABS(a[i+63:i+56] - tmp[i+55:i+48])
//		ENDFOR
//		
//		dst[MAX:128] := 0
//
// Instruction: 'VDBPSADBW'. Intrinsic: '_mm_dbsad_epu8'.
// Requires AVX512BW.
func DbsadEpu8(a M128i, b M128i, imm8 int) M128i {
	return M128i(dbsadEpu8([16]byte(a), [16]byte(b), imm8))
}

func dbsadEpu8(a [16]byte, b [16]byte, imm8 int) [16]byte


// MaskDbsadEpu8: Compute the sum of absolute differences (SADs) of quadruplets
// of unsigned 8-bit integers in 'a' compared to those in 'b', and store the
// 16-bit results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set).
// 	Four SADs are performed on four 8-bit quadruplets for each 64-bit lane. The
// first two SADs use the lower 8-bit quadruplet of the lane from 'a', and the
// last two SADs use the uppper 8-bit quadruplet of the lane from 'a'.
// Quadruplets from 'b' are selected according to the control in 'imm8', and
// each SAD in each 64-bit lane uses the selected quadruplet at 8-bit offsets. 
//
//		tmp[31:0] := select(b[127:0], imm8[1:0])
//		tmp[63:32] := select(b[127:0], imm8[3:2])
//		tmp[95:64] := select(b[127:0], imm8[5:4])
//		tmp[127:96] := select(b[127:0], imm8[7:6])
//		
//		FOR j := 0 to 1
//			i := j*64
//			tmp_dst[i+15:i] := ABS(a[i+7:i] - tmp[i+7:i]) + ABS(a[i+15:i+8] - tmp[i+15:i+8])
//						 + ABS(a[i+23:i+16] - tmp[i+23:i+16]) + ABS(a[i+31:i+24] - tmp[i+31:i+24])
//			
//			tmp_dst[i+31:i+16] := ABS(a[i+7:i] - tmp[i+15:i+8]) + ABS(a[i+15:i+8] - tmp[i+23:i+16])
//						 + ABS(a[i+23:i+16] - tmp[i+31:i+24]) + ABS(a[i+31:i+24] - tmp[i+39:i+32])
//			
//			tmp_dst[i+47:i+32] := ABS(a[i+39:i+32] - tmp[i+23:i+16]) + ABS(a[i+47:i+40] - tmp[i+31:i+24])
//						 + ABS(a[i+55:i+48] - tmp[i+39:i+32]) + ABS(a[i+63:i+56] - tmp[i+47:i+40])
//			
//			tmp_dst[i+63:i+48] := ABS(a[i+39:i+32] - tmp[i+31:i+24]) + ABS(a[i+47:i+40] - tmp[i+39:i+32])
//						 + ABS(a[i+55:i+48] - tmp[i+47:i+40]) + ABS(a[i+63:i+56] - tmp[i+55:i+48])
//		ENDFOR
//		
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VDBPSADBW'. Intrinsic: '_mm_mask_dbsad_epu8'.
// Requires AVX512BW.
func MaskDbsadEpu8(src M128i, k Mmask8, a M128i, b M128i, imm8 int) M128i {
	return M128i(maskDbsadEpu8([16]byte(src), uint8(k), [16]byte(a), [16]byte(b), imm8))
}

func maskDbsadEpu8(src [16]byte, k uint8, a [16]byte, b [16]byte, imm8 int) [16]byte


// MaskzDbsadEpu8: Compute the sum of absolute differences (SADs) of
// quadruplets of unsigned 8-bit integers in 'a' compared to those in 'b', and
// store the 16-bit results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set).
// 	Four SADs are performed on four 8-bit quadruplets for each 64-bit lane. The
// first two SADs use the lower 8-bit quadruplet of the lane from 'a', and the
// last two SADs use the uppper 8-bit quadruplet of the lane from 'a'.
// Quadruplets from 'b' are selected according to the control in 'imm8', and
// each SAD in each 64-bit lane uses the selected quadruplet at 8-bit offsets. 
//
//		tmp[31:0] := select(b[127:0], imm8[1:0])
//		tmp[63:32] := select(b[127:0], imm8[3:2])
//		tmp[95:64] := select(b[127:0], imm8[5:4])
//		tmp[127:96] := select(b[127:0], imm8[7:6])
//		
//		FOR j := 0 to 1
//			i := j*64
//			tmp_dst[i+15:i] := ABS(a[i+7:i] - tmp[i+7:i]) + ABS(a[i+15:i+8] - tmp[i+15:i+8])
//						 + ABS(a[i+23:i+16] - tmp[i+23:i+16]) + ABS(a[i+31:i+24] - tmp[i+31:i+24])
//			
//			tmp_dst[i+31:i+16] := ABS(a[i+7:i] - tmp[i+15:i+8]) + ABS(a[i+15:i+8] - tmp[i+23:i+16])
//						 + ABS(a[i+23:i+16] - tmp[i+31:i+24]) + ABS(a[i+31:i+24] - tmp[i+39:i+32])
//			
//			tmp_dst[i+47:i+32] := ABS(a[i+39:i+32] - tmp[i+23:i+16]) + ABS(a[i+47:i+40] - tmp[i+31:i+24])
//						 + ABS(a[i+55:i+48] - tmp[i+39:i+32]) + ABS(a[i+63:i+56] - tmp[i+47:i+40])
//			
//			tmp_dst[i+63:i+48] := ABS(a[i+39:i+32] - tmp[i+31:i+24]) + ABS(a[i+47:i+40] - tmp[i+39:i+32])
//						 + ABS(a[i+55:i+48] - tmp[i+47:i+40]) + ABS(a[i+63:i+56] - tmp[i+55:i+48])
//		ENDFOR
//		
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VDBPSADBW'. Intrinsic: '_mm_maskz_dbsad_epu8'.
// Requires AVX512BW.
func MaskzDbsadEpu8(k Mmask8, a M128i, b M128i, imm8 int) M128i {
	return M128i(maskzDbsadEpu8(uint8(k), [16]byte(a), [16]byte(b), imm8))
}

func maskzDbsadEpu8(k uint8, a [16]byte, b [16]byte, imm8 int) [16]byte


// Div16: Divide packed 16-bit integers in 'a' by packed elements in 'b', and
// store the truncated results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 16*j
//			dst[i+15:i] := TRUNCATE(a[i+15:i] / b[i+15:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_div_epi16'.
// Requires SSE.
func Div16(a M128i, b M128i) M128i {
	return M128i(div16([16]byte(a), [16]byte(b)))
}

func div16(a [16]byte, b [16]byte) [16]byte


// Div32: Divide packed 32-bit integers in 'a' by packed elements in 'b', and
// store the truncated results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			dst[i+31:i] := TRUNCATE(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_div_epi32'.
// Requires SSE.
func Div32(a M128i, b M128i) M128i {
	return M128i(div32([16]byte(a), [16]byte(b)))
}

func div32(a [16]byte, b [16]byte) [16]byte


// Div64: Divide packed 64-bit integers in 'a' by packed elements in 'b', and
// store the truncated results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			dst[i+63:i] := TRUNCATE(a[i+63:i] / b[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_div_epi64'.
// Requires SSE.
func Div64(a M128i, b M128i) M128i {
	return M128i(div64([16]byte(a), [16]byte(b)))
}

func div64(a [16]byte, b [16]byte) [16]byte


// Div8: Divide packed 8-bit integers in 'a' by packed elements in 'b', and
// store the truncated results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 8*j
//			dst[i+7:i] := TRUNCATE(a[i+7:i] / b[i+7:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_div_epi8'.
// Requires SSE.
func Div8(a M128i, b M128i) M128i {
	return M128i(div8([16]byte(a), [16]byte(b)))
}

func div8(a [16]byte, b [16]byte) [16]byte


// DivEpu16: Divide packed unsigned 16-bit integers in 'a' by packed elements
// in 'b', and store the truncated results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 16*j
//			dst[i+15:i] := TRUNCATE(a[i+15:i] / b[i+15:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_div_epu16'.
// Requires SSE.
func DivEpu16(a M128i, b M128i) M128i {
	return M128i(divEpu16([16]byte(a), [16]byte(b)))
}

func divEpu16(a [16]byte, b [16]byte) [16]byte


// DivEpu32: Divide packed unsigned 32-bit integers in 'a' by packed elements
// in 'b', and store the truncated results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			dst[i+31:i] := TRUNCATE(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_div_epu32'.
// Requires SSE.
func DivEpu32(a M128i, b M128i) M128i {
	return M128i(divEpu32([16]byte(a), [16]byte(b)))
}

func divEpu32(a [16]byte, b [16]byte) [16]byte


// DivEpu64: Divide packed unsigned 64-bit integers in 'a' by packed elements
// in 'b', and store the truncated results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			dst[i+63:i] := TRUNCATE(a[i+63:i] / b[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_div_epu64'.
// Requires SSE.
func DivEpu64(a M128i, b M128i) M128i {
	return M128i(divEpu64([16]byte(a), [16]byte(b)))
}

func divEpu64(a [16]byte, b [16]byte) [16]byte


// DivEpu8: Divide packed unsigned 8-bit integers in 'a' by packed elements in
// 'b', and store the truncated results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 8*j
//			dst[i+7:i] := TRUNCATE(a[i+7:i] / b[i+7:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_div_epu8'.
// Requires SSE.
func DivEpu8(a M128i, b M128i) M128i {
	return M128i(divEpu8([16]byte(a), [16]byte(b)))
}

func divEpu8(a [16]byte, b [16]byte) [16]byte


// DivPd: Divide packed double-precision (64-bit) floating-point elements in
// 'a' by packed elements in 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			dst[i+63:i] := a[i+63:i] / b[i+63:i]
//		ENDFOR
//
// Instruction: 'DIVPD'. Intrinsic: '_mm_div_pd'.
// Requires SSE2.
func DivPd(a M128d, b M128d) M128d {
	return M128d(divPd([2]float64(a), [2]float64(b)))
}

func divPd(a [2]float64, b [2]float64) [2]float64


// MaskDivPd: Divide packed double-precision (64-bit) floating-point elements
// in 'a' by packed elements in 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] / b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VDIVPD'. Intrinsic: '_mm_mask_div_pd'.
// Requires AVX512F.
func MaskDivPd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskDivPd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskDivPd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzDivPd: Divide packed double-precision (64-bit) floating-point elements
// in 'a' by packed elements in 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := 64*j
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] / b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VDIVPD'. Intrinsic: '_mm_maskz_div_pd'.
// Requires AVX512F.
func MaskzDivPd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzDivPd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzDivPd(k uint8, a [2]float64, b [2]float64) [2]float64


// DivPs: Divide packed single-precision (32-bit) floating-point elements in
// 'a' by packed elements in 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			dst[i+31:i] := a[i+31:i] / b[i+31:i]
//		ENDFOR
//
// Instruction: 'DIVPS'. Intrinsic: '_mm_div_ps'.
// Requires SSE.
func DivPs(a M128, b M128) M128 {
	return M128(divPs([4]float32(a), [4]float32(b)))
}

func divPs(a [4]float32, b [4]float32) [4]float32


// MaskDivPs: Divide packed single-precision (32-bit) floating-point elements
// in 'a' by packed elements in 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] / b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VDIVPS'. Intrinsic: '_mm_mask_div_ps'.
// Requires AVX512F.
func MaskDivPs(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskDivPs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskDivPs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzDivPs: Divide packed single-precision (32-bit) floating-point elements
// in 'a' by packed elements in 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] / b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VDIVPS'. Intrinsic: '_mm_maskz_div_ps'.
// Requires AVX512F.
func MaskzDivPs(k Mmask8, a M128, b M128) M128 {
	return M128(maskzDivPs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzDivPs(k uint8, a [4]float32, b [4]float32) [4]float32


// DivRoundSd: Divide the lower double-precision (64-bit) floating-point
// element in 'a' by the lower double-precision (64-bit) floating-point element
// in 'b', store the result in the lower element of 'dst', and copy the upper
// element from 'a' to the upper element of 'dst'.
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := a[63:0] / b[63:0]
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VDIVSD'. Intrinsic: '_mm_div_round_sd'.
// Requires AVX512F.
func DivRoundSd(a M128d, b M128d, rounding int) M128d {
	return M128d(divRoundSd([2]float64(a), [2]float64(b), rounding))
}

func divRoundSd(a [2]float64, b [2]float64, rounding int) [2]float64


// MaskDivRoundSd: Divide the lower double-precision (64-bit) floating-point
// element in 'a' by the lower double-precision (64-bit) floating-point element
// in 'b', store the result in the lower element of 'dst' using writemask 'k'
// (the element is copied from 'src' when mask bit 0 is not set), and copy the
// upper element from 'a' to the upper element of 'dst'. 
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := a[63:0] / b[63:0]
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VDIVSD'. Intrinsic: '_mm_mask_div_round_sd'.
// Requires AVX512F.
func MaskDivRoundSd(src M128d, k Mmask8, a M128d, b M128d, rounding int) M128d {
	return M128d(maskDivRoundSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), rounding))
}

func maskDivRoundSd(src [2]float64, k uint8, a [2]float64, b [2]float64, rounding int) [2]float64


// MaskzDivRoundSd: Divide the lower double-precision (64-bit) floating-point
// element in 'a' by the lower double-precision (64-bit) floating-point element
// in 'b', store the result in the lower element of 'dst' using zeromask 'k'
// (the element is zeroed out when mask bit 0 is not set), and copy the upper
// element from 'a' to the upper element of 'dst'.
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := a[63:0] / b[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VDIVSD'. Intrinsic: '_mm_maskz_div_round_sd'.
// Requires AVX512F.
func MaskzDivRoundSd(k Mmask8, a M128d, b M128d, rounding int) M128d {
	return M128d(maskzDivRoundSd(uint8(k), [2]float64(a), [2]float64(b), rounding))
}

func maskzDivRoundSd(k uint8, a [2]float64, b [2]float64, rounding int) [2]float64


// DivRoundSs: Divide the lower single-precision (32-bit) floating-point
// element in 'a' by the lower single-precision (32-bit) floating-point element
// in 'b', store the result in the lower element of 'dst', and copy the upper 3
// packed elements from 'a' to the upper elements of 'dst'.
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := a[31:0] / b[31:0]
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VDIVSS'. Intrinsic: '_mm_div_round_ss'.
// Requires AVX512F.
func DivRoundSs(a M128, b M128, rounding int) M128 {
	return M128(divRoundSs([4]float32(a), [4]float32(b), rounding))
}

func divRoundSs(a [4]float32, b [4]float32, rounding int) [4]float32


// MaskDivRoundSs: Divide the lower single-precision (32-bit) floating-point
// element in 'a' by the lower single-precision (32-bit) floating-point element
// in 'b', store the result in the lower element of 'dst' using writemask 'k'
// (the element is copied from 'src' when mask bit 0 is not set), and copy the
// upper 3 packed elements from 'a' to the upper elements of 'dst'. 
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := a[31:0] / b[31:0]
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VDIVSS'. Intrinsic: '_mm_mask_div_round_ss'.
// Requires AVX512F.
func MaskDivRoundSs(src M128, k Mmask8, a M128, b M128, rounding int) M128 {
	return M128(maskDivRoundSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), rounding))
}

func maskDivRoundSs(src [4]float32, k uint8, a [4]float32, b [4]float32, rounding int) [4]float32


// MaskzDivRoundSs: Divide the lower single-precision (32-bit) floating-point
// element in 'a' by the lower single-precision (32-bit) floating-point element
// in 'b', store the result in the lower element of 'dst' using zeromask 'k'
// (the element is zeroed out when mask bit 0 is not set), and copy the upper 3
// packed elements from 'a' to the upper elements of 'dst'.
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := a[31:0] / b[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VDIVSS'. Intrinsic: '_mm_maskz_div_round_ss'.
// Requires AVX512F.
func MaskzDivRoundSs(k Mmask8, a M128, b M128, rounding int) M128 {
	return M128(maskzDivRoundSs(uint8(k), [4]float32(a), [4]float32(b), rounding))
}

func maskzDivRoundSs(k uint8, a [4]float32, b [4]float32, rounding int) [4]float32


// DivSd: Divide the lower double-precision (64-bit) floating-point element in
// 'a' by the lower double-precision (64-bit) floating-point element in 'b',
// store the result in the lower element of 'dst', and copy the upper element
// from 'a' to the upper element of 'dst'. 
//
//		dst[63:0] := a[63:0] 0 b[63:0]
//		dst[127:64] := a[127:64]
//
// Instruction: 'DIVSD'. Intrinsic: '_mm_div_sd'.
// Requires SSE2.
func DivSd(a M128d, b M128d) M128d {
	return M128d(divSd([2]float64(a), [2]float64(b)))
}

func divSd(a [2]float64, b [2]float64) [2]float64


// MaskDivSd: Divide the lower double-precision (64-bit) floating-point element
// in 'a' by the lower double-precision (64-bit) floating-point element in 'b',
// store the result in the lower element of 'dst' using writemask 'k' (the
// element is copied from 'src' when mask bit 0 is not set), and copy the upper
// element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := a[63:0] / b[63:0]
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VDIVSD'. Intrinsic: '_mm_mask_div_sd'.
// Requires AVX512F.
func MaskDivSd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskDivSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskDivSd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzDivSd: Divide the lower double-precision (64-bit) floating-point
// element in 'a' by the lower double-precision (64-bit) floating-point element
// in 'b', store the result in the lower element of 'dst' using zeromask 'k'
// (the element is zeroed out when mask bit 0 is not set), and copy the upper
// element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := a[63:0] / b[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VDIVSD'. Intrinsic: '_mm_maskz_div_sd'.
// Requires AVX512F.
func MaskzDivSd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzDivSd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzDivSd(k uint8, a [2]float64, b [2]float64) [2]float64


// DivSs: Divide the lower single-precision (32-bit) floating-point element in
// 'a' by the lower single-precision (32-bit) floating-point element in 'b',
// store the result in the lower element of 'dst', and copy the upper 3 packed
// elements from 'a' to the upper elements of 'dst'. 
//
//		dst[31:0] := a[31:0] / b[31:0]
//		dst[127:32] := a[127:32]
//
// Instruction: 'DIVSS'. Intrinsic: '_mm_div_ss'.
// Requires SSE.
func DivSs(a M128, b M128) M128 {
	return M128(divSs([4]float32(a), [4]float32(b)))
}

func divSs(a [4]float32, b [4]float32) [4]float32


// MaskDivSs: Divide the lower single-precision (32-bit) floating-point element
// in 'a' by the lower single-precision (32-bit) floating-point element in 'b',
// store the result in the lower element of 'dst' using writemask 'k' (the
// element is copied from 'src' when mask bit 0 is not set), and copy the upper
// 3 packed elements from 'a' to the upper elements of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := a[31:0] / b[31:0]
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VDIVSS'. Intrinsic: '_mm_mask_div_ss'.
// Requires AVX512F.
func MaskDivSs(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskDivSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskDivSs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzDivSs: Divide the lower single-precision (32-bit) floating-point
// element in 'a' by the lower single-precision (32-bit) floating-point element
// in 'b', store the result in the lower element of 'dst' using zeromask 'k'
// (the element is zeroed out when mask bit 0 is not set), and copy the upper 3
// packed elements from 'a' to the upper elements of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := a[31:0] / b[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VDIVSS'. Intrinsic: '_mm_maskz_div_ss'.
// Requires AVX512F.
func MaskzDivSs(k Mmask8, a M128, b M128) M128 {
	return M128(maskzDivSs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzDivSs(k uint8, a [4]float32, b [4]float32) [4]float32


// DpPd: Conditionally multiply the packed double-precision (64-bit)
// floating-point elements in 'a' and 'b' using the high 4 bits in 'imm8', sum
// the four products, and conditionally store the sum in 'dst' using the low 4
// bits of 'imm8'. 
//
//		DP(a[127:0], b[127:0], imm8[7:0]) {
//			FOR j := 0 to 1
//				i := j*64
//				IF imm8[(4+j)%8]]
//					temp[i+63:i] := a[i+63:i] * b[i+63:i]
//				ELSE
//					temp[i+63:i] := 0
//				FI
//			ENDFOR
//			
//			sum[63:0] := temp[127:64] + temp[63:0]
//			
//			FOR j := 0 to 1
//				i := j*64
//				IF imm8[j%8]
//					tmpdst[i+63:i] := sum[63:0]
//				ELSE
//					tmpdst[i+63:i] := 0
//				FI
//			ENDFOR
//			RETURN tmpdst[127:0]
//		}
//		
//		dst[127:0] := DP(a[127:0], b[127:0], imm8[7:0])
//
// Instruction: 'DPPD'. Intrinsic: '_mm_dp_pd'.
// Requires SSE4.1.
func DpPd(a M128d, b M128d, imm8 int) M128d {
	return M128d(dpPd([2]float64(a), [2]float64(b), imm8))
}

func dpPd(a [2]float64, b [2]float64, imm8 int) [2]float64


// DpPs: Conditionally multiply the packed single-precision (32-bit)
// floating-point elements in 'a' and 'b' using the high 4 bits in 'imm8', sum
// the four products, and conditionally store the sum in 'dst' using the low 4
// bits of 'imm8'. 
//
//		DP(a[127:0], b[127:0], imm8[7:0]) {
//			FOR j := 0 to 3
//				i := j*32
//				IF imm8[(4+j)%8]
//					temp[i+31:i] := a[i+31:i] * b[i+31:i]
//				ELSE
//					temp[i+31:i] := 0
//				FI
//			ENDFOR
//			
//			sum[31:0] := (temp[127:96] + temp[95:64]) + (temp[63:32] + temp[31:0])
//			
//			FOR j := 0 to 3
//				i := j*32
//				IF imm8[j%8]
//					tmpdst[i+31:i] := sum[31:0]
//				ELSE
//					tmpdst[i+31:i] := 0
//				FI
//			ENDFOR
//			RETURN tmpdst[127:0]
//		}
//		
//		dst[127:0] := DP(a[127:0], b[127:0], imm8[7:0])
//
// Instruction: 'DPPS'. Intrinsic: '_mm_dp_ps'.
// Requires SSE4.1.
func DpPs(a M128, b M128, imm8 int) M128 {
	return M128(dpPs([4]float32(a), [4]float32(b), imm8))
}

func dpPs(a [4]float32, b [4]float32, imm8 int) [4]float32


// ErfPd: Compute the error function of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := ERF(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_erf_pd'.
// Requires SSE.
func ErfPd(a M128d) M128d {
	return M128d(erfPd([2]float64(a)))
}

func erfPd(a [2]float64) [2]float64


// ErfPs: Compute the error function of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := ERF(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_erf_ps'.
// Requires SSE.
func ErfPs(a M128) M128 {
	return M128(erfPs([4]float32(a)))
}

func erfPs(a [4]float32) [4]float32


// ErfcPd: Compute the complementary error function of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := 1.0 - ERF(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_erfc_pd'.
// Requires SSE.
func ErfcPd(a M128d) M128d {
	return M128d(erfcPd([2]float64(a)))
}

func erfcPd(a [2]float64) [2]float64


// ErfcPs: Compute the complementary error function of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := 1.0 - ERF(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_erfc_ps'.
// Requires SSE.
func ErfcPs(a M128) M128 {
	return M128(erfcPs([4]float32(a)))
}

func erfcPs(a [4]float32) [4]float32


// ErfcinvPd: Compute the inverse complementary error function of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := 1.0 / (1.0 - ERF(a[i+63:i]))
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_erfcinv_pd'.
// Requires SSE.
func ErfcinvPd(a M128d) M128d {
	return M128d(erfcinvPd([2]float64(a)))
}

func erfcinvPd(a [2]float64) [2]float64


// ErfcinvPs: Compute the inverse complementary error function of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := 1.0 / (1.0 - ERF(a[i+31:i]))
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_erfcinv_ps'.
// Requires SSE.
func ErfcinvPs(a M128) M128 {
	return M128(erfcinvPs([4]float32(a)))
}

func erfcinvPs(a [4]float32) [4]float32


// ErfinvPd: Compute the inverse error function of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := 1.0 / ERF(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_erfinv_pd'.
// Requires SSE.
func ErfinvPd(a M128d) M128d {
	return M128d(erfinvPd([2]float64(a)))
}

func erfinvPd(a [2]float64) [2]float64


// ErfinvPs: Compute the inverse error function of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := 1.0 / ERF(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_erfinv_ps'.
// Requires SSE.
func ErfinvPs(a M128) M128 {
	return M128(erfinvPs([4]float32(a)))
}

func erfinvPs(a [4]float32) [4]float32


// ExpPd: Compute the exponential value of 'e' raised to the power of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := e^(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_exp_pd'.
// Requires SSE.
func ExpPd(a M128d) M128d {
	return M128d(expPd([2]float64(a)))
}

func expPd(a [2]float64) [2]float64


// ExpPs: Compute the exponential value of 'e' raised to the power of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := e^(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_exp_ps'.
// Requires SSE.
func ExpPs(a M128) M128 {
	return M128(expPs([4]float32(a)))
}

func expPs(a [4]float32) [4]float32


// Exp10Pd: Compute the exponential value of 10 raised to the power of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := 10^(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_exp10_pd'.
// Requires SSE.
func Exp10Pd(a M128d) M128d {
	return M128d(exp10Pd([2]float64(a)))
}

func exp10Pd(a [2]float64) [2]float64


// Exp10Ps: Compute the exponential value of 10 raised to the power of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := 10^(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_exp10_ps'.
// Requires SSE.
func Exp10Ps(a M128) M128 {
	return M128(exp10Ps([4]float32(a)))
}

func exp10Ps(a [4]float32) [4]float32


// Exp2Pd: Compute the exponential value of 2 raised to the power of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := 2^(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_exp2_pd'.
// Requires SSE.
func Exp2Pd(a M128d) M128d {
	return M128d(exp2Pd([2]float64(a)))
}

func exp2Pd(a [2]float64) [2]float64


// Exp2Ps: Compute the exponential value of 2 raised to the power of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := 2^(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_exp2_ps'.
// Requires SSE.
func Exp2Ps(a M128) M128 {
	return M128(exp2Ps([4]float32(a)))
}

func exp2Ps(a [4]float32) [4]float32


// MaskExpand32: Load contiguous active 32-bit integers from 'a' (those with
// their respective bit set in mask 'k'), and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[m+31:m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPEXPANDD'. Intrinsic: '_mm_mask_expand_epi32'.
// Requires AVX512F.
func MaskExpand32(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskExpand32([16]byte(src), uint8(k), [16]byte(a)))
}

func maskExpand32(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzExpand32: Load contiguous active 32-bit integers from 'a' (those with
// their respective bit set in mask 'k'), and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[m+31:m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPEXPANDD'. Intrinsic: '_mm_maskz_expand_epi32'.
// Requires AVX512F.
func MaskzExpand32(k Mmask8, a M128i) M128i {
	return M128i(maskzExpand32(uint8(k), [16]byte(a)))
}

func maskzExpand32(k uint8, a [16]byte) [16]byte


// MaskExpand64: Load contiguous active 64-bit integers from 'a' (those with
// their respective bit set in mask 'k'), and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		m := 0
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[m+63:m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPEXPANDQ'. Intrinsic: '_mm_mask_expand_epi64'.
// Requires AVX512F.
func MaskExpand64(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskExpand64([16]byte(src), uint8(k), [16]byte(a)))
}

func maskExpand64(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzExpand64: Load contiguous active 64-bit integers from 'a' (those with
// their respective bit set in mask 'k'), and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		m := 0
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[m+63:m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPEXPANDQ'. Intrinsic: '_mm_maskz_expand_epi64'.
// Requires AVX512F.
func MaskzExpand64(k Mmask8, a M128i) M128i {
	return M128i(maskzExpand64(uint8(k), [16]byte(a)))
}

func maskzExpand64(k uint8, a [16]byte) [16]byte


// MaskExpandPd: Load contiguous active double-precision (64-bit)
// floating-point elements from 'a' (those with their respective bit set in
// mask 'k'), and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[m+63:m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXPANDPD'. Intrinsic: '_mm_mask_expand_pd'.
// Requires AVX512F.
func MaskExpandPd(src M128d, k Mmask8, a M128d) M128d {
	return M128d(maskExpandPd([2]float64(src), uint8(k), [2]float64(a)))
}

func maskExpandPd(src [2]float64, k uint8, a [2]float64) [2]float64


// MaskzExpandPd: Load contiguous active double-precision (64-bit)
// floating-point elements from 'a' (those with their respective bit set in
// mask 'k'), and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[m+63:m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXPANDPD'. Intrinsic: '_mm_maskz_expand_pd'.
// Requires AVX512F.
func MaskzExpandPd(k Mmask8, a M128d) M128d {
	return M128d(maskzExpandPd(uint8(k), [2]float64(a)))
}

func maskzExpandPd(k uint8, a [2]float64) [2]float64


// MaskExpandPs: Load contiguous active single-precision (32-bit)
// floating-point elements from 'a' (those with their respective bit set in
// mask 'k'), and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[m+31:m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXPANDPS'. Intrinsic: '_mm_mask_expand_ps'.
// Requires AVX512F.
func MaskExpandPs(src M128, k Mmask8, a M128) M128 {
	return M128(maskExpandPs([4]float32(src), uint8(k), [4]float32(a)))
}

func maskExpandPs(src [4]float32, k uint8, a [4]float32) [4]float32


// MaskzExpandPs: Load contiguous active single-precision (32-bit)
// floating-point elements from 'a' (those with their respective bit set in
// mask 'k'), and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[m+31:m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXPANDPS'. Intrinsic: '_mm_maskz_expand_ps'.
// Requires AVX512F.
func MaskzExpandPs(k Mmask8, a M128) M128 {
	return M128(maskzExpandPs(uint8(k), [4]float32(a)))
}

func maskzExpandPs(k uint8, a [4]float32) [4]float32


// MaskExpandloadu32: Load contiguous active 32-bit integers from unaligned
// memory at 'mem_addr' (those with their respective bit set in mask 'k'), and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPEXPANDD'. Intrinsic: '_mm_mask_expandloadu_epi32'.
// Requires AVX512F.
func MaskExpandloadu32(src M128i, k Mmask8, mem_addr uintptr) M128i {
	return M128i(maskExpandloadu32([16]byte(src), uint8(k), uintptr(mem_addr)))
}

func maskExpandloadu32(src [16]byte, k uint8, mem_addr uintptr) [16]byte


// MaskzExpandloadu32: Load contiguous active 32-bit integers from unaligned
// memory at 'mem_addr' (those with their respective bit set in mask 'k'), and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPEXPANDD'. Intrinsic: '_mm_maskz_expandloadu_epi32'.
// Requires AVX512F.
func MaskzExpandloadu32(k Mmask8, mem_addr uintptr) M128i {
	return M128i(maskzExpandloadu32(uint8(k), uintptr(mem_addr)))
}

func maskzExpandloadu32(k uint8, mem_addr uintptr) [16]byte


// MaskExpandloadu64: Load contiguous active 64-bit integers from unaligned
// memory at 'mem_addr' (those with their respective bit set in mask 'k'), and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPEXPANDQ'. Intrinsic: '_mm_mask_expandloadu_epi64'.
// Requires AVX512F.
func MaskExpandloadu64(src M128i, k Mmask8, mem_addr uintptr) M128i {
	return M128i(maskExpandloadu64([16]byte(src), uint8(k), uintptr(mem_addr)))
}

func maskExpandloadu64(src [16]byte, k uint8, mem_addr uintptr) [16]byte


// MaskzExpandloadu64: Load contiguous active 64-bit integers from unaligned
// memory at 'mem_addr' (those with their respective bit set in mask 'k'), and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPEXPANDQ'. Intrinsic: '_mm_maskz_expandloadu_epi64'.
// Requires AVX512F.
func MaskzExpandloadu64(k Mmask8, mem_addr uintptr) M128i {
	return M128i(maskzExpandloadu64(uint8(k), uintptr(mem_addr)))
}

func maskzExpandloadu64(k uint8, mem_addr uintptr) [16]byte


// MaskExpandloaduPd: Load contiguous active double-precision (64-bit)
// floating-point elements from unaligned memory at 'mem_addr' (those with
// their respective bit set in mask 'k'), and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		m := 0
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXPANDPD'. Intrinsic: '_mm_mask_expandloadu_pd'.
// Requires AVX512F.
func MaskExpandloaduPd(src M128d, k Mmask8, mem_addr uintptr) M128d {
	return M128d(maskExpandloaduPd([2]float64(src), uint8(k), uintptr(mem_addr)))
}

func maskExpandloaduPd(src [2]float64, k uint8, mem_addr uintptr) [2]float64


// MaskzExpandloaduPd: Load contiguous active double-precision (64-bit)
// floating-point elements from unaligned memory at 'mem_addr' (those with
// their respective bit set in mask 'k'), and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		m := 0
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXPANDPD'. Intrinsic: '_mm_maskz_expandloadu_pd'.
// Requires AVX512F.
func MaskzExpandloaduPd(k Mmask8, mem_addr uintptr) M128d {
	return M128d(maskzExpandloaduPd(uint8(k), uintptr(mem_addr)))
}

func maskzExpandloaduPd(k uint8, mem_addr uintptr) [2]float64


// MaskExpandloaduPs: Load contiguous active single-precision (32-bit)
// floating-point elements from unaligned memory at 'mem_addr' (those with
// their respective bit set in mask 'k'), and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXPANDPS'. Intrinsic: '_mm_mask_expandloadu_ps'.
// Requires AVX512F.
func MaskExpandloaduPs(src M128, k Mmask8, mem_addr uintptr) M128 {
	return M128(maskExpandloaduPs([4]float32(src), uint8(k), uintptr(mem_addr)))
}

func maskExpandloaduPs(src [4]float32, k uint8, mem_addr uintptr) [4]float32


// MaskzExpandloaduPs: Load contiguous active single-precision (32-bit)
// floating-point elements from unaligned memory at 'mem_addr' (those with
// their respective bit set in mask 'k'), and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXPANDPS'. Intrinsic: '_mm_maskz_expandloadu_ps'.
// Requires AVX512F.
func MaskzExpandloaduPs(k Mmask8, mem_addr uintptr) M128 {
	return M128(maskzExpandloaduPs(uint8(k), uintptr(mem_addr)))
}

func maskzExpandloaduPs(k uint8, mem_addr uintptr) [4]float32


// Expm1Pd: Compute the exponential value of 'e' raised to the power of packed
// double-precision (64-bit) floating-point elements in 'a', subtract one from
// each element, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := e^(a[i+63:i]) - 1.0
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_expm1_pd'.
// Requires SSE.
func Expm1Pd(a M128d) M128d {
	return M128d(expm1Pd([2]float64(a)))
}

func expm1Pd(a [2]float64) [2]float64


// Expm1Ps: Compute the exponential value of 'e' raised to the power of packed
// single-precision (32-bit) floating-point elements in 'a', subtract one from
// each element, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := e^(a[i+31:i]) - 1.0
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_expm1_ps'.
// Requires SSE.
func Expm1Ps(a M128) M128 {
	return M128(expm1Ps([4]float32(a)))
}

func expm1Ps(a [4]float32) [4]float32


// Extract16: Extract a 16-bit integer from 'a', selected with 'imm8', and
// store the result in the lower element of 'dst'. 
//
//		dst[15:0] := (a[127:0] >> (imm8[2:0] * 16))[15:0]
//		dst[31:16] := 0
//
// Instruction: 'PEXTRW'. Intrinsic: '_mm_extract_epi16'.
// Requires SSE2.
func Extract16(a M128i, imm8 int) int {
	return int(extract16([16]byte(a), imm8))
}

func extract16(a [16]byte, imm8 int) int


// Extract32: Extract a 32-bit integer from 'a', selected with 'imm8', and
// store the result in 'dst'. 
//
//		dst[31:0] := (a[127:0] >> (imm8[1:0] * 32))[31:0]
//
// Instruction: 'PEXTRD'. Intrinsic: '_mm_extract_epi32'.
// Requires SSE4.1.
func Extract32(a M128i, imm8 int) int {
	return int(extract32([16]byte(a), imm8))
}

func extract32(a [16]byte, imm8 int) int


// Extract64: Extract a 64-bit integer from 'a', selected with 'imm8', and
// store the result in 'dst'. 
//
//		dst[63:0] := (a[127:0] >> (imm8[0] * 64))[63:0]
//
// Instruction: 'PEXTRQ'. Intrinsic: '_mm_extract_epi64'.
// Requires SSE4.1.
func Extract64(a M128i, imm8 int) int64 {
	return int64(extract64([16]byte(a), imm8))
}

func extract64(a [16]byte, imm8 int) int64


// Extract8: Extract an 8-bit integer from 'a', selected with 'imm8', and store
// the result in the lower element of 'dst'. 
//
//		dst[7:0] := (a[127:0] >> (imm8[3:0] * 8))[7:0]
//		dst[31:8] := 0
//
// Instruction: 'PEXTRB'. Intrinsic: '_mm_extract_epi8'.
// Requires SSE4.1.
func Extract8(a M128i, imm8 int) int {
	return int(extract8([16]byte(a), imm8))
}

func extract8(a [16]byte, imm8 int) int


// ExtractPs: Extract a single-precision (32-bit) floating-point element from
// 'a', selected with 'imm8', and store the result in 'dst'. 
//
//		dst[31:0] := (a[127:0] >> (imm8[1:0] * 32))[31:0]
//
// Instruction: 'EXTRACTPS'. Intrinsic: '_mm_extract_ps'.
// Requires SSE4.1.
func ExtractPs(a M128, imm8 int) int {
	return int(extractPs([4]float32(a), imm8))
}

func extractPs(a [4]float32, imm8 int) int


// FixupimmPd: Fix up packed double-precision (64-bit) floating-point elements
// in 'a' and 'b' using packed 64-bit integers in 'c', and store the results in
// 'dst'. 'imm8' is used to set the required flags reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMPD'. Intrinsic: '_mm_fixupimm_pd'.
// Requires AVX512F.
func FixupimmPd(a M128d, b M128d, c M128i, imm8 int) M128d {
	return M128d(fixupimmPd([2]float64(a), [2]float64(b), [16]byte(c), imm8))
}

func fixupimmPd(a [2]float64, b [2]float64, c [16]byte, imm8 int) [2]float64


// MaskFixupimmPd: Fix up packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' using packed 64-bit integers in 'c', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMPD'. Intrinsic: '_mm_mask_fixupimm_pd'.
// Requires AVX512F.
func MaskFixupimmPd(a M128d, k Mmask8, b M128d, c M128i, imm8 int) M128d {
	return M128d(maskFixupimmPd([2]float64(a), uint8(k), [2]float64(b), [16]byte(c), imm8))
}

func maskFixupimmPd(a [2]float64, k uint8, b [2]float64, c [16]byte, imm8 int) [2]float64


// MaskzFixupimmPd: Fix up packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' using packed 64-bit integers in 'c', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMPD'. Intrinsic: '_mm_maskz_fixupimm_pd'.
// Requires AVX512F.
func MaskzFixupimmPd(k Mmask8, a M128d, b M128d, c M128i, imm8 int) M128d {
	return M128d(maskzFixupimmPd(uint8(k), [2]float64(a), [2]float64(b), [16]byte(c), imm8))
}

func maskzFixupimmPd(k uint8, a [2]float64, b [2]float64, c [16]byte, imm8 int) [2]float64


// FixupimmPs: Fix up packed single-precision (32-bit) floating-point elements
// in 'a' and 'b' using packed 32-bit integers in 'c', and store the results in
// 'dst'. 'imm8' is used to set the required flags reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMPS'. Intrinsic: '_mm_fixupimm_ps'.
// Requires AVX512F.
func FixupimmPs(a M128, b M128, c M128i, imm8 int) M128 {
	return M128(fixupimmPs([4]float32(a), [4]float32(b), [16]byte(c), imm8))
}

func fixupimmPs(a [4]float32, b [4]float32, c [16]byte, imm8 int) [4]float32


// MaskFixupimmPs: Fix up packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' using packed 32-bit integers in 'c', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMPS'. Intrinsic: '_mm_mask_fixupimm_ps'.
// Requires AVX512F.
func MaskFixupimmPs(a M128, k Mmask8, b M128, c M128i, imm8 int) M128 {
	return M128(maskFixupimmPs([4]float32(a), uint8(k), [4]float32(b), [16]byte(c), imm8))
}

func maskFixupimmPs(a [4]float32, k uint8, b [4]float32, c [16]byte, imm8 int) [4]float32


// MaskzFixupimmPs: Fix up packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' using packed 32-bit integers in 'c', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMPS'. Intrinsic: '_mm_maskz_fixupimm_ps'.
// Requires AVX512F.
func MaskzFixupimmPs(k Mmask8, a M128, b M128, c M128i, imm8 int) M128 {
	return M128(maskzFixupimmPs(uint8(k), [4]float32(a), [4]float32(b), [16]byte(c), imm8))
}

func maskzFixupimmPs(k uint8, a [4]float32, b [4]float32, c [16]byte, imm8 int) [4]float32


// FixupimmRoundSd: Fix up the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b' using the lower 64-bit integer in 'c', store the
// result in the lower element of 'dst', and copy the upper element from 'a' to
// the upper element of 'dst'. 'imm8' is used to set the required flags
// reporting.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		dst[63:0] := FIXUPIMMPD(a[63:0], b[63:0], c[63:0], imm8[7:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMSD'. Intrinsic: '_mm_fixupimm_round_sd'.
// Requires AVX512F.
func FixupimmRoundSd(a M128d, b M128d, c M128i, imm8 int, rounding int) M128d {
	return M128d(fixupimmRoundSd([2]float64(a), [2]float64(b), [16]byte(c), imm8, rounding))
}

func fixupimmRoundSd(a [2]float64, b [2]float64, c [16]byte, imm8 int, rounding int) [2]float64


// MaskFixupimmRoundSd: Fix up the lower double-precision (64-bit)
// floating-point elements in 'a' and 'b' using the lower 64-bit integer in
// 'c', store the result in the lower element of 'dst' using writemask 'k' (the
// element is copied from 'a' when mask bit 0 is not set), and copy the upper
// element from 'a' to the upper element of 'dst'. 'imm8' is used to set the
// required flags reporting.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		IF k[0]
//			dst[63:0] := FIXUPIMMPD(a[63:0], b[63:0], c[63:0], imm8[7:0])
//		ELSE
//			dst[63:0] := a[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMSD'. Intrinsic: '_mm_mask_fixupimm_round_sd'.
// Requires AVX512F.
func MaskFixupimmRoundSd(a M128d, k Mmask8, b M128d, c M128i, imm8 int, rounding int) M128d {
	return M128d(maskFixupimmRoundSd([2]float64(a), uint8(k), [2]float64(b), [16]byte(c), imm8, rounding))
}

func maskFixupimmRoundSd(a [2]float64, k uint8, b [2]float64, c [16]byte, imm8 int, rounding int) [2]float64


// MaskzFixupimmRoundSd: Fix up the lower double-precision (64-bit)
// floating-point elements in 'a' and 'b' using the lower 64-bit integer in
// 'c', store the result in the lower element of 'dst' using zeromask 'k' (the
// element is zeroed out when mask bit 0 is not set), and copy the upper
// element from 'a' to the upper element of 'dst'. 'imm8' is used to set the
// required flags reporting.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		IF k[0]
//			dst[63:0] := FIXUPIMMPD(a[63:0], b[63:0], c[63:0], imm8[7:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMSD'. Intrinsic: '_mm_maskz_fixupimm_round_sd'.
// Requires AVX512F.
func MaskzFixupimmRoundSd(k Mmask8, a M128d, b M128d, c M128i, imm8 int, rounding int) M128d {
	return M128d(maskzFixupimmRoundSd(uint8(k), [2]float64(a), [2]float64(b), [16]byte(c), imm8, rounding))
}

func maskzFixupimmRoundSd(k uint8, a [2]float64, b [2]float64, c [16]byte, imm8 int, rounding int) [2]float64


// FixupimmRoundSs: Fix up the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b' using the lower 32-bit integer in 'c', store the
// result in the lower element of 'dst', and copy the upper 3 packed elements
// from 'a' to the upper elements of 'dst'. 'imm8' is used to set the required
// flags reporting.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		dst[31:0] := FIXUPIMMPD(a[31:0], b[31:0], c[31:0], imm8[7:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMSS'. Intrinsic: '_mm_fixupimm_round_ss'.
// Requires AVX512F.
func FixupimmRoundSs(a M128, b M128, c M128i, imm8 int, rounding int) M128 {
	return M128(fixupimmRoundSs([4]float32(a), [4]float32(b), [16]byte(c), imm8, rounding))
}

func fixupimmRoundSs(a [4]float32, b [4]float32, c [16]byte, imm8 int, rounding int) [4]float32


// MaskFixupimmRoundSs: Fix up the lower single-precision (32-bit)
// floating-point elements in 'a' and 'b' using the lower 32-bit integer in
// 'c', store the result in the lower element of 'dst' using writemask 'k' (the
// element is copied from 'a' when mask bit 0 is not set), and copy the upper 3
// packed elements from 'a' to the upper elements of 'dst'. 'imm8' is used to
// set the required flags reporting.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		IF k[0]
//			dst[31:0] := FIXUPIMMPD(a[31:0], b[31:0], c[31:0], imm8[7:0])
//		ELSE
//			dst[31:0] := a[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMSS'. Intrinsic: '_mm_mask_fixupimm_round_ss'.
// Requires AVX512F.
func MaskFixupimmRoundSs(a M128, k Mmask8, b M128, c M128i, imm8 int, rounding int) M128 {
	return M128(maskFixupimmRoundSs([4]float32(a), uint8(k), [4]float32(b), [16]byte(c), imm8, rounding))
}

func maskFixupimmRoundSs(a [4]float32, k uint8, b [4]float32, c [16]byte, imm8 int, rounding int) [4]float32


// MaskzFixupimmRoundSs: Fix up the lower single-precision (32-bit)
// floating-point elements in 'a' and 'b' using the lower 32-bit integer in
// 'c', store the result in the lower element of 'dst' using zeromask 'k' (the
// element is zeroed out when mask bit 0 is not set), and copy the upper 3
// packed elements from 'a' to the upper elements of 'dst'. 'imm8' is used to
// set the required flags reporting.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		IF k[0]
//			dst[31:0] := FIXUPIMMPD(a[31:0], b[31:0], c[31:0], imm8[7:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMSS'. Intrinsic: '_mm_maskz_fixupimm_round_ss'.
// Requires AVX512F.
func MaskzFixupimmRoundSs(k Mmask8, a M128, b M128, c M128i, imm8 int, rounding int) M128 {
	return M128(maskzFixupimmRoundSs(uint8(k), [4]float32(a), [4]float32(b), [16]byte(c), imm8, rounding))
}

func maskzFixupimmRoundSs(k uint8, a [4]float32, b [4]float32, c [16]byte, imm8 int, rounding int) [4]float32


// FixupimmSd: Fix up the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b' using the lower 64-bit integer in 'c', store the
// result in the lower element of 'dst', and copy the upper element from 'a' to
// the upper element of 'dst'. 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		dst[63:0] := FIXUPIMMPD(a[63:0], b[63:0], c[63:0], imm8[7:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMSD'. Intrinsic: '_mm_fixupimm_sd'.
// Requires AVX512F.
func FixupimmSd(a M128d, b M128d, c M128i, imm8 int) M128d {
	return M128d(fixupimmSd([2]float64(a), [2]float64(b), [16]byte(c), imm8))
}

func fixupimmSd(a [2]float64, b [2]float64, c [16]byte, imm8 int) [2]float64


// MaskFixupimmSd: Fix up the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b' using the lower 64-bit integer in 'c', store the
// result in the lower element of 'dst' using writemask 'k' (the element is
// copied from 'a' when mask bit 0 is not set), and copy the upper element from
// 'a' to the upper element of 'dst'. 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		IF k[0]
//			dst[63:0] := FIXUPIMMPD(a[63:0], b[63:0], c[63:0], imm8[7:0])
//		ELSE
//			dst[63:0] := a[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMSD'. Intrinsic: '_mm_mask_fixupimm_sd'.
// Requires AVX512F.
func MaskFixupimmSd(a M128d, k Mmask8, b M128d, c M128i, imm8 int) M128d {
	return M128d(maskFixupimmSd([2]float64(a), uint8(k), [2]float64(b), [16]byte(c), imm8))
}

func maskFixupimmSd(a [2]float64, k uint8, b [2]float64, c [16]byte, imm8 int) [2]float64


// MaskzFixupimmSd: Fix up the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b' using the lower 64-bit integer in 'c', store the
// result in the lower element of 'dst' using zeromask 'k' (the element is
// zeroed out when mask bit 0 is not set), and copy the upper element from 'a'
// to the upper element of 'dst'. 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 12
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		IF k[0]
//			dst[63:0] := FIXUPIMMPD(a[63:0], b[63:0], c[63:0], imm8[7:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMSD'. Intrinsic: '_mm_maskz_fixupimm_sd'.
// Requires AVX512F.
func MaskzFixupimmSd(k Mmask8, a M128d, b M128d, c M128i, imm8 int) M128d {
	return M128d(maskzFixupimmSd(uint8(k), [2]float64(a), [2]float64(b), [16]byte(c), imm8))
}

func maskzFixupimmSd(k uint8, a [2]float64, b [2]float64, c [16]byte, imm8 int) [2]float64


// FixupimmSs: Fix up the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b' using the lower 32-bit integer in 'c', store the
// result in the lower element of 'dst', and copy the upper 3 packed elements
// from 'a' to the upper elements of 'dst'. 'imm8' is used to set the required
// flags reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		dst[31:0] := FIXUPIMMPD(a[31:0], b[31:0], c[31:0], imm8[7:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMSS'. Intrinsic: '_mm_fixupimm_ss'.
// Requires AVX512F.
func FixupimmSs(a M128, b M128, c M128i, imm8 int) M128 {
	return M128(fixupimmSs([4]float32(a), [4]float32(b), [16]byte(c), imm8))
}

func fixupimmSs(a [4]float32, b [4]float32, c [16]byte, imm8 int) [4]float32


// MaskFixupimmSs: Fix up the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b' using the lower 32-bit integer in 'c', store the
// result in the lower element of 'dst' using writemask 'k' (the element is
// copied from 'a' when mask bit 0 is not set), and copy the upper 3 packed
// elements from 'a' to the upper elements of 'dst'. 'imm8' is used to set the
// required flags reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		IF k[0]
//			dst[31:0] := FIXUPIMMPD(a[31:0], b[31:0], c[31:0], imm8[7:0])
//		ELSE
//			dst[31:0] := a[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMSS'. Intrinsic: '_mm_mask_fixupimm_ss'.
// Requires AVX512F.
func MaskFixupimmSs(a M128, k Mmask8, b M128, c M128i, imm8 int) M128 {
	return M128(maskFixupimmSs([4]float32(a), uint8(k), [4]float32(b), [16]byte(c), imm8))
}

func maskFixupimmSs(a [4]float32, k uint8, b [4]float32, c [16]byte, imm8 int) [4]float32


// MaskzFixupimmSs: Fix up the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b' using the lower 32-bit integer in 'c', store the
// result in the lower element of 'dst' using zeromask 'k' (the element is
// zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements
// from 'a' to the upper elements of 'dst'. 'imm8' is used to set the required
// flags reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 12
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		IF k[0]
//			dst[31:0] := FIXUPIMMPD(a[31:0], b[31:0], c[31:0], imm8[7:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFIXUPIMMSS'. Intrinsic: '_mm_maskz_fixupimm_ss'.
// Requires AVX512F.
func MaskzFixupimmSs(k Mmask8, a M128, b M128, c M128i, imm8 int) M128 {
	return M128(maskzFixupimmSs(uint8(k), [4]float32(a), [4]float32(b), [16]byte(c), imm8))
}

func maskzFixupimmSs(k uint8, a [4]float32, b [4]float32, c [16]byte, imm8 int) [4]float32


// FloorPd: Round the packed double-precision (64-bit) floating-point elements
// in 'a' down to an integer value, and store the results as packed
// double-precision floating-point elements in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := FLOOR(a[i+63:i])
//		ENDFOR
//
// Instruction: 'ROUNDPD'. Intrinsic: '_mm_floor_pd'.
// Requires SSE4.1.
func FloorPd(a M128d) M128d {
	return M128d(floorPd([2]float64(a)))
}

func floorPd(a [2]float64) [2]float64


// FloorPs: Round the packed single-precision (32-bit) floating-point elements
// in 'a' down to an integer value, and store the results as packed
// single-precision floating-point elements in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := FLOOR(a[i+31:i])
//		ENDFOR
//
// Instruction: 'ROUNDPS'. Intrinsic: '_mm_floor_ps'.
// Requires SSE4.1.
func FloorPs(a M128) M128 {
	return M128(floorPs([4]float32(a)))
}

func floorPs(a [4]float32) [4]float32


// FloorSd: Round the lower double-precision (64-bit) floating-point element in
// 'b' down to an integer value, store the result as a double-precision
// floating-point element in the lower element of 'dst', and copy the upper
// element from 'a' to the upper element of 'dst'. 
//
//		dst[63:0] := FLOOR(b[63:0])
//		dst[127:64] := a[127:64]
//
// Instruction: 'ROUNDSD'. Intrinsic: '_mm_floor_sd'.
// Requires SSE4.1.
func FloorSd(a M128d, b M128d) M128d {
	return M128d(floorSd([2]float64(a), [2]float64(b)))
}

func floorSd(a [2]float64, b [2]float64) [2]float64


// FloorSs: Round the lower single-precision (32-bit) floating-point element in
// 'b' down to an integer value, store the result as a single-precision
// floating-point element in the lower element of 'dst', and copy the upper 3
// packed elements from 'a' to the upper elements of 'dst'. 
//
//		dst[31:0] := FLOOR(b[31:0])
//		dst[127:32] := a[127:32]
//
// Instruction: 'ROUNDSS'. Intrinsic: '_mm_floor_ss'.
// Requires SSE4.1.
func FloorSs(a M128, b M128) M128 {
	return M128(floorSs([4]float32(a), [4]float32(b)))
}

func floorSs(a [4]float32, b [4]float32) [4]float32


// FmaddPd: Multiply packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', add the intermediate result to packed elements in 'c', and
// store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132PD, VFMADD213PD, VFMADD231PD'. Intrinsic: '_mm_fmadd_pd'.
// Requires FMA.
func FmaddPd(a M128d, b M128d, c M128d) M128d {
	return M128d(fmaddPd([2]float64(a), [2]float64(b), [2]float64(c)))
}

func fmaddPd(a [2]float64, b [2]float64, c [2]float64) [2]float64


// MaskFmaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132PD, VFMADD213PD, VFMADD231PD'. Intrinsic: '_mm_mask_fmadd_pd'.
// Requires AVX512F.
func MaskFmaddPd(a M128d, k Mmask8, b M128d, c M128d) M128d {
	return M128d(maskFmaddPd([2]float64(a), uint8(k), [2]float64(b), [2]float64(c)))
}

func maskFmaddPd(a [2]float64, k uint8, b [2]float64, c [2]float64) [2]float64


// Mask3FmaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132PD, VFMADD213PD, VFMADD231PD'. Intrinsic: '_mm_mask3_fmadd_pd'.
// Requires AVX512F.
func Mask3FmaddPd(a M128d, b M128d, c M128d, k Mmask8) M128d {
	return M128d(mask3FmaddPd([2]float64(a), [2]float64(b), [2]float64(c), uint8(k)))
}

func mask3FmaddPd(a [2]float64, b [2]float64, c [2]float64, k uint8) [2]float64


// MaskzFmaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132PD, VFMADD213PD, VFMADD231PD'. Intrinsic: '_mm_maskz_fmadd_pd'.
// Requires AVX512F.
func MaskzFmaddPd(k Mmask8, a M128d, b M128d, c M128d) M128d {
	return M128d(maskzFmaddPd(uint8(k), [2]float64(a), [2]float64(b), [2]float64(c)))
}

func maskzFmaddPd(k uint8, a [2]float64, b [2]float64, c [2]float64) [2]float64


// FmaddPs: Multiply packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', add the intermediate result to packed elements in 'c', and
// store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132PS, VFMADD213PS, VFMADD231PS'. Intrinsic: '_mm_fmadd_ps'.
// Requires FMA.
func FmaddPs(a M128, b M128, c M128) M128 {
	return M128(fmaddPs([4]float32(a), [4]float32(b), [4]float32(c)))
}

func fmaddPs(a [4]float32, b [4]float32, c [4]float32) [4]float32


// MaskFmaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132PS, VFMADD213PS, VFMADD231PS'. Intrinsic: '_mm_mask_fmadd_ps'.
// Requires AVX512F.
func MaskFmaddPs(a M128, k Mmask8, b M128, c M128) M128 {
	return M128(maskFmaddPs([4]float32(a), uint8(k), [4]float32(b), [4]float32(c)))
}

func maskFmaddPs(a [4]float32, k uint8, b [4]float32, c [4]float32) [4]float32


// Mask3FmaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132PS, VFMADD213PS, VFMADD231PS'. Intrinsic: '_mm_mask3_fmadd_ps'.
// Requires AVX512F.
func Mask3FmaddPs(a M128, b M128, c M128, k Mmask8) M128 {
	return M128(mask3FmaddPs([4]float32(a), [4]float32(b), [4]float32(c), uint8(k)))
}

func mask3FmaddPs(a [4]float32, b [4]float32, c [4]float32, k uint8) [4]float32


// MaskzFmaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132PS, VFMADD213PS, VFMADD231PS'. Intrinsic: '_mm_maskz_fmadd_ps'.
// Requires AVX512F.
func MaskzFmaddPs(k Mmask8, a M128, b M128, c M128) M128 {
	return M128(maskzFmaddPs(uint8(k), [4]float32(a), [4]float32(b), [4]float32(c)))
}

func maskzFmaddPs(k uint8, a [4]float32, b [4]float32, c [4]float32) [4]float32


// MaskFmaddRoundSd: Multiply the lower double-precision (64-bit)
// floating-point elements in 'a' and 'b', and add the intermediate result to
// the lower element in 'c'. Store the result in the lower element of 'dst'
// using writemask 'k' (the element is copied from 'a' when mask bit 0 is not
// set), and copy the upper element from 'a' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := (a[63:0] * b[63:0]) + c[63:0]
//		ELSE
//			dst[63:0] := a[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132SD, VFMADD213SD, VFMADD231SD'. Intrinsic: '_mm_mask_fmadd_round_sd'.
// Requires AVX512F.
func MaskFmaddRoundSd(a M128d, k Mmask8, b M128d, c M128d, rounding int) M128d {
	return M128d(maskFmaddRoundSd([2]float64(a), uint8(k), [2]float64(b), [2]float64(c), rounding))
}

func maskFmaddRoundSd(a [2]float64, k uint8, b [2]float64, c [2]float64, rounding int) [2]float64


// Mask3FmaddRoundSd: Multiply the lower double-precision (64-bit)
// floating-point elements in 'a' and 'b', and add the intermediate result to
// the lower element in 'c'. Store the result in the lower element of 'dst'
// using writemask 'k' (the element is copied from 'c' when mask bit 0 is not
// set), and copy the upper element from 'a' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := (a[63:0] * b[63:0]) + c[63:0]
//		ELSE
//			dst[63:0] := c[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132SD, VFMADD213SD, VFMADD231SD'. Intrinsic: '_mm_mask3_fmadd_round_sd'.
// Requires AVX512F.
func Mask3FmaddRoundSd(a M128d, b M128d, c M128d, k Mmask8, rounding int) M128d {
	return M128d(mask3FmaddRoundSd([2]float64(a), [2]float64(b), [2]float64(c), uint8(k), rounding))
}

func mask3FmaddRoundSd(a [2]float64, b [2]float64, c [2]float64, k uint8, rounding int) [2]float64


// MaskzFmaddRoundSd: Multiply the lower double-precision (64-bit)
// floating-point elements in 'a' and 'b', and add the intermediate result to
// the lower element in 'c'. Store the result in the lower element of 'dst'
// using zeromask 'k' (the element is zeroed out when mask bit 0 is not set),
// and copy the upper element from 'a' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := (a[63:0] * b[63:0]) + c[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132SD, VFMADD213SD, VFMADD231SD'. Intrinsic: '_mm_maskz_fmadd_round_sd'.
// Requires AVX512F.
func MaskzFmaddRoundSd(k Mmask8, a M128d, b M128d, c M128d, rounding int) M128d {
	return M128d(maskzFmaddRoundSd(uint8(k), [2]float64(a), [2]float64(b), [2]float64(c), rounding))
}

func maskzFmaddRoundSd(k uint8, a [2]float64, b [2]float64, c [2]float64, rounding int) [2]float64


// MaskFmaddRoundSs: Multiply the lower single-precision (32-bit)
// floating-point elements in 'a' and 'b', and add the intermediate result to
// the lower element in 'c'. Store the result in the lower element of 'dst'
// using writemask 'k' (the element is copied from 'a' when mask bit 0 is not
// set), and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := (a[31:0] * b[31:0]) + c[31:0]
//		ELSE
//			dst[31:0] := a[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132SS, VFMADD213SS, VFMADD231SS'. Intrinsic: '_mm_mask_fmadd_round_ss'.
// Requires AVX512F.
func MaskFmaddRoundSs(a M128, k Mmask8, b M128, c M128, rounding int) M128 {
	return M128(maskFmaddRoundSs([4]float32(a), uint8(k), [4]float32(b), [4]float32(c), rounding))
}

func maskFmaddRoundSs(a [4]float32, k uint8, b [4]float32, c [4]float32, rounding int) [4]float32


// Mask3FmaddRoundSs: Multiply the lower single-precision (32-bit)
// floating-point elements in 'a' and 'b', and add the intermediate result to
// the lower element in 'c'. Store the result in the lower element of 'dst'
// using writemask 'k' (the element is copied from 'c' when mask bit 0 is not
// set), and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := (a[31:0] * b[31:0]) + c[31:0]
//		ELSE
//			dst[31:0] := c[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132SS, VFMADD213SS, VFMADD231SS'. Intrinsic: '_mm_mask3_fmadd_round_ss'.
// Requires AVX512F.
func Mask3FmaddRoundSs(a M128, b M128, c M128, k Mmask8, rounding int) M128 {
	return M128(mask3FmaddRoundSs([4]float32(a), [4]float32(b), [4]float32(c), uint8(k), rounding))
}

func mask3FmaddRoundSs(a [4]float32, b [4]float32, c [4]float32, k uint8, rounding int) [4]float32


// MaskzFmaddRoundSs: Multiply the lower single-precision (32-bit)
// floating-point elements in 'a' and 'b', and add the intermediate result to
// the lower element in 'c'. Store the result in the lower element of 'dst'
// using zeromask 'k' (the element is zeroed out when mask bit 0 is not set),
// and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := (a[31:0] * b[31:0]) + c[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132SS, VFMADD213SS, VFMADD231SS'. Intrinsic: '_mm_maskz_fmadd_round_ss'.
// Requires AVX512F.
func MaskzFmaddRoundSs(k Mmask8, a M128, b M128, c M128, rounding int) M128 {
	return M128(maskzFmaddRoundSs(uint8(k), [4]float32(a), [4]float32(b), [4]float32(c), rounding))
}

func maskzFmaddRoundSs(k uint8, a [4]float32, b [4]float32, c [4]float32, rounding int) [4]float32


// FmaddSd: Multiply the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', and add the intermediate result to the lower
// element in 'c'. Store the result in the lower element of 'dst', and copy the
// upper element from 'a' to the upper element of 'dst'. 
//
//		dst[63:0] := (a[63:0] * b[63:0]) + c[63:0]
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132SD, VFMADD213SD, VFMADD231SD'. Intrinsic: '_mm_fmadd_sd'.
// Requires FMA.
func FmaddSd(a M128d, b M128d, c M128d) M128d {
	return M128d(fmaddSd([2]float64(a), [2]float64(b), [2]float64(c)))
}

func fmaddSd(a [2]float64, b [2]float64, c [2]float64) [2]float64


// MaskFmaddSd: Multiply the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', and add the intermediate result to the lower
// element in 'c'. Store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'a' when mask bit 0 is not set),
// and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := (a[63:0] * b[63:0]) + c[63:0]
//		ELSE
//			dst[63:0] := a[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132SD, VFMADD213SD, VFMADD231SD'. Intrinsic: '_mm_mask_fmadd_sd'.
// Requires AVX512F.
func MaskFmaddSd(a M128d, k Mmask8, b M128d, c M128d) M128d {
	return M128d(maskFmaddSd([2]float64(a), uint8(k), [2]float64(b), [2]float64(c)))
}

func maskFmaddSd(a [2]float64, k uint8, b [2]float64, c [2]float64) [2]float64


// Mask3FmaddSd: Multiply the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', and add the intermediate result to the lower
// element in 'c'. Store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'c' when mask bit 0 is not set),
// and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := (a[63:0] * b[63:0]) + c[63:0]
//		ELSE
//			dst[63:0] := c[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132SD, VFMADD213SD, VFMADD231SD'. Intrinsic: '_mm_mask3_fmadd_sd'.
// Requires AVX512F.
func Mask3FmaddSd(a M128d, b M128d, c M128d, k Mmask8) M128d {
	return M128d(mask3FmaddSd([2]float64(a), [2]float64(b), [2]float64(c), uint8(k)))
}

func mask3FmaddSd(a [2]float64, b [2]float64, c [2]float64, k uint8) [2]float64


// MaskzFmaddSd: Multiply the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', and add the intermediate result to the lower
// element in 'c'. Store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := (a[63:0] * b[63:0]) + c[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132SD, VFMADD213SD, VFMADD231SD'. Intrinsic: '_mm_maskz_fmadd_sd'.
// Requires AVX512F.
func MaskzFmaddSd(k Mmask8, a M128d, b M128d, c M128d) M128d {
	return M128d(maskzFmaddSd(uint8(k), [2]float64(a), [2]float64(b), [2]float64(c)))
}

func maskzFmaddSd(k uint8, a [2]float64, b [2]float64, c [2]float64) [2]float64


// FmaddSs: Multiply the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', and add the intermediate result to the lower
// element in 'c'. Store the result in the lower element of 'dst', and copy the
// upper 3 packed elements from 'a' to the upper elements of 'dst'. 
//
//		dst[31:0] := (a[31:0] * b[31:0]) + c[31:0]
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132SS, VFMADD213SS, VFMADD231SS'. Intrinsic: '_mm_fmadd_ss'.
// Requires FMA.
func FmaddSs(a M128, b M128, c M128) M128 {
	return M128(fmaddSs([4]float32(a), [4]float32(b), [4]float32(c)))
}

func fmaddSs(a [4]float32, b [4]float32, c [4]float32) [4]float32


// MaskFmaddSs: Multiply the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', and add the intermediate result to the lower
// element in 'c'. Store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'a' when mask bit 0 is not set),
// and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'. 
//
//		IF k[0]
//			dst[31:0] := (a[31:0] * b[31:0]) + c[31:0]
//		ELSE
//			dst[31:0] := a[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132SS, VFMADD213SS, VFMADD231SS'. Intrinsic: '_mm_mask_fmadd_ss'.
// Requires AVX512F.
func MaskFmaddSs(a M128, k Mmask8, b M128, c M128) M128 {
	return M128(maskFmaddSs([4]float32(a), uint8(k), [4]float32(b), [4]float32(c)))
}

func maskFmaddSs(a [4]float32, k uint8, b [4]float32, c [4]float32) [4]float32


// Mask3FmaddSs: Multiply the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', and add the intermediate result to the lower
// element in 'c'. Store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'c' when mask bit 0 is not set),
// and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'. 
//
//		IF k[0]
//			dst[31:0] := (a[31:0] * b[31:0]) + c[31:0]
//		ELSE
//			dst[31:0] := c[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132SS, VFMADD213SS, VFMADD231SS'. Intrinsic: '_mm_mask3_fmadd_ss'.
// Requires AVX512F.
func Mask3FmaddSs(a M128, b M128, c M128, k Mmask8) M128 {
	return M128(mask3FmaddSs([4]float32(a), [4]float32(b), [4]float32(c), uint8(k)))
}

func mask3FmaddSs(a [4]float32, b [4]float32, c [4]float32, k uint8) [4]float32


// MaskzFmaddSs: Multiply the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', and add the intermediate result to the lower
// element in 'c'. Store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := (a[31:0] * b[31:0]) + c[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADD132SS, VFMADD213SS, VFMADD231SS'. Intrinsic: '_mm_maskz_fmadd_ss'.
// Requires AVX512F.
func MaskzFmaddSs(k Mmask8, a M128, b M128, c M128) M128 {
	return M128(maskzFmaddSs(uint8(k), [4]float32(a), [4]float32(b), [4]float32(c)))
}

func maskzFmaddSs(k uint8, a [4]float32, b [4]float32, c [4]float32) [4]float32


// FmaddsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF (j is even) 
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm_fmaddsub_pd'.
// Requires FMA.
func FmaddsubPd(a M128d, b M128d, c M128d) M128d {
	return M128d(fmaddsubPd([2]float64(a), [2]float64(b), [2]float64(c)))
}

func fmaddsubPd(a [2]float64, b [2]float64, c [2]float64) [2]float64


// MaskFmaddsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm_mask_fmaddsub_pd'.
// Requires AVX512F.
func MaskFmaddsubPd(a M128d, k Mmask8, b M128d, c M128d) M128d {
	return M128d(maskFmaddsubPd([2]float64(a), uint8(k), [2]float64(b), [2]float64(c)))
}

func maskFmaddsubPd(a [2]float64, k uint8, b [2]float64, c [2]float64) [2]float64


// Mask3FmaddsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'c' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm_mask3_fmaddsub_pd'.
// Requires AVX512F.
func Mask3FmaddsubPd(a M128d, b M128d, c M128d, k Mmask8) M128d {
	return M128d(mask3FmaddsubPd([2]float64(a), [2]float64(b), [2]float64(c), uint8(k)))
}

func mask3FmaddsubPd(a [2]float64, b [2]float64, c [2]float64, k uint8) [2]float64


// MaskzFmaddsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm_maskz_fmaddsub_pd'.
// Requires AVX512F.
func MaskzFmaddsubPd(k Mmask8, a M128d, b M128d, c M128d) M128d {
	return M128d(maskzFmaddsubPd(uint8(k), [2]float64(a), [2]float64(b), [2]float64(c)))
}

func maskzFmaddsubPd(k uint8, a [2]float64, b [2]float64, c [2]float64) [2]float64


// FmaddsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF (j is even) 
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm_fmaddsub_ps'.
// Requires FMA.
func FmaddsubPs(a M128, b M128, c M128) M128 {
	return M128(fmaddsubPs([4]float32(a), [4]float32(b), [4]float32(c)))
}

func fmaddsubPs(a [4]float32, b [4]float32, c [4]float32) [4]float32


// MaskFmaddsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm_mask_fmaddsub_ps'.
// Requires AVX512F.
func MaskFmaddsubPs(a M128, k Mmask8, b M128, c M128) M128 {
	return M128(maskFmaddsubPs([4]float32(a), uint8(k), [4]float32(b), [4]float32(c)))
}

func maskFmaddsubPs(a [4]float32, k uint8, b [4]float32, c [4]float32) [4]float32


// Mask3FmaddsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'c' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm_mask3_fmaddsub_ps'.
// Requires AVX512F.
func Mask3FmaddsubPs(a M128, b M128, c M128, k Mmask8) M128 {
	return M128(mask3FmaddsubPs([4]float32(a), [4]float32(b), [4]float32(c), uint8(k)))
}

func mask3FmaddsubPs(a [4]float32, b [4]float32, c [4]float32, k uint8) [4]float32


// MaskzFmaddsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm_maskz_fmaddsub_ps'.
// Requires AVX512F.
func MaskzFmaddsubPs(k Mmask8, a M128, b M128, c M128) M128 {
	return M128(maskzFmaddsubPs(uint8(k), [4]float32(a), [4]float32(b), [4]float32(c)))
}

func maskzFmaddsubPs(k uint8, a [4]float32, b [4]float32, c [4]float32) [4]float32


// FmsubPd: Multiply packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', subtract packed elements in 'c' from the intermediate
// result, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132PD, VFMSUB213PD, VFMSUB231PD'. Intrinsic: '_mm_fmsub_pd'.
// Requires FMA.
func FmsubPd(a M128d, b M128d, c M128d) M128d {
	return M128d(fmsubPd([2]float64(a), [2]float64(b), [2]float64(c)))
}

func fmsubPd(a [2]float64, b [2]float64, c [2]float64) [2]float64


// MaskFmsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132PD, VFMSUB213PD, VFMSUB231PD'. Intrinsic: '_mm_mask_fmsub_pd'.
// Requires AVX512F.
func MaskFmsubPd(a M128d, k Mmask8, b M128d, c M128d) M128d {
	return M128d(maskFmsubPd([2]float64(a), uint8(k), [2]float64(b), [2]float64(c)))
}

func maskFmsubPd(a [2]float64, k uint8, b [2]float64, c [2]float64) [2]float64


// Mask3FmsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132PD, VFMSUB213PD, VFMSUB231PD'. Intrinsic: '_mm_mask3_fmsub_pd'.
// Requires AVX512F.
func Mask3FmsubPd(a M128d, b M128d, c M128d, k Mmask8) M128d {
	return M128d(mask3FmsubPd([2]float64(a), [2]float64(b), [2]float64(c), uint8(k)))
}

func mask3FmsubPd(a [2]float64, b [2]float64, c [2]float64, k uint8) [2]float64


// MaskzFmsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132PD, VFMSUB213PD, VFMSUB231PD'. Intrinsic: '_mm_maskz_fmsub_pd'.
// Requires AVX512F.
func MaskzFmsubPd(k Mmask8, a M128d, b M128d, c M128d) M128d {
	return M128d(maskzFmsubPd(uint8(k), [2]float64(a), [2]float64(b), [2]float64(c)))
}

func maskzFmsubPd(k uint8, a [2]float64, b [2]float64, c [2]float64) [2]float64


// FmsubPs: Multiply packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', subtract packed elements in 'c' from the intermediate
// result, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132PS, VFMSUB213PS, VFMSUB231PS'. Intrinsic: '_mm_fmsub_ps'.
// Requires FMA.
func FmsubPs(a M128, b M128, c M128) M128 {
	return M128(fmsubPs([4]float32(a), [4]float32(b), [4]float32(c)))
}

func fmsubPs(a [4]float32, b [4]float32, c [4]float32) [4]float32


// MaskFmsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132PS, VFMSUB213PS, VFMSUB231PS'. Intrinsic: '_mm_mask_fmsub_ps'.
// Requires AVX512F.
func MaskFmsubPs(a M128, k Mmask8, b M128, c M128) M128 {
	return M128(maskFmsubPs([4]float32(a), uint8(k), [4]float32(b), [4]float32(c)))
}

func maskFmsubPs(a [4]float32, k uint8, b [4]float32, c [4]float32) [4]float32


// Mask3FmsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132PS, VFMSUB213PS, VFMSUB231PS'. Intrinsic: '_mm_mask3_fmsub_ps'.
// Requires AVX512F.
func Mask3FmsubPs(a M128, b M128, c M128, k Mmask8) M128 {
	return M128(mask3FmsubPs([4]float32(a), [4]float32(b), [4]float32(c), uint8(k)))
}

func mask3FmsubPs(a [4]float32, b [4]float32, c [4]float32, k uint8) [4]float32


// MaskzFmsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132PS, VFMSUB213PS, VFMSUB231PS'. Intrinsic: '_mm_maskz_fmsub_ps'.
// Requires AVX512F.
func MaskzFmsubPs(k Mmask8, a M128, b M128, c M128) M128 {
	return M128(maskzFmsubPs(uint8(k), [4]float32(a), [4]float32(b), [4]float32(c)))
}

func maskzFmsubPs(k uint8, a [4]float32, b [4]float32, c [4]float32) [4]float32


// MaskFmsubRoundSd: Multiply the lower double-precision (64-bit)
// floating-point elements in 'a' and 'b', and subtract the lower element in
// 'c' from the intermediate result. Store the result in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'a' when mask bit 0 is
// not set), and copy the upper element from 'a' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := (a[63:0] * b[63:0]) - c[63:0]
//		ELSE
//			dst[63:0] := a[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132SD, VFMSUB213SD, VFMSUB231SD'. Intrinsic: '_mm_mask_fmsub_round_sd'.
// Requires AVX512F.
func MaskFmsubRoundSd(a M128d, k Mmask8, b M128d, c M128d, rounding int) M128d {
	return M128d(maskFmsubRoundSd([2]float64(a), uint8(k), [2]float64(b), [2]float64(c), rounding))
}

func maskFmsubRoundSd(a [2]float64, k uint8, b [2]float64, c [2]float64, rounding int) [2]float64


// Mask3FmsubRoundSd: Multiply the lower double-precision (64-bit)
// floating-point elements in 'a' and 'b', and subtract the lower element in
// 'c' from the intermediate result. Store the result in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'c' when mask bit 0 is
// not set), and copy the upper element from 'a' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := (a[63:0] * b[63:0]) - c[63:0]
//		ELSE
//			dst[63:0] := c[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132SD, VFMSUB213SD, VFMSUB231SD'. Intrinsic: '_mm_mask3_fmsub_round_sd'.
// Requires AVX512F.
func Mask3FmsubRoundSd(a M128d, b M128d, c M128d, k Mmask8, rounding int) M128d {
	return M128d(mask3FmsubRoundSd([2]float64(a), [2]float64(b), [2]float64(c), uint8(k), rounding))
}

func mask3FmsubRoundSd(a [2]float64, b [2]float64, c [2]float64, k uint8, rounding int) [2]float64


// MaskzFmsubRoundSd: Multiply the lower double-precision (64-bit)
// floating-point elements in 'a' and 'b', and subtract the lower element in
// 'c' from the intermediate result. Store the result in the lower element of
// 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is not
// set), and copy the upper element from 'a' to the upper element of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := (a[63:0] * b[63:0]) - c[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132SD, VFMSUB213SD, VFMSUB231SD'. Intrinsic: '_mm_maskz_fmsub_round_sd'.
// Requires AVX512F.
func MaskzFmsubRoundSd(k Mmask8, a M128d, b M128d, c M128d, rounding int) M128d {
	return M128d(maskzFmsubRoundSd(uint8(k), [2]float64(a), [2]float64(b), [2]float64(c), rounding))
}

func maskzFmsubRoundSd(k uint8, a [2]float64, b [2]float64, c [2]float64, rounding int) [2]float64


// MaskFmsubRoundSs: Multiply the lower single-precision (32-bit)
// floating-point elements in 'a' and 'b', and subtract the lower element in
// 'c' from the intermediate result. Store the result in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'a' when mask bit 0 is
// not set), and copy the upper 3 packed elements from 'a' to the upper
// elements of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := (a[31:0] * b[31:0]) - c[31:0]
//		ELSE
//			dst[31:0] := a[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132SS, VFMSUB213SS, VFMSUB231SS'. Intrinsic: '_mm_mask_fmsub_round_ss'.
// Requires AVX512F.
func MaskFmsubRoundSs(a M128, k Mmask8, b M128, c M128, rounding int) M128 {
	return M128(maskFmsubRoundSs([4]float32(a), uint8(k), [4]float32(b), [4]float32(c), rounding))
}

func maskFmsubRoundSs(a [4]float32, k uint8, b [4]float32, c [4]float32, rounding int) [4]float32


// Mask3FmsubRoundSs: Multiply the lower single-precision (32-bit)
// floating-point elements in 'a' and 'b', and subtract the lower element in
// 'c' from the intermediate result. Store the result in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'c' when mask bit 0 is
// not set), and copy the upper 3 packed elements from 'a' to the upper
// elements of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := (a[31:0] * b[31:0]) - c[31:0]
//		ELSE
//			dst[31:0] := c[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132SS, VFMSUB213SS, VFMSUB231SS'. Intrinsic: '_mm_mask3_fmsub_round_ss'.
// Requires AVX512F.
func Mask3FmsubRoundSs(a M128, b M128, c M128, k Mmask8, rounding int) M128 {
	return M128(mask3FmsubRoundSs([4]float32(a), [4]float32(b), [4]float32(c), uint8(k), rounding))
}

func mask3FmsubRoundSs(a [4]float32, b [4]float32, c [4]float32, k uint8, rounding int) [4]float32


// MaskzFmsubRoundSs: Multiply the lower single-precision (32-bit)
// floating-point elements in 'a' and 'b', and subtract the lower element in
// 'c' from the intermediate result. Store the result in the lower element of
// 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is not
// set), and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := (a[31:0] * b[31:0]) - c[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132SS, VFMSUB213SS, VFMSUB231SS'. Intrinsic: '_mm_maskz_fmsub_round_ss'.
// Requires AVX512F.
func MaskzFmsubRoundSs(k Mmask8, a M128, b M128, c M128, rounding int) M128 {
	return M128(maskzFmsubRoundSs(uint8(k), [4]float32(a), [4]float32(b), [4]float32(c), rounding))
}

func maskzFmsubRoundSs(k uint8, a [4]float32, b [4]float32, c [4]float32, rounding int) [4]float32


// FmsubSd: Multiply the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', and subtract the lower element in 'c' from the
// intermediate result. Store the result in the lower element of 'dst', and
// copy the upper element from 'a' to the upper element of 'dst'. 
//
//		dst[63:0] := (a[63:0] * b[63:0]) - c[63:0]
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132SD, VFMSUB213SD, VFMSUB231SD'. Intrinsic: '_mm_fmsub_sd'.
// Requires FMA.
func FmsubSd(a M128d, b M128d, c M128d) M128d {
	return M128d(fmsubSd([2]float64(a), [2]float64(b), [2]float64(c)))
}

func fmsubSd(a [2]float64, b [2]float64, c [2]float64) [2]float64


// MaskFmsubSd: Multiply the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', and subtract the lower element in 'c' from the
// intermediate result. Store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'a' when mask bit 0 is not set),
// and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := (a[63:0] * b[63:0]) - c[63:0]
//		ELSE
//			dst[63:0] := a[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132SD, VFMSUB213SD, VFMSUB231SD'. Intrinsic: '_mm_mask_fmsub_sd'.
// Requires AVX512F.
func MaskFmsubSd(a M128d, k Mmask8, b M128d, c M128d) M128d {
	return M128d(maskFmsubSd([2]float64(a), uint8(k), [2]float64(b), [2]float64(c)))
}

func maskFmsubSd(a [2]float64, k uint8, b [2]float64, c [2]float64) [2]float64


// Mask3FmsubSd: Multiply the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', and subtract the lower element in 'c' from the
// intermediate result. Store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'c' when mask bit 0 is not set),
// and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := (a[63:0] * b[63:0]) - c[63:0]
//		ELSE
//			dst[63:0] := c[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132SD, VFMSUB213SD, VFMSUB231SD'. Intrinsic: '_mm_mask3_fmsub_sd'.
// Requires AVX512F.
func Mask3FmsubSd(a M128d, b M128d, c M128d, k Mmask8) M128d {
	return M128d(mask3FmsubSd([2]float64(a), [2]float64(b), [2]float64(c), uint8(k)))
}

func mask3FmsubSd(a [2]float64, b [2]float64, c [2]float64, k uint8) [2]float64


// MaskzFmsubSd: Multiply the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', and subtract the lower element in 'c' from the
// intermediate result. Store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := (a[63:0] * b[63:0]) - c[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132SD, VFMSUB213SD, VFMSUB231SD'. Intrinsic: '_mm_maskz_fmsub_sd'.
// Requires AVX512F.
func MaskzFmsubSd(k Mmask8, a M128d, b M128d, c M128d) M128d {
	return M128d(maskzFmsubSd(uint8(k), [2]float64(a), [2]float64(b), [2]float64(c)))
}

func maskzFmsubSd(k uint8, a [2]float64, b [2]float64, c [2]float64) [2]float64


// FmsubSs: Multiply the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', and subtract the lower element in 'c' from the
// intermediate result. Store the result in the lower element of 'dst', and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'. 
//
//		dst[31:0] := (a[31:0] * b[31:0]) - c[31:0]
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132SS, VFMSUB213SS, VFMSUB231SS'. Intrinsic: '_mm_fmsub_ss'.
// Requires FMA.
func FmsubSs(a M128, b M128, c M128) M128 {
	return M128(fmsubSs([4]float32(a), [4]float32(b), [4]float32(c)))
}

func fmsubSs(a [4]float32, b [4]float32, c [4]float32) [4]float32


// MaskFmsubSs: Multiply the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', and subtract the lower element in 'c' from the
// intermediate result. Store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'a' when mask bit 0 is not set),
// and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'. 
//
//		IF k[0]
//			dst[31:0] := (a[31:0] * b[31:0]) - c[31:0]
//		ELSE
//			dst[31:0] := a[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132SS, VFMSUB213SS, VFMSUB231SS'. Intrinsic: '_mm_mask_fmsub_ss'.
// Requires AVX512F.
func MaskFmsubSs(a M128, k Mmask8, b M128, c M128) M128 {
	return M128(maskFmsubSs([4]float32(a), uint8(k), [4]float32(b), [4]float32(c)))
}

func maskFmsubSs(a [4]float32, k uint8, b [4]float32, c [4]float32) [4]float32


// Mask3FmsubSs: Multiply the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', and subtract the lower element in 'c' from the
// intermediate result. Store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'c' when mask bit 0 is not set),
// and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'. 
//
//		IF k[0]
//			dst[31:0] := (a[31:0] * b[31:0]) - c[31:0]
//		ELSE
//			dst[31:0] := c[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132SS, VFMSUB213SS, VFMSUB231SS'. Intrinsic: '_mm_mask3_fmsub_ss'.
// Requires AVX512F.
func Mask3FmsubSs(a M128, b M128, c M128, k Mmask8) M128 {
	return M128(mask3FmsubSs([4]float32(a), [4]float32(b), [4]float32(c), uint8(k)))
}

func mask3FmsubSs(a [4]float32, b [4]float32, c [4]float32, k uint8) [4]float32


// MaskzFmsubSs: Multiply the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', and subtract the lower element in 'c' from the
// intermediate result. Store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := (a[31:0] * b[31:0]) - c[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUB132SS, VFMSUB213SS, VFMSUB231SS'. Intrinsic: '_mm_maskz_fmsub_ss'.
// Requires AVX512F.
func MaskzFmsubSs(k Mmask8, a M128, b M128, c M128) M128 {
	return M128(maskzFmsubSs(uint8(k), [4]float32(a), [4]float32(b), [4]float32(c)))
}

func maskzFmsubSs(k uint8, a [4]float32, b [4]float32, c [4]float32) [4]float32


// FmsubaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF (j is even) 
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm_fmsubadd_pd'.
// Requires FMA.
func FmsubaddPd(a M128d, b M128d, c M128d) M128d {
	return M128d(fmsubaddPd([2]float64(a), [2]float64(b), [2]float64(c)))
}

func fmsubaddPd(a [2]float64, b [2]float64, c [2]float64) [2]float64


// MaskFmsubaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 1 
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm_mask_fmsubadd_pd'.
// Requires AVX512F.
func MaskFmsubaddPd(a M128d, k Mmask8, b M128d, c M128d) M128d {
	return M128d(maskFmsubaddPd([2]float64(a), uint8(k), [2]float64(b), [2]float64(c)))
}

func maskFmsubaddPd(a [2]float64, k uint8, b [2]float64, c [2]float64) [2]float64


// Mask3FmsubaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'c' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm_mask3_fmsubadd_pd'.
// Requires AVX512F.
func Mask3FmsubaddPd(a M128d, b M128d, c M128d, k Mmask8) M128d {
	return M128d(mask3FmsubaddPd([2]float64(a), [2]float64(b), [2]float64(c), uint8(k)))
}

func mask3FmsubaddPd(a [2]float64, b [2]float64, c [2]float64, k uint8) [2]float64


// MaskzFmsubaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm_maskz_fmsubadd_pd'.
// Requires AVX512F.
func MaskzFmsubaddPd(k Mmask8, a M128d, b M128d, c M128d) M128d {
	return M128d(maskzFmsubaddPd(uint8(k), [2]float64(a), [2]float64(b), [2]float64(c)))
}

func maskzFmsubaddPd(k uint8, a [2]float64, b [2]float64, c [2]float64) [2]float64


// FmsubaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF (j is even) 
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm_fmsubadd_ps'.
// Requires FMA.
func FmsubaddPs(a M128, b M128, c M128) M128 {
	return M128(fmsubaddPs([4]float32(a), [4]float32(b), [4]float32(c)))
}

func fmsubaddPs(a [4]float32, b [4]float32, c [4]float32) [4]float32


// MaskFmsubaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm_mask_fmsubadd_ps'.
// Requires AVX512F.
func MaskFmsubaddPs(a M128, k Mmask8, b M128, c M128) M128 {
	return M128(maskFmsubaddPs([4]float32(a), uint8(k), [4]float32(b), [4]float32(c)))
}

func maskFmsubaddPs(a [4]float32, k uint8, b [4]float32, c [4]float32) [4]float32


// Mask3FmsubaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'c' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm_mask3_fmsubadd_ps'.
// Requires AVX512F.
func Mask3FmsubaddPs(a M128, b M128, c M128, k Mmask8) M128 {
	return M128(mask3FmsubaddPs([4]float32(a), [4]float32(b), [4]float32(c), uint8(k)))
}

func mask3FmsubaddPs(a [4]float32, b [4]float32, c [4]float32, k uint8) [4]float32


// MaskzFmsubaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm_maskz_fmsubadd_ps'.
// Requires AVX512F.
func MaskzFmsubaddPs(k Mmask8, a M128, b M128, c M128) M128 {
	return M128(maskzFmsubaddPs(uint8(k), [4]float32(a), [4]float32(b), [4]float32(c)))
}

func maskzFmsubaddPs(k uint8, a [4]float32, b [4]float32, c [4]float32) [4]float32


// FnmaddPd: Multiply packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', add the negated intermediate result to packed elements in
// 'c', and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
//		ENDFOR	
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132PD, VFNMADD213PD, VFNMADD231PD'. Intrinsic: '_mm_fnmadd_pd'.
// Requires FMA.
func FnmaddPd(a M128d, b M128d, c M128d) M128d {
	return M128d(fnmaddPd([2]float64(a), [2]float64(b), [2]float64(c)))
}

func fnmaddPd(a [2]float64, b [2]float64, c [2]float64) [2]float64


// MaskFnmaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR	
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132PD, VFNMADD213PD, VFNMADD231PD'. Intrinsic: '_mm_mask_fnmadd_pd'.
// Requires AVX512F.
func MaskFnmaddPd(a M128d, k Mmask8, b M128d, c M128d) M128d {
	return M128d(maskFnmaddPd([2]float64(a), uint8(k), [2]float64(b), [2]float64(c)))
}

func maskFnmaddPd(a [2]float64, k uint8, b [2]float64, c [2]float64) [2]float64


// Mask3FnmaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR	
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132PD, VFNMADD213PD, VFNMADD231PD'. Intrinsic: '_mm_mask3_fnmadd_pd'.
// Requires AVX512F.
func Mask3FnmaddPd(a M128d, b M128d, c M128d, k Mmask8) M128d {
	return M128d(mask3FnmaddPd([2]float64(a), [2]float64(b), [2]float64(c), uint8(k)))
}

func mask3FnmaddPd(a [2]float64, b [2]float64, c [2]float64, k uint8) [2]float64


// MaskzFnmaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132PD, VFNMADD213PD, VFNMADD231PD'. Intrinsic: '_mm_maskz_fnmadd_pd'.
// Requires AVX512F.
func MaskzFnmaddPd(k Mmask8, a M128d, b M128d, c M128d) M128d {
	return M128d(maskzFnmaddPd(uint8(k), [2]float64(a), [2]float64(b), [2]float64(c)))
}

func maskzFnmaddPd(k uint8, a [2]float64, b [2]float64, c [2]float64) [2]float64


// FnmaddPs: Multiply packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', add the negated intermediate result to packed elements in
// 'c', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			a[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
//		ENDFOR	
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132PS, VFNMADD213PS, VFNMADD231PS'. Intrinsic: '_mm_fnmadd_ps'.
// Requires FMA.
func FnmaddPs(a M128, b M128, c M128) M128 {
	return M128(fnmaddPs([4]float32(a), [4]float32(b), [4]float32(c)))
}

func fnmaddPs(a [4]float32, b [4]float32, c [4]float32) [4]float32


// MaskFnmaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR	
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132PS, VFNMADD213PS, VFNMADD231PS'. Intrinsic: '_mm_mask_fnmadd_ps'.
// Requires AVX512F.
func MaskFnmaddPs(a M128, k Mmask8, b M128, c M128) M128 {
	return M128(maskFnmaddPs([4]float32(a), uint8(k), [4]float32(b), [4]float32(c)))
}

func maskFnmaddPs(a [4]float32, k uint8, b [4]float32, c [4]float32) [4]float32


// Mask3FnmaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR	
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132PS, VFNMADD213PS, VFNMADD231PS'. Intrinsic: '_mm_mask3_fnmadd_ps'.
// Requires AVX512F.
func Mask3FnmaddPs(a M128, b M128, c M128, k Mmask8) M128 {
	return M128(mask3FnmaddPs([4]float32(a), [4]float32(b), [4]float32(c), uint8(k)))
}

func mask3FnmaddPs(a [4]float32, b [4]float32, c [4]float32, k uint8) [4]float32


// MaskzFnmaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132PS, VFNMADD213PS, VFNMADD231PS'. Intrinsic: '_mm_maskz_fnmadd_ps'.
// Requires AVX512F.
func MaskzFnmaddPs(k Mmask8, a M128, b M128, c M128) M128 {
	return M128(maskzFnmaddPs(uint8(k), [4]float32(a), [4]float32(b), [4]float32(c)))
}

func maskzFnmaddPs(k uint8, a [4]float32, b [4]float32, c [4]float32) [4]float32


// MaskFnmaddRoundSd: Multiply the lower double-precision (64-bit)
// floating-point elements in 'a' and 'b', and add the negated intermediate
// result to the lower element in 'c'. Store the result in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'a' when mask bit 0 is
// not set), and copy the upper element from 'a' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := -(a[63:0] * b[63:0]) + c[63:0]
//		ELSE
//			dst[63:0] := a[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132SD, VFNMADD213SD, VFNMADD231SD'. Intrinsic: '_mm_mask_fnmadd_round_sd'.
// Requires AVX512F.
func MaskFnmaddRoundSd(a M128d, k Mmask8, b M128d, c M128d, rounding int) M128d {
	return M128d(maskFnmaddRoundSd([2]float64(a), uint8(k), [2]float64(b), [2]float64(c), rounding))
}

func maskFnmaddRoundSd(a [2]float64, k uint8, b [2]float64, c [2]float64, rounding int) [2]float64


// Mask3FnmaddRoundSd: Multiply the lower double-precision (64-bit)
// floating-point elements in 'a' and 'b', and add the negated intermediate
// result to the lower element in 'c'. Store the result in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'c' when mask bit 0 is
// not set), and copy the upper element from 'a' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := -(a[63:0] * b[63:0]) + c[63:0]
//		ELSE
//			dst[63:0] := c[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132SD, VFNMADD213SD, VFNMADD231SD'. Intrinsic: '_mm_mask3_fnmadd_round_sd'.
// Requires AVX512F.
func Mask3FnmaddRoundSd(a M128d, b M128d, c M128d, k Mmask8, rounding int) M128d {
	return M128d(mask3FnmaddRoundSd([2]float64(a), [2]float64(b), [2]float64(c), uint8(k), rounding))
}

func mask3FnmaddRoundSd(a [2]float64, b [2]float64, c [2]float64, k uint8, rounding int) [2]float64


// MaskzFnmaddRoundSd: Multiply the lower double-precision (64-bit)
// floating-point elements in 'a' and 'b', and add the negated intermediate
// result to the lower element in 'c'. Store the result in the lower element of
// 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is not
// set), and copy the upper element from 'a' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := -(a[63:0] * b[63:0]) + c[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132SD, VFNMADD213SD, VFNMADD231SD'. Intrinsic: '_mm_maskz_fnmadd_round_sd'.
// Requires AVX512F.
func MaskzFnmaddRoundSd(k Mmask8, a M128d, b M128d, c M128d, rounding int) M128d {
	return M128d(maskzFnmaddRoundSd(uint8(k), [2]float64(a), [2]float64(b), [2]float64(c), rounding))
}

func maskzFnmaddRoundSd(k uint8, a [2]float64, b [2]float64, c [2]float64, rounding int) [2]float64


// MaskFnmaddRoundSs: Multiply the lower single-precision (32-bit)
// floating-point elements in 'a' and 'b', and add the negated intermediate
// result to the lower element in 'c'. Store the result in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'a' when mask bit 0 is
// not set), and copy the upper 3 packed elements from 'a' to the upper
// elements of 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := -(a[31:0] * b[31:0]) + c[31:0]
//		ELSE
//			dst[31:0] := a[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132SS, VFNMADD213SS, VFNMADD231SS'. Intrinsic: '_mm_mask_fnmadd_round_ss'.
// Requires AVX512F.
func MaskFnmaddRoundSs(a M128, k Mmask8, b M128, c M128, rounding int) M128 {
	return M128(maskFnmaddRoundSs([4]float32(a), uint8(k), [4]float32(b), [4]float32(c), rounding))
}

func maskFnmaddRoundSs(a [4]float32, k uint8, b [4]float32, c [4]float32, rounding int) [4]float32


// Mask3FnmaddRoundSs: Multiply the lower single-precision (32-bit)
// floating-point elements in 'a' and 'b', and add the negated intermediate
// result to the lower element in 'c'. Store the result in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'c' when mask bit 0 is
// not set), and copy the upper 3 packed elements from 'a' to the upper
// elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := -(a[31:0] * b[31:0]) + c[31:0]
//		ELSE
//			dst[31:0] := c[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132SS, VFNMADD213SS, VFNMADD231SS'. Intrinsic: '_mm_mask3_fnmadd_round_ss'.
// Requires AVX512F.
func Mask3FnmaddRoundSs(a M128, b M128, c M128, k Mmask8, rounding int) M128 {
	return M128(mask3FnmaddRoundSs([4]float32(a), [4]float32(b), [4]float32(c), uint8(k), rounding))
}

func mask3FnmaddRoundSs(a [4]float32, b [4]float32, c [4]float32, k uint8, rounding int) [4]float32


// MaskzFnmaddRoundSs: Multiply the lower single-precision (32-bit)
// floating-point elements in 'a' and 'b', and add the negated intermediate
// result to the lower element in 'c'. Store the result in the lower element of
// 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is not
// set), and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'. 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := -(a[31:0] * b[31:0]) + c[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132SS, VFNMADD213SS, VFNMADD231SS'. Intrinsic: '_mm_maskz_fnmadd_round_ss'.
// Requires AVX512F.
func MaskzFnmaddRoundSs(k Mmask8, a M128, b M128, c M128, rounding int) M128 {
	return M128(maskzFnmaddRoundSs(uint8(k), [4]float32(a), [4]float32(b), [4]float32(c), rounding))
}

func maskzFnmaddRoundSs(k uint8, a [4]float32, b [4]float32, c [4]float32, rounding int) [4]float32


// FnmaddSd: Multiply the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', and add the negated intermediate result to the
// lower element in 'c'. Store the result in the lower element of 'dst', and
// copy the upper element from 'a' to the upper element of 'dst'. 
//
//		dst[63:0] := -(a[63:0] * b[63:0]) + c[63:0]
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132SD, VFNMADD213SD, VFNMADD231SD'. Intrinsic: '_mm_fnmadd_sd'.
// Requires FMA.
func FnmaddSd(a M128d, b M128d, c M128d) M128d {
	return M128d(fnmaddSd([2]float64(a), [2]float64(b), [2]float64(c)))
}

func fnmaddSd(a [2]float64, b [2]float64, c [2]float64) [2]float64


// MaskFnmaddSd: Multiply the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', and add the negated intermediate result to the
// lower element in 'c'. Store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'a' when mask bit 0 is not set),
// and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := -(a[63:0] * b[63:0]) + c[63:0]
//		ELSE
//			dst[63:0] := a[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132SD, VFNMADD213SD, VFNMADD231SD'. Intrinsic: '_mm_mask_fnmadd_sd'.
// Requires AVX512F.
func MaskFnmaddSd(a M128d, k Mmask8, b M128d, c M128d) M128d {
	return M128d(maskFnmaddSd([2]float64(a), uint8(k), [2]float64(b), [2]float64(c)))
}

func maskFnmaddSd(a [2]float64, k uint8, b [2]float64, c [2]float64) [2]float64


// Mask3FnmaddSd: Multiply the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', and add the negated intermediate result to the
// lower element in 'c'. Store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'c' when mask bit 0 is not set),
// and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := -(a[63:0] * b[63:0]) + c[63:0]
//		ELSE
//			dst[63:0] := c[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132SD, VFNMADD213SD, VFNMADD231SD'. Intrinsic: '_mm_mask3_fnmadd_sd'.
// Requires AVX512F.
func Mask3FnmaddSd(a M128d, b M128d, c M128d, k Mmask8) M128d {
	return M128d(mask3FnmaddSd([2]float64(a), [2]float64(b), [2]float64(c), uint8(k)))
}

func mask3FnmaddSd(a [2]float64, b [2]float64, c [2]float64, k uint8) [2]float64


// MaskzFnmaddSd: Multiply the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', and add the negated intermediate result to the
// lower element in 'c'. Store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := -(a[63:0] * b[63:0]) + c[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD213SD, VFNMADD231SD, VFNMADD132SD'. Intrinsic: '_mm_maskz_fnmadd_sd'.
// Requires AVX512F.
func MaskzFnmaddSd(k Mmask8, a M128d, b M128d, c M128d) M128d {
	return M128d(maskzFnmaddSd(uint8(k), [2]float64(a), [2]float64(b), [2]float64(c)))
}

func maskzFnmaddSd(k uint8, a [2]float64, b [2]float64, c [2]float64) [2]float64


// FnmaddSs: Multiply the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', and add the negated intermediate result to the
// lower element in 'c'. Store the result in the lower element of 'dst', and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'. 
//
//		dst[31:0] := -(a[31:0] * b[31:0]) + c[31:0]
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132SS, VFNMADD213SS, VFNMADD231SS'. Intrinsic: '_mm_fnmadd_ss'.
// Requires FMA.
func FnmaddSs(a M128, b M128, c M128) M128 {
	return M128(fnmaddSs([4]float32(a), [4]float32(b), [4]float32(c)))
}

func fnmaddSs(a [4]float32, b [4]float32, c [4]float32) [4]float32


// MaskFnmaddSs: Multiply the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', and add the negated intermediate result to the
// lower element in 'c'. Store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'a' when mask bit 0 is not set),
// and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'. 
//
//		IF k[0]
//			dst[31:0] := -(a[31:0] * b[31:0]) + c[31:0]
//		ELSE
//			dst[31:0] := a[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132SS, VFNMADD213SS, VFNMADD231SS'. Intrinsic: '_mm_mask_fnmadd_ss'.
// Requires AVX512F.
func MaskFnmaddSs(a M128, k Mmask8, b M128, c M128) M128 {
	return M128(maskFnmaddSs([4]float32(a), uint8(k), [4]float32(b), [4]float32(c)))
}

func maskFnmaddSs(a [4]float32, k uint8, b [4]float32, c [4]float32) [4]float32


// Mask3FnmaddSs: Multiply the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', and add the negated intermediate result to the
// lower element in 'c'. Store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'c' when mask bit 0 is not set),
// and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'. 
//
//		IF k[0]
//			dst[31:0] := -(a[31:0] * b[31:0]) + c[31:0]
//		ELSE
//			dst[31:0] := c[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132SS, VFNMADD213SS, VFNMADD231SS'. Intrinsic: '_mm_mask3_fnmadd_ss'.
// Requires AVX512F.
func Mask3FnmaddSs(a M128, b M128, c M128, k Mmask8) M128 {
	return M128(mask3FnmaddSs([4]float32(a), [4]float32(b), [4]float32(c), uint8(k)))
}

func mask3FnmaddSs(a [4]float32, b [4]float32, c [4]float32, k uint8) [4]float32


// MaskzFnmaddSs: Multiply the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', and add the negated intermediate result to the
// lower element in 'c'. Store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := -(a[31:0] * b[31:0]) + c[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMADD132SS, VFNMADD213SS, VFNMADD231SS'. Intrinsic: '_mm_maskz_fnmadd_ss'.
// Requires AVX512F.
func MaskzFnmaddSs(k Mmask8, a M128, b M128, c M128) M128 {
	return M128(maskzFnmaddSs(uint8(k), [4]float32(a), [4]float32(b), [4]float32(c)))
}

func maskzFnmaddSs(k uint8, a [4]float32, b [4]float32, c [4]float32) [4]float32


// FnmsubPd: Multiply packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
//		ENDFOR	
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD'. Intrinsic: '_mm_fnmsub_pd'.
// Requires FMA.
func FnmsubPd(a M128d, b M128d, c M128d) M128d {
	return M128d(fnmsubPd([2]float64(a), [2]float64(b), [2]float64(c)))
}

func fnmsubPd(a [2]float64, b [2]float64, c [2]float64) [2]float64


// MaskFnmsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR	
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD'. Intrinsic: '_mm_mask_fnmsub_pd'.
// Requires AVX512F.
func MaskFnmsubPd(a M128d, k Mmask8, b M128d, c M128d) M128d {
	return M128d(maskFnmsubPd([2]float64(a), uint8(k), [2]float64(b), [2]float64(c)))
}

func maskFnmsubPd(a [2]float64, k uint8, b [2]float64, c [2]float64) [2]float64


// Mask3FnmsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR	
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD'. Intrinsic: '_mm_mask3_fnmsub_pd'.
// Requires AVX512F.
func Mask3FnmsubPd(a M128d, b M128d, c M128d, k Mmask8) M128d {
	return M128d(mask3FnmsubPd([2]float64(a), [2]float64(b), [2]float64(c), uint8(k)))
}

func mask3FnmsubPd(a [2]float64, b [2]float64, c [2]float64, k uint8) [2]float64


// MaskzFnmsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD'. Intrinsic: '_mm_maskz_fnmsub_pd'.
// Requires AVX512F.
func MaskzFnmsubPd(k Mmask8, a M128d, b M128d, c M128d) M128d {
	return M128d(maskzFnmsubPd(uint8(k), [2]float64(a), [2]float64(b), [2]float64(c)))
}

func maskzFnmsubPd(k uint8, a [2]float64, b [2]float64, c [2]float64) [2]float64


// FnmsubPs: Multiply packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
//		ENDFOR	
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS'. Intrinsic: '_mm_fnmsub_ps'.
// Requires FMA.
func FnmsubPs(a M128, b M128, c M128) M128 {
	return M128(fnmsubPs([4]float32(a), [4]float32(b), [4]float32(c)))
}

func fnmsubPs(a [4]float32, b [4]float32, c [4]float32) [4]float32


// MaskFnmsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR	
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS'. Intrinsic: '_mm_mask_fnmsub_ps'.
// Requires AVX512F.
func MaskFnmsubPs(a M128, k Mmask8, b M128, c M128) M128 {
	return M128(maskFnmsubPs([4]float32(a), uint8(k), [4]float32(b), [4]float32(c)))
}

func maskFnmsubPs(a [4]float32, k uint8, b [4]float32, c [4]float32) [4]float32


// Mask3FnmsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR	
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS'. Intrinsic: '_mm_mask3_fnmsub_ps'.
// Requires AVX512F.
func Mask3FnmsubPs(a M128, b M128, c M128, k Mmask8) M128 {
	return M128(mask3FnmsubPs([4]float32(a), [4]float32(b), [4]float32(c), uint8(k)))
}

func mask3FnmsubPs(a [4]float32, b [4]float32, c [4]float32, k uint8) [4]float32


// MaskzFnmsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS'. Intrinsic: '_mm_maskz_fnmsub_ps'.
// Requires AVX512F.
func MaskzFnmsubPs(k Mmask8, a M128, b M128, c M128) M128 {
	return M128(maskzFnmsubPs(uint8(k), [4]float32(a), [4]float32(b), [4]float32(c)))
}

func maskzFnmsubPs(k uint8, a [4]float32, b [4]float32, c [4]float32) [4]float32


// MaskFnmsubRoundSd: Multiply the lower double-precision (64-bit)
// floating-point elements in 'a' and 'b', and subtract the lower element in
// 'c' from the negated intermediate result. Store the result in the lower
// element of 'dst' using writemask 'k' (the element is copied from 'c' when
// mask bit 0 is not set), and copy the upper element from 'a' to the upper
// element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := -(a[63:0] * b[63:0]) - c[63:0]
//		ELSE
//			dst[63:0] := a[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132SD, VFNMSUB213SD, VFNMSUB231SD'. Intrinsic: '_mm_mask_fnmsub_round_sd'.
// Requires AVX512F.
func MaskFnmsubRoundSd(a M128d, k Mmask8, b M128d, c M128d, rounding int) M128d {
	return M128d(maskFnmsubRoundSd([2]float64(a), uint8(k), [2]float64(b), [2]float64(c), rounding))
}

func maskFnmsubRoundSd(a [2]float64, k uint8, b [2]float64, c [2]float64, rounding int) [2]float64


// Mask3FnmsubRoundSd: Multiply the lower double-precision (64-bit)
// floating-point elements in 'a' and 'b', and subtract the lower element in
// 'c' from the negated intermediate result. Store the result in the lower
// element of 'dst' using writemask 'k' (the element is copied from 'c' when
// mask bit 0 is not set), and copy the upper element from 'a' to the upper
// element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := -(a[63:0] * b[63:0]) - c[63:0]
//		ELSE
//			dst[63:0] := c[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132SD, VFNMSUB213SD, VFNMSUB231SD'. Intrinsic: '_mm_mask3_fnmsub_round_sd'.
// Requires AVX512F.
func Mask3FnmsubRoundSd(a M128d, b M128d, c M128d, k Mmask8, rounding int) M128d {
	return M128d(mask3FnmsubRoundSd([2]float64(a), [2]float64(b), [2]float64(c), uint8(k), rounding))
}

func mask3FnmsubRoundSd(a [2]float64, b [2]float64, c [2]float64, k uint8, rounding int) [2]float64


// MaskzFnmsubRoundSd: Multiply the lower double-precision (64-bit)
// floating-point elements in 'a' and 'b', and subtract the lower element in
// 'c' from the negated intermediate result. Store the result in 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper element from 'a' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := -(a[63:0] * b[63:0]) - c[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132SD, VFNMSUB213SD, VFNMSUB231SD'. Intrinsic: '_mm_maskz_fnmsub_round_sd'.
// Requires AVX512F.
func MaskzFnmsubRoundSd(k Mmask8, a M128d, b M128d, c M128d, rounding int) M128d {
	return M128d(maskzFnmsubRoundSd(uint8(k), [2]float64(a), [2]float64(b), [2]float64(c), rounding))
}

func maskzFnmsubRoundSd(k uint8, a [2]float64, b [2]float64, c [2]float64, rounding int) [2]float64


// MaskFnmsubRoundSs: Multiply the lower single-precision (32-bit)
// floating-point elements in 'a' and 'b', and subtract the lower element in
// 'c' from the negated intermediate result. Store the result in the lower
// element of 'dst' using writemask 'k' (the element is copied from 'c' when
// mask bit 0 is not set), and copy the upper 3 packed elements from 'a' to the
// upper elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := -(a[31:0] * b[31:0]) - c[31:0]
//		ELSE
//			dst[31:0] := a[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132SS, VFNMSUB213SS, VFNMSUB231SS'. Intrinsic: '_mm_mask_fnmsub_round_ss'.
// Requires AVX512F.
func MaskFnmsubRoundSs(a M128, k Mmask8, b M128, c M128, rounding int) M128 {
	return M128(maskFnmsubRoundSs([4]float32(a), uint8(k), [4]float32(b), [4]float32(c), rounding))
}

func maskFnmsubRoundSs(a [4]float32, k uint8, b [4]float32, c [4]float32, rounding int) [4]float32


// Mask3FnmsubRoundSs: Multiply the lower single-precision (32-bit)
// floating-point elements in 'a' and 'b', subtract the lower element in 'c'
// from the negated intermediate result, store the result in the lower element
// of 'dst', and copy the upper element from 'a' to the upper element of 'dst'
// using writemask 'k' (elements are copied from 'c' when the corresponding
// mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := -(a[31:0] * b[31:0]) - c[31:0]
//		ELSE
//			dst[31:0] := c[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132SS, VFNMSUB213SS, VFNMSUB231SS'. Intrinsic: '_mm_mask3_fnmsub_round_ss'.
// Requires AVX512F.
func Mask3FnmsubRoundSs(a M128, b M128, c M128, k Mmask8, rounding int) M128 {
	return M128(mask3FnmsubRoundSs([4]float32(a), [4]float32(b), [4]float32(c), uint8(k), rounding))
}

func mask3FnmsubRoundSs(a [4]float32, b [4]float32, c [4]float32, k uint8, rounding int) [4]float32


// MaskzFnmsubRoundSs: Multiply the lower single-precision (32-bit)
// floating-point elements in 'a' and 'b', and subtract the lower element in
// 'c' from the negated intermediate result. Store the result in the lower
// element of 'dst' using zeromask 'k' (the element is zeroed out when mask bit
// 0 is not set), and copy the upper 3 packed elements from 'a' to the upper
// elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := -(a[31:0] * b[31:0]) - c[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132SS, VFNMSUB213SS, VFNMSUB231SS'. Intrinsic: '_mm_maskz_fnmsub_round_ss'.
// Requires AVX512F.
func MaskzFnmsubRoundSs(k Mmask8, a M128, b M128, c M128, rounding int) M128 {
	return M128(maskzFnmsubRoundSs(uint8(k), [4]float32(a), [4]float32(b), [4]float32(c), rounding))
}

func maskzFnmsubRoundSs(k uint8, a [4]float32, b [4]float32, c [4]float32, rounding int) [4]float32


// FnmsubSd: Multiply the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', and subtract the lower element in 'c' from the
// negated intermediate result. Store the result in the lower element of 'dst',
// and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		dst[63:0] := -(a[63:0] * b[63:0]) - c[63:0]
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132SD, VFNMSUB213SD, VFNMSUB231SD'. Intrinsic: '_mm_fnmsub_sd'.
// Requires FMA.
func FnmsubSd(a M128d, b M128d, c M128d) M128d {
	return M128d(fnmsubSd([2]float64(a), [2]float64(b), [2]float64(c)))
}

func fnmsubSd(a [2]float64, b [2]float64, c [2]float64) [2]float64


// MaskFnmsubSd: Multiply the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', and subtract the lower element in 'c' from the
// negated intermediate result. Store the result in the lower element of 'dst'
// using writemask 'k' (the element is copied from 'c' when mask bit 0 is not
// set), and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := -(a[63:0] * b[63:0]) - c[63:0]
//		ELSE
//			dst[63:0] := a[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132SD, VFNMSUB213SD, VFNMSUB231SD'. Intrinsic: '_mm_mask_fnmsub_sd'.
// Requires AVX512F.
func MaskFnmsubSd(a M128d, k Mmask8, b M128d, c M128d) M128d {
	return M128d(maskFnmsubSd([2]float64(a), uint8(k), [2]float64(b), [2]float64(c)))
}

func maskFnmsubSd(a [2]float64, k uint8, b [2]float64, c [2]float64) [2]float64


// Mask3FnmsubSd: Multiply the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', and subtract the lower element in 'c' from the
// negated intermediate result. Store the result in the lower element of 'dst'
// using writemask 'k' (the element is copied from 'c' when mask bit 0 is not
// set), and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := -(a[63:0] * b[63:0]) - c[63:0]
//		ELSE
//			dst[63:0] := c[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132SD, VFNMSUB213SD, VFNMSUB231SD'. Intrinsic: '_mm_mask3_fnmsub_sd'.
// Requires AVX512F.
func Mask3FnmsubSd(a M128d, b M128d, c M128d, k Mmask8) M128d {
	return M128d(mask3FnmsubSd([2]float64(a), [2]float64(b), [2]float64(c), uint8(k)))
}

func mask3FnmsubSd(a [2]float64, b [2]float64, c [2]float64, k uint8) [2]float64


// MaskzFnmsubSd: Multiply the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', and subtract the lower element in 'c' from the
// negated intermediate result. Store the result in 'dst' using zeromask 'k'
// (the element is zeroed out when mask bit 0 is not set), and copy the upper
// element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := -(a[63:0] * b[63:0]) - c[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132SD, VFNMSUB213SD, VFNMSUB231SD'. Intrinsic: '_mm_maskz_fnmsub_sd'.
// Requires AVX512F.
func MaskzFnmsubSd(k Mmask8, a M128d, b M128d, c M128d) M128d {
	return M128d(maskzFnmsubSd(uint8(k), [2]float64(a), [2]float64(b), [2]float64(c)))
}

func maskzFnmsubSd(k uint8, a [2]float64, b [2]float64, c [2]float64) [2]float64


// FnmsubSs: Multiply the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', and subtract the lower element in 'c' from the
// negated intermediate result. Store the result in the lower element of 'dst',
// and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'. 
//
//		dst[31:0] := -(a[31:0] * b[31:0]) - c[31:0]
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132SS, VFNMSUB213SS, VFNMSUB231SS'. Intrinsic: '_mm_fnmsub_ss'.
// Requires FMA.
func FnmsubSs(a M128, b M128, c M128) M128 {
	return M128(fnmsubSs([4]float32(a), [4]float32(b), [4]float32(c)))
}

func fnmsubSs(a [4]float32, b [4]float32, c [4]float32) [4]float32


// MaskFnmsubSs: Multiply the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', and subtract the lower element in 'c' from the
// negated intermediate result. Store the result in the lower element of 'dst'
// using writemask 'k' (the element is copied from 'c' when mask bit 0 is not
// set), and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'. 
//
//		IF k[0]
//			dst[31:0] := -(a[31:0] * b[31:0]) - c[31:0]
//		ELSE
//			dst[31:0] := a[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132SS, VFNMSUB213SS, VFNMSUB231SS'. Intrinsic: '_mm_mask_fnmsub_ss'.
// Requires AVX512F.
func MaskFnmsubSs(a M128, k Mmask8, b M128, c M128) M128 {
	return M128(maskFnmsubSs([4]float32(a), uint8(k), [4]float32(b), [4]float32(c)))
}

func maskFnmsubSs(a [4]float32, k uint8, b [4]float32, c [4]float32) [4]float32


// Mask3FnmsubSs: Multiply the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', and subtract the lower element in 'c' from the
// negated intermediate result. Store the result in the lower element of 'dst',
// and copy the upper element from 'a' to the upper element of 'dst' using
// writemask 'k' (elements are copied from 'c' when the corresponding mask bit
// is not set). 
//
//		IF k[0]
//			dst[31:0] := -(a[31:0] * b[31:0]) - c[31:0]
//		ELSE
//			dst[31:0] := c[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132SS, VFNMSUB213SS, VFNMSUB231SS'. Intrinsic: '_mm_mask3_fnmsub_ss'.
// Requires AVX512F.
func Mask3FnmsubSs(a M128, b M128, c M128, k Mmask8) M128 {
	return M128(mask3FnmsubSs([4]float32(a), [4]float32(b), [4]float32(c), uint8(k)))
}

func mask3FnmsubSs(a [4]float32, b [4]float32, c [4]float32, k uint8) [4]float32


// MaskzFnmsubSs: Multiply the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', and subtract the lower element in 'c' from the
// negated intermediate result. Store the result in the lower element of 'dst'
// using zeromask 'k' (the element is zeroed out when mask bit 0 is not set),
// and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'. 
//
//		IF k[0]
//			dst[31:0] := -(a[31:0] * b[31:0]) - c[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VFNMSUB132SS, VFNMSUB213SS, VFNMSUB231SS'. Intrinsic: '_mm_maskz_fnmsub_ss'.
// Requires AVX512F.
func MaskzFnmsubSs(k Mmask8, a M128, b M128, c M128) M128 {
	return M128(maskzFnmsubSs(uint8(k), [4]float32(a), [4]float32(b), [4]float32(c)))
}

func maskzFnmsubSs(k uint8, a [4]float32, b [4]float32, c [4]float32) [4]float32


// FpclassPdMask: Test packed double-precision (64-bit) floating-point elements
// in 'a' for special categories specified by 'imm8', and store the results in
// mask vector 'k'.
// 	'imm' can be a combination of:
//     0x01 // QNaN
//     0x02 // Positive Zero
//     0x04 // Negative Zero
//     0x08 // Positive Infinity
//     0x10 // Negative Infinity
//     0x20 // Denormal
//     0x40 // Negative
//     0x80 // SNaN 
//
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := CheckFPClass_FP64(a[i+63:i], imm8[7:0])
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VFPCLASSPD'. Intrinsic: '_mm_fpclass_pd_mask'.
// Requires AVX512DQ.
func FpclassPdMask(a M128d, imm8 int) Mmask8 {
	return Mmask8(fpclassPdMask([2]float64(a), imm8))
}

func fpclassPdMask(a [2]float64, imm8 int) uint8


// MaskFpclassPdMask: Test packed double-precision (64-bit) floating-point
// elements in 'a' for special categories specified by 'imm8', and store the
// results in mask vector 'k' using zeromask 'k1' (elements are zeroed out when
// the corresponding mask bit is not set).
// 	'imm' can be a combination of:
//     0x01 // QNaN
//     0x02 // Positive Zero
//     0x04 // Negative Zero
//     0x08 // Positive Infinity
//     0x10 // Negative Infinity
//     0x20 // Denormal
//     0x40 // Negative
//     0x80 // SNaN 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := CheckFPClass_FP64(a[i+63:i], imm8[7:0])
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VFPCLASSPD'. Intrinsic: '_mm_mask_fpclass_pd_mask'.
// Requires AVX512DQ.
func MaskFpclassPdMask(k1 Mmask8, a M128d, imm8 int) Mmask8 {
	return Mmask8(maskFpclassPdMask(uint8(k1), [2]float64(a), imm8))
}

func maskFpclassPdMask(k1 uint8, a [2]float64, imm8 int) uint8


// FpclassPsMask: Test packed single-precision (32-bit) floating-point elements
// in 'a' for special categories specified by 'imm8', and store the results in
// mask vector 'k'.
// 	'imm' can be a combination of:
//     0x01 // QNaN
//     0x02 // Positive Zero
//     0x04 // Negative Zero
//     0x08 // Positive Infinity
//     0x10 // Negative Infinity
//     0x20 // Denormal
//     0x40 // Negative
//     0x80 // SNaN 
//
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := CheckFPClass_FP32(a[i+31:i], imm8[7:0])
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VFPCLASSPS'. Intrinsic: '_mm_fpclass_ps_mask'.
// Requires AVX512DQ.
func FpclassPsMask(a M128, imm8 int) Mmask8 {
	return Mmask8(fpclassPsMask([4]float32(a), imm8))
}

func fpclassPsMask(a [4]float32, imm8 int) uint8


// MaskFpclassPsMask: Test packed single-precision (32-bit) floating-point
// elements in 'a' for special categories specified by 'imm8', and store the
// results in mask vector 'k' using zeromask 'k1' (elements are zeroed out when
// the corresponding mask bit is not set).
// 	'imm' can be a combination of:
//     0x01 // QNaN
//     0x02 // Positive Zero
//     0x04 // Negative Zero
//     0x08 // Positive Infinity
//     0x10 // Negative Infinity
//     0x20 // Denormal
//     0x40 // Negative
//     0x80 // SNaN 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := CheckFPClass_FP32(a[i+31:i], imm8[7:0])
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VFPCLASSPS'. Intrinsic: '_mm_mask_fpclass_ps_mask'.
// Requires AVX512DQ.
func MaskFpclassPsMask(k1 Mmask8, a M128, imm8 int) Mmask8 {
	return Mmask8(maskFpclassPsMask(uint8(k1), [4]float32(a), imm8))
}

func maskFpclassPsMask(k1 uint8, a [4]float32, imm8 int) uint8


// FpclassSdMask: Test the lower double-precision (64-bit) floating-point
// element in 'a' for special categories specified by 'imm8', and store the
// result in mask vector 'k'.
// 	'imm' can be a combination of:
//     0x01 // QNaN
//     0x02 // Positive Zero
//     0x04 // Negative Zero
//     0x08 // Positive Infinity
//     0x10 // Negative Infinity
//     0x20 // Denormal
//     0x40 // Negative
//     0x80 // SNaN 
//
//		k[0] := CheckFPClass_FP64(a[63:0], imm8[7:0])
//		k[MAX:1] := 0
//
// Instruction: 'VFPCLASSSD'. Intrinsic: '_mm_fpclass_sd_mask'.
// Requires AVX512DQ.
func FpclassSdMask(a M128d, imm8 int) Mmask8 {
	return Mmask8(fpclassSdMask([2]float64(a), imm8))
}

func fpclassSdMask(a [2]float64, imm8 int) uint8


// MaskFpclassSdMask: Test the lower double-precision (64-bit) floating-point
// element in 'a' for special categories specified by 'imm8', and store the
// result in mask vector 'k' using zeromask 'k1' (the element is zeroed out
// when mask bit 0 is not set).
// 	'imm' can be a combination of:
//     0x01 // QNaN
//     0x02 // Positive Zero
//     0x04 // Negative Zero
//     0x08 // Positive Infinity
//     0x10 // Negative Infinity
//     0x20 // Denormal
//     0x40 // Negative
//     0x80 // SNaN 
//
//		IF k1[0]
//			k[0] := CheckFPClass_FP64(a[63:0], imm8[7:0])
//		ELSE
//			k[0] := 0
//		FI
//		k[MAX:1] := 0
//
// Instruction: 'VFPCLASSSD'. Intrinsic: '_mm_mask_fpclass_sd_mask'.
// Requires AVX512DQ.
func MaskFpclassSdMask(k1 Mmask8, a M128d, imm8 int) Mmask8 {
	return Mmask8(maskFpclassSdMask(uint8(k1), [2]float64(a), imm8))
}

func maskFpclassSdMask(k1 uint8, a [2]float64, imm8 int) uint8


// FpclassSsMask: Test the lower single-precision (32-bit) floating-point
// element in 'a' for special categories specified by 'imm8', and store the
// result in mask vector 'k.
// 	'imm" can be a combination of:
//     0x01 // QNaN
//     0x02 // Positive Zero
//     0x04 // Negative Zero
//     0x08 // Positive Infinity
//     0x10 // Negative Infinity
//     0x20 // Denormal
//     0x40 // Negative
//     0x80 // SNaN 
//
//		k[0] := CheckFPClass_FP32(a[31:0], imm8[7:0])
//		k[MAX:1] := 0
//
// Instruction: 'VFPCLASSSS'. Intrinsic: '_mm_fpclass_ss_mask'.
// Requires AVX512DQ.
func FpclassSsMask(a M128, imm8 int) Mmask8 {
	return Mmask8(fpclassSsMask([4]float32(a), imm8))
}

func fpclassSsMask(a [4]float32, imm8 int) uint8


// MaskFpclassSsMask: Test the lower single-precision (32-bit) floating-point
// element in 'a' for special categories specified by 'imm8', and store the
// result in mask vector 'k' using zeromask 'k1' (the element is zeroed out
// when mask bit 0 is not set).
// 	'imm' can be a combination of:
//     0x01 // QNaN
//     0x02 // Positive Zero
//     0x04 // Negative Zero
//     0x08 // Positive Infinity
//     0x10 // Negative Infinity
//     0x20 // Denormal
//     0x40 // Negative
//     0x80 // SNaN 
//
//		IF k1[0]
//			k[0] := CheckFPClass_FP32(a[31:0], imm8[7:0])
//		ELSE
//			k[0] := 0
//		FI
//		k[MAX:1] := 0
//
// Instruction: 'VFPCLASSSS'. Intrinsic: '_mm_mask_fpclass_ss_mask'.
// Requires AVX512DQ.
func MaskFpclassSsMask(k1 Mmask8, a M128, imm8 int) Mmask8 {
	return Mmask8(maskFpclassSsMask(uint8(k1), [4]float32(a), imm8))
}

func maskFpclassSsMask(k1 uint8, a [4]float32, imm8 int) uint8


// GetexpPd: Convert the exponent of each packed double-precision (64-bit)
// floating-point element in 'a' to a double-precision (64-bit) floating-point
// number representing the integer exponent, and store the results in 'dst'.
// This intrinsic essentially calculates 'floor(log2(x))' for each element. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := ConvertExpFP64(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPPD'. Intrinsic: '_mm_getexp_pd'.
// Requires AVX512F.
func GetexpPd(a M128d) M128d {
	return M128d(getexpPd([2]float64(a)))
}

func getexpPd(a [2]float64) [2]float64


// MaskGetexpPd: Convert the exponent of each packed double-precision (64-bit)
// floating-point element in 'a' to a double-precision (64-bit) floating-point
// number representing the integer exponent, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). This intrinsic essentially calculates 'floor(log2(x))'
// for each element. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ConvertExpFP64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPPD'. Intrinsic: '_mm_mask_getexp_pd'.
// Requires AVX512F.
func MaskGetexpPd(src M128d, k Mmask8, a M128d) M128d {
	return M128d(maskGetexpPd([2]float64(src), uint8(k), [2]float64(a)))
}

func maskGetexpPd(src [2]float64, k uint8, a [2]float64) [2]float64


// MaskzGetexpPd: Convert the exponent of each packed double-precision (64-bit)
// floating-point element in 'a' to a double-precision (64-bit) floating-point
// number representing the integer exponent, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). This intrinsic essentially calculates 'floor(log2(x))' for each
// element. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ConvertExpFP64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPPD'. Intrinsic: '_mm_maskz_getexp_pd'.
// Requires AVX512F.
func MaskzGetexpPd(k Mmask8, a M128d) M128d {
	return M128d(maskzGetexpPd(uint8(k), [2]float64(a)))
}

func maskzGetexpPd(k uint8, a [2]float64) [2]float64


// GetexpPs: Convert the exponent of each packed single-precision (32-bit)
// floating-point element in 'a' to a single-precision (32-bit) floating-point
// number representing the integer exponent, and store the results in 'dst'.
// This intrinsic essentially calculates 'floor(log2(x))' for each element. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := ConvertExpFP32(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPPS'. Intrinsic: '_mm_getexp_ps'.
// Requires AVX512F.
func GetexpPs(a M128) M128 {
	return M128(getexpPs([4]float32(a)))
}

func getexpPs(a [4]float32) [4]float32


// MaskGetexpPs: Convert the exponent of each packed single-precision (32-bit)
// floating-point element in 'a' to a single-precision (32-bit) floating-point
// number representing the integer exponent, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). This intrinsic essentially calculates 'floor(log2(x))'
// for each element. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ConvertExpFP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPPS'. Intrinsic: '_mm_mask_getexp_ps'.
// Requires AVX512F.
func MaskGetexpPs(src M128, k Mmask8, a M128) M128 {
	return M128(maskGetexpPs([4]float32(src), uint8(k), [4]float32(a)))
}

func maskGetexpPs(src [4]float32, k uint8, a [4]float32) [4]float32


// MaskzGetexpPs: Convert the exponent of each packed single-precision (32-bit)
// floating-point element in 'a' to a single-precision (32-bit) floating-point
// number representing the integer exponent, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). This intrinsic essentially calculates 'floor(log2(x))' for each
// element. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ConvertExpFP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPPS'. Intrinsic: '_mm_maskz_getexp_ps'.
// Requires AVX512F.
func MaskzGetexpPs(k Mmask8, a M128) M128 {
	return M128(maskzGetexpPs(uint8(k), [4]float32(a)))
}

func maskzGetexpPs(k uint8, a [4]float32) [4]float32


// GetexpRoundSd: Convert the exponent of the lower double-precision (64-bit)
// floating-point element in 'b' to a double-precision (64-bit) floating-point
// number representing the integer exponent, store the result in the lower
// element of 'dst', and copy the upper element from 'a' to the upper element
// of 'dst'. This intrinsic essentially calculates 'floor(log2(x))' for the
// lower element.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := ConvertExpFP64(b[63:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPSD'. Intrinsic: '_mm_getexp_round_sd'.
// Requires AVX512F.
func GetexpRoundSd(a M128d, b M128d, rounding int) M128d {
	return M128d(getexpRoundSd([2]float64(a), [2]float64(b), rounding))
}

func getexpRoundSd(a [2]float64, b [2]float64, rounding int) [2]float64


// MaskGetexpRoundSd: Convert the exponent of the lower double-precision
// (64-bit) floating-point element in 'b' to a double-precision (64-bit)
// floating-point number representing the integer exponent, store the result in
// the lower element of 'dst' using writemask 'k' (the element is copied from
// 'src' when mask bit 0 is not set), and copy the upper element from 'a' to
// the upper element of 'dst'. This intrinsic essentially calculates
// 'floor(log2(x))' for the lower element.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := ConvertExpFP64(b[63:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPSD'. Intrinsic: '_mm_mask_getexp_round_sd'.
// Requires AVX512F.
func MaskGetexpRoundSd(src M128d, k Mmask8, a M128d, b M128d, rounding int) M128d {
	return M128d(maskGetexpRoundSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), rounding))
}

func maskGetexpRoundSd(src [2]float64, k uint8, a [2]float64, b [2]float64, rounding int) [2]float64


// MaskzGetexpRoundSd: Convert the exponent of the lower double-precision
// (64-bit) floating-point element in 'b' to a double-precision (64-bit)
// floating-point number representing the integer exponent, store the result in
// the lower element of 'dst' using zeromask 'k' (the element is zeroed out
// when mask bit 0 is not set), and copy the upper element from 'a' to the
// upper element of 'dst'. This intrinsic essentially calculates
// 'floor(log2(x))' for the lower element.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := ConvertExpFP64(b[63:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPSD'. Intrinsic: '_mm_maskz_getexp_round_sd'.
// Requires AVX512F.
func MaskzGetexpRoundSd(k Mmask8, a M128d, b M128d, rounding int) M128d {
	return M128d(maskzGetexpRoundSd(uint8(k), [2]float64(a), [2]float64(b), rounding))
}

func maskzGetexpRoundSd(k uint8, a [2]float64, b [2]float64, rounding int) [2]float64


// GetexpRoundSs: Convert the exponent of the lower single-precision (32-bit)
// floating-point element in 'b' to a single-precision (32-bit) floating-point
// number representing the integer exponent, store the result in the lower
// element of 'dst', and copy the upper 3 packed elements from 'a' to the upper
// elements of 'dst'. This intrinsic essentially calculates 'floor(log2(x))'
// for the lower element.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := ConvertExpFP32(b[31:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPSS'. Intrinsic: '_mm_getexp_round_ss'.
// Requires AVX512F.
func GetexpRoundSs(a M128, b M128, rounding int) M128 {
	return M128(getexpRoundSs([4]float32(a), [4]float32(b), rounding))
}

func getexpRoundSs(a [4]float32, b [4]float32, rounding int) [4]float32


// MaskGetexpRoundSs: Convert the exponent of the lower single-precision
// (32-bit) floating-point element in 'b' to a single-precision (32-bit)
// floating-point number representing the integer exponent, store the result in
// the lower element of 'dst' using writemask 'k' (the element is copied from
// 'src' when mask bit 0 is not set), and copy the upper 3 packed elements from
// 'a' to the upper elements of 'dst'. This intrinsic essentially calculates
// 'floor(log2(x))' for the lower element.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := ConvertExpFP32(b[31:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPSS'. Intrinsic: '_mm_mask_getexp_round_ss'.
// Requires AVX512F.
func MaskGetexpRoundSs(src M128, k Mmask8, a M128, b M128, rounding int) M128 {
	return M128(maskGetexpRoundSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), rounding))
}

func maskGetexpRoundSs(src [4]float32, k uint8, a [4]float32, b [4]float32, rounding int) [4]float32


// MaskzGetexpRoundSs: Convert the exponent of the lower single-precision
// (32-bit) floating-point element in 'b' to a single-precision (32-bit)
// floating-point number representing the integer exponent, store the result in
// the lower element of 'dst' using zeromask 'k' (the element is zeroed out
// when mask bit 0 is not set), and copy the upper 3 packed elements from 'a'
// to the upper elements of 'dst'. This intrinsic essentially calculates
// 'floor(log2(x))' for the lower element.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := ConvertExpFP32(b[31:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPSS'. Intrinsic: '_mm_maskz_getexp_round_ss'.
// Requires AVX512F.
func MaskzGetexpRoundSs(k Mmask8, a M128, b M128, rounding int) M128 {
	return M128(maskzGetexpRoundSs(uint8(k), [4]float32(a), [4]float32(b), rounding))
}

func maskzGetexpRoundSs(k uint8, a [4]float32, b [4]float32, rounding int) [4]float32


// GetexpSd: Convert the exponent of the lower double-precision (64-bit)
// floating-point element in 'b' to a double-precision (64-bit) floating-point
// number representing the integer exponent, store the result in the lower
// element of 'dst', and copy the upper element from 'a' to the upper element
// of 'dst'. This intrinsic essentially calculates 'floor(log2(x))' for the
// lower element. 
//
//		dst[63:0] := ConvertExpFP64(b[63:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPSD'. Intrinsic: '_mm_getexp_sd'.
// Requires AVX512F.
func GetexpSd(a M128d, b M128d) M128d {
	return M128d(getexpSd([2]float64(a), [2]float64(b)))
}

func getexpSd(a [2]float64, b [2]float64) [2]float64


// MaskGetexpSd: Convert the exponent of the lower double-precision (64-bit)
// floating-point element in 'b' to a double-precision (64-bit) floating-point
// number representing the integer exponent, store the result in the lower
// element of 'dst' using writemask 'k' (the element is copied from 'src' when
// mask bit 0 is not set), and copy the upper element from 'a' to the upper
// element of 'dst'. This intrinsic essentially calculates 'floor(log2(x))' for
// the lower element. 
//
//		IF k[0]
//			dst[63:0] := ConvertExpFP64(b[63:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPSD'. Intrinsic: '_mm_mask_getexp_sd'.
// Requires AVX512F.
func MaskGetexpSd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskGetexpSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskGetexpSd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzGetexpSd: Convert the exponent of the lower double-precision (64-bit)
// floating-point element in 'b' to a double-precision (64-bit) floating-point
// number representing the integer exponent, store the result in the lower
// element of 'dst' using zeromask 'k' (the element is zeroed out when mask bit
// 0 is not set), and copy the upper element from 'a' to the upper element of
// 'dst'. This intrinsic essentially calculates 'floor(log2(x))' for the lower
// element. 
//
//		IF k[0]
//			dst[63:0] := ConvertExpFP64(b[63:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPSD'. Intrinsic: '_mm_maskz_getexp_sd'.
// Requires AVX512F.
func MaskzGetexpSd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzGetexpSd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzGetexpSd(k uint8, a [2]float64, b [2]float64) [2]float64


// GetexpSs: Convert the exponent of the lower single-precision (32-bit)
// floating-point element in 'b' to a single-precision (32-bit) floating-point
// number representing the integer exponent, store the result in the lower
// element of 'dst', and copy the upper 3 packed elements from 'a' to the upper
// elements of 'dst'. This intrinsic essentially calculates 'floor(log2(x))'
// for the lower element. 
//
//		dst[31:0] := ConvertExpFP32(b[31:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPSS'. Intrinsic: '_mm_getexp_ss'.
// Requires AVX512F.
func GetexpSs(a M128, b M128) M128 {
	return M128(getexpSs([4]float32(a), [4]float32(b)))
}

func getexpSs(a [4]float32, b [4]float32) [4]float32


// MaskGetexpSs: Convert the exponent of the lower single-precision (32-bit)
// floating-point element in 'b' to a single-precision (32-bit) floating-point
// number representing the integer exponent, store the result in the lower
// element of 'dst' using writemask 'k' (the element is copied from 'src' when
// mask bit 0 is not set), and copy the upper 3 packed elements from 'a' to the
// upper elements of 'dst'. This intrinsic essentially calculates
// 'floor(log2(x))' for the lower element. 
//
//		IF k[0]
//			dst[31:0] := ConvertExpFP32(b[31:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPSS'. Intrinsic: '_mm_mask_getexp_ss'.
// Requires AVX512F.
func MaskGetexpSs(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskGetexpSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskGetexpSs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzGetexpSs: Convert the exponent of the lower single-precision (32-bit)
// floating-point element in 'b' to a single-precision (32-bit) floating-point
// number representing the integer exponent, store the result in the lower
// element of 'dst' using zeromask 'k' (the element is zeroed out when mask bit
// 0 is not set), and copy the upper 3 packed elements from 'a' to the upper
// elements of 'dst'. This intrinsic essentially calculates 'floor(log2(x))'
// for the lower element. 
//
//		IF k[0]
//			dst[31:0] := ConvertExpFP32(b[31:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETEXPSS'. Intrinsic: '_mm_maskz_getexp_ss'.
// Requires AVX512F.
func MaskzGetexpSs(k Mmask8, a M128, b M128) M128 {
	return M128(maskzGetexpSs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzGetexpSs(k uint8, a [4]float32, b [4]float32) [4]float32


// GetmantPd: Normalize the mantissas of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. This
// intrinsic essentially calculates '(2^k)*|x.significand|', where 'k'
// depends on the interval range defined by 'interv' and the sign depends on
// 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTPD'. Intrinsic: '_mm_getmant_pd'.
// Requires AVX512F.
func GetmantPd(a M128d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M128d {
	return M128d(getmantPd([2]float64(a), interv, sc))
}

func getmantPd(a [2]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [2]float64


// MaskGetmantPd: Normalize the mantissas of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). This intrinsic essentially calculates
// '(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTPD'. Intrinsic: '_mm_mask_getmant_pd'.
// Requires AVX512F.
func MaskGetmantPd(src M128d, k Mmask8, a M128d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M128d {
	return M128d(maskGetmantPd([2]float64(src), uint8(k), [2]float64(a), interv, sc))
}

func maskGetmantPd(src [2]float64, k uint8, a [2]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [2]float64


// MaskzGetmantPd: Normalize the mantissas of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). This intrinsic essentially calculates '(2^k)*|x.significand|', where
// 'k' depends on the interval range defined by 'interv' and the sign depends
// on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTPD'. Intrinsic: '_mm_maskz_getmant_pd'.
// Requires AVX512F.
func MaskzGetmantPd(k Mmask8, a M128d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M128d {
	return M128d(maskzGetmantPd(uint8(k), [2]float64(a), interv, sc))
}

func maskzGetmantPd(k uint8, a [2]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [2]float64


// GetmantPs: Normalize the mantissas of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. This
// intrinsic essentially calculates '(2^k)*|x.significand|', where 'k'
// depends on the interval range defined by 'interv' and the sign depends on
// 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTPS'. Intrinsic: '_mm_getmant_ps'.
// Requires AVX512F.
func GetmantPs(a M128, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M128 {
	return M128(getmantPs([4]float32(a), interv, sc))
}

func getmantPs(a [4]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [4]float32


// MaskGetmantPs: Normalize the mantissas of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). This intrinsic essentially calculates
// '(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTPS'. Intrinsic: '_mm_mask_getmant_ps'.
// Requires AVX512F.
func MaskGetmantPs(src M128, k Mmask8, a M128, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M128 {
	return M128(maskGetmantPs([4]float32(src), uint8(k), [4]float32(a), interv, sc))
}

func maskGetmantPs(src [4]float32, k uint8, a [4]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [4]float32


// MaskzGetmantPs: Normalize the mantissas of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). This intrinsic essentially calculates '(2^k)*|x.significand|', where
// 'k' depends on the interval range defined by 'interv' and the sign depends
// on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTPS'. Intrinsic: '_mm_maskz_getmant_ps'.
// Requires AVX512F.
func MaskzGetmantPs(k Mmask8, a M128, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M128 {
	return M128(maskzGetmantPs(uint8(k), [4]float32(a), interv, sc))
}

func maskzGetmantPs(k uint8, a [4]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [4]float32


// GetmantRoundSd: Normalize the mantissas of the lower double-precision
// (64-bit) floating-point element in 'a', store the result in the lower
// element of 'dst', and copy the upper element from 'b' to the upper element
// of 'dst'. This intrinsic essentially calculates '(2^k)*|x.significand|',
// where 'k' depends on the interval range defined by 'interv' and the sign
// depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1Rounding is done according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := GetNormalizedMantissa(a[63:0], sc, interv)
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTSD'. Intrinsic: '_mm_getmant_round_sd'.
// Requires AVX512F.
func GetmantRoundSd(a M128d, b M128d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) M128d {
	return M128d(getmantRoundSd([2]float64(a), [2]float64(b), interv, sc, rounding))
}

func getmantRoundSd(a [2]float64, b [2]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) [2]float64


// MaskGetmantRoundSd: Normalize the mantissas of the lower double-precision
// (64-bit) floating-point element in 'a', store the result in the lower
// element of 'dst' using writemask 'k' (the element is copied from 'src' when
// mask bit 0 is not set), and copy the upper element from 'b' to the upper
// element of 'dst'. This intrinsic essentially calculates
// '(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1Rounding is done according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := GetNormalizedMantissa(a[63:0], sc, interv)
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTSD'. Intrinsic: '_mm_mask_getmant_round_sd'.
// Requires AVX512F.
func MaskGetmantRoundSd(src M128d, k Mmask8, a M128d, b M128d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) M128d {
	return M128d(maskGetmantRoundSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), interv, sc, rounding))
}

func maskGetmantRoundSd(src [2]float64, k uint8, a [2]float64, b [2]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) [2]float64


// MaskzGetmantRoundSd: Normalize the mantissas of the lower double-precision
// (64-bit) floating-point element in 'a', store the result in the lower
// element of 'dst' using zeromask 'k' (the element is zeroed out when mask bit
// 0 is not set), and copy the upper element from 'b' to the upper element of
// 'dst'. This intrinsic essentially calculates '(2^k)*|x.significand|',
// where 'k' depends on the interval range defined by 'interv' and the sign
// depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1Rounding is done according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := GetNormalizedMantissa(a[63:0], sc, interv)
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTSD'. Intrinsic: '_mm_maskz_getmant_round_sd'.
// Requires AVX512F.
func MaskzGetmantRoundSd(k Mmask8, a M128d, b M128d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) M128d {
	return M128d(maskzGetmantRoundSd(uint8(k), [2]float64(a), [2]float64(b), interv, sc, rounding))
}

func maskzGetmantRoundSd(k uint8, a [2]float64, b [2]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) [2]float64


// GetmantRoundSs: Normalize the mantissas of the lower single-precision
// (32-bit) floating-point element in 'a', store the result in the lower
// element of 'dst', and copy the upper 3 packed elements from 'b' to the upper
// elements of 'dst'. This intrinsic essentially calculates
// '(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1Rounding is done according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := GetNormalizedMantissa(a[31:0], sc, interv)
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTSS'. Intrinsic: '_mm_getmant_round_ss'.
// Requires AVX512F.
func GetmantRoundSs(a M128, b M128, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) M128 {
	return M128(getmantRoundSs([4]float32(a), [4]float32(b), interv, sc, rounding))
}

func getmantRoundSs(a [4]float32, b [4]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) [4]float32


// MaskGetmantRoundSs: Normalize the mantissas of the lower single-precision
// (32-bit) floating-point element in 'a', store the result in the lower
// element of 'dst' using writemask 'k' (the element is copied from 'src' when
// mask bit 0 is not set), and copy the upper 3 packed elements from 'b' to the
// upper elements of 'dst'. This intrinsic essentially calculates
// '(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1Rounding is done according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := GetNormalizedMantissa(a[31:0], sc, interv)
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTSS'. Intrinsic: '_mm_mask_getmant_round_ss'.
// Requires AVX512F.
func MaskGetmantRoundSs(src M128, k Mmask8, a M128, b M128, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) M128 {
	return M128(maskGetmantRoundSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), interv, sc, rounding))
}

func maskGetmantRoundSs(src [4]float32, k uint8, a [4]float32, b [4]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) [4]float32


// MaskzGetmantRoundSs: Normalize the mantissas of the lower single-precision
// (32-bit) floating-point element in 'a', store the result in the lower
// element of 'dst' using zeromask 'k' (the element is zeroed out when mask bit
// 0 is not set), and copy the upper 3 packed elements from 'b' to the upper
// elements of 'dst'. This intrinsic essentially calculates
// '(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1Rounding is done according to the 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := GetNormalizedMantissa(a[31:0], sc, interv)
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTSS'. Intrinsic: '_mm_maskz_getmant_round_ss'.
// Requires AVX512F.
func MaskzGetmantRoundSs(k Mmask8, a M128, b M128, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) M128 {
	return M128(maskzGetmantRoundSs(uint8(k), [4]float32(a), [4]float32(b), interv, sc, rounding))
}

func maskzGetmantRoundSs(k uint8, a [4]float32, b [4]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM, rounding int) [4]float32


// GetmantSd: Normalize the mantissas of the lower double-precision (64-bit)
// floating-point element in 'a', store the result in the lower element of
// 'dst', and copy the upper element from 'b' to the upper element of 'dst'.
// This intrinsic essentially calculates '(2^k)*|x.significand|', where 'k'
// depends on the interval range defined by 'interv' and the sign depends on
// 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		dst[63:0] := GetNormalizedMantissa(a[63:0], sc, interv)
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTSD'. Intrinsic: '_mm_getmant_sd'.
// Requires AVX512F.
func GetmantSd(a M128d, b M128d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M128d {
	return M128d(getmantSd([2]float64(a), [2]float64(b), interv, sc))
}

func getmantSd(a [2]float64, b [2]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [2]float64


// MaskGetmantSd: Normalize the mantissas of the lower double-precision
// (64-bit) floating-point element in 'a', store the result in the lower
// element of 'dst' using writemask 'k' (the element is copied from 'src' when
// mask bit 0 is not set), and copy the upper element from 'b' to the upper
// element of 'dst'. This intrinsic essentially calculates
// '(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		IF k[0]
//			dst[63:0] := GetNormalizedMantissa(a[63:0], sc, interv)
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTSD'. Intrinsic: '_mm_mask_getmant_sd'.
// Requires AVX512F.
func MaskGetmantSd(src M128d, k Mmask8, a M128d, b M128d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M128d {
	return M128d(maskGetmantSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), interv, sc))
}

func maskGetmantSd(src [2]float64, k uint8, a [2]float64, b [2]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [2]float64


// MaskzGetmantSd: Normalize the mantissas of the lower double-precision
// (64-bit) floating-point element in 'a', store the result in the lower
// element of 'dst' using zeromask 'k' (the element is zeroed out when mask bit
// 0 is not set), and copy the upper element from 'b' to the upper element of
// 'dst'. This intrinsic essentially calculates '(2^k)*|x.significand|',
// where 'k' depends on the interval range defined by 'interv' and the sign
// depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		IF k[0]
//			dst[63:0] := GetNormalizedMantissa(a[63:0], sc, interv)
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTSD'. Intrinsic: '_mm_maskz_getmant_sd'.
// Requires AVX512F.
func MaskzGetmantSd(k Mmask8, a M128d, b M128d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M128d {
	return M128d(maskzGetmantSd(uint8(k), [2]float64(a), [2]float64(b), interv, sc))
}

func maskzGetmantSd(k uint8, a [2]float64, b [2]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [2]float64


// GetmantSs: Normalize the mantissas of the lower single-precision (32-bit)
// floating-point element in 'a', store the result in the lower element of
// 'dst', and copy the upper 3 packed elements from 'b' to the upper elements
// of 'dst'. This intrinsic essentially calculates '(2^k)*|x.significand|',
// where 'k' depends on the interval range defined by 'interv' and the sign
// depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		dst[31:0] := GetNormalizedMantissa(a[31:0], sc, interv)
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTSS'. Intrinsic: '_mm_getmant_ss'.
// Requires AVX512F.
func GetmantSs(a M128, b M128, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M128 {
	return M128(getmantSs([4]float32(a), [4]float32(b), interv, sc))
}

func getmantSs(a [4]float32, b [4]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [4]float32


// MaskGetmantSs: Normalize the mantissas of the lower single-precision
// (32-bit) floating-point element in 'a', store the result in the lower
// element of 'dst' using writemask 'k' (the element is copied from 'src' when
// mask bit 0 is not set), and copy the upper 3 packed elements from 'b' to the
// upper elements of 'dst'. This intrinsic essentially calculates
// '(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		IF k[0]
//			dst[31:0] := GetNormalizedMantissa(a[31:0], sc, interv)
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTSS'. Intrinsic: '_mm_mask_getmant_ss'.
// Requires AVX512F.
func MaskGetmantSs(src M128, k Mmask8, a M128, b M128, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M128 {
	return M128(maskGetmantSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), interv, sc))
}

func maskGetmantSs(src [4]float32, k uint8, a [4]float32, b [4]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [4]float32


// MaskzGetmantSs: Normalize the mantissas of the lower single-precision
// (32-bit) floating-point element in 'a', store the result in the lower
// element of 'dst' using zeromask 'k' (the element is zeroed out when mask bit
// 0 is not set), and copy the upper 3 packed elements from 'b' to the upper
// elements of 'dst'. This intrinsic essentially calculates
// '(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		IF k[0]
//			dst[31:0] := GetNormalizedMantissa(a[31:0], sc, interv)
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VGETMANTSS'. Intrinsic: '_mm_maskz_getmant_ss'.
// Requires AVX512F.
func MaskzGetmantSs(k Mmask8, a M128, b M128, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M128 {
	return M128(maskzGetmantSs(uint8(k), [4]float32(a), [4]float32(b), interv, sc))
}

func maskzGetmantSs(k uint8, a [4]float32, b [4]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [4]float32


// Hadd16: Horizontally add adjacent pairs of 16-bit integers in 'a' and 'b',
// and pack the signed 16-bit results in 'dst'. 
//
//		dst[15:0] := a[31:16] + a[15:0]
//		dst[31:16] := a[63:48] + a[47:32]
//		dst[47:32] := a[95:80] + a[79:64]
//		dst[63:48] := a[127:112] + a[111:96]
//		dst[79:64] := b[31:16] + b[15:0]
//		dst[95:80] := b[63:48] + b[47:32]
//		dst[111:96] := b[95:80] + b[79:64]
//		dst[127:112] := b[127:112] + b[111:96]
//
// Instruction: 'PHADDW'. Intrinsic: '_mm_hadd_epi16'.
// Requires SSSE3.
func Hadd16(a M128i, b M128i) M128i {
	return M128i(hadd16([16]byte(a), [16]byte(b)))
}

func hadd16(a [16]byte, b [16]byte) [16]byte


// Hadd32: Horizontally add adjacent pairs of 32-bit integers in 'a' and 'b',
// and pack the signed 32-bit results in 'dst'. 
//
//		dst[31:0] := a[63:32] + a[31:0]
//		dst[63:32] := a[127:96] + a[95:64]
//		dst[95:64] := b[63:32] + b[31:0]
//		dst[127:96] := b[127:96] + b[95:64]
//
// Instruction: 'PHADDD'. Intrinsic: '_mm_hadd_epi32'.
// Requires SSSE3.
func Hadd32(a M128i, b M128i) M128i {
	return M128i(hadd32([16]byte(a), [16]byte(b)))
}

func hadd32(a [16]byte, b [16]byte) [16]byte


// HaddPd: Horizontally add adjacent pairs of double-precision (64-bit)
// floating-point elements in 'a' and 'b', and pack the results in 'dst'. 
//
//		dst[63:0] := a[127:64] + a[63:0]
//		dst[127:64] := b[127:64] + b[63:0]
//
// Instruction: 'HADDPD'. Intrinsic: '_mm_hadd_pd'.
// Requires SSE3.
func HaddPd(a M128d, b M128d) M128d {
	return M128d(haddPd([2]float64(a), [2]float64(b)))
}

func haddPd(a [2]float64, b [2]float64) [2]float64


// HaddPs: Horizontally add adjacent pairs of single-precision (32-bit)
// floating-point elements in 'a' and 'b', and pack the results in 'dst'. 
//
//		dst[31:0] := a[63:32] + a[31:0]
//		dst[63:32] := a[127:96] + a[95:64]
//		dst[95:64] := b[63:32] + b[31:0]
//		dst[127:96] := b[127:96] + b[95:64]
//
// Instruction: 'HADDPS'. Intrinsic: '_mm_hadd_ps'.
// Requires SSE3.
func HaddPs(a M128, b M128) M128 {
	return M128(haddPs([4]float32(a), [4]float32(b)))
}

func haddPs(a [4]float32, b [4]float32) [4]float32


// Hadds16: Horizontally add adjacent pairs of 16-bit integers in 'a' and 'b'
// using saturation, and pack the signed 16-bit results in 'dst'. 
//
//		dst[15:0]= Saturate_To_Int16(a[31:16] + a[15:0])
//		dst[31:16] = Saturate_To_Int16(a[63:48] + a[47:32])
//		dst[47:32] = Saturate_To_Int16(a[95:80] + a[79:64])
//		dst[63:48] = Saturate_To_Int16(a[127:112] + a[111:96])
//		dst[79:64] = Saturate_To_Int16(b[31:16] + b[15:0])
//		dst[95:80] = Saturate_To_Int16(b[63:48] + b[47:32])
//		dst[111:96] = Saturate_To_Int16(b[95:80] + b[79:64])
//		dst[127:112] = Saturate_To_Int16(b[127:112] + b[111:96])
//
// Instruction: 'PHADDSW'. Intrinsic: '_mm_hadds_epi16'.
// Requires SSSE3.
func Hadds16(a M128i, b M128i) M128i {
	return M128i(hadds16([16]byte(a), [16]byte(b)))
}

func hadds16(a [16]byte, b [16]byte) [16]byte


// Hsub16: Horizontally subtract adjacent pairs of 16-bit integers in 'a' and
// 'b', and pack the signed 16-bit results in 'dst'. 
//
//		dst[15:0] := a[15:0] - a[31:16]
//		dst[31:16] := a[47:32] - a[63:48]
//		dst[47:32] := a[79:64] - a[95:80]
//		dst[63:48] := a[111:96] - a[127:112]
//		dst[79:64] := b[15:0] - b[31:16]
//		dst[95:80] := b[47:32] - b[63:48]
//		dst[111:96] := b[79:64] - b[95:80]
//		dst[127:112] := b[111:96] - b[127:112]
//
// Instruction: 'PHSUBW'. Intrinsic: '_mm_hsub_epi16'.
// Requires SSSE3.
func Hsub16(a M128i, b M128i) M128i {
	return M128i(hsub16([16]byte(a), [16]byte(b)))
}

func hsub16(a [16]byte, b [16]byte) [16]byte


// Hsub32: Horizontally subtract adjacent pairs of 32-bit integers in 'a' and
// 'b', and pack the signed 32-bit results in 'dst'. 
//
//		dst[31:0] := a[31:0] - a[63:32]
//		dst[63:32] := a[95:64] - a[127:96]
//		dst[95:64] := b[31:0] - b[63:32]
//		dst[127:96] := b[95:64] - b[127:96]
//
// Instruction: 'PHSUBD'. Intrinsic: '_mm_hsub_epi32'.
// Requires SSSE3.
func Hsub32(a M128i, b M128i) M128i {
	return M128i(hsub32([16]byte(a), [16]byte(b)))
}

func hsub32(a [16]byte, b [16]byte) [16]byte


// HsubPd: Horizontally subtract adjacent pairs of double-precision (64-bit)
// floating-point elements in 'a' and 'b', and pack the results in 'dst'. 
//
//		dst[63:0] := a[63:0] - a[127:64]
//		dst[127:64] := b[63:0] - b[127:64]
//
// Instruction: 'HSUBPD'. Intrinsic: '_mm_hsub_pd'.
// Requires SSE3.
func HsubPd(a M128d, b M128d) M128d {
	return M128d(hsubPd([2]float64(a), [2]float64(b)))
}

func hsubPd(a [2]float64, b [2]float64) [2]float64


// HsubPs: Horizontally add adjacent pairs of single-precision (32-bit)
// floating-point elements in 'a' and 'b', and pack the results in 'dst'. 
//
//		dst[31:0] := a[31:0] - a[63:32]
//		dst[63:32] := a[95:64] - a[127:96]
//		dst[95:64] := b[31:0] - b[63:32]
//		dst[127:96] := b[95:64] - b[127:96]
//
// Instruction: 'HSUBPS'. Intrinsic: '_mm_hsub_ps'.
// Requires SSE3.
func HsubPs(a M128, b M128) M128 {
	return M128(hsubPs([4]float32(a), [4]float32(b)))
}

func hsubPs(a [4]float32, b [4]float32) [4]float32


// Hsubs16: Horizontally subtract adjacent pairs of 16-bit integers in 'a' and
// 'b' using saturation, and pack the signed 16-bit results in 'dst'. 
//
//		dst[15:0]= Saturate_To_Int16(a[15:0] - a[31:16])
//		dst[31:16] = Saturate_To_Int16(a[47:32] - a[63:48])
//		dst[47:32] = Saturate_To_Int16(a[79:64] - a[95:80])
//		dst[63:48] = Saturate_To_Int16(a[111:96] - a[127:112])
//		dst[79:64] = Saturate_To_Int16(b[15:0] - b[31:16])
//		dst[95:80] = Saturate_To_Int16(b[47:32] - b[63:48])
//		dst[111:96] = Saturate_To_Int16(b[79:64] - b[95:80])
//		dst[127:112] = Saturate_To_Int16(b[111:96] - b[127:112])
//
// Instruction: 'PHSUBSW'. Intrinsic: '_mm_hsubs_epi16'.
// Requires SSSE3.
func Hsubs16(a M128i, b M128i) M128i {
	return M128i(hsubs16([16]byte(a), [16]byte(b)))
}

func hsubs16(a [16]byte, b [16]byte) [16]byte


// HypotPd: Compute the length of the hypotenous of a right triangle, with the
// lengths of the other two sides of the triangle stored as packed
// double-precision (64-bit) floating-point elements in 'a' and 'b', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := SQRT(a[i+63:i]^2 + b[i+63:i]^2)
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_hypot_pd'.
// Requires SSE.
func HypotPd(a M128d, b M128d) M128d {
	return M128d(hypotPd([2]float64(a), [2]float64(b)))
}

func hypotPd(a [2]float64, b [2]float64) [2]float64


// HypotPs: Compute the length of the hypotenous of a right triangle, with the
// lengths of the other two sides of the triangle stored as packed
// single-precision (32-bit) floating-point elements in 'a' and 'b', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := SQRT(a[i+31:i]^2 + b[i+31:i]^2)
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_hypot_ps'.
// Requires SSE.
func HypotPs(a M128, b M128) M128 {
	return M128(hypotPs([4]float32(a), [4]float32(b)))
}

func hypotPs(a [4]float32, b [4]float32) [4]float32


// I32gather32: Gather 32-bit integers from memory using 32-bit indices. 32-bit
// elements are loaded from addresses starting at 'base_addr' and offset by
// each 32-bit element in 'vindex' (each index is scaled by the factor in
// 'scale'). Gathered elements are merged into 'dst'. 'scale' should be 1, 2, 4
// or 8. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+31:i])*scale]
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPGATHERDD'. Intrinsic: '_mm_i32gather_epi32'.
// Requires AVX2.
func I32gather32(base_addr int, vindex M128i, scale int) M128i {
	return M128i(i32gather32(base_addr, [16]byte(vindex), scale))
}

func i32gather32(base_addr int, vindex [16]byte, scale int) [16]byte


// MaskI32gather32: Gather 32-bit integers from memory using 32-bit indices.
// 32-bit elements are loaded from addresses starting at 'base_addr' and offset
// by each 32-bit element in 'vindex' (each index is scaled by the factor in
// 'scale'). Gathered elements are merged into 'dst' using 'mask' (elements are
// copied from 'src' when the highest bit is not set in the corresponding
// element). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF mask[i+31]
//				dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+31:i])*scale]
//				mask[i+31] := 0
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		mask[MAX:128] := 0
//		dst[MAX:128] := 0
//
// Instruction: 'VPGATHERDD'. Intrinsic: '_mm_mask_i32gather_epi32'.
// Requires AVX2.
func MaskI32gather32(src M128i, base_addr int, vindex M128i, mask M128i, scale int) M128i {
	return M128i(maskI32gather32([16]byte(src), base_addr, [16]byte(vindex), [16]byte(mask), scale))
}

func maskI32gather32(src [16]byte, base_addr int, vindex [16]byte, mask [16]byte, scale int) [16]byte


// MmaskI32gather32: Gather 32-bit integers from memory using 32-bit indices.
// 32-bit elements are loaded from addresses starting at 'base_addr' and offset
// by each 32-bit element in 'vindex' (each index is scaled by the factor in
// 'scale'). Gathered elements are merged into 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+31:i])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//		dst[MAX:128] := 0
//
// Instruction: 'VPGATHERDD'. Intrinsic: '_mm_mmask_i32gather_epi32'.
// Requires AVX512F.
func MmaskI32gather32(src M128i, k Mmask8, vindex M128i, base_addr uintptr, scale int) M128i {
	return M128i(mmaskI32gather32([16]byte(src), uint8(k), [16]byte(vindex), uintptr(base_addr), scale))
}

func mmaskI32gather32(src [16]byte, k uint8, vindex [16]byte, base_addr uintptr, scale int) [16]byte


// I32gather64: Gather 64-bit integers from memory using 32-bit indices. 64-bit
// elements are loaded from addresses starting at 'base_addr' and offset by
// each 32-bit element in 'vindex' (each index is scaled by the factor in
// 'scale'). Gathered elements are merged into 'dst'. 'scale' should be 1, 2, 4
// or 8. 
//
//		FOR j := 0 to 1
//			i := j*64
//			m := j*32
//			dst[i+63:i] := MEM[base_addr + SignExtend(vindex[m+31:m])*scale]
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPGATHERDQ'. Intrinsic: '_mm_i32gather_epi64'.
// Requires AVX2.
func I32gather64(base_addr int, vindex M128i, scale int) M128i {
	return M128i(i32gather64(base_addr, [16]byte(vindex), scale))
}

func i32gather64(base_addr int, vindex [16]byte, scale int) [16]byte


// MaskI32gather64: Gather 64-bit integers from memory using 32-bit indices.
// 64-bit elements are loaded from addresses starting at 'base_addr' and offset
// by each 32-bit element in 'vindex' (each index is scaled by the factor in
// 'scale'). Gathered elements are merged into 'dst' using 'mask' (elements are
// copied from 'src' when the highest bit is not set in the corresponding
// element). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 1
//			i := j*64
//			m := j*32
//			IF mask[i+63]
//				dst[i+63:i] := MEM[base_addr + SignExtend(vindex[m+31:m])*scale]
//				mask[i+63] := 0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		mask[MAX:128] := 0
//		dst[MAX:128] := 0
//
// Instruction: 'VPGATHERDQ'. Intrinsic: '_mm_mask_i32gather_epi64'.
// Requires AVX2.
func MaskI32gather64(src M128i, base_addr int, vindex M128i, mask M128i, scale int) M128i {
	return M128i(maskI32gather64([16]byte(src), base_addr, [16]byte(vindex), [16]byte(mask), scale))
}

func maskI32gather64(src [16]byte, base_addr int, vindex [16]byte, mask [16]byte, scale int) [16]byte


// MmaskI32gather64: Gather 64-bit integers from memory using 32-bit indices.
// 64-bit elements are loaded from addresses starting at 'base_addr' and offset
// by each 32-bit element in 'vindex' (each index is scaled by the factor in
// 'scale'). Gathered elements are merged into 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 1
//			i := j*64
//			m := j*32
//			IF k[j]
//				dst[i+63:i] := MEM[base_addr + SignExtend(vindex[m+31:m])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//		dst[MAX:128] := 0
//
// Instruction: 'VPGATHERDQ'. Intrinsic: '_mm_mmask_i32gather_epi64'.
// Requires AVX512F.
func MmaskI32gather64(src M128i, k Mmask8, vindex M128i, base_addr uintptr, scale int) M128i {
	return M128i(mmaskI32gather64([16]byte(src), uint8(k), [16]byte(vindex), uintptr(base_addr), scale))
}

func mmaskI32gather64(src [16]byte, k uint8, vindex [16]byte, base_addr uintptr, scale int) [16]byte


// I32gatherPd: Gather double-precision (64-bit) floating-point elements from
// memory using 32-bit indices. 64-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 32-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst'. 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 1
//			i := j*64
//			m := j*32
//			dst[i+63:i] := MEM[base_addr + SignExtend(vindex[m+31:m])*scale]
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VGATHERDPD'. Intrinsic: '_mm_i32gather_pd'.
// Requires AVX2.
func I32gatherPd(base_addr float64, vindex M128i, scale int) M128d {
	return M128d(i32gatherPd(base_addr, [16]byte(vindex), scale))
}

func i32gatherPd(base_addr float64, vindex [16]byte, scale int) [2]float64


// MaskI32gatherPd: Gather double-precision (64-bit) floating-point elements
// from memory using 32-bit indices. 64-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 32-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst' using 'mask' (elements are copied from 'src' when the highest bit is
// not set in the corresponding element). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 1
//			i := j*64
//			m := j*32
//			IF mask[i+63]
//				dst[i+63:i] := MEM[base_addr + SignExtend(vindex[m+31:m])*scale]
//				mask[i+63] := 0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		mask[MAX:128] := 0
//		dst[MAX:128] := 0
//
// Instruction: 'VGATHERDPD'. Intrinsic: '_mm_mask_i32gather_pd'.
// Requires AVX2.
func MaskI32gatherPd(src M128d, base_addr float64, vindex M128i, mask M128d, scale int) M128d {
	return M128d(maskI32gatherPd([2]float64(src), base_addr, [16]byte(vindex), [2]float64(mask), scale))
}

func maskI32gatherPd(src [2]float64, base_addr float64, vindex [16]byte, mask [2]float64, scale int) [2]float64


// MmaskI32gatherPd: Gather double-precision (64-bit) floating-point elements
// from memory using 32-bit indices. 64-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 32-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 1
//			i := j*64
//			m := j*32
//			IF k[j]
//				dst[i+63:i] := MEM[base_addr + SignExtend(vindex[m+31:m])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//		dst[MAX:128] := 0
//
// Instruction: 'VGATHERDPD'. Intrinsic: '_mm_mmask_i32gather_pd'.
// Requires AVX512F.
func MmaskI32gatherPd(src M128d, k Mmask8, vindex M128i, base_addr uintptr, scale int) M128d {
	return M128d(mmaskI32gatherPd([2]float64(src), uint8(k), [16]byte(vindex), uintptr(base_addr), scale))
}

func mmaskI32gatherPd(src [2]float64, k uint8, vindex [16]byte, base_addr uintptr, scale int) [2]float64


// I32gatherPs: Gather single-precision (32-bit) floating-point elements from
// memory using 32-bit indices. 32-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 32-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst'. 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+31:i])*scale]
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VGATHERDPS'. Intrinsic: '_mm_i32gather_ps'.
// Requires AVX2.
func I32gatherPs(base_addr float32, vindex M128i, scale int) M128 {
	return M128(i32gatherPs(base_addr, [16]byte(vindex), scale))
}

func i32gatherPs(base_addr float32, vindex [16]byte, scale int) [4]float32


// MaskI32gatherPs: Gather single-precision (32-bit) floating-point elements
// from memory using 32-bit indices. 32-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 32-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst' using 'mask' (elements are copied from 'src' when the highest bit is
// not set in the corresponding element). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF mask[i+31]
//				dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+31:i])*scale]
//				mask[i+31] := 0
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		mask[MAX:128] := 0
//		dst[MAX:128] := 0
//
// Instruction: 'VGATHERDPS'. Intrinsic: '_mm_mask_i32gather_ps'.
// Requires AVX2.
func MaskI32gatherPs(src M128, base_addr float32, vindex M128i, mask M128, scale int) M128 {
	return M128(maskI32gatherPs([4]float32(src), base_addr, [16]byte(vindex), [4]float32(mask), scale))
}

func maskI32gatherPs(src [4]float32, base_addr float32, vindex [16]byte, mask [4]float32, scale int) [4]float32


// MmaskI32gatherPs: Gather single-precision (32-bit) floating-point elements
// from memory using 32-bit indices. 32-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 32-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+31:i])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//		dst[MAX:128] := 0
//
// Instruction: 'VGATHERDPS'. Intrinsic: '_mm_mmask_i32gather_ps'.
// Requires AVX512F.
func MmaskI32gatherPs(src M128, k Mmask8, vindex M128i, base_addr uintptr, scale int) M128 {
	return M128(mmaskI32gatherPs([4]float32(src), uint8(k), [16]byte(vindex), uintptr(base_addr), scale))
}

func mmaskI32gatherPs(src [4]float32, k uint8, vindex [16]byte, base_addr uintptr, scale int) [4]float32


// I32scatter32: Scatter 32-bit integers from 'a' into memory using 32-bit
// indices. 32-bit elements are stored at addresses starting at 'base_addr' and
// offset by each 32-bit element in 'vindex' (each index is scaled by the
// factor in 'scale'). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*32
//			MEM[base_addr + SignExtend(vindex[i+31:i])*scale] := a[i+31:i]
//		ENDFOR
//
// Instruction: 'VPSCATTERDD'. Intrinsic: '_mm_i32scatter_epi32'.
// Requires AVX512F.
func I32scatter32(base_addr uintptr, vindex M128i, a M128i, scale int)  {
	i32scatter32(uintptr(base_addr), [16]byte(vindex), [16]byte(a), scale)
}

func i32scatter32(base_addr uintptr, vindex [16]byte, a [16]byte, scale int) 


// MaskI32scatter32: Scatter 32-bit integers from 'a' into memory using 32-bit
// indices. 32-bit elements are stored at addresses starting at 'base_addr' and
// offset by each 32-bit element in 'vindex' (each index is scaled by the
// factor in 'scale') subject to mask 'k' (elements are not stored when the
// corresponding mask bit is not set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[i+31:i])*scale] := a[i+31:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPSCATTERDD'. Intrinsic: '_mm_mask_i32scatter_epi32'.
// Requires AVX512F.
func MaskI32scatter32(base_addr uintptr, k Mmask8, vindex M128i, a M128i, scale int)  {
	maskI32scatter32(uintptr(base_addr), uint8(k), [16]byte(vindex), [16]byte(a), scale)
}

func maskI32scatter32(base_addr uintptr, k uint8, vindex [16]byte, a [16]byte, scale int) 


// I32scatter64: Scatter 64-bit integers from 'a' into memory using 32-bit
// indices. 64-bit elements are stored at addresses starting at 'base_addr' and
// offset by each 32-bit element in 'vindex' (each index is scaled by the
// factor in 'scale'). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 1
//			i := j*64
//			l := j*32
//			MEM[base_addr + SignExtend(vindex[l+31:l])*scale] := a[i+63:i]
//		ENDFOR
//
// Instruction: 'VPSCATTERDQ'. Intrinsic: '_mm_i32scatter_epi64'.
// Requires AVX512F.
func I32scatter64(base_addr uintptr, vindex M128i, a M128i, scale int)  {
	i32scatter64(uintptr(base_addr), [16]byte(vindex), [16]byte(a), scale)
}

func i32scatter64(base_addr uintptr, vindex [16]byte, a [16]byte, scale int) 


// MaskI32scatter64: Scatter 64-bit integers from 'a' into memory using 32-bit
// indices. 64-bit elements are stored at addresses starting at 'base_addr' and
// offset by each 32-bit element in 'vindex' (each index is scaled by the
// factor in 'scale') subject to mask 'k' (elements are not stored when the
// corresponding mask bit is not set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 1
//			i := j*64
//			l := j*32
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[l+31:l])*scale] := a[i+63:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPSCATTERDQ'. Intrinsic: '_mm_mask_i32scatter_epi64'.
// Requires AVX512F.
func MaskI32scatter64(base_addr uintptr, k Mmask8, vindex M128i, a M128i, scale int)  {
	maskI32scatter64(uintptr(base_addr), uint8(k), [16]byte(vindex), [16]byte(a), scale)
}

func maskI32scatter64(base_addr uintptr, k uint8, vindex [16]byte, a [16]byte, scale int) 


// I32scatterPd: Scatter double-precision (64-bit) floating-point elements from
// 'a' into memory using 32-bit indices. 64-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 32-bit element in
// 'vindex' (each index is scaled by the factor in 'scale'). 'scale' should be
// 1, 2, 4 or 8. 
//
//		FOR j := 0 to 1
//			i := j*64
//			l := j*32
//			MEM[base_addr + SignExtend(vindex[l+31:l])*scale] := a[i+63:i]
//		ENDFOR
//
// Instruction: 'VSCATTERDPD'. Intrinsic: '_mm_i32scatter_pd'.
// Requires AVX512F.
func I32scatterPd(base_addr uintptr, vindex M128i, a M128d, scale int)  {
	i32scatterPd(uintptr(base_addr), [16]byte(vindex), [2]float64(a), scale)
}

func i32scatterPd(base_addr uintptr, vindex [16]byte, a [2]float64, scale int) 


// MaskI32scatterPd: Scatter double-precision (64-bit) floating-point elements
// from 'a' into memory using 32-bit indices. 64-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 32-bit element in
// 'vindex' (each index is scaled by the factor in 'scale') subject to mask 'k'
// (elements are not stored when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 1
//			i := j*64
//			l := j*32
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[l+31:l])*scale] := a[i+63:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VSCATTERDPD'. Intrinsic: '_mm_mask_i32scatter_pd'.
// Requires AVX512F.
func MaskI32scatterPd(base_addr uintptr, k Mmask8, vindex M128i, a M128d, scale int)  {
	maskI32scatterPd(uintptr(base_addr), uint8(k), [16]byte(vindex), [2]float64(a), scale)
}

func maskI32scatterPd(base_addr uintptr, k uint8, vindex [16]byte, a [2]float64, scale int) 


// I32scatterPs: Scatter single-precision (32-bit) floating-point elements from
// 'a' into memory using 32-bit indices. 32-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 32-bit element in
// 'vindex' (each index is scaled by the factor in 'scale'). 'scale' should be
// 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*32
//			MEM[base_addr + SignExtend(vindex[i+31:i])*scale] := a[i+31:i]
//		ENDFOR
//
// Instruction: 'VSCATTERDPS'. Intrinsic: '_mm_i32scatter_ps'.
// Requires AVX512F.
func I32scatterPs(base_addr uintptr, vindex M128i, a M128, scale int)  {
	i32scatterPs(uintptr(base_addr), [16]byte(vindex), [4]float32(a), scale)
}

func i32scatterPs(base_addr uintptr, vindex [16]byte, a [4]float32, scale int) 


// MaskI32scatterPs: Scatter single-precision (32-bit) floating-point elements
// from 'a' into memory using 32-bit indices. 32-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 32-bit element in
// 'vindex' (each index is scaled by the factor in 'scale') subject to mask 'k'
// (elements are not stored when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[i+31:i])*scale] := a[i+31:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VSCATTERDPS'. Intrinsic: '_mm_mask_i32scatter_ps'.
// Requires AVX512F.
func MaskI32scatterPs(base_addr uintptr, k Mmask8, vindex M128i, a M128, scale int)  {
	maskI32scatterPs(uintptr(base_addr), uint8(k), [16]byte(vindex), [4]float32(a), scale)
}

func maskI32scatterPs(base_addr uintptr, k uint8, vindex [16]byte, a [4]float32, scale int) 


// I64gather32: Gather 32-bit integers from memory using 64-bit indices. 32-bit
// elements are loaded from addresses starting at 'base_addr' and offset by
// each 64-bit element in 'vindex' (each index is scaled by the factor in
// 'scale'). Gathered elements are merged into 'dst'. 'scale' should be 1, 2, 4
// or 8. 
//
//		FOR j := 0 to 1
//			i := j*32
//			m := j*64
//			dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPGATHERQD'. Intrinsic: '_mm_i64gather_epi32'.
// Requires AVX2.
func I64gather32(base_addr int, vindex M128i, scale int) M128i {
	return M128i(i64gather32(base_addr, [16]byte(vindex), scale))
}

func i64gather32(base_addr int, vindex [16]byte, scale int) [16]byte


// MaskI64gather32: Gather 32-bit integers from memory using 64-bit indices.
// 32-bit elements are loaded from addresses starting at 'base_addr' and offset
// by each 64-bit element in 'vindex' (each index is scaled by the factor in
// 'scale'). Gathered elements are merged into 'dst' using 'mask' (elements are
// copied from 'src' when the highest bit is not set in the corresponding
// element). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 1
//			i := j*32
//			m := j*64
//			IF mask[i+31]
//				dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//				mask[i+31] := 0
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		mask[MAX:64] := 0
//		dst[MAX:64] := 0
//
// Instruction: 'VPGATHERQD'. Intrinsic: '_mm_mask_i64gather_epi32'.
// Requires AVX2.
func MaskI64gather32(src M128i, base_addr int, vindex M128i, mask M128i, scale int) M128i {
	return M128i(maskI64gather32([16]byte(src), base_addr, [16]byte(vindex), [16]byte(mask), scale))
}

func maskI64gather32(src [16]byte, base_addr int, vindex [16]byte, mask [16]byte, scale int) [16]byte


// MmaskI64gather32: Gather 32-bit integers from memory using 64-bit indices.
// 32-bit elements are loaded from addresses starting at 'base_addr' and offset
// by each 64-bit element in 'vindex' (each index is scaled by the factor in
// 'scale'). Gathered elements are merged into 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 1
//			i := j*32
//			m := j*64
//			IF k[j]
//				dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//		dst[MAX:64] := 0
//
// Instruction: 'VPGATHERQD'. Intrinsic: '_mm_mmask_i64gather_epi32'.
// Requires AVX512F.
func MmaskI64gather32(src M128i, k Mmask8, vindex M128i, base_addr uintptr, scale int) M128i {
	return M128i(mmaskI64gather32([16]byte(src), uint8(k), [16]byte(vindex), uintptr(base_addr), scale))
}

func mmaskI64gather32(src [16]byte, k uint8, vindex [16]byte, base_addr uintptr, scale int) [16]byte


// I64gather64: Gather 64-bit integers from memory using 64-bit indices. 64-bit
// elements are loaded from addresses starting at 'base_addr' and offset by
// each 64-bit element in 'vindex' (each index is scaled by the factor in
// 'scale'). Gathered elements are merged into 'dst'. 'scale' should be 1, 2, 4
// or 8. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPGATHERQQ'. Intrinsic: '_mm_i64gather_epi64'.
// Requires AVX2.
func I64gather64(base_addr int, vindex M128i, scale int) M128i {
	return M128i(i64gather64(base_addr, [16]byte(vindex), scale))
}

func i64gather64(base_addr int, vindex [16]byte, scale int) [16]byte


// MaskI64gather64: Gather 64-bit integers from memory using 64-bit indices.
// 64-bit elements are loaded from addresses starting at 'base_addr' and offset
// by each 64-bit element in 'vindex' (each index is scaled by the factor in
// 'scale'). Gathered elements are merged into 'dst' using 'mask' (elements are
// copied from 'src' when the highest bit is not set in the corresponding
// element). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF mask[i+63]
//				dst[i+63:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//				mask[i+63] := 0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		mask[MAX:128] := 0
//		dst[MAX:128] := 0
//
// Instruction: 'VPGATHERQQ'. Intrinsic: '_mm_mask_i64gather_epi64'.
// Requires AVX2.
func MaskI64gather64(src M128i, base_addr int, vindex M128i, mask M128i, scale int) M128i {
	return M128i(maskI64gather64([16]byte(src), base_addr, [16]byte(vindex), [16]byte(mask), scale))
}

func maskI64gather64(src [16]byte, base_addr int, vindex [16]byte, mask [16]byte, scale int) [16]byte


// MmaskI64gather64: Gather 64-bit integers from memory using 64-bit indices.
// 64-bit elements are loaded from addresses starting at 'base_addr' and offset
// by each 64-bit element in 'vindex' (each index is scaled by the factor in
// 'scale'). Gathered elements are merged into 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//		dst[MAX:128] := 0
//
// Instruction: 'VPGATHERQQ'. Intrinsic: '_mm_mmask_i64gather_epi64'.
// Requires AVX512F.
func MmaskI64gather64(src M128i, k Mmask8, vindex M128i, base_addr uintptr, scale int) M128i {
	return M128i(mmaskI64gather64([16]byte(src), uint8(k), [16]byte(vindex), uintptr(base_addr), scale))
}

func mmaskI64gather64(src [16]byte, k uint8, vindex [16]byte, base_addr uintptr, scale int) [16]byte


// I64gatherPd: Gather double-precision (64-bit) floating-point elements from
// memory using 64-bit indices. 64-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 64-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst'. 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VGATHERQPD'. Intrinsic: '_mm_i64gather_pd'.
// Requires AVX2.
func I64gatherPd(base_addr float64, vindex M128i, scale int) M128d {
	return M128d(i64gatherPd(base_addr, [16]byte(vindex), scale))
}

func i64gatherPd(base_addr float64, vindex [16]byte, scale int) [2]float64


// MaskI64gatherPd: Gather double-precision (64-bit) floating-point elements
// from memory using 64-bit indices. 64-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 64-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst' using 'mask' (elements are copied from 'src' when the highest bit is
// not set in the corresponding element). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF mask[i+63]
//				dst[i+63:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//				mask[i+63] := 0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		mask[MAX:128] := 0
//		dst[MAX:128] := 0
//
// Instruction: 'VGATHERQPD'. Intrinsic: '_mm_mask_i64gather_pd'.
// Requires AVX2.
func MaskI64gatherPd(src M128d, base_addr float64, vindex M128i, mask M128d, scale int) M128d {
	return M128d(maskI64gatherPd([2]float64(src), base_addr, [16]byte(vindex), [2]float64(mask), scale))
}

func maskI64gatherPd(src [2]float64, base_addr float64, vindex [16]byte, mask [2]float64, scale int) [2]float64


// MmaskI64gatherPd: Gather double-precision (64-bit) floating-point elements
// from memory using 64-bit indices. 64-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 64-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//		dst[MAX:128] := 0
//
// Instruction: 'VGATHERQPD'. Intrinsic: '_mm_mmask_i64gather_pd'.
// Requires AVX512F.
func MmaskI64gatherPd(src M128d, k Mmask8, vindex M128i, base_addr uintptr, scale int) M128d {
	return M128d(mmaskI64gatherPd([2]float64(src), uint8(k), [16]byte(vindex), uintptr(base_addr), scale))
}

func mmaskI64gatherPd(src [2]float64, k uint8, vindex [16]byte, base_addr uintptr, scale int) [2]float64


// I64gatherPs: Gather single-precision (32-bit) floating-point elements from
// memory using 64-bit indices. 32-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 64-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst'. 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 1
//			i := j*32
//			m := j*64
//			dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VGATHERQPS'. Intrinsic: '_mm_i64gather_ps'.
// Requires AVX2.
func I64gatherPs(base_addr float32, vindex M128i, scale int) M128 {
	return M128(i64gatherPs(base_addr, [16]byte(vindex), scale))
}

func i64gatherPs(base_addr float32, vindex [16]byte, scale int) [4]float32


// MaskI64gatherPs: Gather single-precision (32-bit) floating-point elements
// from memory using 64-bit indices. 32-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 64-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst' using 'mask' (elements are copied from 'src' when the highest bit is
// not set in the corresponding element). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 1
//			i := j*32
//			m := j*64
//			IF mask[i+31]
//				dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//				mask[i+31] := 0
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		mask[MAX:64] := 0
//		dst[MAX:64] := 0
//
// Instruction: 'VGATHERQPS'. Intrinsic: '_mm_mask_i64gather_ps'.
// Requires AVX2.
func MaskI64gatherPs(src M128, base_addr float32, vindex M128i, mask M128, scale int) M128 {
	return M128(maskI64gatherPs([4]float32(src), base_addr, [16]byte(vindex), [4]float32(mask), scale))
}

func maskI64gatherPs(src [4]float32, base_addr float32, vindex [16]byte, mask [4]float32, scale int) [4]float32


// MmaskI64gatherPs: Gather single-precision (32-bit) floating-point elements
// from memory using 64-bit indices. 32-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 64-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 1
//			i := j*32
//			m := j*64
//			IF k[j]
//				dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//		dst[MAX:64] := 0
//
// Instruction: 'VGATHERQPS'. Intrinsic: '_mm_mmask_i64gather_ps'.
// Requires AVX512F.
func MmaskI64gatherPs(src M128, k Mmask8, vindex M128i, base_addr uintptr, scale int) M128 {
	return M128(mmaskI64gatherPs([4]float32(src), uint8(k), [16]byte(vindex), uintptr(base_addr), scale))
}

func mmaskI64gatherPs(src [4]float32, k uint8, vindex [16]byte, base_addr uintptr, scale int) [4]float32


// I64scatter32: Scatter 32-bit integers from 'a' into memory using 64-bit
// indices. 32-bit elements are stored at addresses starting at 'base_addr' and
// offset by each 64-bit element in 'vindex' (each index is scaled by the
// factor in 'scale'). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 1
//			i := j*32
//			l := j*64
//			MEM[base_addr + SignExtend(vindex[l+63:l])*scale] := a[i+31:i]
//		ENDFOR
//
// Instruction: 'VPSCATTERQD'. Intrinsic: '_mm_i64scatter_epi32'.
// Requires AVX512F.
func I64scatter32(base_addr uintptr, vindex M128i, a M128i, scale int)  {
	i64scatter32(uintptr(base_addr), [16]byte(vindex), [16]byte(a), scale)
}

func i64scatter32(base_addr uintptr, vindex [16]byte, a [16]byte, scale int) 


// MaskI64scatter32: Scatter 32-bit integers from 'a' into memory using 64-bit
// indices. 32-bit elements are stored at addresses starting at 'base_addr' and
// offset by each 64-bit element in 'vindex' (each index is scaled by the
// factor in 'scale') subject to mask 'k' (elements are not stored when the
// corresponding mask bit is not set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 1
//			i := j*32
//			l := j*64
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[l+63:l])*scale] := a[i+31:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPSCATTERQD'. Intrinsic: '_mm_mask_i64scatter_epi32'.
// Requires AVX512F.
func MaskI64scatter32(base_addr uintptr, k Mmask8, vindex M128i, a M128i, scale int)  {
	maskI64scatter32(uintptr(base_addr), uint8(k), [16]byte(vindex), [16]byte(a), scale)
}

func maskI64scatter32(base_addr uintptr, k uint8, vindex [16]byte, a [16]byte, scale int) 


// I64scatter64: Scatter 64-bit integers from 'a' into memory using 64-bit
// indices. 64-bit elements are stored at addresses starting at 'base_addr' and
// offset by each 64-bit element in 'vindex' (each index is scaled by the
// factor in 'scale'). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 1
//			i := j*64
//			MEM[base_addr + SignExtend(vindex[i+63:i])*scale] := a[i+63:i]
//		ENDFOR
//
// Instruction: 'VPSCATTERQQ'. Intrinsic: '_mm_i64scatter_epi64'.
// Requires AVX512F.
func I64scatter64(base_addr uintptr, vindex M128i, a M128i, scale int)  {
	i64scatter64(uintptr(base_addr), [16]byte(vindex), [16]byte(a), scale)
}

func i64scatter64(base_addr uintptr, vindex [16]byte, a [16]byte, scale int) 


// MaskI64scatter64: Scatter 64-bit integers from 'a' into memory using 64-bit
// indices. 64-bit elements are stored at addresses starting at 'base_addr' and
// offset by each 64-bit element in 'vindex' (each index is scaled by the
// factor in 'scale') subject to mask 'k' (elements are not stored when the
// corresponding mask bit is not set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[i+63:i])*scale] := a[i+63:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPSCATTERQQ'. Intrinsic: '_mm_mask_i64scatter_epi64'.
// Requires AVX512F.
func MaskI64scatter64(base_addr uintptr, k Mmask8, vindex M128i, a M128i, scale int)  {
	maskI64scatter64(uintptr(base_addr), uint8(k), [16]byte(vindex), [16]byte(a), scale)
}

func maskI64scatter64(base_addr uintptr, k uint8, vindex [16]byte, a [16]byte, scale int) 


// I64scatterPd: Scatter double-precision (64-bit) floating-point elements from
// 'a' into memory using 64-bit indices. 64-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 64-bit element in
// 'vindex' (each index is scaled by the factor in 'scale'). 'scale' should be
// 1, 2, 4 or 8. 
//
//		FOR j := 0 to 1
//			i := j*64
//			MEM[base_addr + SignExtend(vindex[i+63:i])*scale] := a[i+63:i]
//		ENDFOR
//
// Instruction: 'VSCATTERQPD'. Intrinsic: '_mm_i64scatter_pd'.
// Requires AVX512F.
func I64scatterPd(base_addr uintptr, vindex M128i, a M128d, scale int)  {
	i64scatterPd(uintptr(base_addr), [16]byte(vindex), [2]float64(a), scale)
}

func i64scatterPd(base_addr uintptr, vindex [16]byte, a [2]float64, scale int) 


// MaskI64scatterPd: Scatter double-precision (64-bit) floating-point elements
// from 'a' into memory using 64-bit indices. 64-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 64-bit element in
// 'vindex' (each index is scaled by the factor in 'scale') subject to mask 'k'
// (elements are not stored when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[i+63:i])*scale] := a[i+63:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VSCATTERQPD'. Intrinsic: '_mm_mask_i64scatter_pd'.
// Requires AVX512F.
func MaskI64scatterPd(base_addr uintptr, k Mmask8, vindex M128i, a M128d, scale int)  {
	maskI64scatterPd(uintptr(base_addr), uint8(k), [16]byte(vindex), [2]float64(a), scale)
}

func maskI64scatterPd(base_addr uintptr, k uint8, vindex [16]byte, a [2]float64, scale int) 


// I64scatterPs: Scatter single-precision (32-bit) floating-point elements from
// 'a' into memory using 64-bit indices. 32-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 64-bit element in
// 'vindex' (each index is scaled by the factor in 'scale') subject to mask 'k'
// (elements are not stored when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 1
//			i := j*32
//			l := j*64
//			MEM[base_addr + SignExtend(vindex[l+63:l])*scale] := a[i+31:i]
//		ENDFOR
//
// Instruction: 'VSCATTERQPS'. Intrinsic: '_mm_i64scatter_ps'.
// Requires AVX512F.
func I64scatterPs(base_addr uintptr, vindex M128i, a M128, scale int)  {
	i64scatterPs(uintptr(base_addr), [16]byte(vindex), [4]float32(a), scale)
}

func i64scatterPs(base_addr uintptr, vindex [16]byte, a [4]float32, scale int) 


// MaskI64scatterPs: Scatter single-precision (32-bit) floating-point elements
// from 'a' into memory using 64-bit indices. 32-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 64-bit element in
// 'vindex' (each index is scaled by the factor in 'scale') subject to mask 'k'
// (elements are not stored when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 1
//			i := j*32
//			l := j*64
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[l+63:l])*scale] := a[i+31:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VSCATTERQPS'. Intrinsic: '_mm_mask_i64scatter_ps'.
// Requires AVX512F.
func MaskI64scatterPs(base_addr uintptr, k Mmask8, vindex M128i, a M128, scale int)  {
	maskI64scatterPs(uintptr(base_addr), uint8(k), [16]byte(vindex), [4]float32(a), scale)
}

func maskI64scatterPs(base_addr uintptr, k uint8, vindex [16]byte, a [4]float32, scale int) 


// Idiv32: Divide packed 32-bit integers in 'a' by packed elements in 'b', and
// store the truncated results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			dst[i+31:i] := TRUNCATE(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_idiv_epi32'.
// Requires SSE.
func Idiv32(a M128i, b M128i) M128i {
	return M128i(idiv32([16]byte(a), [16]byte(b)))
}

func idiv32(a [16]byte, b [16]byte) [16]byte


// Idivrem32: Divide packed 32-bit integers in 'a' by packed elements in 'b',
// store the truncated results in 'dst', and store the remainders as packed
// 32-bit integers into memory at 'mem_addr'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			dst[i+31:i] := TRUNCATE(a[i+31:i] / b[i+31:i])
//			MEM[mem_addr+i+31:mem_addr+i] := REMAINDER(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_idivrem_epi32'.
// Requires SSE.
func Idivrem32(mem_addr M128i, a M128i, b M128i) M128i {
	return M128i(idivrem32([16]byte(mem_addr), [16]byte(a), [16]byte(b)))
}

func idivrem32(mem_addr [16]byte, a [16]byte, b [16]byte) [16]byte


// Insert16: Copy 'a' to 'dst', and insert the 16-bit integer 'i' into 'dst' at
// the location specified by 'imm8'. 
//
//		dst[127:0] := a[127:0]
//		sel := imm8[2:0]*16
//		dst[sel+15:sel] := i[15:0]
//
// Instruction: 'PINSRW'. Intrinsic: '_mm_insert_epi16'.
// Requires SSE2.
func Insert16(a M128i, i int, imm8 int) M128i {
	return M128i(insert16([16]byte(a), i, imm8))
}

func insert16(a [16]byte, i int, imm8 int) [16]byte


// Insert32: Copy 'a' to 'dst', and insert the 32-bit integer 'i' into 'dst' at
// the location specified by 'imm8'. 
//
//		dst[127:0] := a[127:0]
//		sel := imm8[1:0]*32
//		dst[sel+31:sel] := i[31:0]
//
// Instruction: 'PINSRD'. Intrinsic: '_mm_insert_epi32'.
// Requires SSE4.1.
func Insert32(a M128i, i int, imm8 int) M128i {
	return M128i(insert32([16]byte(a), i, imm8))
}

func insert32(a [16]byte, i int, imm8 int) [16]byte


// Insert64: Copy 'a' to 'dst', and insert the 64-bit integer 'i' into 'dst' at
// the location specified by 'imm8'. 
//
//		dst[127:0] := a[127:0]
//		sel := imm8[0]*64
//		dst[sel+63:sel] := i[63:0]
//
// Instruction: 'PINSRQ'. Intrinsic: '_mm_insert_epi64'.
// Requires SSE4.1.
func Insert64(a M128i, i int64, imm8 int) M128i {
	return M128i(insert64([16]byte(a), i, imm8))
}

func insert64(a [16]byte, i int64, imm8 int) [16]byte


// Insert8: Copy 'a' to 'dst', and insert the lower 8-bit integer from 'i' into
// 'dst' at the location specified by 'imm8'. 
//
//		dst[127:0] := a[127:0]
//		sel := imm8[3:0]*8
//		dst[sel+7:sel] := i[7:0]
//
// Instruction: 'PINSRB'. Intrinsic: '_mm_insert_epi8'.
// Requires SSE4.1.
func Insert8(a M128i, i int, imm8 int) M128i {
	return M128i(insert8([16]byte(a), i, imm8))
}

func insert8(a [16]byte, i int, imm8 int) [16]byte


// InsertPs: Copy 'a' to 'tmp', then insert a single-precision (32-bit)
// floating-point element from 'b' into 'tmp' using the control in 'imm8'.
// Store 'tmp' to 'dst' using the mask in 'imm8' (elements are zeroed out when
// the corresponding bit is set). 
//
//		tmp2[127:0] := a[127:0]
//		CASE (imm8[7:6]) of
//		0: tmp1[31:0] := b[31:0]
//		1: tmp1[31:0] := b[63:32]
//		2: tmp1[31:0] := b[95:64]
//		3: tmp1[31:0] := b[127:96]
//		ESAC
//		CASE (imm8[5:4]) of
//		0: tmp2[31:0] := tmp1[31:0]
//		1: tmp2[63:32] := tmp1[31:0]
//		2: tmp2[95:64] := tmp1[31:0]
//		3: tmp2[127:96] := tmp1[31:0]
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			IF imm8[j%8]
//				dst[i+31:i] := 0
//			ELSE
//				dst[i+31:i] := tmp2[i+31:i]
//			FI
//		ENDFOR
//
// Instruction: 'INSERTPS'. Intrinsic: '_mm_insert_ps'.
// Requires SSE4.1.
func InsertPs(a M128, b M128, imm8 int) M128 {
	return M128(insertPs([4]float32(a), [4]float32(b), imm8))
}

func insertPs(a [4]float32, b [4]float32, imm8 int) [4]float32


// InvcbrtPd: Compute the inverse cube root of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := InvCubeRoot(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_invcbrt_pd'.
// Requires SSE.
func InvcbrtPd(a M128d) M128d {
	return M128d(invcbrtPd([2]float64(a)))
}

func invcbrtPd(a [2]float64) [2]float64


// InvcbrtPs: Compute the inverse cube root of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := InvCubeRoot(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_invcbrt_ps'.
// Requires SSE.
func InvcbrtPs(a M128) M128 {
	return M128(invcbrtPs([4]float32(a)))
}

func invcbrtPs(a [4]float32) [4]float32


// InvsqrtPd: Compute the inverse square root of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := InvSQRT(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_invsqrt_pd'.
// Requires SSE.
func InvsqrtPd(a M128d) M128d {
	return M128d(invsqrtPd([2]float64(a)))
}

func invsqrtPd(a [2]float64) [2]float64


// InvsqrtPs: Compute the inverse square root of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := InvSQRT(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_invsqrt_ps'.
// Requires SSE.
func InvsqrtPs(a M128) M128 {
	return M128(invsqrtPs([4]float32(a)))
}

func invsqrtPs(a [4]float32) [4]float32


// Irem32: Divide packed 32-bit integers in 'a' by packed elements in 'b', and
// store the remainders as packed 32-bit integers in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			dst[i+31:i] := REMAINDER(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_irem_epi32'.
// Requires SSE.
func Irem32(a M128i, b M128i) M128i {
	return M128i(irem32([16]byte(a), [16]byte(b)))
}

func irem32(a [16]byte, b [16]byte) [16]byte


// LddquSi128: Load 128-bits of integer data from unaligned memory into 'dst'.
// This intrinsic may perform better than '_mm_loadu_si128' when the data
// crosses a cache line boundary. 
//
//		dst[127:0] := MEM[mem_addr+127:mem_addr]
//
// Instruction: 'LDDQU'. Intrinsic: '_mm_lddqu_si128'.
// Requires SSE3.
func LddquSi128(mem_addr M128iConst) M128i {
	return M128i(lddquSi128(mem_addr))
}

func lddquSi128(mem_addr M128iConst) [16]byte


// MaskLoad32: Load packed 32-bit integers from memory into 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
// 	'mem_addr' must be aligned on a 16-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVDQA32'. Intrinsic: '_mm_mask_load_epi32'.
// Requires AVX512F.
func MaskLoad32(src M128i, k Mmask8, mem_addr uintptr) M128i {
	return M128i(maskLoad32([16]byte(src), uint8(k), uintptr(mem_addr)))
}

func maskLoad32(src [16]byte, k uint8, mem_addr uintptr) [16]byte


// MaskzLoad32: Load packed 32-bit integers from memory into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
// 	'mem_addr' must be aligned on a 16-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVDQA32'. Intrinsic: '_mm_maskz_load_epi32'.
// Requires AVX512F.
func MaskzLoad32(k Mmask8, mem_addr uintptr) M128i {
	return M128i(maskzLoad32(uint8(k), uintptr(mem_addr)))
}

func maskzLoad32(k uint8, mem_addr uintptr) [16]byte


// MaskLoad64: Load packed 64-bit integers from memory into 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
// 	'mem_addr' must be aligned on a 16-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVDQA64'. Intrinsic: '_mm_mask_load_epi64'.
// Requires AVX512F.
func MaskLoad64(src M128i, k Mmask8, mem_addr uintptr) M128i {
	return M128i(maskLoad64([16]byte(src), uint8(k), uintptr(mem_addr)))
}

func maskLoad64(src [16]byte, k uint8, mem_addr uintptr) [16]byte


// MaskzLoad64: Load packed 64-bit integers from memory into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
// 	'mem_addr' must be aligned on a 16-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVDQA64'. Intrinsic: '_mm_maskz_load_epi64'.
// Requires AVX512F.
func MaskzLoad64(k Mmask8, mem_addr uintptr) M128i {
	return M128i(maskzLoad64(uint8(k), uintptr(mem_addr)))
}

func maskzLoad64(k uint8, mem_addr uintptr) [16]byte


// LoadPd: Load 128-bits (composed of 2 packed double-precision (64-bit)
// floating-point elements) from memory into 'dst'.
// 	'mem_addr' must be aligned on a 16-byte boundary or a general-protection
// exception may be generated. 
//
//		dst[127:0] := MEM[mem_addr+127:mem_addr]
//
// Instruction: 'MOVAPD'. Intrinsic: '_mm_load_pd'.
// Requires SSE2.
func LoadPd(mem_addr float64) M128d {
	return M128d(loadPd(mem_addr))
}

func loadPd(mem_addr float64) [2]float64


// MaskLoadPd: Load packed double-precision (64-bit) floating-point elements
// from memory into 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 'mem_addr' must be aligned on a
// 16-byte boundary or a general-protection exception may be generated. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVAPD'. Intrinsic: '_mm_mask_load_pd'.
// Requires AVX512F.
func MaskLoadPd(src M128d, k Mmask8, mem_addr uintptr) M128d {
	return M128d(maskLoadPd([2]float64(src), uint8(k), uintptr(mem_addr)))
}

func maskLoadPd(src [2]float64, k uint8, mem_addr uintptr) [2]float64


// MaskzLoadPd: Load packed double-precision (64-bit) floating-point elements
// from memory into 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 'mem_addr' must be aligned on a 16-byte
// boundary or a general-protection exception may be generated. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVAPD'. Intrinsic: '_mm_maskz_load_pd'.
// Requires AVX512F.
func MaskzLoadPd(k Mmask8, mem_addr uintptr) M128d {
	return M128d(maskzLoadPd(uint8(k), uintptr(mem_addr)))
}

func maskzLoadPd(k uint8, mem_addr uintptr) [2]float64


// LoadPd1: Load a double-precision (64-bit) floating-point element from memory
// into both elements of 'dst'. 
//
//		dst[63:0] := MEM[mem_addr+63:mem_addr]
//		dst[127:64] := MEM[mem_addr+63:mem_addr]
//
// Instruction: '...'. Intrinsic: '_mm_load_pd1'.
// Requires SSE2.
func LoadPd1(mem_addr float64) M128d {
	return M128d(loadPd1(mem_addr))
}

func loadPd1(mem_addr float64) [2]float64


// LoadPs: Load 128-bits (composed of 4 packed single-precision (32-bit)
// floating-point elements) from memory into 'dst'.
// 	'mem_addr' must be aligned on a 16-byte boundary or a general-protection
// exception may be generated. 
//
//		dst[127:0] := MEM[mem_addr+127:mem_addr]
//
// Instruction: 'MOVAPS'. Intrinsic: '_mm_load_ps'.
// Requires SSE.
func LoadPs(mem_addr float32) M128 {
	return M128(loadPs(mem_addr))
}

func loadPs(mem_addr float32) [4]float32


// MaskLoadPs: Load packed single-precision (32-bit) floating-point elements
// from memory into 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 'mem_addr' must be aligned on a
// 16-byte boundary or a general-protection exception may be generated. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVAPS'. Intrinsic: '_mm_mask_load_ps'.
// Requires AVX512F.
func MaskLoadPs(src M128, k Mmask8, mem_addr uintptr) M128 {
	return M128(maskLoadPs([4]float32(src), uint8(k), uintptr(mem_addr)))
}

func maskLoadPs(src [4]float32, k uint8, mem_addr uintptr) [4]float32


// MaskzLoadPs: Load packed single-precision (32-bit) floating-point elements
// from memory into 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 'mem_addr' must be aligned on a 16-byte
// boundary or a general-protection exception may be generated. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVAPS'. Intrinsic: '_mm_maskz_load_ps'.
// Requires AVX512F.
func MaskzLoadPs(k Mmask8, mem_addr uintptr) M128 {
	return M128(maskzLoadPs(uint8(k), uintptr(mem_addr)))
}

func maskzLoadPs(k uint8, mem_addr uintptr) [4]float32


// LoadPs1: Load a single-precision (32-bit) floating-point element from memory
// into all elements of 'dst'. 
//
//		dst[31:0] := MEM[mem_addr+31:mem_addr]
//		dst[63:32] := MEM[mem_addr+31:mem_addr]
//		dst[95:64] := MEM[mem_addr+31:mem_addr]
//		dst[127:96] := MEM[mem_addr+31:mem_addr]
//
// Instruction: '...'. Intrinsic: '_mm_load_ps1'.
// Requires SSE.
func LoadPs1(mem_addr float32) M128 {
	return M128(loadPs1(mem_addr))
}

func loadPs1(mem_addr float32) [4]float32


// LoadSd: Load a double-precision (64-bit) floating-point element from memory
// into the lower of 'dst', and zero the upper element. 'mem_addr' does not
// need to be aligned on any particular boundary. 
//
//		dst[63:0] := MEM[mem_addr+63:mem_addr]
//		dst[127:64] := 0
//
// Instruction: 'MOVSD'. Intrinsic: '_mm_load_sd'.
// Requires SSE2.
func LoadSd(mem_addr float64) M128d {
	return M128d(loadSd(mem_addr))
}

func loadSd(mem_addr float64) [2]float64


// MaskLoadSd: Load a double-precision (64-bit) floating-point element from
// memory into the lower element of 'dst' using writemask 'k' (the element is
// copied from 'src' when mask bit 0 is not set), and set the upper element of
// 'dst' to zero. 'mem_addr' must be aligned on a 16-byte boundary or a
// general-protection exception may be generated. 
//
//		IF k[0]
//			dst[63:0] := MEM[mem_addr+63:mem_addr]
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[MAX:64] := 0
//
// Instruction: 'VMOVSD'. Intrinsic: '_mm_mask_load_sd'.
// Requires AVX512F.
func MaskLoadSd(src M128d, k Mmask8, mem_addr float64) M128d {
	return M128d(maskLoadSd([2]float64(src), uint8(k), mem_addr))
}

func maskLoadSd(src [2]float64, k uint8, mem_addr float64) [2]float64


// MaskzLoadSd: Load a double-precision (64-bit) floating-point element from
// memory into the lower element of 'dst' using zeromask 'k' (the element is
// zeroed out when mask bit 0 is not set), and set the upper element of 'dst'
// to zero. 'mem_addr' must be aligned on a 16-byte boundary or a
// general-protection exception may be generated. 
//
//		IF k[0]
//			dst[63:0] := MEM[mem_addr+63:mem_addr]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[MAX:64] := 0
//
// Instruction: 'VMOVSD'. Intrinsic: '_mm_maskz_load_sd'.
// Requires AVX512F.
func MaskzLoadSd(k Mmask8, mem_addr float64) M128d {
	return M128d(maskzLoadSd(uint8(k), mem_addr))
}

func maskzLoadSd(k uint8, mem_addr float64) [2]float64


// LoadSi128: Load 128-bits of integer data from memory into 'dst'. 
// 	'mem_addr' must be aligned on a 16-byte boundary or a general-protection
// exception may be generated. 
//
//		dst[127:0] := MEM[mem_addr+127:mem_addr]
//
// Instruction: 'MOVDQA'. Intrinsic: '_mm_load_si128'.
// Requires SSE2.
func LoadSi128(mem_addr M128iConst) M128i {
	return M128i(loadSi128(mem_addr))
}

func loadSi128(mem_addr M128iConst) [16]byte


// LoadSs: Load a single-precision (32-bit) floating-point element from memory
// into the lower of 'dst', and zero the upper 3 elements. 'mem_addr' does not
// need to be aligned on any particular boundary. 
//
//		dst[31:0] := MEM[mem_addr+31:mem_addr]
//		dst[127:32] := 0
//
// Instruction: 'MOVSS'. Intrinsic: '_mm_load_ss'.
// Requires SSE.
func LoadSs(mem_addr float32) M128 {
	return M128(loadSs(mem_addr))
}

func loadSs(mem_addr float32) [4]float32


// MaskLoadSs: Load a single-precision (32-bit) floating-point element from
// memory into the lower element of 'dst' using writemask 'k' (the element is
// copied from 'src' when mask bit 0 is not set), and set the upper elements of
// 'dst' to zero. 'mem_addr' must be aligned on a 16-byte boundary or a
// general-protection exception may be generated. 
//
//		IF k[0]
//			dst[31:0] := MEM[mem_addr+31:mem_addr]
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[MAX:32] := 0
//
// Instruction: 'VMOVSS'. Intrinsic: '_mm_mask_load_ss'.
// Requires AVX512F.
func MaskLoadSs(src M128, k Mmask8, mem_addr float32) M128 {
	return M128(maskLoadSs([4]float32(src), uint8(k), mem_addr))
}

func maskLoadSs(src [4]float32, k uint8, mem_addr float32) [4]float32


// MaskzLoadSs: Load a single-precision (32-bit) floating-point element from
// memory into the lower element of 'dst' using zeromask 'k' (the element is
// zeroed out when mask bit 0 is not set), and set the upper elements of 'dst'
// to zero. 'mem_addr' must be aligned on a 16-byte boundary or a
// general-protection exception may be generated. 
//
//		IF k[0]
//			dst[31:0] := MEM[mem_addr+31:mem_addr]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[MAX:32] := 0
//
// Instruction: 'VMOVSS'. Intrinsic: '_mm_maskz_load_ss'.
// Requires AVX512F.
func MaskzLoadSs(k Mmask8, mem_addr float32) M128 {
	return M128(maskzLoadSs(uint8(k), mem_addr))
}

func maskzLoadSs(k uint8, mem_addr float32) [4]float32


// Load1Pd: Load a double-precision (64-bit) floating-point element from memory
// into both elements of 'dst'. 
//
//		dst[63:0] := MEM[mem_addr+63:mem_addr]
//		dst[127:64] := MEM[mem_addr+63:mem_addr]
//
// Instruction: '...'. Intrinsic: '_mm_load1_pd'.
// Requires SSE2.
func Load1Pd(mem_addr float64) M128d {
	return M128d(load1Pd(mem_addr))
}

func load1Pd(mem_addr float64) [2]float64


// Load1Ps: Load a single-precision (32-bit) floating-point element from memory
// into all elements of 'dst'. 
//
//		dst[31:0] := MEM[mem_addr+31:mem_addr]
//		dst[63:32] := MEM[mem_addr+31:mem_addr]
//		dst[95:64] := MEM[mem_addr+31:mem_addr]
//		dst[127:96] := MEM[mem_addr+31:mem_addr]
//
// Instruction: '...'. Intrinsic: '_mm_load1_ps'.
// Requires SSE.
func Load1Ps(mem_addr float32) M128 {
	return M128(load1Ps(mem_addr))
}

func load1Ps(mem_addr float32) [4]float32


// LoaddupPd: Load a double-precision (64-bit) floating-point element from
// memory into both elements of 'dst'. 
//
//		tmp[63:0] := MEM[mem_addr+63:mem_addr]
//		tmp[127:64] := MEM[mem_addr+63:mem_addr]
//
// Instruction: 'MOVDDUP'. Intrinsic: '_mm_loaddup_pd'.
// Requires SSE3.
func LoaddupPd(mem_addr float64) M128d {
	return M128d(loaddupPd(mem_addr))
}

func loaddupPd(mem_addr float64) [2]float64


// LoadhPd: Load a double-precision (64-bit) floating-point element from memory
// into the upper element of 'dst', and copy the lower element from 'a' to
// 'dst'. 'mem_addr' does not need to be aligned on any particular boundary. 
//
//		dst[63:0] := a[63:0]
//		dst[127:64] := MEM[mem_addr+63:mem_addr]
//
// Instruction: 'MOVHPD'. Intrinsic: '_mm_loadh_pd'.
// Requires SSE2.
func LoadhPd(a M128d, mem_addr float64) M128d {
	return M128d(loadhPd([2]float64(a), mem_addr))
}

func loadhPd(a [2]float64, mem_addr float64) [2]float64


// Loadl64: Load 64-bit integer from memory into the first element of 'dst'. 
//
//		dst[63:0] := MEM[mem_addr+63:mem_addr]
//		dst[MAX:64] := 0
//
// Instruction: 'MOVQ'. Intrinsic: '_mm_loadl_epi64'.
// Requires SSE2.
func Loadl64(mem_addr M128iConst) M128i {
	return M128i(loadl64(mem_addr))
}

func loadl64(mem_addr M128iConst) [16]byte


// LoadlPd: Load a double-precision (64-bit) floating-point element from memory
// into the lower element of 'dst', and copy the upper element from 'a' to
// 'dst'. 'mem_addr' does not need to be aligned on any particular boundary. 
//
//		dst[63:0] := MEM[mem_addr+63:mem_addr]
//		dst[127:64] := a[127:64]
//
// Instruction: 'MOVLPD'. Intrinsic: '_mm_loadl_pd'.
// Requires SSE2.
func LoadlPd(a M128d, mem_addr float64) M128d {
	return M128d(loadlPd([2]float64(a), mem_addr))
}

func loadlPd(a [2]float64, mem_addr float64) [2]float64


// LoadrPd: Load 2 double-precision (64-bit) floating-point elements from
// memory into 'dst' in reverse order. mem_addr must be aligned on a 16-byte
// boundary or a general-protection exception may be generated. 
//
//		dst[63:0] := MEM[mem_addr+127:mem_addr+64]
//		dst[127:64] := MEM[mem_addr+63:mem_addr]
//
// Instruction: '...'. Intrinsic: '_mm_loadr_pd'.
// Requires SSE2.
func LoadrPd(mem_addr float64) M128d {
	return M128d(loadrPd(mem_addr))
}

func loadrPd(mem_addr float64) [2]float64


// LoadrPs: Load 4 single-precision (32-bit) floating-point elements from
// memory into 'dst' in reverse order. mem_addr must be aligned on a 16-byte
// boundary or a general-protection exception may be generated. 
//
//		dst[31:0] := MEM[mem_addr+127:mem_addr+96]
//		dst[63:32] := MEM[mem_addr+95:mem_addr+64]
//		dst[95:64] := MEM[mem_addr+63:mem_addr+32]
//		dst[127:96] := MEM[mem_addr+31:mem_addr]
//
// Instruction: '...'. Intrinsic: '_mm_loadr_ps'.
// Requires SSE.
func LoadrPs(mem_addr float32) M128 {
	return M128(loadrPs(mem_addr))
}

func loadrPs(mem_addr float32) [4]float32


// MaskLoadu16: Load packed 16-bit integers from memory into 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := MEM[mem_addr+i+15:mem_addr+i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVDQU16'. Intrinsic: '_mm_mask_loadu_epi16'.
// Requires AVX512BW.
func MaskLoadu16(src M128i, k Mmask8, mem_addr uintptr) M128i {
	return M128i(maskLoadu16([16]byte(src), uint8(k), uintptr(mem_addr)))
}

func maskLoadu16(src [16]byte, k uint8, mem_addr uintptr) [16]byte


// MaskzLoadu16: Load packed 16-bit integers from memory into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := MEM[mem_addr+i+15:mem_addr+i]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVDQU16'. Intrinsic: '_mm_maskz_loadu_epi16'.
// Requires AVX512BW.
func MaskzLoadu16(k Mmask8, mem_addr uintptr) M128i {
	return M128i(maskzLoadu16(uint8(k), uintptr(mem_addr)))
}

func maskzLoadu16(k uint8, mem_addr uintptr) [16]byte


// MaskLoadu32: Load packed 32-bit integers from memory into 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVDQU32'. Intrinsic: '_mm_mask_loadu_epi32'.
// Requires AVX512F.
func MaskLoadu32(src M128i, k Mmask8, mem_addr uintptr) M128i {
	return M128i(maskLoadu32([16]byte(src), uint8(k), uintptr(mem_addr)))
}

func maskLoadu32(src [16]byte, k uint8, mem_addr uintptr) [16]byte


// MaskzLoadu32: Load packed 32-bit integers from memory into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVDQU32'. Intrinsic: '_mm_maskz_loadu_epi32'.
// Requires AVX512F.
func MaskzLoadu32(k Mmask8, mem_addr uintptr) M128i {
	return M128i(maskzLoadu32(uint8(k), uintptr(mem_addr)))
}

func maskzLoadu32(k uint8, mem_addr uintptr) [16]byte


// MaskLoadu64: Load packed 64-bit integers from memory into 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVDQU64'. Intrinsic: '_mm_mask_loadu_epi64'.
// Requires AVX512F.
func MaskLoadu64(src M128i, k Mmask8, mem_addr uintptr) M128i {
	return M128i(maskLoadu64([16]byte(src), uint8(k), uintptr(mem_addr)))
}

func maskLoadu64(src [16]byte, k uint8, mem_addr uintptr) [16]byte


// MaskzLoadu64: Load packed 64-bit integers from memory into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVDQU64'. Intrinsic: '_mm_maskz_loadu_epi64'.
// Requires AVX512F.
func MaskzLoadu64(k Mmask8, mem_addr uintptr) M128i {
	return M128i(maskzLoadu64(uint8(k), uintptr(mem_addr)))
}

func maskzLoadu64(k uint8, mem_addr uintptr) [16]byte


// MaskLoadu8: Load packed 8-bit integers from memory into 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := MEM[mem_addr+i+7:mem_addr+i]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVDQU8'. Intrinsic: '_mm_mask_loadu_epi8'.
// Requires AVX512BW.
func MaskLoadu8(src M128i, k Mmask16, mem_addr uintptr) M128i {
	return M128i(maskLoadu8([16]byte(src), uint16(k), uintptr(mem_addr)))
}

func maskLoadu8(src [16]byte, k uint16, mem_addr uintptr) [16]byte


// MaskzLoadu8: Load packed 8-bit integers from memory into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := MEM[mem_addr+i+7:mem_addr+i]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVDQU8'. Intrinsic: '_mm_maskz_loadu_epi8'.
// Requires AVX512BW.
func MaskzLoadu8(k Mmask16, mem_addr uintptr) M128i {
	return M128i(maskzLoadu8(uint16(k), uintptr(mem_addr)))
}

func maskzLoadu8(k uint16, mem_addr uintptr) [16]byte


// LoaduPd: Load 128-bits (composed of 2 packed double-precision (64-bit)
// floating-point elements) from memory into 'dst'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		dst[127:0] := MEM[mem_addr+127:mem_addr]
//
// Instruction: 'MOVUPD'. Intrinsic: '_mm_loadu_pd'.
// Requires SSE2.
func LoaduPd(mem_addr float64) M128d {
	return M128d(loaduPd(mem_addr))
}

func loaduPd(mem_addr float64) [2]float64


// MaskLoaduPd: Load packed double-precision (64-bit) floating-point elements
// from memoy into 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVUPD'. Intrinsic: '_mm_mask_loadu_pd'.
// Requires AVX512F.
func MaskLoaduPd(src M128d, k Mmask8, mem_addr uintptr) M128d {
	return M128d(maskLoaduPd([2]float64(src), uint8(k), uintptr(mem_addr)))
}

func maskLoaduPd(src [2]float64, k uint8, mem_addr uintptr) [2]float64


// MaskzLoaduPd: Load packed double-precision (64-bit) floating-point elements
// from memoy into 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVUPD'. Intrinsic: '_mm_maskz_loadu_pd'.
// Requires AVX512F.
func MaskzLoaduPd(k Mmask8, mem_addr uintptr) M128d {
	return M128d(maskzLoaduPd(uint8(k), uintptr(mem_addr)))
}

func maskzLoaduPd(k uint8, mem_addr uintptr) [2]float64


// LoaduPs: Load 128-bits (composed of 4 packed single-precision (32-bit)
// floating-point elements) from memory into 'dst'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		dst[127:0] := MEM[mem_addr+127:mem_addr]
//
// Instruction: 'MOVUPS'. Intrinsic: '_mm_loadu_ps'.
// Requires SSE.
func LoaduPs(mem_addr float32) M128 {
	return M128(loaduPs(mem_addr))
}

func loaduPs(mem_addr float32) [4]float32


// MaskLoaduPs: Load packed single-precision (32-bit) floating-point elements
// from memory into 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVUPS'. Intrinsic: '_mm_mask_loadu_ps'.
// Requires AVX512F.
func MaskLoaduPs(src M128, k Mmask8, mem_addr uintptr) M128 {
	return M128(maskLoaduPs([4]float32(src), uint8(k), uintptr(mem_addr)))
}

func maskLoaduPs(src [4]float32, k uint8, mem_addr uintptr) [4]float32


// MaskzLoaduPs: Load packed single-precision (32-bit) floating-point elements
// from memory into 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVUPS'. Intrinsic: '_mm_maskz_loadu_ps'.
// Requires AVX512F.
func MaskzLoaduPs(k Mmask8, mem_addr uintptr) M128 {
	return M128(maskzLoaduPs(uint8(k), uintptr(mem_addr)))
}

func maskzLoaduPs(k uint8, mem_addr uintptr) [4]float32


// LoaduSi128: Load 128-bits of integer data from memory into 'dst'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		dst[127:0] := MEM[mem_addr+127:mem_addr]
//
// Instruction: 'MOVDQU'. Intrinsic: '_mm_loadu_si128'.
// Requires SSE2.
func LoaduSi128(mem_addr M128iConst) M128i {
	return M128i(loaduSi128(mem_addr))
}

func loaduSi128(mem_addr M128iConst) [16]byte


// LogPd: Compute the natural logarithm of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := ln(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_log_pd'.
// Requires SSE.
func LogPd(a M128d) M128d {
	return M128d(logPd([2]float64(a)))
}

func logPd(a [2]float64) [2]float64


// LogPs: Compute the natural logarithm of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := ln(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_log_ps'.
// Requires SSE.
func LogPs(a M128) M128 {
	return M128(logPs([4]float32(a)))
}

func logPs(a [4]float32) [4]float32


// Log10Pd: Compute the base-10 logarithm of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := log10(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_log10_pd'.
// Requires SSE.
func Log10Pd(a M128d) M128d {
	return M128d(log10Pd([2]float64(a)))
}

func log10Pd(a [2]float64) [2]float64


// Log10Ps: Compute the base-10 logarithm of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := log10(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_log10_ps'.
// Requires SSE.
func Log10Ps(a M128) M128 {
	return M128(log10Ps([4]float32(a)))
}

func log10Ps(a [4]float32) [4]float32


// Log1pPd: Compute the natural logarithm of one plus packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := ln(1.0 + a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_log1p_pd'.
// Requires SSE.
func Log1pPd(a M128d) M128d {
	return M128d(log1pPd([2]float64(a)))
}

func log1pPd(a [2]float64) [2]float64


// Log1pPs: Compute the natural logarithm of one plus packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := ln(1.0 + a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_log1p_ps'.
// Requires SSE.
func Log1pPs(a M128) M128 {
	return M128(log1pPs([4]float32(a)))
}

func log1pPs(a [4]float32) [4]float32


// Log2Pd: Compute the base-2 logarithm of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := log2(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_log2_pd'.
// Requires SSE.
func Log2Pd(a M128d) M128d {
	return M128d(log2Pd([2]float64(a)))
}

func log2Pd(a [2]float64) [2]float64


// Log2Ps: Compute the base-2 logarithm of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := log2(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_log2_ps'.
// Requires SSE.
func Log2Ps(a M128) M128 {
	return M128(log2Ps([4]float32(a)))
}

func log2Ps(a [4]float32) [4]float32


// LogbPd: Convert the exponent of each packed double-precision (64-bit)
// floating-point element in 'a' to a double-precision floating-point number
// representing the integer exponent, and store the results in 'dst'. This
// intrinsic essentially calculates 'floor(log2(x))' for each element. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := ConvertExpFP64(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_logb_pd'.
// Requires SSE.
func LogbPd(a M128d) M128d {
	return M128d(logbPd([2]float64(a)))
}

func logbPd(a [2]float64) [2]float64


// LogbPs: Convert the exponent of each packed single-precision (32-bit)
// floating-point element in 'a' to a single-precision floating-point number
// representing the integer exponent, and store the results in 'dst'. This
// intrinsic essentially calculates 'floor(log2(x))' for each element. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := ConvertExpFP32(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_logb_ps'.
// Requires SSE.
func LogbPs(a M128) M128 {
	return M128(logbPs([4]float32(a)))
}

func logbPs(a [4]float32) [4]float32


// Lzcnt32: Counts the number of leading zero bits in each packed 32-bit
// integer in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			tmp := 31
//			dst[i+31:i] := 0
//			DO WHILE (tmp >= 0 AND a[i+tmp] == 0)
//				tmp := tmp - 1
//				dst[i+31:i] := dst[i+31:i] + 1
//			OD
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPLZCNTD'. Intrinsic: '_mm_lzcnt_epi32'.
// Requires AVX512CD.
func Lzcnt32(a M128i) M128i {
	return M128i(lzcnt32([16]byte(a)))
}

func lzcnt32(a [16]byte) [16]byte


// MaskLzcnt32: Counts the number of leading zero bits in each packed 32-bit
// integer in 'a', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				tmp := 31
//				dst[i+31:i] := 0
//				DO WHILE (tmp >= 0 AND a[i+tmp] == 0)
//					tmp := tmp - 1
//					dst[i+31:i] := dst[i+31:i] + 1
//				OD
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPLZCNTD'. Intrinsic: '_mm_mask_lzcnt_epi32'.
// Requires AVX512CD.
func MaskLzcnt32(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskLzcnt32([16]byte(src), uint8(k), [16]byte(a)))
}

func maskLzcnt32(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzLzcnt32: Counts the number of leading zero bits in each packed 32-bit
// integer in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				tmp := 31
//				dst[i+31:i] := 0
//				DO WHILE (tmp >= 0 AND a[i+tmp] == 0)
//					tmp := tmp - 1
//					dst[i+31:i] := dst[i+31:i] + 1
//				OD
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPLZCNTD'. Intrinsic: '_mm_maskz_lzcnt_epi32'.
// Requires AVX512CD.
func MaskzLzcnt32(k Mmask8, a M128i) M128i {
	return M128i(maskzLzcnt32(uint8(k), [16]byte(a)))
}

func maskzLzcnt32(k uint8, a [16]byte) [16]byte


// Lzcnt64: Counts the number of leading zero bits in each packed 64-bit
// integer in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			tmp := 63
//			dst[i+63:i] := 0
//			DO WHILE (tmp >= 0 AND a[i+tmp] == 0)
//				tmp := tmp - 1
//				dst[i+63:i] := dst[i+63:i] + 1
//			OD
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPLZCNTQ'. Intrinsic: '_mm_lzcnt_epi64'.
// Requires AVX512CD.
func Lzcnt64(a M128i) M128i {
	return M128i(lzcnt64([16]byte(a)))
}

func lzcnt64(a [16]byte) [16]byte


// MaskLzcnt64: Counts the number of leading zero bits in each packed 64-bit
// integer in 'a', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				tmp := 63
//				dst[i+63:i] := 0
//				DO WHILE (tmp >= 0 AND a[i+tmp] == 0)
//					tmp := tmp - 1
//					dst[i+63:i] := dst[i+63:i] + 1
//				OD
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPLZCNTQ'. Intrinsic: '_mm_mask_lzcnt_epi64'.
// Requires AVX512CD.
func MaskLzcnt64(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskLzcnt64([16]byte(src), uint8(k), [16]byte(a)))
}

func maskLzcnt64(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzLzcnt64: Counts the number of leading zero bits in each packed 64-bit
// integer in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				tmp := 63
//				dst[i+63:i] := 0
//				DO WHILE (tmp >= 0 AND a[i+tmp] == 0)
//					tmp := tmp - 1
//					dst[i+63:i] := dst[i+63:i] + 1
//				OD
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPLZCNTQ'. Intrinsic: '_mm_maskz_lzcnt_epi64'.
// Requires AVX512CD.
func MaskzLzcnt64(k Mmask8, a M128i) M128i {
	return M128i(maskzLzcnt64(uint8(k), [16]byte(a)))
}

func maskzLzcnt64(k uint8, a [16]byte) [16]byte


// Madd16: Multiply packed signed 16-bit integers in 'a' and 'b', producing
// intermediate signed 32-bit integers. Horizontally add adjacent pairs of
// intermediate 32-bit integers, and pack the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			st[i+31:i] := a[i+31:i+16]*b[i+31:i+16] + a[i+15:i]*b[i+15:i]
//		ENDFOR
//
// Instruction: 'PMADDWD'. Intrinsic: '_mm_madd_epi16'.
// Requires SSE2.
func Madd16(a M128i, b M128i) M128i {
	return M128i(madd16([16]byte(a), [16]byte(b)))
}

func madd16(a [16]byte, b [16]byte) [16]byte


// MaskMadd16: Multiply packed 16-bit integers in 'a' and 'b', producing
// intermediate 32-bit integers. Horizontally add adjacent pairs of
// intermediate 32-bit integers, and pack the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i+16]*b[i+31:i+16] + a[i+15:i]*b[i+15:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMADDWD'. Intrinsic: '_mm_mask_madd_epi16'.
// Requires AVX512BW.
func MaskMadd16(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskMadd16([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskMadd16(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzMadd16: Multiply packed 16-bit integers in 'a' and 'b', producing
// intermediate 32-bit integers. Horizontally add adjacent pairs of
// intermediate 32-bit integers, and pack the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i+16]*b[i+31:i+16] + a[i+15:i]*b[i+15:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMADDWD'. Intrinsic: '_mm_maskz_madd_epi16'.
// Requires AVX512BW.
func MaskzMadd16(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzMadd16(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzMadd16(k uint8, a [16]byte, b [16]byte) [16]byte


// Madd52hiEpu64: Multiply packed unsigned 52-bit integers in each 64-bit
// element of 'b' and 'c' to form a 104-bit intermediate result. Add the high
// 52-bit unsigned integer from the intermediate result with the corresponding
// unsigned 64-bit integer in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
//			dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[103:52])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMADD52HUQ'. Intrinsic: '_mm_madd52hi_epu64'.
// Requires AVX512VL.
func Madd52hiEpu64(a M128i, b M128i, c M128i) M128i {
	return M128i(madd52hiEpu64([16]byte(a), [16]byte(b), [16]byte(c)))
}

func madd52hiEpu64(a [16]byte, b [16]byte, c [16]byte) [16]byte


// MaskMadd52hiEpu64: Multiply packed unsigned 52-bit integers in each 64-bit
// element of 'b' and 'c' to form a 104-bit intermediate result. Add the high
// 52-bit unsigned integer from the intermediate result with the corresponding
// unsigned 64-bit integer in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
//				dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[103:52])
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMADD52HUQ'. Intrinsic: '_mm_mask_madd52hi_epu64'.
// Requires AVX512VL.
func MaskMadd52hiEpu64(a M128i, k Mmask8, b M128i, c M128i) M128i {
	return M128i(maskMadd52hiEpu64([16]byte(a), uint8(k), [16]byte(b), [16]byte(c)))
}

func maskMadd52hiEpu64(a [16]byte, k uint8, b [16]byte, c [16]byte) [16]byte


// MaskzMadd52hiEpu64: Multiply packed unsigned 52-bit integers in each 64-bit
// element of 'b' and 'c' to form a 104-bit intermediate result. Add the high
// 52-bit unsigned integer from the intermediate result with the corresponding
// unsigned 64-bit integer in 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
//				dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[103:52])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMADD52HUQ'. Intrinsic: '_mm_maskz_madd52hi_epu64'.
// Requires AVX512VL.
func MaskzMadd52hiEpu64(k Mmask8, a M128i, b M128i, c M128i) M128i {
	return M128i(maskzMadd52hiEpu64(uint8(k), [16]byte(a), [16]byte(b), [16]byte(c)))
}

func maskzMadd52hiEpu64(k uint8, a [16]byte, b [16]byte, c [16]byte) [16]byte


// Madd52loEpu64: Multiply packed unsigned 52-bit integers in each 64-bit
// element of 'b' and 'c' to form a 104-bit intermediate result. Add the low
// 52-bit unsigned integer from the intermediate result with the corresponding
// unsigned 64-bit integer in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
//			dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[51:0])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMADD52LUQ'. Intrinsic: '_mm_madd52lo_epu64'.
// Requires AVX512VL.
func Madd52loEpu64(a M128i, b M128i, c M128i) M128i {
	return M128i(madd52loEpu64([16]byte(a), [16]byte(b), [16]byte(c)))
}

func madd52loEpu64(a [16]byte, b [16]byte, c [16]byte) [16]byte


// MaskMadd52loEpu64: Multiply packed unsigned 52-bit integers in each 64-bit
// element of 'b' and 'c' to form a 104-bit intermediate result. Add the low
// 52-bit unsigned integer from the intermediate result with the corresponding
// unsigned 64-bit integer in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
//				dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[51:0])
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMADD52LUQ'. Intrinsic: '_mm_mask_madd52lo_epu64'.
// Requires AVX512VL.
func MaskMadd52loEpu64(a M128i, k Mmask8, b M128i, c M128i) M128i {
	return M128i(maskMadd52loEpu64([16]byte(a), uint8(k), [16]byte(b), [16]byte(c)))
}

func maskMadd52loEpu64(a [16]byte, k uint8, b [16]byte, c [16]byte) [16]byte


// MaskzMadd52loEpu64: Multiply packed unsigned 52-bit integers in each 64-bit
// element of 'b' and 'c' to form a 104-bit intermediate result. Add the low
// 52-bit unsigned integer from the intermediate result with the corresponding
// unsigned 64-bit integer in 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
//				dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[51:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMADD52LUQ'. Intrinsic: '_mm_maskz_madd52lo_epu64'.
// Requires AVX512VL.
func MaskzMadd52loEpu64(k Mmask8, a M128i, b M128i, c M128i) M128i {
	return M128i(maskzMadd52loEpu64(uint8(k), [16]byte(a), [16]byte(b), [16]byte(c)))
}

func maskzMadd52loEpu64(k uint8, a [16]byte, b [16]byte, c [16]byte) [16]byte


// Maddubs16: Vertically multiply each unsigned 8-bit integer from 'a' with the
// corresponding signed 8-bit integer from 'b', producing intermediate signed
// 16-bit integers. Horizontally add adjacent pairs of intermediate signed
// 16-bit integers, and pack the saturated results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			dst[i+15:i] := Saturate_To_Int16( a[i+15:i+8]*b[i+15:i+8] + a[i+7:i]*b[i+7:i] )
//		ENDFOR
//
// Instruction: 'PMADDUBSW'. Intrinsic: '_mm_maddubs_epi16'.
// Requires SSSE3.
func Maddubs16(a M128i, b M128i) M128i {
	return M128i(maddubs16([16]byte(a), [16]byte(b)))
}

func maddubs16(a [16]byte, b [16]byte) [16]byte


// MaskMaddubs16: Multiply packed unsigned 8-bit integers in 'a' by packed
// signed 8-bit integers in 'b', producing intermediate signed 16-bit integers.
// Horizontally add adjacent pairs of intermediate signed 16-bit integers, and
// pack the saturated results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := Saturate_To_Int16( a[i+15:i+8]*b[i+15:i+8] + a[i+7:i]*b[i+7:i] )
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMADDUBSW'. Intrinsic: '_mm_mask_maddubs_epi16'.
// Requires AVX512BW.
func MaskMaddubs16(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskMaddubs16([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskMaddubs16(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzMaddubs16: Multiply packed unsigned 8-bit integers in 'a' by packed
// signed 8-bit integers in 'b', producing intermediate signed 16-bit integers.
// Horizontally add adjacent pairs of intermediate signed 16-bit integers, and
// pack the saturated results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := Saturate_To_Int16( a[i+15:i+8]*b[i+15:i+8] + a[i+7:i]*b[i+7:i] )
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMADDUBSW'. Intrinsic: '_mm_maskz_maddubs_epi16'.
// Requires AVX512BW.
func MaskzMaddubs16(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzMaddubs16(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzMaddubs16(k uint8, a [16]byte, b [16]byte) [16]byte


// Maskload32: Load packed 32-bit integers from memory into 'dst' using 'mask'
// (elements are zeroed out when the highest bit is not set in the
// corresponding element). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF mask[i+31]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMASKMOVD'. Intrinsic: '_mm_maskload_epi32'.
// Requires AVX2.
func Maskload32(mem_addr int, mask M128i) M128i {
	return M128i(maskload32(mem_addr, [16]byte(mask)))
}

func maskload32(mem_addr int, mask [16]byte) [16]byte


// Maskload64: Load packed 64-bit integers from memory into 'dst' using 'mask'
// (elements are zeroed out when the highest bit is not set in the
// corresponding element). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF mask[i+63]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMASKMOVQ'. Intrinsic: '_mm_maskload_epi64'.
// Requires AVX2.
func Maskload64(mem_addr int, mask M128i) M128i {
	return M128i(maskload64(mem_addr, [16]byte(mask)))
}

func maskload64(mem_addr int, mask [16]byte) [16]byte


// MaskloadPd: Load packed double-precision (64-bit) floating-point elements
// from memory into 'dst' using 'mask' (elements are zeroed out when the high
// bit of the corresponding element is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF mask[i+63]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMASKMOVPD'. Intrinsic: '_mm_maskload_pd'.
// Requires AVX.
func MaskloadPd(mem_addr float64, mask M128i) M128d {
	return M128d(maskloadPd(mem_addr, [16]byte(mask)))
}

func maskloadPd(mem_addr float64, mask [16]byte) [2]float64


// MaskloadPs: Load packed single-precision (32-bit) floating-point elements
// from memory into 'dst' using 'mask' (elements are zeroed out when the high
// bit of the corresponding element is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF mask[i+31]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMASKMOVPS'. Intrinsic: '_mm_maskload_ps'.
// Requires AVX.
func MaskloadPs(mem_addr float32, mask M128i) M128 {
	return M128(maskloadPs(mem_addr, [16]byte(mask)))
}

func maskloadPs(mem_addr float32, mask [16]byte) [4]float32


// MaskmoveuSi128: Conditionally store 8-bit integer elements from 'a' into
// memory using 'mask' (elements are not stored when the highest bit is not set
// in the corresponding element) and a non-temporal memory hint. 'mem_addr'
// does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF mask[i+7]
//				MEM[mem_addr+i+7:mem_addr+i] := a[i+7:i]
//			FI
//		ENDFOR
//
// Instruction: 'MASKMOVDQU'. Intrinsic: '_mm_maskmoveu_si128'.
// Requires SSE2.
func MaskmoveuSi128(a M128i, mask M128i, mem_addr byte)  {
	maskmoveuSi128([16]byte(a), [16]byte(mask), mem_addr)
}

func maskmoveuSi128(a [16]byte, mask [16]byte, mem_addr byte) 


// Maskstore32: Store packed 32-bit integers from 'a' into memory using 'mask'
// (elements are not stored when the highest bit is not set in the
// corresponding element). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF mask[i+31]
//				MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
//			FI
//		ENDFOR
//
// Instruction: 'VPMASKMOVD'. Intrinsic: '_mm_maskstore_epi32'.
// Requires AVX2.
func Maskstore32(mem_addr int, mask M128i, a M128i)  {
	maskstore32(mem_addr, [16]byte(mask), [16]byte(a))
}

func maskstore32(mem_addr int, mask [16]byte, a [16]byte) 


// Maskstore64: Store packed 64-bit integers from 'a' into memory using 'mask'
// (elements are not stored when the highest bit is not set in the
// corresponding element). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF mask[i+63]
//				MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
//			FI
//		ENDFOR
//
// Instruction: 'VPMASKMOVQ'. Intrinsic: '_mm_maskstore_epi64'.
// Requires AVX2.
func Maskstore64(mem_addr int64, mask M128i, a M128i)  {
	maskstore64(mem_addr, [16]byte(mask), [16]byte(a))
}

func maskstore64(mem_addr int64, mask [16]byte, a [16]byte) 


// MaskstorePd: Store packed double-precision (64-bit) floating-point elements
// from 'a' into memory using 'mask'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF mask[i+63]
//				MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMASKMOVPD'. Intrinsic: '_mm_maskstore_pd'.
// Requires AVX.
func MaskstorePd(mem_addr float64, mask M128i, a M128d)  {
	maskstorePd(mem_addr, [16]byte(mask), [2]float64(a))
}

func maskstorePd(mem_addr float64, mask [16]byte, a [2]float64) 


// MaskstorePs: Store packed single-precision (32-bit) floating-point elements
// from 'a' into memory using 'mask'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF mask[i+31]
//				MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMASKMOVPS'. Intrinsic: '_mm_maskstore_ps'.
// Requires AVX.
func MaskstorePs(mem_addr float32, mask M128i, a M128)  {
	maskstorePs(mem_addr, [16]byte(mask), [4]float32(a))
}

func maskstorePs(mem_addr float32, mask [16]byte, a [4]float32) 


// MaskMax16: Compare packed 16-bit integers in 'a' and 'b', and store packed
// maximum values in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				IF a[i+15:i] > b[i+15:i]
//					dst[i+15:i] := a[i+15:i]
//				ELSE
//					dst[i+15:i] := b[i+15:i]
//				FI
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMAXSW'. Intrinsic: '_mm_mask_max_epi16'.
// Requires AVX512BW.
func MaskMax16(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskMax16([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskMax16(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzMax16: Compare packed 16-bit integers in 'a' and 'b', and store packed
// maximum values in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				IF a[i+15:i] > b[i+15:i]
//					dst[i+15:i] := a[i+15:i]
//				ELSE
//					dst[i+15:i] := b[i+15:i]
//				FI
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMAXSW'. Intrinsic: '_mm_maskz_max_epi16'.
// Requires AVX512BW.
func MaskzMax16(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzMax16(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzMax16(k uint8, a [16]byte, b [16]byte) [16]byte


// Max16: Compare packed 16-bit integers in 'a' and 'b', and store packed
// maximum values in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF a[i+15:i] > b[i+15:i]
//				dst[i+15:i] := a[i+15:i]
//			ELSE
//				dst[i+15:i] := b[i+15:i]
//			FI
//		ENDFOR
//
// Instruction: 'PMAXSW'. Intrinsic: '_mm_max_epi16'.
// Requires SSE2.
func Max16(a M128i, b M128i) M128i {
	return M128i(max16([16]byte(a), [16]byte(b)))
}

func max16(a [16]byte, b [16]byte) [16]byte


// MaskMax32: Compare packed 32-bit integers in 'a' and 'b', and store packed
// maximum values in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] > b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMAXSD'. Intrinsic: '_mm_mask_max_epi32'.
// Requires AVX512F.
func MaskMax32(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskMax32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskMax32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzMax32: Compare packed 32-bit integers in 'a' and 'b', and store packed
// maximum values in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] > b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMAXSD'. Intrinsic: '_mm_maskz_max_epi32'.
// Requires AVX512F.
func MaskzMax32(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzMax32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzMax32(k uint8, a [16]byte, b [16]byte) [16]byte


// Max32: Compare packed 32-bit integers in 'a' and 'b', and store packed
// maximum values in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF a[i+31:i] > b[i+31:i]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := b[i+31:i]
//			FI
//		ENDFOR
//
// Instruction: 'PMAXSD'. Intrinsic: '_mm_max_epi32'.
// Requires SSE4.1.
func Max32(a M128i, b M128i) M128i {
	return M128i(max32([16]byte(a), [16]byte(b)))
}

func max32(a [16]byte, b [16]byte) [16]byte


// MaskMax64: Compare packed 64-bit integers in 'a' and 'b', and store packed
// maximum values in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] > b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMAXSQ'. Intrinsic: '_mm_mask_max_epi64'.
// Requires AVX512F.
func MaskMax64(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskMax64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskMax64(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzMax64: Compare packed 64-bit integers in 'a' and 'b', and store packed
// maximum values in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] > b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMAXSQ'. Intrinsic: '_mm_maskz_max_epi64'.
// Requires AVX512F.
func MaskzMax64(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzMax64(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzMax64(k uint8, a [16]byte, b [16]byte) [16]byte


// Max64: Compare packed 64-bit integers in 'a' and 'b', and store packed
// maximum values in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF a[i+63:i] > b[i+63:i]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := b[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMAXSQ'. Intrinsic: '_mm_max_epi64'.
// Requires AVX512F.
func Max64(a M128i, b M128i) M128i {
	return M128i(max64([16]byte(a), [16]byte(b)))
}

func max64(a [16]byte, b [16]byte) [16]byte


// MaskMax8: Compare packed 8-bit integers in 'a' and 'b', and store packed
// maximum values in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				IF a[i+7:i] > b[i+7:i]
//					dst[i+7:i] := a[i+7:i]
//				ELSE
//					dst[i+7:i] := b[i+7:i]
//				FI
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMAXSB'. Intrinsic: '_mm_mask_max_epi8'.
// Requires AVX512BW.
func MaskMax8(src M128i, k Mmask16, a M128i, b M128i) M128i {
	return M128i(maskMax8([16]byte(src), uint16(k), [16]byte(a), [16]byte(b)))
}

func maskMax8(src [16]byte, k uint16, a [16]byte, b [16]byte) [16]byte


// MaskzMax8: Compare packed 8-bit integers in 'a' and 'b', and store packed
// maximum values in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				IF a[i+7:i] > b[i+7:i]
//					dst[i+7:i] := a[i+7:i]
//				ELSE
//					dst[i+7:i] := b[i+7:i]
//				FI
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMAXSB'. Intrinsic: '_mm_maskz_max_epi8'.
// Requires AVX512BW.
func MaskzMax8(k Mmask16, a M128i, b M128i) M128i {
	return M128i(maskzMax8(uint16(k), [16]byte(a), [16]byte(b)))
}

func maskzMax8(k uint16, a [16]byte, b [16]byte) [16]byte


// Max8: Compare packed 8-bit integers in 'a' and 'b', and store packed maximum
// values in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF a[i+7:i] > b[i+7:i]
//				dst[i+7:i] := a[i+7:i]
//			ELSE
//				dst[i+7:i] := b[i+7:i]
//			FI
//		ENDFOR
//
// Instruction: 'PMAXSB'. Intrinsic: '_mm_max_epi8'.
// Requires SSE4.1.
func Max8(a M128i, b M128i) M128i {
	return M128i(max8([16]byte(a), [16]byte(b)))
}

func max8(a [16]byte, b [16]byte) [16]byte


// MaskMaxEpu16: Compare packed unsigned 16-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				IF a[i+15:i] > b[i+15:i]
//					dst[i+15:i] := a[i+15:i]
//				ELSE
//					dst[i+15:i] := b[i+15:i]
//				FI
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMAXUW'. Intrinsic: '_mm_mask_max_epu16'.
// Requires AVX512BW.
func MaskMaxEpu16(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskMaxEpu16([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskMaxEpu16(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzMaxEpu16: Compare packed unsigned 16-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				IF a[i+15:i] > b[i+15:i]
//					dst[i+15:i] := a[i+15:i]
//				ELSE
//					dst[i+15:i] := b[i+15:i]
//				FI
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMAXUW'. Intrinsic: '_mm_maskz_max_epu16'.
// Requires AVX512BW.
func MaskzMaxEpu16(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzMaxEpu16(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzMaxEpu16(k uint8, a [16]byte, b [16]byte) [16]byte


// MaxEpu16: Compare packed unsigned 16-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF a[i+15:i] > b[i+15:i]
//				dst[i+15:i] := a[i+15:i]
//			ELSE
//				dst[i+15:i] := b[i+15:i]
//			FI
//		ENDFOR
//
// Instruction: 'PMAXUW'. Intrinsic: '_mm_max_epu16'.
// Requires SSE4.1.
func MaxEpu16(a M128i, b M128i) M128i {
	return M128i(maxEpu16([16]byte(a), [16]byte(b)))
}

func maxEpu16(a [16]byte, b [16]byte) [16]byte


// MaskMaxEpu32: Compare packed unsigned 32-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] > b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMAXUD'. Intrinsic: '_mm_mask_max_epu32'.
// Requires AVX512F.
func MaskMaxEpu32(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskMaxEpu32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskMaxEpu32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzMaxEpu32: Compare packed unsigned 32-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] > b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMAXUD'. Intrinsic: '_mm_maskz_max_epu32'.
// Requires AVX512F.
func MaskzMaxEpu32(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzMaxEpu32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzMaxEpu32(k uint8, a [16]byte, b [16]byte) [16]byte


// MaxEpu32: Compare packed unsigned 32-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF a[i+31:i] > b[i+31:i]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := b[i+31:i]
//			FI
//		ENDFOR
//
// Instruction: 'PMAXUD'. Intrinsic: '_mm_max_epu32'.
// Requires SSE4.1.
func MaxEpu32(a M128i, b M128i) M128i {
	return M128i(maxEpu32([16]byte(a), [16]byte(b)))
}

func maxEpu32(a [16]byte, b [16]byte) [16]byte


// MaskMaxEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] > b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMAXUQ'. Intrinsic: '_mm_mask_max_epu64'.
// Requires AVX512F.
func MaskMaxEpu64(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskMaxEpu64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskMaxEpu64(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzMaxEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] > b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMAXUQ'. Intrinsic: '_mm_maskz_max_epu64'.
// Requires AVX512F.
func MaskzMaxEpu64(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzMaxEpu64(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzMaxEpu64(k uint8, a [16]byte, b [16]byte) [16]byte


// MaxEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF a[i+63:i] > b[i+63:i]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := b[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMAXUQ'. Intrinsic: '_mm_max_epu64'.
// Requires AVX512F.
func MaxEpu64(a M128i, b M128i) M128i {
	return M128i(maxEpu64([16]byte(a), [16]byte(b)))
}

func maxEpu64(a [16]byte, b [16]byte) [16]byte


// MaskMaxEpu8: Compare packed unsigned 8-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				IF a[i+7:i] > b[i+7:i]
//					dst[i+7:i] := a[i+7:i]
//				ELSE
//					dst[i+7:i] := b[i+7:i]
//				FI
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMAXUB'. Intrinsic: '_mm_mask_max_epu8'.
// Requires AVX512BW.
func MaskMaxEpu8(src M128i, k Mmask16, a M128i, b M128i) M128i {
	return M128i(maskMaxEpu8([16]byte(src), uint16(k), [16]byte(a), [16]byte(b)))
}

func maskMaxEpu8(src [16]byte, k uint16, a [16]byte, b [16]byte) [16]byte


// MaskzMaxEpu8: Compare packed unsigned 8-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				IF a[i+7:i] > b[i+7:i]
//					dst[i+7:i] := a[i+7:i]
//				ELSE
//					dst[i+7:i] := b[i+7:i]
//				FI
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMAXUB'. Intrinsic: '_mm_maskz_max_epu8'.
// Requires AVX512BW.
func MaskzMaxEpu8(k Mmask16, a M128i, b M128i) M128i {
	return M128i(maskzMaxEpu8(uint16(k), [16]byte(a), [16]byte(b)))
}

func maskzMaxEpu8(k uint16, a [16]byte, b [16]byte) [16]byte


// MaxEpu8: Compare packed unsigned 8-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF a[i+7:i] > b[i+7:i]
//				dst[i+7:i] := a[i+7:i]
//			ELSE
//				dst[i+7:i] := b[i+7:i]
//			FI
//		ENDFOR
//
// Instruction: 'PMAXUB'. Intrinsic: '_mm_max_epu8'.
// Requires SSE2.
func MaxEpu8(a M128i, b M128i) M128i {
	return M128i(maxEpu8([16]byte(a), [16]byte(b)))
}

func maxEpu8(a [16]byte, b [16]byte) [16]byte


// MaskMaxPd: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', and store packed maximum values in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMAXPD'. Intrinsic: '_mm_mask_max_pd'.
// Requires AVX512F.
func MaskMaxPd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskMaxPd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskMaxPd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzMaxPd: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', and store packed maximum values in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMAXPD'. Intrinsic: '_mm_maskz_max_pd'.
// Requires AVX512F.
func MaskzMaxPd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzMaxPd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzMaxPd(k uint8, a [2]float64, b [2]float64) [2]float64


// MaxPd: Compare packed double-precision (64-bit) floating-point elements in
// 'a' and 'b', and store packed maximum values in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
//		ENDFOR
//
// Instruction: 'MAXPD'. Intrinsic: '_mm_max_pd'.
// Requires SSE2.
func MaxPd(a M128d, b M128d) M128d {
	return M128d(maxPd([2]float64(a), [2]float64(b)))
}

func maxPd(a [2]float64, b [2]float64) [2]float64


// MaskMaxPs: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', and store packed maximum values in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMAXPS'. Intrinsic: '_mm_mask_max_ps'.
// Requires AVX512F.
func MaskMaxPs(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskMaxPs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskMaxPs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzMaxPs: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', and store packed maximum values in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMAXPS'. Intrinsic: '_mm_maskz_max_ps'.
// Requires AVX512F.
func MaskzMaxPs(k Mmask8, a M128, b M128) M128 {
	return M128(maskzMaxPs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzMaxPs(k uint8, a [4]float32, b [4]float32) [4]float32


// MaxPs: Compare packed single-precision (32-bit) floating-point elements in
// 'a' and 'b', and store packed maximum values in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
//		ENDFOR
//
// Instruction: 'MAXPS'. Intrinsic: '_mm_max_ps'.
// Requires SSE.
func MaxPs(a M128, b M128) M128 {
	return M128(maxPs([4]float32(a), [4]float32(b)))
}

func maxPs(a [4]float32, b [4]float32) [4]float32


// MaskMaxRoundSd: Compare the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', store the maximum value in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'src' when mask bit 0
// is not set), and copy the upper element from 'a' to the upper element of
// 'dst'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		IF k[0]
//			dst[63:0] := MAX(a[63:0], b[63:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMAXSD'. Intrinsic: '_mm_mask_max_round_sd'.
// Requires AVX512F.
func MaskMaxRoundSd(src M128d, k Mmask8, a M128d, b M128d, sae int) M128d {
	return M128d(maskMaxRoundSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), sae))
}

func maskMaxRoundSd(src [2]float64, k uint8, a [2]float64, b [2]float64, sae int) [2]float64


// MaskzMaxRoundSd: Compare the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', store the maximum value in the lower element of
// 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is not
// set), and copy the upper element from 'a' to the upper element of 'dst'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		IF k[0]
//			dst[63:0] := MAX(a[63:0], b[63:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMAXSD'. Intrinsic: '_mm_maskz_max_round_sd'.
// Requires AVX512F.
func MaskzMaxRoundSd(k Mmask8, a M128d, b M128d, sae int) M128d {
	return M128d(maskzMaxRoundSd(uint8(k), [2]float64(a), [2]float64(b), sae))
}

func maskzMaxRoundSd(k uint8, a [2]float64, b [2]float64, sae int) [2]float64


// MaxRoundSd: Compare the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', store the maximum value in the lower element of
// 'dst', and copy the upper element from 'a' to the upper element of 'dst'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		dst[63:0] := MAX(a[63:0], b[63:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMAXSD'. Intrinsic: '_mm_max_round_sd'.
// Requires AVX512F.
func MaxRoundSd(a M128d, b M128d, sae int) M128d {
	return M128d(maxRoundSd([2]float64(a), [2]float64(b), sae))
}

func maxRoundSd(a [2]float64, b [2]float64, sae int) [2]float64


// MaskMaxRoundSs: Compare the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', store the maximum value in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'src' when mask bit 0
// is not set), and copy the upper element from 'a' to the upper element of
// 'dst'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		IF k[0]
//			dst[31:0] := MAX(a[31:0], b[31:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMAXSS'. Intrinsic: '_mm_mask_max_round_ss'.
// Requires AVX512F.
func MaskMaxRoundSs(src M128, k Mmask8, a M128, b M128, sae int) M128 {
	return M128(maskMaxRoundSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), sae))
}

func maskMaxRoundSs(src [4]float32, k uint8, a [4]float32, b [4]float32, sae int) [4]float32


// MaskzMaxRoundSs: Compare the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', store the maximum value in the lower element of
// 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is not
// set), and copy the upper element from 'a' to the upper element of 'dst'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		IF k[0]
//			dst[31:0] := MAX(a[31:0], b[31:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMAXSS'. Intrinsic: '_mm_maskz_max_round_ss'.
// Requires AVX512F.
func MaskzMaxRoundSs(k Mmask8, a M128, b M128, sae int) M128 {
	return M128(maskzMaxRoundSs(uint8(k), [4]float32(a), [4]float32(b), sae))
}

func maskzMaxRoundSs(k uint8, a [4]float32, b [4]float32, sae int) [4]float32


// MaxRoundSs: Compare the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', store the maximum value in the lower element of
// 'dst', and copy the upper element from 'a' to the upper element of 'dst'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		dst[31:0] := MAX(a[31:0], b[31:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMAXSS'. Intrinsic: '_mm_max_round_ss'.
// Requires AVX512F.
func MaxRoundSs(a M128, b M128, sae int) M128 {
	return M128(maxRoundSs([4]float32(a), [4]float32(b), sae))
}

func maxRoundSs(a [4]float32, b [4]float32, sae int) [4]float32


// MaskMaxSd: Compare the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', store the maximum value in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'src' when mask bit 0
// is not set), and copy the upper element from 'a' to the upper element of
// 'dst'. 
//
//		IF k[0]
//			dst[63:0] := MAX(a[63:0], b[63:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMAXSD'. Intrinsic: '_mm_mask_max_sd'.
// Requires AVX512F.
func MaskMaxSd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskMaxSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskMaxSd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzMaxSd: Compare the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', store the maximum value in the lower element of
// 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is not
// set), and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := MAX(a[63:0], b[63:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMAXSD'. Intrinsic: '_mm_maskz_max_sd'.
// Requires AVX512F.
func MaskzMaxSd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzMaxSd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzMaxSd(k uint8, a [2]float64, b [2]float64) [2]float64


// MaxSd: Compare the lower double-precision (64-bit) floating-point elements
// in 'a' and 'b', store the maximum value in the lower element of 'dst', and
// copy the upper element from 'a' to the upper element of 'dst'. 
//
//		dst[63:0] := MAX(a[63:0], b[63:0])
//		dst[127:64] := a[127:64]
//
// Instruction: 'MAXSD'. Intrinsic: '_mm_max_sd'.
// Requires SSE2.
func MaxSd(a M128d, b M128d) M128d {
	return M128d(maxSd([2]float64(a), [2]float64(b)))
}

func maxSd(a [2]float64, b [2]float64) [2]float64


// MaskMaxSs: Compare the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', store the maximum value in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'src' when mask bit 0
// is not set), and copy the upper element from 'a' to the upper element of
// 'dst'. 
//
//		IF k[0]
//			dst[31:0] := MAX(a[31:0], b[31:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMAXSS'. Intrinsic: '_mm_mask_max_ss'.
// Requires AVX512F.
func MaskMaxSs(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskMaxSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskMaxSs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzMaxSs: Compare the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', store the maximum value in the lower element of
// 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is not
// set), and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := MAX(a[31:0], b[31:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMAXSS'. Intrinsic: '_mm_maskz_max_ss'.
// Requires AVX512F.
func MaskzMaxSs(k Mmask8, a M128, b M128) M128 {
	return M128(maskzMaxSs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzMaxSs(k uint8, a [4]float32, b [4]float32) [4]float32


// MaxSs: Compare the lower single-precision (32-bit) floating-point elements
// in 'a' and 'b', store the maximum value in the lower element of 'dst', and
// copy the upper element from 'a' to the upper element of 'dst'. 
//
//		dst[31:0] := MAX(a[31:0], b[31:0])
//		dst[127:32] := a[127:32]
//
// Instruction: 'MAXSS'. Intrinsic: '_mm_max_ss'.
// Requires SSE.
func MaxSs(a M128, b M128) M128 {
	return M128(maxSs([4]float32(a), [4]float32(b)))
}

func maxSs(a [4]float32, b [4]float32) [4]float32


// MaskMin16: Compare packed 16-bit integers in 'a' and 'b', and store packed
// minimum values in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				IF a[i+15:i] < b[i+15:i]
//					dst[i+15:i] := a[i+15:i]
//				ELSE
//					dst[i+15:i] := b[i+15:i]
//				FI
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMINSW'. Intrinsic: '_mm_mask_min_epi16'.
// Requires AVX512BW.
func MaskMin16(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskMin16([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskMin16(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzMin16: Compare packed 16-bit integers in 'a' and 'b', and store packed
// minimum values in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				IF a[i+15:i] < b[i+15:i]
//					dst[i+15:i] := a[i+15:i]
//				ELSE
//					dst[i+15:i] := b[i+15:i]
//				FI
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMINSW'. Intrinsic: '_mm_maskz_min_epi16'.
// Requires AVX512BW.
func MaskzMin16(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzMin16(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzMin16(k uint8, a [16]byte, b [16]byte) [16]byte


// Min16: Compare packed 16-bit integers in 'a' and 'b', and store packed
// minimum values in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF a[i+15:i] < b[i+15:i]
//				dst[i+15:i] := a[i+15:i]
//			ELSE
//				dst[i+15:i] := b[i+15:i]
//			FI
//		ENDFOR
//
// Instruction: 'PMINSW'. Intrinsic: '_mm_min_epi16'.
// Requires SSE2.
func Min16(a M128i, b M128i) M128i {
	return M128i(min16([16]byte(a), [16]byte(b)))
}

func min16(a [16]byte, b [16]byte) [16]byte


// MaskMin32: Compare packed 32-bit integers in 'a' and 'b', and store packed
// minimum values in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] < b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMINSD'. Intrinsic: '_mm_mask_min_epi32'.
// Requires AVX512F.
func MaskMin32(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskMin32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskMin32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzMin32: Compare packed 32-bit integers in 'a' and 'b', and store packed
// minimum values in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] < b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMINSD'. Intrinsic: '_mm_maskz_min_epi32'.
// Requires AVX512F.
func MaskzMin32(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzMin32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzMin32(k uint8, a [16]byte, b [16]byte) [16]byte


// Min32: Compare packed 32-bit integers in 'a' and 'b', and store packed
// minimum values in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF a[i+31:i] < b[i+31:i]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := b[i+31:i]
//			FI
//		ENDFOR
//
// Instruction: 'PMINSD'. Intrinsic: '_mm_min_epi32'.
// Requires SSE4.1.
func Min32(a M128i, b M128i) M128i {
	return M128i(min32([16]byte(a), [16]byte(b)))
}

func min32(a [16]byte, b [16]byte) [16]byte


// MaskMin64: Compare packed 64-bit integers in 'a' and 'b', and store packed
// minimum values in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] < b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMINSQ'. Intrinsic: '_mm_mask_min_epi64'.
// Requires AVX512F.
func MaskMin64(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskMin64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskMin64(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzMin64: Compare packed 64-bit integers in 'a' and 'b', and store packed
// minimum values in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] < b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMINSQ'. Intrinsic: '_mm_maskz_min_epi64'.
// Requires AVX512F.
func MaskzMin64(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzMin64(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzMin64(k uint8, a [16]byte, b [16]byte) [16]byte


// Min64: Compare packed 64-bit integers in 'a' and 'b', and store packed
// minimum values in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF a[i+63:i] < b[i+63:i]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := b[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMINSQ'. Intrinsic: '_mm_min_epi64'.
// Requires AVX512F.
func Min64(a M128i, b M128i) M128i {
	return M128i(min64([16]byte(a), [16]byte(b)))
}

func min64(a [16]byte, b [16]byte) [16]byte


// MaskMin8: Compare packed 8-bit integers in 'a' and 'b', and store packed
// minimum values in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				IF a[i+7:i] < b[i+7:i]
//					dst[i+7:i] := a[i+7:i]
//				ELSE
//					dst[i+7:i] := b[i+7:i]
//				FI
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMINSB'. Intrinsic: '_mm_mask_min_epi8'.
// Requires AVX512BW.
func MaskMin8(src M128i, k Mmask16, a M128i, b M128i) M128i {
	return M128i(maskMin8([16]byte(src), uint16(k), [16]byte(a), [16]byte(b)))
}

func maskMin8(src [16]byte, k uint16, a [16]byte, b [16]byte) [16]byte


// MaskzMin8: Compare packed 8-bit integers in 'a' and 'b', and store packed
// minimum values in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				IF a[i+7:i] < b[i+7:i]
//					dst[i+7:i] := a[i+7:i]
//				ELSE
//					dst[i+7:i] := b[i+7:i]
//				FI
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMINSB'. Intrinsic: '_mm_maskz_min_epi8'.
// Requires AVX512BW.
func MaskzMin8(k Mmask16, a M128i, b M128i) M128i {
	return M128i(maskzMin8(uint16(k), [16]byte(a), [16]byte(b)))
}

func maskzMin8(k uint16, a [16]byte, b [16]byte) [16]byte


// Min8: Compare packed 8-bit integers in 'a' and 'b', and store packed minimum
// values in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF a[i+7:i] < b[i+7:i]
//				dst[i+7:i] := a[i+7:i]
//			ELSE
//				dst[i+7:i] := b[i+7:i]
//			FI
//		ENDFOR
//
// Instruction: 'PMINSB'. Intrinsic: '_mm_min_epi8'.
// Requires SSE4.1.
func Min8(a M128i, b M128i) M128i {
	return M128i(min8([16]byte(a), [16]byte(b)))
}

func min8(a [16]byte, b [16]byte) [16]byte


// MaskMinEpu16: Compare packed unsigned 16-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				IF a[i+15:i] < b[i+15:i]
//					dst[i+15:i] := a[i+15:i]
//				ELSE
//					dst[i+15:i] := b[i+15:i]
//				FI
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMINUW'. Intrinsic: '_mm_mask_min_epu16'.
// Requires AVX512BW.
func MaskMinEpu16(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskMinEpu16([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskMinEpu16(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzMinEpu16: Compare packed unsigned 16-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				IF a[i+15:i] < b[i+15:i]
//					dst[i+15:i] := a[i+15:i]
//				ELSE
//					dst[i+15:i] := b[i+15:i]
//				FI
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMINUW'. Intrinsic: '_mm_maskz_min_epu16'.
// Requires AVX512BW.
func MaskzMinEpu16(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzMinEpu16(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzMinEpu16(k uint8, a [16]byte, b [16]byte) [16]byte


// MinEpu16: Compare packed unsigned 16-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF a[i+15:i] < b[i+15:i]
//				dst[i+15:i] := a[i+15:i]
//			ELSE
//				dst[i+15:i] := b[i+15:i]
//			FI
//		ENDFOR
//
// Instruction: 'PMINUW'. Intrinsic: '_mm_min_epu16'.
// Requires SSE4.1.
func MinEpu16(a M128i, b M128i) M128i {
	return M128i(minEpu16([16]byte(a), [16]byte(b)))
}

func minEpu16(a [16]byte, b [16]byte) [16]byte


// MaskMinEpu32: Compare packed unsigned 32-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] < b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMINUD'. Intrinsic: '_mm_mask_min_epu32'.
// Requires AVX512F.
func MaskMinEpu32(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskMinEpu32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskMinEpu32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzMinEpu32: Compare packed unsigned 32-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] < b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMINUD'. Intrinsic: '_mm_maskz_min_epu32'.
// Requires AVX512F.
func MaskzMinEpu32(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzMinEpu32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzMinEpu32(k uint8, a [16]byte, b [16]byte) [16]byte


// MinEpu32: Compare packed unsigned 32-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF a[i+31:i] < b[i+31:i]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := b[i+31:i]
//			FI
//		ENDFOR
//
// Instruction: 'PMINUD'. Intrinsic: '_mm_min_epu32'.
// Requires SSE4.1.
func MinEpu32(a M128i, b M128i) M128i {
	return M128i(minEpu32([16]byte(a), [16]byte(b)))
}

func minEpu32(a [16]byte, b [16]byte) [16]byte


// MaskMinEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] < b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMINUQ'. Intrinsic: '_mm_mask_min_epu64'.
// Requires AVX512F.
func MaskMinEpu64(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskMinEpu64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskMinEpu64(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzMinEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] < b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMINUQ'. Intrinsic: '_mm_maskz_min_epu64'.
// Requires AVX512F.
func MaskzMinEpu64(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzMinEpu64(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzMinEpu64(k uint8, a [16]byte, b [16]byte) [16]byte


// MinEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF a[i+63:i] < b[i+63:i]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := b[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMINUQ'. Intrinsic: '_mm_min_epu64'.
// Requires AVX512F.
func MinEpu64(a M128i, b M128i) M128i {
	return M128i(minEpu64([16]byte(a), [16]byte(b)))
}

func minEpu64(a [16]byte, b [16]byte) [16]byte


// MaskMinEpu8: Compare packed unsigned 8-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				IF a[i+7:i] < b[i+7:i]
//					dst[i+7:i] := a[i+7:i]
//				ELSE
//					dst[i+7:i] := b[i+7:i]
//				FI
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMINUB'. Intrinsic: '_mm_mask_min_epu8'.
// Requires AVX512BW.
func MaskMinEpu8(src M128i, k Mmask16, a M128i, b M128i) M128i {
	return M128i(maskMinEpu8([16]byte(src), uint16(k), [16]byte(a), [16]byte(b)))
}

func maskMinEpu8(src [16]byte, k uint16, a [16]byte, b [16]byte) [16]byte


// MaskzMinEpu8: Compare packed unsigned 8-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				IF a[i+7:i] < b[i+7:i]
//					dst[i+7:i] := a[i+7:i]
//				ELSE
//					dst[i+7:i] := b[i+7:i]
//				FI
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMINUB'. Intrinsic: '_mm_maskz_min_epu8'.
// Requires AVX512BW.
func MaskzMinEpu8(k Mmask16, a M128i, b M128i) M128i {
	return M128i(maskzMinEpu8(uint16(k), [16]byte(a), [16]byte(b)))
}

func maskzMinEpu8(k uint16, a [16]byte, b [16]byte) [16]byte


// MinEpu8: Compare packed unsigned 8-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF a[i+7:i] < b[i+7:i]
//				dst[i+7:i] := a[i+7:i]
//			ELSE
//				dst[i+7:i] := b[i+7:i]
//			FI
//		ENDFOR
//
// Instruction: 'PMINUB'. Intrinsic: '_mm_min_epu8'.
// Requires SSE2.
func MinEpu8(a M128i, b M128i) M128i {
	return M128i(minEpu8([16]byte(a), [16]byte(b)))
}

func minEpu8(a [16]byte, b [16]byte) [16]byte


// MaskMinPd: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', and store packed minimum values in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMINPD'. Intrinsic: '_mm_mask_min_pd'.
// Requires AVX512F.
func MaskMinPd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskMinPd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskMinPd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzMinPd: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', and store packed minimum values in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMINPD'. Intrinsic: '_mm_maskz_min_pd'.
// Requires AVX512F.
func MaskzMinPd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzMinPd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzMinPd(k uint8, a [2]float64, b [2]float64) [2]float64


// MinPd: Compare packed double-precision (64-bit) floating-point elements in
// 'a' and 'b', and store packed minimum values in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
//		ENDFOR
//
// Instruction: 'MINPD'. Intrinsic: '_mm_min_pd'.
// Requires SSE2.
func MinPd(a M128d, b M128d) M128d {
	return M128d(minPd([2]float64(a), [2]float64(b)))
}

func minPd(a [2]float64, b [2]float64) [2]float64


// MaskMinPs: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', and store packed minimum values in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMINPS'. Intrinsic: '_mm_mask_min_ps'.
// Requires AVX512F.
func MaskMinPs(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskMinPs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskMinPs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzMinPs: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', and store packed minimum values in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMINPS'. Intrinsic: '_mm_maskz_min_ps'.
// Requires AVX512F.
func MaskzMinPs(k Mmask8, a M128, b M128) M128 {
	return M128(maskzMinPs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzMinPs(k uint8, a [4]float32, b [4]float32) [4]float32


// MinPs: Compare packed single-precision (32-bit) floating-point elements in
// 'a' and 'b', and store packed minimum values in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
//		ENDFOR
//
// Instruction: 'MINPS'. Intrinsic: '_mm_min_ps'.
// Requires SSE.
func MinPs(a M128, b M128) M128 {
	return M128(minPs([4]float32(a), [4]float32(b)))
}

func minPs(a [4]float32, b [4]float32) [4]float32


// MaskMinRoundSd: Compare the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', store the minimum value in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'src' when mask bit 0
// is not set), and copy the upper element from 'a' to the upper element of
// 'dst'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		IF k[0]
//			dst[63:0] := MIN(a[63:0], b[63:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMINSD'. Intrinsic: '_mm_mask_min_round_sd'.
// Requires AVX512F.
func MaskMinRoundSd(src M128d, k Mmask8, a M128d, b M128d, sae int) M128d {
	return M128d(maskMinRoundSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), sae))
}

func maskMinRoundSd(src [2]float64, k uint8, a [2]float64, b [2]float64, sae int) [2]float64


// MaskzMinRoundSd: Compare the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', store the minimum value in the lower element of
// 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is not
// set), and copy the upper element from 'a' to the upper element of 'dst'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		IF k[0]
//			dst[63:0] := MIN(a[63:0], b[63:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMINSD'. Intrinsic: '_mm_maskz_min_round_sd'.
// Requires AVX512F.
func MaskzMinRoundSd(k Mmask8, a M128d, b M128d, sae int) M128d {
	return M128d(maskzMinRoundSd(uint8(k), [2]float64(a), [2]float64(b), sae))
}

func maskzMinRoundSd(k uint8, a [2]float64, b [2]float64, sae int) [2]float64


// MinRoundSd: Compare the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', store the minimum value in the lower element of
// 'dst' , and copy the upper element from 'a' to the upper element of 'dst'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		dst[63:0] := MIN(a[63:0], b[63:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMINSD'. Intrinsic: '_mm_min_round_sd'.
// Requires AVX512F.
func MinRoundSd(a M128d, b M128d, sae int) M128d {
	return M128d(minRoundSd([2]float64(a), [2]float64(b), sae))
}

func minRoundSd(a [2]float64, b [2]float64, sae int) [2]float64


// MaskMinRoundSs: Compare the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', store the minimum value in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'src' when mask bit 0
// is not set), and copy the upper element from 'a' to the upper element of
// 'dst'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		IF k[0]
//			dst[31:0] := MIN(a[31:0], b[31:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMINSS'. Intrinsic: '_mm_mask_min_round_ss'.
// Requires AVX512F.
func MaskMinRoundSs(src M128, k Mmask8, a M128, b M128, sae int) M128 {
	return M128(maskMinRoundSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), sae))
}

func maskMinRoundSs(src [4]float32, k uint8, a [4]float32, b [4]float32, sae int) [4]float32


// MaskzMinRoundSs: Compare the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', store the minimum value in the lower element of
// 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is not
// set), and copy the upper element from 'a' to the upper element of 'dst'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		IF k[0]
//			dst[31:0] := MIN(a[31:0], b[31:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMINSS'. Intrinsic: '_mm_maskz_min_round_ss'.
// Requires AVX512F.
func MaskzMinRoundSs(k Mmask8, a M128, b M128, sae int) M128 {
	return M128(maskzMinRoundSs(uint8(k), [4]float32(a), [4]float32(b), sae))
}

func maskzMinRoundSs(k uint8, a [4]float32, b [4]float32, sae int) [4]float32


// MinRoundSs: Compare the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', store the minimum value in the lower element of
// 'dst', and copy the upper element from 'a' to the upper element of 'dst'.
// 	Pass __MM_FROUND_NO_EXC to 'sae' to suppress all exceptions. 
//
//		dst[31:0] := MIN(a[31:0], b[31:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMINSS'. Intrinsic: '_mm_min_round_ss'.
// Requires AVX512F.
func MinRoundSs(a M128, b M128, sae int) M128 {
	return M128(minRoundSs([4]float32(a), [4]float32(b), sae))
}

func minRoundSs(a [4]float32, b [4]float32, sae int) [4]float32


// MaskMinSd: Compare the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', store the minimum value in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'src' when mask bit 0
// is not set), and copy the upper element from 'a' to the upper element of
// 'dst'. 
//
//		IF k[0]
//			dst[63:0] := MIN(a[63:0], b[63:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMINSD'. Intrinsic: '_mm_mask_min_sd'.
// Requires AVX512F.
func MaskMinSd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskMinSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskMinSd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzMinSd: Compare the lower double-precision (64-bit) floating-point
// elements in 'a' and 'b', store the minimum value in the lower element of
// 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is not
// set), and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := MIN(a[63:0], b[63:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMINSD'. Intrinsic: '_mm_maskz_min_sd'.
// Requires AVX512F.
func MaskzMinSd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzMinSd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzMinSd(k uint8, a [2]float64, b [2]float64) [2]float64


// MinSd: Compare the lower double-precision (64-bit) floating-point elements
// in 'a' and 'b', store the minimum value in the lower element of 'dst', and
// copy the upper element from 'a' to the upper element of 'dst'. 
//
//		dst[63:0] := MIN(a[63:0], b[63:0])
//		dst[127:64] := a[127:64]
//
// Instruction: 'MINSD'. Intrinsic: '_mm_min_sd'.
// Requires SSE2.
func MinSd(a M128d, b M128d) M128d {
	return M128d(minSd([2]float64(a), [2]float64(b)))
}

func minSd(a [2]float64, b [2]float64) [2]float64


// MaskMinSs: Compare the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', store the minimum value in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'src' when mask bit 0
// is not set), and copy the upper element from 'a' to the upper element of
// 'dst'. 
//
//		IF k[0]
//			dst[31:0] := MIN(a[31:0], b[31:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMINSS'. Intrinsic: '_mm_mask_min_ss'.
// Requires AVX512F.
func MaskMinSs(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskMinSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskMinSs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzMinSs: Compare the lower single-precision (32-bit) floating-point
// elements in 'a' and 'b', store the minimum value in the lower element of
// 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is not
// set), and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := MIN(a[31:0], b[31:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMINSS'. Intrinsic: '_mm_maskz_min_ss'.
// Requires AVX512F.
func MaskzMinSs(k Mmask8, a M128, b M128) M128 {
	return M128(maskzMinSs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzMinSs(k uint8, a [4]float32, b [4]float32) [4]float32


// MinSs: Compare the lower single-precision (32-bit) floating-point elements
// in 'a' and 'b', store the minimum value in the lower element of 'dst', and
// copy the upper element from 'a' to the upper element of 'dst'. 
//
//		dst[31:0] := MIN(a[31:0], b[31:0])
//		dst[127:32] := a[127:32]
//
// Instruction: 'MINSS'. Intrinsic: '_mm_min_ss'.
// Requires SSE.
func MinSs(a M128, b M128) M128 {
	return M128(minSs([4]float32(a), [4]float32(b)))
}

func minSs(a [4]float32, b [4]float32) [4]float32


// MinposEpu16: Horizontally compute the minimum amongst the packed unsigned
// 16-bit integers in 'a', store the minimum and index in 'dst', and zero the
// remaining bits in 'dst'. 
//
//		index[2:0] := 0
//		min[15:0] := a[15:0]
//		FOR j := 0 to 7
//			i := j*16
//			IF a[i+15:i] < min[15:0]
//				index[2:0] := j
//				min[15:0] := a[i+15:i]
//			FI
//		ENDFOR
//		dst[15:0] := min[15:0]
//		dst[18:16] := index[2:0]
//		dst[127:19] := 0
//
// Instruction: 'PHMINPOSUW'. Intrinsic: '_mm_minpos_epu16'.
// Requires SSE4.1.
func MinposEpu16(a M128i) M128i {
	return M128i(minposEpu16([16]byte(a)))
}

func minposEpu16(a [16]byte) [16]byte


// MaskMov16: Move packed 16-bit integers from 'a' into 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := a[i+15:i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVDQU16'. Intrinsic: '_mm_mask_mov_epi16'.
// Requires AVX512BW.
func MaskMov16(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskMov16([16]byte(src), uint8(k), [16]byte(a)))
}

func maskMov16(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzMov16: Move packed 16-bit integers from 'a' into 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := a[i+15:i]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVDQU16'. Intrinsic: '_mm_maskz_mov_epi16'.
// Requires AVX512BW.
func MaskzMov16(k Mmask8, a M128i) M128i {
	return M128i(maskzMov16(uint8(k), [16]byte(a)))
}

func maskzMov16(k uint8, a [16]byte) [16]byte


// MaskMov32: Move packed 32-bit integers from 'a' to 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVDQA32'. Intrinsic: '_mm_mask_mov_epi32'.
// Requires AVX512F.
func MaskMov32(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskMov32([16]byte(src), uint8(k), [16]byte(a)))
}

func maskMov32(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzMov32: Move packed 32-bit integers from 'a' into 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVDQA32'. Intrinsic: '_mm_maskz_mov_epi32'.
// Requires AVX512F.
func MaskzMov32(k Mmask8, a M128i) M128i {
	return M128i(maskzMov32(uint8(k), [16]byte(a)))
}

func maskzMov32(k uint8, a [16]byte) [16]byte


// MaskMov64: Move packed 64-bit integers from 'a' to 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVDQA64'. Intrinsic: '_mm_mask_mov_epi64'.
// Requires AVX512F.
func MaskMov64(src M128i, k Mmask8, a M128i) M128i {
	return M128i(maskMov64([16]byte(src), uint8(k), [16]byte(a)))
}

func maskMov64(src [16]byte, k uint8, a [16]byte) [16]byte


// MaskzMov64: Move packed 64-bit integers from 'a' into 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVDQA64'. Intrinsic: '_mm_maskz_mov_epi64'.
// Requires AVX512F.
func MaskzMov64(k Mmask8, a M128i) M128i {
	return M128i(maskzMov64(uint8(k), [16]byte(a)))
}

func maskzMov64(k uint8, a [16]byte) [16]byte


// MaskMov8: Move packed 8-bit integers from 'a' into 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := a[i+7:i]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVDQU8'. Intrinsic: '_mm_mask_mov_epi8'.
// Requires AVX512BW.
func MaskMov8(src M128i, k Mmask16, a M128i) M128i {
	return M128i(maskMov8([16]byte(src), uint16(k), [16]byte(a)))
}

func maskMov8(src [16]byte, k uint16, a [16]byte) [16]byte


// MaskzMov8: Move packed 8-bit integers from 'a' into 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := a[i+7:i]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVDQU8'. Intrinsic: '_mm_maskz_mov_epi8'.
// Requires AVX512BW.
func MaskzMov8(k Mmask16, a M128i) M128i {
	return M128i(maskzMov8(uint16(k), [16]byte(a)))
}

func maskzMov8(k uint16, a [16]byte) [16]byte


// MaskMovPd: Move packed double-precision (64-bit) floating-point elements
// from 'a' to 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVAPD'. Intrinsic: '_mm_mask_mov_pd'.
// Requires AVX512F.
func MaskMovPd(src M128d, k Mmask8, a M128d) M128d {
	return M128d(maskMovPd([2]float64(src), uint8(k), [2]float64(a)))
}

func maskMovPd(src [2]float64, k uint8, a [2]float64) [2]float64


// MaskzMovPd: Move packed double-precision (64-bit) floating-point elements
// from 'a' into 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVAPD'. Intrinsic: '_mm_maskz_mov_pd'.
// Requires AVX512F.
func MaskzMovPd(k Mmask8, a M128d) M128d {
	return M128d(maskzMovPd(uint8(k), [2]float64(a)))
}

func maskzMovPd(k uint8, a [2]float64) [2]float64


// MaskMovPs: Move packed single-precision (32-bit) floating-point elements
// from 'a' to 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVAPS'. Intrinsic: '_mm_mask_mov_ps'.
// Requires AVX512F.
func MaskMovPs(src M128, k Mmask8, a M128) M128 {
	return M128(maskMovPs([4]float32(src), uint8(k), [4]float32(a)))
}

func maskMovPs(src [4]float32, k uint8, a [4]float32) [4]float32


// MaskzMovPs: Move packed single-precision (32-bit) floating-point elements
// from 'a' into 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVAPS'. Intrinsic: '_mm_maskz_mov_ps'.
// Requires AVX512F.
func MaskzMovPs(k Mmask8, a M128) M128 {
	return M128(maskzMovPs(uint8(k), [4]float32(a)))
}

func maskzMovPs(k uint8, a [4]float32) [4]float32


// Move64: Copy the lower 64-bit integer in 'a' to the lower element of 'dst',
// and zero the upper element. 
//
//		dst[63:0] := a[63:0]
//		dst[127:64] := 0
//
// Instruction: 'MOVQ'. Intrinsic: '_mm_move_epi64'.
// Requires SSE2.
func Move64(a M128i) M128i {
	return M128i(move64([16]byte(a)))
}

func move64(a [16]byte) [16]byte


// MaskMoveSd: Move the lower double-precision (64-bit) floating-point element
// from 'b' to the lower element of 'dst' using writemask 'k' (the element is
// copied from 'src' when mask bit 0 is not set), and copy the upper element
// from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := b[63:0]
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVSD'. Intrinsic: '_mm_mask_move_sd'.
// Requires AVX512F.
func MaskMoveSd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskMoveSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskMoveSd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzMoveSd: Move the lower double-precision (64-bit) floating-point element
// from 'b' to the lower element of 'dst' using zeromask 'k' (the element is
// zeroed out when mask bit 0 is not set), and copy the upper element from 'a'
// to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := b[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVSD'. Intrinsic: '_mm_maskz_move_sd'.
// Requires AVX512F.
func MaskzMoveSd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzMoveSd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzMoveSd(k uint8, a [2]float64, b [2]float64) [2]float64


// MoveSd: Move the lower double-precision (64-bit) floating-point element from
// 'b' to the lower element of 'dst', and copy the upper element from 'a' to
// the upper element of 'dst'. 
//
//		dst[63:0] := b[63:0]
//		dst[127:64] := a[127:64]
//
// Instruction: 'MOVSD'. Intrinsic: '_mm_move_sd'.
// Requires SSE2.
func MoveSd(a M128d, b M128d) M128d {
	return M128d(moveSd([2]float64(a), [2]float64(b)))
}

func moveSd(a [2]float64, b [2]float64) [2]float64


// MaskMoveSs: Move the lower single-precision (32-bit) floating-point element
// from 'b' to the lower element of 'dst' using writemask 'k' (the element is
// copied from 'src' when mask bit 0 is not set), and copy the upper 3 packed
// elements from 'a' to the upper elements of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := b[31:0]
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVSS'. Intrinsic: '_mm_mask_move_ss'.
// Requires AVX512F.
func MaskMoveSs(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskMoveSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskMoveSs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzMoveSs: Move the lower single-precision (32-bit) floating-point element
// from 'b' to the lower element of 'dst' using zeromask 'k' (the element is
// zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements
// from 'a' to the upper elements of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := b[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVSS'. Intrinsic: '_mm_maskz_move_ss'.
// Requires AVX512F.
func MaskzMoveSs(k Mmask8, a M128, b M128) M128 {
	return M128(maskzMoveSs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzMoveSs(k uint8, a [4]float32, b [4]float32) [4]float32


// MoveSs: Move the lower single-precision (32-bit) floating-point element from
// 'b' to the lower element of 'dst', and copy the upper 3 elements from 'a' to
// the upper elements of 'dst'. 
//
//		dst[31:0] := b[31:0]
//		dst[63:32] := a[63:32]
//		dst[95:64] := a[95:64]
//		dst[127:96] := a[127:96]
//
// Instruction: 'MOVSS'. Intrinsic: '_mm_move_ss'.
// Requires SSE.
func MoveSs(a M128, b M128) M128 {
	return M128(moveSs([4]float32(a), [4]float32(b)))
}

func moveSs(a [4]float32, b [4]float32) [4]float32


// MaskMovedupPd: Duplicate even-indexed double-precision (64-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp[63:0] := a[63:0]
//		tmp[127:64] := a[63:0]
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVDDUP'. Intrinsic: '_mm_mask_movedup_pd'.
// Requires AVX512F.
func MaskMovedupPd(src M128d, k Mmask8, a M128d) M128d {
	return M128d(maskMovedupPd([2]float64(src), uint8(k), [2]float64(a)))
}

func maskMovedupPd(src [2]float64, k uint8, a [2]float64) [2]float64


// MaskzMovedupPd: Duplicate even-indexed double-precision (64-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp[63:0] := a[63:0]
//		tmp[127:64] := a[63:0]
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVDDUP'. Intrinsic: '_mm_maskz_movedup_pd'.
// Requires AVX512F.
func MaskzMovedupPd(k Mmask8, a M128d) M128d {
	return M128d(maskzMovedupPd(uint8(k), [2]float64(a)))
}

func maskzMovedupPd(k uint8, a [2]float64) [2]float64


// MovedupPd: Duplicate the low double-precision (64-bit) floating-point
// element from 'a', and store the results in 'dst'. 
//
//		tmp[63:0] := a[63:0]
//		tmp[127:64] := a[63:0]
//
// Instruction: 'MOVDDUP'. Intrinsic: '_mm_movedup_pd'.
// Requires SSE3.
func MovedupPd(a M128d) M128d {
	return M128d(movedupPd([2]float64(a)))
}

func movedupPd(a [2]float64) [2]float64


// MaskMovehdupPs: Duplicate odd-indexed single-precision (32-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp[31:0] := a[63:32] 
//		tmp[63:32] := a[63:32] 
//		tmp[95:64] := a[127:96] 
//		tmp[127:96] := a[127:96]
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVSHDUP'. Intrinsic: '_mm_mask_movehdup_ps'.
// Requires AVX512F.
func MaskMovehdupPs(src M128, k Mmask8, a M128) M128 {
	return M128(maskMovehdupPs([4]float32(src), uint8(k), [4]float32(a)))
}

func maskMovehdupPs(src [4]float32, k uint8, a [4]float32) [4]float32


// MaskzMovehdupPs: Duplicate odd-indexed single-precision (32-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp[31:0] := a[63:32] 
//		tmp[63:32] := a[63:32] 
//		tmp[95:64] := a[127:96] 
//		tmp[127:96] := a[127:96]
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVSHDUP'. Intrinsic: '_mm_maskz_movehdup_ps'.
// Requires AVX512F.
func MaskzMovehdupPs(k Mmask8, a M128) M128 {
	return M128(maskzMovehdupPs(uint8(k), [4]float32(a)))
}

func maskzMovehdupPs(k uint8, a [4]float32) [4]float32


// MovehdupPs: Duplicate odd-indexed single-precision (32-bit) floating-point
// elements from 'a', and store the results in 'dst'. 
//
//		dst[31:0] := a[63:32] 
//		dst[63:32] := a[63:32]
//		dst[95:64] := a[127:96] 
//		dst[127:96] := a[127:96]
//
// Instruction: 'MOVSHDUP'. Intrinsic: '_mm_movehdup_ps'.
// Requires SSE3.
func MovehdupPs(a M128) M128 {
	return M128(movehdupPs([4]float32(a)))
}

func movehdupPs(a [4]float32) [4]float32


// MovehlPs: Move the upper 2 single-precision (32-bit) floating-point elements
// from 'b' to the lower 2 elements of 'dst', and copy the upper 2 elements
// from 'a' to the upper 2 elements of 'dst'. 
//
//		dst[31:0] := b[95:64]
//		dst[63:32] := b[127:96]
//		dst[95:64] := a[95:64]
//		dst[127:96] := a[127:96]
//
// Instruction: 'MOVHLPS'. Intrinsic: '_mm_movehl_ps'.
// Requires SSE.
func MovehlPs(a M128, b M128) M128 {
	return M128(movehlPs([4]float32(a), [4]float32(b)))
}

func movehlPs(a [4]float32, b [4]float32) [4]float32


// MaskMoveldupPs: Duplicate even-indexed single-precision (32-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp[31:0] := a[31:0] 
//		tmp[63:32] := a[31:0] 
//		tmp[95:64] := a[95:64] 
//		tmp[127:96] := a[95:64]
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR	
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVSLDUP'. Intrinsic: '_mm_mask_moveldup_ps'.
// Requires AVX512F.
func MaskMoveldupPs(src M128, k Mmask8, a M128) M128 {
	return M128(maskMoveldupPs([4]float32(src), uint8(k), [4]float32(a)))
}

func maskMoveldupPs(src [4]float32, k uint8, a [4]float32) [4]float32


// MaskzMoveldupPs: Duplicate even-indexed single-precision (32-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp[31:0] := a[31:0] 
//		tmp[63:32] := a[31:0] 
//		tmp[95:64] := a[95:64] 
//		tmp[127:96] := a[95:64]
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVSLDUP'. Intrinsic: '_mm_maskz_moveldup_ps'.
// Requires AVX512F.
func MaskzMoveldupPs(k Mmask8, a M128) M128 {
	return M128(maskzMoveldupPs(uint8(k), [4]float32(a)))
}

func maskzMoveldupPs(k uint8, a [4]float32) [4]float32


// MoveldupPs: Duplicate even-indexed single-precision (32-bit) floating-point
// elements from 'a', and store the results in 'dst'. 
//
//		dst[31:0] := a[31:0] 
//		dst[63:32] := a[31:0]
//		dst[95:64] := a[95:64] 
//		dst[127:96] := a[95:64]
//
// Instruction: 'MOVSLDUP'. Intrinsic: '_mm_moveldup_ps'.
// Requires SSE3.
func MoveldupPs(a M128) M128 {
	return M128(moveldupPs([4]float32(a)))
}

func moveldupPs(a [4]float32) [4]float32


// MovelhPs: Move the lower 2 single-precision (32-bit) floating-point elements
// from 'b' to the upper 2 elements of 'dst', and copy the lower 2 elements
// from 'a' to the lower 2 elements of 'dst'. 
//
//		dst[31:0] := a[31:0]
//		dst[63:32] := a[63:32]
//		dst[95:64] := b[31:0]
//		dst[127:96] := b[63:32]
//
// Instruction: 'MOVLHPS'. Intrinsic: '_mm_movelh_ps'.
// Requires SSE.
func MovelhPs(a M128, b M128) M128 {
	return M128(movelhPs([4]float32(a), [4]float32(b)))
}

func movelhPs(a [4]float32, b [4]float32) [4]float32


// Movemask8: Create mask from the most significant bit of each 8-bit element
// in 'a', and store the result in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*8
//			dst[j] := a[i+7]
//		ENDFOR
//		dst[MAX:16] := 0
//
// Instruction: 'PMOVMSKB'. Intrinsic: '_mm_movemask_epi8'.
// Requires SSE2.
func Movemask8(a M128i) int {
	return int(movemask8([16]byte(a)))
}

func movemask8(a [16]byte) int


// MovemaskPd: Set each bit of mask 'dst' based on the most significant bit of
// the corresponding packed double-precision (64-bit) floating-point element in
// 'a'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF a[i+63]
//				dst[j] := 1
//			ELSE
//				dst[j] := 0
//			FI
//		ENDFOR
//		dst[MAX:2] := 0
//
// Instruction: 'MOVMSKPD'. Intrinsic: '_mm_movemask_pd'.
// Requires SSE2.
func MovemaskPd(a M128d) int {
	return int(movemaskPd([2]float64(a)))
}

func movemaskPd(a [2]float64) int


// MovemaskPs: Set each bit of mask 'dst' based on the most significant bit of
// the corresponding packed single-precision (32-bit) floating-point element in
// 'a'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF a[i+31]
//				dst[j] := 1
//			ELSE
//				dst[j] := 0
//			FI
//		ENDFOR
//		dst[MAX:4] := 0
//
// Instruction: 'MOVMSKPS'. Intrinsic: '_mm_movemask_ps'.
// Requires SSE.
func MovemaskPs(a M128) int {
	return int(movemaskPs([4]float32(a)))
}

func movemaskPs(a [4]float32) int


// Movm16: Set each packed 16-bit integer in 'dst' to all ones or all zeros
// based on the value of the corresponding bit in 'k'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := 0xFFFF
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVM2W'. Intrinsic: '_mm_movm_epi16'.
// Requires AVX512BW.
func Movm16(k Mmask8) M128i {
	return M128i(movm16(uint8(k)))
}

func movm16(k uint8) [16]byte


// Movm32: Set each packed 32-bit integer in 'dst' to all ones or all zeros
// based on the value of the corresponding bit in 'k'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := 0xFFFFFFFF
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVM2D'. Intrinsic: '_mm_movm_epi32'.
// Requires AVX512DQ.
func Movm32(k Mmask8) M128i {
	return M128i(movm32(uint8(k)))
}

func movm32(k uint8) [16]byte


// Movm64: Set each packed 64-bit integer in 'dst' to all ones or all zeros
// based on the value of the corresponding bit in 'k'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := 0xFFFFFFFFffffffff
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVM2Q'. Intrinsic: '_mm_movm_epi64'.
// Requires AVX512DQ.
func Movm64(k Mmask8) M128i {
	return M128i(movm64(uint8(k)))
}

func movm64(k uint8) [16]byte


// Movm8: Set each packed 8-bit integer in 'dst' to all ones or all zeros based
// on the value of the corresponding bit in 'k'. 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := 0xFF
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVM2B'. Intrinsic: '_mm_movm_epi8'.
// Requires AVX512VL.
func Movm8(k Mmask16) M128i {
	return M128i(movm8(uint16(k)))
}

func movm8(k uint16) [16]byte


// Movpi6464: Copy the 64-bit integer 'a' to the lower element of 'dst', and
// zero the upper element. 
//
//		dst[63:0] := a[63:0]
//		dst[127:64] := 0
//
// Instruction: 'MOVQ2DQ'. Intrinsic: '_mm_movpi64_epi64'.
// Requires SSE2.
func Movpi6464(a M64) M128i {
	return M128i(movpi6464(a))
}

func movpi6464(a M64) [16]byte


// MpsadbwEpu8: Compute the sum of absolute differences (SADs) of quadruplets
// of unsigned 8-bit integers in 'a' compared to those in 'b', and store the
// 16-bit results in 'dst'.
// 	Eight SADs are performed using one quadruplet from 'b' and eight
// quadruplets from 'a'. One quadruplet is selected from 'b' starting at on the
// offset specified in 'imm8'. Eight quadruplets are formed from sequential
// 8-bit integers selected from 'a' starting at the offset specified in 'imm8'. 
//
//		MPSADBW(a[127:0], b[127:0], imm8[2:0]) {
//			a_offset := imm8[2]*32
//			b_offset := imm8[1:0]*32
//			FOR j := 0 to 7
//				i := j*8
//				k := a_offset+i
//				l := b_offset
//				tmp[i+15:i] := ABS(a[k+7:k] - b[l+7:l]) + ABS(a[k+15:k+8] - b[l+15:l+8]) + ABS(a[k+23:k+16] - b[l+23:l+16]) + ABS(a[k+31:k+24] - b[l+31:l+24])
//			ENDFOR
//			RETURN tmp[127:0]
//		}
//		
//		dst[127:0] := MPSADBW(a[127:0], b[127:0], imm8[2:0])
//
// Instruction: 'MPSADBW'. Intrinsic: '_mm_mpsadbw_epu8'.
// Requires SSE4.1.
func MpsadbwEpu8(a M128i, b M128i, imm8 int) M128i {
	return M128i(mpsadbwEpu8([16]byte(a), [16]byte(b), imm8))
}

func mpsadbwEpu8(a [16]byte, b [16]byte, imm8 int) [16]byte


// MaskMul32: Multiply the low 32-bit integers from each packed 64-bit element
// in 'a' and 'b', and store the signed 64-bit results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMULDQ'. Intrinsic: '_mm_mask_mul_epi32'.
// Requires AVX512F.
func MaskMul32(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskMul32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskMul32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzMul32: Multiply the low 32-bit integers from each packed 64-bit element
// in 'a' and 'b', and store the signed 64-bit results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMULDQ'. Intrinsic: '_mm_maskz_mul_epi32'.
// Requires AVX512F.
func MaskzMul32(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzMul32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzMul32(k uint8, a [16]byte, b [16]byte) [16]byte


// Mul32: Multiply the low 32-bit integers from each packed 64-bit element in
// 'a' and 'b', and store the signed 64-bit results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := a[i+31:i] * b[i+31:i]
//		ENDFOR
//
// Instruction: 'PMULDQ'. Intrinsic: '_mm_mul_epi32'.
// Requires SSE4.1.
func Mul32(a M128i, b M128i) M128i {
	return M128i(mul32([16]byte(a), [16]byte(b)))
}

func mul32(a [16]byte, b [16]byte) [16]byte


// MaskMulEpu32: Multiply the low unsigned 32-bit integers from each packed
// 64-bit element in 'a' and 'b', and store the unsigned 64-bit results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMULUDQ'. Intrinsic: '_mm_mask_mul_epu32'.
// Requires AVX512F.
func MaskMulEpu32(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskMulEpu32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskMulEpu32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzMulEpu32: Multiply the low unsigned 32-bit integers from each packed
// 64-bit element in 'a' and 'b', and store the unsigned 64-bit results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMULUDQ'. Intrinsic: '_mm_maskz_mul_epu32'.
// Requires AVX512F.
func MaskzMulEpu32(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzMulEpu32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzMulEpu32(k uint8, a [16]byte, b [16]byte) [16]byte


// MulEpu32: Multiply the low unsigned 32-bit integers from each packed 64-bit
// element in 'a' and 'b', and store the unsigned 64-bit results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := a[i+31:i] * b[i+31:i]
//		ENDFOR
//
// Instruction: 'PMULUDQ'. Intrinsic: '_mm_mul_epu32'.
// Requires SSE2.
func MulEpu32(a M128i, b M128i) M128i {
	return M128i(mulEpu32([16]byte(a), [16]byte(b)))
}

func mulEpu32(a [16]byte, b [16]byte) [16]byte


// MaskMulPd: Multiply packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] * b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMULPD'. Intrinsic: '_mm_mask_mul_pd'.
// Requires AVX512F.
func MaskMulPd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskMulPd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskMulPd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzMulPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] * b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMULPD'. Intrinsic: '_mm_maskz_mul_pd'.
// Requires AVX512F.
func MaskzMulPd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzMulPd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzMulPd(k uint8, a [2]float64, b [2]float64) [2]float64


// MulPd: Multiply packed double-precision (64-bit) floating-point elements in
// 'a' and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := a[i+63:i] * b[i+63:i]
//		ENDFOR
//
// Instruction: 'MULPD'. Intrinsic: '_mm_mul_pd'.
// Requires SSE2.
func MulPd(a M128d, b M128d) M128d {
	return M128d(mulPd([2]float64(a), [2]float64(b)))
}

func mulPd(a [2]float64, b [2]float64) [2]float64


// MaskMulPs: Multiply packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set).  RM. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMULPS'. Intrinsic: '_mm_mask_mul_ps'.
// Requires AVX512F.
func MaskMulPs(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskMulPs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskMulPs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzMulPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMULPS'. Intrinsic: '_mm_maskz_mul_ps'.
// Requires AVX512F.
func MaskzMulPs(k Mmask8, a M128, b M128) M128 {
	return M128(maskzMulPs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzMulPs(k uint8, a [4]float32, b [4]float32) [4]float32


// MulPs: Multiply packed single-precision (32-bit) floating-point elements in
// 'a' and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := a[i+31:i] * b[i+31:i]
//		ENDFOR
//
// Instruction: 'MULPS'. Intrinsic: '_mm_mul_ps'.
// Requires SSE.
func MulPs(a M128, b M128) M128 {
	return M128(mulPs([4]float32(a), [4]float32(b)))
}

func mulPs(a [4]float32, b [4]float32) [4]float32


// MaskMulRoundSd: Multiply the lower double-precision (64-bit) floating-point
// element in 'a' and 'b', store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'src' when mask bit 0 is not set),
// and copy the upper element from 'a' to the upper element of 'dst'.
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := a[63:0] * b[63:0]
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMULSD'. Intrinsic: '_mm_mask_mul_round_sd'.
// Requires AVX512F.
func MaskMulRoundSd(src M128d, k Mmask8, a M128d, b M128d, rounding int) M128d {
	return M128d(maskMulRoundSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), rounding))
}

func maskMulRoundSd(src [2]float64, k uint8, a [2]float64, b [2]float64, rounding int) [2]float64


// MaskzMulRoundSd: Multiply the lower double-precision (64-bit) floating-point
// element in 'a' and 'b', store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper element from 'a' to the upper element of 'dst'.
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := a[63:0] * b[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMULSD'. Intrinsic: '_mm_maskz_mul_round_sd'.
// Requires AVX512F.
func MaskzMulRoundSd(k Mmask8, a M128d, b M128d, rounding int) M128d {
	return M128d(maskzMulRoundSd(uint8(k), [2]float64(a), [2]float64(b), rounding))
}

func maskzMulRoundSd(k uint8, a [2]float64, b [2]float64, rounding int) [2]float64


// MulRoundSd: Multiply the lower double-precision (64-bit) floating-point
// element in 'a' and 'b', store the result in the lower element of 'dst', and
// copy the upper element from 'a' to the upper element of 'dst'.
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := a[63:0] * b[63:0]
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMULSD'. Intrinsic: '_mm_mul_round_sd'.
// Requires AVX512F.
func MulRoundSd(a M128d, b M128d, rounding int) M128d {
	return M128d(mulRoundSd([2]float64(a), [2]float64(b), rounding))
}

func mulRoundSd(a [2]float64, b [2]float64, rounding int) [2]float64


// MaskMulRoundSs: Multiply the lower single-precision (32-bit) floating-point
// element in 'a' and 'b', store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'src' when mask bit 0 is not set),
// and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'.
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := a[31:0] * b[31:0]
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMULSS'. Intrinsic: '_mm_mask_mul_round_ss'.
// Requires AVX512F.
func MaskMulRoundSs(src M128, k Mmask8, a M128, b M128, rounding int) M128 {
	return M128(maskMulRoundSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), rounding))
}

func maskMulRoundSs(src [4]float32, k uint8, a [4]float32, b [4]float32, rounding int) [4]float32


// MaskzMulRoundSs: Multiply the lower single-precision (32-bit) floating-point
// element in 'a' and 'b', store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'.
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := a[31:0] * b[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMULSS'. Intrinsic: '_mm_maskz_mul_round_ss'.
// Requires AVX512F.
func MaskzMulRoundSs(k Mmask8, a M128, b M128, rounding int) M128 {
	return M128(maskzMulRoundSs(uint8(k), [4]float32(a), [4]float32(b), rounding))
}

func maskzMulRoundSs(k uint8, a [4]float32, b [4]float32, rounding int) [4]float32


// MulRoundSs: Multiply the lower single-precision (32-bit) floating-point
// element in 'a' and 'b', store the result in the lower element of 'dst', and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'.
// 		Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := a[31:0] * b[31:0]
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMULSS'. Intrinsic: '_mm_mul_round_ss'.
// Requires AVX512F.
func MulRoundSs(a M128, b M128, rounding int) M128 {
	return M128(mulRoundSs([4]float32(a), [4]float32(b), rounding))
}

func mulRoundSs(a [4]float32, b [4]float32, rounding int) [4]float32


// MaskMulSd: Multiply the lower double-precision (64-bit) floating-point
// element in 'a' and 'b', store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'src' when mask bit 0 is not set),
// and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := a[63:0] * b[63:0]
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMULSD'. Intrinsic: '_mm_mask_mul_sd'.
// Requires AVX512F.
func MaskMulSd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskMulSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskMulSd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzMulSd: Multiply the lower double-precision (64-bit) floating-point
// element in 'a' and 'b', store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := a[63:0] * b[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VMULSD'. Intrinsic: '_mm_maskz_mul_sd'.
// Requires AVX512F.
func MaskzMulSd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzMulSd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzMulSd(k uint8, a [2]float64, b [2]float64) [2]float64


// MulSd: Multiply the lower double-precision (64-bit) floating-point element
// in 'a' and 'b', store the result in the lower element of 'dst', and copy the
// upper element from 'a' to the upper element of 'dst'. 
//
//		dst[63:0] := a[63:0] * b[63:0]
//		dst[127:64] := a[127:64]
//
// Instruction: 'MULSD'. Intrinsic: '_mm_mul_sd'.
// Requires SSE2.
func MulSd(a M128d, b M128d) M128d {
	return M128d(mulSd([2]float64(a), [2]float64(b)))
}

func mulSd(a [2]float64, b [2]float64) [2]float64


// MaskMulSs: Multiply the lower single-precision (32-bit) floating-point
// element in 'a' and 'b', store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'src' when mask bit 0 is not set),
// and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'. 
//
//		IF k[0]
//			dst[31:0] := a[31:0] * b[31:0]
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMULSS'. Intrinsic: '_mm_mask_mul_ss'.
// Requires AVX512F.
func MaskMulSs(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskMulSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskMulSs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzMulSs: Multiply the lower single-precision (32-bit) floating-point
// element in 'a' and 'b', store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := a[31:0] * b[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VMULSS'. Intrinsic: '_mm_maskz_mul_ss'.
// Requires AVX512F.
func MaskzMulSs(k Mmask8, a M128, b M128) M128 {
	return M128(maskzMulSs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzMulSs(k uint8, a [4]float32, b [4]float32) [4]float32


// MulSs: Multiply the lower single-precision (32-bit) floating-point element
// in 'a' and 'b', store the result in the lower element of 'dst', and copy the
// upper 3 packed elements from 'a' to the upper elements of 'dst'. 
//
//		dst[31:0] := a[31:0] * b[31:0]
//		dst[127:32] := a[127:32]
//
// Instruction: 'MULSS'. Intrinsic: '_mm_mul_ss'.
// Requires SSE.
func MulSs(a M128, b M128) M128 {
	return M128(mulSs([4]float32(a), [4]float32(b)))
}

func mulSs(a [4]float32, b [4]float32) [4]float32


// MaskMulhi16: Multiply the packed 16-bit integers in 'a' and 'b', producing
// intermediate 32-bit integers, and store the high 16 bits of the intermediate
// integers in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				tmp[31:0] := a[i+15:i] * b[i+15:i]
//				dst[i+15:i] := tmp[31:16]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMULHW'. Intrinsic: '_mm_mask_mulhi_epi16'.
// Requires AVX512BW.
func MaskMulhi16(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskMulhi16([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskMulhi16(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzMulhi16: Multiply the packed 16-bit integers in 'a' and 'b', producing
// intermediate 32-bit integers, and store the high 16 bits of the intermediate
// integers in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				tmp[31:0] := a[i+15:i] * b[i+15:i]
//				dst[i+15:i] := tmp[31:16]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMULHW'. Intrinsic: '_mm_maskz_mulhi_epi16'.
// Requires AVX512BW.
func MaskzMulhi16(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzMulhi16(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzMulhi16(k uint8, a [16]byte, b [16]byte) [16]byte


// Mulhi16: Multiply the packed 16-bit integers in 'a' and 'b', producing
// intermediate 32-bit integers, and store the high 16 bits of the intermediate
// integers in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			tmp[31:0] := a[i+15:i] * b[i+15:i]
//			dst[i+15:i] := tmp[31:16]
//		ENDFOR
//
// Instruction: 'PMULHW'. Intrinsic: '_mm_mulhi_epi16'.
// Requires SSE2.
func Mulhi16(a M128i, b M128i) M128i {
	return M128i(mulhi16([16]byte(a), [16]byte(b)))
}

func mulhi16(a [16]byte, b [16]byte) [16]byte


// MaskMulhiEpu16: Multiply the packed unsigned 16-bit integers in 'a' and 'b',
// producing intermediate 32-bit integers, and store the high 16 bits of the
// intermediate integers in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				tmp[31:0] := a[i+15:i] * b[i+15:i]
//				dst[i+15:i] := tmp[31:16]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMULHUW'. Intrinsic: '_mm_mask_mulhi_epu16'.
// Requires AVX512BW.
func MaskMulhiEpu16(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskMulhiEpu16([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskMulhiEpu16(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzMulhiEpu16: Multiply the packed unsigned 16-bit integers in 'a' and
// 'b', producing intermediate 32-bit integers, and store the high 16 bits of
// the intermediate integers in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				tmp[31:0] := a[i+15:i] * b[i+15:i]
//				dst[i+15:i] := tmp[31:16]
//			ELSE
//				dst[i+15:i] := o
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMULHUW'. Intrinsic: '_mm_maskz_mulhi_epu16'.
// Requires AVX512BW.
func MaskzMulhiEpu16(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzMulhiEpu16(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzMulhiEpu16(k uint8, a [16]byte, b [16]byte) [16]byte


// MulhiEpu16: Multiply the packed unsigned 16-bit integers in 'a' and 'b',
// producing intermediate 32-bit integers, and store the high 16 bits of the
// intermediate integers in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			tmp[31:0] := a[i+15:i] * b[i+15:i]
//			dst[i+15:i] := tmp[31:16]
//		ENDFOR
//
// Instruction: 'PMULHUW'. Intrinsic: '_mm_mulhi_epu16'.
// Requires SSE2.
func MulhiEpu16(a M128i, b M128i) M128i {
	return M128i(mulhiEpu16([16]byte(a), [16]byte(b)))
}

func mulhiEpu16(a [16]byte, b [16]byte) [16]byte


// MaskMulhrs16: Multiply packed 16-bit integers in 'a' and 'b', producing
// intermediate signed 32-bit integers. Truncate each intermediate integer to
// the 18 most significant bits, round by adding 1, and store bits [16:1] to
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				tmp[31:0] := ((a[i+15:i] * b[i+15:i]) >> 14) + 1
//				dst[i+15:i] := tmp[16:1]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMULHRSW'. Intrinsic: '_mm_mask_mulhrs_epi16'.
// Requires AVX512BW.
func MaskMulhrs16(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskMulhrs16([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskMulhrs16(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzMulhrs16: Multiply packed 16-bit integers in 'a' and 'b', producing
// intermediate signed 32-bit integers. Truncate each intermediate integer to
// the 18 most significant bits, round by adding 1, and store bits [16:1] to
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				tmp[31:0] := ((a[i+15:i] * b[i+15:i]) >> 14) + 1
//				dst[i+15:i] := tmp[16:1]
//			ELSE
//				dst[i+15:i] := 9
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMULHRSW'. Intrinsic: '_mm_maskz_mulhrs_epi16'.
// Requires AVX512BW.
func MaskzMulhrs16(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzMulhrs16(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzMulhrs16(k uint8, a [16]byte, b [16]byte) [16]byte


// Mulhrs16: Multiply packed 16-bit integers in 'a' and 'b', producing
// intermediate signed 32-bit integers. Truncate each intermediate integer to
// the 18 most significant bits, round by adding 1, and store bits [16:1] to
// 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			tmp[31:0] := ((a[i+15:i] * b[i+15:i]) >> 14) + 1
//			dst[i+15:i] := tmp[16:1]
//		ENDFOR
//
// Instruction: 'PMULHRSW'. Intrinsic: '_mm_mulhrs_epi16'.
// Requires SSSE3.
func Mulhrs16(a M128i, b M128i) M128i {
	return M128i(mulhrs16([16]byte(a), [16]byte(b)))
}

func mulhrs16(a [16]byte, b [16]byte) [16]byte


// MaskMullo16: Multiply the packed 16-bit integers in 'a' and 'b', producing
// intermediate 32-bit integers, and store the low 16 bits of the intermediate
// integers in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				tmp[31:0] := a[i+15:i] * b[i+15:i]
//				dst[i+15:i] := tmp[15:0]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMULLW'. Intrinsic: '_mm_mask_mullo_epi16'.
// Requires AVX512BW.
func MaskMullo16(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskMullo16([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskMullo16(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzMullo16: Multiply the packed 16-bit integers in 'a' and 'b', producing
// intermediate 32-bit integers, and store the low 16 bits of the intermediate
// integers in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				tmp[31:0] := a[i+15:i] * b[i+15:i]
//				dst[i+15:i] := tmp[15:0]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMULLW'. Intrinsic: '_mm_maskz_mullo_epi16'.
// Requires AVX512BW.
func MaskzMullo16(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzMullo16(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzMullo16(k uint8, a [16]byte, b [16]byte) [16]byte


// Mullo16: Multiply the packed 16-bit integers in 'a' and 'b', producing
// intermediate 32-bit integers, and store the low 16 bits of the intermediate
// integers in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			tmp[31:0] := a[i+15:i] * b[i+15:i]
//			dst[i+15:i] := tmp[15:0]
//		ENDFOR
//
// Instruction: 'PMULLW'. Intrinsic: '_mm_mullo_epi16'.
// Requires SSE2.
func Mullo16(a M128i, b M128i) M128i {
	return M128i(mullo16([16]byte(a), [16]byte(b)))
}

func mullo16(a [16]byte, b [16]byte) [16]byte


// MaskMullo32: Multiply the packed 32-bit integers in 'a' and 'b', producing
// intermediate 64-bit integers, and store the low 32 bits of the intermediate
// integers in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				tmp[63:0] := a[i+31:i] * b[i+31:i]
//				dst[i+31:i] := tmp[31:0]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMULLD'. Intrinsic: '_mm_mask_mullo_epi32'.
// Requires AVX512F.
func MaskMullo32(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskMullo32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskMullo32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzMullo32: Multiply the packed 32-bit integers in 'a' and 'b', producing
// intermediate 64-bit integers, and store the low 32 bits of the intermediate
// integers in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				tmp[63:0] := a[i+31:i] * b[i+31:i]
//				dst[i+31:i] := tmp[31:0]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMULLD'. Intrinsic: '_mm_maskz_mullo_epi32'.
// Requires AVX512F.
func MaskzMullo32(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzMullo32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzMullo32(k uint8, a [16]byte, b [16]byte) [16]byte


// Mullo32: Multiply the packed 32-bit integers in 'a' and 'b', producing
// intermediate 64-bit integers, and store the low 32 bits of the intermediate
// integers in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			tmp[63:0] := a[i+31:i] * b[i+31:i]
//			dst[i+31:i] := tmp[31:0]
//		ENDFOR
//
// Instruction: 'PMULLD'. Intrinsic: '_mm_mullo_epi32'.
// Requires SSE4.1.
func Mullo32(a M128i, b M128i) M128i {
	return M128i(mullo32([16]byte(a), [16]byte(b)))
}

func mullo32(a [16]byte, b [16]byte) [16]byte


// MaskMullo64: Multiply the packed 64-bit integers in 'a' and 'b', producing
// intermediate 128-bit integers, and store the low 64 bits of the intermediate
// integers in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				tmp[127:0] := a[i+63:i] * b[i+63:i]
//				dst[i+63:i] := tmp[63:0]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMULLQ'. Intrinsic: '_mm_mask_mullo_epi64'.
// Requires AVX512DQ.
func MaskMullo64(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskMullo64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskMullo64(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzMullo64: Multiply the packed 64-bit integers in 'a' and 'b', producing
// intermediate 128-bit integers, and store the low 64 bits of the intermediate
// integers in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				tmp[127:0] := a[i+63:i] * b[i+63:i]
//				dst[i+63:i] := tmp[63:0]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMULLQ'. Intrinsic: '_mm_maskz_mullo_epi64'.
// Requires AVX512DQ.
func MaskzMullo64(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzMullo64(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzMullo64(k uint8, a [16]byte, b [16]byte) [16]byte


// Mullo64: Multiply the packed 64-bit integers in 'a' and 'b', producing
// intermediate 128-bit integers, and store the low 64 bits of the intermediate
// integers in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			tmp[127:0] := a[i+63:i] * b[i+63:i]
//			dst[i+63:i] := tmp[63:0]
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMULLQ'. Intrinsic: '_mm_mullo_epi64'.
// Requires AVX512DQ.
func Mullo64(a M128i, b M128i) M128i {
	return M128i(mullo64([16]byte(a), [16]byte(b)))
}

func mullo64(a [16]byte, b [16]byte) [16]byte


// MaskMultishift64Epi8: For each 64-bit element in 'b', select 8 unaligned
// bytes using a byte-granular shift control within the corresponding 64-bit
// element of 'a', and store the 8 assembled bytes to the corresponding 64-bit
// element of 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR i := 0 to 1
//			q := i * 64
//			FOR j := 0 to 7
//				tmp8 := 0
//				ctrl := a[q+j*8+7:q+j*8] & 63
//				FOR l := 0 to 7
//					tmp8[k] := b[q+((ctrl+k) & 63)]
//				ENDFOR
//				IF k[i*8+j]
//					dst[q+j*8+7:q+j*8] := tmp8[7:0]
//				ELSE
//					dst[q+j*8+7:q+j*8] := src[q+j*8+7:q+j*8]
//				FI
//			ENDFOR
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMULTISHIFTQB'. Intrinsic: '_mm_mask_multishift_epi64_epi8'.
// Requires AVX512VL.
func MaskMultishift64Epi8(src M128i, k Mmask16, a M128i, b M128i) M128i {
	return M128i(maskMultishift64Epi8([16]byte(src), uint16(k), [16]byte(a), [16]byte(b)))
}

func maskMultishift64Epi8(src [16]byte, k uint16, a [16]byte, b [16]byte) [16]byte


// MaskzMultishift64Epi8: For each 64-bit element in 'b', select 8 unaligned
// bytes using a byte-granular shift control within the corresponding 64-bit
// element of 'a', and store the 8 assembled bytes to the corresponding 64-bit
// element of 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR i := 0 to 1
//			q := i * 64
//			FOR j := 0 to 7
//				tmp8 := 0
//				ctrl := a[q+j*8+7:q+j*8] & 63
//				FOR l := 0 to 7
//					tmp8[k] := b[q+((ctrl+k) & 63)]
//				ENDFOR
//				IF k[i*8+j]
//					dst[q+j*8+7:q+j*8] := tmp8[7:0]
//				ELSE
//					dst[q+j*8+7:q+j*8] := 0
//				FI
//			ENDFOR
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMULTISHIFTQB'. Intrinsic: '_mm_maskz_multishift_epi64_epi8'.
// Requires AVX512VL.
func MaskzMultishift64Epi8(k Mmask16, a M128i, b M128i) M128i {
	return M128i(maskzMultishift64Epi8(uint16(k), [16]byte(a), [16]byte(b)))
}

func maskzMultishift64Epi8(k uint16, a [16]byte, b [16]byte) [16]byte


// Multishift64Epi8: For each 64-bit element in 'b', select 8 unaligned bytes
// using a byte-granular shift control within the corresponding 64-bit element
// of 'a', and store the 8 assembled bytes to the corresponding 64-bit element
// of 'dst'. 
//
//		FOR i := 0 to 1
//			q := i * 64
//			FOR j := 0 to 7
//				tmp8 := 0
//				ctrl := a[q+j*8+7:q+j*8] & 63
//				FOR l := 0 to 7
//					tmp8[k] := b[q+((ctrl+k) & 63)]
//				ENDFOR
//				dst[q+j*8+7:q+j*8] := tmp8[7:0]
//			ENDFOR
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMULTISHIFTQB'. Intrinsic: '_mm_multishift_epi64_epi8'.
// Requires AVX512VL.
func Multishift64Epi8(a M128i, b M128i) M128i {
	return M128i(multishift64Epi8([16]byte(a), [16]byte(b)))
}

func multishift64Epi8(a [16]byte, b [16]byte) [16]byte


// MaskOr32: Compute the bitwise OR of packed 32-bit integers in 'a' and 'b',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] OR b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPORD'. Intrinsic: '_mm_mask_or_epi32'.
// Requires AVX512F.
func MaskOr32(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskOr32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskOr32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzOr32: Compute the bitwise OR of packed 32-bit integers in 'a' and 'b',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] OR b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPORD'. Intrinsic: '_mm_maskz_or_epi32'.
// Requires AVX512F.
func MaskzOr32(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzOr32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzOr32(k uint8, a [16]byte, b [16]byte) [16]byte


// MaskOr64: Compute the bitwise OR of packed 64-bit integers in 'a' and 'b',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] OR b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPORQ'. Intrinsic: '_mm_mask_or_epi64'.
// Requires AVX512F.
func MaskOr64(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskOr64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskOr64(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzOr64: Compute the bitwise OR of packed 64-bit integers in 'a' and 'b',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] OR b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPORQ'. Intrinsic: '_mm_maskz_or_epi64'.
// Requires AVX512F.
func MaskzOr64(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzOr64(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzOr64(k uint8, a [16]byte, b [16]byte) [16]byte


// MaskOrPd: Compute the bitwise OR of packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] BITWISE OR b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VORPD'. Intrinsic: '_mm_mask_or_pd'.
// Requires AVX512DQ.
func MaskOrPd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskOrPd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskOrPd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzOrPd: Compute the bitwise OR of packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] BITWISE OR b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VORPD'. Intrinsic: '_mm_maskz_or_pd'.
// Requires AVX512DQ.
func MaskzOrPd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzOrPd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzOrPd(k uint8, a [2]float64, b [2]float64) [2]float64


// OrPd: Compute the bitwise OR of packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := a[i+63:i] BITWISE OR b[i+63:i]
//		ENDFOR
//
// Instruction: 'ORPD'. Intrinsic: '_mm_or_pd'.
// Requires SSE2.
func OrPd(a M128d, b M128d) M128d {
	return M128d(orPd([2]float64(a), [2]float64(b)))
}

func orPd(a [2]float64, b [2]float64) [2]float64


// MaskOrPs: Compute the bitwise OR of packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] BITWISE OR b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VORPS'. Intrinsic: '_mm_mask_or_ps'.
// Requires AVX512DQ.
func MaskOrPs(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskOrPs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskOrPs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzOrPs: Compute the bitwise OR of packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] BITWISE OR b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VORPS'. Intrinsic: '_mm_maskz_or_ps'.
// Requires AVX512DQ.
func MaskzOrPs(k Mmask8, a M128, b M128) M128 {
	return M128(maskzOrPs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzOrPs(k uint8, a [4]float32, b [4]float32) [4]float32


// OrPs: Compute the bitwise OR of packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := a[i+31:i] BITWISE OR b[i+31:i]
//		ENDFOR
//
// Instruction: 'ORPS'. Intrinsic: '_mm_or_ps'.
// Requires SSE.
func OrPs(a M128, b M128) M128 {
	return M128(orPs([4]float32(a), [4]float32(b)))
}

func orPs(a [4]float32, b [4]float32) [4]float32


// OrSi128: Compute the bitwise OR of 128 bits (representing integer data) in
// 'a' and 'b', and store the result in 'dst'. 
//
//		dst[127:0] := (a[127:0] OR b[127:0])
//
// Instruction: 'POR'. Intrinsic: '_mm_or_si128'.
// Requires SSE2.
func OrSi128(a M128i, b M128i) M128i {
	return M128i(orSi128([16]byte(a), [16]byte(b)))
}

func orSi128(a [16]byte, b [16]byte) [16]byte


// MaskPacks16: Convert packed 16-bit integers from 'a' and 'b' to packed 8-bit
// integers using signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp_dst[7:0] := Saturate_Int16_To_Int8 (a[15:0])
//		tmp_dst[15:8] := Saturate_Int16_To_Int8 (a[31:16])
//		tmp_dst[23:16] := Saturate_Int16_To_Int8 (a[47:32])
//		tmp_dst[31:24] := Saturate_Int16_To_Int8 (a[63:48])
//		tmp_dst[39:32] := Saturate_Int16_To_Int8 (a[79:64])
//		tmp_dst[47:40] := Saturate_Int16_To_Int8 (a[95:80])
//		tmp_dst[55:48] := Saturate_Int16_To_Int8 (a[111:96])
//		tmp_dst[63:56] := Saturate_Int16_To_Int8 (a[127:112])
//		tmp_dst[71:64] := Saturate_Int16_To_Int8 (b[15:0])
//		tmp_dst[79:72] := Saturate_Int16_To_Int8 (b[31:16])
//		tmp_dst[87:80] := Saturate_Int16_To_Int8 (b[47:32])
//		tmp_dst[95:88] := Saturate_Int16_To_Int8 (b[63:48])
//		tmp_dst[103:96] := Saturate_Int16_To_Int8 (b[79:64])
//		tmp_dst[111:104] := Saturate_Int16_To_Int8 (b[95:80])
//		tmp_dst[119:112] := Saturate_Int16_To_Int8 (b[111:96])
//		tmp_dst[127:120] := Saturate_Int16_To_Int8 (b[127:112])
//		
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := tmp_dst[i+7:i]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPACKSSWB'. Intrinsic: '_mm_mask_packs_epi16'.
// Requires AVX512BW.
func MaskPacks16(src M128i, k Mmask16, a M128i, b M128i) M128i {
	return M128i(maskPacks16([16]byte(src), uint16(k), [16]byte(a), [16]byte(b)))
}

func maskPacks16(src [16]byte, k uint16, a [16]byte, b [16]byte) [16]byte


// MaskzPacks16: Convert packed 16-bit integers from 'a' and 'b' to packed
// 8-bit integers using signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp_dst[7:0] := Saturate_Int16_To_Int8 (a[15:0])
//		tmp_dst[15:8] := Saturate_Int16_To_Int8 (a[31:16])
//		tmp_dst[23:16] := Saturate_Int16_To_Int8 (a[47:32])
//		tmp_dst[31:24] := Saturate_Int16_To_Int8 (a[63:48])
//		tmp_dst[39:32] := Saturate_Int16_To_Int8 (a[79:64])
//		tmp_dst[47:40] := Saturate_Int16_To_Int8 (a[95:80])
//		tmp_dst[55:48] := Saturate_Int16_To_Int8 (a[111:96])
//		tmp_dst[63:56] := Saturate_Int16_To_Int8 (a[127:112])
//		tmp_dst[71:64] := Saturate_Int16_To_Int8 (b[15:0])
//		tmp_dst[79:72] := Saturate_Int16_To_Int8 (b[31:16])
//		tmp_dst[87:80] := Saturate_Int16_To_Int8 (b[47:32])
//		tmp_dst[95:88] := Saturate_Int16_To_Int8 (b[63:48])
//		tmp_dst[103:96] := Saturate_Int16_To_Int8 (b[79:64])
//		tmp_dst[111:104] := Saturate_Int16_To_Int8 (b[95:80])
//		tmp_dst[119:112] := Saturate_Int16_To_Int8 (b[111:96])
//		tmp_dst[127:120] := Saturate_Int16_To_Int8 (b[127:112])
//		
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := tmp_dst[i+7:i]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPACKSSWB'. Intrinsic: '_mm_maskz_packs_epi16'.
// Requires AVX512BW.
func MaskzPacks16(k Mmask16, a M128i, b M128i) M128i {
	return M128i(maskzPacks16(uint16(k), [16]byte(a), [16]byte(b)))
}

func maskzPacks16(k uint16, a [16]byte, b [16]byte) [16]byte


// Packs16: Convert packed 16-bit integers from 'a' and 'b' to packed 8-bit
// integers using signed saturation, and store the results in 'dst'. 
//
//		dst[7:0] := Saturate_Int16_To_Int8 (a[15:0])
//		dst[15:8] := Saturate_Int16_To_Int8 (a[31:16])
//		dst[23:16] := Saturate_Int16_To_Int8 (a[47:32])
//		dst[31:24] := Saturate_Int16_To_Int8 (a[63:48])
//		dst[39:32] := Saturate_Int16_To_Int8 (a[79:64])
//		dst[47:40] := Saturate_Int16_To_Int8 (a[95:80])
//		dst[55:48] := Saturate_Int16_To_Int8 (a[111:96])
//		dst[63:56] := Saturate_Int16_To_Int8 (a[127:112])
//		dst[71:64] := Saturate_Int16_To_Int8 (b[15:0])
//		dst[79:72] := Saturate_Int16_To_Int8 (b[31:16])
//		dst[87:80] := Saturate_Int16_To_Int8 (b[47:32])
//		dst[95:88] := Saturate_Int16_To_Int8 (b[63:48])
//		dst[103:96] := Saturate_Int16_To_Int8 (b[79:64])
//		dst[111:104] := Saturate_Int16_To_Int8 (b[95:80])
//		dst[119:112] := Saturate_Int16_To_Int8 (b[111:96])
//		dst[127:120] := Saturate_Int16_To_Int8 (b[127:112])
//
// Instruction: 'PACKSSWB'. Intrinsic: '_mm_packs_epi16'.
// Requires SSE2.
func Packs16(a M128i, b M128i) M128i {
	return M128i(packs16([16]byte(a), [16]byte(b)))
}

func packs16(a [16]byte, b [16]byte) [16]byte


// MaskPacks32: Convert packed 32-bit integers from 'a' and 'b' to packed
// 16-bit integers using signed saturation, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		tmp_dst[15:0] := Saturate_Int32_To_Int16 (a[31:0])
//		tmp_dst[31:16] := Saturate_Int32_To_Int16 (a[63:32])
//		tmp_dst[47:32] := Saturate_Int32_To_Int16 (a[95:64])
//		tmp_dst[63:48] := Saturate_Int32_To_Int16 (a[127:96])
//		tmp_dst[79:64] := Saturate_Int32_To_Int16 (b[31:0])
//		tmp_dst[95:80] := Saturate_Int32_To_Int16 (b[63:32])
//		tmp_dst[111:96] := Saturate_Int32_To_Int16 (b[95:64])
//		tmp_dst[127:112] := Saturate_Int32_To_Int16 (b[127:96])
//		
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPACKSSDW'. Intrinsic: '_mm_mask_packs_epi32'.
// Requires AVX512BW.
func MaskPacks32(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskPacks32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskPacks32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzPacks32: Convert packed 32-bit integers from 'a' and 'b' to packed
// 16-bit integers using signed saturation, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		tmp_dst[15:0] := Saturate_Int32_To_Int16 (a[31:0])
//		tmp_dst[31:16] := Saturate_Int32_To_Int16 (a[63:32])
//		tmp_dst[47:32] := Saturate_Int32_To_Int16 (a[95:64])
//		tmp_dst[63:48] := Saturate_Int32_To_Int16 (a[127:96])
//		tmp_dst[79:64] := Saturate_Int32_To_Int16 (b[31:0])
//		tmp_dst[95:80] := Saturate_Int32_To_Int16 (b[63:32])
//		tmp_dst[111:96] := Saturate_Int32_To_Int16 (b[95:64])
//		tmp_dst[127:112] := Saturate_Int32_To_Int16 (b[127:96])
//		
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPACKSSDW'. Intrinsic: '_mm_maskz_packs_epi32'.
// Requires AVX512BW.
func MaskzPacks32(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzPacks32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzPacks32(k uint8, a [16]byte, b [16]byte) [16]byte


// Packs32: Convert packed 32-bit integers from 'a' and 'b' to packed 16-bit
// integers using signed saturation, and store the results in 'dst'. 
//
//		dst[15:0] := Saturate_Int32_To_Int16 (a[31:0])
//		dst[31:16] := Saturate_Int32_To_Int16 (a[63:32])
//		dst[47:32] := Saturate_Int32_To_Int16 (a[95:64])
//		dst[63:48] := Saturate_Int32_To_Int16 (a[127:96])
//		dst[79:64] := Saturate_Int32_To_Int16 (b[31:0])
//		dst[95:80] := Saturate_Int32_To_Int16 (b[63:32])
//		dst[111:96] := Saturate_Int32_To_Int16 (b[95:64])
//		dst[127:112] := Saturate_Int32_To_Int16 (b[127:96])
//
// Instruction: 'PACKSSDW'. Intrinsic: '_mm_packs_epi32'.
// Requires SSE2.
func Packs32(a M128i, b M128i) M128i {
	return M128i(packs32([16]byte(a), [16]byte(b)))
}

func packs32(a [16]byte, b [16]byte) [16]byte


// MaskPackus16: Convert packed 16-bit integers from 'a' and 'b' to packed
// 8-bit integers using unsigned saturation, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		tmp_dst[7:0] := Saturate_Int16_To_UnsignedInt8 (a[15:0])
//		tmp_dst[15:8] := Saturate_Int16_To_UnsignedInt8 (a[31:16])
//		tmp_dst[23:16] := Saturate_Int16_To_UnsignedInt8 (a[47:32])
//		tmp_dst[31:24] := Saturate_Int16_To_UnsignedInt8 (a[63:48])
//		tmp_dst[39:32] := Saturate_Int16_To_UnsignedInt8 (a[79:64])
//		tmp_dst[47:40] := Saturate_Int16_To_UnsignedInt8 (a[95:80])
//		tmp_dst[55:48] := Saturate_Int16_To_UnsignedInt8 (a[111:96])
//		tmp_dst[63:56] := Saturate_Int16_To_UnsignedInt8 (a[127:112])
//		tmp_dst[71:64] := Saturate_Int16_To_UnsignedInt8 (b[15:0])
//		tmp_dst[79:72] := Saturate_Int16_To_UnsignedInt8 (b[31:16])
//		tmp_dst[87:80] := Saturate_Int16_To_UnsignedInt8 (b[47:32])
//		tmp_dst[95:88] := Saturate_Int16_To_UnsignedInt8 (b[63:48])
//		tmp_dst[103:96] := Saturate_Int16_To_UnsignedInt8 (b[79:64])
//		tmp_dst[111:104] := Saturate_Int16_To_UnsignedInt8 (b[95:80])
//		tmp_dst[119:112] := Saturate_Int16_To_UnsignedInt8 (b[111:96])
//		tmp_dst[127:120] := Saturate_Int16_To_UnsignedInt8 (b[127:112])
//		
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := tmp_dst[i+7:i]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPACKUSWB'. Intrinsic: '_mm_mask_packus_epi16'.
// Requires AVX512BW.
func MaskPackus16(src M128i, k Mmask16, a M128i, b M128i) M128i {
	return M128i(maskPackus16([16]byte(src), uint16(k), [16]byte(a), [16]byte(b)))
}

func maskPackus16(src [16]byte, k uint16, a [16]byte, b [16]byte) [16]byte


// MaskzPackus16: Convert packed 16-bit integers from 'a' and 'b' to packed
// 8-bit integers using unsigned saturation, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		tmp_dst[7:0] := Saturate_Int16_To_UnsignedInt8 (a[15:0])
//		tmp_dst[15:8] := Saturate_Int16_To_UnsignedInt8 (a[31:16])
//		tmp_dst[23:16] := Saturate_Int16_To_UnsignedInt8 (a[47:32])
//		tmp_dst[31:24] := Saturate_Int16_To_UnsignedInt8 (a[63:48])
//		tmp_dst[39:32] := Saturate_Int16_To_UnsignedInt8 (a[79:64])
//		tmp_dst[47:40] := Saturate_Int16_To_UnsignedInt8 (a[95:80])
//		tmp_dst[55:48] := Saturate_Int16_To_UnsignedInt8 (a[111:96])
//		tmp_dst[63:56] := Saturate_Int16_To_UnsignedInt8 (a[127:112])
//		tmp_dst[71:64] := Saturate_Int16_To_UnsignedInt8 (b[15:0])
//		tmp_dst[79:72] := Saturate_Int16_To_UnsignedInt8 (b[31:16])
//		tmp_dst[87:80] := Saturate_Int16_To_UnsignedInt8 (b[47:32])
//		tmp_dst[95:88] := Saturate_Int16_To_UnsignedInt8 (b[63:48])
//		tmp_dst[103:96] := Saturate_Int16_To_UnsignedInt8 (b[79:64])
//		tmp_dst[111:104] := Saturate_Int16_To_UnsignedInt8 (b[95:80])
//		tmp_dst[119:112] := Saturate_Int16_To_UnsignedInt8 (b[111:96])
//		tmp_dst[127:120] := Saturate_Int16_To_UnsignedInt8 (b[127:112])
//		
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := tmp_dst[i+7:i]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPACKUSWB'. Intrinsic: '_mm_maskz_packus_epi16'.
// Requires AVX512BW.
func MaskzPackus16(k Mmask16, a M128i, b M128i) M128i {
	return M128i(maskzPackus16(uint16(k), [16]byte(a), [16]byte(b)))
}

func maskzPackus16(k uint16, a [16]byte, b [16]byte) [16]byte


// Packus16: Convert packed 16-bit integers from 'a' and 'b' to packed 8-bit
// integers using unsigned saturation, and store the results in 'dst'. 
//
//		dst[7:0] := Saturate_Int16_To_UnsignedInt8 (a[15:0])
//		dst[15:8] := Saturate_Int16_To_UnsignedInt8 (a[31:16])
//		dst[23:16] := Saturate_Int16_To_UnsignedInt8 (a[47:32])
//		dst[31:24] := Saturate_Int16_To_UnsignedInt8 (a[63:48])
//		dst[39:32] := Saturate_Int16_To_UnsignedInt8 (a[79:64])
//		dst[47:40] := Saturate_Int16_To_UnsignedInt8 (a[95:80])
//		dst[55:48] := Saturate_Int16_To_UnsignedInt8 (a[111:96])
//		dst[63:56] := Saturate_Int16_To_UnsignedInt8 (a[127:112])
//		dst[71:64] := Saturate_Int16_To_UnsignedInt8 (b[15:0])
//		dst[79:72] := Saturate_Int16_To_UnsignedInt8 (b[31:16])
//		dst[87:80] := Saturate_Int16_To_UnsignedInt8 (b[47:32])
//		dst[95:88] := Saturate_Int16_To_UnsignedInt8 (b[63:48])
//		dst[103:96] := Saturate_Int16_To_UnsignedInt8 (b[79:64])
//		dst[111:104] := Saturate_Int16_To_UnsignedInt8 (b[95:80])
//		dst[119:112] := Saturate_Int16_To_UnsignedInt8 (b[111:96])
//		dst[127:120] := Saturate_Int16_To_UnsignedInt8 (b[127:112])
//
// Instruction: 'PACKUSWB'. Intrinsic: '_mm_packus_epi16'.
// Requires SSE2.
func Packus16(a M128i, b M128i) M128i {
	return M128i(packus16([16]byte(a), [16]byte(b)))
}

func packus16(a [16]byte, b [16]byte) [16]byte


// MaskPackus32: Convert packed 32-bit integers from 'a' and 'b' to packed
// 16-bit integers using unsigned saturation, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		tmp_dst[15:0] := Saturate_Int32_To_UnsignedInt16 (a[31:0])
//		tmp_dst[31:16] := Saturate_Int32_To_UnsignedInt16 (a[63:32])
//		tmp_dst[47:32] := Saturate_Int32_To_UnsignedInt16 (a[95:64])
//		tmp_dst[63:48] := Saturate_Int32_To_UnsignedInt16 (a[127:96])
//		tmp_dst[79:64] := Saturate_Int32_To_UnsignedInt16 (b[31:0])
//		tmp_dst[95:80] := Saturate_Int32_To_UnsignedInt16 (b[63:32])
//		tmp_dst[111:96] := Saturate_Int32_To_UnsignedInt16 (b[95:64])
//		tmp_dst[127:112] := Saturate_Int32_To_UnsignedInt16 (b[127:96])
//		
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPACKUSDW'. Intrinsic: '_mm_mask_packus_epi32'.
// Requires AVX512BW.
func MaskPackus32(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskPackus32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskPackus32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzPackus32: Convert packed 32-bit integers from 'a' and 'b' to packed
// 16-bit integers using unsigned saturation, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		tmp_dst[15:0] := Saturate_Int32_To_UnsignedInt16 (a[31:0])
//		tmp_dst[31:16] := Saturate_Int32_To_UnsignedInt16 (a[63:32])
//		tmp_dst[47:32] := Saturate_Int32_To_UnsignedInt16 (a[95:64])
//		tmp_dst[63:48] := Saturate_Int32_To_UnsignedInt16 (a[127:96])
//		tmp_dst[79:64] := Saturate_Int32_To_UnsignedInt16 (b[31:0])
//		tmp_dst[95:80] := Saturate_Int32_To_UnsignedInt16 (b[63:32])
//		tmp_dst[111:96] := Saturate_Int32_To_UnsignedInt16 (b[95:64])
//		tmp_dst[127:112] := Saturate_Int32_To_UnsignedInt16 (b[127:96])
//		
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPACKUSDW'. Intrinsic: '_mm_maskz_packus_epi32'.
// Requires AVX512BW.
func MaskzPackus32(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzPackus32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzPackus32(k uint8, a [16]byte, b [16]byte) [16]byte


// Packus32: Convert packed 32-bit integers from 'a' and 'b' to packed 16-bit
// integers using unsigned saturation, and store the results in 'dst'. 
//
//		dst[15:0] := Saturate_Int32_To_UnsignedInt16 (a[31:0])
//		dst[31:16] := Saturate_Int32_To_UnsignedInt16 (a[63:32])
//		dst[47:32] := Saturate_Int32_To_UnsignedInt16 (a[95:64])
//		dst[63:48] := Saturate_Int32_To_UnsignedInt16 (a[127:96])
//		dst[79:64] := Saturate_Int32_To_UnsignedInt16 (b[31:0])
//		dst[95:80] := Saturate_Int32_To_UnsignedInt16 (b[63:32])
//		dst[111:96] := Saturate_Int32_To_UnsignedInt16 (b[95:64])
//		dst[127:112] := Saturate_Int32_To_UnsignedInt16 (b[127:96])
//
// Instruction: 'PACKUSDW'. Intrinsic: '_mm_packus_epi32'.
// Requires SSE4.1.
func Packus32(a M128i, b M128i) M128i {
	return M128i(packus32([16]byte(a), [16]byte(b)))
}

func packus32(a [16]byte, b [16]byte) [16]byte


// PdepU32: Deposit contiguous low bits from unsigned 32-bit integer 'a' to
// 'dst' at the corresponding bit locations specified by 'mask'; all other bits
// in 'dst' are set to zero. 
//
//		tmp := a
//		dst := 0
//		m := 0
//		k := 0
//		DO WHILE m < 32
//			IF mask[m] = 1
//				dst[m] := tmp[k]
//				k := k + 1
//			FI
//			m := m + 1
//		OD
//
// Instruction: 'PDEP'. Intrinsic: '_pdep_u32'.
// Requires BMI2.
func PdepU32(a uint32, mask uint32) uint32 {
	return uint32(pdepU32(a, mask))
}

func pdepU32(a uint32, mask uint32) uint32


// PdepU64: Deposit contiguous low bits from unsigned 64-bit integer 'a' to
// 'dst' at the corresponding bit locations specified by 'mask'; all other bits
// in 'dst' are set to zero. 
//
//		tmp := a
//		dst := 0
//		m := 0
//		k := 0
//		DO WHILE m < 64
//			IF mask[m] = 1
//				dst[m] := tmp[k]
//				k := k + 1
//			FI
//			m := m + 1
//		OD
//
// Instruction: 'PDEP'. Intrinsic: '_pdep_u64'.
// Requires BMI2.
func PdepU64(a uint64, mask uint64) uint64 {
	return uint64(pdepU64(a, mask))
}

func pdepU64(a uint64, mask uint64) uint64


// MaskPermutePd: Shuffle double-precision (64-bit) floating-point elements in
// 'a' using the control in 'imm8', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		IF (imm8[0] == 0) tmp_dst[63:0] := a[63:0]
//		IF (imm8[0] == 1) tmp_dst[63:0] := a[127:64]
//		IF (imm8[1] == 0) tmp_dst[127:64] := a[63:0]
//		IF (imm8[1] == 1) tmp_dst[127:64] := a[127:64]
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm_mask_permute_pd'.
// Requires AVX512F.
func MaskPermutePd(src M128d, k Mmask8, a M128d, imm8 int) M128d {
	return M128d(maskPermutePd([2]float64(src), uint8(k), [2]float64(a), imm8))
}

func maskPermutePd(src [2]float64, k uint8, a [2]float64, imm8 int) [2]float64


// MaskzPermutePd: Shuffle double-precision (64-bit) floating-point elements in
// 'a' using the control in 'imm8', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		IF (imm8[0] == 0) tmp_dst[63:0] := a[63:0]
//		IF (imm8[0] == 1) tmp_dst[63:0] := a[127:64]
//		IF (imm8[1] == 0) tmp_dst[127:64] := a[63:0]
//		IF (imm8[1] == 1) tmp_dst[127:64] := a[127:64]
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm_maskz_permute_pd'.
// Requires AVX512F.
func MaskzPermutePd(k Mmask8, a M128d, imm8 int) M128d {
	return M128d(maskzPermutePd(uint8(k), [2]float64(a), imm8))
}

func maskzPermutePd(k uint8, a [2]float64, imm8 int) [2]float64


// PermutePd: Shuffle double-precision (64-bit) floating-point elements in 'a'
// using the control in 'imm8', and store the results in 'dst'. 
//
//		IF (imm8[0] == 0) dst[63:0] := a[63:0]
//		IF (imm8[0] == 1) dst[63:0] := a[127:64]
//		IF (imm8[1] == 0) dst[127:64] := a[63:0]
//		IF (imm8[1] == 1) dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm_permute_pd'.
// Requires AVX.
func PermutePd(a M128d, imm8 int) M128d {
	return M128d(permutePd([2]float64(a), imm8))
}

func permutePd(a [2]float64, imm8 int) [2]float64


// MaskPermutePs: Shuffle single-precision (32-bit) floating-point elements in
// 'a' using the control in 'imm8', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm_mask_permute_ps'.
// Requires AVX512F.
func MaskPermutePs(src M128, k Mmask8, a M128, imm8 int) M128 {
	return M128(maskPermutePs([4]float32(src), uint8(k), [4]float32(a), imm8))
}

func maskPermutePs(src [4]float32, k uint8, a [4]float32, imm8 int) [4]float32


// MaskzPermutePs: Shuffle single-precision (32-bit) floating-point elements in
// 'a' using the control in 'imm8', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm_maskz_permute_ps'.
// Requires AVX512F.
func MaskzPermutePs(k Mmask8, a M128, imm8 int) M128 {
	return M128(maskzPermutePs(uint8(k), [4]float32(a), imm8))
}

func maskzPermutePs(k uint8, a [4]float32, imm8 int) [4]float32


// PermutePs: Shuffle single-precision (32-bit) floating-point elements in 'a'
// using the control in 'imm8', and store the results in 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm_permute_ps'.
// Requires AVX.
func PermutePs(a M128, imm8 int) M128 {
	return M128(permutePs([4]float32(a), imm8))
}

func permutePs(a [4]float32, imm8 int) [4]float32


// MaskPermutevarPd: Shuffle double-precision (64-bit) floating-point elements
// in 'a' using the control in 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		IF (b[1] == 0) tmp_dst[63:0] := a[63:0]
//		IF (b[1] == 1) tmp_dst[63:0] := a[127:64]
//		IF (b[65] == 0) tmp_dst[127:64] := a[63:0]
//		IF (b[65] == 1) tmp_dst[127:64] := a[127:64]
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm_mask_permutevar_pd'.
// Requires AVX512F.
func MaskPermutevarPd(src M128d, k Mmask8, a M128d, b M128i) M128d {
	return M128d(maskPermutevarPd([2]float64(src), uint8(k), [2]float64(a), [16]byte(b)))
}

func maskPermutevarPd(src [2]float64, k uint8, a [2]float64, b [16]byte) [2]float64


// MaskzPermutevarPd: Shuffle double-precision (64-bit) floating-point elements
// in 'a' using the control in 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		IF (b[1] == 0) tmp_dst[63:0] := a[63:0]
//		IF (b[1] == 1) tmp_dst[63:0] := a[127:64]
//		IF (b[65] == 0) tmp_dst[127:64] := a[63:0]
//		IF (b[65] == 1) tmp_dst[127:64] := a[127:64]
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm_maskz_permutevar_pd'.
// Requires AVX512F.
func MaskzPermutevarPd(k Mmask8, a M128d, b M128i) M128d {
	return M128d(maskzPermutevarPd(uint8(k), [2]float64(a), [16]byte(b)))
}

func maskzPermutevarPd(k uint8, a [2]float64, b [16]byte) [2]float64


// PermutevarPd: Shuffle double-precision (64-bit) floating-point elements in
// 'a' using the control in 'b', and store the results in 'dst'. 
//
//		IF (b[1] == 0) dst[63:0] := a[63:0]
//		IF (b[1] == 1) dst[63:0] := a[127:64]
//		IF (b[65] == 0) dst[127:64] := a[63:0]
//		IF (b[65] == 1) dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm_permutevar_pd'.
// Requires AVX.
func PermutevarPd(a M128d, b M128i) M128d {
	return M128d(permutevarPd([2]float64(a), [16]byte(b)))
}

func permutevarPd(a [2]float64, b [16]byte) [2]float64


// MaskPermutevarPs: Shuffle single-precision (32-bit) floating-point elements
// in 'a' using the control in 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], b[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], b[33:32])
//		tmp_dst[95:64] := SELECT4(a[127:0], b[65:64])
//		tmp_dst[127:96] := SELECT4(a[127:0], b[97:96])
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm_mask_permutevar_ps'.
// Requires AVX512F.
func MaskPermutevarPs(src M128, k Mmask8, a M128, b M128i) M128 {
	return M128(maskPermutevarPs([4]float32(src), uint8(k), [4]float32(a), [16]byte(b)))
}

func maskPermutevarPs(src [4]float32, k uint8, a [4]float32, b [16]byte) [4]float32


// MaskzPermutevarPs: Shuffle single-precision (32-bit) floating-point elements
// in 'a' using the control in 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], b[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], b[33:32])
//		tmp_dst[95:64] := SELECT4(a[127:0], b[65:64])
//		tmp_dst[127:96] := SELECT4(a[127:0], b[97:96])
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm_maskz_permutevar_ps'.
// Requires AVX512F.
func MaskzPermutevarPs(k Mmask8, a M128, b M128i) M128 {
	return M128(maskzPermutevarPs(uint8(k), [4]float32(a), [16]byte(b)))
}

func maskzPermutevarPs(k uint8, a [4]float32, b [16]byte) [4]float32


// PermutevarPs: Shuffle single-precision (32-bit) floating-point elements in
// 'a' using the control in 'b', and store the results in 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		dst[31:0] := SELECT4(a[127:0], b[1:0])
//		dst[63:32] := SELECT4(a[127:0], b[33:32])
//		dst[95:64] := SELECT4(a[127:0], b[65:64])
//		dst[127:96] := SELECT4(a[127:0], b[97:96])
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm_permutevar_ps'.
// Requires AVX.
func PermutevarPs(a M128, b M128i) M128 {
	return M128(permutevarPs([4]float32(a), [16]byte(b)))
}

func permutevarPs(a [4]float32, b [16]byte) [4]float32


// MaskPermutex2var16: Shuffle 16-bit integers in 'a' and 'b' using the
// corresponding selector and index in 'idx', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'a' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				off := 16*idx[i+2:i]
//				dst[i+15:i] := idx[i+3] ? b[off+15:off] : a[off+15:off]
//			ELSE
//				dst[i+15:i] := a[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMT2W'. Intrinsic: '_mm_mask_permutex2var_epi16'.
// Requires AVX512BW.
func MaskPermutex2var16(a M128i, k Mmask8, idx M128i, b M128i) M128i {
	return M128i(maskPermutex2var16([16]byte(a), uint8(k), [16]byte(idx), [16]byte(b)))
}

func maskPermutex2var16(a [16]byte, k uint8, idx [16]byte, b [16]byte) [16]byte


// Mask2Permutex2var16: Shuffle 16-bit integers in 'a' and 'b' using the
// corresponding selector and index in 'idx', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'idx' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				off := 16*idx[i+2:i]
//				dst[i+15:i] := idx[i+3] ? b[off+15:off] : a[off+15:off]
//			ELSE
//				dst[i+15:i] := idx[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMI2W'. Intrinsic: '_mm_mask2_permutex2var_epi16'.
// Requires AVX512BW.
func Mask2Permutex2var16(a M128i, idx M128i, k Mmask8, b M128i) M128i {
	return M128i(mask2Permutex2var16([16]byte(a), [16]byte(idx), uint8(k), [16]byte(b)))
}

func mask2Permutex2var16(a [16]byte, idx [16]byte, k uint8, b [16]byte) [16]byte


// MaskzPermutex2var16: Shuffle 16-bit integers in 'a' and 'b' using the
// corresponding selector and index in 'idx', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				off := 16*idx[i+2:i]
//				dst[i+15:i] := idx[i+3] ? b[off+15:off] : a[off+15:off]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMI2W, VPERMT2W'. Intrinsic: '_mm_maskz_permutex2var_epi16'.
// Requires AVX512BW.
func MaskzPermutex2var16(k Mmask8, a M128i, idx M128i, b M128i) M128i {
	return M128i(maskzPermutex2var16(uint8(k), [16]byte(a), [16]byte(idx), [16]byte(b)))
}

func maskzPermutex2var16(k uint8, a [16]byte, idx [16]byte, b [16]byte) [16]byte


// Permutex2var16: Shuffle 16-bit integers in 'a' and 'b' using the
// corresponding selector and index in 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			off := 16*idx[i+2:i]
//			dst[i+15:i] := idx[i+4] ? b[off+15:off] : a[off+15:off]
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMI2W, VPERMT2W'. Intrinsic: '_mm_permutex2var_epi16'.
// Requires AVX512BW.
func Permutex2var16(a M128i, idx M128i, b M128i) M128i {
	return M128i(permutex2var16([16]byte(a), [16]byte(idx), [16]byte(b)))
}

func permutex2var16(a [16]byte, idx [16]byte, b [16]byte) [16]byte


// MaskPermutex2var32: Shuffle 32-bit integers in 'a' and 'b' using the
// corresponding selector and index in 'idx', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'a' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			off := idx[i+1:i]*32
//			IF k[j]
//				dst[i+31:i] := idx[i+2] ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMT2D'. Intrinsic: '_mm_mask_permutex2var_epi32'.
// Requires AVX512F.
func MaskPermutex2var32(a M128i, k Mmask8, idx M128i, b M128i) M128i {
	return M128i(maskPermutex2var32([16]byte(a), uint8(k), [16]byte(idx), [16]byte(b)))
}

func maskPermutex2var32(a [16]byte, k uint8, idx [16]byte, b [16]byte) [16]byte


// Mask2Permutex2var32: Shuffle 32-bit integers in 'a' and 'b' using the
// corresponding selector and index in 'idx', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'idx' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			off := idx[i+1:i]*32
//			IF k[j]
//				dst[i+31:i] := idx[i+2] ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := idx[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMI2D'. Intrinsic: '_mm_mask2_permutex2var_epi32'.
// Requires AVX512F.
func Mask2Permutex2var32(a M128i, idx M128i, k Mmask8, b M128i) M128i {
	return M128i(mask2Permutex2var32([16]byte(a), [16]byte(idx), uint8(k), [16]byte(b)))
}

func mask2Permutex2var32(a [16]byte, idx [16]byte, k uint8, b [16]byte) [16]byte


// MaskzPermutex2var32: Shuffle 32-bit integers in 'a' and 'b' using the
// corresponding selector and index in 'idx', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			off := idx[i+1:i]*32
//			IF k[j]
//				dst[i+31:i] := (idx[i+2]) ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMI2D, VPERMT2D'. Intrinsic: '_mm_maskz_permutex2var_epi32'.
// Requires AVX512F.
func MaskzPermutex2var32(k Mmask8, a M128i, idx M128i, b M128i) M128i {
	return M128i(maskzPermutex2var32(uint8(k), [16]byte(a), [16]byte(idx), [16]byte(b)))
}

func maskzPermutex2var32(k uint8, a [16]byte, idx [16]byte, b [16]byte) [16]byte


// Permutex2var32: Shuffle 32-bit integers in 'a' and 'b' using the
// corresponding selector and index in 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			off := idx[i+2:i]*32
//			dst[i+31:i] := idx[i+3] ? b[off+31:off] : a[off+31:off]
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMI2D, VPERMT2D'. Intrinsic: '_mm_permutex2var_epi32'.
// Requires AVX512F.
func Permutex2var32(a M128i, idx M128i, b M128i) M128i {
	return M128i(permutex2var32([16]byte(a), [16]byte(idx), [16]byte(b)))
}

func permutex2var32(a [16]byte, idx [16]byte, b [16]byte) [16]byte


// MaskPermutex2var64: Shuffle 64-bit integers in 'a' and 'b' using the
// corresponding selector and index in 'idx', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'a' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			off := idx[i]*64
//			IF k[j]
//				dst[i+63:i] := idx[i+1] ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMT2Q'. Intrinsic: '_mm_mask_permutex2var_epi64'.
// Requires AVX512F.
func MaskPermutex2var64(a M128i, k Mmask8, idx M128i, b M128i) M128i {
	return M128i(maskPermutex2var64([16]byte(a), uint8(k), [16]byte(idx), [16]byte(b)))
}

func maskPermutex2var64(a [16]byte, k uint8, idx [16]byte, b [16]byte) [16]byte


// Mask2Permutex2var64: Shuffle 64-bit integers in 'a' and 'b' using the
// corresponding selector and index in 'idx', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'idx' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			off := idx[i]*64
//			IF k[j]
//				dst[i+63:i] := idx[i+1] ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := idx[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMI2Q'. Intrinsic: '_mm_mask2_permutex2var_epi64'.
// Requires AVX512F.
func Mask2Permutex2var64(a M128i, idx M128i, k Mmask8, b M128i) M128i {
	return M128i(mask2Permutex2var64([16]byte(a), [16]byte(idx), uint8(k), [16]byte(b)))
}

func mask2Permutex2var64(a [16]byte, idx [16]byte, k uint8, b [16]byte) [16]byte


// MaskzPermutex2var64: Shuffle 64-bit integers in 'a' and 'b' using the
// corresponding selector and index in 'idx', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			off := idx[i]*64
//			IF k[j]
//				dst[i+63:i] := (idx[i+1]) ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMI2Q, VPERMT2Q'. Intrinsic: '_mm_maskz_permutex2var_epi64'.
// Requires AVX512F.
func MaskzPermutex2var64(k Mmask8, a M128i, idx M128i, b M128i) M128i {
	return M128i(maskzPermutex2var64(uint8(k), [16]byte(a), [16]byte(idx), [16]byte(b)))
}

func maskzPermutex2var64(k uint8, a [16]byte, idx [16]byte, b [16]byte) [16]byte


// Permutex2var64: Shuffle 64-bit integers in 'a' and 'b' using the
// corresponding selector and index in 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			off := idx[i]*64
//			dst[i+63:i] := idx[i+1] ? b[off+63:off] : a[off+63:off]
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMI2Q, VPERMT2Q'. Intrinsic: '_mm_permutex2var_epi64'.
// Requires AVX512F.
func Permutex2var64(a M128i, idx M128i, b M128i) M128i {
	return M128i(permutex2var64([16]byte(a), [16]byte(idx), [16]byte(b)))
}

func permutex2var64(a [16]byte, idx [16]byte, b [16]byte) [16]byte


// MaskPermutex2var8: Shuffle 8-bit integers in 'a' and 'b' using the
// corresponding selector and index in 'idx', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'a' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				off := 8*idx[i+3:i]
//				dst[i+7:i] := idx[i+4] ? b[off+7:off] : a[off+7:off]
//			ELSE
//				dst[i+7:i] := a[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMT2B'. Intrinsic: '_mm_mask_permutex2var_epi8'.
// Requires AVX512VL.
func MaskPermutex2var8(a M128i, k Mmask16, idx M128i, b M128i) M128i {
	return M128i(maskPermutex2var8([16]byte(a), uint16(k), [16]byte(idx), [16]byte(b)))
}

func maskPermutex2var8(a [16]byte, k uint16, idx [16]byte, b [16]byte) [16]byte


// Mask2Permutex2var8: Shuffle 8-bit integers in 'a' and 'b' using the
// corresponding selector and index in 'idx', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'a' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				off := 8*idx[i+3:i]
//				dst[i+7:i] := idx[i+4] ? b[off+7:off] : a[off+7:off]
//			ELSE
//				dst[i+7:i] := a[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMI2B'. Intrinsic: '_mm_mask2_permutex2var_epi8'.
// Requires AVX512VL.
func Mask2Permutex2var8(a M128i, idx M128i, k Mmask16, b M128i) M128i {
	return M128i(mask2Permutex2var8([16]byte(a), [16]byte(idx), uint16(k), [16]byte(b)))
}

func mask2Permutex2var8(a [16]byte, idx [16]byte, k uint16, b [16]byte) [16]byte


// MaskzPermutex2var8: Shuffle 8-bit integers in 'a' and 'b' using the
// corresponding selector and index in 'idx', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				off := 8*idx[i+3:i]
//				dst[i+7:i] := idx[i+4] ? b[off+7:off] : a[off+7:off]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMI2B, VPERMT2B'. Intrinsic: '_mm_maskz_permutex2var_epi8'.
// Requires AVX512VL.
func MaskzPermutex2var8(k Mmask16, a M128i, idx M128i, b M128i) M128i {
	return M128i(maskzPermutex2var8(uint16(k), [16]byte(a), [16]byte(idx), [16]byte(b)))
}

func maskzPermutex2var8(k uint16, a [16]byte, idx [16]byte, b [16]byte) [16]byte


// Permutex2var8: Shuffle 8-bit integers in 'a' and 'b' using the corresponding
// selector and index in 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*8
//			off := 8*idx[i+3:i]
//			dst[i+7:i] := idx[i+4] ? b[off+7:off] : a[off+7:off]
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMI2B'. Intrinsic: '_mm_permutex2var_epi8'.
// Requires AVX512VL.
func Permutex2var8(a M128i, idx M128i, b M128i) M128i {
	return M128i(permutex2var8([16]byte(a), [16]byte(idx), [16]byte(b)))
}

func permutex2var8(a [16]byte, idx [16]byte, b [16]byte) [16]byte


// MaskPermutex2varPd: Shuffle double-precision (64-bit) floating-point
// elements in 'a' and 'b' using the corresponding selector and index in 'idx',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			off := idx[i]*64
//			IF k[j]
//				dst[i+63:i] := idx[i+1] ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMT2PD'. Intrinsic: '_mm_mask_permutex2var_pd'.
// Requires AVX512F.
func MaskPermutex2varPd(a M128d, k Mmask8, idx M128i, b M128d) M128d {
	return M128d(maskPermutex2varPd([2]float64(a), uint8(k), [16]byte(idx), [2]float64(b)))
}

func maskPermutex2varPd(a [2]float64, k uint8, idx [16]byte, b [2]float64) [2]float64


// Mask2Permutex2varPd: Shuffle double-precision (64-bit) floating-point
// elements in 'a' and 'b' using the corresponding selector and index in 'idx',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'idx' when the corresponding mask bit is not set) 
//
//		FOR j := 0 to 1
//			i := j*64
//			off := idx[i]*64
//			IF k[j]
//				dst[i+63:i] := idx[i+1] ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := idx[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMI2PD'. Intrinsic: '_mm_mask2_permutex2var_pd'.
// Requires AVX512F.
func Mask2Permutex2varPd(a M128d, idx M128i, k Mmask8, b M128d) M128d {
	return M128d(mask2Permutex2varPd([2]float64(a), [16]byte(idx), uint8(k), [2]float64(b)))
}

func mask2Permutex2varPd(a [2]float64, idx [16]byte, k uint8, b [2]float64) [2]float64


// MaskzPermutex2varPd: Shuffle double-precision (64-bit) floating-point
// elements in 'a' and 'b' using the corresponding selector and index in 'idx',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			off := idx[i]*64
//			IF k[j]
//				dst[i+63:i] := (idx[i+1]) ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMI2PD, VPERMT2PD'. Intrinsic: '_mm_maskz_permutex2var_pd'.
// Requires AVX512F.
func MaskzPermutex2varPd(k Mmask8, a M128d, idx M128i, b M128d) M128d {
	return M128d(maskzPermutex2varPd(uint8(k), [2]float64(a), [16]byte(idx), [2]float64(b)))
}

func maskzPermutex2varPd(k uint8, a [2]float64, idx [16]byte, b [2]float64) [2]float64


// Permutex2varPd: Shuffle double-precision (64-bit) floating-point elements in
// 'a' and 'b' using the corresponding selector and index in 'idx', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			off := idx[i]*64
//			dst[i+63:i] := idx[i+1] ? b[off+63:off] : a[off+63:off]
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMI2PD, VPERMT2PD'. Intrinsic: '_mm_permutex2var_pd'.
// Requires AVX512F.
func Permutex2varPd(a M128d, idx M128i, b M128d) M128d {
	return M128d(permutex2varPd([2]float64(a), [16]byte(idx), [2]float64(b)))
}

func permutex2varPd(a [2]float64, idx [16]byte, b [2]float64) [2]float64


// MaskPermutex2varPs: Shuffle single-precision (32-bit) floating-point
// elements in 'a' and 'b' using the corresponding selector and index in 'idx',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			off := idx[i+1:i]*32
//			IF k[j]
//				dst[i+31:i] := idx[i+2] ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMT2PS'. Intrinsic: '_mm_mask_permutex2var_ps'.
// Requires AVX512F.
func MaskPermutex2varPs(a M128, k Mmask8, idx M128i, b M128) M128 {
	return M128(maskPermutex2varPs([4]float32(a), uint8(k), [16]byte(idx), [4]float32(b)))
}

func maskPermutex2varPs(a [4]float32, k uint8, idx [16]byte, b [4]float32) [4]float32


// Mask2Permutex2varPs: Shuffle single-precision (32-bit) floating-point
// elements in 'a' and 'b' using the corresponding selector and index in 'idx',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'idx' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			off := idx[i+1:i]*32
//			IF k[j]
//				dst[i+31:i] := idx[i+2] ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := idx[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMI2PS'. Intrinsic: '_mm_mask2_permutex2var_ps'.
// Requires AVX512F.
func Mask2Permutex2varPs(a M128, idx M128i, k Mmask8, b M128) M128 {
	return M128(mask2Permutex2varPs([4]float32(a), [16]byte(idx), uint8(k), [4]float32(b)))
}

func mask2Permutex2varPs(a [4]float32, idx [16]byte, k uint8, b [4]float32) [4]float32


// MaskzPermutex2varPs: Shuffle single-precision (32-bit) floating-point
// elements in 'a' and 'b' using the corresponding selector and index in 'idx',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			off := idx[i+1:i]*32
//			IF k[j]
//				dst[i+31:i] := (idx[i+2]) ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMI2PS, VPERMT2PS'. Intrinsic: '_mm_maskz_permutex2var_ps'.
// Requires AVX512F.
func MaskzPermutex2varPs(k Mmask8, a M128, idx M128i, b M128) M128 {
	return M128(maskzPermutex2varPs(uint8(k), [4]float32(a), [16]byte(idx), [4]float32(b)))
}

func maskzPermutex2varPs(k uint8, a [4]float32, idx [16]byte, b [4]float32) [4]float32


// Permutex2varPs: Shuffle single-precision (32-bit) floating-point elements in
// 'a' and 'b' using the corresponding selector and index in 'idx', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			off := idx[i+1:i]*32
//			dst[i+31:i] := idx[i+2] ? b[off+31:off] : a[off+31:off]
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMI2PS, VPERMT2PS'. Intrinsic: '_mm_permutex2var_ps'.
// Requires AVX512F.
func Permutex2varPs(a M128, idx M128i, b M128) M128 {
	return M128(permutex2varPs([4]float32(a), [16]byte(idx), [4]float32(b)))
}

func permutex2varPs(a [4]float32, idx [16]byte, b [4]float32) [4]float32


// MaskPermutexvar16: Shuffle 16-bit integers in 'a' using the corresponding
// index in 'idx', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			id := idx[i+2:i]*16
//			IF k[j]
//				dst[i+15:i] := a[id+15:id]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMW'. Intrinsic: '_mm_mask_permutexvar_epi16'.
// Requires AVX512BW.
func MaskPermutexvar16(src M128i, k Mmask8, idx M128i, a M128i) M128i {
	return M128i(maskPermutexvar16([16]byte(src), uint8(k), [16]byte(idx), [16]byte(a)))
}

func maskPermutexvar16(src [16]byte, k uint8, idx [16]byte, a [16]byte) [16]byte


// MaskzPermutexvar16: Shuffle 16-bit integers in 'a' using the corresponding
// index in 'idx', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			id := idx[i+2:i]*16
//			IF k[j]
//				dst[i+15:i] := a[id+15:id]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMW'. Intrinsic: '_mm_maskz_permutexvar_epi16'.
// Requires AVX512BW.
func MaskzPermutexvar16(k Mmask8, idx M128i, a M128i) M128i {
	return M128i(maskzPermutexvar16(uint8(k), [16]byte(idx), [16]byte(a)))
}

func maskzPermutexvar16(k uint8, idx [16]byte, a [16]byte) [16]byte


// Permutexvar16: Shuffle 16-bit integers in 'a' using the corresponding index
// in 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			id := idx[i+2:i]*16
//			dst[i+15:i] := a[id+15:id]
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMW'. Intrinsic: '_mm_permutexvar_epi16'.
// Requires AVX512BW.
func Permutexvar16(idx M128i, a M128i) M128i {
	return M128i(permutexvar16([16]byte(idx), [16]byte(a)))
}

func permutexvar16(idx [16]byte, a [16]byte) [16]byte


// MaskPermutexvar8: Shuffle 8-bit integers in 'a' using the corresponding
// index in 'idx', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			id := idx[i+3:i]*8
//			IF k[j]
//				dst[i+7:i] := a[id+7:id]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMB'. Intrinsic: '_mm_mask_permutexvar_epi8'.
// Requires AVX512VL.
func MaskPermutexvar8(src M128i, k Mmask16, idx M128i, a M128i) M128i {
	return M128i(maskPermutexvar8([16]byte(src), uint16(k), [16]byte(idx), [16]byte(a)))
}

func maskPermutexvar8(src [16]byte, k uint16, idx [16]byte, a [16]byte) [16]byte


// MaskzPermutexvar8: Shuffle 8-bit integers in 'a' using the corresponding
// index in 'idx', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			id := idx[i+3:i]*8
//			IF k[j]
//				dst[i+7:i] := a[id+7:id]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMB'. Intrinsic: '_mm_maskz_permutexvar_epi8'.
// Requires AVX512VL.
func MaskzPermutexvar8(k Mmask16, idx M128i, a M128i) M128i {
	return M128i(maskzPermutexvar8(uint16(k), [16]byte(idx), [16]byte(a)))
}

func maskzPermutexvar8(k uint16, idx [16]byte, a [16]byte) [16]byte


// Permutexvar8: Shuffle 8-bit integers in 'a' using the corresponding index in
// 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*8
//			id := idx[i+3:i]*8
//			dst[i+7:i] := a[id+7:id]
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPERMB'. Intrinsic: '_mm_permutexvar_epi8'.
// Requires AVX512VL.
func Permutexvar8(idx M128i, a M128i) M128i {
	return M128i(permutexvar8([16]byte(idx), [16]byte(a)))
}

func permutexvar8(idx [16]byte, a [16]byte) [16]byte


// PowPd: Compute the exponential value of packed double-precision (64-bit)
// floating-point elements in 'a' raised by packed elements in 'b', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := (a[i+63:i])^(b[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_pow_pd'.
// Requires SSE.
func PowPd(a M128d, b M128d) M128d {
	return M128d(powPd([2]float64(a), [2]float64(b)))
}

func powPd(a [2]float64, b [2]float64) [2]float64


// PowPs: Compute the exponential value of packed single-precision (32-bit)
// floating-point elements in 'a' raised by packed elements in 'b', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := (a[i+31:i])^(b[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_pow_ps'.
// Requires SSE.
func PowPs(a M128, b M128) M128 {
	return M128(powPs([4]float32(a), [4]float32(b)))
}

func powPs(a [4]float32, b [4]float32) [4]float32


// MaskRangePd: Calculate the max, min, absolute max, or absolute min
// (depending on control in 'imm8') for packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set).
// 	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 =
// absolute max, 11 = absolute min.
// 	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from
// compare result, 10 = clear sign bit, 11 = set sign bit. 
//
//		RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0])
//		{
//			CASE opCtl[1:0]
//			0: tmp[63:0] := (src1[63:0] <= src2[63:0]) ? src1[63:0] : src2[63:0]
//			1: tmp[63:0] := (src1[63:0] <= src2[63:0]) ? src2[63:0] : src1[63:0]
//			2: tmp[63:0] := (ABS(src1[63:0]) <= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
//			3: tmp[63:0] := (ABS(src1[63:0]) <= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
//			ESAC
//			
//			CASE signSelCtl[1:0]
//			0: dst[63:0] := (src1[63] << 63) OR (tmp[62:0])
//			1: dst[63:0] := tmp[63:0]
//			2: dst[63:0] := (0 << 63) OR (tmp[62:0])
//			3: dst[63:0] := (1 << 63) OR (tmp[62:0])
//			ESAC
//			
//			RETURN dst
//		}
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RANGE(a[i+63:i], b[i+63:i], imm8[1:0], imm8[3:2])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRANGEPD'. Intrinsic: '_mm_mask_range_pd'.
// Requires AVX512DQ.
func MaskRangePd(src M128d, k Mmask8, a M128d, b M128d, imm8 int) M128d {
	return M128d(maskRangePd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), imm8))
}

func maskRangePd(src [2]float64, k uint8, a [2]float64, b [2]float64, imm8 int) [2]float64


// MaskzRangePd: Calculate the max, min, absolute max, or absolute min
// (depending on control in 'imm8') for packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set).
// 	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 =
// absolute max, 11 = absolute min.
// 	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from
// compare result, 10 = clear sign bit, 11 = set sign bit. 
//
//		RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0])
//		{
//			CASE opCtl[1:0]
//			0: tmp[63:0] := (src1[63:0] <= src2[63:0]) ? src1[63:0] : src2[63:0]
//			1: tmp[63:0] := (src1[63:0] <= src2[63:0]) ? src2[63:0] : src1[63:0]
//			2: tmp[63:0] := (ABS(src1[63:0]) <= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
//			3: tmp[63:0] := (ABS(src1[63:0]) <= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
//			ESAC
//			
//			CASE signSelCtl[1:0]
//			0: dst[63:0] := (src1[63] << 63) OR (tmp[62:0])
//			1: dst[63:0] := tmp[63:0]
//			2: dst[63:0] := (0 << 63) OR (tmp[62:0])
//			3: dst[63:0] := (1 << 63) OR (tmp[62:0])
//			ESAC
//			
//			RETURN dst
//		}
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RANGE(a[i+63:i], b[i+63:i], imm8[1:0], imm8[3:2])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRANGEPD'. Intrinsic: '_mm_maskz_range_pd'.
// Requires AVX512DQ.
func MaskzRangePd(k Mmask8, a M128d, b M128d, imm8 int) M128d {
	return M128d(maskzRangePd(uint8(k), [2]float64(a), [2]float64(b), imm8))
}

func maskzRangePd(k uint8, a [2]float64, b [2]float64, imm8 int) [2]float64


// RangePd: Calculate the max, min, absolute max, or absolute min (depending on
// control in 'imm8') for packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store the results in 'dst'.
// 	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 =
// absolute max, 11 = absolute min.
// 	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from
// compare result, 10 = clear sign bit, 11 = set sign bit. 
//
//		RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0])
//		{
//			CASE opCtl[1:0]
//			0: tmp[63:0] := (src1[63:0] <= src2[63:0]) ? src1[63:0] : src2[63:0]
//			1: tmp[63:0] := (src1[63:0] <= src2[63:0]) ? src2[63:0] : src1[63:0]
//			2: tmp[63:0] := (ABS(src1[63:0]) <= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
//			3: tmp[63:0] := (ABS(src1[63:0]) <= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
//			ESAC
//			
//			CASE signSelCtl[1:0]
//			0: dst[63:0] := (src1[63] << 63) OR (tmp[62:0])
//			1: dst[63:0] := tmp[63:0]
//			2: dst[63:0] := (0 << 63) OR (tmp[62:0])
//			3: dst[63:0] := (1 << 63) OR (tmp[62:0])
//			ESAC
//			
//			RETURN dst
//		}
//		
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := RANGE(a[i+63:i], b[i+63:i], imm8[1:0], imm8[3:2])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRANGEPD'. Intrinsic: '_mm_range_pd'.
// Requires AVX512DQ.
func RangePd(a M128d, b M128d, imm8 int) M128d {
	return M128d(rangePd([2]float64(a), [2]float64(b), imm8))
}

func rangePd(a [2]float64, b [2]float64, imm8 int) [2]float64


// MaskRangePs: Calculate the max, min, absolute max, or absolute min
// (depending on control in 'imm8') for packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set).
// 	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 =
// absolute max, 11 = absolute min.
// 	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from
// compare result, 10 = clear sign bit, 11 = set sign bit. 
//
//		RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0])
//		{
//			CASE opCtl[1:0]
//			0: tmp[31:0] := (src1[31:0] <= src2[31:0]) ? src1[31:0] : src2[31:0]
//			1: tmp[31:0] := (src1[31:0] <= src2[31:0]) ? src2[31:0] : src1[31:0]
//			2: tmp[31:0] := (ABS(src1[31:0]) <= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
//			3: tmp[31:0] := (ABS(src1[31:0]) <= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
//			ESAC
//			
//			CASE signSelCtl[1:0]
//			0: dst[31:0] := (src1[31] << 31) OR (tmp[30:0])
//			1: dst[31:0] := tmp[63:0]
//			2: dst[31:0] := (0 << 31) OR (tmp[30:0])
//			3: dst[31:0] := (1 << 31) OR (tmp[30:0])
//			ESAC
//			
//			RETURN dst
//		}
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RANGE(a[i+31:i], b[i+31:i], imm8[1:0], imm8[3:2])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRANGEPS'. Intrinsic: '_mm_mask_range_ps'.
// Requires AVX512DQ.
func MaskRangePs(src M128, k Mmask8, a M128, b M128, imm8 int) M128 {
	return M128(maskRangePs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), imm8))
}

func maskRangePs(src [4]float32, k uint8, a [4]float32, b [4]float32, imm8 int) [4]float32


// MaskzRangePs: Calculate the max, min, absolute max, or absolute min
// (depending on control in 'imm8') for packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set).
// 	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 =
// absolute max, 11 = absolute min.
// 	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from
// compare result, 10 = clear sign bit, 11 = set sign bit. 
//
//		RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0])
//		{
//			CASE opCtl[1:0]
//			0: tmp[31:0] := (src1[31:0] <= src2[31:0]) ? src1[31:0] : src2[31:0]
//			1: tmp[31:0] := (src1[31:0] <= src2[31:0]) ? src2[31:0] : src1[31:0]
//			2: tmp[31:0] := (ABS(src1[31:0]) <= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
//			3: tmp[31:0] := (ABS(src1[31:0]) <= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
//			ESAC
//			
//			CASE signSelCtl[1:0]
//			0: dst[31:0] := (src1[31] << 31) OR (tmp[30:0])
//			1: dst[31:0] := tmp[63:0]
//			2: dst[31:0] := (0 << 31) OR (tmp[30:0])
//			3: dst[31:0] := (1 << 31) OR (tmp[30:0])
//			ESAC
//			
//			RETURN dst
//		}
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RANGE(a[i+31:i], b[i+31:i], imm8[1:0], imm8[3:2])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRANGEPS'. Intrinsic: '_mm_maskz_range_ps'.
// Requires AVX512DQ.
func MaskzRangePs(k Mmask8, a M128, b M128, imm8 int) M128 {
	return M128(maskzRangePs(uint8(k), [4]float32(a), [4]float32(b), imm8))
}

func maskzRangePs(k uint8, a [4]float32, b [4]float32, imm8 int) [4]float32


// RangePs: Calculate the max, min, absolute max, or absolute min (depending on
// control in 'imm8') for packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store the results in 'dst'.
// 	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 =
// absolute max, 11 = absolute min.
// 	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from
// compare result, 10 = clear sign bit, 11 = set sign bit. 
//
//		RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0])
//		{
//			CASE opCtl[1:0]
//			0: tmp[31:0] := (src1[31:0] <= src2[31:0]) ? src1[31:0] : src2[31:0]
//			1: tmp[31:0] := (src1[31:0] <= src2[31:0]) ? src2[31:0] : src1[31:0]
//			2: tmp[31:0] := (ABS(src1[31:0]) <= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
//			3: tmp[31:0] := (ABS(src1[31:0]) <= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
//			ESAC
//			
//			CASE signSelCtl[1:0]
//			0: dst[31:0] := (src1[31] << 31) OR (tmp[30:0])
//			1: dst[31:0] := tmp[63:0]
//			2: dst[31:0] := (0 << 31) OR (tmp[30:0])
//			3: dst[31:0] := (1 << 31) OR (tmp[30:0])
//			ESAC
//			
//			RETURN dst
//		}
//		
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := RANGE(a[i+31:i], b[i+31:i], imm8[1:0], imm8[3:2])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRANGEPS'. Intrinsic: '_mm_range_ps'.
// Requires AVX512DQ.
func RangePs(a M128, b M128, imm8 int) M128 {
	return M128(rangePs([4]float32(a), [4]float32(b), imm8))
}

func rangePs(a [4]float32, b [4]float32, imm8 int) [4]float32


// MaskRangeRoundSd: Calculate the max, min, absolute max, or absolute min
// (depending on control in 'imm8') for the lower double-precision (64-bit)
// floating-point element in 'a' and 'b', store the result in the lower element
// of 'dst' using writemask 'k' (the element is copied from 'src' when mask bit
// 0 is not set), and copy the upper element from 'a' to the upper element of
// 'dst'.
// 	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 =
// absolute max, 11 = absolute min.
// 	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from
// compare result, 10 = clear sign bit, 11 = set sign bit.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0])
//		{
//			CASE opCtl[1:0]
//			0: tmp[63:0] := (src1[63:0] <= src2[63:0]) ? src1[63:0] : src2[63:0]
//			1: tmp[63:0] := (src1[63:0] <= src2[63:0]) ? src2[63:0] : src1[63:0]
//			2: tmp[63:0] := (ABS(src1[63:0]) <= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
//			3: tmp[63:0] := (ABS(src1[63:0]) <= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
//			ESAC
//			
//			CASE signSelCtl[1:0]
//			0: dst[63:0] := (src1[63] << 63) OR (tmp[62:0])
//			1: dst[63:0] := tmp[63:0]
//			2: dst[63:0] := (0 << 63) OR (tmp[62:0])
//			3: dst[63:0] := (1 << 63) OR (tmp[62:0])
//			ESAC
//			
//			RETURN dst
//		}
//		
//		IF k[0]
//			dst[63:0]] := RANGE(a[63:0], b[63:0], imm8[1:0], imm8[3:2])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VRANGESD'. Intrinsic: '_mm_mask_range_round_sd'.
// Requires AVX512DQ.
func MaskRangeRoundSd(src M128d, k Mmask8, a M128d, b M128d, imm8 int, rounding int) M128d {
	return M128d(maskRangeRoundSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), imm8, rounding))
}

func maskRangeRoundSd(src [2]float64, k uint8, a [2]float64, b [2]float64, imm8 int, rounding int) [2]float64


// MaskzRangeRoundSd: Calculate the max, min, absolute max, or absolute min
// (depending on control in 'imm8') for the lower double-precision (64-bit)
// floating-point element in 'a' and 'b', store the result in the lower element
// of 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is
// not set), and copy the upper element from 'a' to the upper element of 'dst'.
// 	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 =
// absolute max, 11 = absolute min.
// 	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from
// compare result, 10 = clear sign bit, 11 = set sign bit.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0])
//		{
//			CASE opCtl[1:0]
//			0: tmp[63:0] := (src1[63:0] <= src2[63:0]) ? src1[63:0] : src2[63:0]
//			1: tmp[63:0] := (src1[63:0] <= src2[63:0]) ? src2[63:0] : src1[63:0]
//			2: tmp[63:0] := (ABS(src1[63:0]) <= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
//			3: tmp[63:0] := (ABS(src1[63:0]) <= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
//			ESAC
//			
//			CASE signSelCtl[1:0]
//			0: dst[63:0] := (src1[63] << 63) OR (tmp[62:0])
//			1: dst[63:0] := tmp[63:0]
//			2: dst[63:0] := (0 << 63) OR (tmp[62:0])
//			3: dst[63:0] := (1 << 63) OR (tmp[62:0])
//			ESAC
//			
//			RETURN dst
//		}
//		
//		IF k[0]
//			dst[63:0]] := RANGE(a[63:0], b[63:0], imm8[1:0], imm8[3:2])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VRANGESD'. Intrinsic: '_mm_maskz_range_round_sd'.
// Requires AVX512DQ.
func MaskzRangeRoundSd(k Mmask8, a M128d, b M128d, imm8 int, rounding int) M128d {
	return M128d(maskzRangeRoundSd(uint8(k), [2]float64(a), [2]float64(b), imm8, rounding))
}

func maskzRangeRoundSd(k uint8, a [2]float64, b [2]float64, imm8 int, rounding int) [2]float64


// RangeRoundSd: Calculate the max, min, absolute max, or absolute min
// (depending on control in 'imm8') for the lower double-precision (64-bit)
// floating-point element in 'a' and 'b', store the result in the lower element
// of 'dst', and copy the upper element from 'a' to the upper element of 'dst'.
// 	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 =
// absolute max, 11 = absolute min.
// 	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from
// compare result, 10 = clear sign bit, 11 = set sign bit.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0])
//		{
//			CASE opCtl[1:0]
//			0: tmp[63:0] := (src1[63:0] <= src2[63:0]) ? src1[63:0] : src2[63:0]
//			1: tmp[63:0] := (src1[63:0] <= src2[63:0]) ? src2[63:0] : src1[63:0]
//			2: tmp[63:0] := (ABS(src1[63:0]) <= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
//			3: tmp[63:0] := (ABS(src1[63:0]) <= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
//			ESAC
//			
//			CASE signSelCtl[1:0]
//			0: dst[63:0] := (src1[63] << 63) OR (tmp[62:0])
//			1: dst[63:0] := tmp[63:0]
//			2: dst[63:0] := (0 << 63) OR (tmp[62:0])
//			3: dst[63:0] := (1 << 63) OR (tmp[62:0])
//			ESAC
//			
//			RETURN dst
//		}
//		
//		dst[63:0]] := RANGE(a[63:0], b[63:0], imm8[1:0], imm8[3:2])
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VRANGESD'. Intrinsic: '_mm_range_round_sd'.
// Requires AVX512DQ.
func RangeRoundSd(a M128d, b M128d, imm8 int, rounding int) M128d {
	return M128d(rangeRoundSd([2]float64(a), [2]float64(b), imm8, rounding))
}

func rangeRoundSd(a [2]float64, b [2]float64, imm8 int, rounding int) [2]float64


// MaskRangeRoundSs: Calculate the max, min, absolute max, or absolute min
// (depending on control in 'imm8') for the lower single-precision (32-bit)
// floating-point element in 'a' and 'b', store the result in the lower element
// of 'dst' using writemask 'k' (the element is copied from 'src' when mask bit
// 0 is not set), and copy the upper 3 packed elements from 'a' to the upper
// elements of 'dst'.
// 	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 =
// absolute max, 11 = absolute min.
// 	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from
// compare result, 10 = clear sign bit, 11 = set sign bit.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0])
//		{
//			CASE opCtl[1:0]
//			0: tmp[31:0] := (src1[31:0] <= src2[31:0]) ? src1[31:0] : src2[31:0]
//			1: tmp[31:0] := (src1[31:0] <= src2[31:0]) ? src2[31:0] : src1[31:0]
//			2: tmp[31:0] := (ABS(src1[31:0]) <= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
//			3: tmp[31:0] := (ABS(src1[31:0]) <= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
//			ESAC
//			
//			CASE signSelCtl[1:0]
//			0: dst[31:0] := (src1[31] << 31) OR (tmp[30:0])
//			1: dst[31:0] := tmp[31:0]
//			2: dst[31:0] := (0 << 31) OR (tmp[30:0])
//			3: dst[31:0] := (1 << 31) OR (tmp[30:0])
//			ESAC
//			
//			RETURN dst
//		}
//		
//		IF k[0]
//			dst[31:0]] := RANGE(a[31:0], b[31:0], imm8[1:0], imm8[3:2])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VRANGESS'. Intrinsic: '_mm_mask_range_round_ss'.
// Requires AVX512DQ.
func MaskRangeRoundSs(src M128, k Mmask8, a M128, b M128, imm8 int, rounding int) M128 {
	return M128(maskRangeRoundSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), imm8, rounding))
}

func maskRangeRoundSs(src [4]float32, k uint8, a [4]float32, b [4]float32, imm8 int, rounding int) [4]float32


// MaskzRangeRoundSs: Calculate the max, min, absolute max, or absolute min
// (depending on control in 'imm8') for the lower single-precision (32-bit)
// floating-point element in 'a' and 'b', store the result in the lower element
// of 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is
// not set), and copy the upper 3 packed elements from 'a' to the upper
// elements of 'dst'.
// 	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 =
// absolute max, 11 = absolute min.
// 	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from
// compare result, 10 = clear sign bit, 11 = set sign bit.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0])
//		{
//			CASE opCtl[1:0]
//			0: tmp[31:0] := (src1[31:0] <= src2[31:0]) ? src1[31:0] : src2[31:0]
//			1: tmp[31:0] := (src1[31:0] <= src2[31:0]) ? src2[31:0] : src1[31:0]
//			2: tmp[31:0] := (ABS(src1[31:0]) <= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
//			3: tmp[31:0] := (ABS(src1[31:0]) <= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
//			ESAC
//			
//			CASE signSelCtl[1:0]
//			0: dst[31:0] := (src1[31] << 31) OR (tmp[30:0])
//			1: dst[31:0] := tmp[31:0]
//			2: dst[31:0] := (0 << 31) OR (tmp[30:0])
//			3: dst[31:0] := (1 << 31) OR (tmp[30:0])
//			ESAC
//			
//			RETURN dst
//		}
//		
//		IF k[0]
//			dst[31:0]] := RANGE(a[31:0], b[31:0], imm8[1:0], imm8[3:2])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VRANGESS'. Intrinsic: '_mm_maskz_range_round_ss'.
// Requires AVX512DQ.
func MaskzRangeRoundSs(k Mmask8, a M128, b M128, imm8 int, rounding int) M128 {
	return M128(maskzRangeRoundSs(uint8(k), [4]float32(a), [4]float32(b), imm8, rounding))
}

func maskzRangeRoundSs(k uint8, a [4]float32, b [4]float32, imm8 int, rounding int) [4]float32


// RangeRoundSs: Calculate the max, min, absolute max, or absolute min
// (depending on control in 'imm8') for the lower single-precision (32-bit)
// floating-point element in 'a' and 'b', store the result in the lower element
// of 'dst', and copy the upper 3 packed elements from 'a' to the upper
// elements of 'dst'.
// 	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 =
// absolute max, 11 = absolute min.
// 	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from
// compare result, 10 = clear sign bit, 11 = set sign bit.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0])
//		{
//			CASE opCtl[1:0]
//			0: tmp[31:0] := (src1[31:0] <= src2[31:0]) ? src1[31:0] : src2[31:0]
//			1: tmp[31:0] := (src1[31:0] <= src2[31:0]) ? src2[31:0] : src1[31:0]
//			2: tmp[31:0] := (ABS(src1[31:0]) <= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
//			3: tmp[31:0] := (ABS(src1[31:0]) <= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
//			ESAC
//			
//			CASE signSelCtl[1:0]
//			0: dst[31:0] := (src1[31] << 31) OR (tmp[30:0])
//			1: dst[31:0] := tmp[31:0]
//			2: dst[31:0] := (0 << 31) OR (tmp[30:0])
//			3: dst[31:0] := (1 << 31) OR (tmp[30:0])
//			ESAC
//			
//			RETURN dst
//		}
//		
//		dst[31:0]] := RANGE(a[31:0], b[31:0], imm8[1:0], imm8[3:2])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VRANGESS'. Intrinsic: '_mm_range_round_ss'.
// Requires AVX512DQ.
func RangeRoundSs(a M128, b M128, imm8 int, rounding int) M128 {
	return M128(rangeRoundSs([4]float32(a), [4]float32(b), imm8, rounding))
}

func rangeRoundSs(a [4]float32, b [4]float32, imm8 int, rounding int) [4]float32


// MaskRangeSd: Calculate the max, min, absolute max, or absolute min
// (depending on control in 'imm8') for the lower double-precision (64-bit)
// floating-point element in 'a' and 'b', store the result in the lower element
// of 'dst' using writemask 'k' (the element is copied from 'src' when mask bit
// 0 is not set), and copy the upper element from 'a' to the upper element of
// 'dst'.
// 	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 =
// absolute max, 11 = absolute min.
// 	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from
// compare result, 10 = clear sign bit, 11 = set sign bit. 
//
//		RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0])
//		{
//			CASE opCtl[1:0]
//			0: tmp[63:0] := (src1[63:0] <= src2[63:0]) ? src1[63:0] : src2[63:0]
//			1: tmp[63:0] := (src1[63:0] <= src2[63:0]) ? src2[63:0] : src1[63:0]
//			2: tmp[63:0] := (ABS(src1[63:0]) <= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
//			3: tmp[63:0] := (ABS(src1[63:0]) <= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
//			ESAC
//			
//			CASE signSelCtl[1:0]
//			0: dst[63:0] := (src1[63] << 63) OR (tmp[62:0])
//			1: dst[63:0] := tmp[63:0]
//			2: dst[63:0] := (0 << 63) OR (tmp[62:0])
//			3: dst[63:0] := (1 << 63) OR (tmp[62:0])
//			ESAC
//			
//			RETURN dst
//		}
//		
//		IF k[0]
//			dst[63:0]] := RANGE(a[63:0], b[63:0], imm8[1:0], imm8[3:2])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VRANGESD'. Intrinsic: '_mm_mask_range_sd'.
// Requires AVX512DQ.
func MaskRangeSd(src M128d, k Mmask8, a M128d, b M128d, imm8 int) M128d {
	return M128d(maskRangeSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), imm8))
}

func maskRangeSd(src [2]float64, k uint8, a [2]float64, b [2]float64, imm8 int) [2]float64


// MaskzRangeSd: Calculate the max, min, absolute max, or absolute min
// (depending on control in 'imm8') for the lower double-precision (64-bit)
// floating-point element in 'a' and 'b', store the result in the lower element
// of 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is
// not set), and copy the upper element from 'a' to the upper element of 'dst'.
// 	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 =
// absolute max, 11 = absolute min.
// 	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from
// compare result, 10 = clear sign bit, 11 = set sign bit. 
//
//		RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0])
//		{
//			CASE opCtl[1:0]
//			0: tmp[63:0] := (src1[63:0] <= src2[63:0]) ? src1[63:0] : src2[63:0]
//			1: tmp[63:0] := (src1[63:0] <= src2[63:0]) ? src2[63:0] : src1[63:0]
//			2: tmp[63:0] := (ABS(src1[63:0]) <= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
//			3: tmp[63:0] := (ABS(src1[63:0]) <= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
//			ESAC
//			
//			CASE signSelCtl[1:0]
//			0: dst[63:0] := (src1[63] << 63) OR (tmp[62:0])
//			1: dst[63:0] := tmp[63:0]
//			2: dst[63:0] := (0 << 63) OR (tmp[62:0])
//			3: dst[63:0] := (1 << 63) OR (tmp[62:0])
//			ESAC
//			
//			RETURN dst
//		}
//		
//		IF k[0]
//			dst[63:0]] := RANGE(a[63:0], b[63:0], imm8[1:0], imm8[3:2])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VRANGESD'. Intrinsic: '_mm_maskz_range_sd'.
// Requires AVX512DQ.
func MaskzRangeSd(k Mmask8, a M128d, b M128d, imm8 int) M128d {
	return M128d(maskzRangeSd(uint8(k), [2]float64(a), [2]float64(b), imm8))
}

func maskzRangeSd(k uint8, a [2]float64, b [2]float64, imm8 int) [2]float64


// MaskRangeSs: Calculate the max, min, absolute max, or absolute min
// (depending on control in 'imm8') for the lower single-precision (32-bit)
// floating-point element in 'a' and 'b', store the result in the lower element
// of 'dst' using writemask 'k' (the element is copied from 'src' when mask bit
// 0 is not set), and copy the upper 3 packed elements from 'a' to the upper
// elements of 'dst'.
// 	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 =
// absolute max, 11 = absolute min.
// 	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from
// compare result, 10 = clear sign bit, 11 = set sign bit. 
//
//		RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0])
//		{
//			CASE opCtl[1:0]
//			0: tmp[31:0] := (src1[31:0] <= src2[31:0]) ? src1[31:0] : src2[31:0]
//			1: tmp[31:0] := (src1[31:0] <= src2[31:0]) ? src2[31:0] : src1[31:0]
//			2: tmp[31:0] := (ABS(src1[31:0]) <= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
//			3: tmp[31:0] := (ABS(src1[31:0]) <= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
//			ESAC
//			
//			CASE signSelCtl[1:0]
//			0: dst[31:0] := (src1[31] << 31) OR (tmp[30:0])
//			1: dst[31:0] := tmp[31:0]
//			2: dst[31:0] := (0 << 31) OR (tmp[30:0])
//			3: dst[31:0] := (1 << 31) OR (tmp[30:0])
//			ESAC
//			
//			RETURN dst
//		}
//		
//		IF k[0]
//			dst[31:0]] := RANGE(a[31:0], b[31:0], imm8[1:0], imm8[3:2])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VRANGESS'. Intrinsic: '_mm_mask_range_ss'.
// Requires AVX512DQ.
func MaskRangeSs(src M128, k Mmask8, a M128, b M128, imm8 int) M128 {
	return M128(maskRangeSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), imm8))
}

func maskRangeSs(src [4]float32, k uint8, a [4]float32, b [4]float32, imm8 int) [4]float32


// MaskzRangeSs: Calculate the max, min, absolute max, or absolute min
// (depending on control in 'imm8') for the lower single-precision (32-bit)
// floating-point element in 'a' and 'b', store the result in the lower element
// of 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is
// not set), and copy the upper 3 packed elements from 'a' to the upper
// elements of 'dst'.
// 	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 =
// absolute max, 11 = absolute min.
// 	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from
// compare result, 10 = clear sign bit, 11 = set sign bit. 
//
//		RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0])
//		{
//			CASE opCtl[1:0]
//			0: tmp[31:0] := (src1[31:0] <= src2[31:0]) ? src1[31:0] : src2[31:0]
//			1: tmp[31:0] := (src1[31:0] <= src2[31:0]) ? src2[31:0] : src1[31:0]
//			2: tmp[31:0] := (ABS(src1[31:0]) <= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
//			3: tmp[31:0] := (ABS(src1[31:0]) <= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
//			ESAC
//			
//			CASE signSelCtl[1:0]
//			0: dst[31:0] := (src1[31] << 31) OR (tmp[30:0])
//			1: dst[31:0] := tmp[31:0]
//			2: dst[31:0] := (0 << 31) OR (tmp[30:0])
//			3: dst[31:0] := (1 << 31) OR (tmp[30:0])
//			ESAC
//			
//			RETURN dst
//		}
//		
//		IF k[0]
//			dst[31:0]] := RANGE(a[31:0], b[31:0], imm8[1:0], imm8[3:2])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VRANGESS'. Intrinsic: '_mm_maskz_range_ss'.
// Requires AVX512DQ.
func MaskzRangeSs(k Mmask8, a M128, b M128, imm8 int) M128 {
	return M128(maskzRangeSs(uint8(k), [4]float32(a), [4]float32(b), imm8))
}

func maskzRangeSs(k uint8, a [4]float32, b [4]float32, imm8 int) [4]float32


// RcpPs: Compute the approximate reciprocal of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'. The
// maximum relative error for this approximation is less than 1.5*2^-12. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
//		ENDFOR
//
// Instruction: 'RCPPS'. Intrinsic: '_mm_rcp_ps'.
// Requires SSE.
func RcpPs(a M128) M128 {
	return M128(rcpPs([4]float32(a)))
}

func rcpPs(a [4]float32) [4]float32


// RcpSs: Compute the approximate reciprocal of the lower single-precision
// (32-bit) floating-point element in 'a', store the result in the lower
// element of 'dst', and copy the upper 3 packed elements from 'a' to the upper
// elements of 'dst'. The maximum relative error for this approximation is less
// than 1.5*2^-12. 
//
//		dst[31:0] := APPROXIMATE(1.0/a[31:0])
//		dst[127:32] := a[127:32]
//
// Instruction: 'RCPSS'. Intrinsic: '_mm_rcp_ss'.
// Requires SSE.
func RcpSs(a M128) M128 {
	return M128(rcpSs([4]float32(a)))
}

func rcpSs(a [4]float32) [4]float32


// MaskRcp14Pd: Compute the approximate reciprocal of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). The maximum relative error for this approximation is
// less than 2^-14. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := APPROXIMATE(1.0/a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRCP14PD'. Intrinsic: '_mm_mask_rcp14_pd'.
// Requires AVX512F.
func MaskRcp14Pd(src M128d, k Mmask8, a M128d) M128d {
	return M128d(maskRcp14Pd([2]float64(src), uint8(k), [2]float64(a)))
}

func maskRcp14Pd(src [2]float64, k uint8, a [2]float64) [2]float64


// MaskzRcp14Pd: Compute the approximate reciprocal of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). The maximum relative error for this approximation is less than
// 2^-14. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := APPROXIMATE(1.0/a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRCP14PD'. Intrinsic: '_mm_maskz_rcp14_pd'.
// Requires AVX512F.
func MaskzRcp14Pd(k Mmask8, a M128d) M128d {
	return M128d(maskzRcp14Pd(uint8(k), [2]float64(a)))
}

func maskzRcp14Pd(k uint8, a [2]float64) [2]float64


// Rcp14Pd: Compute the approximate reciprocal of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'. The
// maximum relative error for this approximation is less than 2^-14. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := APPROXIMATE(1.0/a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRCP14PD'. Intrinsic: '_mm_rcp14_pd'.
// Requires AVX512F.
func Rcp14Pd(a M128d) M128d {
	return M128d(rcp14Pd([2]float64(a)))
}

func rcp14Pd(a [2]float64) [2]float64


// MaskRcp14Ps: Compute the approximate reciprocal of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). The maximum relative error for this approximation is
// less than 2^-14. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRCP14PS'. Intrinsic: '_mm_mask_rcp14_ps'.
// Requires AVX512F.
func MaskRcp14Ps(src M128, k Mmask8, a M128) M128 {
	return M128(maskRcp14Ps([4]float32(src), uint8(k), [4]float32(a)))
}

func maskRcp14Ps(src [4]float32, k uint8, a [4]float32) [4]float32


// MaskzRcp14Ps: Compute the approximate reciprocal of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). The maximum relative error for this approximation is less than
// 2^-14. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRCP14PS'. Intrinsic: '_mm_maskz_rcp14_ps'.
// Requires AVX512F.
func MaskzRcp14Ps(k Mmask8, a M128) M128 {
	return M128(maskzRcp14Ps(uint8(k), [4]float32(a)))
}

func maskzRcp14Ps(k uint8, a [4]float32) [4]float32


// Rcp14Ps: Compute the approximate reciprocal of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'. The
// maximum relative error for this approximation is less than 2^-14. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRCP14PS'. Intrinsic: '_mm_rcp14_ps'.
// Requires AVX512F.
func Rcp14Ps(a M128) M128 {
	return M128(rcp14Ps([4]float32(a)))
}

func rcp14Ps(a [4]float32) [4]float32


// MaskRcp14Sd: Compute the approximate reciprocal of the lower
// double-precision (64-bit) floating-point element in 'b', store the result in
// the lower element of 'dst' using writemask 'k' (the element is copied from
// 'src' when mask bit 0 is not set), and copy the upper element from 'a' to
// the upper element of 'dst'. The maximum relative error for this
// approximation is less than 2^-14. 
//
//		IF k[0]
//			dst[63:0] := APPROXIMATE(1.0/b[63:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VRCP14SD'. Intrinsic: '_mm_mask_rcp14_sd'.
// Requires AVX512F.
func MaskRcp14Sd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskRcp14Sd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskRcp14Sd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzRcp14Sd: Compute the approximate reciprocal of the lower
// double-precision (64-bit) floating-point element in 'b', store the result in
// the lower element of 'dst' using zeromask 'k' (the element is zeroed out
// when mask bit 0 is not set), and copy the upper element from 'a' to the
// upper element of 'dst'. The maximum relative error for this approximation is
// less than 2^-14. 
//
//		IF k[0]
//			dst[63:0] := APPROXIMATE(1.0/b[63:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VRCP14SD'. Intrinsic: '_mm_maskz_rcp14_sd'.
// Requires AVX512F.
func MaskzRcp14Sd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzRcp14Sd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzRcp14Sd(k uint8, a [2]float64, b [2]float64) [2]float64


// Rcp14Sd: Compute the approximate reciprocal of the lower double-precision
// (64-bit) floating-point element in 'b', store the result in the lower
// element of 'dst', and copy the upper element from 'a' to the upper element
// of 'dst'. The maximum relative error for this approximation is less than
// 2^-14. 
//
//		dst[63:0] := APPROXIMATE(1.0/b[63:0])
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VRCP14SD'. Intrinsic: '_mm_rcp14_sd'.
// Requires AVX512F.
func Rcp14Sd(a M128d, b M128d) M128d {
	return M128d(rcp14Sd([2]float64(a), [2]float64(b)))
}

func rcp14Sd(a [2]float64, b [2]float64) [2]float64


// MaskRcp14Ss: Compute the approximate reciprocal of the lower
// single-precision (32-bit) floating-point element in 'b', store the result in
// the lower element of 'dst' using writemask 'k' (the element is copied from
// 'src' when mask bit 0 is not set), and copy the upper 3 packed elements from
// 'a' to the upper elements of 'dst'. The maximum relative error for this
// approximation is less than 2^-14. 
//
//		IF k[0]
//			dst[31:0] := APPROXIMATE(1.0/b[31:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VRCP14SS'. Intrinsic: '_mm_mask_rcp14_ss'.
// Requires AVX512F.
func MaskRcp14Ss(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskRcp14Ss([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskRcp14Ss(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzRcp14Ss: Compute the approximate reciprocal of the lower
// single-precision (32-bit) floating-point element in 'b', store the result in
// the lower element of 'dst' using zeromask 'k' (the element is zeroed out
// when mask bit 0 is not set), and copy the upper 3 packed elements from 'a'
// to the upper elements of 'dst'. The maximum relative error for this
// approximation is less than 2^-14. 
//
//		IF k[0]
//			dst[31:0] := APPROXIMATE(1.0/b[31:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VRCP14SS'. Intrinsic: '_mm_maskz_rcp14_ss'.
// Requires AVX512F.
func MaskzRcp14Ss(k Mmask8, a M128, b M128) M128 {
	return M128(maskzRcp14Ss(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzRcp14Ss(k uint8, a [4]float32, b [4]float32) [4]float32


// Rcp14Ss: Compute the approximate reciprocal of the lower single-precision
// (32-bit) floating-point element in 'b', store the result in the lower
// element of 'dst', and copy the upper 3 packed elements from 'a' to the upper
// elements of 'dst'. The maximum relative error for this approximation is less
// than 2^-14. 
//
//		dst[31:0] := APPROXIMATE(1.0/b[31:0])
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VRCP14SS'. Intrinsic: '_mm_rcp14_ss'.
// Requires AVX512F.
func Rcp14Ss(a M128, b M128) M128 {
	return M128(rcp14Ss([4]float32(a), [4]float32(b)))
}

func rcp14Ss(a [4]float32, b [4]float32) [4]float32


// MaskRcp28RoundSd: Compute the approximate reciprocal of the lower
// double-precision (64-bit) floating-point element in 'b', store the result in
// the lower element of 'dst' using writemask 'k' (the element is copied from
// 'src' when mask bit 0 is not set), and copy the upper element from 'a' to
// the upper element of 'dst'. The maximum relative error for this
// approximation is less than 2^-28. Rounding is done according to the
// 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0] THEN
//			dst[63:0] := RCP_28_DP(1.0/b[63:0];
//		ELSE
//			dst[63:0] := src[63:0];
//		FI
//		dst[127:64] := a[127:64];
//		dst[MAX:128] := 0;
//
// Instruction: 'VRCP28SD'. Intrinsic: '_mm_mask_rcp28_round_sd'.
// Requires AVX512ER.
func MaskRcp28RoundSd(src M128d, k Mmask8, a M128d, b M128d, rounding int) M128d {
	return M128d(maskRcp28RoundSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), rounding))
}

func maskRcp28RoundSd(src [2]float64, k uint8, a [2]float64, b [2]float64, rounding int) [2]float64


// MaskzRcp28RoundSd: Compute the approximate reciprocal of the lower
// double-precision (64-bit) floating-point element in 'b', store the result in
// the lower element of 'dst' using zeromask 'k' (the element is zeroed out
// when mask bit 0 is not set), and copy the upper element from 'a' to the
// upper element of 'dst'. The maximum relative error for this approximation is
// less than 2^-28. Rounding is done according to the 'rounding' parameter,
// which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0] THEN
//			dst[63:0] := RCP_28_DP(1.0/b[63:0];
//		ELSE
//			dst[63:0] := 0;
//		FI
//		dst[127:64] := a[127:64];
//		dst[MAX:128] := 0;
//
// Instruction: 'VRCP28SD'. Intrinsic: '_mm_maskz_rcp28_round_sd'.
// Requires AVX512ER.
func MaskzRcp28RoundSd(k Mmask8, a M128d, b M128d, rounding int) M128d {
	return M128d(maskzRcp28RoundSd(uint8(k), [2]float64(a), [2]float64(b), rounding))
}

func maskzRcp28RoundSd(k uint8, a [2]float64, b [2]float64, rounding int) [2]float64


// Rcp28RoundSd: Compute the approximate reciprocal of the lower
// double-precision (64-bit) floating-point element in 'b', store the result in
// the lower element of 'dst', and copy the upper element from 'a' to the upper
// element of 'dst'. The maximum relative error for this approximation is less
// than 2^-28. Rounding is done according to the 'rounding' parameter, which
// can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := RCP_28_DP(1.0/b[63:0];
//		dst[127:64] := a[127:64];
//		dst[MAX:128] := 0;
//
// Instruction: 'VRCP28SD'. Intrinsic: '_mm_rcp28_round_sd'.
// Requires AVX512ER.
func Rcp28RoundSd(a M128d, b M128d, rounding int) M128d {
	return M128d(rcp28RoundSd([2]float64(a), [2]float64(b), rounding))
}

func rcp28RoundSd(a [2]float64, b [2]float64, rounding int) [2]float64


// MaskRcp28RoundSs: Compute the approximate reciprocal of the lower
// single-precision (32-bit) floating-point element in 'b', store the result in
// the lower element of 'dst' using writemask 'k' (the element is copied from
// 'src' when mask bit 0 is not set), and copy the upper 3 packed elements from
// 'a' to the upper elements of 'dst'. The maximum relative error for this
// approximation is less than 2^-28. Rounding is done according to the
// 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0] THEN
//			dst[31:0] := RCP_28_DP(1.0/b[31:0];
//		ELSE
//			dst[31:0] := src[31:0];
//		FI
//		dst[127:32] := a[127:32];
//		dst[MAX:128] := 0;
//
// Instruction: 'VRCP28SS'. Intrinsic: '_mm_mask_rcp28_round_ss'.
// Requires AVX512ER.
func MaskRcp28RoundSs(src M128, k Mmask8, a M128, b M128, rounding int) M128 {
	return M128(maskRcp28RoundSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), rounding))
}

func maskRcp28RoundSs(src [4]float32, k uint8, a [4]float32, b [4]float32, rounding int) [4]float32


// MaskzRcp28RoundSs: Compute the approximate reciprocal of the lower
// single-precision (32-bit) floating-point element in 'b', store the result in
// the lower element of 'dst' using zeromask 'k' (the element is zeroed out
// when mask bit 0 is not set), and copy the upper 3 packed elements from 'a'
// to the upper elements of 'dst'. The maximum relative error for this
// approximation is less than 2^-28. Rounding is done according to the
// 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0] THEN
//			dst[31:0] := RCP_28_DP(1.0/b[31:0];
//		ELSE
//			dst[31:0] := 0;
//		FI
//		dst[127:32] := a[127:32];
//		dst[MAX:128] := 0;
//
// Instruction: 'VRCP28SS'. Intrinsic: '_mm_maskz_rcp28_round_ss'.
// Requires AVX512ER.
func MaskzRcp28RoundSs(k Mmask8, a M128, b M128, rounding int) M128 {
	return M128(maskzRcp28RoundSs(uint8(k), [4]float32(a), [4]float32(b), rounding))
}

func maskzRcp28RoundSs(k uint8, a [4]float32, b [4]float32, rounding int) [4]float32


// Rcp28RoundSs: Compute the approximate reciprocal of the lower
// single-precision (32-bit) floating-point element in 'b', store the result in
// the lower element of 'dst'. The maximum relative error for this
// approximation is less than 2^-28, and copy the upper 3 packed elements from
// 'a' to the upper elements of 'dst'. Rounding is done according to the
// 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := RCP_28_DP(1.0/b[31:0];
//		dst[127:32] := a[127:32];
//		dst[MAX:128] := 0;
//
// Instruction: 'VRCP28SS'. Intrinsic: '_mm_rcp28_round_ss'.
// Requires AVX512ER.
func Rcp28RoundSs(a M128, b M128, rounding int) M128 {
	return M128(rcp28RoundSs([4]float32(a), [4]float32(b), rounding))
}

func rcp28RoundSs(a [4]float32, b [4]float32, rounding int) [4]float32


// MaskRcp28Sd: Compute the approximate reciprocal of the lower
// double-precision (64-bit) floating-point element in 'b', store the result in
// the lower element of 'dst' using writemask 'k' (the element is copied from
// 'src' when mask bit 0 is not set), and copy the upper element from 'a' to
// the upper element of 'dst'. The maximum relative error for this
// approximation is less than 2^-28. 
//
//		IF k[0] THEN
//			dst[63:0] := RCP_28_DP(1.0/b[63:0];
//		ELSE
//			dst[63:0] := src[63:0];
//		FI
//		dst[127:64] := a[127:64];
//		dst[MAX:128] := 0;
//
// Instruction: 'VRCP28SD'. Intrinsic: '_mm_mask_rcp28_sd'.
// Requires AVX512ER.
func MaskRcp28Sd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskRcp28Sd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskRcp28Sd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzRcp28Sd: Compute the approximate reciprocal of the lower
// double-precision (64-bit) floating-point element in 'b', store the result in
// the lower element of 'dst' using zeromask 'k' (the element is zeroed out
// when mask bit 0 is not set), and copy the upper element from 'a' to the
// upper element of 'dst'. The maximum relative error for this approximation is
// less than 2^-28. 
//
//		IF k[0] THEN
//			dst[63:0] := RCP_28_DP(1.0/b[63:0];
//		ELSE
//			dst[63:0] := 0;
//		FI
//		dst[127:64] := a[127:64];
//		dst[MAX:128] := 0;
//
// Instruction: 'VRCP28SD'. Intrinsic: '_mm_maskz_rcp28_sd'.
// Requires AVX512ER.
func MaskzRcp28Sd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzRcp28Sd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzRcp28Sd(k uint8, a [2]float64, b [2]float64) [2]float64


// Rcp28Sd: Compute the approximate reciprocal of the lower double-precision
// (64-bit) floating-point element in 'b', store the result in the lower
// element of 'dst', and copy the upper element from 'a' to the upper element
// of 'dst'. The maximum relative error for this approximation is less than
// 2^-28. 
//
//		dst[63:0] := RCP_28_DP(1.0/b[63:0];
//		dst[127:64] := a[127:64];
//		dst[MAX:128] := 0;
//
// Instruction: 'VRCP28SD'. Intrinsic: '_mm_rcp28_sd'.
// Requires AVX512ER.
func Rcp28Sd(a M128d, b M128d) M128d {
	return M128d(rcp28Sd([2]float64(a), [2]float64(b)))
}

func rcp28Sd(a [2]float64, b [2]float64) [2]float64


// MaskRcp28Ss: Compute the approximate reciprocal of the lower
// single-precision (32-bit) floating-point element in 'b', store the result in
// the lower element of 'dst' using writemask 'k' (the element is copied from
// 'src' when mask bit 0 is not set), and copy the upper 3 packed elements from
// 'a' to the upper elements of 'dst'. The maximum relative error for this
// approximation is less than 2^-28. 
//
//		IF k[0] THEN
//			dst[31:0] := RCP_28_DP(1.0/b[31:0];
//		ELSE
//			dst[31:0] := src[31:0];
//		FI
//		dst[127:32] := a[127:32];
//		dst[MAX:128] := 0;
//
// Instruction: 'VRCP28SS'. Intrinsic: '_mm_mask_rcp28_ss'.
// Requires AVX512ER.
func MaskRcp28Ss(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskRcp28Ss([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskRcp28Ss(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzRcp28Ss: Compute the approximate reciprocal of the lower
// single-precision (32-bit) floating-point element in 'b', store the result in
// the lower element of 'dst' using zeromask 'k' (the element is zeroed out
// when mask bit 0 is not set), and copy the upper 3 packed elements from 'a'
// to the upper elements of 'dst'. The maximum relative error for this
// approximation is less than 2^-28. 
//
//		IF k[0] THEN
//			dst[31:0] := RCP_28_DP(1.0/b[31:0];
//		ELSE
//			dst[31:0] := 0;
//		FI
//		dst[127:32] := a[127:32];
//		dst[MAX:128] := 0;
//
// Instruction: 'VRCP28SS'. Intrinsic: '_mm_maskz_rcp28_ss'.
// Requires AVX512ER.
func MaskzRcp28Ss(k Mmask8, a M128, b M128) M128 {
	return M128(maskzRcp28Ss(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzRcp28Ss(k uint8, a [4]float32, b [4]float32) [4]float32


// Rcp28Ss: Compute the approximate reciprocal of the lower single-precision
// (32-bit) floating-point element in 'b', store the result in the lower
// element of 'dst', and copy the upper 3 packed elements from 'a' to the upper
// elements of 'dst'. The maximum relative error for this approximation is less
// than 2^-28. 
//
//		dst[31:0] := RCP_28_DP(1.0/b[31:0];
//		dst[127:32] := a[127:32];
//		dst[MAX:128] := 0;
//
// Instruction: 'VRCP28SS'. Intrinsic: '_mm_rcp28_ss'.
// Requires AVX512ER.
func Rcp28Ss(a M128, b M128) M128 {
	return M128(rcp28Ss([4]float32(a), [4]float32(b)))
}

func rcp28Ss(a [4]float32, b [4]float32) [4]float32


// MaskReducePd: Extract the reduced argument of packed double-precision
// (64-bit) floating-point elements in 'a' by the number of bits specified by
// 'imm8', and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		ReduceArgumentPD(src1[63:0], imm8[7:0])
//		{
//			m := imm8[7:4] // number of fraction bits after the binary point to be preserved
//			rc := imm8[1:0] // round control
//			rc_src := imm8[2] // round ccontrol source
//			spe := 0
//			tmp[63:0] := pow(2, -m) * ROUND(pow(2, m) * src1[63:0], spe, rc_source, rc)
//			tmp[63:0] := src1[63:0] - tmp[63:0]
//			RETURN tmp[63:0]
//		}
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ReduceArgumentPD(src[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VREDUCEPD'. Intrinsic: '_mm_mask_reduce_pd'.
// Requires AVX512DQ.
func MaskReducePd(src M128d, k Mmask8, a M128d, imm8 int) M128d {
	return M128d(maskReducePd([2]float64(src), uint8(k), [2]float64(a), imm8))
}

func maskReducePd(src [2]float64, k uint8, a [2]float64, imm8 int) [2]float64


// MaskzReducePd: Extract the reduced argument of packed double-precision
// (64-bit) floating-point elements in 'a' by the number of bits specified by
// 'imm8', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		ReduceArgumentPD(src1[63:0], imm8[7:0])
//		{
//			m := imm8[7:4] // number of fraction bits after the binary point to be preserved
//			rc := imm8[1:0] // round control
//			rc_src := imm8[2] // round ccontrol source
//			spe := 0
//			tmp[63:0] := pow(2, -m) * ROUND(pow(2, m) * src1[63:0], spe, rc_source, rc)
//			tmp[63:0] := src1[63:0] - tmp[63:0]
//			RETURN tmp[63:0]
//		}
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ReduceArgumentPD(src[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VREDUCEPD'. Intrinsic: '_mm_maskz_reduce_pd'.
// Requires AVX512DQ.
func MaskzReducePd(k Mmask8, a M128d, imm8 int) M128d {
	return M128d(maskzReducePd(uint8(k), [2]float64(a), imm8))
}

func maskzReducePd(k uint8, a [2]float64, imm8 int) [2]float64


// ReducePd: Extract the reduced argument of packed double-precision (64-bit)
// floating-point elements in 'a' by the number of bits specified by 'imm8',
// and store the results in 'dst'. 
//
//		ReduceArgumentPD(src1[63:0], imm8[7:0])
//		{
//			m := imm8[7:4] // number of fraction bits after the binary point to be preserved
//			rc := imm8[1:0] // round control
//			rc_src := imm8[2] // round ccontrol source
//			spe := 0
//			tmp[63:0] := pow(2, -m) * ROUND(pow(2, m) * src1[63:0], spe, rc_source, rc)
//			tmp[63:0] := src1[63:0] - tmp[63:0]
//			RETURN tmp[63:0]
//		}
//		
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := ReduceArgumentPD(src[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VREDUCEPD'. Intrinsic: '_mm_reduce_pd'.
// Requires AVX512DQ.
func ReducePd(a M128d, imm8 int) M128d {
	return M128d(reducePd([2]float64(a), imm8))
}

func reducePd(a [2]float64, imm8 int) [2]float64


// MaskReducePs: Extract the reduced argument of packed single-precision
// (32-bit) floating-point elements in 'a' by the number of bits specified by
// 'imm8', and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		ReduceArgumentPS(src1[31:0], imm8[7:0])
//		{
//			IF src1[31:0] == NAN
//				RETURN (convert src1[31:0] to QNaN)
//			FI
//			
//			m := imm8[7:4] // number of fraction bits after the binary point to be preserved
//			rc := imm8[1:0] // round control
//			rc_src := imm8[2] // round ccontrol source
//			spe := 0
//			tmp[31:0] := pow(2, -m)*ROUND(pow(2, m)*src1[31:0], spe, rc_source, rc)
//			tmp[31:0] := src1[31:0] - tmp[31:0]
//			RETURN tmp[31:0]
//		}
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ReduceArgumentPS(src[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VREDUCEPS'. Intrinsic: '_mm_mask_reduce_ps'.
// Requires AVX512DQ.
func MaskReducePs(src M128, k Mmask8, a M128, imm8 int) M128 {
	return M128(maskReducePs([4]float32(src), uint8(k), [4]float32(a), imm8))
}

func maskReducePs(src [4]float32, k uint8, a [4]float32, imm8 int) [4]float32


// MaskzReducePs: Extract the reduced argument of packed single-precision
// (32-bit) floating-point elements in 'a' by the number of bits specified by
// 'imm8', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		ReduceArgumentPS(src1[31:0], imm8[7:0])
//		{
//			IF src1[31:0] == NAN
//				RETURN (convert src1[31:0] to QNaN)
//			FI
//			
//			m := imm8[7:4] // number of fraction bits after the binary point to be preserved
//			rc := imm8[1:0] // round control
//			rc_src := imm8[2] // round ccontrol source
//			spe := 0
//			tmp[31:0] := pow(2, -m)*ROUND(pow(2, m)*src1[31:0], spe, rc_source, rc)
//			tmp[31:0] := src1[31:0] - tmp[31:0]
//			RETURN tmp[31:0]
//		}
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ReduceArgumentPS(src[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VREDUCEPS'. Intrinsic: '_mm_maskz_reduce_ps'.
// Requires AVX512DQ.
func MaskzReducePs(k Mmask8, a M128, imm8 int) M128 {
	return M128(maskzReducePs(uint8(k), [4]float32(a), imm8))
}

func maskzReducePs(k uint8, a [4]float32, imm8 int) [4]float32


// ReducePs: Extract the reduced argument of packed single-precision (32-bit)
// floating-point elements in 'a' by the number of bits specified by 'imm8',
// and store the results in 'dst'. 
//
//		ReduceArgumentPS(src1[31:0], imm8[7:0])
//		{
//			IF src1[31:0] == NAN
//				RETURN (convert src1[31:0] to QNaN)
//			FI
//			
//			m := imm8[7:4] // number of fraction bits after the binary point to be preserved
//			rc := imm8[1:0] // round control
//			rc_src := imm8[2] // round ccontrol source
//			spe := 0
//			tmp[31:0] := pow(2, -m)*ROUND(pow(2, m)*src1[31:0], spe, rc_source, rc)
//			tmp[31:0] := src1[31:0] - tmp[31:0]
//			RETURN tmp[31:0]
//		}
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := ReduceArgumentPS(src[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VREDUCEPS'. Intrinsic: '_mm_reduce_ps'.
// Requires AVX512DQ.
func ReducePs(a M128, imm8 int) M128 {
	return M128(reducePs([4]float32(a), imm8))
}

func reducePs(a [4]float32, imm8 int) [4]float32


// MaskReduceRoundSd: Extract the reduced argument of the lower
// double-precision (64-bit) floating-point element in 'a' by the number of
// bits specified by 'imm8', store the result in the lower element of 'dst'
// using writemask 'k' (the element is copied from 'src' when mask bit 0 is not
// set), and copy the upper element from 'b' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		ReduceArgumentPD(src1[63:0], imm8[7:0])
//		{
//			m := imm8[7:4] // number of fraction bits after the binary point to be preserved
//			rc := imm8[1:0] // round control
//			rc_src := imm8[2] // round ccontrol source
//			spe := 0
//			tmp[63:0] := pow(2, -m) * ROUND(pow(2, m) * src1[63:0], spe, rc_source, rc)
//			tmp[63:0] := src1[63:0] - tmp[63:0]
//			RETURN tmp[63:0]
//		}
//		
//		IF k[0]
//			dst[63:0] := ReduceArgumentPD(a[63:0], imm8[7:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VREDUCESD'. Intrinsic: '_mm_mask_reduce_round_sd'.
// Requires AVX512DQ.
func MaskReduceRoundSd(src M128d, k Mmask8, a M128d, b M128d, imm8 int, rounding int) M128d {
	return M128d(maskReduceRoundSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), imm8, rounding))
}

func maskReduceRoundSd(src [2]float64, k uint8, a [2]float64, b [2]float64, imm8 int, rounding int) [2]float64


// MaskzReduceRoundSd: Extract the reduced argument of the lower
// double-precision (64-bit) floating-point element in 'a' by the number of
// bits specified by 'imm8', store the result in the lower element of 'dst'
// using zeromask 'k' (the element is zeroed out when mask bit 0 is not set),
// and copy the upper element from 'b' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		ReduceArgumentPD(src1[63:0], imm8[7:0])
//		{
//			m := imm8[7:4] // number of fraction bits after the binary point to be preserved
//			rc := imm8[1:0] // round control
//			rc_src := imm8[2] // round ccontrol source
//			spe := 0
//			tmp[63:0] := pow(2, -m) * ROUND(pow(2, m) * src1[63:0], spe, rc_source, rc)
//			tmp[63:0] := src1[63:0] - tmp[63:0]
//			RETURN tmp[63:0]
//		}
//		
//		IF k[0]
//			dst[63:0] := ReduceArgumentPD(a[63:0], imm8[7:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VREDUCESD'. Intrinsic: '_mm_maskz_reduce_round_sd'.
// Requires AVX512DQ.
func MaskzReduceRoundSd(k Mmask8, a M128d, b M128d, imm8 int, rounding int) M128d {
	return M128d(maskzReduceRoundSd(uint8(k), [2]float64(a), [2]float64(b), imm8, rounding))
}

func maskzReduceRoundSd(k uint8, a [2]float64, b [2]float64, imm8 int, rounding int) [2]float64


// ReduceRoundSd: Extract the reduced argument of the lower double-precision
// (64-bit) floating-point element in 'a' by the number of bits specified by
// 'imm8', store the result in the lower element of 'dst', and copy the upper
// element from 'b' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		ReduceArgumentPD(src1[63:0], imm8[7:0])
//		{
//			m := imm8[7:4] // number of fraction bits after the binary point to be preserved
//			rc := imm8[1:0] // round control
//			rc_src := imm8[2] // round ccontrol source
//			spe := 0
//			tmp[63:0] := pow(2, -m) * ROUND(pow(2, m) * src1[63:0], spe, rc_source, rc)
//			tmp[63:0] := src1[63:0] - tmp[63:0]
//			RETURN tmp[63:0]
//		}
//		
//		dst[63:0] := ReduceArgumentPD(a[63:0], imm8[7:0])
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VREDUCESD'. Intrinsic: '_mm_reduce_round_sd'.
// Requires AVX512DQ.
func ReduceRoundSd(a M128d, b M128d, imm8 int, rounding int) M128d {
	return M128d(reduceRoundSd([2]float64(a), [2]float64(b), imm8, rounding))
}

func reduceRoundSd(a [2]float64, b [2]float64, imm8 int, rounding int) [2]float64


// MaskReduceRoundSs: Extract the reduced argument of the lower
// single-precision (32-bit) floating-point element in 'a' by the number of
// bits specified by 'imm8', store the result in the lower element of 'dst'
// using writemask 'k' (the element is copied from 'src' when mask bit 0 is not
// set), and copy the upper 3 packed elements from 'b' to the upper elements of
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		ReduceArgumentPS(src1[31:0], imm8[7:0])
//		{
//			IF src1[31:0] == NAN
//				RETURN (convert src1[31:0] to QNaN)
//			FI
//			
//			m := imm8[7:4] // number of fraction bits after the binary point to be preserved
//			rc := imm8[1:0] // round control
//			rc_src := imm8[2] // round ccontrol source
//			spe := 0
//			tmp[31:0] := pow(2, -m)*ROUND(pow(2, m)*src1[31:0], spe, rc_source, rc)
//			tmp[31:0] := src1[31:0] - tmp[31:0]
//			RETURN tmp[31:0]
//		}
//		
//		IF k[0]
//			dst[31:0] := ReduceArgumentPS(a[31:0], imm8[7:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:64] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VREDUCESS'. Intrinsic: '_mm_mask_reduce_round_ss'.
// Requires AVX512DQ.
func MaskReduceRoundSs(src M128, k Mmask8, a M128, b M128, imm8 int, rounding int) M128 {
	return M128(maskReduceRoundSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), imm8, rounding))
}

func maskReduceRoundSs(src [4]float32, k uint8, a [4]float32, b [4]float32, imm8 int, rounding int) [4]float32


// MaskzReduceRoundSs: Extract the reduced argument of the lower
// single-precision (32-bit) floating-point element in 'a' by the number of
// bits specified by 'imm8', store the result in the lower element of 'dst'
// using zeromask 'k' (the element is zeroed out when mask bit 0 is not set),
// and copy the upper 3 packed elements from 'b' to the upper elements of
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		ReduceArgumentPS(src1[31:0], imm8[7:0])
//		{
//			IF src1[31:0] == NAN
//				RETURN (convert src1[31:0] to QNaN)
//			FI
//			
//			m := imm8[7:4] // number of fraction bits after the binary point to be preserved
//			rc := imm8[1:0] // round control
//			rc_src := imm8[2] // round ccontrol source
//			spe := 0
//			tmp[31:0] := pow(2, -m)*ROUND(pow(2, m)*src1[31:0], spe, rc_source, rc)
//			tmp[31:0] := src1[31:0] - tmp[31:0]
//			RETURN tmp[31:0]
//		}
//		
//		IF k[0]
//			dst[31:0] := ReduceArgumentPS(a[31:0], imm8[7:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:64] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VREDUCESS'. Intrinsic: '_mm_maskz_reduce_round_ss'.
// Requires AVX512DQ.
func MaskzReduceRoundSs(k Mmask8, a M128, b M128, imm8 int, rounding int) M128 {
	return M128(maskzReduceRoundSs(uint8(k), [4]float32(a), [4]float32(b), imm8, rounding))
}

func maskzReduceRoundSs(k uint8, a [4]float32, b [4]float32, imm8 int, rounding int) [4]float32


// ReduceRoundSs: Extract the reduced argument of the lower single-precision
// (32-bit) floating-point element in 'a' by the number of bits specified by
// 'imm8', store the result in the lower element of 'dst', and copy the upper 3
// packed elements from 'b' to the upper elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		ReduceArgumentPS(src1[31:0], imm8[7:0])
//		{
//			IF src1[31:0] == NAN
//				RETURN (convert src1[31:0] to QNaN)
//			FI
//			
//			m := imm8[7:4] // number of fraction bits after the binary point to be preserved
//			rc := imm8[1:0] // round control
//			rc_src := imm8[2] // round ccontrol source
//			spe := 0
//			tmp[31:0] := pow(2, -m)*ROUND(pow(2, m)*src1[31:0], spe, rc_source, rc)
//			tmp[31:0] := src1[31:0] - tmp[31:0]
//			RETURN tmp[31:0]
//		}
//		
//		dst[31:0] := ReduceArgumentPS(a[31:0], imm8[7:0])
//		dst[127:64] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VREDUCESS'. Intrinsic: '_mm_reduce_round_ss'.
// Requires AVX512DQ.
func ReduceRoundSs(a M128, b M128, imm8 int, rounding int) M128 {
	return M128(reduceRoundSs([4]float32(a), [4]float32(b), imm8, rounding))
}

func reduceRoundSs(a [4]float32, b [4]float32, imm8 int, rounding int) [4]float32


// MaskReduceSd: Extract the reduced argument of the lower double-precision
// (64-bit) floating-point element in 'a' by the number of bits specified by
// 'imm8', store the result in the lower element of 'dst' using writemask 'k'
// (the element is copied from 'src' when mask bit 0 is not set), and copy the
// upper element from 'b' to the upper element of 'dst'. 
//
//		ReduceArgumentPD(src1[63:0], imm8[7:0])
//		{
//			m := imm8[7:4] // number of fraction bits after the binary point to be preserved
//			rc := imm8[1:0] // round control
//			rc_src := imm8[2] // round ccontrol source
//			spe := 0
//			tmp[63:0] := pow(2, -m) * ROUND(pow(2, m) * src1[63:0], spe, rc_source, rc)
//			tmp[63:0] := src1[63:0] - tmp[63:0]
//			RETURN tmp[63:0]
//		}
//		
//		IF k[0]
//			dst[63:0] := ReduceArgumentPD(a[63:0], imm8[7:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VREDUCESD'. Intrinsic: '_mm_mask_reduce_sd'.
// Requires AVX512DQ.
func MaskReduceSd(src M128d, k Mmask8, a M128d, b M128d, imm8 int) M128d {
	return M128d(maskReduceSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), imm8))
}

func maskReduceSd(src [2]float64, k uint8, a [2]float64, b [2]float64, imm8 int) [2]float64


// MaskzReduceSd: Extract the reduced argument of the lower double-precision
// (64-bit) floating-point element in 'a' by the number of bits specified by
// 'imm8', store the result in the lower element of 'dst' using zeromask 'k'
// (the element is zeroed out when mask bit 0 is not set), and copy the upper
// element from 'b' to the upper element of 'dst'. 
//
//		ReduceArgumentPD(src1[63:0], imm8[7:0])
//		{
//			m := imm8[7:4] // number of fraction bits after the binary point to be preserved
//			rc := imm8[1:0] // round control
//			rc_src := imm8[2] // round ccontrol source
//			spe := 0
//			tmp[63:0] := pow(2, -m) * ROUND(pow(2, m) * src1[63:0], spe, rc_source, rc)
//			tmp[63:0] := src1[63:0] - tmp[63:0]
//			RETURN tmp[63:0]
//		}
//		
//		IF k[0]
//			dst[63:0] := ReduceArgumentPD(a[63:0], imm8[7:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VREDUCESD'. Intrinsic: '_mm_maskz_reduce_sd'.
// Requires AVX512DQ.
func MaskzReduceSd(k Mmask8, a M128d, b M128d, imm8 int) M128d {
	return M128d(maskzReduceSd(uint8(k), [2]float64(a), [2]float64(b), imm8))
}

func maskzReduceSd(k uint8, a [2]float64, b [2]float64, imm8 int) [2]float64


// ReduceSd: Extract the reduced argument of the lower double-precision
// (64-bit) floating-point element in 'a' by the number of bits specified by
// 'imm8', store the result in the lower element of 'dst', and copy the upper
// element from 'b' to the upper element of 'dst'. 
//
//		ReduceArgumentPD(src1[63:0], imm8[7:0])
//		{
//			m := imm8[7:4] // number of fraction bits after the binary point to be preserved
//			rc := imm8[1:0] // round control
//			rc_src := imm8[2] // round ccontrol source
//			spe := 0
//			tmp[63:0] := pow(2, -m) * ROUND(pow(2, m) * src1[63:0], spe, rc_source, rc)
//			tmp[63:0] := src1[63:0] - tmp[63:0]
//			RETURN tmp[63:0]
//		}
//		
//		dst[63:0] := ReduceArgumentPD(a[63:0], imm8[7:0])
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VREDUCESD'. Intrinsic: '_mm_reduce_sd'.
// Requires AVX512DQ.
func ReduceSd(a M128d, b M128d, imm8 int) M128d {
	return M128d(reduceSd([2]float64(a), [2]float64(b), imm8))
}

func reduceSd(a [2]float64, b [2]float64, imm8 int) [2]float64


// MaskReduceSs: Extract the reduced argument of the lower single-precision
// (32-bit) floating-point element in 'a' by the number of bits specified by
// 'imm8', store the result in the lower element of 'dst' using writemask 'k'
// (the element is copied from 'src' when mask bit 0 is not set), and copy the
// upper 3 packed elements from 'b' to the upper elements of 'dst'. 
//
//		ReduceArgumentPS(src1[31:0], imm8[7:0])
//		{
//			IF src1[31:0] == NAN
//				RETURN (convert src1[31:0] to QNaN)
//			FI
//			
//			m := imm8[7:4] // number of fraction bits after the binary point to be preserved
//			rc := imm8[1:0] // round control
//			rc_src := imm8[2] // round ccontrol source
//			spe := 0
//			tmp[31:0] := pow(2, -m)*ROUND(pow(2, m)*src1[31:0], spe, rc_source, rc)
//			tmp[31:0] := src1[31:0] - tmp[31:0]
//			RETURN tmp[31:0]
//		}
//		
//		IF k[0]
//			dst[31:0] := ReduceArgumentPS(a[31:0], imm8[7:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:64] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VREDUCESS'. Intrinsic: '_mm_mask_reduce_ss'.
// Requires AVX512DQ.
func MaskReduceSs(src M128, k Mmask8, a M128, b M128, imm8 int) M128 {
	return M128(maskReduceSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), imm8))
}

func maskReduceSs(src [4]float32, k uint8, a [4]float32, b [4]float32, imm8 int) [4]float32


// MaskzReduceSs: Extract the reduced argument of the lower single-precision
// (32-bit) floating-point element in 'a' by the number of bits specified by
// 'imm8', store the result in the lower element of 'dst' using zeromask 'k'
// (the element is zeroed out when mask bit 0 is not set), and copy the upper 3
// packed elements from 'b' to the upper elements of 'dst'. 
//
//		ReduceArgumentPS(src1[31:0], imm8[7:0])
//		{
//			IF src1[31:0] == NAN
//				RETURN (convert src1[31:0] to QNaN)
//			FI
//			
//			m := imm8[7:4] // number of fraction bits after the binary point to be preserved
//			rc := imm8[1:0] // round control
//			rc_src := imm8[2] // round ccontrol source
//			spe := 0
//			tmp[31:0] := pow(2, -m)*ROUND(pow(2, m)*src1[31:0], spe, rc_source, rc)
//			tmp[31:0] := src1[31:0] - tmp[31:0]
//			RETURN tmp[31:0]
//		}
//		
//		IF k[0]
//			dst[31:0] := ReduceArgumentPS(a[31:0], imm8[7:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:64] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VREDUCESS'. Intrinsic: '_mm_maskz_reduce_ss'.
// Requires AVX512DQ.
func MaskzReduceSs(k Mmask8, a M128, b M128, imm8 int) M128 {
	return M128(maskzReduceSs(uint8(k), [4]float32(a), [4]float32(b), imm8))
}

func maskzReduceSs(k uint8, a [4]float32, b [4]float32, imm8 int) [4]float32


// ReduceSs: Extract the reduced argument of the lower single-precision
// (32-bit) floating-point element in 'a' by the number of bits specified by
// 'imm8', store the result in the lower element of 'dst', and copy the upper 3
// packed elements from 'b' to the upper elements of 'dst'. 
//
//		ReduceArgumentPS(src1[31:0], imm8[7:0])
//		{
//			IF src1[31:0] == NAN
//				RETURN (convert src1[31:0] to QNaN)
//			FI
//			
//			m := imm8[7:4] // number of fraction bits after the binary point to be preserved
//			rc := imm8[1:0] // round control
//			rc_src := imm8[2] // round ccontrol source
//			spe := 0
//			tmp[31:0] := pow(2, -m)*ROUND(pow(2, m)*src1[31:0], spe, rc_source, rc)
//			tmp[31:0] := src1[31:0] - tmp[31:0]
//			RETURN tmp[31:0]
//		}
//		
//		dst[31:0] := ReduceArgumentPS(a[31:0], imm8[7:0])
//		dst[127:64] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VREDUCESS'. Intrinsic: '_mm_reduce_ss'.
// Requires AVX512DQ.
func ReduceSs(a M128, b M128, imm8 int) M128 {
	return M128(reduceSs([4]float32(a), [4]float32(b), imm8))
}

func reduceSs(a [4]float32, b [4]float32, imm8 int) [4]float32


// Rem16: Divide packed 16-bit integers in 'a' by packed elements in 'b', and
// store the remainders as packed 32-bit integers in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 16*j
//			dst[i+15:i] := REMAINDER(a[i+15:i] / b[i+15:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_rem_epi16'.
// Requires SSE.
func Rem16(a M128i, b M128i) M128i {
	return M128i(rem16([16]byte(a), [16]byte(b)))
}

func rem16(a [16]byte, b [16]byte) [16]byte


// Rem32: Divide packed 32-bit integers in 'a' by packed elements in 'b', and
// store the remainders as packed 32-bit integers in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			dst[i+31:i] := REMAINDER(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_rem_epi32'.
// Requires SSE.
func Rem32(a M128i, b M128i) M128i {
	return M128i(rem32([16]byte(a), [16]byte(b)))
}

func rem32(a [16]byte, b [16]byte) [16]byte


// Rem64: Divide packed 64-bit integers in 'a' by packed elements in 'b', and
// store the remainders as packed 32-bit integers in 'dst'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			dst[i+63:i] := REMAINDER(a[i+63:i] / b[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_rem_epi64'.
// Requires SSE.
func Rem64(a M128i, b M128i) M128i {
	return M128i(rem64([16]byte(a), [16]byte(b)))
}

func rem64(a [16]byte, b [16]byte) [16]byte


// Rem8: Divide packed 8-bit integers in 'a' by packed elements in 'b', and
// store the remainders as packed 32-bit integers in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 8*j
//			dst[i+7:i] := REMAINDER(a[i+7:i] / b[i+7:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_rem_epi8'.
// Requires SSE.
func Rem8(a M128i, b M128i) M128i {
	return M128i(rem8([16]byte(a), [16]byte(b)))
}

func rem8(a [16]byte, b [16]byte) [16]byte


// RemEpu16: Divide packed unsigned 16-bit integers in 'a' by packed elements
// in 'b', and store the remainders as packed unsigned 32-bit integers in
// 'dst'. 
//
//		FOR j := 0 to 7
//			i := 16*j
//			dst[i+15:i] := REMAINDER(a[i+15:i] / b[i+15:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_rem_epu16'.
// Requires SSE.
func RemEpu16(a M128i, b M128i) M128i {
	return M128i(remEpu16([16]byte(a), [16]byte(b)))
}

func remEpu16(a [16]byte, b [16]byte) [16]byte


// RemEpu32: Divide packed unsigned 32-bit integers in 'a' by packed elements
// in 'b', and store the remainders as packed unsigned 32-bit integers in
// 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			dst[i+31:i] := REMAINDER(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_rem_epu32'.
// Requires SSE.
func RemEpu32(a M128i, b M128i) M128i {
	return M128i(remEpu32([16]byte(a), [16]byte(b)))
}

func remEpu32(a [16]byte, b [16]byte) [16]byte


// RemEpu64: Divide packed unsigned 64-bit integers in 'a' by packed elements
// in 'b', and store the remainders as packed unsigned 32-bit integers in
// 'dst'. 
//
//		FOR j := 0 to 1
//			i := 64*j
//			dst[i+63:i] := REMAINDER(a[i+63:i] / b[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_rem_epu64'.
// Requires SSE.
func RemEpu64(a M128i, b M128i) M128i {
	return M128i(remEpu64([16]byte(a), [16]byte(b)))
}

func remEpu64(a [16]byte, b [16]byte) [16]byte


// RemEpu8: Divide packed unsigned 8-bit integers in 'a' by packed elements in
// 'b', and store the remainders as packed unsigned 32-bit integers in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 8*j
//			dst[i+7:i] := REMAINDER(a[i+7:i] / b[i+7:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_rem_epu8'.
// Requires SSE.
func RemEpu8(a M128i, b M128i) M128i {
	return M128i(remEpu8([16]byte(a), [16]byte(b)))
}

func remEpu8(a [16]byte, b [16]byte) [16]byte


// MaskRol32: Rotate the bits in each packed 32-bit integer in 'a' to the left
// by the number of bits specified in 'imm8', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPROLD'. Intrinsic: '_mm_mask_rol_epi32'.
// Requires AVX512F.
func MaskRol32(src M128i, k Mmask8, a M128i, imm8 int) M128i {
	return M128i(maskRol32([16]byte(src), uint8(k), [16]byte(a), imm8))
}

func maskRol32(src [16]byte, k uint8, a [16]byte, imm8 int) [16]byte


// MaskzRol32: Rotate the bits in each packed 32-bit integer in 'a' to the left
// by the number of bits specified in 'imm8', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPROLD'. Intrinsic: '_mm_maskz_rol_epi32'.
// Requires AVX512F.
func MaskzRol32(k Mmask8, a M128i, imm8 int) M128i {
	return M128i(maskzRol32(uint8(k), [16]byte(a), imm8))
}

func maskzRol32(k uint8, a [16]byte, imm8 int) [16]byte


// Rol32: Rotate the bits in each packed 32-bit integer in 'a' to the left by
// the number of bits specified in 'imm8', and store the results in 'dst'. 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPROLD'. Intrinsic: '_mm_rol_epi32'.
// Requires AVX512F.
func Rol32(a M128i, imm8 int) M128i {
	return M128i(rol32([16]byte(a), imm8))
}

func rol32(a [16]byte, imm8 int) [16]byte


// MaskRol64: Rotate the bits in each packed 64-bit integer in 'a' to the left
// by the number of bits specified in 'imm8', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPROLQ'. Intrinsic: '_mm_mask_rol_epi64'.
// Requires AVX512F.
func MaskRol64(src M128i, k Mmask8, a M128i, imm8 int) M128i {
	return M128i(maskRol64([16]byte(src), uint8(k), [16]byte(a), imm8))
}

func maskRol64(src [16]byte, k uint8, a [16]byte, imm8 int) [16]byte


// MaskzRol64: Rotate the bits in each packed 64-bit integer in 'a' to the left
// by the number of bits specified in 'imm8', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPROLQ'. Intrinsic: '_mm_maskz_rol_epi64'.
// Requires AVX512F.
func MaskzRol64(k Mmask8, a M128i, imm8 int) M128i {
	return M128i(maskzRol64(uint8(k), [16]byte(a), imm8))
}

func maskzRol64(k uint8, a [16]byte, imm8 int) [16]byte


// Rol64: Rotate the bits in each packed 64-bit integer in 'a' to the left by
// the number of bits specified in 'imm8', and store the results in 'dst'. 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPROLQ'. Intrinsic: '_mm_rol_epi64'.
// Requires AVX512F.
func Rol64(a M128i, imm8 int) M128i {
	return M128i(rol64([16]byte(a), imm8))
}

func rol64(a [16]byte, imm8 int) [16]byte


// MaskRolv32: Rotate the bits in each packed 32-bit integer in 'a' to the left
// by the number of bits specified in the corresponding element of 'b', and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPROLVD'. Intrinsic: '_mm_mask_rolv_epi32'.
// Requires AVX512F.
func MaskRolv32(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskRolv32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskRolv32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzRolv32: Rotate the bits in each packed 32-bit integer in 'a' to the
// left by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPROLVD'. Intrinsic: '_mm_maskz_rolv_epi32'.
// Requires AVX512F.
func MaskzRolv32(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzRolv32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzRolv32(k uint8, a [16]byte, b [16]byte) [16]byte


// Rolv32: Rotate the bits in each packed 32-bit integer in 'a' to the left by
// the number of bits specified in the corresponding element of 'b', and store
// the results in 'dst'. 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPROLVD'. Intrinsic: '_mm_rolv_epi32'.
// Requires AVX512F.
func Rolv32(a M128i, b M128i) M128i {
	return M128i(rolv32([16]byte(a), [16]byte(b)))
}

func rolv32(a [16]byte, b [16]byte) [16]byte


// MaskRolv64: Rotate the bits in each packed 64-bit integer in 'a' to the left
// by the number of bits specified in the corresponding element of 'b', and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPROLVQ'. Intrinsic: '_mm_mask_rolv_epi64'.
// Requires AVX512F.
func MaskRolv64(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskRolv64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskRolv64(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzRolv64: Rotate the bits in each packed 64-bit integer in 'a' to the
// left by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPROLVQ'. Intrinsic: '_mm_maskz_rolv_epi64'.
// Requires AVX512F.
func MaskzRolv64(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzRolv64(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzRolv64(k uint8, a [16]byte, b [16]byte) [16]byte


// Rolv64: Rotate the bits in each packed 64-bit integer in 'a' to the left by
// the number of bits specified in the corresponding element of 'b', and store
// the results in 'dst'. 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPROLVQ'. Intrinsic: '_mm_rolv_epi64'.
// Requires AVX512F.
func Rolv64(a M128i, b M128i) M128i {
	return M128i(rolv64([16]byte(a), [16]byte(b)))
}

func rolv64(a [16]byte, b [16]byte) [16]byte


// MaskRor32: Rotate the bits in each packed 32-bit integer in 'a' to the right
// by the number of bits specified in 'imm8', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPRORD'. Intrinsic: '_mm_mask_ror_epi32'.
// Requires AVX512F.
func MaskRor32(src M128i, k Mmask8, a M128i, imm8 int) M128i {
	return M128i(maskRor32([16]byte(src), uint8(k), [16]byte(a), imm8))
}

func maskRor32(src [16]byte, k uint8, a [16]byte, imm8 int) [16]byte


// MaskzRor32: Rotate the bits in each packed 32-bit integer in 'a' to the
// right by the number of bits specified in 'imm8', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPRORD'. Intrinsic: '_mm_maskz_ror_epi32'.
// Requires AVX512F.
func MaskzRor32(k Mmask8, a M128i, imm8 int) M128i {
	return M128i(maskzRor32(uint8(k), [16]byte(a), imm8))
}

func maskzRor32(k uint8, a [16]byte, imm8 int) [16]byte


// Ror32: Rotate the bits in each packed 32-bit integer in 'a' to the right by
// the number of bits specified in 'imm8', and store the results in 'dst'. 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPRORD'. Intrinsic: '_mm_ror_epi32'.
// Requires AVX512F.
func Ror32(a M128i, imm8 int) M128i {
	return M128i(ror32([16]byte(a), imm8))
}

func ror32(a [16]byte, imm8 int) [16]byte


// MaskRor64: Rotate the bits in each packed 64-bit integer in 'a' to the right
// by the number of bits specified in 'imm8', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPRORQ'. Intrinsic: '_mm_mask_ror_epi64'.
// Requires AVX512F.
func MaskRor64(src M128i, k Mmask8, a M128i, imm8 int) M128i {
	return M128i(maskRor64([16]byte(src), uint8(k), [16]byte(a), imm8))
}

func maskRor64(src [16]byte, k uint8, a [16]byte, imm8 int) [16]byte


// MaskzRor64: Rotate the bits in each packed 64-bit integer in 'a' to the
// right by the number of bits specified in 'imm8', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPRORQ'. Intrinsic: '_mm_maskz_ror_epi64'.
// Requires AVX512F.
func MaskzRor64(k Mmask8, a M128i, imm8 int) M128i {
	return M128i(maskzRor64(uint8(k), [16]byte(a), imm8))
}

func maskzRor64(k uint8, a [16]byte, imm8 int) [16]byte


// Ror64: Rotate the bits in each packed 64-bit integer in 'a' to the right by
// the number of bits specified in 'imm8', and store the results in 'dst'. 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPRORQ'. Intrinsic: '_mm_ror_epi64'.
// Requires AVX512F.
func Ror64(a M128i, imm8 int) M128i {
	return M128i(ror64([16]byte(a), imm8))
}

func ror64(a [16]byte, imm8 int) [16]byte


// MaskRorv32: Rotate the bits in each packed 32-bit integer in 'a' to the
// right by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPRORVD'. Intrinsic: '_mm_mask_rorv_epi32'.
// Requires AVX512F.
func MaskRorv32(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskRorv32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskRorv32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzRorv32: Rotate the bits in each packed 32-bit integer in 'a' to the
// right by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPRORVD'. Intrinsic: '_mm_maskz_rorv_epi32'.
// Requires AVX512F.
func MaskzRorv32(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzRorv32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzRorv32(k uint8, a [16]byte, b [16]byte) [16]byte


// Rorv32: Rotate the bits in each packed 32-bit integer in 'a' to the right by
// the number of bits specified in the corresponding element of 'b', and store
// the results in 'dst'. 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPRORVD'. Intrinsic: '_mm_rorv_epi32'.
// Requires AVX512F.
func Rorv32(a M128i, b M128i) M128i {
	return M128i(rorv32([16]byte(a), [16]byte(b)))
}

func rorv32(a [16]byte, b [16]byte) [16]byte


// MaskRorv64: Rotate the bits in each packed 64-bit integer in 'a' to the
// right by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPRORVQ'. Intrinsic: '_mm_mask_rorv_epi64'.
// Requires AVX512F.
func MaskRorv64(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskRorv64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskRorv64(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzRorv64: Rotate the bits in each packed 64-bit integer in 'a' to the
// right by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPRORVQ'. Intrinsic: '_mm_maskz_rorv_epi64'.
// Requires AVX512F.
func MaskzRorv64(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzRorv64(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzRorv64(k uint8, a [16]byte, b [16]byte) [16]byte


// Rorv64: Rotate the bits in each packed 64-bit integer in 'a' to the right by
// the number of bits specified in the corresponding element of 'b', and store
// the results in 'dst'. 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPRORVQ'. Intrinsic: '_mm_rorv_epi64'.
// Requires AVX512F.
func Rorv64(a M128i, b M128i) M128i {
	return M128i(rorv64([16]byte(a), [16]byte(b)))
}

func rorv64(a [16]byte, b [16]byte) [16]byte


// RoundPd: Round the packed double-precision (64-bit) floating-point elements
// in 'a' using the 'rounding' parameter, and store the results as packed
// double-precision floating-point elements in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := ROUND(a[i+63:i])
//		ENDFOR
//
// Instruction: 'ROUNDPD'. Intrinsic: '_mm_round_pd'.
// Requires SSE4.1.
func RoundPd(a M128d, rounding int) M128d {
	return M128d(roundPd([2]float64(a), rounding))
}

func roundPd(a [2]float64, rounding int) [2]float64


// RoundPs: Round the packed single-precision (32-bit) floating-point elements
// in 'a' using the 'rounding' parameter, and store the results as packed
// single-precision floating-point elements in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := ROUND(a[i+31:i])
//		ENDFOR
//
// Instruction: 'ROUNDPS'. Intrinsic: '_mm_round_ps'.
// Requires SSE4.1.
func RoundPs(a M128, rounding int) M128 {
	return M128(roundPs([4]float32(a), rounding))
}

func roundPs(a [4]float32, rounding int) [4]float32


// RoundSd: Round the lower double-precision (64-bit) floating-point element in
// 'b' using the 'rounding' parameter, store the result as a double-precision
// floating-point element in the lower element of 'dst', and copy the upper
// element from 'a' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := ROUND(b[63:0])
//		dst[127:64] := a[127:64]
//
// Instruction: 'ROUNDSD'. Intrinsic: '_mm_round_sd'.
// Requires SSE4.1.
func RoundSd(a M128d, b M128d, rounding int) M128d {
	return M128d(roundSd([2]float64(a), [2]float64(b), rounding))
}

func roundSd(a [2]float64, b [2]float64, rounding int) [2]float64


// RoundSs: Round the lower single-precision (32-bit) floating-point element in
// 'b' using the 'rounding' parameter, store the result as a single-precision
// floating-point element in the lower element of 'dst', and copy the upper 3
// packed elements from 'a' to the upper elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := ROUND(b[31:0])
//		dst[127:32] := a[127:32]
//
// Instruction: 'ROUNDSS'. Intrinsic: '_mm_round_ss'.
// Requires SSE4.1.
func RoundSs(a M128, b M128, rounding int) M128 {
	return M128(roundSs([4]float32(a), [4]float32(b), rounding))
}

func roundSs(a [4]float32, b [4]float32, rounding int) [4]float32


// MaskRoundscalePd: Round packed double-precision (64-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}	
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALEPD'. Intrinsic: '_mm_mask_roundscale_pd'.
// Requires AVX512F.
func MaskRoundscalePd(src M128d, k Mmask8, a M128d, imm8 int) M128d {
	return M128d(maskRoundscalePd([2]float64(src), uint8(k), [2]float64(a), imm8))
}

func maskRoundscalePd(src [2]float64, k uint8, a [2]float64, imm8 int) [2]float64


// MaskzRoundscalePd: Round packed double-precision (64-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}	
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALEPD'. Intrinsic: '_mm_maskz_roundscale_pd'.
// Requires AVX512F.
func MaskzRoundscalePd(k Mmask8, a M128d, imm8 int) M128d {
	return M128d(maskzRoundscalePd(uint8(k), [2]float64(a), imm8))
}

func maskzRoundscalePd(k uint8, a [2]float64, imm8 int) [2]float64


// RoundscalePd: Round packed double-precision (64-bit) floating-point elements
// in 'a' to the number of fraction bits specified by 'imm8', and store the
// results in 'dst'. 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}	
//		
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALEPD'. Intrinsic: '_mm_roundscale_pd'.
// Requires AVX512F.
func RoundscalePd(a M128d, imm8 int) M128d {
	return M128d(roundscalePd([2]float64(a), imm8))
}

func roundscalePd(a [2]float64, imm8 int) [2]float64


// MaskRoundscalePs: Round packed single-precision (32-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}	
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALEPS'. Intrinsic: '_mm_mask_roundscale_ps'.
// Requires AVX512F.
func MaskRoundscalePs(src M128, k Mmask8, a M128, imm8 int) M128 {
	return M128(maskRoundscalePs([4]float32(src), uint8(k), [4]float32(a), imm8))
}

func maskRoundscalePs(src [4]float32, k uint8, a [4]float32, imm8 int) [4]float32


// MaskzRoundscalePs: Round packed single-precision (32-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}	
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALEPS'. Intrinsic: '_mm_maskz_roundscale_ps'.
// Requires AVX512F.
func MaskzRoundscalePs(k Mmask8, a M128, imm8 int) M128 {
	return M128(maskzRoundscalePs(uint8(k), [4]float32(a), imm8))
}

func maskzRoundscalePs(k uint8, a [4]float32, imm8 int) [4]float32


// RoundscalePs: Round packed single-precision (32-bit) floating-point elements
// in 'a' to the number of fraction bits specified by 'imm8', and store the
// results in 'dst'. 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}	
//		
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALEPS'. Intrinsic: '_mm_roundscale_ps'.
// Requires AVX512F.
func RoundscalePs(a M128, imm8 int) M128 {
	return M128(roundscalePs([4]float32(a), imm8))
}

func roundscalePs(a [4]float32, imm8 int) [4]float32


// MaskRoundscaleRoundSd: Round the lower double-precision (64-bit)
// floating-point element in 'a' to the number of fraction bits specified by
// 'imm8', store the result in the lower element of 'dst' using writemask 'k'
// (the element is copied from 'src' when mask bit 0 is not set), and copy the
// upper element from 'b' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}		
//		
//		IF k[0]
//			dst[63:0] := RoundTo_IntegerPD(a[63:0], imm8[7:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALESD'. Intrinsic: '_mm_mask_roundscale_round_sd'.
// Requires AVX512F.
func MaskRoundscaleRoundSd(src M128d, k Mmask8, a M128d, b M128d, imm8 int, rounding int) M128d {
	return M128d(maskRoundscaleRoundSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), imm8, rounding))
}

func maskRoundscaleRoundSd(src [2]float64, k uint8, a [2]float64, b [2]float64, imm8 int, rounding int) [2]float64


// MaskzRoundscaleRoundSd: Round the lower double-precision (64-bit)
// floating-point element in 'a' to the number of fraction bits specified by
// 'imm8', store the result in the lower element of 'dst' using zeromask 'k'
// (the element is zeroed out when mask bit 0 is not set), and copy the upper
// element from 'b' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}		
//		
//		IF k[0]
//			dst[63:0] := RoundTo_IntegerPD(a[63:0], imm8[7:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALESD'. Intrinsic: '_mm_maskz_roundscale_round_sd'.
// Requires AVX512F.
func MaskzRoundscaleRoundSd(k Mmask8, a M128d, b M128d, imm8 int, rounding int) M128d {
	return M128d(maskzRoundscaleRoundSd(uint8(k), [2]float64(a), [2]float64(b), imm8, rounding))
}

func maskzRoundscaleRoundSd(k uint8, a [2]float64, b [2]float64, imm8 int, rounding int) [2]float64


// RoundscaleRoundSd: Round the lower double-precision (64-bit) floating-point
// element in 'a' to the number of fraction bits specified by 'imm8', store the
// result in the lower element of 'dst', and copy the upper element from 'b' to
// the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}		
//		
//		dst[63:0] := RoundTo_IntegerPD(a[63:0], imm8[7:0])
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALESD'. Intrinsic: '_mm_roundscale_round_sd'.
// Requires AVX512F.
func RoundscaleRoundSd(a M128d, b M128d, imm8 int, rounding int) M128d {
	return M128d(roundscaleRoundSd([2]float64(a), [2]float64(b), imm8, rounding))
}

func roundscaleRoundSd(a [2]float64, b [2]float64, imm8 int, rounding int) [2]float64


// MaskRoundscaleRoundSs: Round the lower single-precision (32-bit)
// floating-point element in 'a' to the number of fraction bits specified by
// 'imm8', store the result in the lower element of 'dst' using writemask 'k'
// (the element is copied from 'src' when mask bit 0 is not set), and copy the
// upper 3 packed elements from 'b' to the upper elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}
//		
//		IF k[0]
//			dst[31:0] := RoundTo_IntegerPS(a[31:0], imm8[7:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALESS'. Intrinsic: '_mm_mask_roundscale_round_ss'.
// Requires AVX512F.
func MaskRoundscaleRoundSs(src M128, k Mmask8, a M128, b M128, imm8 int, rounding int) M128 {
	return M128(maskRoundscaleRoundSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), imm8, rounding))
}

func maskRoundscaleRoundSs(src [4]float32, k uint8, a [4]float32, b [4]float32, imm8 int, rounding int) [4]float32


// MaskzRoundscaleRoundSs: Round the lower single-precision (32-bit)
// floating-point element in 'a' to the number of fraction bits specified by
// 'imm8', store the result in the lower element of 'dst' using zeromask 'k'
// (the element is zeroed out when mask bit 0 is not set), and copy the upper 3
// packed elements from 'b' to the upper elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}
//		
//		IF k[0]
//			dst[31:0] := RoundTo_IntegerPS(a[31:0], imm8[7:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALESS'. Intrinsic: '_mm_maskz_roundscale_round_ss'.
// Requires AVX512F.
func MaskzRoundscaleRoundSs(k Mmask8, a M128, b M128, imm8 int, rounding int) M128 {
	return M128(maskzRoundscaleRoundSs(uint8(k), [4]float32(a), [4]float32(b), imm8, rounding))
}

func maskzRoundscaleRoundSs(k uint8, a [4]float32, b [4]float32, imm8 int, rounding int) [4]float32


// RoundscaleRoundSs: Round the lower single-precision (32-bit) floating-point
// element in 'a' to the number of fraction bits specified by 'imm8', store the
// result in the lower element of 'dst', and copy the upper 3 packed elements
// from 'b' to the upper elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}
//		
//		dst[31:0] := RoundTo_IntegerPS(a[31:0], imm8[7:0])
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALESS'. Intrinsic: '_mm_roundscale_round_ss'.
// Requires AVX512F.
func RoundscaleRoundSs(a M128, b M128, imm8 int, rounding int) M128 {
	return M128(roundscaleRoundSs([4]float32(a), [4]float32(b), imm8, rounding))
}

func roundscaleRoundSs(a [4]float32, b [4]float32, imm8 int, rounding int) [4]float32


// MaskRoundscaleSd: Round the lower double-precision (64-bit) floating-point
// element in 'a' to the number of fraction bits specified by 'imm8', store the
// result in the lower element of 'dst' using writemask 'k' (the element is
// copied from 'src' when mask bit 0 is not set), and copy the upper element
// from 'b' to the upper element of 'dst'. 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}		
//		
//		IF k[0]
//			dst[63:0] := RoundTo_IntegerPD(a[63:0], imm8[7:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALESD'. Intrinsic: '_mm_mask_roundscale_sd'.
// Requires AVX512F.
func MaskRoundscaleSd(src M128d, k Mmask8, a M128d, b M128d, imm8 int) M128d {
	return M128d(maskRoundscaleSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), imm8))
}

func maskRoundscaleSd(src [2]float64, k uint8, a [2]float64, b [2]float64, imm8 int) [2]float64


// MaskzRoundscaleSd: Round the lower double-precision (64-bit) floating-point
// element in 'a' to the number of fraction bits specified by 'imm8', store the
// result in the lower element of 'dst' using zeromask 'k' (the element is
// zeroed out when mask bit 0 is not set), and copy the upper element from 'b'
// to the upper element of 'dst'. 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}		
//		
//		IF k[0]
//			dst[63:0] := RoundTo_IntegerPD(a[63:0], imm8[7:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALESD'. Intrinsic: '_mm_maskz_roundscale_sd'.
// Requires AVX512F.
func MaskzRoundscaleSd(k Mmask8, a M128d, b M128d, imm8 int) M128d {
	return M128d(maskzRoundscaleSd(uint8(k), [2]float64(a), [2]float64(b), imm8))
}

func maskzRoundscaleSd(k uint8, a [2]float64, b [2]float64, imm8 int) [2]float64


// RoundscaleSd: Round the lower double-precision (64-bit) floating-point
// element in 'a' to the number of fraction bits specified by 'imm8', store the
// result in the lower element of 'dst', and copy the upper element from 'b' to
// the upper element of 'dst'. 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}		
//		
//		dst[63:0] := RoundTo_IntegerPD(a[63:0], imm8[7:0])
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALESD'. Intrinsic: '_mm_roundscale_sd'.
// Requires AVX512F.
func RoundscaleSd(a M128d, b M128d, imm8 int) M128d {
	return M128d(roundscaleSd([2]float64(a), [2]float64(b), imm8))
}

func roundscaleSd(a [2]float64, b [2]float64, imm8 int) [2]float64


// MaskRoundscaleSs: Round the lower single-precision (32-bit) floating-point
// element in 'a' to the number of fraction bits specified by 'imm8', store the
// result in the lower element of 'dst' using writemask 'k' (the element is
// copied from 'src' when mask bit 0 is not set), and copy the upper 3 packed
// elements from 'b' to the upper elements of 'dst'. 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}
//		
//		IF k[0]
//			dst[31:0] := RoundTo_IntegerPS(a[31:0], imm8[7:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALESS'. Intrinsic: '_mm_mask_roundscale_ss'.
// Requires AVX512F.
func MaskRoundscaleSs(src M128, k Mmask8, a M128, b M128, imm8 int) M128 {
	return M128(maskRoundscaleSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), imm8))
}

func maskRoundscaleSs(src [4]float32, k uint8, a [4]float32, b [4]float32, imm8 int) [4]float32


// MaskzRoundscaleSs: Round the lower single-precision (32-bit) floating-point
// element in 'a' to the number of fraction bits specified by 'imm8', store the
// result in the lower element of 'dst' using zeromask 'k' (the element is
// zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements
// from 'b' to the upper elements of 'dst'. 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}
//		
//		IF k[0]
//			dst[31:0] := RoundTo_IntegerPS(a[31:0], imm8[7:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALESS'. Intrinsic: '_mm_maskz_roundscale_ss'.
// Requires AVX512F.
func MaskzRoundscaleSs(k Mmask8, a M128, b M128, imm8 int) M128 {
	return M128(maskzRoundscaleSs(uint8(k), [4]float32(a), [4]float32(b), imm8))
}

func maskzRoundscaleSs(k uint8, a [4]float32, b [4]float32, imm8 int) [4]float32


// RoundscaleSs: Round the lower single-precision (32-bit) floating-point
// element in 'a' to the number of fraction bits specified by 'imm8', store the
// result in the lower element of 'dst', and copy the upper 3 packed elements
// from 'b' to the upper elements of 'dst'. 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}
//		
//		dst[31:0] := RoundTo_IntegerPS(a[31:0], imm8[7:0])
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VRNDSCALESS'. Intrinsic: '_mm_roundscale_ss'.
// Requires AVX512F.
func RoundscaleSs(a M128, b M128, imm8 int) M128 {
	return M128(roundscaleSs([4]float32(a), [4]float32(b), imm8))
}

func roundscaleSs(a [4]float32, b [4]float32, imm8 int) [4]float32


// RsqrtPs: Compute the approximate reciprocal square root of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst'. The maximum relative error for this approximation is less
// than 1.5*2^-12. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := APPROXIMATE(1.0 / SQRT(a[i+31:i]))
//		ENDFOR
//
// Instruction: 'RSQRTPS'. Intrinsic: '_mm_rsqrt_ps'.
// Requires SSE.
func RsqrtPs(a M128) M128 {
	return M128(rsqrtPs([4]float32(a)))
}

func rsqrtPs(a [4]float32) [4]float32


// RsqrtSs: Compute the approximate reciprocal square root of the lower
// single-precision (32-bit) floating-point element in 'a', store the result in
// the lower element of 'dst', and copy the upper 3 packed elements from 'a' to
// the upper elements of 'dst'. The maximum relative error for this
// approximation is less than 1.5*2^-12. 
//
//		dst[31:0] := APPROXIMATE(1.0 / SQRT(a[31:0]))
//		dst[127:32] := a[127:32]
//
// Instruction: 'RSQRTSS'. Intrinsic: '_mm_rsqrt_ss'.
// Requires SSE.
func RsqrtSs(a M128) M128 {
	return M128(rsqrtSs([4]float32(a)))
}

func rsqrtSs(a [4]float32) [4]float32


// MaskRsqrt14Pd: Compute the approximate reciprocal square root of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := APPROXIMATE(1.0 / SQRT(a[i+63:i]))
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRSQRT14PD'. Intrinsic: '_mm_mask_rsqrt14_pd'.
// Requires AVX512F.
func MaskRsqrt14Pd(src M128d, k Mmask8, a M128d) M128d {
	return M128d(maskRsqrt14Pd([2]float64(src), uint8(k), [2]float64(a)))
}

func maskRsqrt14Pd(src [2]float64, k uint8, a [2]float64) [2]float64


// MaskzRsqrt14Pd: Compute the approximate reciprocal square root of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := APPROXIMATE(1.0 / SQRT(a[i+63:i]))
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRSQRT14PD'. Intrinsic: '_mm_maskz_rsqrt14_pd'.
// Requires AVX512F.
func MaskzRsqrt14Pd(k Mmask8, a M128d) M128d {
	return M128d(maskzRsqrt14Pd(uint8(k), [2]float64(a)))
}

func maskzRsqrt14Pd(k uint8, a [2]float64) [2]float64


// MaskRsqrt14Ps: Compute the approximate reciprocal square root of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := APPROXIMATE(1.0 / SQRT(a[i+31:i]))
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRSQRT14PS'. Intrinsic: '_mm_mask_rsqrt14_ps'.
// Requires AVX512F.
func MaskRsqrt14Ps(src M128, k Mmask8, a M128) M128 {
	return M128(maskRsqrt14Ps([4]float32(src), uint8(k), [4]float32(a)))
}

func maskRsqrt14Ps(src [4]float32, k uint8, a [4]float32) [4]float32


// MaskzRsqrt14Ps: Compute the approximate reciprocal square root of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := APPROXIMATE(1.0 / SQRT(a[i+31:i]))
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VRSQRT14PS'. Intrinsic: '_mm_maskz_rsqrt14_ps'.
// Requires AVX512F.
func MaskzRsqrt14Ps(k Mmask8, a M128) M128 {
	return M128(maskzRsqrt14Ps(uint8(k), [4]float32(a)))
}

func maskzRsqrt14Ps(k uint8, a [4]float32) [4]float32


// MaskRsqrt14Sd: Compute the approximate reciprocal square root of the lower
// double-precision (64-bit) floating-point element in 'b', store the result in
// the lower element of 'dst' using writemask 'k' (the element is copied from
// 'src' when mask bit 0 is not set), and copy the upper element from 'a' to
// the upper element of 'dst'. The maximum relative error for this
// approximation is less than 2^-14. 
//
//		IF k[0]
//			dst[63:0] := APPROXIMATE(1.0 / SQRT(b[63:0]))
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VRSQRT14SD'. Intrinsic: '_mm_mask_rsqrt14_sd'.
// Requires AVX512F.
func MaskRsqrt14Sd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskRsqrt14Sd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskRsqrt14Sd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzRsqrt14Sd: Compute the approximate reciprocal square root of the lower
// double-precision (64-bit) floating-point element in 'b', store the result in
// the lower element of 'dst' using zeromask 'k' (the element is zeroed out
// when mask bit 0 is not set), and copy the upper element from 'a' to the
// upper element of 'dst'. The maximum relative error for this approximation is
// less than 2^-14. 
//
//		IF k[0]
//			dst[63:0] := APPROXIMATE(1.0 / SQRT(b[63:0]))
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VRSQRT14SD'. Intrinsic: '_mm_maskz_rsqrt14_sd'.
// Requires AVX512F.
func MaskzRsqrt14Sd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzRsqrt14Sd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzRsqrt14Sd(k uint8, a [2]float64, b [2]float64) [2]float64


// Rsqrt14Sd: Compute the approximate reciprocal square root of the lower
// double-precision (64-bit) floating-point element in 'b', store the result in
// the lower element of 'dst', and copy the upper element from 'a' to the upper
// element of 'dst'. The maximum relative error for this approximation is less
// than 2^-14. 
//
//		dst[63:0] := APPROXIMATE(1.0 / SQRT(b[63:0]))
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VRSQRT14SD'. Intrinsic: '_mm_rsqrt14_sd'.
// Requires AVX512F.
func Rsqrt14Sd(a M128d, b M128d) M128d {
	return M128d(rsqrt14Sd([2]float64(a), [2]float64(b)))
}

func rsqrt14Sd(a [2]float64, b [2]float64) [2]float64


// MaskRsqrt14Ss: Compute the approximate reciprocal square root of the lower
// single-precision (32-bit) floating-point element in 'b', store the result in
// the lower element of 'dst' using writemask 'k' (the element is copied from
// 'src' when mask bit 0 is not set), and copy the upper 3 packed elements from
// 'a' to the upper elements of 'dst'. The maximum relative error for this
// approximation is less than 2^-14. 
//
//		IF k[0]
//			dst[31:0] := APPROXIMATE(1.0 / SQRT(b[31:0]))
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VRSQRT14SS'. Intrinsic: '_mm_mask_rsqrt14_ss'.
// Requires AVX512F.
func MaskRsqrt14Ss(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskRsqrt14Ss([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskRsqrt14Ss(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzRsqrt14Ss: Compute the approximate reciprocal square root of the lower
// single-precision (32-bit) floating-point element in 'b', store the result in
// the lower element of 'dst' using zeromask 'k' (the element is zeroed out
// when mask bit 0 is not set), and copy the upper 3 packed elements from 'a'
// to the upper elements of 'dst'. The maximum relative error for this
// approximation is less than 2^-14. 
//
//		IF k[0]
//			dst[31:0] := APPROXIMATE(1.0 / SQRT(b[31:0]))
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VRSQRT14SS'. Intrinsic: '_mm_maskz_rsqrt14_ss'.
// Requires AVX512F.
func MaskzRsqrt14Ss(k Mmask8, a M128, b M128) M128 {
	return M128(maskzRsqrt14Ss(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzRsqrt14Ss(k uint8, a [4]float32, b [4]float32) [4]float32


// Rsqrt14Ss: Compute the approximate reciprocal square root of the lower
// single-precision (32-bit) floating-point element in 'b', store the result in
// the lower element of 'dst', and copy the upper 3 packed elements from 'a' to
// the upper elements of 'dst'. The maximum relative error for this
// approximation is less than 2^-14. 
//
//		dst[31:0] := APPROXIMATE(1.0 / SQRT(b[31:0]))
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VRSQRT14SS'. Intrinsic: '_mm_rsqrt14_ss'.
// Requires AVX512F.
func Rsqrt14Ss(a M128, b M128) M128 {
	return M128(rsqrt14Ss([4]float32(a), [4]float32(b)))
}

func rsqrt14Ss(a [4]float32, b [4]float32) [4]float32


// MaskRsqrt28RoundSd: Compute the approximate reciprocal square root of the
// lower double-precision (64-bit) floating-point element in 'b', store the
// result in the lower element of 'dst' using writemask 'k' (the element is
// copied from 'src' when mask bit 0 is not set), and copy the upper element
// from 'a' to the upper element of 'dst'. The maximum relative error for this
// approximation is less than 2^-28. Rounding is done according to the
// 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0] THEN
//			dst[63:0] := (1.0/SQRT(b[63:0]));
//		ELSE
//			dst[63:0] := src[63:0];
//		FI
//		dst[127:64] := a[127:64];
//		dst[MAX:128] := 0;
//
// Instruction: 'VRSQRT28SD'. Intrinsic: '_mm_mask_rsqrt28_round_sd'.
// Requires AVX512ER.
func MaskRsqrt28RoundSd(src M128d, k Mmask8, a M128d, b M128d, rounding int) M128d {
	return M128d(maskRsqrt28RoundSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), rounding))
}

func maskRsqrt28RoundSd(src [2]float64, k uint8, a [2]float64, b [2]float64, rounding int) [2]float64


// MaskzRsqrt28RoundSd: Compute the approximate reciprocal square root of the
// lower double-precision (64-bit) floating-point element in 'b', store the
// result in the lower element of 'dst' using zeromask 'k' (the element is
// zeroed out when mask bit 0 is not set), and copy the upper element from 'a'
// to the upper element of 'dst'. The maximum relative error for this
// approximation is less than 2^-28. Rounding is done according to the
// 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0] THEN
//			dst[63:0] := (1.0/SQRT(b[63:0]));
//		ELSE
//			dst[63:0] := 0;
//		FI
//		dst[127:64] := a[127:64];
//		dst[MAX:128] := 0;
//
// Instruction: 'VRSQRT28SD'. Intrinsic: '_mm_maskz_rsqrt28_round_sd'.
// Requires AVX512ER.
func MaskzRsqrt28RoundSd(k Mmask8, a M128d, b M128d, rounding int) M128d {
	return M128d(maskzRsqrt28RoundSd(uint8(k), [2]float64(a), [2]float64(b), rounding))
}

func maskzRsqrt28RoundSd(k uint8, a [2]float64, b [2]float64, rounding int) [2]float64


// Rsqrt28RoundSd: Compute the approximate reciprocal square root of the lower
// double-precision (64-bit) floating-point element in 'b', store the result in
// the lower element of 'dst', and copy the upper element from 'a' to the upper
// element of 'dst'. The maximum relative error for this approximation is less
// than 2^-28. Rounding is done according to the 'rounding' parameter, which
// can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := (1.0/SQRT(b[63:0]));
//		dst[127:64] := a[127:64];
//		dst[MAX:128] := 0;
//
// Instruction: 'VRSQRT28SD'. Intrinsic: '_mm_rsqrt28_round_sd'.
// Requires AVX512ER.
func Rsqrt28RoundSd(a M128d, b M128d, rounding int) M128d {
	return M128d(rsqrt28RoundSd([2]float64(a), [2]float64(b), rounding))
}

func rsqrt28RoundSd(a [2]float64, b [2]float64, rounding int) [2]float64


// MaskRsqrt28RoundSs: Compute the approximate reciprocal square root of the
// lower single-precision (32-bit) floating-point element in 'b', store the
// result in the lower element of 'dst' using writemask 'k' (the element is
// copied from 'src' when mask bit 0 is not set), and copy the upper 3 packed
// elements from 'a' to the upper elements of 'dst'. The maximum relative error
// for this approximation is less than 2^-28. Rounding is done according to the
// 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0] THEN
//			dst[31:0] := (1.0/SQRT(b[31:0]));
//		ELSE
//			dst[31:0] := src[31:0];
//		FI
//		dst[127:32] := a[127:32];
//		dst[MAX:128] := 0;
//
// Instruction: 'VRSQRT28SS'. Intrinsic: '_mm_mask_rsqrt28_round_ss'.
// Requires AVX512ER.
func MaskRsqrt28RoundSs(src M128, k Mmask8, a M128, b M128, rounding int) M128 {
	return M128(maskRsqrt28RoundSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), rounding))
}

func maskRsqrt28RoundSs(src [4]float32, k uint8, a [4]float32, b [4]float32, rounding int) [4]float32


// MaskzRsqrt28RoundSs: Compute the approximate reciprocal square root of the
// lower single-precision (32-bit) floating-point element in 'b', store the
// result in the lower element of 'dst' using zeromask 'k' (the element is
// zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements
// from 'a' to the upper elements of 'dst'. The maximum relative error for this
// approximation is less than 2^-28. Rounding is done according to the
// 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0] THEN
//			dst[31:0] := (1.0/SQRT(b[31:0]));
//		ELSE
//			dst[31:0] := 0;
//		FI
//		dst[127:32] := a[127:32];
//		dst[MAX:128] := 0;
//
// Instruction: 'VRSQRT28SS'. Intrinsic: '_mm_maskz_rsqrt28_round_ss'.
// Requires AVX512ER.
func MaskzRsqrt28RoundSs(k Mmask8, a M128, b M128, rounding int) M128 {
	return M128(maskzRsqrt28RoundSs(uint8(k), [4]float32(a), [4]float32(b), rounding))
}

func maskzRsqrt28RoundSs(k uint8, a [4]float32, b [4]float32, rounding int) [4]float32


// Rsqrt28RoundSs: Compute the approximate reciprocal square root of the lower
// single-precision (32-bit) floating-point element in 'b', store the result in
// the lower element of 'dst', and copy the upper 3 packed elements from 'a' to
// the upper elements of 'dst'. The maximum relative error for this
// approximation is less than 2^-28. Rounding is done according to the
// 'rounding' parameter, which can be one of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := (1.0/SQRT(b[31:0]));
//		dst[127:32] := a[127:32];
//		dst[MAX:128] := 0;
//
// Instruction: 'VRSQRT28SS'. Intrinsic: '_mm_rsqrt28_round_ss'.
// Requires AVX512ER.
func Rsqrt28RoundSs(a M128, b M128, rounding int) M128 {
	return M128(rsqrt28RoundSs([4]float32(a), [4]float32(b), rounding))
}

func rsqrt28RoundSs(a [4]float32, b [4]float32, rounding int) [4]float32


// MaskRsqrt28Sd: Compute the approximate reciprocal square root of the lower
// double-precision (64-bit) floating-point element in 'b', store the result in
// the lower element of 'dst' using writemask 'k' (the element is copied from
// 'src' when mask bit 0 is not set), and copy the upper element from 'a' to
// the upper element of 'dst'. The maximum relative error for this
// approximation is less than 2^-28. 
//
//		IF k[0] THEN
//			dst[63:0] := (1.0/SQRT(b[63:0]));
//		ELSE
//			dst[63:0] := src[63:0];
//		FI
//		dst[127:64] := a[127:64];
//		dst[MAX:128] := 0;
//
// Instruction: 'VRSQRT28SD'. Intrinsic: '_mm_mask_rsqrt28_sd'.
// Requires AVX512ER.
func MaskRsqrt28Sd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskRsqrt28Sd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskRsqrt28Sd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzRsqrt28Sd: Compute the approximate reciprocal square root of the lower
// double-precision (64-bit) floating-point element in 'b', store the result in
// the lower element of 'dst' using zeromask 'k' (the element is zeroed out
// when mask bit 0 is not set), and copy the upper element from 'a' to the
// upper element of 'dst'. The maximum relative error for this approximation is
// less than 2^-28. 
//
//		IF k[0] THEN
//			dst[63:0] := (1.0/SQRT(b[63:0]));
//		ELSE
//			dst[63:0] := 0;
//		FI
//		dst[127:64] := a[127:64];
//		dst[MAX:128] := 0;
//
// Instruction: 'VRSQRT28SD'. Intrinsic: '_mm_maskz_rsqrt28_sd'.
// Requires AVX512ER.
func MaskzRsqrt28Sd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzRsqrt28Sd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzRsqrt28Sd(k uint8, a [2]float64, b [2]float64) [2]float64


// Rsqrt28Sd: Compute the approximate reciprocal square root of the lower
// double-precision (64-bit) floating-point element in 'b', store the result in
// the lower element of 'dst', and copy the upper element from 'a' to the upper
// element of 'dst'. The maximum relative error for this approximation is less
// than 2^-28. 
//
//		dst[63:0] := (1.0/SQRT(b[63:0]));
//		dst[127:64] := a[127:64];
//		dst[MAX:128] := 0;
//
// Instruction: 'VRSQRT28SD'. Intrinsic: '_mm_rsqrt28_sd'.
// Requires AVX512ER.
func Rsqrt28Sd(a M128d, b M128d) M128d {
	return M128d(rsqrt28Sd([2]float64(a), [2]float64(b)))
}

func rsqrt28Sd(a [2]float64, b [2]float64) [2]float64


// MaskRsqrt28Ss: Compute the approximate reciprocal square root of the lower
// single-precision (32-bit) floating-point element in 'b', store the result in
// the lower element of 'dst' using writemask 'k' (the element is copied from
// 'src' when mask bit 0 is not set), and copy the upper 3 packed elements from
// 'a' to the upper elements of 'dst'. The maximum relative error for this
// approximation is less than 2^-28. 
//
//		IF k[0] THEN
//			dst[31:0] := (1.0/SQRT(b[31:0]));
//		ELSE
//			dst[31:0] := src[31:0];
//		FI
//		dst[127:32] := a[127:32];
//		dst[MAX:128] := 0;
//
// Instruction: 'VRSQRT28SS'. Intrinsic: '_mm_mask_rsqrt28_ss'.
// Requires AVX512ER.
func MaskRsqrt28Ss(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskRsqrt28Ss([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskRsqrt28Ss(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzRsqrt28Ss: Compute the approximate reciprocal square root of the lower
// single-precision (32-bit) floating-point element in 'b', store the result in
// the lower element of 'dst' using zeromask 'k' (the element is zeroed out
// when mask bit 0 is not set), and copy the upper 3 packed elements from 'a'
// to the upper elements of 'dst'. The maximum relative error for this
// approximation is less than 2^-28. 
//
//		IF k[0] THEN
//			dst[31:0] := (1.0/SQRT(b[31:0]));
//		ELSE
//			dst[31:0] := 0;
//		FI
//		dst[127:32] := a[127:32];
//		dst[MAX:128] := 0;
//
// Instruction: 'VRSQRT28SS'. Intrinsic: '_mm_maskz_rsqrt28_ss'.
// Requires AVX512ER.
func MaskzRsqrt28Ss(k Mmask8, a M128, b M128) M128 {
	return M128(maskzRsqrt28Ss(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzRsqrt28Ss(k uint8, a [4]float32, b [4]float32) [4]float32


// Rsqrt28Ss: Compute the approximate reciprocal square root of the lower
// single-precision (32-bit) floating-point element in 'b', store the result in
// the lower element of 'dst', and copy the upper 3 packed elements from 'a' to
// the upper elements of 'dst'. The maximum relative error for this
// approximation is less than 2^-28. 
//
//		dst[31:0] := (1.0/SQRT(b[31:0]));
//		dst[127:32] := a[127:32];
//		dst[MAX:128] := 0;
//
// Instruction: 'VRSQRT28SS'. Intrinsic: '_mm_rsqrt28_ss'.
// Requires AVX512ER.
func Rsqrt28Ss(a M128, b M128) M128 {
	return M128(rsqrt28Ss([4]float32(a), [4]float32(b)))
}

func rsqrt28Ss(a [4]float32, b [4]float32) [4]float32


// SadEpu8: Compute the absolute differences of packed unsigned 8-bit integers
// in 'a' and 'b', then horizontally sum each consecutive 8 differences to
// produce two unsigned 16-bit integers, and pack these unsigned 16-bit
// integers in the low 16 bits of 64-bit elements in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*8
//			tmp[i+7:i] := ABS(a[i+7:i] - b[i+7:i])
//		ENDFOR
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+15:i] := tmp[i+7:i] + tmp[i+15:i+8] + tmp[i+23:i+16] + tmp[i+31:i+24] +
//			               tmp[i+39:i+32] + tmp[i+47:i+40] + tmp[i+55:i+48] + tmp[i+63:i+56]
//			dst[i+63:i+16] := 0
//		ENDFOR
//
// Instruction: 'PSADBW'. Intrinsic: '_mm_sad_epu8'.
// Requires SSE2.
func SadEpu8(a M128i, b M128i) M128i {
	return M128i(sadEpu8([16]byte(a), [16]byte(b)))
}

func sadEpu8(a [16]byte, b [16]byte) [16]byte


// MaskScalefPd: Scale the packed double-precision (64-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFPD'. Intrinsic: '_mm_mask_scalef_pd'.
// Requires AVX512F.
func MaskScalefPd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskScalefPd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskScalefPd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzScalefPd: Scale the packed double-precision (64-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFPD'. Intrinsic: '_mm_maskz_scalef_pd'.
// Requires AVX512F.
func MaskzScalefPd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzScalefPd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzScalefPd(k uint8, a [2]float64, b [2]float64) [2]float64


// ScalefPd: Scale the packed double-precision (64-bit) floating-point elements
// in 'a' using values from 'b', and store the results in 'dst'. 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFPD'. Intrinsic: '_mm_scalef_pd'.
// Requires AVX512F.
func ScalefPd(a M128d, b M128d) M128d {
	return M128d(scalefPd([2]float64(a), [2]float64(b)))
}

func scalefPd(a [2]float64, b [2]float64) [2]float64


// MaskScalefPs: Scale the packed single-precision (32-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[31:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFPS'. Intrinsic: '_mm_mask_scalef_ps'.
// Requires AVX512F.
func MaskScalefPs(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskScalefPs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskScalefPs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzScalefPs: Scale the packed single-precision (32-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[31:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFPS'. Intrinsic: '_mm_maskz_scalef_ps'.
// Requires AVX512F.
func MaskzScalefPs(k Mmask8, a M128, b M128) M128 {
	return M128(maskzScalefPs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzScalefPs(k uint8, a [4]float32, b [4]float32) [4]float32


// ScalefPs: Scale the packed single-precision (32-bit) floating-point elements
// in 'a' using values from 'b', and store the results in 'dst'. 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[31:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFPS'. Intrinsic: '_mm_scalef_ps'.
// Requires AVX512F.
func ScalefPs(a M128, b M128) M128 {
	return M128(scalefPs([4]float32(a), [4]float32(b)))
}

func scalefPs(a [4]float32, b [4]float32) [4]float32


// MaskScalefRoundSd: Scale the packed double-precision (64-bit) floating-point
// elements in 'a' using values from 'b', store the result in the lower element
// of 'dst' using writemask 'k' (the element is copied from 'src' when mask bit
// 0 is not set), and copy the upper element from 'b' to the upper element of
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		IF k[0]
//			dst[63:0] := SCALE(a[63:0], b[63:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFSD'. Intrinsic: '_mm_mask_scalef_round_sd'.
// Requires AVX512F.
func MaskScalefRoundSd(src M128d, k Mmask8, a M128d, b M128d, rounding int) M128d {
	return M128d(maskScalefRoundSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), rounding))
}

func maskScalefRoundSd(src [2]float64, k uint8, a [2]float64, b [2]float64, rounding int) [2]float64


// MaskzScalefRoundSd: Scale the packed double-precision (64-bit)
// floating-point elements in 'a' using values from 'b', store the result in
// the lower element of 'dst' using zeromask 'k' (the element is zeroed out
// when mask bit 0 is not set), and copy the upper element from 'b' to the
// upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		IF k[0]
//			dst[63:0] := SCALE(a[63:0], b[63:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFSD'. Intrinsic: '_mm_maskz_scalef_round_sd'.
// Requires AVX512F.
func MaskzScalefRoundSd(k Mmask8, a M128d, b M128d, rounding int) M128d {
	return M128d(maskzScalefRoundSd(uint8(k), [2]float64(a), [2]float64(b), rounding))
}

func maskzScalefRoundSd(k uint8, a [2]float64, b [2]float64, rounding int) [2]float64


// ScalefRoundSd: Scale the packed double-precision (64-bit) floating-point
// elements in 'a' using values from 'b', store the result in the lower element
// of 'dst', and copy the upper element from 'b' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		dst[63:0] := SCALE(a[63:0], b[63:0])
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFSD'. Intrinsic: '_mm_scalef_round_sd'.
// Requires AVX512F.
func ScalefRoundSd(a M128d, b M128d, rounding int) M128d {
	return M128d(scalefRoundSd([2]float64(a), [2]float64(b), rounding))
}

func scalefRoundSd(a [2]float64, b [2]float64, rounding int) [2]float64


// MaskScalefRoundSs: Scale the packed single-precision (32-bit) floating-point
// elements in 'a' using values from 'b', store the result in the lower element
// of 'dst' using writemask 'k' (the element is copied from 'src' when mask bit
// 0 is not set), and copy the upper 3 packed elements from 'b' to the upper
// elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[63:0]
//		}
//		
//		IF k[0]
//			dst[31:0] := SCALE(a[31:0], b[31:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFSS'. Intrinsic: '_mm_mask_scalef_round_ss'.
// Requires AVX512F.
func MaskScalefRoundSs(src M128, k Mmask8, a M128, b M128, rounding int) M128 {
	return M128(maskScalefRoundSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), rounding))
}

func maskScalefRoundSs(src [4]float32, k uint8, a [4]float32, b [4]float32, rounding int) [4]float32


// MaskzScalefRoundSs: Scale the packed single-precision (32-bit)
// floating-point elements in 'a' using values from 'b', store the result in
// the lower element of 'dst' using zeromask 'k' (the element is zeroed out
// when mask bit 0 is not set), and copy the upper 3 packed elements from 'b'
// to the upper elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[63:0]
//		}
//		
//		IF k[0]
//			dst[31:0] := SCALE(a[31:0], b[31:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFSS'. Intrinsic: '_mm_maskz_scalef_round_ss'.
// Requires AVX512F.
func MaskzScalefRoundSs(k Mmask8, a M128, b M128, rounding int) M128 {
	return M128(maskzScalefRoundSs(uint8(k), [4]float32(a), [4]float32(b), rounding))
}

func maskzScalefRoundSs(k uint8, a [4]float32, b [4]float32, rounding int) [4]float32


// ScalefRoundSs: Scale the packed single-precision (32-bit) floating-point
// elements in 'a' using values from 'b', store the result in the lower element
// of 'dst', and copy the upper 3 packed elements from 'b' to the upper
// elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[63:0]
//		}
//		
//		dst[31:0] := SCALE(a[31:0], b[31:0])
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFSS'. Intrinsic: '_mm_scalef_round_ss'.
// Requires AVX512F.
func ScalefRoundSs(a M128, b M128, rounding int) M128 {
	return M128(scalefRoundSs([4]float32(a), [4]float32(b), rounding))
}

func scalefRoundSs(a [4]float32, b [4]float32, rounding int) [4]float32


// MaskScalefSd: Scale the packed double-precision (64-bit) floating-point
// elements in 'a' using values from 'b', store the result in the lower element
// of 'dst' using writemask 'k' (the element is copied from 'src' when mask bit
// 0 is not set), and copy the upper element from 'b' to the upper element of
// 'dst'. 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		IF k[0]
//			dst[63:0] := SCALE(a[63:0], b[63:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFSD'. Intrinsic: '_mm_mask_scalef_sd'.
// Requires AVX512F.
func MaskScalefSd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskScalefSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskScalefSd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzScalefSd: Scale the packed double-precision (64-bit) floating-point
// elements in 'a' using values from 'b', store the result in the lower element
// of 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is
// not set), and copy the upper element from 'b' to the upper element of 'dst'. 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		IF k[0]
//			dst[63:0] := SCALE(a[63:0], b[63:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFSD'. Intrinsic: '_mm_maskz_scalef_sd'.
// Requires AVX512F.
func MaskzScalefSd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzScalefSd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzScalefSd(k uint8, a [2]float64, b [2]float64) [2]float64


// ScalefSd: Scale the packed double-precision (64-bit) floating-point elements
// in 'a' using values from 'b', store the result in the lower element of
// 'dst', and copy the upper element from 'b' to the upper element of 'dst'. 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		dst[63:0] := SCALE(a[63:0], b[63:0])
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFSD'. Intrinsic: '_mm_scalef_sd'.
// Requires AVX512F.
func ScalefSd(a M128d, b M128d) M128d {
	return M128d(scalefSd([2]float64(a), [2]float64(b)))
}

func scalefSd(a [2]float64, b [2]float64) [2]float64


// MaskScalefSs: Scale the packed single-precision (32-bit) floating-point
// elements in 'a' using values from 'b', store the result in the lower element
// of 'dst' using writemask 'k' (the element is copied from 'src' when mask bit
// 0 is not set), and copy the upper 3 packed elements from 'b' to the upper
// elements of 'dst'. 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[63:0]
//		}
//		
//		IF k[0]
//			dst[31:0] := SCALE(a[31:0], b[31:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFSS'. Intrinsic: '_mm_mask_scalef_ss'.
// Requires AVX512F.
func MaskScalefSs(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskScalefSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskScalefSs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzScalefSs: Scale the packed single-precision (32-bit) floating-point
// elements in 'a' using values from 'b', store the result in the lower element
// of 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is
// not set), and copy the upper 3 packed elements from 'b' to the upper
// elements of 'dst'. 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[63:0]
//		}
//		
//		IF k[0]
//			dst[31:0] := SCALE(a[31:0], b[31:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFSS'. Intrinsic: '_mm_maskz_scalef_ss'.
// Requires AVX512F.
func MaskzScalefSs(k Mmask8, a M128, b M128) M128 {
	return M128(maskzScalefSs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzScalefSs(k uint8, a [4]float32, b [4]float32) [4]float32


// ScalefSs: Scale the packed single-precision (32-bit) floating-point elements
// in 'a' using values from 'b', store the result in the lower element of
// 'dst', and copy the upper 3 packed elements from 'b' to the upper elements
// of 'dst'. 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[63:0]
//		}
//		
//		dst[31:0] := SCALE(a[31:0], b[31:0])
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSCALEFSS'. Intrinsic: '_mm_scalef_ss'.
// Requires AVX512F.
func ScalefSs(a M128, b M128) M128 {
	return M128(scalefSs([4]float32(a), [4]float32(b)))
}

func scalefSs(a [4]float32, b [4]float32) [4]float32


// Set16: Set packed 16-bit integers in 'dst' with the supplied values. 
//
//		dst[15:0] := e0
//		dst[31:16] := e1
//		dst[47:32] := e2
//		dst[63:48] := e3
//		dst[79:64] := e4
//		dst[95:80] := e5
//		dst[111:96] := e6
//		dst[127:112] := e7
//
// Instruction: '...'. Intrinsic: '_mm_set_epi16'.
// Requires SSE2.
func Set16(e7 int16, e6 int16, e5 int16, e4 int16, e3 int16, e2 int16, e1 int16, e0 int16) M128i {
	return M128i(set16(e7, e6, e5, e4, e3, e2, e1, e0))
}

func set16(e7 int16, e6 int16, e5 int16, e4 int16, e3 int16, e2 int16, e1 int16, e0 int16) [16]byte


// Set32: Set packed 32-bit integers in 'dst' with the supplied values. 
//
//		dst[31:0] := e0
//		dst[63:32] := e1
//		dst[95:64] := e2
//		dst[127:96] := e3
//
// Instruction: '...'. Intrinsic: '_mm_set_epi32'.
// Requires SSE2.
func Set32(e3 int, e2 int, e1 int, e0 int) M128i {
	return M128i(set32(e3, e2, e1, e0))
}

func set32(e3 int, e2 int, e1 int, e0 int) [16]byte


// Set64: Set packed 64-bit integers in 'dst' with the supplied values. 
//
//		dst[63:0] := e0
//		dst[127:64] := e1
//
// Instruction: '...'. Intrinsic: '_mm_set_epi64'.
// Requires SSE2.
func Set64(e1 M64, e0 M64) M128i {
	return M128i(set64(e1, e0))
}

func set64(e1 M64, e0 M64) [16]byte


// Set64x: Set packed 64-bit integers in 'dst' with the supplied values. 
//
//		dst[63:0] := e0
//		dst[127:64] := e1
//
// Instruction: '...'. Intrinsic: '_mm_set_epi64x'.
// Requires SSE2.
func Set64x(e1 int64, e0 int64) M128i {
	return M128i(set64x(e1, e0))
}

func set64x(e1 int64, e0 int64) [16]byte


// Set8: Set packed 8-bit integers in 'dst' with the supplied values in reverse
// order. 
//
//		dst[7:0] := e0
//		dst[15:8] := e1
//		dst[23:16] := e2
//		dst[31:24] := e3
//		dst[39:32] := e4
//		dst[47:40] := e5
//		dst[55:48] := e6
//		dst[63:56] := e7
//		dst[71:64] := e8
//		dst[79:72] := e9
//		dst[87:80] := e10
//		dst[95:88] := e11
//		dst[103:96] := e12
//		dst[111:104] := e13
//		dst[119:112] := e14
//		dst[127:120] := e15
//
// Instruction: '...'. Intrinsic: '_mm_set_epi8'.
// Requires SSE2.
func Set8(e15 byte, e14 byte, e13 byte, e12 byte, e11 byte, e10 byte, e9 byte, e8 byte, e7 byte, e6 byte, e5 byte, e4 byte, e3 byte, e2 byte, e1 byte, e0 byte) M128i {
	return M128i(set8(e15, e14, e13, e12, e11, e10, e9, e8, e7, e6, e5, e4, e3, e2, e1, e0))
}

func set8(e15 byte, e14 byte, e13 byte, e12 byte, e11 byte, e10 byte, e9 byte, e8 byte, e7 byte, e6 byte, e5 byte, e4 byte, e3 byte, e2 byte, e1 byte, e0 byte) [16]byte


// SetPd: Set packed double-precision (64-bit) floating-point elements in 'dst'
// with the supplied values. 
//
//		dst[63:0] := e0
//		dst[127:64] := e1
//
// Instruction: '...'. Intrinsic: '_mm_set_pd'.
// Requires SSE2.
func SetPd(e1 float64, e0 float64) M128d {
	return M128d(setPd(e1, e0))
}

func setPd(e1 float64, e0 float64) [2]float64


// SetPd1: Broadcast double-precision (64-bit) floating-point value 'a' to all
// elements of 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := a[63:0]
//		ENDFOR
//
// Instruction: '...'. Intrinsic: '_mm_set_pd1'.
// Requires SSE2.
func SetPd1(a float64) M128d {
	return M128d(setPd1(a))
}

func setPd1(a float64) [2]float64


// SetPs: Set packed single-precision (32-bit) floating-point elements in 'dst'
// with the supplied values. 
//
//		dst[31:0] := e0
//		dst[63:32] := e1
//		dst[95:64] := e2
//		dst[127:96] := e3
//
// Instruction: '...'. Intrinsic: '_mm_set_ps'.
// Requires SSE.
func SetPs(e3 float32, e2 float32, e1 float32, e0 float32) M128 {
	return M128(setPs(e3, e2, e1, e0))
}

func setPs(e3 float32, e2 float32, e1 float32, e0 float32) [4]float32


// SetPs1: Broadcast single-precision (32-bit) floating-point value 'a' to all
// elements of 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := a[31:0]
//		ENDFOR
//
// Instruction: '...'. Intrinsic: '_mm_set_ps1'.
// Requires SSE.
func SetPs1(a float32) M128 {
	return M128(setPs1(a))
}

func setPs1(a float32) [4]float32


// SetSd: Copy double-precision (64-bit) floating-point element 'a' to the
// lower element of 'dst', and zero the upper element. 
//
//		dst[63:0] := a[63:0]
//		dst[127:64] := 0
//
// Instruction: '...'. Intrinsic: '_mm_set_sd'.
// Requires SSE2.
func SetSd(a float64) M128d {
	return M128d(setSd(a))
}

func setSd(a float64) [2]float64


// SetSs: Copy single-precision (32-bit) floating-point element 'a' to the
// lower element of 'dst', and zero the upper 3 elements. 
//
//		dst[31:0] := a[31:0]
//		dst[127:32] := 0
//
// Instruction: '...'. Intrinsic: '_mm_set_ss'.
// Requires SSE.
func SetSs(a float32) M128 {
	return M128(setSs(a))
}

func setSs(a float32) [4]float32


// MaskSet116: Broadcast the low packed 16-bit integer from 'a' to all elements
// of 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := a[15:0]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBROADCASTW'. Intrinsic: '_mm_mask_set1_epi16'.
// Requires AVX512BW.
func MaskSet116(src M128i, k Mmask8, a int16) M128i {
	return M128i(maskSet116([16]byte(src), uint8(k), a))
}

func maskSet116(src [16]byte, k uint8, a int16) [16]byte


// MaskzSet116: Broadcast the low packed 16-bit integer from 'a' to all
// elements of 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := a[15:0]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBROADCASTW'. Intrinsic: '_mm_maskz_set1_epi16'.
// Requires AVX512BW.
func MaskzSet116(k Mmask8, a int16) M128i {
	return M128i(maskzSet116(uint8(k), a))
}

func maskzSet116(k uint8, a int16) [16]byte


// Set116: Broadcast 16-bit integer 'a' to all all elements of 'dst'. This
// intrinsic may generate 'vpbroadcastw'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			dst[i+15:i] := a[15:0]
//		ENDFOR
//
// Instruction: '...'. Intrinsic: '_mm_set1_epi16'.
// Requires SSE2.
func Set116(a int16) M128i {
	return M128i(set116(a))
}

func set116(a int16) [16]byte


// MaskSet132: Broadcast 32-bit integer 'a' to all elements of 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm_mask_set1_epi32'.
// Requires AVX512F.
func MaskSet132(src M128i, k Mmask8, a int) M128i {
	return M128i(maskSet132([16]byte(src), uint8(k), a))
}

func maskSet132(src [16]byte, k uint8, a int) [16]byte


// MaskzSet132: Broadcast 32-bit integer 'a' to all elements of 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm_maskz_set1_epi32'.
// Requires AVX512F.
func MaskzSet132(k Mmask8, a int) M128i {
	return M128i(maskzSet132(uint8(k), a))
}

func maskzSet132(k uint8, a int) [16]byte


// Set132: Broadcast 32-bit integer 'a' to all elements of 'dst'. This
// intrinsic may generate 'vpbroadcastd'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := a[31:0]
//		ENDFOR
//
// Instruction: '...'. Intrinsic: '_mm_set1_epi32'.
// Requires SSE2.
func Set132(a int) M128i {
	return M128i(set132(a))
}

func set132(a int) [16]byte


// MaskSet164: Broadcast 64-bit integer 'a' to all elements of 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm_mask_set1_epi64'.
// Requires AVX512F.
func MaskSet164(src M128i, k Mmask8, a int64) M128i {
	return M128i(maskSet164([16]byte(src), uint8(k), a))
}

func maskSet164(src [16]byte, k uint8, a int64) [16]byte


// MaskzSet164: Broadcast 64-bit integer 'a' to all elements of 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm_maskz_set1_epi64'.
// Requires AVX512F.
func MaskzSet164(k Mmask8, a int64) M128i {
	return M128i(maskzSet164(uint8(k), a))
}

func maskzSet164(k uint8, a int64) [16]byte


// Set164: Broadcast 64-bit integer 'a' to all elements of 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := a[63:0]
//		ENDFOR
//
// Instruction: '...'. Intrinsic: '_mm_set1_epi64'.
// Requires SSE2.
func Set164(a M64) M128i {
	return M128i(set164(a))
}

func set164(a M64) [16]byte


// Set164x: Broadcast 64-bit integer 'a' to all elements of 'dst'. This
// intrinsic may generate the 'vpbroadcastq'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := a[63:0]
//		ENDFOR
//
// Instruction: '...'. Intrinsic: '_mm_set1_epi64x'.
// Requires SSE2.
func Set164x(a int64) M128i {
	return M128i(set164x(a))
}

func set164x(a int64) [16]byte


// MaskSet18: Broadcast 8-bit integer 'a' to all elements of 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := a[7:0]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBROADCASTB'. Intrinsic: '_mm_mask_set1_epi8'.
// Requires AVX512BW.
func MaskSet18(src M128i, k Mmask16, a byte) M128i {
	return M128i(maskSet18([16]byte(src), uint16(k), a))
}

func maskSet18(src [16]byte, k uint16, a byte) [16]byte


// MaskzSet18: Broadcast 8-bit integer 'a' to all elements of 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := a[7:0]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPBROADCASTB'. Intrinsic: '_mm_maskz_set1_epi8'.
// Requires AVX512BW.
func MaskzSet18(k Mmask16, a byte) M128i {
	return M128i(maskzSet18(uint16(k), a))
}

func maskzSet18(k uint16, a byte) [16]byte


// Set18: Broadcast 8-bit integer 'a' to all elements of 'dst'. This intrinsic
// may generate 'vpbroadcastb'. 
//
//		FOR j := 0 to 15
//			i := j*8
//			dst[i+7:i] := a[7:0]
//		ENDFOR
//
// Instruction: '...'. Intrinsic: '_mm_set1_epi8'.
// Requires SSE2.
func Set18(a byte) M128i {
	return M128i(set18(a))
}

func set18(a byte) [16]byte


// Set1Pd: Broadcast double-precision (64-bit) floating-point value 'a' to all
// elements of 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := a[63:0]
//		ENDFOR
//
// Instruction: '...'. Intrinsic: '_mm_set1_pd'.
// Requires SSE2.
func Set1Pd(a float64) M128d {
	return M128d(set1Pd(a))
}

func set1Pd(a float64) [2]float64


// Set1Ps: Broadcast single-precision (32-bit) floating-point value 'a' to all
// elements of 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := a[31:0]
//		ENDFOR
//
// Instruction: '...'. Intrinsic: '_mm_set1_ps'.
// Requires SSE.
func Set1Ps(a float32) M128 {
	return M128(set1Ps(a))
}

func set1Ps(a float32) [4]float32


// Setr16: Set packed 16-bit integers in 'dst' with the supplied values in
// reverse order. 
//
//		dst[15:0] := e7
//		dst[31:16] := e6
//		dst[47:32] := e5
//		dst[63:48] := e4
//		dst[79:64] := e3
//		dst[95:80] := e2
//		dst[111:96] := e1
//		dst[127:112] := e0
//
// Instruction: '...'. Intrinsic: '_mm_setr_epi16'.
// Requires SSE2.
func Setr16(e7 int16, e6 int16, e5 int16, e4 int16, e3 int16, e2 int16, e1 int16, e0 int16) M128i {
	return M128i(setr16(e7, e6, e5, e4, e3, e2, e1, e0))
}

func setr16(e7 int16, e6 int16, e5 int16, e4 int16, e3 int16, e2 int16, e1 int16, e0 int16) [16]byte


// Setr32: Set packed 32-bit integers in 'dst' with the supplied values in
// reverse order. 
//
//		dst[31:0] := e3
//		dst[63:32] := e2
//		dst[95:64] := e1
//		dst[127:96] := e0
//
// Instruction: '...'. Intrinsic: '_mm_setr_epi32'.
// Requires SSE2.
func Setr32(e3 int, e2 int, e1 int, e0 int) M128i {
	return M128i(setr32(e3, e2, e1, e0))
}

func setr32(e3 int, e2 int, e1 int, e0 int) [16]byte


// Setr64: Set packed 64-bit integers in 'dst' with the supplied values in
// reverse order. 
//
//		dst[63:0] := e1
//		dst[127:64] := e0
//
// Instruction: '...'. Intrinsic: '_mm_setr_epi64'.
// Requires SSE2.
func Setr64(e1 M64, e0 M64) M128i {
	return M128i(setr64(e1, e0))
}

func setr64(e1 M64, e0 M64) [16]byte


// Setr8: Set packed 8-bit integers in 'dst' with the supplied values in
// reverse order. 
//
//		dst[7:0] := e15
//		dst[15:8] := e14
//		dst[23:16] := e13
//		dst[31:24] := e12
//		dst[39:32] := e11
//		dst[47:40] := e10
//		dst[55:48] := e9
//		dst[63:56] := e8
//		dst[71:64] := e7
//		dst[79:72] := e6
//		dst[87:80] := e5
//		dst[95:88] := e4
//		dst[103:96] := e3
//		dst[111:104] := e2
//		dst[119:112] := e1
//		dst[127:120] := e0
//
// Instruction: '...'. Intrinsic: '_mm_setr_epi8'.
// Requires SSE2.
func Setr8(e15 byte, e14 byte, e13 byte, e12 byte, e11 byte, e10 byte, e9 byte, e8 byte, e7 byte, e6 byte, e5 byte, e4 byte, e3 byte, e2 byte, e1 byte, e0 byte) M128i {
	return M128i(setr8(e15, e14, e13, e12, e11, e10, e9, e8, e7, e6, e5, e4, e3, e2, e1, e0))
}

func setr8(e15 byte, e14 byte, e13 byte, e12 byte, e11 byte, e10 byte, e9 byte, e8 byte, e7 byte, e6 byte, e5 byte, e4 byte, e3 byte, e2 byte, e1 byte, e0 byte) [16]byte


// SetrPd: Set packed double-precision (64-bit) floating-point elements in
// 'dst' with the supplied values in reverse order. 
//
//		dst[63:0] := e1
//		dst[127:64] := e0
//
// Instruction: '...'. Intrinsic: '_mm_setr_pd'.
// Requires SSE2.
func SetrPd(e1 float64, e0 float64) M128d {
	return M128d(setrPd(e1, e0))
}

func setrPd(e1 float64, e0 float64) [2]float64


// SetrPs: Set packed single-precision (32-bit) floating-point elements in
// 'dst' with the supplied values in reverse order. 
//
//		dst[31:0] := e3
//		dst[63:32] := e2
//		dst[95:64] := e1
//		dst[127:96] := e0
//
// Instruction: '...'. Intrinsic: '_mm_setr_ps'.
// Requires SSE.
func SetrPs(e3 float32, e2 float32, e1 float32, e0 float32) M128 {
	return M128(setrPs(e3, e2, e1, e0))
}

func setrPs(e3 float32, e2 float32, e1 float32, e0 float32) [4]float32


// SetzeroPd: Return vector of type __m128d with all elements set to zero. 
//
//		dst[MAX:0] := 0
//
// Instruction: 'XORPD'. Intrinsic: '_mm_setzero_pd'.
// Requires SSE2.
func SetzeroPd() M128d {
	return M128d(setzeroPd())
}

func setzeroPd() [2]float64


// SetzeroPs: Return vector of type __m128 with all elements set to zero. 
//
//		dst[MAX:0] := 0
//
// Instruction: 'XORPS'. Intrinsic: '_mm_setzero_ps'.
// Requires SSE.
func SetzeroPs() M128 {
	return M128(setzeroPs())
}

func setzeroPs() [4]float32


// SetzeroSi128: Return vector of type __m128i with all elements set to zero. 
//
//		dst[MAX:0] := 0
//
// Instruction: 'PXOR'. Intrinsic: '_mm_setzero_si128'.
// Requires SSE2.
func SetzeroSi128() M128i {
	return M128i(setzeroSi128())
}

func setzeroSi128() [16]byte


// Sha1msg1Epu32: Perform an intermediate calculation for the next four SHA1
// message values (unsigned 32-bit integers) using previous message values from
// 'a' and 'b', and store the result in 'dst'. 
//
//		W0 := a[127:96];
//		W1 := a[95:64];
//		W2 := a[63:32];
//		W3 := a[31:0];
//		W4 := b[127:96];
//		W5 := b[95:64];
//		
//		dst[127:96] := W2 XOR W0;
//		dst[95:64] := W3 XOR W1;
//		dst[63:32] := W4 XOR W2;
//		dst[31:0] := W5 XOR W3;
//
// Instruction: 'SHA1MSG1'. Intrinsic: '_mm_sha1msg1_epu32'.
// Requires SHA.
func Sha1msg1Epu32(a M128i, b M128i) M128i {
	return M128i(sha1msg1Epu32([16]byte(a), [16]byte(b)))
}

func sha1msg1Epu32(a [16]byte, b [16]byte) [16]byte


// Sha1msg2Epu32: Perform the final calculation for the next four SHA1 message
// values (unsigned 32-bit integers) using the intermediate result in 'a' and
// the previous message values in 'b', and store the result in 'dst'. 
//
//		W13 := b[95:64];
//		W14 := b[63:32];
//		W15 := b[31:0];
//		W16 := (a[127:96] XOR W13) <<< 1;
//		W17 := (a[95:64] XOR W14) <<< 1;
//		W18 := (a[63:32] XOR W15) <<< 1;
//		W19 := (a[31:0] XOR W16) <<< 1;
//		
//		dst[127:96] := W16;
//		dst[95:64] := W17;
//		dst[63:32] := W18;
//		dst[31:0] := W19;
//
// Instruction: 'SHA1MSG2'. Intrinsic: '_mm_sha1msg2_epu32'.
// Requires SHA.
func Sha1msg2Epu32(a M128i, b M128i) M128i {
	return M128i(sha1msg2Epu32([16]byte(a), [16]byte(b)))
}

func sha1msg2Epu32(a [16]byte, b [16]byte) [16]byte


// Sha1nexteEpu32: Calculate SHA1 state variable E after four rounds of
// operation from the current SHA1 state variable 'a', add that value to the
// scheduled values (unsigned 32-bit integers) in 'b', and store the result in
// 'dst'. 
//
//		tmp := (a[127:96] <<< 30);
//		dst[127:96] := b[127:96] + tmp;
//		dst[95:64] := b[95:64];
//		dst[63:32] := b[63:32];
//		dst[31:0] := b[31:0];
//
// Instruction: 'SHA1NEXTE'. Intrinsic: '_mm_sha1nexte_epu32'.
// Requires SHA.
func Sha1nexteEpu32(a M128i, b M128i) M128i {
	return M128i(sha1nexteEpu32([16]byte(a), [16]byte(b)))
}

func sha1nexteEpu32(a [16]byte, b [16]byte) [16]byte


// Sha1rnds4Epu32: Perform four rounds of SHA1 operation using an initial SHA1
// state (A,B,C,D) from 'a' and some pre-computed sum of the next 4 round
// message values (unsigned 32-bit integers), and state variable E from 'b',
// and store the updated SHA1 state (A,B,C,D) in 'dst'. 'func' contains the
// logic functions and round constants. 
//
//		IF (func[1:0] = 0) THEN
//			f() := f0(), K := K0;
//		ELSE IF (func[1:0] = 1) THEN
//			f() := f1(), K := K1;
//		ELSE IF (func[1:0] = 2) THEN
//			f() := f2(), K := K2;
//		ELSE IF (func[1:0] = 3) THEN
//			f() := f3(), K := K3;
//		FI;
//		
//		A := a[127:96];
//		B := a[95:64];
//		C := a[63:32];
//		D := a[31:0];
//		
//		W[0] := b[127:96];
//		W[1] := b[95:64];
//		W[2] := b[63:32];
//		W[3] := b[31:0];
//		
//		A[1] := f(B, C, D) + (A <<< 5) + W[0] + K;
//		B[1] := A;
//		C[1] := B <<< 30;
//		D[1] := C;
//		E[1] := D;
//		
//		FOR i = 1 to 3
//				A[i+1] := f(B[i], C[i], D[i]) + (A[i] <<< 5) + W[i] + E[i] + K;
//				B[i+1] := A[i];
//				C[i+1] := B[i] <<< 30;
//				D[i+1] := C[i];
//				E[i+1] := D[i];
//		ENDFOR;
//		
//		dst[127:96] := A[4];
//		dst[95:64] := B[4];
//		dst[63:32] := C[4];
//		dst[31:0] := D[4];
//
// Instruction: 'SHA1RNDS4'. Intrinsic: '_mm_sha1rnds4_epu32'.
// Requires SHA.
func Sha1rnds4Epu32(a M128i, b M128i, fnc int) M128i {
	return M128i(sha1rnds4Epu32([16]byte(a), [16]byte(b), fnc))
}

func sha1rnds4Epu32(a [16]byte, b [16]byte, fnc int) [16]byte


// Sha256msg1Epu32: Perform an intermediate calculation for the next four
// SHA256 message values (unsigned 32-bit integers) using previous message
// values from 'a' and 'b', and store the result in 'dst'. 
//
//		W4 := b[31:0];
//		W3 := a[127:96];
//		W2 := a[95:64];
//		W1 := a[63:32];
//		W0 := a[31:0];
//		
//		dst[127:96] := W3 + sigma0(W4);
//		dst[95:64] := W2 + sigma0(W3);
//		dst[63:32] := W1 + sigma0(W2);
//		dst[31:0] := W0 + sigma0(W1);
//
// Instruction: 'SHA256MSG1'. Intrinsic: '_mm_sha256msg1_epu32'.
// Requires SHA.
func Sha256msg1Epu32(a M128i, b M128i) M128i {
	return M128i(sha256msg1Epu32([16]byte(a), [16]byte(b)))
}

func sha256msg1Epu32(a [16]byte, b [16]byte) [16]byte


// Sha256msg2Epu32: Perform the final calculation for the next four SHA256
// message values (unsigned 32-bit integers) using previous message values from
// 'a' and 'b', and store the result in 'dst'." 
//
//		W14 := b[95:64];
//		W15 := b[127:96];
//		W16 := a[31:0] + sigma1(W14);
//		W17 := a[63:32] + sigma1(W15);
//		W18 := a[95:64] + sigma1(W16);
//		W19 := a[127:96] + sigma1(W17);
//		
//		dst[127:96] := W19;
//		dst[95:64] := W18;
//		dst[63:32] := W17;
//		dst[31:0] := W16;
//
// Instruction: 'SHA256MSG2'. Intrinsic: '_mm_sha256msg2_epu32'.
// Requires SHA.
func Sha256msg2Epu32(a M128i, b M128i) M128i {
	return M128i(sha256msg2Epu32([16]byte(a), [16]byte(b)))
}

func sha256msg2Epu32(a [16]byte, b [16]byte) [16]byte


// Sha256rnds2Epu32: Perform 2 rounds of SHA256 operation using an initial
// SHA256 state (C,D,G,H) from 'a', an initial SHA256 state (A,B,E,F) from 'b',
// and a pre-computed sum of the next 2 round message values (unsigned 32-bit
// integers) and the corresponding round constants from 'k', and store the
// updated SHA256 state (A,B,E,F) in 'dst'. 
//
//		A[0] := b[127:96];
//		B[0] := b[95:64];
//		C[0] := a[127:96];
//		D[0] := a[95:64];
//		E[0] := b[63:32];
//		F[0] := b[31:0];
//		G[0] := a[63:32];
//		H[0] := a[31:0];
//		
//		W_K0 := k[31:0];
//		W_K1 := k[63:32];
//		
//		FOR i = 0 to 1
//				A_(i+1) := Ch(E[i], F[i], G[i]) + sum1(E[i]) + WKi + H[i] + Maj(A[i], B[i], C[i]) + sum0(A[i]);
//				B_(i+1) := A[i];
//				C_(i+1) := B[i];
//				D_(i+1) := C[i];
//				E_(i+1) := Ch(E[i], F[i], G[i]) + sum1(E[i]) + WKi + H[i] + D[i];
//				F_(i+1) := E[i];
//				G_(i+1) := F[i];
//				H_(i+1) := G[i];
//		ENDFOR;
//		
//		dst[127:96] := A[2];
//		dst[95:64] := B[2];
//		dst[63:32] := E[2];
//		dst[31:0] := F[2];
//
// Instruction: 'SHA256RNDS2'. Intrinsic: '_mm_sha256rnds2_epu32'.
// Requires SHA.
func Sha256rnds2Epu32(a M128i, b M128i, k M128i) M128i {
	return M128i(sha256rnds2Epu32([16]byte(a), [16]byte(b), [16]byte(k)))
}

func sha256rnds2Epu32(a [16]byte, b [16]byte, k [16]byte) [16]byte


// MaskShuffle32: Shuffle 32-bit integers in 'a' using the control in 'imm8',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSHUFD'. Intrinsic: '_mm_mask_shuffle_epi32'.
// Requires AVX512F.
func MaskShuffle32(src M128i, k Mmask8, a M128i, imm8 MMPERMENUM) M128i {
	return M128i(maskShuffle32([16]byte(src), uint8(k), [16]byte(a), imm8))
}

func maskShuffle32(src [16]byte, k uint8, a [16]byte, imm8 MMPERMENUM) [16]byte


// MaskzShuffle32: Shuffle 32-bit integers in 'a' using the control in 'imm8',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSHUFD'. Intrinsic: '_mm_maskz_shuffle_epi32'.
// Requires AVX512F.
func MaskzShuffle32(k Mmask8, a M128i, imm8 MMPERMENUM) M128i {
	return M128i(maskzShuffle32(uint8(k), [16]byte(a), imm8))
}

func maskzShuffle32(k uint8, a [16]byte, imm8 MMPERMENUM) [16]byte


// Shuffle32: Shuffle 32-bit integers in 'a' using the control in 'imm8', and
// store the results in 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//
// Instruction: 'PSHUFD'. Intrinsic: '_mm_shuffle_epi32'.
// Requires SSE2.
func Shuffle32(a M128i, imm8 int) M128i {
	return M128i(shuffle32([16]byte(a), imm8))
}

func shuffle32(a [16]byte, imm8 int) [16]byte


// MaskShuffle8: Shuffle packed 8-bit integers in 'a' according to shuffle
// control mask in the corresponding 8-bit element of 'b', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				IF b[i+7] == 1
//					dst[i+7:i] := 0
//				ELSE
//					index[3:0] := b[i+3:i]
//					dst[i+7:i] := a[index*8+7:index*8]
//				FI
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSHUFB'. Intrinsic: '_mm_mask_shuffle_epi8'.
// Requires AVX512BW.
func MaskShuffle8(src M128i, k Mmask16, a M128i, b M128i) M128i {
	return M128i(maskShuffle8([16]byte(src), uint16(k), [16]byte(a), [16]byte(b)))
}

func maskShuffle8(src [16]byte, k uint16, a [16]byte, b [16]byte) [16]byte


// MaskzShuffle8: Shuffle packed 8-bit integers in 'a' according to shuffle
// control mask in the corresponding 8-bit element of 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				IF b[i+7] == 1
//					dst[i+7:i] := 0
//				ELSE
//					index[3:0] := b[i+3:i]
//					dst[i+7:i] := a[index*8+7:index*8]
//				FI
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSHUFB'. Intrinsic: '_mm_maskz_shuffle_epi8'.
// Requires AVX512BW.
func MaskzShuffle8(k Mmask16, a M128i, b M128i) M128i {
	return M128i(maskzShuffle8(uint16(k), [16]byte(a), [16]byte(b)))
}

func maskzShuffle8(k uint16, a [16]byte, b [16]byte) [16]byte


// Shuffle8: Shuffle packed 8-bit integers in 'a' according to shuffle control
// mask in the corresponding 8-bit element of 'b', and store the results in
// 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF b[i+7] == 1
//				dst[i+7:i] := 0
//			ELSE
//				index[3:0] := b[i+3:i]
//				dst[i+7:i] := a[index*8+7:index*8]
//			FI
//		ENDFOR
//
// Instruction: 'PSHUFB'. Intrinsic: '_mm_shuffle_epi8'.
// Requires SSSE3.
func Shuffle8(a M128i, b M128i) M128i {
	return M128i(shuffle8([16]byte(a), [16]byte(b)))
}

func shuffle8(a [16]byte, b [16]byte) [16]byte


// MaskShufflePd: Shuffle double-precision (64-bit) floating-point elements
// using the control in 'imm8', and store the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		tmp_dst[63:0] := (imm8[0] == 0) ? a[63:0] : a[127:64]
//		tmp_dst[127:64] := (imm8[1] == 0) ? b[63:0] : b[127:64]
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSHUFPD'. Intrinsic: '_mm_mask_shuffle_pd'.
// Requires AVX512F.
func MaskShufflePd(src M128d, k Mmask8, a M128d, b M128d, imm8 int) M128d {
	return M128d(maskShufflePd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), imm8))
}

func maskShufflePd(src [2]float64, k uint8, a [2]float64, b [2]float64, imm8 int) [2]float64


// MaskzShufflePd: Shuffle double-precision (64-bit) floating-point elements
// using the control in 'imm8', and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		tmp_dst[63:0] := (imm8[0] == 0) ? a[63:0] : a[127:64]
//		tmp_dst[127:64] := (imm8[1] == 0) ? b[63:0] : b[127:64]
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSHUFPD'. Intrinsic: '_mm_maskz_shuffle_pd'.
// Requires AVX512F.
func MaskzShufflePd(k Mmask8, a M128d, b M128d, imm8 int) M128d {
	return M128d(maskzShufflePd(uint8(k), [2]float64(a), [2]float64(b), imm8))
}

func maskzShufflePd(k uint8, a [2]float64, b [2]float64, imm8 int) [2]float64


// ShufflePd: Shuffle double-precision (64-bit) floating-point elements using
// the control in 'imm8', and store the results in 'dst'. 
//
//		dst[63:0] := (imm8[0] == 0) ? a[63:0] : a[127:64]
//		dst[127:64] := (imm8[1] == 0) ? b[63:0] : b[127:64]
//
// Instruction: 'SHUFPD'. Intrinsic: '_mm_shuffle_pd'.
// Requires SSE2.
func ShufflePd(a M128d, b M128d, imm8 int) M128d {
	return M128d(shufflePd([2]float64(a), [2]float64(b), imm8))
}

func shufflePd(a [2]float64, b [2]float64, imm8 int) [2]float64


// MaskShufflePs: Shuffle single-precision (32-bit) floating-point elements in
// 'a' using the control in 'imm8', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(b[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(b[127:0], imm8[7:6])
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSHUFPS'. Intrinsic: '_mm_mask_shuffle_ps'.
// Requires AVX512F.
func MaskShufflePs(src M128, k Mmask8, a M128, b M128, imm8 int) M128 {
	return M128(maskShufflePs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), imm8))
}

func maskShufflePs(src [4]float32, k uint8, a [4]float32, b [4]float32, imm8 int) [4]float32


// MaskzShufflePs: Shuffle single-precision (32-bit) floating-point elements in
// 'a' using the control in 'imm8', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(b[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(b[127:0], imm8[7:6])
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSHUFPS'. Intrinsic: '_mm_maskz_shuffle_ps'.
// Requires AVX512F.
func MaskzShufflePs(k Mmask8, a M128, b M128, imm8 int) M128 {
	return M128(maskzShufflePs(uint8(k), [4]float32(a), [4]float32(b), imm8))
}

func maskzShufflePs(k uint8, a [4]float32, b [4]float32, imm8 int) [4]float32


// ShufflePs: Shuffle single-precision (32-bit) floating-point elements in 'a'
// using the control in 'imm8', and store the results in 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		dst[95:64] := SELECT4(b[127:0], imm8[5:4])
//		dst[127:96] := SELECT4(b[127:0], imm8[7:6])
//
// Instruction: 'SHUFPS'. Intrinsic: '_mm_shuffle_ps'.
// Requires SSE.
func ShufflePs(a M128, b M128, imm8 uint32) M128 {
	return M128(shufflePs([4]float32(a), [4]float32(b), imm8))
}

func shufflePs(a [4]float32, b [4]float32, imm8 uint32) [4]float32


// MaskShufflehi16: Shuffle 16-bit integers in the high 64 bits of 'a' using
// the control in 'imm8'. Store the results in the high 64 bits of 'dst', with
// the low 64 bits being copied from from 'a' to 'dst', using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		tmp_dst[63:0] := a[63:0]
//		tmp_dst[79:64] := (a >> (imm8[1:0] * 16))[79:64]
//		tmp_dst[95:80] := (a >> (imm8[3:2] * 16))[79:64]
//		tmp_dst[111:96] := (a >> (imm8[5:4] * 16))[79:64]
//		tmp_dst[127:112] := (a >> (imm8[7:6] * 16))[79:64]
//		
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSHUFHW'. Intrinsic: '_mm_mask_shufflehi_epi16'.
// Requires AVX512BW.
func MaskShufflehi16(src M128i, k Mmask8, a M128i, imm8 int) M128i {
	return M128i(maskShufflehi16([16]byte(src), uint8(k), [16]byte(a), imm8))
}

func maskShufflehi16(src [16]byte, k uint8, a [16]byte, imm8 int) [16]byte


// MaskzShufflehi16: Shuffle 16-bit integers in the high 64 bits of 'a' using
// the control in 'imm8'. Store the results in the high 64 bits of 'dst', with
// the low 64 bits being copied from from 'a' to 'dst', using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		tmp_dst[63:0] := a[63:0]
//		tmp_dst[79:64] := (a >> (imm8[1:0] * 16))[79:64]
//		tmp_dst[95:80] := (a >> (imm8[3:2] * 16))[79:64]
//		tmp_dst[111:96] := (a >> (imm8[5:4] * 16))[79:64]
//		tmp_dst[127:112] := (a >> (imm8[7:6] * 16))[79:64]
//		
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSHUFHW'. Intrinsic: '_mm_maskz_shufflehi_epi16'.
// Requires AVX512BW.
func MaskzShufflehi16(k Mmask8, a M128i, imm8 int) M128i {
	return M128i(maskzShufflehi16(uint8(k), [16]byte(a), imm8))
}

func maskzShufflehi16(k uint8, a [16]byte, imm8 int) [16]byte


// Shufflehi16: Shuffle 16-bit integers in the high 64 bits of 'a' using the
// control in 'imm8'. Store the results in the high 64 bits of 'dst', with the
// low 64 bits being copied from from 'a' to 'dst'. 
//
//		dst[63:0] := a[63:0]
//		dst[79:64] := (a >> (imm8[1:0] * 16))[79:64]
//		dst[95:80] := (a >> (imm8[3:2] * 16))[79:64]
//		dst[111:96] := (a >> (imm8[5:4] * 16))[79:64]
//		dst[127:112] := (a >> (imm8[7:6] * 16))[79:64]
//
// Instruction: 'PSHUFHW'. Intrinsic: '_mm_shufflehi_epi16'.
// Requires SSE2.
func Shufflehi16(a M128i, imm8 int) M128i {
	return M128i(shufflehi16([16]byte(a), imm8))
}

func shufflehi16(a [16]byte, imm8 int) [16]byte


// MaskShufflelo16: Shuffle 16-bit integers in the low 64 bits of 'a' using the
// control in 'imm8'. Store the results in the low 64 bits of 'dst', with the
// high 64 bits being copied from from 'a' to 'dst', using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		tmp_dst[15:0] := (a >> (imm8[1:0] * 16))[15:0]
//		tmp_dst[31:16] := (a >> (imm8[3:2] * 16))[15:0]
//		tmp_dst[47:32] := (a >> (imm8[5:4] * 16))[15:0]
//		tmp_dst[63:48] := (a >> (imm8[7:6] * 16))[15:0]
//		tmp_dst[127:64] := a[127:64]
//		
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSHUFLW'. Intrinsic: '_mm_mask_shufflelo_epi16'.
// Requires AVX512BW.
func MaskShufflelo16(src M128i, k Mmask8, a M128i, imm8 int) M128i {
	return M128i(maskShufflelo16([16]byte(src), uint8(k), [16]byte(a), imm8))
}

func maskShufflelo16(src [16]byte, k uint8, a [16]byte, imm8 int) [16]byte


// MaskzShufflelo16: Shuffle 16-bit integers in the low 64 bits of 'a' using
// the control in 'imm8'. Store the results in the low 64 bits of 'dst', with
// the high 64 bits being copied from from 'a' to 'dst', using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		tmp_dst[15:0] := (a >> (imm8[1:0] * 16))[15:0]
//		tmp_dst[31:16] := (a >> (imm8[3:2] * 16))[15:0]
//		tmp_dst[47:32] := (a >> (imm8[5:4] * 16))[15:0]
//		tmp_dst[63:48] := (a >> (imm8[7:6] * 16))[15:0]
//		tmp_dst[127:64] := a[127:64]
//		
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSHUFLW'. Intrinsic: '_mm_maskz_shufflelo_epi16'.
// Requires AVX512BW.
func MaskzShufflelo16(k Mmask8, a M128i, imm8 int) M128i {
	return M128i(maskzShufflelo16(uint8(k), [16]byte(a), imm8))
}

func maskzShufflelo16(k uint8, a [16]byte, imm8 int) [16]byte


// Shufflelo16: Shuffle 16-bit integers in the low 64 bits of 'a' using the
// control in 'imm8'. Store the results in the low 64 bits of 'dst', with the
// high 64 bits being copied from from 'a' to 'dst'. 
//
//		dst[15:0] := (a >> (imm8[1:0] * 16))[15:0]
//		dst[31:16] := (a >> (imm8[3:2] * 16))[15:0]
//		dst[47:32] := (a >> (imm8[5:4] * 16))[15:0]
//		dst[63:48] := (a >> (imm8[7:6] * 16))[15:0]
//		dst[127:64] := a[127:64]
//
// Instruction: 'PSHUFLW'. Intrinsic: '_mm_shufflelo_epi16'.
// Requires SSE2.
func Shufflelo16(a M128i, imm8 int) M128i {
	return M128i(shufflelo16([16]byte(a), imm8))
}

func shufflelo16(a [16]byte, imm8 int) [16]byte


// Sign16: Negate packed 16-bit integers in 'a' when the corresponding signed
// 16-bit integer in 'b' is negative, and store the results in 'dst'. Element
// in 'dst' are zeroed out when the corresponding element in 'b' is zero. 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF b[i+15:i] < 0
//				dst[i+15:i] := NEG(a[i+15:i])
//			ELSE IF b[i+15:i] = 0
//				dst[i+15:i] := 0
//			ELSE
//				dst[i+15:i] := a[i+15:i]
//			FI
//		ENDFOR
//
// Instruction: 'PSIGNW'. Intrinsic: '_mm_sign_epi16'.
// Requires SSSE3.
func Sign16(a M128i, b M128i) M128i {
	return M128i(sign16([16]byte(a), [16]byte(b)))
}

func sign16(a [16]byte, b [16]byte) [16]byte


// Sign32: Negate packed 32-bit integers in 'a' when the corresponding signed
// 32-bit integer in 'b' is negative, and store the results in 'dst'. Element
// in 'dst' are zeroed out when the corresponding element in 'b' is zero. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF b[i+31:i] < 0
//				dst[i+31:i] := NEG(a[i+31:i])
//			ELSE IF b[i+31:i] = 0
//				dst[i+31:i] := 0
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//
// Instruction: 'PSIGND'. Intrinsic: '_mm_sign_epi32'.
// Requires SSSE3.
func Sign32(a M128i, b M128i) M128i {
	return M128i(sign32([16]byte(a), [16]byte(b)))
}

func sign32(a [16]byte, b [16]byte) [16]byte


// Sign8: Negate packed 8-bit integers in 'a' when the corresponding signed
// 8-bit integer in 'b' is negative, and store the results in 'dst'. Element in
// 'dst' are zeroed out when the corresponding element in 'b' is zero. 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF b[i+7:i] < 0
//				dst[i+7:i] := NEG(a[i+7:i])
//			ELSE IF b[i+7:i] = 0
//				dst[i+7:i] := 0
//			ELSE
//				dst[i+7:i] := a[i+7:i]
//			FI
//		ENDFOR
//
// Instruction: 'PSIGNB'. Intrinsic: '_mm_sign_epi8'.
// Requires SSSE3.
func Sign8(a M128i, b M128i) M128i {
	return M128i(sign8([16]byte(a), [16]byte(b)))
}

func sign8(a [16]byte, b [16]byte) [16]byte


// SinPd: Compute the sine of packed double-precision (64-bit) floating-point
// elements in 'a' expressed in radians, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := SIN(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_sin_pd'.
// Requires SSE.
func SinPd(a M128d) M128d {
	return M128d(sinPd([2]float64(a)))
}

func sinPd(a [2]float64) [2]float64


// SinPs: Compute the sine of packed single-precision (32-bit) floating-point
// elements in 'a' expressed in radians, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := SIN(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_sin_ps'.
// Requires SSE.
func SinPs(a M128) M128 {
	return M128(sinPs([4]float32(a)))
}

func sinPs(a [4]float32) [4]float32


// SincosPd: Compute the sine and cosine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, store the sine in
// 'dst', and store the cosine into memory at 'mem_addr'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := SIN(a[i+63:i])
//			MEM[mem_addr+i+63:mem_addr+i] := COS(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_sincos_pd'.
// Requires SSE.
func SincosPd(mem_addr M128d, a M128d) M128d {
	return M128d(sincosPd([2]float64(mem_addr), [2]float64(a)))
}

func sincosPd(mem_addr [2]float64, a [2]float64) [2]float64


// SincosPs: Compute the sine and cosine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, store the sine in
// 'dst', and store the cosine into memory at 'mem_addr'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := SIN(a[i+31:i])
//			MEM[mem_addr+i+31:mem_addr+i] := COS(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_sincos_ps'.
// Requires SSE.
func SincosPs(mem_addr M128, a M128) M128 {
	return M128(sincosPs([4]float32(mem_addr), [4]float32(a)))
}

func sincosPs(mem_addr [4]float32, a [4]float32) [4]float32


// SindPd: Compute the sine of packed double-precision (64-bit) floating-point
// elements in 'a' expressed in degrees, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := SIND(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_sind_pd'.
// Requires SSE.
func SindPd(a M128d) M128d {
	return M128d(sindPd([2]float64(a)))
}

func sindPd(a [2]float64) [2]float64


// SindPs: Compute the sine of packed single-precision (32-bit) floating-point
// elements in 'a' expressed in degrees, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := SIND(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_sind_ps'.
// Requires SSE.
func SindPs(a M128) M128 {
	return M128(sindPs([4]float32(a)))
}

func sindPs(a [4]float32) [4]float32


// SinhPd: Compute the hyperbolic sine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := SINH(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_sinh_pd'.
// Requires SSE.
func SinhPd(a M128d) M128d {
	return M128d(sinhPd([2]float64(a)))
}

func sinhPd(a [2]float64) [2]float64


// SinhPs: Compute the hyperbolic sine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := SINH(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_sinh_ps'.
// Requires SSE.
func SinhPs(a M128) M128 {
	return M128(sinhPs([4]float32(a)))
}

func sinhPs(a [4]float32) [4]float32


// MaskSll16: Shift packed 16-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				IF count[63:0] > 15
//					dst[i+15:i] := 0
//				ELSE
//					dst[i+15:i] := ZeroExtend(a[i+15:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSLLW'. Intrinsic: '_mm_mask_sll_epi16'.
// Requires AVX512BW.
func MaskSll16(src M128i, k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskSll16([16]byte(src), uint8(k), [16]byte(a), [16]byte(count)))
}

func maskSll16(src [16]byte, k uint8, a [16]byte, count [16]byte) [16]byte


// MaskzSll16: Shift packed 16-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				IF count[63:0] > 15
//					dst[i+15:i] := 0
//				ELSE
//					dst[i+15:i] := ZeroExtend(a[i+15:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSLLW'. Intrinsic: '_mm_maskz_sll_epi16'.
// Requires AVX512BW.
func MaskzSll16(k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskzSll16(uint8(k), [16]byte(a), [16]byte(count)))
}

func maskzSll16(k uint8, a [16]byte, count [16]byte) [16]byte


// Sll16: Shift packed 16-bit integers in 'a' left by 'count' while shifting in
// zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF count[63:0] > 15
//				dst[i+15:i] := 0
//			ELSE
//				dst[i+15:i] := ZeroExtend(a[i+15:i] << count[63:0])
//			FI
//		ENDFOR
//
// Instruction: 'PSLLW'. Intrinsic: '_mm_sll_epi16'.
// Requires SSE2.
func Sll16(a M128i, count M128i) M128i {
	return M128i(sll16([16]byte(a), [16]byte(count)))
}

func sll16(a [16]byte, count [16]byte) [16]byte


// MaskSll32: Shift packed 32-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSLLD'. Intrinsic: '_mm_mask_sll_epi32'.
// Requires AVX512F.
func MaskSll32(src M128i, k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskSll32([16]byte(src), uint8(k), [16]byte(a), [16]byte(count)))
}

func maskSll32(src [16]byte, k uint8, a [16]byte, count [16]byte) [16]byte


// MaskzSll32: Shift packed 32-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSLLD'. Intrinsic: '_mm_maskz_sll_epi32'.
// Requires AVX512F.
func MaskzSll32(k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskzSll32(uint8(k), [16]byte(a), [16]byte(count)))
}

func maskzSll32(k uint8, a [16]byte, count [16]byte) [16]byte


// Sll32: Shift packed 32-bit integers in 'a' left by 'count' while shifting in
// zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF count[63:0] > 31
//				dst[i+31:i] := 0
//			ELSE
//				dst[i+31:i] := ZeroExtend(a[i+31:i] << count[63:0])
//			FI
//		ENDFOR
//
// Instruction: 'PSLLD'. Intrinsic: '_mm_sll_epi32'.
// Requires SSE2.
func Sll32(a M128i, count M128i) M128i {
	return M128i(sll32([16]byte(a), [16]byte(count)))
}

func sll32(a [16]byte, count [16]byte) [16]byte


// MaskSll64: Shift packed 64-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm_mask_sll_epi64'.
// Requires AVX512F.
func MaskSll64(src M128i, k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskSll64([16]byte(src), uint8(k), [16]byte(a), [16]byte(count)))
}

func maskSll64(src [16]byte, k uint8, a [16]byte, count [16]byte) [16]byte


// MaskzSll64: Shift packed 64-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm_maskz_sll_epi64'.
// Requires AVX512F.
func MaskzSll64(k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskzSll64(uint8(k), [16]byte(a), [16]byte(count)))
}

func maskzSll64(k uint8, a [16]byte, count [16]byte) [16]byte


// Sll64: Shift packed 64-bit integers in 'a' left by 'count' while shifting in
// zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF count[63:0] > 63
//				dst[i+63:i] := 0
//			ELSE
//				dst[i+63:i] := ZeroExtend(a[i+63:i] << count[63:0])
//			FI
//		ENDFOR
//
// Instruction: 'PSLLQ'. Intrinsic: '_mm_sll_epi64'.
// Requires SSE2.
func Sll64(a M128i, count M128i) M128i {
	return M128i(sll64([16]byte(a), [16]byte(count)))
}

func sll64(a [16]byte, count [16]byte) [16]byte


// MaskSlli16: Shift packed 16-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				IF imm8[7:0] > 15
//					dst[i+15:i] := 0
//				ELSE
//					dst[i+15:i] := ZeroExtend(a[i+15:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSLLW'. Intrinsic: '_mm_mask_slli_epi16'.
// Requires AVX512BW.
func MaskSlli16(src M128i, k Mmask8, a M128i, imm8 uint32) M128i {
	return M128i(maskSlli16([16]byte(src), uint8(k), [16]byte(a), imm8))
}

func maskSlli16(src [16]byte, k uint8, a [16]byte, imm8 uint32) [16]byte


// MaskzSlli16: Shift packed 16-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				IF imm8[7:0] > 15
//					dst[i+15:i] := 0
//				ELSE
//					dst[i+15:i] := ZeroExtend(a[i+15:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSLLW'. Intrinsic: '_mm_maskz_slli_epi16'.
// Requires AVX512BW.
func MaskzSlli16(k Mmask8, a M128i, imm8 uint32) M128i {
	return M128i(maskzSlli16(uint8(k), [16]byte(a), imm8))
}

func maskzSlli16(k uint8, a [16]byte, imm8 uint32) [16]byte


// Slli16: Shift packed 16-bit integers in 'a' left by 'imm8' while shifting in
// zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF imm8[7:0] > 15
//				dst[i+15:i] := 0
//			ELSE
//				dst[i+15:i] := ZeroExtend(a[i+15:i] << imm8[7:0])
//			FI
//		ENDFOR
//
// Instruction: 'PSLLW'. Intrinsic: '_mm_slli_epi16'.
// Requires SSE2.
func Slli16(a M128i, imm8 int) M128i {
	return M128i(slli16([16]byte(a), imm8))
}

func slli16(a [16]byte, imm8 int) [16]byte


// MaskSlli32: Shift packed 32-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSLLD'. Intrinsic: '_mm_mask_slli_epi32'.
// Requires AVX512F.
func MaskSlli32(src M128i, k Mmask8, a M128i, imm8 uint32) M128i {
	return M128i(maskSlli32([16]byte(src), uint8(k), [16]byte(a), imm8))
}

func maskSlli32(src [16]byte, k uint8, a [16]byte, imm8 uint32) [16]byte


// MaskzSlli32: Shift packed 32-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSLLD'. Intrinsic: '_mm_maskz_slli_epi32'.
// Requires AVX512F.
func MaskzSlli32(k Mmask8, a M128i, imm8 uint32) M128i {
	return M128i(maskzSlli32(uint8(k), [16]byte(a), imm8))
}

func maskzSlli32(k uint8, a [16]byte, imm8 uint32) [16]byte


// Slli32: Shift packed 32-bit integers in 'a' left by 'imm8' while shifting in
// zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF imm8[7:0] > 31
//				dst[i+31:i] := 0
//			ELSE
//				dst[i+31:i] := ZeroExtend(a[i+31:i] << imm8[7:0])
//			FI
//		ENDFOR
//
// Instruction: 'PSLLD'. Intrinsic: '_mm_slli_epi32'.
// Requires SSE2.
func Slli32(a M128i, imm8 int) M128i {
	return M128i(slli32([16]byte(a), imm8))
}

func slli32(a [16]byte, imm8 int) [16]byte


// MaskSlli64: Shift packed 64-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm_mask_slli_epi64'.
// Requires AVX512F.
func MaskSlli64(src M128i, k Mmask8, a M128i, imm8 uint32) M128i {
	return M128i(maskSlli64([16]byte(src), uint8(k), [16]byte(a), imm8))
}

func maskSlli64(src [16]byte, k uint8, a [16]byte, imm8 uint32) [16]byte


// MaskzSlli64: Shift packed 64-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm_maskz_slli_epi64'.
// Requires AVX512F.
func MaskzSlli64(k Mmask8, a M128i, imm8 uint32) M128i {
	return M128i(maskzSlli64(uint8(k), [16]byte(a), imm8))
}

func maskzSlli64(k uint8, a [16]byte, imm8 uint32) [16]byte


// Slli64: Shift packed 64-bit integers in 'a' left by 'imm8' while shifting in
// zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF imm8[7:0] > 63
//				dst[i+63:i] := 0
//			ELSE
//				dst[i+63:i] := ZeroExtend(a[i+63:i] << imm8[7:0])
//			FI
//		ENDFOR
//
// Instruction: 'PSLLQ'. Intrinsic: '_mm_slli_epi64'.
// Requires SSE2.
func Slli64(a M128i, imm8 int) M128i {
	return M128i(slli64([16]byte(a), imm8))
}

func slli64(a [16]byte, imm8 int) [16]byte


// SlliSi128: Shift 'a' left by 'imm8' bytes while shifting in zeros, and store
// the results in 'dst'. 
//
//		tmp := imm8[7:0]
//		IF tmp > 15
//			tmp := 16
//		FI
//		dst[127:0] := a[127:0] << (tmp*8)
//
// Instruction: 'PSLLDQ'. Intrinsic: '_mm_slli_si128'.
// Requires SSE2.
func SlliSi128(a M128i, imm8 int) M128i {
	return M128i(slliSi128([16]byte(a), imm8))
}

func slliSi128(a [16]byte, imm8 int) [16]byte


// MaskSllv16: Shift packed 16-bit integers in 'a' left by the amount specified
// by the corresponding element in 'count' while shifting in zeros, and store
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := ZeroExtend(a[i+15:i] << count[i+15:i])
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSLLVW'. Intrinsic: '_mm_mask_sllv_epi16'.
// Requires AVX512BW.
func MaskSllv16(src M128i, k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskSllv16([16]byte(src), uint8(k), [16]byte(a), [16]byte(count)))
}

func maskSllv16(src [16]byte, k uint8, a [16]byte, count [16]byte) [16]byte


// MaskzSllv16: Shift packed 16-bit integers in 'a' left by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := ZeroExtend(a[i+15:i] << count[i+15:i])
//			ELSE
//				dst[i+15:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSLLVW'. Intrinsic: '_mm_maskz_sllv_epi16'.
// Requires AVX512BW.
func MaskzSllv16(k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskzSllv16(uint8(k), [16]byte(a), [16]byte(count)))
}

func maskzSllv16(k uint8, a [16]byte, count [16]byte) [16]byte


// Sllv16: Shift packed 16-bit integers in 'a' left by the amount specified by
// the corresponding element in 'count' while shifting in zeros, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := ZeroExtend(a[i+15:i] << count[i+15:i])
//			ELSE
//				dst[i+15:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSLLVW'. Intrinsic: '_mm_sllv_epi16'.
// Requires AVX512BW.
func Sllv16(a M128i, count M128i) M128i {
	return M128i(sllv16([16]byte(a), [16]byte(count)))
}

func sllv16(a [16]byte, count [16]byte) [16]byte


// MaskSllv32: Shift packed 32-bit integers in 'a' left by the amount specified
// by the corresponding element in 'count' while shifting in zeros, and store
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[i+31:i] << count[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSLLVD'. Intrinsic: '_mm_mask_sllv_epi32'.
// Requires AVX512F.
func MaskSllv32(src M128i, k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskSllv32([16]byte(src), uint8(k), [16]byte(a), [16]byte(count)))
}

func maskSllv32(src [16]byte, k uint8, a [16]byte, count [16]byte) [16]byte


// MaskzSllv32: Shift packed 32-bit integers in 'a' left by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[i+31:i] << count[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSLLVD'. Intrinsic: '_mm_maskz_sllv_epi32'.
// Requires AVX512F.
func MaskzSllv32(k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskzSllv32(uint8(k), [16]byte(a), [16]byte(count)))
}

func maskzSllv32(k uint8, a [16]byte, count [16]byte) [16]byte


// Sllv32: Shift packed 32-bit integers in 'a' left by the amount specified by
// the corresponding element in 'count' while shifting in zeros, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := ZeroExtend(a[i+31:i] << count[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSLLVD'. Intrinsic: '_mm_sllv_epi32'.
// Requires AVX2.
func Sllv32(a M128i, count M128i) M128i {
	return M128i(sllv32([16]byte(a), [16]byte(count)))
}

func sllv32(a [16]byte, count [16]byte) [16]byte


// MaskSllv64: Shift packed 64-bit integers in 'a' left by the amount specified
// by the corresponding element in 'count' while shifting in zeros, and store
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[i+63:i] << count[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSLLVQ'. Intrinsic: '_mm_mask_sllv_epi64'.
// Requires AVX512F.
func MaskSllv64(src M128i, k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskSllv64([16]byte(src), uint8(k), [16]byte(a), [16]byte(count)))
}

func maskSllv64(src [16]byte, k uint8, a [16]byte, count [16]byte) [16]byte


// MaskzSllv64: Shift packed 64-bit integers in 'a' left by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[i+63:i] << count[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSLLVQ'. Intrinsic: '_mm_maskz_sllv_epi64'.
// Requires AVX512F.
func MaskzSllv64(k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskzSllv64(uint8(k), [16]byte(a), [16]byte(count)))
}

func maskzSllv64(k uint8, a [16]byte, count [16]byte) [16]byte


// Sllv64: Shift packed 64-bit integers in 'a' left by the amount specified by
// the corresponding element in 'count' while shifting in zeros, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := ZeroExtend(a[i+63:i] << count[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSLLVQ'. Intrinsic: '_mm_sllv_epi64'.
// Requires AVX2.
func Sllv64(a M128i, count M128i) M128i {
	return M128i(sllv64([16]byte(a), [16]byte(count)))
}

func sllv64(a [16]byte, count [16]byte) [16]byte


// MaskSqrtPd: Compute the square root of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SQRT(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSQRTPD'. Intrinsic: '_mm_mask_sqrt_pd'.
// Requires AVX512F.
func MaskSqrtPd(src M128d, k Mmask8, a M128d) M128d {
	return M128d(maskSqrtPd([2]float64(src), uint8(k), [2]float64(a)))
}

func maskSqrtPd(src [2]float64, k uint8, a [2]float64) [2]float64


// MaskzSqrtPd: Compute the square root of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SQRT(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSQRTPD'. Intrinsic: '_mm_maskz_sqrt_pd'.
// Requires AVX512F.
func MaskzSqrtPd(k Mmask8, a M128d) M128d {
	return M128d(maskzSqrtPd(uint8(k), [2]float64(a)))
}

func maskzSqrtPd(k uint8, a [2]float64) [2]float64


// SqrtPd: Compute the square root of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := SQRT(a[i+63:i])
//		ENDFOR
//
// Instruction: 'SQRTPD'. Intrinsic: '_mm_sqrt_pd'.
// Requires SSE2.
func SqrtPd(a M128d) M128d {
	return M128d(sqrtPd([2]float64(a)))
}

func sqrtPd(a [2]float64) [2]float64


// MaskSqrtPs: Compute the square root of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SQRT(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSQRTPS'. Intrinsic: '_mm_mask_sqrt_ps'.
// Requires AVX512F.
func MaskSqrtPs(src M128, k Mmask8, a M128) M128 {
	return M128(maskSqrtPs([4]float32(src), uint8(k), [4]float32(a)))
}

func maskSqrtPs(src [4]float32, k uint8, a [4]float32) [4]float32


// MaskzSqrtPs: Compute the square root of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SQRT(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSQRTPS'. Intrinsic: '_mm_maskz_sqrt_ps'.
// Requires AVX512F.
func MaskzSqrtPs(k Mmask8, a M128) M128 {
	return M128(maskzSqrtPs(uint8(k), [4]float32(a)))
}

func maskzSqrtPs(k uint8, a [4]float32) [4]float32


// SqrtPs: Compute the square root of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := SQRT(a[i+31:i])
//		ENDFOR
//
// Instruction: 'SQRTPS'. Intrinsic: '_mm_sqrt_ps'.
// Requires SSE.
func SqrtPs(a M128) M128 {
	return M128(sqrtPs([4]float32(a)))
}

func sqrtPs(a [4]float32) [4]float32


// MaskSqrtRoundSd: Compute the square root of the lower double-precision
// (64-bit) floating-point element in 'a', store the result in the lower
// element of 'dst' using writemask 'k' (the element is copied from 'src' when
// mask bit 0 is not set), and copy the upper element from 'b' to the upper
// element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := SQRT(a[63:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSQRTSD'. Intrinsic: '_mm_mask_sqrt_round_sd'.
// Requires AVX512F.
func MaskSqrtRoundSd(src M128d, k Mmask8, a M128d, b M128d, rounding int) M128d {
	return M128d(maskSqrtRoundSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), rounding))
}

func maskSqrtRoundSd(src [2]float64, k uint8, a [2]float64, b [2]float64, rounding int) [2]float64


// MaskzSqrtRoundSd: Compute the square root of the lower double-precision
// (64-bit) floating-point element in 'a', store the result in the lower
// element of 'dst' using zeromask 'k' (the element is zeroed out when mask bit
// 0 is not set), and copy the upper element from 'b' to the upper element of
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := SQRT(a[63:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSQRTSD'. Intrinsic: '_mm_maskz_sqrt_round_sd'.
// Requires AVX512F.
func MaskzSqrtRoundSd(k Mmask8, a M128d, b M128d, rounding int) M128d {
	return M128d(maskzSqrtRoundSd(uint8(k), [2]float64(a), [2]float64(b), rounding))
}

func maskzSqrtRoundSd(k uint8, a [2]float64, b [2]float64, rounding int) [2]float64


// SqrtRoundSd: Compute the square root of the lower double-precision (64-bit)
// floating-point element in 'a', store the result in the lower element of
// 'dst', and copy the upper element from 'b' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := SQRT(a[63:0])
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSQRTSD'. Intrinsic: '_mm_sqrt_round_sd'.
// Requires AVX512F.
func SqrtRoundSd(a M128d, b M128d, rounding int) M128d {
	return M128d(sqrtRoundSd([2]float64(a), [2]float64(b), rounding))
}

func sqrtRoundSd(a [2]float64, b [2]float64, rounding int) [2]float64


// MaskSqrtRoundSs: Compute the square root of the lower single-precision
// (32-bit) floating-point element in 'a', store the result in the lower
// element of 'dst' using writemask 'k' (the element is copied from 'src' when
// mask bit 0 is not set), and copy the upper 3 packed elements from 'b' to the
// upper elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := SQRT(a[31:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSQRTSS'. Intrinsic: '_mm_mask_sqrt_round_ss'.
// Requires AVX512F.
func MaskSqrtRoundSs(src M128, k Mmask8, a M128, b M128, rounding int) M128 {
	return M128(maskSqrtRoundSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), rounding))
}

func maskSqrtRoundSs(src [4]float32, k uint8, a [4]float32, b [4]float32, rounding int) [4]float32


// MaskzSqrtRoundSs: Compute the square root of the lower single-precision
// (32-bit) floating-point element in 'a', store the result in the lower
// element of 'dst' using zeromask 'k' (the element is zeroed out when mask bit
// 0 is not set), and copy the upper 3 packed elements from 'b' to the upper
// elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := SQRT(a[31:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSQRTSS'. Intrinsic: '_mm_maskz_sqrt_round_ss'.
// Requires AVX512F.
func MaskzSqrtRoundSs(k Mmask8, a M128, b M128, rounding int) M128 {
	return M128(maskzSqrtRoundSs(uint8(k), [4]float32(a), [4]float32(b), rounding))
}

func maskzSqrtRoundSs(k uint8, a [4]float32, b [4]float32, rounding int) [4]float32


// SqrtRoundSs: Compute the square root of the lower single-precision (32-bit)
// floating-point element in 'a', store the result in the lower element of
// 'dst', and copy the upper 3 packed elements from 'b' to the upper elements
// of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := SQRT(a[31:0])
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSQRTSS'. Intrinsic: '_mm_sqrt_round_ss'.
// Requires AVX512F.
func SqrtRoundSs(a M128, b M128, rounding int) M128 {
	return M128(sqrtRoundSs([4]float32(a), [4]float32(b), rounding))
}

func sqrtRoundSs(a [4]float32, b [4]float32, rounding int) [4]float32


// MaskSqrtSd: Compute the square root of the lower double-precision (64-bit)
// floating-point element in 'a', store the result in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'src' when mask bit 0
// is not set), and copy the upper element from 'b' to the upper element of
// 'dst'. 
//
//		IF k[0]
//			dst[63:0] := SQRT(a[63:0])
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSQRTSD'. Intrinsic: '_mm_mask_sqrt_sd'.
// Requires AVX512F.
func MaskSqrtSd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskSqrtSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskSqrtSd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzSqrtSd: Compute the square root of the lower double-precision (64-bit)
// floating-point element in 'a', store the result in the lower element of
// 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is not
// set), and copy the upper element from 'b' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := SQRT(a[63:0])
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := b[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSQRTSD'. Intrinsic: '_mm_maskz_sqrt_sd'.
// Requires AVX512F.
func MaskzSqrtSd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzSqrtSd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzSqrtSd(k uint8, a [2]float64, b [2]float64) [2]float64


// SqrtSd: Compute the square root of the lower double-precision (64-bit)
// floating-point element in 'a', store the result in the lower element of
// 'dst', and copy the upper element from 'b' to the upper element of 'dst'. 
//
//		dst[63:0] := SQRT(a[63:0])
//		dst[127:64] := b[127:64]
//
// Instruction: 'SQRTSD'. Intrinsic: '_mm_sqrt_sd'.
// Requires SSE2.
func SqrtSd(a M128d, b M128d) M128d {
	return M128d(sqrtSd([2]float64(a), [2]float64(b)))
}

func sqrtSd(a [2]float64, b [2]float64) [2]float64


// MaskSqrtSs: Compute the square root of the lower single-precision (32-bit)
// floating-point element in 'a', store the result in the lower element of
// 'dst' using writemask 'k' (the element is copied from 'src' when mask bit 0
// is not set), and copy the upper 3 packed elements from 'b' to the upper
// elements of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := SQRT(a[31:0])
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSQRTSS'. Intrinsic: '_mm_mask_sqrt_ss'.
// Requires AVX512F.
func MaskSqrtSs(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskSqrtSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskSqrtSs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzSqrtSs: Compute the square root of the lower single-precision (32-bit)
// floating-point element in 'a', store the result in the lower element of
// 'dst' using zeromask 'k' (the element is zeroed out when mask bit 0 is not
// set), and copy the upper 3 packed elements from 'b' to the upper elements of
// 'dst'. 
//
//		IF k[0]
//			dst[31:0] := SQRT(a[31:0])
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := b[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSQRTSS'. Intrinsic: '_mm_maskz_sqrt_ss'.
// Requires AVX512F.
func MaskzSqrtSs(k Mmask8, a M128, b M128) M128 {
	return M128(maskzSqrtSs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzSqrtSs(k uint8, a [4]float32, b [4]float32) [4]float32


// SqrtSs: Compute the square root of the lower single-precision (32-bit)
// floating-point element in 'a', store the result in the lower element of
// 'dst', and copy the upper 3 packed elements from 'a' to the upper elements
// of 'dst'. 
//
//		dst[31:0] := SQRT(a[31:0])
//		dst[127:32] := a[127:32]
//
// Instruction: 'SQRTSS'. Intrinsic: '_mm_sqrt_ss'.
// Requires SSE.
func SqrtSs(a M128) M128 {
	return M128(sqrtSs([4]float32(a)))
}

func sqrtSs(a [4]float32) [4]float32


// MaskSra16: Shift packed 16-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				IF count[63:0] > 15
//					dst[i+15:i] := SignBit
//				ELSE
//					dst[i+15:i] := SignExtend(a[i+15:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAW'. Intrinsic: '_mm_mask_sra_epi16'.
// Requires AVX512BW.
func MaskSra16(src M128i, k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskSra16([16]byte(src), uint8(k), [16]byte(a), [16]byte(count)))
}

func maskSra16(src [16]byte, k uint8, a [16]byte, count [16]byte) [16]byte


// MaskzSra16: Shift packed 16-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				IF count[63:0] > 15
//					dst[i+15:i] := SignBit
//				ELSE
//					dst[i+15:i] := SignExtend(a[i+15:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAW'. Intrinsic: '_mm_maskz_sra_epi16'.
// Requires AVX512BW.
func MaskzSra16(k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskzSra16(uint8(k), [16]byte(a), [16]byte(count)))
}

func maskzSra16(k uint8, a [16]byte, count [16]byte) [16]byte


// Sra16: Shift packed 16-bit integers in 'a' right by 'count' while shifting
// in sign bits, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF count[63:0] > 15
//				dst[i+15:i] := SignBit
//			ELSE
//				dst[i+15:i] := SignExtend(a[i+15:i] >> count[63:0])
//			FI
//		ENDFOR
//
// Instruction: 'PSRAW'. Intrinsic: '_mm_sra_epi16'.
// Requires SSE2.
func Sra16(a M128i, count M128i) M128i {
	return M128i(sra16([16]byte(a), [16]byte(count)))
}

func sra16(a [16]byte, count [16]byte) [16]byte


// MaskSra32: Shift packed 32-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := SignBit
//				ELSE
//					dst[i+31:i] := SignExtend(a[i+31:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAD'. Intrinsic: '_mm_mask_sra_epi32'.
// Requires AVX512F.
func MaskSra32(src M128i, k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskSra32([16]byte(src), uint8(k), [16]byte(a), [16]byte(count)))
}

func maskSra32(src [16]byte, k uint8, a [16]byte, count [16]byte) [16]byte


// MaskzSra32: Shift packed 32-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := SignBit
//				ELSE
//					dst[i+31:i] := SignExtend(a[i+31:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAD'. Intrinsic: '_mm_maskz_sra_epi32'.
// Requires AVX512F.
func MaskzSra32(k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskzSra32(uint8(k), [16]byte(a), [16]byte(count)))
}

func maskzSra32(k uint8, a [16]byte, count [16]byte) [16]byte


// Sra32: Shift packed 32-bit integers in 'a' right by 'count' while shifting
// in sign bits, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF count[63:0] > 31
//				dst[i+31:i] := SignBit
//			ELSE
//				dst[i+31:i] := SignExtend(a[i+31:i] >> count[63:0])
//			FI
//		ENDFOR
//
// Instruction: 'PSRAD'. Intrinsic: '_mm_sra_epi32'.
// Requires SSE2.
func Sra32(a M128i, count M128i) M128i {
	return M128i(sra32([16]byte(a), [16]byte(count)))
}

func sra32(a [16]byte, count [16]byte) [16]byte


// MaskSra64: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := SignBit
//				ELSE
//					dst[i+63:i] := SignExtend(a[i+63:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm_mask_sra_epi64'.
// Requires AVX512F.
func MaskSra64(src M128i, k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskSra64([16]byte(src), uint8(k), [16]byte(a), [16]byte(count)))
}

func maskSra64(src [16]byte, k uint8, a [16]byte, count [16]byte) [16]byte


// MaskzSra64: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := SignBit
//				ELSE
//					dst[i+63:i] := SignExtend(a[i+63:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm_maskz_sra_epi64'.
// Requires AVX512F.
func MaskzSra64(k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskzSra64(uint8(k), [16]byte(a), [16]byte(count)))
}

func maskzSra64(k uint8, a [16]byte, count [16]byte) [16]byte


// Sra64: Shift packed 64-bit integers in 'a' right by 'count' while shifting
// in sign bits, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF count[63:0] > 63
//				dst[i+63:i] := SignBit
//			ELSE
//				dst[i+63:i] := SignExtend(a[i+63:i] >> count[63:0])
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm_sra_epi64'.
// Requires AVX512F.
func Sra64(a M128i, count M128i) M128i {
	return M128i(sra64([16]byte(a), [16]byte(count)))
}

func sra64(a [16]byte, count [16]byte) [16]byte


// MaskSrai16: Shift packed 16-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				IF imm8[7:0] > 15
//					dst[i+15:i] := SignBit
//				ELSE
//					dst[i+15:i] := SignExtend(a[i+15:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAW'. Intrinsic: '_mm_mask_srai_epi16'.
// Requires AVX512BW.
func MaskSrai16(src M128i, k Mmask8, a M128i, imm8 uint32) M128i {
	return M128i(maskSrai16([16]byte(src), uint8(k), [16]byte(a), imm8))
}

func maskSrai16(src [16]byte, k uint8, a [16]byte, imm8 uint32) [16]byte


// MaskzSrai16: Shift packed 16-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				IF imm8[7:0] > 15
//					dst[i+15:i] := SignBit
//				ELSE
//					dst[i+15:i] := SignExtend(a[i+15:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAW'. Intrinsic: '_mm_maskz_srai_epi16'.
// Requires AVX512BW.
func MaskzSrai16(k Mmask8, a M128i, imm8 uint32) M128i {
	return M128i(maskzSrai16(uint8(k), [16]byte(a), imm8))
}

func maskzSrai16(k uint8, a [16]byte, imm8 uint32) [16]byte


// Srai16: Shift packed 16-bit integers in 'a' right by 'imm8' while shifting
// in sign bits, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF imm8[7:0] > 15
//				dst[i+15:i] := SignBit
//			ELSE
//				dst[i+15:i] := SignExtend(a[i+15:i] >> imm8[7:0])
//			FI
//		ENDFOR
//
// Instruction: 'PSRAW'. Intrinsic: '_mm_srai_epi16'.
// Requires SSE2.
func Srai16(a M128i, imm8 int) M128i {
	return M128i(srai16([16]byte(a), imm8))
}

func srai16(a [16]byte, imm8 int) [16]byte


// MaskSrai32: Shift packed 32-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := SignBit
//				ELSE
//					dst[i+31:i] := SignExtend(a[i+31:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAD'. Intrinsic: '_mm_mask_srai_epi32'.
// Requires AVX512F.
func MaskSrai32(src M128i, k Mmask8, a M128i, imm8 uint32) M128i {
	return M128i(maskSrai32([16]byte(src), uint8(k), [16]byte(a), imm8))
}

func maskSrai32(src [16]byte, k uint8, a [16]byte, imm8 uint32) [16]byte


// MaskzSrai32: Shift packed 32-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := SignBit
//				ELSE
//					dst[i+31:i] := SignExtend(a[i+31:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAD'. Intrinsic: '_mm_maskz_srai_epi32'.
// Requires AVX512F.
func MaskzSrai32(k Mmask8, a M128i, imm8 uint32) M128i {
	return M128i(maskzSrai32(uint8(k), [16]byte(a), imm8))
}

func maskzSrai32(k uint8, a [16]byte, imm8 uint32) [16]byte


// Srai32: Shift packed 32-bit integers in 'a' right by 'imm8' while shifting
// in sign bits, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF imm8[7:0] > 31
//				dst[i+31:i] := SignBit
//			ELSE
//				dst[i+31:i] := SignExtend(a[i+31:i] >> imm8[7:0])
//			FI
//		ENDFOR
//
// Instruction: 'PSRAD'. Intrinsic: '_mm_srai_epi32'.
// Requires SSE2.
func Srai32(a M128i, imm8 int) M128i {
	return M128i(srai32([16]byte(a), imm8))
}

func srai32(a [16]byte, imm8 int) [16]byte


// MaskSrai64: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := SignBit
//				ELSE
//					dst[i+63:i] := SignExtend(a[i+63:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm_mask_srai_epi64'.
// Requires AVX512F.
func MaskSrai64(src M128i, k Mmask8, a M128i, imm8 uint32) M128i {
	return M128i(maskSrai64([16]byte(src), uint8(k), [16]byte(a), imm8))
}

func maskSrai64(src [16]byte, k uint8, a [16]byte, imm8 uint32) [16]byte


// MaskzSrai64: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := SignBit
//				ELSE
//					dst[i+63:i] := SignExtend(a[i+63:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm_maskz_srai_epi64'.
// Requires AVX512F.
func MaskzSrai64(k Mmask8, a M128i, imm8 uint32) M128i {
	return M128i(maskzSrai64(uint8(k), [16]byte(a), imm8))
}

func maskzSrai64(k uint8, a [16]byte, imm8 uint32) [16]byte


// Srai64: Shift packed 64-bit integers in 'a' right by 'imm8' while shifting
// in sign bits, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF imm8[7:0] > 63
//				dst[i+63:i] := SignBit
//			ELSE
//				dst[i+63:i] := SignExtend(a[i+63:i] << imm8[7:0])
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm_srai_epi64'.
// Requires AVX512F.
func Srai64(a M128i, imm8 uint32) M128i {
	return M128i(srai64([16]byte(a), imm8))
}

func srai64(a [16]byte, imm8 uint32) [16]byte


// MaskSrav16: Shift packed 16-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := SignExtend(a[i+15:i] >> count[i+15:i])
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAVW'. Intrinsic: '_mm_mask_srav_epi16'.
// Requires AVX512BW.
func MaskSrav16(src M128i, k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskSrav16([16]byte(src), uint8(k), [16]byte(a), [16]byte(count)))
}

func maskSrav16(src [16]byte, k uint8, a [16]byte, count [16]byte) [16]byte


// MaskzSrav16: Shift packed 16-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := SignExtend(a[i+15:i] >> count[i+15:i])
//			ELSE
//				dst[i+15:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAVW'. Intrinsic: '_mm_maskz_srav_epi16'.
// Requires AVX512BW.
func MaskzSrav16(k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskzSrav16(uint8(k), [16]byte(a), [16]byte(count)))
}

func maskzSrav16(k uint8, a [16]byte, count [16]byte) [16]byte


// Srav16: Shift packed 16-bit integers in 'a' right by the amount specified by
// the corresponding element in 'count' while shifting in sign bits, and store
// the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			dst[i+15:i] := SignExtend(a[i+15:i] >> count[i+15:i])	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAVW'. Intrinsic: '_mm_srav_epi16'.
// Requires AVX512BW.
func Srav16(a M128i, count M128i) M128i {
	return M128i(srav16([16]byte(a), [16]byte(count)))
}

func srav16(a [16]byte, count [16]byte) [16]byte


// MaskSrav32: Shift packed 32-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[i+31:i] >> count[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAVD'. Intrinsic: '_mm_mask_srav_epi32'.
// Requires AVX512F.
func MaskSrav32(src M128i, k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskSrav32([16]byte(src), uint8(k), [16]byte(a), [16]byte(count)))
}

func maskSrav32(src [16]byte, k uint8, a [16]byte, count [16]byte) [16]byte


// MaskzSrav32: Shift packed 32-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[i+31:i] >> count[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAVD'. Intrinsic: '_mm_maskz_srav_epi32'.
// Requires AVX512F.
func MaskzSrav32(k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskzSrav32(uint8(k), [16]byte(a), [16]byte(count)))
}

func maskzSrav32(k uint8, a [16]byte, count [16]byte) [16]byte


// Srav32: Shift packed 32-bit integers in 'a' right by the amount specified by
// the corresponding element in 'count' while shifting in sign bits, and store
// the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := SignExtend(a[i+31:i] >> count[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAVD'. Intrinsic: '_mm_srav_epi32'.
// Requires AVX2.
func Srav32(a M128i, count M128i) M128i {
	return M128i(srav32([16]byte(a), [16]byte(count)))
}

func srav32(a [16]byte, count [16]byte) [16]byte


// MaskSrav64: Shift packed 64-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[i+63:i] >> count[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAVQ'. Intrinsic: '_mm_mask_srav_epi64'.
// Requires AVX512F.
func MaskSrav64(src M128i, k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskSrav64([16]byte(src), uint8(k), [16]byte(a), [16]byte(count)))
}

func maskSrav64(src [16]byte, k uint8, a [16]byte, count [16]byte) [16]byte


// MaskzSrav64: Shift packed 64-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[i+63:i] >> count[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAVQ'. Intrinsic: '_mm_maskz_srav_epi64'.
// Requires AVX512F.
func MaskzSrav64(k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskzSrav64(uint8(k), [16]byte(a), [16]byte(count)))
}

func maskzSrav64(k uint8, a [16]byte, count [16]byte) [16]byte


// Srav64: Shift packed 64-bit integers in 'a' right by the amount specified by
// the corresponding element in 'count' while shifting in sign bits, and store
// the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := SignExtend(a[i+63:i] >> count[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRAVQ'. Intrinsic: '_mm_srav_epi64'.
// Requires AVX512F.
func Srav64(a M128i, count M128i) M128i {
	return M128i(srav64([16]byte(a), [16]byte(count)))
}

func srav64(a [16]byte, count [16]byte) [16]byte


// MaskSrl16: Shift packed 16-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				IF count[63:0] > 15
//					dst[i+15:i] := 0
//				ELSE
//					dst[i+15:i] := ZeroExtend(a[i+15:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRLW'. Intrinsic: '_mm_mask_srl_epi16'.
// Requires AVX512BW.
func MaskSrl16(src M128i, k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskSrl16([16]byte(src), uint8(k), [16]byte(a), [16]byte(count)))
}

func maskSrl16(src [16]byte, k uint8, a [16]byte, count [16]byte) [16]byte


// MaskzSrl16: Shift packed 16-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				IF count[63:0] > 15
//					dst[i+15:i] := 0
//				ELSE
//					dst[i+15:i] := ZeroExtend(a[i+15:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRLW'. Intrinsic: '_mm_maskz_srl_epi16'.
// Requires AVX512BW.
func MaskzSrl16(k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskzSrl16(uint8(k), [16]byte(a), [16]byte(count)))
}

func maskzSrl16(k uint8, a [16]byte, count [16]byte) [16]byte


// Srl16: Shift packed 16-bit integers in 'a' right by 'count' while shifting
// in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF count[63:0] > 15
//				dst[i+15:i] := 0
//			ELSE
//				dst[i+15:i] := ZeroExtend(a[i+15:i] >> count[63:0])
//			FI
//		ENDFOR
//
// Instruction: 'PSRLW'. Intrinsic: '_mm_srl_epi16'.
// Requires SSE2.
func Srl16(a M128i, count M128i) M128i {
	return M128i(srl16([16]byte(a), [16]byte(count)))
}

func srl16(a [16]byte, count [16]byte) [16]byte


// MaskSrl32: Shift packed 32-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRLD'. Intrinsic: '_mm_mask_srl_epi32'.
// Requires AVX512F.
func MaskSrl32(src M128i, k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskSrl32([16]byte(src), uint8(k), [16]byte(a), [16]byte(count)))
}

func maskSrl32(src [16]byte, k uint8, a [16]byte, count [16]byte) [16]byte


// MaskzSrl32: Shift packed 32-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRLD'. Intrinsic: '_mm_maskz_srl_epi32'.
// Requires AVX512F.
func MaskzSrl32(k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskzSrl32(uint8(k), [16]byte(a), [16]byte(count)))
}

func maskzSrl32(k uint8, a [16]byte, count [16]byte) [16]byte


// Srl32: Shift packed 32-bit integers in 'a' right by 'count' while shifting
// in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF count[63:0] > 31
//				dst[i+31:i] := 0
//			ELSE
//				dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[63:0])
//			FI
//		ENDFOR
//
// Instruction: 'PSRLD'. Intrinsic: '_mm_srl_epi32'.
// Requires SSE2.
func Srl32(a M128i, count M128i) M128i {
	return M128i(srl32([16]byte(a), [16]byte(count)))
}

func srl32(a [16]byte, count [16]byte) [16]byte


// MaskSrl64: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm_mask_srl_epi64'.
// Requires AVX512F.
func MaskSrl64(src M128i, k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskSrl64([16]byte(src), uint8(k), [16]byte(a), [16]byte(count)))
}

func maskSrl64(src [16]byte, k uint8, a [16]byte, count [16]byte) [16]byte


// MaskzSrl64: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm_maskz_srl_epi64'.
// Requires AVX512F.
func MaskzSrl64(k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskzSrl64(uint8(k), [16]byte(a), [16]byte(count)))
}

func maskzSrl64(k uint8, a [16]byte, count [16]byte) [16]byte


// Srl64: Shift packed 64-bit integers in 'a' right by 'count' while shifting
// in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF count[63:0] > 63
//				dst[i+63:i] := 0
//			ELSE
//				dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[63:0])
//			FI
//		ENDFOR
//
// Instruction: 'PSRLQ'. Intrinsic: '_mm_srl_epi64'.
// Requires SSE2.
func Srl64(a M128i, count M128i) M128i {
	return M128i(srl64([16]byte(a), [16]byte(count)))
}

func srl64(a [16]byte, count [16]byte) [16]byte


// MaskSrli16: Shift packed 16-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				IF imm8[7:0] > 15
//					dst[i+15:i] := 0
//				ELSE
//					dst[i+15:i] := ZeroExtend(a[i+15:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRLW'. Intrinsic: '_mm_mask_srli_epi16'.
// Requires AVX512BW.
func MaskSrli16(src M128i, k Mmask8, a M128i, imm8 int) M128i {
	return M128i(maskSrli16([16]byte(src), uint8(k), [16]byte(a), imm8))
}

func maskSrli16(src [16]byte, k uint8, a [16]byte, imm8 int) [16]byte


// MaskzSrli16: Shift packed 16-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				IF imm8[7:0] > 15
//					dst[i+15:i] := 0
//				ELSE
//					dst[i+15:i] := ZeroExtend(a[i+15:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRLW'. Intrinsic: '_mm_maskz_srli_epi16'.
// Requires AVX512BW.
func MaskzSrli16(k Mmask8, a M128i, imm8 int) M128i {
	return M128i(maskzSrli16(uint8(k), [16]byte(a), imm8))
}

func maskzSrli16(k uint8, a [16]byte, imm8 int) [16]byte


// Srli16: Shift packed 16-bit integers in 'a' right by 'imm8' while shifting
// in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF imm8[7:0] > 15
//				dst[i+15:i] := 0
//			ELSE
//				dst[i+15:i] := ZeroExtend(a[i+15:i] >> imm8[7:0])
//			FI
//		ENDFOR
//
// Instruction: 'PSRLW'. Intrinsic: '_mm_srli_epi16'.
// Requires SSE2.
func Srli16(a M128i, imm8 int) M128i {
	return M128i(srli16([16]byte(a), imm8))
}

func srli16(a [16]byte, imm8 int) [16]byte


// MaskSrli32: Shift packed 32-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRLD'. Intrinsic: '_mm_mask_srli_epi32'.
// Requires AVX512F.
func MaskSrli32(src M128i, k Mmask8, a M128i, imm8 uint32) M128i {
	return M128i(maskSrli32([16]byte(src), uint8(k), [16]byte(a), imm8))
}

func maskSrli32(src [16]byte, k uint8, a [16]byte, imm8 uint32) [16]byte


// MaskzSrli32: Shift packed 32-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRLD'. Intrinsic: '_mm_maskz_srli_epi32'.
// Requires AVX512F.
func MaskzSrli32(k Mmask8, a M128i, imm8 uint32) M128i {
	return M128i(maskzSrli32(uint8(k), [16]byte(a), imm8))
}

func maskzSrli32(k uint8, a [16]byte, imm8 uint32) [16]byte


// Srli32: Shift packed 32-bit integers in 'a' right by 'imm8' while shifting
// in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF imm8[7:0] > 31
//				dst[i+31:i] := 0
//			ELSE
//				dst[i+31:i] := ZeroExtend(a[i+31:i] >> imm8[7:0])
//			FI
//		ENDFOR
//
// Instruction: 'PSRLD'. Intrinsic: '_mm_srli_epi32'.
// Requires SSE2.
func Srli32(a M128i, imm8 int) M128i {
	return M128i(srli32([16]byte(a), imm8))
}

func srli32(a [16]byte, imm8 int) [16]byte


// MaskSrli64: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm_mask_srli_epi64'.
// Requires AVX512F.
func MaskSrli64(src M128i, k Mmask8, a M128i, imm8 uint32) M128i {
	return M128i(maskSrli64([16]byte(src), uint8(k), [16]byte(a), imm8))
}

func maskSrli64(src [16]byte, k uint8, a [16]byte, imm8 uint32) [16]byte


// MaskzSrli64: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm_maskz_srli_epi64'.
// Requires AVX512F.
func MaskzSrli64(k Mmask8, a M128i, imm8 uint32) M128i {
	return M128i(maskzSrli64(uint8(k), [16]byte(a), imm8))
}

func maskzSrli64(k uint8, a [16]byte, imm8 uint32) [16]byte


// Srli64: Shift packed 64-bit integers in 'a' right by 'imm8' while shifting
// in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF imm8[7:0] > 63
//				dst[i+63:i] := 0
//			ELSE
//				dst[i+63:i] := ZeroExtend(a[i+63:i] >> imm8[7:0])
//			FI
//		ENDFOR
//
// Instruction: 'PSRLQ'. Intrinsic: '_mm_srli_epi64'.
// Requires SSE2.
func Srli64(a M128i, imm8 int) M128i {
	return M128i(srli64([16]byte(a), imm8))
}

func srli64(a [16]byte, imm8 int) [16]byte


// SrliSi128: Shift 'a' right by 'imm8' bytes while shifting in zeros, and
// store the results in 'dst'. 
//
//		tmp := imm8[7:0]
//		IF tmp > 15
//			tmp := 16
//		FI
//		dst[127:0] := a[127:0] >> (tmp*8)
//
// Instruction: 'PSRLDQ'. Intrinsic: '_mm_srli_si128'.
// Requires SSE2.
func SrliSi128(a M128i, imm8 int) M128i {
	return M128i(srliSi128([16]byte(a), imm8))
}

func srliSi128(a [16]byte, imm8 int) [16]byte


// MaskSrlv16: Shift packed 16-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := ZeroExtend(a[i+15:i] >> count[i+15:i])
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRLVW'. Intrinsic: '_mm_mask_srlv_epi16'.
// Requires AVX512BW.
func MaskSrlv16(src M128i, k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskSrlv16([16]byte(src), uint8(k), [16]byte(a), [16]byte(count)))
}

func maskSrlv16(src [16]byte, k uint8, a [16]byte, count [16]byte) [16]byte


// MaskzSrlv16: Shift packed 16-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := ZeroExtend(a[i+15:i] >> count[i+15:i])
//			ELSE
//				dst[i+15:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRLVW'. Intrinsic: '_mm_maskz_srlv_epi16'.
// Requires AVX512BW.
func MaskzSrlv16(k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskzSrlv16(uint8(k), [16]byte(a), [16]byte(count)))
}

func maskzSrlv16(k uint8, a [16]byte, count [16]byte) [16]byte


// Srlv16: Shift packed 16-bit integers in 'a' right by the amount specified by
// the corresponding element in 'count' while shifting in zeros, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := ZeroExtend(a[i+15:i] >> count[i+15:i])
//			ELSE
//				dst[i+15:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRLVW'. Intrinsic: '_mm_srlv_epi16'.
// Requires AVX512BW.
func Srlv16(a M128i, count M128i) M128i {
	return M128i(srlv16([16]byte(a), [16]byte(count)))
}

func srlv16(a [16]byte, count [16]byte) [16]byte


// MaskSrlv32: Shift packed 32-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRLVD'. Intrinsic: '_mm_mask_srlv_epi32'.
// Requires AVX512F.
func MaskSrlv32(src M128i, k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskSrlv32([16]byte(src), uint8(k), [16]byte(a), [16]byte(count)))
}

func maskSrlv32(src [16]byte, k uint8, a [16]byte, count [16]byte) [16]byte


// MaskzSrlv32: Shift packed 32-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRLVD'. Intrinsic: '_mm_maskz_srlv_epi32'.
// Requires AVX512F.
func MaskzSrlv32(k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskzSrlv32(uint8(k), [16]byte(a), [16]byte(count)))
}

func maskzSrlv32(k uint8, a [16]byte, count [16]byte) [16]byte


// Srlv32: Shift packed 32-bit integers in 'a' right by the amount specified by
// the corresponding element in 'count' while shifting in zeros, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRLVD'. Intrinsic: '_mm_srlv_epi32'.
// Requires AVX2.
func Srlv32(a M128i, count M128i) M128i {
	return M128i(srlv32([16]byte(a), [16]byte(count)))
}

func srlv32(a [16]byte, count [16]byte) [16]byte


// MaskSrlv64: Shift packed 64-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRLVQ'. Intrinsic: '_mm_mask_srlv_epi64'.
// Requires AVX512F.
func MaskSrlv64(src M128i, k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskSrlv64([16]byte(src), uint8(k), [16]byte(a), [16]byte(count)))
}

func maskSrlv64(src [16]byte, k uint8, a [16]byte, count [16]byte) [16]byte


// MaskzSrlv64: Shift packed 64-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRLVQ'. Intrinsic: '_mm_maskz_srlv_epi64'.
// Requires AVX512F.
func MaskzSrlv64(k Mmask8, a M128i, count M128i) M128i {
	return M128i(maskzSrlv64(uint8(k), [16]byte(a), [16]byte(count)))
}

func maskzSrlv64(k uint8, a [16]byte, count [16]byte) [16]byte


// Srlv64: Shift packed 64-bit integers in 'a' right by the amount specified by
// the corresponding element in 'count' while shifting in zeros, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSRLVQ'. Intrinsic: '_mm_srlv_epi64'.
// Requires AVX2.
func Srlv64(a M128i, count M128i) M128i {
	return M128i(srlv64([16]byte(a), [16]byte(count)))
}

func srlv64(a [16]byte, count [16]byte) [16]byte


// MaskStore32: Store packed 32-bit integers from 'a' into memory using
// writemask 'k'.
// 	'mem_addr' must be aligned on a 16-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVDQA32'. Intrinsic: '_mm_mask_store_epi32'.
// Requires AVX512F.
func MaskStore32(mem_addr uintptr, k Mmask8, a M128i)  {
	maskStore32(uintptr(mem_addr), uint8(k), [16]byte(a))
}

func maskStore32(mem_addr uintptr, k uint8, a [16]byte) 


// MaskStore64: Store packed 64-bit integers from 'a' into memory using
// writemask 'k'.
// 	'mem_addr' must be aligned on a 16-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVDQA64'. Intrinsic: '_mm_mask_store_epi64'.
// Requires AVX512F.
func MaskStore64(mem_addr uintptr, k Mmask8, a M128i)  {
	maskStore64(uintptr(mem_addr), uint8(k), [16]byte(a))
}

func maskStore64(mem_addr uintptr, k uint8, a [16]byte) 


// MaskStorePd: Store packed double-precision (64-bit) floating-point elements
// from 'a' into memory using writemask 'k'.
// 	'mem_addr' must be aligned on a 16-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVAPD'. Intrinsic: '_mm_mask_store_pd'.
// Requires AVX512F.
func MaskStorePd(mem_addr uintptr, k Mmask8, a M128d)  {
	maskStorePd(uintptr(mem_addr), uint8(k), [2]float64(a))
}

func maskStorePd(mem_addr uintptr, k uint8, a [2]float64) 


// StorePd: Store 128-bits (composed of 2 packed double-precision (64-bit)
// floating-point elements) from 'a' into memory.
// 	'mem_addr' must be aligned on a 16-byte boundary or a general-protection
// exception may be generated. 
//
//		MEM[mem_addr+127:mem_addr] := a[127:0]
//
// Instruction: 'MOVAPD'. Intrinsic: '_mm_store_pd'.
// Requires SSE2.
func StorePd(mem_addr float64, a M128d)  {
	storePd(mem_addr, [2]float64(a))
}

func storePd(mem_addr float64, a [2]float64) 


// StorePd1: Store the lower double-precision (64-bit) floating-point element
// from 'a' into 2 contiguous elements in memory. 'mem_addr' must be aligned on
// a 16-byte boundary or a general-protection exception may be generated. 
//
//		MEM[mem_addr+63:mem_addr] := a[63:0]
//		MEM[mem_addr+127:mem_addr+64] := a[63:0]
//
// Instruction: '...'. Intrinsic: '_mm_store_pd1'.
// Requires SSE2.
func StorePd1(mem_addr float64, a M128d)  {
	storePd1(mem_addr, [2]float64(a))
}

func storePd1(mem_addr float64, a [2]float64) 


// MaskStorePs: Store packed single-precision (32-bit) floating-point elements
// from 'a' into memory using writemask 'k'.
// 	'mem_addr' must be aligned on a 16-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVAPS'. Intrinsic: '_mm_mask_store_ps'.
// Requires AVX512F.
func MaskStorePs(mem_addr uintptr, k Mmask8, a M128)  {
	maskStorePs(uintptr(mem_addr), uint8(k), [4]float32(a))
}

func maskStorePs(mem_addr uintptr, k uint8, a [4]float32) 


// StorePs: Store 128-bits (composed of 4 packed single-precision (32-bit)
// floating-point elements) from 'a' into memory.
// 	'mem_addr' must be aligned on a 16-byte boundary or a general-protection
// exception may be generated. 
//
//		MEM[mem_addr+127:mem_addr] := a[127:0]
//
// Instruction: 'MOVAPS'. Intrinsic: '_mm_store_ps'.
// Requires SSE.
func StorePs(mem_addr float32, a M128)  {
	storePs(mem_addr, [4]float32(a))
}

func storePs(mem_addr float32, a [4]float32) 


// StorePs1: Store the lower single-precision (32-bit) floating-point element
// from 'a' into 4 contiguous elements in memory. 'mem_addr' must be aligned on
// a 16-byte boundary or a general-protection exception may be generated. 
//
//		MEM[mem_addr+31:mem_addr] := a[31:0]
//		MEM[mem_addr+63:mem_addr+32] := a[31:0]
//		MEM[mem_addr+95:mem_addr+64] := a[31:0]
//		MEM[mem_addr+127:mem_addr+96] := a[31:0]
//
// Instruction: '...'. Intrinsic: '_mm_store_ps1'.
// Requires SSE.
func StorePs1(mem_addr float32, a M128)  {
	storePs1(mem_addr, [4]float32(a))
}

func storePs1(mem_addr float32, a [4]float32) 


// MaskStoreSd: Store the lower double-precision (64-bit) floating-point
// element from 'a' into memory using writemask 'k'.
// 	'mem_addr' must be aligned on a 16-byte boundary or a general-protection
// exception may be generated. 
//
//		IF k[0]
//			MEM[mem_addr+63:mem_addr] := a[63:0]
//		FI
//
// Instruction: 'VMOVSD'. Intrinsic: '_mm_mask_store_sd'.
// Requires AVX512F.
func MaskStoreSd(mem_addr float64, k Mmask8, a M128d)  {
	maskStoreSd(mem_addr, uint8(k), [2]float64(a))
}

func maskStoreSd(mem_addr float64, k uint8, a [2]float64) 


// StoreSd: Store the lower double-precision (64-bit) floating-point element
// from 'a' into memory. 'mem_addr' does not need to be aligned on any
// particular boundary. 
//
//		MEM[mem_addr+63:mem_addr] := a[63:0]
//
// Instruction: 'MOVSD'. Intrinsic: '_mm_store_sd'.
// Requires SSE2.
func StoreSd(mem_addr float64, a M128d)  {
	storeSd(mem_addr, [2]float64(a))
}

func storeSd(mem_addr float64, a [2]float64) 


// StoreSi128: Store 128-bits of integer data from 'a' into memory. 
// 	'mem_addr' must be aligned on a 16-byte boundary or a general-protection
// exception may be generated. 
//
//		MEM[mem_addr+127:mem_addr] := a[127:0]
//
// Instruction: 'MOVDQA'. Intrinsic: '_mm_store_si128'.
// Requires SSE2.
func StoreSi128(mem_addr M128i, a M128i)  {
	storeSi128([16]byte(mem_addr), [16]byte(a))
}

func storeSi128(mem_addr [16]byte, a [16]byte) 


// MaskStoreSs: Store the lower single-precision (32-bit) floating-point
// element from 'a' into memory using writemask 'k'.
// 	'mem_addr' must be aligned on a 16-byte boundary or a general-protection
// exception may be generated. 
//
//		IF k[0]
//			MEM[mem_addr+31:mem_addr] := a[31:0]
//		FI
//
// Instruction: 'VMOVSS'. Intrinsic: '_mm_mask_store_ss'.
// Requires AVX512F.
func MaskStoreSs(mem_addr float32, k Mmask8, a M128)  {
	maskStoreSs(mem_addr, uint8(k), [4]float32(a))
}

func maskStoreSs(mem_addr float32, k uint8, a [4]float32) 


// StoreSs: Store the lower single-precision (32-bit) floating-point element
// from 'a' into memory. 'mem_addr' does not need to be aligned on any
// particular boundary. 
//
//		MEM[mem_addr+31:mem_addr] := a[31:0]
//
// Instruction: 'MOVSS'. Intrinsic: '_mm_store_ss'.
// Requires SSE.
func StoreSs(mem_addr float32, a M128)  {
	storeSs(mem_addr, [4]float32(a))
}

func storeSs(mem_addr float32, a [4]float32) 


// Store1Pd: Store the lower double-precision (64-bit) floating-point element
// from 'a' into 2 contiguous elements in memory. 'mem_addr' must be aligned on
// a 16-byte boundary or a general-protection exception may be generated. 
//
//		MEM[mem_addr+63:mem_addr] := a[63:0]
//		MEM[mem_addr+127:mem_addr+64] := a[63:0]
//
// Instruction: '...'. Intrinsic: '_mm_store1_pd'.
// Requires SSE2.
func Store1Pd(mem_addr float64, a M128d)  {
	store1Pd(mem_addr, [2]float64(a))
}

func store1Pd(mem_addr float64, a [2]float64) 


// Store1Ps: Store the lower single-precision (32-bit) floating-point element
// from 'a' into 4 contiguous elements in memory. 'mem_addr' must be aligned on
// a 16-byte boundary or a general-protection exception may be generated. 
//
//		MEM[mem_addr+31:mem_addr] := a[31:0]
//		MEM[mem_addr+63:mem_addr+32] := a[31:0]
//		MEM[mem_addr+95:mem_addr+64] := a[31:0]
//		MEM[mem_addr+127:mem_addr+96] := a[31:0]
//
// Instruction: '...'. Intrinsic: '_mm_store1_ps'.
// Requires SSE.
func Store1Ps(mem_addr float32, a M128)  {
	store1Ps(mem_addr, [4]float32(a))
}

func store1Ps(mem_addr float32, a [4]float32) 


// StorehPd: Store the upper double-precision (64-bit) floating-point element
// from 'a' into memory. 
//
//		MEM[mem_addr+63:mem_addr] := a[127:64]
//
// Instruction: 'MOVHPD'. Intrinsic: '_mm_storeh_pd'.
// Requires SSE2.
func StorehPd(mem_addr float64, a M128d)  {
	storehPd(mem_addr, [2]float64(a))
}

func storehPd(mem_addr float64, a [2]float64) 


// Storel64: Store 64-bit integer from the first element of 'a' into memory. 
//
//		MEM[mem_addr+63:mem_addr] := a[63:0]
//
// Instruction: 'MOVQ'. Intrinsic: '_mm_storel_epi64'.
// Requires SSE2.
func Storel64(mem_addr M128i, a M128i)  {
	storel64([16]byte(mem_addr), [16]byte(a))
}

func storel64(mem_addr [16]byte, a [16]byte) 


// StorelPd: Store the lower double-precision (64-bit) floating-point element
// from 'a' into memory. 
//
//		MEM[mem_addr+63:mem_addr] := a[63:0]
//
// Instruction: 'MOVLPD'. Intrinsic: '_mm_storel_pd'.
// Requires SSE2.
func StorelPd(mem_addr float64, a M128d)  {
	storelPd(mem_addr, [2]float64(a))
}

func storelPd(mem_addr float64, a [2]float64) 


// StorerPd: Store 2 double-precision (64-bit) floating-point elements from 'a'
// into memory in reverse order.
// 	'mem_addr' must be aligned on a 16-byte boundary or a general-protection
// exception may be generated. 
//
//		MEM[mem_addr+63:mem_addr] := a[127:64]
//		MEM[mem_addr+127:mem_addr+64] := a[63:0]
//
// Instruction: '...'. Intrinsic: '_mm_storer_pd'.
// Requires SSE2.
func StorerPd(mem_addr float64, a M128d)  {
	storerPd(mem_addr, [2]float64(a))
}

func storerPd(mem_addr float64, a [2]float64) 


// StorerPs: Store 4 single-precision (32-bit) floating-point elements from 'a'
// into memory in reverse order.
// 	'mem_addr' must be aligned on a 16-byte boundary or a general-protection
// exception may be generated. 
//
//		MEM[mem_addr+31:mem_addr] := a[127:96]
//		MEM[mem_addr+63:mem_addr+32] := a[95:64]
//		MEM[mem_addr+95:mem_addr+64] := a[63:32]
//		MEM[mem_addr+127:mem_addr+96] := a[31:0]
//
// Instruction: '...'. Intrinsic: '_mm_storer_ps'.
// Requires SSE.
func StorerPs(mem_addr float32, a M128)  {
	storerPs(mem_addr, [4]float32(a))
}

func storerPs(mem_addr float32, a [4]float32) 


// MaskStoreu16: Store packed 16-bit integers from 'a' into memory using
// writemask 'k'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				MEM[mem_addr+i+15:mem_addr+i] := a[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VMOVDQU16'. Intrinsic: '_mm_mask_storeu_epi16'.
// Requires AVX512BW.
func MaskStoreu16(mem_addr uintptr, k Mmask8, a M128i)  {
	maskStoreu16(uintptr(mem_addr), uint8(k), [16]byte(a))
}

func maskStoreu16(mem_addr uintptr, k uint8, a [16]byte) 


// MaskStoreu32: Store packed 32-bit integers from 'a' into memory using
// writemask 'k'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVDQU32'. Intrinsic: '_mm_mask_storeu_epi32'.
// Requires AVX512F.
func MaskStoreu32(mem_addr uintptr, k Mmask8, a M128i)  {
	maskStoreu32(uintptr(mem_addr), uint8(k), [16]byte(a))
}

func maskStoreu32(mem_addr uintptr, k uint8, a [16]byte) 


// MaskStoreu64: Store packed 64-bit integers from 'a' into memory using
// writemask 'k'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVDQU64'. Intrinsic: '_mm_mask_storeu_epi64'.
// Requires AVX512F.
func MaskStoreu64(mem_addr uintptr, k Mmask8, a M128i)  {
	maskStoreu64(uintptr(mem_addr), uint8(k), [16]byte(a))
}

func maskStoreu64(mem_addr uintptr, k uint8, a [16]byte) 


// MaskStoreu8: Store packed 8-bit integers from 'a' into memory using
// writemask 'k'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				MEM[mem_addr+i+7:mem_addr+i] := a[i+7:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVDQU8'. Intrinsic: '_mm_mask_storeu_epi8'.
// Requires AVX512BW.
func MaskStoreu8(mem_addr uintptr, k Mmask16, a M128i)  {
	maskStoreu8(uintptr(mem_addr), uint16(k), [16]byte(a))
}

func maskStoreu8(mem_addr uintptr, k uint16, a [16]byte) 


// MaskStoreuPd: Store packed double-precision (64-bit) floating-point elements
// from 'a' into memory using writemask 'k'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVUPD'. Intrinsic: '_mm_mask_storeu_pd'.
// Requires AVX512F.
func MaskStoreuPd(mem_addr uintptr, k Mmask8, a M128d)  {
	maskStoreuPd(uintptr(mem_addr), uint8(k), [2]float64(a))
}

func maskStoreuPd(mem_addr uintptr, k uint8, a [2]float64) 


// StoreuPd: Store 128-bits (composed of 2 packed double-precision (64-bit)
// floating-point elements) from 'a' into memory.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		MEM[mem_addr+127:mem_addr] := a[127:0]
//
// Instruction: 'MOVUPD'. Intrinsic: '_mm_storeu_pd'.
// Requires SSE2.
func StoreuPd(mem_addr float64, a M128d)  {
	storeuPd(mem_addr, [2]float64(a))
}

func storeuPd(mem_addr float64, a [2]float64) 


// MaskStoreuPs: Store packed single-precision (32-bit) floating-point elements
// from 'a' into memory using writemask 'k'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVUPS'. Intrinsic: '_mm_mask_storeu_ps'.
// Requires AVX512F.
func MaskStoreuPs(mem_addr uintptr, k Mmask8, a M128)  {
	maskStoreuPs(uintptr(mem_addr), uint8(k), [4]float32(a))
}

func maskStoreuPs(mem_addr uintptr, k uint8, a [4]float32) 


// StoreuPs: Store 128-bits (composed of 4 packed single-precision (32-bit)
// floating-point elements) from 'a' into memory.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		MEM[mem_addr+127:mem_addr] := a[127:0]
//
// Instruction: 'MOVUPS'. Intrinsic: '_mm_storeu_ps'.
// Requires SSE.
func StoreuPs(mem_addr float32, a M128)  {
	storeuPs(mem_addr, [4]float32(a))
}

func storeuPs(mem_addr float32, a [4]float32) 


// StoreuSi128: Store 128-bits of integer data from 'a' into memory.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		MEM[mem_addr+127:mem_addr] := a[127:0]
//
// Instruction: 'MOVDQU'. Intrinsic: '_mm_storeu_si128'.
// Requires SSE2.
func StoreuSi128(mem_addr M128i, a M128i)  {
	storeuSi128([16]byte(mem_addr), [16]byte(a))
}

func storeuSi128(mem_addr [16]byte, a [16]byte) 


// StreamLoadSi128: Load 128-bits of integer data from memory into 'dst' using
// a non-temporal memory hint.
// 	'mem_addr' must be aligned on a 16-byte boundary or a general-protection
// exception may be generated. 
//
//		dst[127:0] := MEM[mem_addr+127:mem_addr]
//
// Instruction: 'MOVNTDQA'. Intrinsic: '_mm_stream_load_si128'.
// Requires SSE4.1.
func StreamLoadSi128(mem_addr M128i) M128i {
	return M128i(streamLoadSi128([16]byte(mem_addr)))
}

func streamLoadSi128(mem_addr [16]byte) [16]byte


// StreamPd: Store 128-bits (composed of 2 packed double-precision (64-bit)
// floating-point elements) from 'a' into memory using a non-temporal memory
// hint.
// 	'mem_addr' must be aligned on a 16-byte boundary or a general-protection
// exception may be generated. 
//
//		MEM[mem_addr+127:mem_addr] := a[127:0]
//
// Instruction: 'MOVNTPD'. Intrinsic: '_mm_stream_pd'.
// Requires SSE2.
func StreamPd(mem_addr float64, a M128d)  {
	streamPd(mem_addr, [2]float64(a))
}

func streamPd(mem_addr float64, a [2]float64) 


// StreamPs: Store 128-bits (composed of 4 packed single-precision (32-bit)
// floating-point elements) from 'a' into memory using a non-temporal memory
// hint.
// 	'mem_addr' must be aligned on a 16-byte boundary or a general-protection
// exception may be generated. 
//
//		MEM[mem_addr+127:mem_addr] := a[127:0]
//
// Instruction: 'MOVNTPS'. Intrinsic: '_mm_stream_ps'.
// Requires SSE.
func StreamPs(mem_addr float32, a M128)  {
	streamPs(mem_addr, [4]float32(a))
}

func streamPs(mem_addr float32, a [4]float32) 


// StreamSi128: Store 128-bits of integer data from 'a' into memory using a
// non-temporal memory hint. 
// 	'mem_addr' must be aligned on a 16-byte boundary or a general-protection
// exception may be generated. 
//
//		MEM[mem_addr+127:mem_addr] := a[127:0]
//
// Instruction: 'MOVNTDQ'. Intrinsic: '_mm_stream_si128'.
// Requires SSE2.
func StreamSi128(mem_addr M128i, a M128i)  {
	streamSi128([16]byte(mem_addr), [16]byte(a))
}

func streamSi128(mem_addr [16]byte, a [16]byte) 


// MaskSub16: Subtract packed 16-bit integers in 'b' from packed 16-bit
// integers in 'a', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := a[i+15:i] - b[i+15:i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSUBW'. Intrinsic: '_mm_mask_sub_epi16'.
// Requires AVX512BW.
func MaskSub16(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskSub16([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskSub16(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzSub16: Subtract packed 16-bit integers in 'b' from packed 16-bit
// integers in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := a[i+15:i] - b[i+15:i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSUBW'. Intrinsic: '_mm_maskz_sub_epi16'.
// Requires AVX512BW.
func MaskzSub16(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzSub16(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzSub16(k uint8, a [16]byte, b [16]byte) [16]byte


// Sub16: Subtract packed 16-bit integers in 'b' from packed 16-bit integers in
// 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			dst[i+15:i] := a[i+15:i] - b[i+15:i]
//		ENDFOR
//
// Instruction: 'PSUBW'. Intrinsic: '_mm_sub_epi16'.
// Requires SSE2.
func Sub16(a M128i, b M128i) M128i {
	return M128i(sub16([16]byte(a), [16]byte(b)))
}

func sub16(a [16]byte, b [16]byte) [16]byte


// MaskSub32: Subtract packed 32-bit integers in 'b' from packed 32-bit
// integers in 'a', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] - b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSUBD'. Intrinsic: '_mm_mask_sub_epi32'.
// Requires AVX512F.
func MaskSub32(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskSub32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskSub32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzSub32: Subtract packed 32-bit integers in 'b' from packed 32-bit
// integers in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] - b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSUBD'. Intrinsic: '_mm_maskz_sub_epi32'.
// Requires AVX512F.
func MaskzSub32(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzSub32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzSub32(k uint8, a [16]byte, b [16]byte) [16]byte


// Sub32: Subtract packed 32-bit integers in 'b' from packed 32-bit integers in
// 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := a[i+31:i] - b[i+31:i]
//		ENDFOR
//
// Instruction: 'PSUBD'. Intrinsic: '_mm_sub_epi32'.
// Requires SSE2.
func Sub32(a M128i, b M128i) M128i {
	return M128i(sub32([16]byte(a), [16]byte(b)))
}

func sub32(a [16]byte, b [16]byte) [16]byte


// MaskSub64: Subtract packed 64-bit integers in 'b' from packed 64-bit
// integers in 'a', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] - b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSUBQ'. Intrinsic: '_mm_mask_sub_epi64'.
// Requires AVX512F.
func MaskSub64(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskSub64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskSub64(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzSub64: Subtract packed 64-bit integers in 'b' from packed 64-bit
// integers in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] - b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSUBQ'. Intrinsic: '_mm_maskz_sub_epi64'.
// Requires AVX512F.
func MaskzSub64(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzSub64(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzSub64(k uint8, a [16]byte, b [16]byte) [16]byte


// Sub64: Subtract packed 64-bit integers in 'b' from packed 64-bit integers in
// 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := a[i+63:i] - b[i+63:i]
//		ENDFOR
//
// Instruction: 'PSUBQ'. Intrinsic: '_mm_sub_epi64'.
// Requires SSE2.
func Sub64(a M128i, b M128i) M128i {
	return M128i(sub64([16]byte(a), [16]byte(b)))
}

func sub64(a [16]byte, b [16]byte) [16]byte


// MaskSub8: Subtract packed 8-bit integers in 'b' from packed 8-bit integers
// in 'a', and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := a[i+7:i] - b[i+7:i]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSUBB'. Intrinsic: '_mm_mask_sub_epi8'.
// Requires AVX512BW.
func MaskSub8(src M128i, k Mmask16, a M128i, b M128i) M128i {
	return M128i(maskSub8([16]byte(src), uint16(k), [16]byte(a), [16]byte(b)))
}

func maskSub8(src [16]byte, k uint16, a [16]byte, b [16]byte) [16]byte


// MaskzSub8: Subtract packed 8-bit integers in 'b' from packed 8-bit integers
// in 'a', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := a[i+7:i] - b[i+7:i]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSUBB'. Intrinsic: '_mm_maskz_sub_epi8'.
// Requires AVX512BW.
func MaskzSub8(k Mmask16, a M128i, b M128i) M128i {
	return M128i(maskzSub8(uint16(k), [16]byte(a), [16]byte(b)))
}

func maskzSub8(k uint16, a [16]byte, b [16]byte) [16]byte


// Sub8: Subtract packed 8-bit integers in 'b' from packed 8-bit integers in
// 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*8
//			dst[i+7:i] := a[i+7:i] - b[i+7:i]
//		ENDFOR
//
// Instruction: 'PSUBB'. Intrinsic: '_mm_sub_epi8'.
// Requires SSE2.
func Sub8(a M128i, b M128i) M128i {
	return M128i(sub8([16]byte(a), [16]byte(b)))
}

func sub8(a [16]byte, b [16]byte) [16]byte


// MaskSubPd: Subtract packed double-precision (64-bit) floating-point elements
// in 'b' from packed double-precision (64-bit) floating-point elements in 'a',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] - b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSUBPD'. Intrinsic: '_mm_mask_sub_pd'.
// Requires AVX512F.
func MaskSubPd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskSubPd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskSubPd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzSubPd: Subtract packed double-precision (64-bit) floating-point
// elements in 'b' from packed double-precision (64-bit) floating-point
// elements in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] - b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSUBPD'. Intrinsic: '_mm_maskz_sub_pd'.
// Requires AVX512F.
func MaskzSubPd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzSubPd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzSubPd(k uint8, a [2]float64, b [2]float64) [2]float64


// SubPd: Subtract packed double-precision (64-bit) floating-point elements in
// 'b' from packed double-precision (64-bit) floating-point elements in 'a',
// and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := a[i+63:i] - b[i+63:i]
//		ENDFOR
//
// Instruction: 'SUBPD'. Intrinsic: '_mm_sub_pd'.
// Requires SSE2.
func SubPd(a M128d, b M128d) M128d {
	return M128d(subPd([2]float64(a), [2]float64(b)))
}

func subPd(a [2]float64, b [2]float64) [2]float64


// MaskSubPs: Subtract packed single-precision (32-bit) floating-point elements
// in 'b' from packed single-precision (32-bit) floating-point elements in 'a',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] - b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSUBPS'. Intrinsic: '_mm_mask_sub_ps'.
// Requires AVX512F.
func MaskSubPs(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskSubPs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskSubPs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzSubPs: Subtract packed single-precision (32-bit) floating-point
// elements in 'b' from packed single-precision (32-bit) floating-point
// elements in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] - b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VSUBPS'. Intrinsic: '_mm_maskz_sub_ps'.
// Requires AVX512F.
func MaskzSubPs(k Mmask8, a M128, b M128) M128 {
	return M128(maskzSubPs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzSubPs(k uint8, a [4]float32, b [4]float32) [4]float32


// SubPs: Subtract packed single-precision (32-bit) floating-point elements in
// 'b' from packed single-precision (32-bit) floating-point elements in 'a',
// and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := a[i+31:i] - b[i+31:i]
//		ENDFOR
//
// Instruction: 'SUBPS'. Intrinsic: '_mm_sub_ps'.
// Requires SSE.
func SubPs(a M128, b M128) M128 {
	return M128(subPs([4]float32(a), [4]float32(b)))
}

func subPs(a [4]float32, b [4]float32) [4]float32


// MaskSubRoundSd: Subtract the lower double-precision (64-bit) floating-point
// element in 'b' from the lower double-precision (64-bit) floating-point
// element in 'a', store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'src' when mask bit 0 is not set),
// and copy the upper element from 'a' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := a[63:0] - b[63:0]
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSUBSD'. Intrinsic: '_mm_mask_sub_round_sd'.
// Requires AVX512F.
func MaskSubRoundSd(src M128d, k Mmask8, a M128d, b M128d, rounding int) M128d {
	return M128d(maskSubRoundSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b), rounding))
}

func maskSubRoundSd(src [2]float64, k uint8, a [2]float64, b [2]float64, rounding int) [2]float64


// MaskzSubRoundSd: Subtract the lower double-precision (64-bit) floating-point
// element in 'b' from the lower double-precision (64-bit) floating-point
// element in 'a', store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper element from 'a' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[63:0] := a[63:0] - b[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSUBSD'. Intrinsic: '_mm_maskz_sub_round_sd'.
// Requires AVX512F.
func MaskzSubRoundSd(k Mmask8, a M128d, b M128d, rounding int) M128d {
	return M128d(maskzSubRoundSd(uint8(k), [2]float64(a), [2]float64(b), rounding))
}

func maskzSubRoundSd(k uint8, a [2]float64, b [2]float64, rounding int) [2]float64


// SubRoundSd: Subtract the lower double-precision (64-bit) floating-point
// element in 'b' from the lower double-precision (64-bit) floating-point
// element in 'a', store the result in the lower element of 'dst', and copy the
// upper element from 'a' to the upper element of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[63:0] := a[63:0] - b[63:0]
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSUBSD'. Intrinsic: '_mm_sub_round_sd'.
// Requires AVX512F.
func SubRoundSd(a M128d, b M128d, rounding int) M128d {
	return M128d(subRoundSd([2]float64(a), [2]float64(b), rounding))
}

func subRoundSd(a [2]float64, b [2]float64, rounding int) [2]float64


// MaskSubRoundSs: Subtract the lower single-precision (32-bit) floating-point
// element in 'b' from the lower single-precision (32-bit) floating-point
// element in 'a', store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'src' when mask bit 0 is not set),
// and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := a[31:0] - b[31:0]
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSUBSS'. Intrinsic: '_mm_mask_sub_round_ss'.
// Requires AVX512F.
func MaskSubRoundSs(src M128, k Mmask8, a M128, b M128, rounding int) M128 {
	return M128(maskSubRoundSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b), rounding))
}

func maskSubRoundSs(src [4]float32, k uint8, a [4]float32, b [4]float32, rounding int) [4]float32


// MaskzSubRoundSs: Subtract the lower single-precision (32-bit) floating-point
// element in 'b' from the lower single-precision (32-bit) floating-point
// element in 'a', store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		IF k[0]
//			dst[31:0] := a[31:0] - b[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSUBSS'. Intrinsic: '_mm_maskz_sub_round_ss'.
// Requires AVX512F.
func MaskzSubRoundSs(k Mmask8, a M128, b M128, rounding int) M128 {
	return M128(maskzSubRoundSs(uint8(k), [4]float32(a), [4]float32(b), rounding))
}

func maskzSubRoundSs(k uint8, a [4]float32, b [4]float32, rounding int) [4]float32


// SubRoundSs: Subtract the lower single-precision (32-bit) floating-point
// element in 'b' from the lower single-precision (32-bit) floating-point
// element in 'a', store the result in the lower element of 'dst', and copy the
// upper 3 packed elements from 'a' to the upper elements of 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		dst[31:0] := a[31:0] - b[31:0]
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSUBSS'. Intrinsic: '_mm_sub_round_ss'.
// Requires AVX512F.
func SubRoundSs(a M128, b M128, rounding int) M128 {
	return M128(subRoundSs([4]float32(a), [4]float32(b), rounding))
}

func subRoundSs(a [4]float32, b [4]float32, rounding int) [4]float32


// MaskSubSd: Subtract the lower double-precision (64-bit) floating-point
// element in 'b' from the lower double-precision (64-bit) floating-point
// element in 'a', store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'src' when mask bit 0 is not set),
// and copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := a[63:0] - b[63:0]
//		ELSE
//			dst[63:0] := src[63:0]
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSUBSD'. Intrinsic: '_mm_mask_sub_sd'.
// Requires AVX512F.
func MaskSubSd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskSubSd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskSubSd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzSubSd: Subtract the lower double-precision (64-bit) floating-point
// element in 'b' from the lower double-precision (64-bit) floating-point
// element in 'a', store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper element from 'a' to the upper element of 'dst'. 
//
//		IF k[0]
//			dst[63:0] := a[63:0] - b[63:0]
//		ELSE
//			dst[63:0] := 0
//		FI
//		dst[127:64] := a[127:64]
//		dst[MAX:128] := 0
//
// Instruction: 'VSUBSD'. Intrinsic: '_mm_maskz_sub_sd'.
// Requires AVX512F.
func MaskzSubSd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzSubSd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzSubSd(k uint8, a [2]float64, b [2]float64) [2]float64


// SubSd: Subtract the lower double-precision (64-bit) floating-point element
// in 'b' from the lower double-precision (64-bit) floating-point element in
// 'a', store the result in the lower element of 'dst', and copy the upper
// element from 'a' to the upper element of 'dst'. 
//
//		dst[63:0] := a[63:0] - b[63:0]
//		dst[127:64] := a[127:64]
//
// Instruction: 'SUBSD'. Intrinsic: '_mm_sub_sd'.
// Requires SSE2.
func SubSd(a M128d, b M128d) M128d {
	return M128d(subSd([2]float64(a), [2]float64(b)))
}

func subSd(a [2]float64, b [2]float64) [2]float64


// MaskSubSs: Subtract the lower single-precision (32-bit) floating-point
// element in 'b' from the lower single-precision (32-bit) floating-point
// element in 'a', store the result in the lower element of 'dst' using
// writemask 'k' (the element is copied from 'src' when mask bit 0 is not set),
// and copy the upper 3 packed elements from 'a' to the upper elements of
// 'dst'. 
//
//		IF k[0]
//			dst[31:0] := a[31:0] - b[31:0]
//		ELSE
//			dst[31:0] := src[31:0]
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSUBSS'. Intrinsic: '_mm_mask_sub_ss'.
// Requires AVX512F.
func MaskSubSs(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskSubSs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskSubSs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzSubSs: Subtract the lower single-precision (32-bit) floating-point
// element in 'b' from the lower single-precision (32-bit) floating-point
// element in 'a', store the result in the lower element of 'dst' using
// zeromask 'k' (the element is zeroed out when mask bit 0 is not set), and
// copy the upper 3 packed elements from 'a' to the upper elements of 'dst'. 
//
//		IF k[0]
//			dst[31:0] := a[31:0] - b[31:0]
//		ELSE
//			dst[31:0] := 0
//		FI
//		dst[127:32] := a[127:32]
//		dst[MAX:128] := 0
//
// Instruction: 'VSUBSS'. Intrinsic: '_mm_maskz_sub_ss'.
// Requires AVX512F.
func MaskzSubSs(k Mmask8, a M128, b M128) M128 {
	return M128(maskzSubSs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzSubSs(k uint8, a [4]float32, b [4]float32) [4]float32


// SubSs: Subtract the lower single-precision (32-bit) floating-point element
// in 'b' from the lower single-precision (32-bit) floating-point element in
// 'a', store the result in the lower element of 'dst', and copy the upper 3
// packed elements from 'a' to the upper elements of 'dst'. 
//
//		dst[31:0] := a[31:0] - b[31:0]
//		dst[127:32] := a[127:32]
//
// Instruction: 'SUBSS'. Intrinsic: '_mm_sub_ss'.
// Requires SSE.
func SubSs(a M128, b M128) M128 {
	return M128(subSs([4]float32(a), [4]float32(b)))
}

func subSs(a [4]float32, b [4]float32) [4]float32


// MaskSubs16: Subtract packed 16-bit integers in 'b' from packed 16-bit
// integers in 'a' using saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := Saturate_To_Int16(a[i+15:i] - b[i+15:i])
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSUBSW'. Intrinsic: '_mm_mask_subs_epi16'.
// Requires AVX512BW.
func MaskSubs16(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskSubs16([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskSubs16(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzSubs16: Subtract packed 16-bit integers in 'b' from packed 16-bit
// integers in 'a' using saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := Saturate_To_Int16(a[i+15:i] - b[i+15:i])
//			ELSE
//				dst[i+15:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSUBSW'. Intrinsic: '_mm_maskz_subs_epi16'.
// Requires AVX512BW.
func MaskzSubs16(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzSubs16(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzSubs16(k uint8, a [16]byte, b [16]byte) [16]byte


// Subs16: Subtract packed 16-bit integers in 'b' from packed 16-bit integers
// in 'a' using saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			dst[i+15:i] := Saturate_To_Int16(a[i+15:i] - b[i+15:i])
//		ENDFOR
//
// Instruction: 'PSUBSW'. Intrinsic: '_mm_subs_epi16'.
// Requires SSE2.
func Subs16(a M128i, b M128i) M128i {
	return M128i(subs16([16]byte(a), [16]byte(b)))
}

func subs16(a [16]byte, b [16]byte) [16]byte


// MaskSubs8: Subtract packed 8-bit integers in 'b' from packed 8-bit integers
// in 'a' using saturation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := Saturate_To_Int8(a[i+7:i] - b[i+7:i])
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSUBSB'. Intrinsic: '_mm_mask_subs_epi8'.
// Requires AVX512BW.
func MaskSubs8(src M128i, k Mmask16, a M128i, b M128i) M128i {
	return M128i(maskSubs8([16]byte(src), uint16(k), [16]byte(a), [16]byte(b)))
}

func maskSubs8(src [16]byte, k uint16, a [16]byte, b [16]byte) [16]byte


// MaskzSubs8: Subtract packed 8-bit integers in 'b' from packed 8-bit integers
// in 'a' using saturation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := Saturate_To_Int8(a[i+7:i] - b[i+7:i])
//			ELSE
//				dst[i+7:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSUBSB'. Intrinsic: '_mm_maskz_subs_epi8'.
// Requires AVX512BW.
func MaskzSubs8(k Mmask16, a M128i, b M128i) M128i {
	return M128i(maskzSubs8(uint16(k), [16]byte(a), [16]byte(b)))
}

func maskzSubs8(k uint16, a [16]byte, b [16]byte) [16]byte


// Subs8: Subtract packed 8-bit integers in 'b' from packed 8-bit integers in
// 'a' using saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*8
//			dst[i+7:i] := Saturate_To_Int8(a[i+7:i] - b[i+7:i])	
//		ENDFOR
//
// Instruction: 'PSUBSB'. Intrinsic: '_mm_subs_epi8'.
// Requires SSE2.
func Subs8(a M128i, b M128i) M128i {
	return M128i(subs8([16]byte(a), [16]byte(b)))
}

func subs8(a [16]byte, b [16]byte) [16]byte


// MaskSubsEpu16: Subtract packed unsigned 16-bit integers in 'b' from packed
// unsigned 16-bit integers in 'a' using saturation, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := Saturate_To_UnsignedInt16(a[i+15:i] - b[i+15:i])
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSUBUSW'. Intrinsic: '_mm_mask_subs_epu16'.
// Requires AVX512BW.
func MaskSubsEpu16(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskSubsEpu16([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskSubsEpu16(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzSubsEpu16: Subtract packed unsigned 16-bit integers in 'b' from packed
// unsigned 16-bit integers in 'a' using saturation, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := Saturate_To_UnsignedInt16(a[i+15:i] - b[i+15:i])
//			ELSE
//				dst[i+15:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSUBUSW'. Intrinsic: '_mm_maskz_subs_epu16'.
// Requires AVX512BW.
func MaskzSubsEpu16(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzSubsEpu16(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzSubsEpu16(k uint8, a [16]byte, b [16]byte) [16]byte


// SubsEpu16: Subtract packed unsigned 16-bit integers in 'b' from packed
// unsigned 16-bit integers in 'a' using saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*16
//			dst[i+15:i] := Saturate_To_UnsignedInt16(a[i+15:i] - b[i+15:i])	
//		ENDFOR
//
// Instruction: 'PSUBUSW'. Intrinsic: '_mm_subs_epu16'.
// Requires SSE2.
func SubsEpu16(a M128i, b M128i) M128i {
	return M128i(subsEpu16([16]byte(a), [16]byte(b)))
}

func subsEpu16(a [16]byte, b [16]byte) [16]byte


// MaskSubsEpu8: Subtract packed unsigned 8-bit integers in 'b' from packed
// unsigned 8-bit integers in 'a' using saturation, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := Saturate_To_UnsignedInt8(a[i+7:i] - b[i+7:i])
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSUBUSB'. Intrinsic: '_mm_mask_subs_epu8'.
// Requires AVX512BW.
func MaskSubsEpu8(src M128i, k Mmask16, a M128i, b M128i) M128i {
	return M128i(maskSubsEpu8([16]byte(src), uint16(k), [16]byte(a), [16]byte(b)))
}

func maskSubsEpu8(src [16]byte, k uint16, a [16]byte, b [16]byte) [16]byte


// MaskzSubsEpu8: Subtract packed unsigned 8-bit integers in 'b' from packed
// unsigned 8-bit integers in 'a' using saturation, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := Saturate_To_UnsignedInt8(a[i+7:i] - b[i+7:i])
//			ELSE
//				dst[i+7:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPSUBUSB'. Intrinsic: '_mm_maskz_subs_epu8'.
// Requires AVX512BW.
func MaskzSubsEpu8(k Mmask16, a M128i, b M128i) M128i {
	return M128i(maskzSubsEpu8(uint16(k), [16]byte(a), [16]byte(b)))
}

func maskzSubsEpu8(k uint16, a [16]byte, b [16]byte) [16]byte


// SubsEpu8: Subtract packed unsigned 8-bit integers in 'b' from packed
// unsigned 8-bit integers in 'a' using saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*8
//			dst[i+7:i] := Saturate_To_UnsignedInt8(a[i+7:i] - b[i+7:i])	
//		ENDFOR
//
// Instruction: 'PSUBUSB'. Intrinsic: '_mm_subs_epu8'.
// Requires SSE2.
func SubsEpu8(a M128i, b M128i) M128i {
	return M128i(subsEpu8([16]byte(a), [16]byte(b)))
}

func subsEpu8(a [16]byte, b [16]byte) [16]byte


// SvmlCeilPd: Round the packed double-precision (64-bit) floating-point
// elements in 'a' up to an integer value, and store the results as packed
// double-precision floating-point elements in 'dst'. This intrinsic may
// generate the 'roundpd'/'vroundpd' instruction. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := CEIL(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_svml_ceil_pd'.
// Requires SSE.
func SvmlCeilPd(a M128d) M128d {
	return M128d(svmlCeilPd([2]float64(a)))
}

func svmlCeilPd(a [2]float64) [2]float64


// SvmlCeilPs: Round the packed single-precision (32-bit) floating-point
// elements in 'a' up to an integer value, and store the results as packed
// single-precision floating-point elements in 'dst'. This intrinsic may
// generate the 'roundps'/'vroundps' instruction. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := CEIL(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_svml_ceil_ps'.
// Requires SSE.
func SvmlCeilPs(a M128) M128 {
	return M128(svmlCeilPs([4]float32(a)))
}

func svmlCeilPs(a [4]float32) [4]float32


// SvmlFloorPd: Round the packed double-precision (64-bit) floating-point
// elements in 'a' down to an integer value, and store the results as packed
// double-precision floating-point elements in 'dst'. This intrinsic may
// generate the 'roundpd'/'vroundpd' instruction. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := FLOOR(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_svml_floor_pd'.
// Requires SSE.
func SvmlFloorPd(a M128d) M128d {
	return M128d(svmlFloorPd([2]float64(a)))
}

func svmlFloorPd(a [2]float64) [2]float64


// SvmlFloorPs: Round the packed single-precision (32-bit) floating-point
// elements in 'a' down to an integer value, and store the results as packed
// single-precision floating-point elements in 'dst'. This intrinsic may
// generate the 'roundps'/'vroundps' instruction. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := FLOOR(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_svml_floor_ps'.
// Requires SSE.
func SvmlFloorPs(a M128) M128 {
	return M128(svmlFloorPs([4]float32(a)))
}

func svmlFloorPs(a [4]float32) [4]float32


// SvmlRoundPd: Round the packed double-precision (64-bit) floating-point
// elements in 'a' to the nearest integer value, and store the results as
// packed double-precision floating-point elements in 'dst'. This intrinsic may
// generate the 'roundpd'/'vroundpd' instruction. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := ROUND(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_svml_round_pd'.
// Requires SSE.
func SvmlRoundPd(a M128d) M128d {
	return M128d(svmlRoundPd([2]float64(a)))
}

func svmlRoundPd(a [2]float64) [2]float64


// SvmlRoundPs: Round the packed single-precision (32-bit) floating-point
// elements in 'a' to the nearest integer value, and store the results as
// packed single-precision floating-point elements in 'dst'. This intrinsic may
// generate the 'roundps'/'vroundps' instruction. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := ROUND(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_svml_round_ps'.
// Requires SSE.
func SvmlRoundPs(a M128) M128 {
	return M128(svmlRoundPs([4]float32(a)))
}

func svmlRoundPs(a [4]float32) [4]float32


// SvmlSqrtPd: Compute the square root of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. Note that
// this intrinsic is less efficient than '_mm_sqrt_pd'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := SQRT(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_svml_sqrt_pd'.
// Requires SSE.
func SvmlSqrtPd(a M128d) M128d {
	return M128d(svmlSqrtPd([2]float64(a)))
}

func svmlSqrtPd(a [2]float64) [2]float64


// SvmlSqrtPs: Compute the square root of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. Note that
// this intrinsic is less efficient than '_mm_sqrt_ps'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := SQRT(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_svml_sqrt_ps'.
// Requires SSE.
func SvmlSqrtPs(a M128) M128 {
	return M128(svmlSqrtPs([4]float32(a)))
}

func svmlSqrtPs(a [4]float32) [4]float32


// TanPd: Compute the tangent of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := TAN(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_tan_pd'.
// Requires SSE.
func TanPd(a M128d) M128d {
	return M128d(tanPd([2]float64(a)))
}

func tanPd(a [2]float64) [2]float64


// TanPs: Compute the tangent of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := TAN(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_tan_ps'.
// Requires SSE.
func TanPs(a M128) M128 {
	return M128(tanPs([4]float32(a)))
}

func tanPs(a [4]float32) [4]float32


// TandPd: Compute the tangent of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in degrees, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := TAND(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_tand_pd'.
// Requires SSE.
func TandPd(a M128d) M128d {
	return M128d(tandPd([2]float64(a)))
}

func tandPd(a [2]float64) [2]float64


// TandPs: Compute the tangent of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in degrees, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := TAND(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_tand_ps'.
// Requires SSE.
func TandPs(a M128) M128 {
	return M128(tandPs([4]float32(a)))
}

func tandPs(a [4]float32) [4]float32


// TanhPd: Compute the hyperbolic tangent of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := TANH(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_tanh_pd'.
// Requires SSE.
func TanhPd(a M128d) M128d {
	return M128d(tanhPd([2]float64(a)))
}

func tanhPd(a [2]float64) [2]float64


// TanhPs: Compute the hyperbolic tangent of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := TANH(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_tanh_ps'.
// Requires SSE.
func TanhPs(a M128) M128 {
	return M128(tanhPs([4]float32(a)))
}

func tanhPs(a [4]float32) [4]float32


// MaskTernarylogic32: Bitwise ternary logic that provides the capability to
// implement any three-operand binary function; the specific binary function is
// specified by value in 'imm8'. For each bit in each packed 32-bit integer,
// the corresponding bit from 'src', 'a', and 'b' are used to form a 3 bit
// index into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst' using writemask 'k' at 32-bit granularity (32-bit
// elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				FOR h := 0 to 31
//					index[2:0] := (src[i+h] << 2) OR (a[i+h] << 1) OR b[i+h]
//					dst[i+h] := imm8[index[2:0]]
//				ENDFOR
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPTERNLOGD'. Intrinsic: '_mm_mask_ternarylogic_epi32'.
// Requires AVX512F.
func MaskTernarylogic32(src M128i, k Mmask8, a M128i, b M128i, imm8 int) M128i {
	return M128i(maskTernarylogic32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b), imm8))
}

func maskTernarylogic32(src [16]byte, k uint8, a [16]byte, b [16]byte, imm8 int) [16]byte


// MaskzTernarylogic32: Bitwise ternary logic that provides the capability to
// implement any three-operand binary function; the specific binary function is
// specified by value in 'imm8'. For each bit in each packed 32-bit integer,
// the corresponding bit from 'a', 'b', and 'c' are used to form a 3 bit index
// into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst' using zeromask 'k' at 32-bit granularity (32-bit
// elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				FOR h := 0 to 31
//					index[2:0] := (a[i+h] << 2) OR (b[i+h] << 1) OR c[i+h]
//					dst[i+h] := imm8[index[2:0]]
//				ENDFOR
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPTERNLOGD'. Intrinsic: '_mm_maskz_ternarylogic_epi32'.
// Requires AVX512F.
func MaskzTernarylogic32(k Mmask8, a M128i, b M128i, c M128i, imm8 int) M128i {
	return M128i(maskzTernarylogic32(uint8(k), [16]byte(a), [16]byte(b), [16]byte(c), imm8))
}

func maskzTernarylogic32(k uint8, a [16]byte, b [16]byte, c [16]byte, imm8 int) [16]byte


// Ternarylogic32: Bitwise ternary logic that provides the capability to
// implement any three-operand binary function; the specific binary function is
// specified by value in 'imm8'. For each bit in each packed 32-bit integer,
// the corresponding bit from 'a', 'b', and 'c' are used to form a 3 bit index
// into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			FOR h := 0 to 31
//				index[2:0] := (a[i+h] << 2) OR (b[i+h] << 1) OR c[i+h]
//				dst[i+h] := imm8[index[2:0]]
//			ENDFOR
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPTERNLOGD'. Intrinsic: '_mm_ternarylogic_epi32'.
// Requires AVX512F.
func Ternarylogic32(a M128i, b M128i, c M128i, imm8 int) M128i {
	return M128i(ternarylogic32([16]byte(a), [16]byte(b), [16]byte(c), imm8))
}

func ternarylogic32(a [16]byte, b [16]byte, c [16]byte, imm8 int) [16]byte


// MaskTernarylogic64: Bitwise ternary logic that provides the capability to
// implement any three-operand binary function; the specific binary function is
// specified by value in 'imm8'. For each bit in each packed 64-bit integer,
// the corresponding bit from 'src', 'a', and 'b' are used to form a 3 bit
// index into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst' using writemask 'k' at 64-bit granularity (64-bit
// elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				FOR h := 0 to 63
//					index[2:0] := (src[i+h] << 2) OR (a[i+h] << 1) OR b[i+h]
//					dst[i+h] := imm8[index[2:0]]
//				ENDFOR
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPTERNLOGQ'. Intrinsic: '_mm_mask_ternarylogic_epi64'.
// Requires AVX512F.
func MaskTernarylogic64(src M128i, k Mmask8, a M128i, b M128i, imm8 int) M128i {
	return M128i(maskTernarylogic64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b), imm8))
}

func maskTernarylogic64(src [16]byte, k uint8, a [16]byte, b [16]byte, imm8 int) [16]byte


// MaskzTernarylogic64: Bitwise ternary logic that provides the capability to
// implement any three-operand binary function; the specific binary function is
// specified by value in 'imm8'. For each bit in each packed 64-bit integer,
// the corresponding bit from 'a', 'b', and 'c' are used to form a 3 bit index
// into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst' using zeromask 'k' at 64-bit granularity (64-bit
// elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				FOR h := 0 to 63
//					index[2:0] := (a[i+h] << 2) OR (b[i+h] << 1) OR c[i+h]
//					dst[i+h] := imm8[index[2:0]]
//				ENDFOR
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPTERNLOGQ'. Intrinsic: '_mm_maskz_ternarylogic_epi64'.
// Requires AVX512F.
func MaskzTernarylogic64(k Mmask8, a M128i, b M128i, c M128i, imm8 int) M128i {
	return M128i(maskzTernarylogic64(uint8(k), [16]byte(a), [16]byte(b), [16]byte(c), imm8))
}

func maskzTernarylogic64(k uint8, a [16]byte, b [16]byte, c [16]byte, imm8 int) [16]byte


// Ternarylogic64: Bitwise ternary logic that provides the capability to
// implement any three-operand binary function; the specific binary function is
// specified by value in 'imm8'. For each bit in each packed 64-bit integer,
// the corresponding bit from 'a', 'b', and 'c' are used to form a 3 bit index
// into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			FOR h := 0 to 63
//				index[2:0] := (a[i+h] << 2) OR (b[i+h] << 1) OR c[i+h]
//				dst[i+h] := imm8[index[2:0]]
//			ENDFOR
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPTERNLOGQ'. Intrinsic: '_mm_ternarylogic_epi64'.
// Requires AVX512F.
func Ternarylogic64(a M128i, b M128i, c M128i, imm8 int) M128i {
	return M128i(ternarylogic64([16]byte(a), [16]byte(b), [16]byte(c), imm8))
}

func ternarylogic64(a [16]byte, b [16]byte, c [16]byte, imm8 int) [16]byte


// MaskTest16Mask: Compute the bitwise AND of packed 16-bit integers in 'a' and
// 'b', producing intermediate 16-bit values, and set the corresponding bit in
// result mask 'k' (subject to writemask 'k') if the intermediate value is
// non-zero. 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k1[j]
//				k[j] := ((a[i+15:i] AND b[i+15:i]) != 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPTESTMW'. Intrinsic: '_mm_mask_test_epi16_mask'.
// Requires AVX512BW.
func MaskTest16Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskTest16Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskTest16Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// Test16Mask: Compute the bitwise AND of packed 16-bit integers in 'a' and
// 'b', producing intermediate 16-bit values, and set the corresponding bit in
// result mask 'k' if the intermediate value is non-zero. 
//
//		FOR j := 0 to 7
//			i := j*16
//			k[j] := ((a[i+15:i] AND b[i+15:i]) != 0) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPTESTMW'. Intrinsic: '_mm_test_epi16_mask'.
// Requires AVX512BW.
func Test16Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(test16Mask([16]byte(a), [16]byte(b)))
}

func test16Mask(a [16]byte, b [16]byte) uint8


// MaskTest32Mask: Compute the bitwise AND of packed 32-bit integers in 'a' and
// 'b', producing intermediate 32-bit values, and set the corresponding bit in
// result mask 'k' (subject to writemask 'k') if the intermediate value is
// non-zero. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ((a[i+31:i] AND b[i+31:i]) != 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPTESTMD'. Intrinsic: '_mm_mask_test_epi32_mask'.
// Requires AVX512F.
func MaskTest32Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskTest32Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskTest32Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// Test32Mask: Compute the bitwise AND of packed 32-bit integers in 'a' and
// 'b', producing intermediate 32-bit values, and set the corresponding bit in
// result mask 'k' if the intermediate value is non-zero. 
//
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ((a[i+31:i] AND b[i+31:i]) != 0) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPTESTMD'. Intrinsic: '_mm_test_epi32_mask'.
// Requires AVX512F.
func Test32Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(test32Mask([16]byte(a), [16]byte(b)))
}

func test32Mask(a [16]byte, b [16]byte) uint8


// MaskTest64Mask: Compute the bitwise AND of packed 64-bit integers in 'a' and
// 'b', producing intermediate 64-bit values, and set the corresponding bit in
// result mask 'k' (subject to writemask 'k') if the intermediate value is
// non-zero. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ((a[i+63:i] AND b[i+63:i]) != 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPTESTMQ'. Intrinsic: '_mm_mask_test_epi64_mask'.
// Requires AVX512F.
func MaskTest64Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskTest64Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskTest64Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// Test64Mask: Compute the bitwise AND of packed 64-bit integers in 'a' and
// 'b', producing intermediate 64-bit values, and set the corresponding bit in
// result mask 'k' if the intermediate value is non-zero. 
//
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ((a[i+63:i] AND b[i+63:i]) != 0) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPTESTMQ'. Intrinsic: '_mm_test_epi64_mask'.
// Requires AVX512F.
func Test64Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(test64Mask([16]byte(a), [16]byte(b)))
}

func test64Mask(a [16]byte, b [16]byte) uint8


// MaskTest8Mask: Compute the bitwise AND of packed 8-bit integers in 'a' and
// 'b', producing intermediate 8-bit values, and set the corresponding bit in
// result mask 'k' (subject to writemask 'k') if the intermediate value is
// non-zero. 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k1[j]
//				k[j] := ((a[i+7:i] AND b[i+7:i]) != 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPTESTMB'. Intrinsic: '_mm_mask_test_epi8_mask'.
// Requires AVX512BW.
func MaskTest8Mask(k1 Mmask16, a M128i, b M128i) Mmask16 {
	return Mmask16(maskTest8Mask(uint16(k1), [16]byte(a), [16]byte(b)))
}

func maskTest8Mask(k1 uint16, a [16]byte, b [16]byte) uint16


// Test8Mask: Compute the bitwise AND of packed 8-bit integers in 'a' and 'b',
// producing intermediate 8-bit values, and set the corresponding bit in result
// mask 'k' if the intermediate value is non-zero. 
//
//		FOR j := 0 to 15
//			i := j*8
//			k[j] := ((a[i+7:i] AND b[i+7:i]) != 0) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPTESTMB'. Intrinsic: '_mm_test_epi8_mask'.
// Requires AVX512BW.
func Test8Mask(a M128i, b M128i) Mmask16 {
	return Mmask16(test8Mask([16]byte(a), [16]byte(b)))
}

func test8Mask(a [16]byte, b [16]byte) uint16


// TestcPd: Compute the bitwise AND of 128 bits (representing double-precision
// (64-bit) floating-point elements) in 'a' and 'b', producing an intermediate
// 128-bit value, and set 'ZF' to 1 if the sign bit of each 64-bit element in
// the intermediate value is zero, otherwise set 'ZF' to 0. Compute the bitwise
// AND NOT of 'a' and 'b', producing an intermediate value, and set 'CF' to 1
// if the sign bit of each 64-bit element in the intermediate value is zero,
// otherwise set 'CF' to 0. Return the 'CF' value. 
//
//		tmp[127:0] := a[127:0] AND b[127:0]
//		IF (tmp[63] == tmp[127] == 0)
//			ZF := 1
//		ELSE
//			ZF := 0
//		FI
//		tmp[127:0] := a[127:0] AND NOT b[127:0]
//		IF (tmp[63] == tmp[127] == 0)
//			CF := 1
//		ELSE
//			CF := 0
//		FI
//		RETURN CF
//
// Instruction: 'VTESTPD'. Intrinsic: '_mm_testc_pd'.
// Requires AVX.
func TestcPd(a M128d, b M128d) int {
	return int(testcPd([2]float64(a), [2]float64(b)))
}

func testcPd(a [2]float64, b [2]float64) int


// TestcPs: Compute the bitwise AND of 128 bits (representing single-precision
// (32-bit) floating-point elements) in 'a' and 'b', producing an intermediate
// 128-bit value, and set 'ZF' to 1 if the sign bit of each 32-bit element in
// the intermediate value is zero, otherwise set 'ZF' to 0. Compute the bitwise
// AND NOT of 'a' and 'b', producing an intermediate value, and set 'CF' to 1
// if the sign bit of each 32-bit element in the intermediate value is zero,
// otherwise set 'CF' to 0. Return the 'CF' value. 
//
//		tmp[127:0] := a[127:0] AND b[127:0]
//		IF (tmp[31] == tmp[63] == tmp[95] == tmp[127] == 0)
//			ZF := 1
//		ELSE
//			ZF := 0
//		FI
//		tmp[127:0] := a[127:0] AND NOT b[127:0]
//		IF (tmp[31] == tmp[63] == tmp[95] == tmp[127] == 0)
//			CF := 1
//		ELSE
//			CF := 0
//		FI
//		RETURN CF
//
// Instruction: 'VTESTPS'. Intrinsic: '_mm_testc_ps'.
// Requires AVX.
func TestcPs(a M128, b M128) int {
	return int(testcPs([4]float32(a), [4]float32(b)))
}

func testcPs(a [4]float32, b [4]float32) int


// TestcSi128: Compute the bitwise AND of 128 bits (representing integer data)
// in 'a' and 'b', and set 'ZF' to 1 if the result is zero, otherwise set 'ZF'
// to 0. Compute the bitwise AND NOT of 'a' and 'b', and set 'CF' to 1 if the
// result is zero, otherwise set 'CF' to 0. Return the 'CF' value. 
//
//		IF (a[127:0] AND b[127:0] == 0)
//			ZF := 1
//		ELSE
//			ZF := 0
//		FI
//		IF (a[127:0] AND NOT b[127:0] == 0)
//			CF := 1
//		ELSE
//			CF := 0
//		FI
//		RETURN CF
//
// Instruction: 'PTEST'. Intrinsic: '_mm_testc_si128'.
// Requires SSE4.1.
func TestcSi128(a M128i, b M128i) int {
	return int(testcSi128([16]byte(a), [16]byte(b)))
}

func testcSi128(a [16]byte, b [16]byte) int


// MaskTestn16Mask: Compute the bitwise NAND of packed 16-bit integers in 'a'
// and 'b', producing intermediate 16-bit values, and set the corresponding bit
// in result mask 'k' (subject to writemask 'k') if the intermediate value is
// zero. 
//
//		FOR j := 0 to 7
//			i := j*16
//			IF k1[j]
//				k[j] := ((a[i+15:i] AND b[i+15:i]) == 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPTESTNMW'. Intrinsic: '_mm_mask_testn_epi16_mask'.
// Requires AVX512BW.
func MaskTestn16Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskTestn16Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskTestn16Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// Testn16Mask: Compute the bitwise NAND of packed 16-bit integers in 'a' and
// 'b', producing intermediate 16-bit values, and set the corresponding bit in
// result mask 'k' if the intermediate value is zero. 
//
//		FOR j := 0 to 7
//			i := j*16
//			k[j] := ((a[i+15:i] AND b[i+15:i]) == 0) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPTESTNMW'. Intrinsic: '_mm_testn_epi16_mask'.
// Requires AVX512BW.
func Testn16Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(testn16Mask([16]byte(a), [16]byte(b)))
}

func testn16Mask(a [16]byte, b [16]byte) uint8


// MaskTestn32Mask: Compute the bitwise NAND of packed 32-bit integers in 'a'
// and 'b', producing intermediate 32-bit values, and set the corresponding bit
// in result mask 'k' (subject to writemask 'k') if the intermediate value is
// zero. 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k1[j]
//				k[j] := ((a[i+31:i] AND b[i+31:i]) == 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPTESTNMD'. Intrinsic: '_mm_mask_testn_epi32_mask'.
// Requires AVX512F.
func MaskTestn32Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskTestn32Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskTestn32Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// Testn32Mask: Compute the bitwise NAND of packed 32-bit integers in 'a' and
// 'b', producing intermediate 32-bit values, and set the corresponding bit in
// result mask 'k' if the intermediate value is zero. 
//
//		FOR j := 0 to 3
//			i := j*32
//			k[j] := ((a[i+31:i] NAND b[i+31:i]) == 0) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPTESTNMD'. Intrinsic: '_mm_testn_epi32_mask'.
// Requires AVX512F.
func Testn32Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(testn32Mask([16]byte(a), [16]byte(b)))
}

func testn32Mask(a [16]byte, b [16]byte) uint8


// MaskTestn64Mask: Compute the bitwise NAND of packed 64-bit integers in 'a'
// and 'b', producing intermediate 64-bit values, and set the corresponding bit
// in result mask 'k' (subject to writemask 'k') if the intermediate value is
// zero. 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k1[j]
//				k[j] := ((a[i+63:i] AND b[i+63:i]) == 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPTESTNMQ'. Intrinsic: '_mm_mask_testn_epi64_mask'.
// Requires AVX512F.
func MaskTestn64Mask(k1 Mmask8, a M128i, b M128i) Mmask8 {
	return Mmask8(maskTestn64Mask(uint8(k1), [16]byte(a), [16]byte(b)))
}

func maskTestn64Mask(k1 uint8, a [16]byte, b [16]byte) uint8


// Testn64Mask: Compute the bitwise NAND of packed 64-bit integers in 'a' and
// 'b', producing intermediate 64-bit values, and set the corresponding bit in
// result mask 'k' if the intermediate value is zero. 
//
//		FOR j := 0 to 1
//			i := j*64
//			k[j] := ((a[i+63:i] AND b[i+63:i]) == 0) ? 1 : 0
//		ENDFOR
//		k[MAX:2] := 0
//
// Instruction: 'VPTESTNMQ'. Intrinsic: '_mm_testn_epi64_mask'.
// Requires AVX512F.
func Testn64Mask(a M128i, b M128i) Mmask8 {
	return Mmask8(testn64Mask([16]byte(a), [16]byte(b)))
}

func testn64Mask(a [16]byte, b [16]byte) uint8


// MaskTestn8Mask: Compute the bitwise NAND of packed 8-bit integers in 'a' and
// 'b', producing intermediate 8-bit values, and set the corresponding bit in
// result mask 'k' (subject to writemask 'k') if the intermediate value is
// zero. 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF k1[j]
//				k[j] := ((a[i+7:i] AND b[i+7:i]) == 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPTESTNMB'. Intrinsic: '_mm_mask_testn_epi8_mask'.
// Requires AVX512BW.
func MaskTestn8Mask(k1 Mmask16, a M128i, b M128i) Mmask16 {
	return Mmask16(maskTestn8Mask(uint16(k1), [16]byte(a), [16]byte(b)))
}

func maskTestn8Mask(k1 uint16, a [16]byte, b [16]byte) uint16


// Testn8Mask: Compute the bitwise NAND of packed 8-bit integers in 'a' and
// 'b', producing intermediate 8-bit values, and set the corresponding bit in
// result mask 'k' if the intermediate value is zero. 
//
//		FOR j := 0 to 15
//			i := j*8
//			k[j] := ((a[i+7:i] AND b[i+7:i]) == 0) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPTESTNMB'. Intrinsic: '_mm_testn_epi8_mask'.
// Requires AVX512BW.
func Testn8Mask(a M128i, b M128i) Mmask16 {
	return Mmask16(testn8Mask([16]byte(a), [16]byte(b)))
}

func testn8Mask(a [16]byte, b [16]byte) uint16


// TestnzcPd: Compute the bitwise AND of 128 bits (representing
// double-precision (64-bit) floating-point elements) in 'a' and 'b', producing
// an intermediate 128-bit value, and set 'ZF' to 1 if the sign bit of each
// 64-bit element in the intermediate value is zero, otherwise set 'ZF' to 0.
// Compute the bitwise AND NOT of 'a' and 'b', producing an intermediate value,
// and set 'CF' to 1 if the sign bit of each 64-bit element in the intermediate
// value is zero, otherwise set 'CF' to 0. Return 1 if both the 'ZF' and 'CF'
// values are zero, otherwise return 0. 
//
//		tmp[127:0] := a[127:0] AND b[127:0]
//		IF (tmp[63] == tmp[127] == 0)
//			ZF := 1
//		ELSE
//			ZF := 0
//		FI
//		tmp[127:0] := a[127:0] AND NOT b[127:0]
//		IF (tmp[63] == tmp[127] == 0)
//			CF := 1
//		ELSE
//			CF := 0
//		FI
//		IF (ZF == 0 && CF == 0)
//			RETURN 1
//		ELSE
//			RETURN 0
//		FI
//
// Instruction: 'VTESTPD'. Intrinsic: '_mm_testnzc_pd'.
// Requires AVX.
func TestnzcPd(a M128d, b M128d) int {
	return int(testnzcPd([2]float64(a), [2]float64(b)))
}

func testnzcPd(a [2]float64, b [2]float64) int


// TestnzcPs: Compute the bitwise AND of 128 bits (representing
// single-precision (32-bit) floating-point elements) in 'a' and 'b', producing
// an intermediate 128-bit value, and set 'ZF' to 1 if the sign bit of each
// 32-bit element in the intermediate value is zero, otherwise set 'ZF' to 0.
// Compute the bitwise AND NOT of 'a' and 'b', producing an intermediate value,
// and set 'CF' to 1 if the sign bit of each 32-bit element in the intermediate
// value is zero, otherwise set 'CF' to 0. Return 1 if both the 'ZF' and 'CF'
// values are zero, otherwise return 0. 
//
//		tmp[127:0] := a[127:0] AND b[127:0]
//		IF (tmp[31] == tmp[63] == tmp[95] == tmp[127] == 0)
//			ZF := 1
//		ELSE
//			ZF := 0
//		FI
//		tmp[127:0] := a[127:0] AND NOT b[127:0]
//		IF (tmp[31] == tmp[63] == tmp[95] == tmp[127] == 0)
//			CF := 1
//		ELSE
//			CF := 0
//		FI
//		IF (ZF == 0 && CF == 0)
//			RETURN 1
//		ELSE
//			RETURN 0
//		FI
//
// Instruction: 'VTESTPS'. Intrinsic: '_mm_testnzc_ps'.
// Requires AVX.
func TestnzcPs(a M128, b M128) int {
	return int(testnzcPs([4]float32(a), [4]float32(b)))
}

func testnzcPs(a [4]float32, b [4]float32) int


// TestnzcSi128: Compute the bitwise AND of 128 bits (representing integer
// data) in 'a' and 'b', and set 'ZF' to 1 if the result is zero, otherwise set
// 'ZF' to 0. Compute the bitwise AND NOT of 'a' and 'b', and set 'CF' to 1 if
// the result is zero, otherwise set 'CF' to 0. Return 1 if both the 'ZF' and
// 'CF' values are zero, otherwise return 0. 
//
//		IF (a[127:0] AND b[127:0] == 0)
//			ZF := 1
//		ELSE
//			ZF := 0
//		FI
//		IF (a[127:0] AND NOT b[127:0] == 0)
//			CF := 1
//		ELSE
//			CF := 0
//		FI
//		IF (ZF == 0 && CF == 0)
//			RETURN 1
//		ELSE
//			RETURN 0
//		FI
//
// Instruction: 'PTEST'. Intrinsic: '_mm_testnzc_si128'.
// Requires SSE4.1.
func TestnzcSi128(a M128i, b M128i) int {
	return int(testnzcSi128([16]byte(a), [16]byte(b)))
}

func testnzcSi128(a [16]byte, b [16]byte) int


// TestzPd: Compute the bitwise AND of 128 bits (representing double-precision
// (64-bit) floating-point elements) in 'a' and 'b', producing an intermediate
// 128-bit value, and set 'ZF' to 1 if the sign bit of each 64-bit element in
// the intermediate value is zero, otherwise set 'ZF' to 0. Compute the bitwise
// AND NOT of 'a' and 'b', producing an intermediate value, and set 'CF' to 1
// if the sign bit of each 64-bit element in the intermediate value is zero,
// otherwise set 'CF' to 0. Return the 'ZF' value. 
//
//		tmp[127:0] := a[127:0] AND b[127:0]
//		IF (tmp[63] == tmp[127] == 0)
//			ZF := 1
//		ELSE
//			ZF := 0
//		FI
//		tmp[127:0] := a[127:0] AND NOT b[127:0]
//		IF (tmp[63] == tmp[127] == 0)
//			CF := 1
//		ELSE
//			CF := 0
//		FI
//		RETURN ZF
//
// Instruction: 'VTESTPD'. Intrinsic: '_mm_testz_pd'.
// Requires AVX.
func TestzPd(a M128d, b M128d) int {
	return int(testzPd([2]float64(a), [2]float64(b)))
}

func testzPd(a [2]float64, b [2]float64) int


// TestzPs: Compute the bitwise AND of 128 bits (representing single-precision
// (32-bit) floating-point elements) in 'a' and 'b', producing an intermediate
// 128-bit value, and set 'ZF' to 1 if the sign bit of each 32-bit element in
// the intermediate value is zero, otherwise set 'ZF' to 0. Compute the bitwise
// AND NOT of 'a' and 'b', producing an intermediate value, and set 'CF' to 1
// if the sign bit of each 32-bit element in the intermediate value is zero,
// otherwise set 'CF' to 0. Return the 'ZF' value. 
//
//		tmp[127:0] := a[127:0] AND b[127:0]
//		IF (tmp[31] == tmp[63] == tmp[95] == tmp[127] == 0)
//			ZF := 1
//		ELSE
//			ZF := 0
//		FI
//		tmp[127:0] := a[127:0] AND NOT b[127:0]
//		IF (tmp[31] == tmp[63] == tmp[95] == tmp[127] == 0)
//			CF := 1
//		ELSE
//			CF := 0
//		FI
//		RETURN ZF
//
// Instruction: 'VTESTPS'. Intrinsic: '_mm_testz_ps'.
// Requires AVX.
func TestzPs(a M128, b M128) int {
	return int(testzPs([4]float32(a), [4]float32(b)))
}

func testzPs(a [4]float32, b [4]float32) int


// TestzSi128: Compute the bitwise AND of 128 bits (representing integer data)
// in 'a' and 'b', and set 'ZF' to 1 if the result is zero, otherwise set 'ZF'
// to 0. Compute the bitwise AND NOT of 'a' and 'b', and set 'CF' to 1 if the
// result is zero, otherwise set 'CF' to 0. Return the 'ZF' value. 
//
//		IF (a[127:0] AND b[127:0] == 0)
//			ZF := 1
//		ELSE
//			ZF := 0
//		FI
//		IF (a[127:0] AND NOT b[127:0] == 0)
//			CF := 1
//		ELSE
//			CF := 0
//		FI
//		RETURN ZF
//
// Instruction: 'PTEST'. Intrinsic: '_mm_testz_si128'.
// Requires SSE4.1.
func TestzSi128(a M128i, b M128i) int {
	return int(testzSi128([16]byte(a), [16]byte(b)))
}

func testzSi128(a [16]byte, b [16]byte) int


// TruncPd: Truncate the packed double-precision (64-bit) floating-point
// elements in 'a', and store the results as packed double-precision
// floating-point elements in 'dst'. This intrinsic may generate the
// 'roundpd'/'vroundpd' instruction. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := TRUNCATE(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_trunc_pd'.
// Requires SSE.
func TruncPd(a M128d) M128d {
	return M128d(truncPd([2]float64(a)))
}

func truncPd(a [2]float64) [2]float64


// TruncPs: Truncate the packed single-precision (32-bit) floating-point
// elements in 'a', and store the results as packed single-precision
// floating-point elements in 'dst'. This intrinsic may generate the
// 'roundps'/'vroundps' instruction. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := TRUNCATE(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_trunc_ps'.
// Requires SSE.
func TruncPs(a M128) M128 {
	return M128(truncPs([4]float32(a)))
}

func truncPs(a [4]float32) [4]float32


// UcomieqSd: Compare the lower double-precision (64-bit) floating-point
// element in 'a' and 'b' for equality, and return the boolean result (0 or 1).
// This instruction will not signal an exception for QNaNs. 
//
//		RETURN ( a[63:0] == b[63:0] ) ? 1 : 0
//
// Instruction: 'UCOMISD'. Intrinsic: '_mm_ucomieq_sd'.
// Requires SSE2.
func UcomieqSd(a M128d, b M128d) int {
	return int(ucomieqSd([2]float64(a), [2]float64(b)))
}

func ucomieqSd(a [2]float64, b [2]float64) int


// UcomieqSs: Compare the lower single-precision (32-bit) floating-point
// element in 'a' and 'b' for equality, and return the boolean result (0 or 1).
// This instruction will not signal an exception for QNaNs. 
//
//		RETURN ( a[31:0] == b[31:0] ) ? 1 : 0
//
// Instruction: 'UCOMISS'. Intrinsic: '_mm_ucomieq_ss'.
// Requires SSE.
func UcomieqSs(a M128, b M128) int {
	return int(ucomieqSs([4]float32(a), [4]float32(b)))
}

func ucomieqSs(a [4]float32, b [4]float32) int


// UcomigeSd: Compare the lower double-precision (64-bit) floating-point
// element in 'a' and 'b' for greater-than-or-equal, and return the boolean
// result (0 or 1). This instruction will not signal an exception for QNaNs. 
//
//		RETURN ( a[63:0] >= b[63:0] ) ? 1 : 0
//
// Instruction: 'UCOMISD'. Intrinsic: '_mm_ucomige_sd'.
// Requires SSE2.
func UcomigeSd(a M128d, b M128d) int {
	return int(ucomigeSd([2]float64(a), [2]float64(b)))
}

func ucomigeSd(a [2]float64, b [2]float64) int


// UcomigeSs: Compare the lower single-precision (32-bit) floating-point
// element in 'a' and 'b' for greater-than-or-equal, and return the boolean
// result (0 or 1). This instruction will not signal an exception for QNaNs. 
//
//		RETURN ( a[31:0] >= b[31:0] ) ? 1 : 0
//
// Instruction: 'UCOMISS'. Intrinsic: '_mm_ucomige_ss'.
// Requires SSE.
func UcomigeSs(a M128, b M128) int {
	return int(ucomigeSs([4]float32(a), [4]float32(b)))
}

func ucomigeSs(a [4]float32, b [4]float32) int


// UcomigtSd: Compare the lower double-precision (64-bit) floating-point
// element in 'a' and 'b' for greater-than, and return the boolean result (0 or
// 1). This instruction will not signal an exception for QNaNs. 
//
//		RETURN ( a[63:0] > b[63:0] ) ? 1 : 0
//
// Instruction: 'UCOMISD'. Intrinsic: '_mm_ucomigt_sd'.
// Requires SSE2.
func UcomigtSd(a M128d, b M128d) int {
	return int(ucomigtSd([2]float64(a), [2]float64(b)))
}

func ucomigtSd(a [2]float64, b [2]float64) int


// UcomigtSs: Compare the lower single-precision (32-bit) floating-point
// element in 'a' and 'b' for greater-than, and return the boolean result (0 or
// 1). This instruction will not signal an exception for QNaNs. 
//
//		RETURN ( a[31:0] > b[31:0] ) ? 1 : 0
//
// Instruction: 'UCOMISS'. Intrinsic: '_mm_ucomigt_ss'.
// Requires SSE.
func UcomigtSs(a M128, b M128) int {
	return int(ucomigtSs([4]float32(a), [4]float32(b)))
}

func ucomigtSs(a [4]float32, b [4]float32) int


// UcomileSd: Compare the lower double-precision (64-bit) floating-point
// element in 'a' and 'b' for less-than-or-equal, and return the boolean result
// (0 or 1). This instruction will not signal an exception for QNaNs. 
//
//		RETURN ( a[63:0] <= b[63:0] ) ? 1 : 0
//
// Instruction: 'UCOMISD'. Intrinsic: '_mm_ucomile_sd'.
// Requires SSE2.
func UcomileSd(a M128d, b M128d) int {
	return int(ucomileSd([2]float64(a), [2]float64(b)))
}

func ucomileSd(a [2]float64, b [2]float64) int


// UcomileSs: Compare the lower single-precision (32-bit) floating-point
// element in 'a' and 'b' for less-than-or-equal, and return the boolean result
// (0 or 1). This instruction will not signal an exception for QNaNs. 
//
//		RETURN ( a[31:0] <= b[31:0] ) ? 1 : 0
//
// Instruction: 'UCOMISS'. Intrinsic: '_mm_ucomile_ss'.
// Requires SSE.
func UcomileSs(a M128, b M128) int {
	return int(ucomileSs([4]float32(a), [4]float32(b)))
}

func ucomileSs(a [4]float32, b [4]float32) int


// UcomiltSd: Compare the lower double-precision (64-bit) floating-point
// element in 'a' and 'b' for less-than, and return the boolean result (0 or
// 1). This instruction will not signal an exception for QNaNs. 
//
//		RETURN ( a[63:0] < b[63:0] ) ? 1 : 0
//
// Instruction: 'UCOMISD'. Intrinsic: '_mm_ucomilt_sd'.
// Requires SSE2.
func UcomiltSd(a M128d, b M128d) int {
	return int(ucomiltSd([2]float64(a), [2]float64(b)))
}

func ucomiltSd(a [2]float64, b [2]float64) int


// UcomiltSs: Compare the lower single-precision (32-bit) floating-point
// element in 'a' and 'b' for less-than, and return the boolean result (0 or
// 1). This instruction will not signal an exception for QNaNs. 
//
//		RETURN ( a[31:0] < b[31:0] ) ? 1 : 0
//
// Instruction: 'UCOMISS'. Intrinsic: '_mm_ucomilt_ss'.
// Requires SSE.
func UcomiltSs(a M128, b M128) int {
	return int(ucomiltSs([4]float32(a), [4]float32(b)))
}

func ucomiltSs(a [4]float32, b [4]float32) int


// UcomineqSd: Compare the lower double-precision (64-bit) floating-point
// element in 'a' and 'b' for not-equal, and return the boolean result (0 or
// 1). This instruction will not signal an exception for QNaNs. 
//
//		RETURN ( a[63:0] != b[63:0] ) ? 1 : 0
//
// Instruction: 'UCOMISD'. Intrinsic: '_mm_ucomineq_sd'.
// Requires SSE2.
func UcomineqSd(a M128d, b M128d) int {
	return int(ucomineqSd([2]float64(a), [2]float64(b)))
}

func ucomineqSd(a [2]float64, b [2]float64) int


// UcomineqSs: Compare the lower single-precision (32-bit) floating-point
// element in 'a' and 'b' for not-equal, and return the boolean result (0 or
// 1). This instruction will not signal an exception for QNaNs. 
//
//		RETURN ( a[31:0] != b[31:0] ) ? 1 : 0
//
// Instruction: 'UCOMISS'. Intrinsic: '_mm_ucomineq_ss'.
// Requires SSE.
func UcomineqSs(a M128, b M128) int {
	return int(ucomineqSs([4]float32(a), [4]float32(b)))
}

func ucomineqSs(a [4]float32, b [4]float32) int


// Udiv32: Divide packed unsigned 32-bit integers in 'a' by packed elements in
// 'b', and store the truncated results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			dst[i+31:i] := TRUNCATE(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_udiv_epi32'.
// Requires SSE.
func Udiv32(a M128i, b M128i) M128i {
	return M128i(udiv32([16]byte(a), [16]byte(b)))
}

func udiv32(a [16]byte, b [16]byte) [16]byte


// Udivrem32: Divide packed unsigned 32-bit integers in 'a' by packed elements
// in 'b', store the truncated results in 'dst', and store the remainders as
// packed unsigned 32-bit integers into memory at 'mem_addr'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			dst[i+31:i] := TRUNCATE(a[i+31:i] / b[i+31:i])
//			MEM[mem_addr+i+31:mem_addr+i] := REMAINDER(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_udivrem_epi32'.
// Requires SSE.
func Udivrem32(mem_addr M128i, a M128i, b M128i) M128i {
	return M128i(udivrem32([16]byte(mem_addr), [16]byte(a), [16]byte(b)))
}

func udivrem32(mem_addr [16]byte, a [16]byte, b [16]byte) [16]byte


// UndefinedPd: Return vector of type __m128d with undefined elements. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm_undefined_pd'.
// Requires AVX.
func UndefinedPd() M128d {
	return M128d(undefinedPd())
}

func undefinedPd() [2]float64


// UndefinedPs: Return vector of type __m128 with undefined elements. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm_undefined_ps'.
// Requires AVX.
func UndefinedPs() M128 {
	return M128(undefinedPs())
}

func undefinedPs() [4]float32


// UndefinedSi128: Return vector of type __m128i with undefined elements. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm_undefined_si128'.
// Requires AVX.
func UndefinedSi128() M128i {
	return M128i(undefinedSi128())
}

func undefinedSi128() [16]byte


// MaskUnpackhi16: Unpack and interleave 16-bit integers from the high half of
// 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_WORDS(src1[127:0], src2[127:0]){
//			dst[15:0] := src1[79:64]
//			dst[31:16] := src2[79:64] 
//			dst[47:32] := src1[95:80] 
//			dst[63:48] := src2[95:80] 
//			dst[79:64] := src1[111:96] 
//			dst[95:80] := src2[111:96] 
//			dst[111:96] := src1[127:112] 
//			dst[127:112] := src2[127:112] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_WORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPUNPCKHWD'. Intrinsic: '_mm_mask_unpackhi_epi16'.
// Requires AVX512BW.
func MaskUnpackhi16(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskUnpackhi16([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskUnpackhi16(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzUnpackhi16: Unpack and interleave 16-bit integers from the high half of
// 'a' and 'b', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_WORDS(src1[127:0], src2[127:0]){
//			dst[15:0] := src1[79:64]
//			dst[31:16] := src2[79:64] 
//			dst[47:32] := src1[95:80] 
//			dst[63:48] := src2[95:80] 
//			dst[79:64] := src1[111:96] 
//			dst[95:80] := src2[111:96] 
//			dst[111:96] := src1[127:112] 
//			dst[127:112] := src2[127:112] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_WORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPUNPCKHWD'. Intrinsic: '_mm_maskz_unpackhi_epi16'.
// Requires AVX512BW.
func MaskzUnpackhi16(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzUnpackhi16(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzUnpackhi16(k uint8, a [16]byte, b [16]byte) [16]byte


// Unpackhi16: Unpack and interleave 16-bit integers from the high half of 'a'
// and 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_HIGH_WORDS(src1[127:0], src2[127:0]){
//			dst[15:0] := src1[79:64]
//			dst[31:16] := src2[79:64] 
//			dst[47:32] := src1[95:80] 
//			dst[63:48] := src2[95:80] 
//			dst[79:64] := src1[111:96] 
//			dst[95:80] := src2[111:96] 
//			dst[111:96] := src1[127:112] 
//			dst[127:112] := src2[127:112] 
//			RETURN dst[127:0]
//		}
//		
//		dst[127:0] := INTERLEAVE_HIGH_WORDS(a[127:0], b[127:0])
//
// Instruction: 'PUNPCKHWD'. Intrinsic: '_mm_unpackhi_epi16'.
// Requires SSE2.
func Unpackhi16(a M128i, b M128i) M128i {
	return M128i(unpackhi16([16]byte(a), [16]byte(b)))
}

func unpackhi16(a [16]byte, b [16]byte) [16]byte


// MaskUnpackhi32: Unpack and interleave 32-bit integers from the high half of
// 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPUNPCKHDQ'. Intrinsic: '_mm_mask_unpackhi_epi32'.
// Requires AVX512F.
func MaskUnpackhi32(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskUnpackhi32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskUnpackhi32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzUnpackhi32: Unpack and interleave 32-bit integers from the high half of
// 'a' and 'b', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPUNPCKHDQ'. Intrinsic: '_mm_maskz_unpackhi_epi32'.
// Requires AVX512F.
func MaskzUnpackhi32(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzUnpackhi32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzUnpackhi32(k uint8, a [16]byte, b [16]byte) [16]byte


// Unpackhi32: Unpack and interleave 32-bit integers from the high half of 'a'
// and 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}	
//		
//		dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//
// Instruction: 'PUNPCKHDQ'. Intrinsic: '_mm_unpackhi_epi32'.
// Requires SSE2.
func Unpackhi32(a M128i, b M128i) M128i {
	return M128i(unpackhi32([16]byte(a), [16]byte(b)))
}

func unpackhi32(a [16]byte, b [16]byte) [16]byte


// MaskUnpackhi64: Unpack and interleave 64-bit integers from the high half of
// 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPUNPCKHQDQ'. Intrinsic: '_mm_mask_unpackhi_epi64'.
// Requires AVX512F.
func MaskUnpackhi64(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskUnpackhi64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskUnpackhi64(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzUnpackhi64: Unpack and interleave 64-bit integers from the high half of
// 'a' and 'b', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPUNPCKHQDQ'. Intrinsic: '_mm_maskz_unpackhi_epi64'.
// Requires AVX512F.
func MaskzUnpackhi64(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzUnpackhi64(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzUnpackhi64(k uint8, a [16]byte, b [16]byte) [16]byte


// Unpackhi64: Unpack and interleave 64-bit integers from the high half of 'a'
// and 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//
// Instruction: 'PUNPCKHQDQ'. Intrinsic: '_mm_unpackhi_epi64'.
// Requires SSE2.
func Unpackhi64(a M128i, b M128i) M128i {
	return M128i(unpackhi64([16]byte(a), [16]byte(b)))
}

func unpackhi64(a [16]byte, b [16]byte) [16]byte


// MaskUnpackhi8: Unpack and interleave 8-bit integers from the high half of
// 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_BYTES(src1[127:0], src2[127:0]){
//			dst[7:0] := src1[71:64] 
//			dst[15:8] := src2[71:64] 
//			dst[23:16] := src1[79:72] 
//			dst[31:24] := src2[79:72] 
//			dst[39:32] := src1[87:80] 
//			dst[47:40] := src2[87:80] 
//			dst[55:48] := src1[95:88] 
//			dst[63:56] := src2[95:88] 
//			dst[71:64] := src1[103:96] 
//			dst[79:72] := src2[103:96] 
//			dst[87:80] := src1[111:104] 
//			dst[95:88] := src2[111:104] 
//			dst[103:96] := src1[119:112] 
//			dst[111:104] := src2[119:112] 
//			dst[119:112] := src1[127:120] 
//			dst[127:120] := src2[127:120] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_BYTES(a[127:0], b[127:0])
//		
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := tmp_dst[i+7:i]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPUNPCKHBW'. Intrinsic: '_mm_mask_unpackhi_epi8'.
// Requires AVX512BW.
func MaskUnpackhi8(src M128i, k Mmask16, a M128i, b M128i) M128i {
	return M128i(maskUnpackhi8([16]byte(src), uint16(k), [16]byte(a), [16]byte(b)))
}

func maskUnpackhi8(src [16]byte, k uint16, a [16]byte, b [16]byte) [16]byte


// MaskzUnpackhi8: Unpack and interleave 8-bit integers from the high half of
// 'a' and 'b', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_BYTES(src1[127:0], src2[127:0]){
//			dst[7:0] := src1[71:64] 
//			dst[15:8] := src2[71:64] 
//			dst[23:16] := src1[79:72] 
//			dst[31:24] := src2[79:72] 
//			dst[39:32] := src1[87:80] 
//			dst[47:40] := src2[87:80] 
//			dst[55:48] := src1[95:88] 
//			dst[63:56] := src2[95:88] 
//			dst[71:64] := src1[103:96] 
//			dst[79:72] := src2[103:96] 
//			dst[87:80] := src1[111:104] 
//			dst[95:88] := src2[111:104] 
//			dst[103:96] := src1[119:112] 
//			dst[111:104] := src2[119:112] 
//			dst[119:112] := src1[127:120] 
//			dst[127:120] := src2[127:120] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_BYTES(a[127:0], b[127:0])
//		
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := tmp_dst[i+7:i]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPUNPCKHBW'. Intrinsic: '_mm_maskz_unpackhi_epi8'.
// Requires AVX512BW.
func MaskzUnpackhi8(k Mmask16, a M128i, b M128i) M128i {
	return M128i(maskzUnpackhi8(uint16(k), [16]byte(a), [16]byte(b)))
}

func maskzUnpackhi8(k uint16, a [16]byte, b [16]byte) [16]byte


// Unpackhi8: Unpack and interleave 8-bit integers from the high half of 'a'
// and 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_HIGH_BYTES(src1[127:0], src2[127:0]){
//			dst[7:0] := src1[71:64] 
//			dst[15:8] := src2[71:64] 
//			dst[23:16] := src1[79:72] 
//			dst[31:24] := src2[79:72] 
//			dst[39:32] := src1[87:80] 
//			dst[47:40] := src2[87:80] 
//			dst[55:48] := src1[95:88] 
//			dst[63:56] := src2[95:88] 
//			dst[71:64] := src1[103:96] 
//			dst[79:72] := src2[103:96] 
//			dst[87:80] := src1[111:104] 
//			dst[95:88] := src2[111:104] 
//			dst[103:96] := src1[119:112] 
//			dst[111:104] := src2[119:112] 
//			dst[119:112] := src1[127:120] 
//			dst[127:120] := src2[127:120] 
//			RETURN dst[127:0]
//		}	
//		
//		dst[127:0] := INTERLEAVE_HIGH_BYTES(a[127:0], b[127:0])
//
// Instruction: 'PUNPCKHBW'. Intrinsic: '_mm_unpackhi_epi8'.
// Requires SSE2.
func Unpackhi8(a M128i, b M128i) M128i {
	return M128i(unpackhi8([16]byte(a), [16]byte(b)))
}

func unpackhi8(a [16]byte, b [16]byte) [16]byte


// MaskUnpackhiPd: Unpack and interleave double-precision (64-bit)
// floating-point elements from the high half of 'a' and 'b', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VUNPCKHPD'. Intrinsic: '_mm_mask_unpackhi_pd'.
// Requires AVX512F.
func MaskUnpackhiPd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskUnpackhiPd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskUnpackhiPd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzUnpackhiPd: Unpack and interleave double-precision (64-bit)
// floating-point elements from the high half of 'a' and 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VUNPCKHPD'. Intrinsic: '_mm_maskz_unpackhi_pd'.
// Requires AVX512F.
func MaskzUnpackhiPd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzUnpackhiPd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzUnpackhiPd(k uint8, a [2]float64, b [2]float64) [2]float64


// UnpackhiPd: Unpack and interleave double-precision (64-bit) floating-point
// elements from the high half of 'a' and 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//
// Instruction: 'UNPCKHPD'. Intrinsic: '_mm_unpackhi_pd'.
// Requires SSE2.
func UnpackhiPd(a M128d, b M128d) M128d {
	return M128d(unpackhiPd([2]float64(a), [2]float64(b)))
}

func unpackhiPd(a [2]float64, b [2]float64) [2]float64


// MaskUnpackhiPs: Unpack and interleave single-precision (32-bit)
// floating-point elements from the high half of 'a' and 'b', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VUNPCKHPS'. Intrinsic: '_mm_mask_unpackhi_ps'.
// Requires AVX512F.
func MaskUnpackhiPs(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskUnpackhiPs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskUnpackhiPs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzUnpackhiPs: Unpack and interleave single-precision (32-bit)
// floating-point elements from the high half of 'a' and 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VUNPCKHPS'. Intrinsic: '_mm_maskz_unpackhi_ps'.
// Requires AVX512F.
func MaskzUnpackhiPs(k Mmask8, a M128, b M128) M128 {
	return M128(maskzUnpackhiPs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzUnpackhiPs(k uint8, a [4]float32, b [4]float32) [4]float32


// UnpackhiPs: Unpack and interleave single-precision (32-bit) floating-point
// elements from the high half 'a' and 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}	
//		
//		dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//
// Instruction: 'UNPCKHPS'. Intrinsic: '_mm_unpackhi_ps'.
// Requires SSE.
func UnpackhiPs(a M128, b M128) M128 {
	return M128(unpackhiPs([4]float32(a), [4]float32(b)))
}

func unpackhiPs(a [4]float32, b [4]float32) [4]float32


// MaskUnpacklo16: Unpack and interleave 16-bit integers from the low half of
// 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		INTERLEAVE_WORDS(src1[127:0], src2[127:0]){
//			dst[15:0] := src1[15:0] 
//			dst[31:16] := src2[15:0] 
//			dst[47:32] := src1[31:16] 
//			dst[63:48] := src2[31:16] 
//			dst[79:64] := src1[47:32] 
//			dst[95:80] := src2[47:32] 
//			dst[111:96] := src1[63:48] 
//			dst[127:112] := src2[63:48] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_WORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPUNPCKLWD'. Intrinsic: '_mm_mask_unpacklo_epi16'.
// Requires AVX512BW.
func MaskUnpacklo16(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskUnpacklo16([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskUnpacklo16(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzUnpacklo16: Unpack and interleave 16-bit integers from the low half of
// 'a' and 'b', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		INTERLEAVE_WORDS(src1[127:0], src2[127:0]){
//			dst[15:0] := src1[15:0] 
//			dst[31:16] := src2[15:0] 
//			dst[47:32] := src1[31:16] 
//			dst[63:48] := src2[31:16] 
//			dst[79:64] := src1[47:32] 
//			dst[95:80] := src2[47:32] 
//			dst[111:96] := src1[63:48] 
//			dst[127:112] := src2[63:48] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_WORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 7
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPUNPCKLWD'. Intrinsic: '_mm_maskz_unpacklo_epi16'.
// Requires AVX512BW.
func MaskzUnpacklo16(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzUnpacklo16(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzUnpacklo16(k uint8, a [16]byte, b [16]byte) [16]byte


// Unpacklo16: Unpack and interleave 16-bit integers from the low half of 'a'
// and 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_WORDS(src1[127:0], src2[127:0]){
//			dst[15:0] := src1[15:0] 
//			dst[31:16] := src2[15:0] 
//			dst[47:32] := src1[31:16] 
//			dst[63:48] := src2[31:16] 
//			dst[79:64] := src1[47:32] 
//			dst[95:80] := src2[47:32] 
//			dst[111:96] := src1[63:48] 
//			dst[127:112] := src2[63:48] 
//			RETURN dst[127:0]
//		}	
//		
//		dst[127:0] := INTERLEAVE_WORDS(a[127:0], b[127:0])
//
// Instruction: 'PUNPCKLWD'. Intrinsic: '_mm_unpacklo_epi16'.
// Requires SSE2.
func Unpacklo16(a M128i, b M128i) M128i {
	return M128i(unpacklo16([16]byte(a), [16]byte(b)))
}

func unpacklo16(a [16]byte, b [16]byte) [16]byte


// MaskUnpacklo32: Unpack and interleave 32-bit integers from the low half of
// 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPUNPCKLDQ'. Intrinsic: '_mm_mask_unpacklo_epi32'.
// Requires AVX512F.
func MaskUnpacklo32(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskUnpacklo32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskUnpacklo32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzUnpacklo32: Unpack and interleave 32-bit integers from the low half of
// 'a' and 'b', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPUNPCKLDQ'. Intrinsic: '_mm_maskz_unpacklo_epi32'.
// Requires AVX512F.
func MaskzUnpacklo32(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzUnpacklo32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzUnpacklo32(k uint8, a [16]byte, b [16]byte) [16]byte


// Unpacklo32: Unpack and interleave 32-bit integers from the low half of 'a'
// and 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//
// Instruction: 'PUNPCKLDQ'. Intrinsic: '_mm_unpacklo_epi32'.
// Requires SSE2.
func Unpacklo32(a M128i, b M128i) M128i {
	return M128i(unpacklo32([16]byte(a), [16]byte(b)))
}

func unpacklo32(a [16]byte, b [16]byte) [16]byte


// MaskUnpacklo64: Unpack and interleave 64-bit integers from the low half of
// 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPUNPCKLQDQ'. Intrinsic: '_mm_mask_unpacklo_epi64'.
// Requires AVX512F.
func MaskUnpacklo64(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskUnpacklo64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskUnpacklo64(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzUnpacklo64: Unpack and interleave 64-bit integers from the low half of
// 'a' and 'b', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPUNPCKLQDQ'. Intrinsic: '_mm_maskz_unpacklo_epi64'.
// Requires AVX512F.
func MaskzUnpacklo64(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzUnpacklo64(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzUnpacklo64(k uint8, a [16]byte, b [16]byte) [16]byte


// Unpacklo64: Unpack and interleave 64-bit integers from the low half of 'a'
// and 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//
// Instruction: 'PUNPCKLQDQ'. Intrinsic: '_mm_unpacklo_epi64'.
// Requires SSE2.
func Unpacklo64(a M128i, b M128i) M128i {
	return M128i(unpacklo64([16]byte(a), [16]byte(b)))
}

func unpacklo64(a [16]byte, b [16]byte) [16]byte


// MaskUnpacklo8: Unpack and interleave 8-bit integers from the low half of 'a'
// and 'b', and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		INTERLEAVE_BYTES(src1[127:0], src2[127:0]){
//			dst[7:0] := src1[7:0] 
//			dst[15:8] := src2[7:0] 
//			dst[23:16] := src1[15:8] 
//			dst[31:24] := src2[15:8] 
//			dst[39:32] := src1[23:16] 
//			dst[47:40] := src2[23:16] 
//			dst[55:48] := src1[31:24] 
//			dst[63:56] := src2[31:24] 
//			dst[71:64] := src1[39:32]
//			dst[79:72] := src2[39:32] 
//			dst[87:80] := src1[47:40] 
//			dst[95:88] := src2[47:40] 
//			dst[103:96] := src1[55:48] 
//			dst[111:104] := src2[55:48] 
//			dst[119:112] := src1[63:56] 
//			dst[127:120] := src2[63:56] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_BYTES(a[127:0], b[127:0])
//		
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := tmp_dst[i+7:i]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPUNPCKLBW'. Intrinsic: '_mm_mask_unpacklo_epi8'.
// Requires AVX512BW.
func MaskUnpacklo8(src M128i, k Mmask16, a M128i, b M128i) M128i {
	return M128i(maskUnpacklo8([16]byte(src), uint16(k), [16]byte(a), [16]byte(b)))
}

func maskUnpacklo8(src [16]byte, k uint16, a [16]byte, b [16]byte) [16]byte


// MaskzUnpacklo8: Unpack and interleave 8-bit integers from the low half of
// 'a' and 'b', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		INTERLEAVE_BYTES(src1[127:0], src2[127:0]){
//			dst[7:0] := src1[7:0] 
//			dst[15:8] := src2[7:0] 
//			dst[23:16] := src1[15:8] 
//			dst[31:24] := src2[15:8] 
//			dst[39:32] := src1[23:16] 
//			dst[47:40] := src2[23:16] 
//			dst[55:48] := src1[31:24] 
//			dst[63:56] := src2[31:24] 
//			dst[71:64] := src1[39:32]
//			dst[79:72] := src2[39:32] 
//			dst[87:80] := src1[47:40] 
//			dst[95:88] := src2[47:40] 
//			dst[103:96] := src1[55:48] 
//			dst[111:104] := src2[55:48] 
//			dst[119:112] := src1[63:56] 
//			dst[127:120] := src2[63:56] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_BYTES(a[127:0], b[127:0])
//		
//		FOR j := 0 to 15
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := tmp_dst[i+7:i]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPUNPCKLBW'. Intrinsic: '_mm_maskz_unpacklo_epi8'.
// Requires AVX512BW.
func MaskzUnpacklo8(k Mmask16, a M128i, b M128i) M128i {
	return M128i(maskzUnpacklo8(uint16(k), [16]byte(a), [16]byte(b)))
}

func maskzUnpacklo8(k uint16, a [16]byte, b [16]byte) [16]byte


// Unpacklo8: Unpack and interleave 8-bit integers from the low half of 'a' and
// 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_BYTES(src1[127:0], src2[127:0]){
//			dst[7:0] := src1[7:0] 
//			dst[15:8] := src2[7:0] 
//			dst[23:16] := src1[15:8] 
//			dst[31:24] := src2[15:8] 
//			dst[39:32] := src1[23:16] 
//			dst[47:40] := src2[23:16] 
//			dst[55:48] := src1[31:24] 
//			dst[63:56] := src2[31:24] 
//			dst[71:64] := src1[39:32]
//			dst[79:72] := src2[39:32] 
//			dst[87:80] := src1[47:40] 
//			dst[95:88] := src2[47:40] 
//			dst[103:96] := src1[55:48] 
//			dst[111:104] := src2[55:48] 
//			dst[119:112] := src1[63:56] 
//			dst[127:120] := src2[63:56] 
//			RETURN dst[127:0]
//		}	
//		
//		dst[127:0] := INTERLEAVE_BYTES(a[127:0], b[127:0])
//
// Instruction: 'PUNPCKLBW'. Intrinsic: '_mm_unpacklo_epi8'.
// Requires SSE2.
func Unpacklo8(a M128i, b M128i) M128i {
	return M128i(unpacklo8([16]byte(a), [16]byte(b)))
}

func unpacklo8(a [16]byte, b [16]byte) [16]byte


// MaskUnpackloPd: Unpack and interleave double-precision (64-bit)
// floating-point elements from the low half of 'a' and 'b', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VUNPCKLPD'. Intrinsic: '_mm_mask_unpacklo_pd'.
// Requires AVX512F.
func MaskUnpackloPd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskUnpackloPd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskUnpackloPd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzUnpackloPd: Unpack and interleave double-precision (64-bit)
// floating-point elements from the low half of 'a' and 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VUNPCKLPD'. Intrinsic: '_mm_maskz_unpacklo_pd'.
// Requires AVX512F.
func MaskzUnpackloPd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzUnpackloPd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzUnpackloPd(k uint8, a [2]float64, b [2]float64) [2]float64


// UnpackloPd: Unpack and interleave double-precision (64-bit) floating-point
// elements from the low half of 'a' and 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//
// Instruction: 'UNPCKLPD'. Intrinsic: '_mm_unpacklo_pd'.
// Requires SSE2.
func UnpackloPd(a M128d, b M128d) M128d {
	return M128d(unpackloPd([2]float64(a), [2]float64(b)))
}

func unpackloPd(a [2]float64, b [2]float64) [2]float64


// MaskUnpackloPs: Unpack and interleave single-precision (32-bit)
// floating-point elements from the low half of 'a' and 'b', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VUNPCKLPS'. Intrinsic: '_mm_mask_unpacklo_ps'.
// Requires AVX512F.
func MaskUnpackloPs(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskUnpackloPs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskUnpackloPs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzUnpackloPs: Unpack and interleave single-precision (32-bit)
// floating-point elements from the low half of 'a' and 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VUNPCKLPS'. Intrinsic: '_mm_maskz_unpacklo_ps'.
// Requires AVX512F.
func MaskzUnpackloPs(k Mmask8, a M128, b M128) M128 {
	return M128(maskzUnpackloPs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzUnpackloPs(k uint8, a [4]float32, b [4]float32) [4]float32


// UnpackloPs: Unpack and interleave single-precision (32-bit) floating-point
// elements from the low half of 'a' and 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//
// Instruction: 'UNPCKLPS'. Intrinsic: '_mm_unpacklo_ps'.
// Requires SSE.
func UnpackloPs(a M128, b M128) M128 {
	return M128(unpackloPs([4]float32(a), [4]float32(b)))
}

func unpackloPs(a [4]float32, b [4]float32) [4]float32


// Urem32: Divide packed unsigned 32-bit integers in 'a' by packed elements in
// 'b', and store the remainders as packed unsigned 32-bit integers in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			dst[i+31:i] := REMAINDER(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: '...'. Intrinsic: '_mm_urem_epi32'.
// Requires SSE.
func Urem32(a M128i, b M128i) M128i {
	return M128i(urem32([16]byte(a), [16]byte(b)))
}

func urem32(a [16]byte, b [16]byte) [16]byte


// MaskXor32: Compute the bitwise XOR of packed 32-bit integers in 'a' and 'b',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPXORD'. Intrinsic: '_mm_mask_xor_epi32'.
// Requires AVX512F.
func MaskXor32(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskXor32([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskXor32(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzXor32: Compute the bitwise XOR of packed 32-bit integers in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPXORD'. Intrinsic: '_mm_maskz_xor_epi32'.
// Requires AVX512F.
func MaskzXor32(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzXor32(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzXor32(k uint8, a [16]byte, b [16]byte) [16]byte


// MaskXor64: Compute the bitwise XOR of packed 64-bit integers in 'a' and 'b',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPXORQ'. Intrinsic: '_mm_mask_xor_epi64'.
// Requires AVX512F.
func MaskXor64(src M128i, k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskXor64([16]byte(src), uint8(k), [16]byte(a), [16]byte(b)))
}

func maskXor64(src [16]byte, k uint8, a [16]byte, b [16]byte) [16]byte


// MaskzXor64: Compute the bitwise XOR of packed 64-bit integers in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPXORQ'. Intrinsic: '_mm_maskz_xor_epi64'.
// Requires AVX512F.
func MaskzXor64(k Mmask8, a M128i, b M128i) M128i {
	return M128i(maskzXor64(uint8(k), [16]byte(a), [16]byte(b)))
}

func maskzXor64(k uint8, a [16]byte, b [16]byte) [16]byte


// MaskXorPd: Compute the bitwise XOR of packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VXORPD'. Intrinsic: '_mm_mask_xor_pd'.
// Requires AVX512DQ.
func MaskXorPd(src M128d, k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskXorPd([2]float64(src), uint8(k), [2]float64(a), [2]float64(b)))
}

func maskXorPd(src [2]float64, k uint8, a [2]float64, b [2]float64) [2]float64


// MaskzXorPd: Compute the bitwise XOR of packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VXORPD'. Intrinsic: '_mm_maskz_xor_pd'.
// Requires AVX512DQ.
func MaskzXorPd(k Mmask8, a M128d, b M128d) M128d {
	return M128d(maskzXorPd(uint8(k), [2]float64(a), [2]float64(b)))
}

func maskzXorPd(k uint8, a [2]float64, b [2]float64) [2]float64


// XorPd: Compute the bitwise XOR of packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*64
//			dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
//		ENDFOR
//
// Instruction: 'XORPD'. Intrinsic: '_mm_xor_pd'.
// Requires SSE2.
func XorPd(a M128d, b M128d) M128d {
	return M128d(xorPd([2]float64(a), [2]float64(b)))
}

func xorPd(a [2]float64, b [2]float64) [2]float64


// MaskXorPs: Compute the bitwise XOR of packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VXORPS'. Intrinsic: '_mm_mask_xor_ps'.
// Requires AVX512DQ.
func MaskXorPs(src M128, k Mmask8, a M128, b M128) M128 {
	return M128(maskXorPs([4]float32(src), uint8(k), [4]float32(a), [4]float32(b)))
}

func maskXorPs(src [4]float32, k uint8, a [4]float32, b [4]float32) [4]float32


// MaskzXorPs: Compute the bitwise XOR of packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VXORPS'. Intrinsic: '_mm_maskz_xor_ps'.
// Requires AVX512DQ.
func MaskzXorPs(k Mmask8, a M128, b M128) M128 {
	return M128(maskzXorPs(uint8(k), [4]float32(a), [4]float32(b)))
}

func maskzXorPs(k uint8, a [4]float32, b [4]float32) [4]float32


// XorPs: Compute the bitwise XOR of packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
//		ENDFOR
//
// Instruction: 'XORPS'. Intrinsic: '_mm_xor_ps'.
// Requires SSE.
func XorPs(a M128, b M128) M128 {
	return M128(xorPs([4]float32(a), [4]float32(b)))
}

func xorPs(a [4]float32, b [4]float32) [4]float32


// XorSi128: Compute the bitwise XOR of 128 bits (representing integer data) in
// 'a' and 'b', and store the result in 'dst'. 
//
//		dst[127:0] := (a[127:0] XOR b[127:0])
//
// Instruction: 'PXOR'. Intrinsic: '_mm_xor_si128'.
// Requires SSE2.
func XorSi128(a M128i, b M128i) M128i {
	return M128i(xorSi128([16]byte(a), [16]byte(b)))
}

func xorSi128(a [16]byte, b [16]byte) [16]byte

